# Use default configuration if not shown
megatron:
  set_environment:
    - source /root/miniconda3/etc/profile.d/conda.sh
    - conda activate flagscale-train
    - python tools/patch/unpatch.py --backend Megatron-LM
    - cd third_party/Megatron-LM
    - export PYTHONPATH=../..:$PYTHONPATH
    - export NVTE_FLASH_ATTN=0
    - export NVTE_FUSED_ATTN=0
    - ulimit -n 65535
  coverage_fold:
    core
  subset:
    data:
      deselect:
      - skip_unit_test_modules
    dist_checkpointing:
      type: single
      ignore:
        - models/test_mamba.py
      deselect:
        - skip_unit_test_modules
    distributed:
      type: single
    export:
      - default
    fusions:
      - default
    inference:
      ignore:
        - skip_unit_test_modules
      deselect:
        - engines/test_dynamic_engine.py::TestDynamicInferenceEngine
    models:
      ignore:
        - test_mamba_model.py
      deselect:
        - test_t5_model.py::TestT5Model::test_post_process_forward
        - test_t5_model.py::TestT5Model::test_forward_output_encoder_hidden_only
        - test_t5_model.py::TestT5Model::test_forward_with_encoder_hidden_states
        - test_llava_model.py::TestLLaVAModel::test_forward
    pipeline_parallel:
      - default
    post_training:
      ignore: 
        - test_modelopt_module_spec.py
    ssm:
      ignore:
        - test_mamba_block.py
        - test_mamba_layer.py
        - test_mamba_mixer.py
    tensor_parallel:
      - default
    transformer/moe:
      type: single
      ignore:
        - skip_unit_test_modules
      deselect:
        - test_a2a_token_dispatcher.py::TestAlltoAllDispatcher
        - test_moe_layer_discrepancy.py
    transformer:
      depth: 1
      ignore:
        - test_transformer_block_custom_pgs.py
      deselect:
        - test_attention.py::TestParallelAttention::test_gpu_forward
        - test_attention.py::TestParallelAttention::test_fused_rope_gpu_forward
        - test_attention.py::TestParallelAttention::test_checkpointed_gpu_forward
        - test_attention_packed_seq.py::TestParallelAttentionWithPackedSequence::test_gpu_forward
        - test_attention_packed_seq.py::TestParallelAttentionWithPackedSequence::test_fused_rope_gpu_forward
        - test_attention_packed_seq.py::TestParallelAttentionWithPackedSequence::test_checkpointed_gpu_forward
        - test_attention_packed_seq.py::TestParallelAttentionWithPackedPaddedSequence::test_gpu_forward
        - test_attention_packed_seq.py::TestParallelAttentionWithPackedPaddedSequence::test_fused_rope_gpu_forward
        - test_attention_packed_seq.py::TestParallelAttentionWithPackedPaddedSequence::test_checkpointed_gpu_forward
        - test_multi_latent_attention.py::TestParallelMLAAttention::test_gpu_forward
        - test_multi_latent_attention.py::TestParallelMLAAttentionPrecision::test_gpu_forward_thd_precision
        - test_multi_token_prediction.py::TestMultiTokenPrediction::test_fp8_support
        - test_retro_attention.py::TestRetroAttention::test_gpu_forward
    ./:
      depth: 1
      ignore:
        - skip_unit_test_modules
      deselect:
        - skip_unit_test_modules

flagscale:
  set_environment:
    - source /root/miniconda3/etc/profile.d/conda.sh
    - conda activate flagscale-train
    - python tools/patch/unpatch.py --backend Megatron-LM
    - export PYTHONPATH=./third_party/Megatron-LM:$PYTHONPATH
    - export NVTE_FLASH_ATTN=0
    - export NVTE_FUSED_ATTN=0
    - ulimit -n 65535
  subset:
    runner:
      type: batch
      depth: all
      ignore:
        - test_parse_hostfile.py
    ./:
      deselect:
        - test_spiky_loss_detector.py::test_spiky_loss_detector
      depth: 1
