# Use default configuration if not shown
megatron:
  set_environment:
    - source /root/miniconda3/etc/profile.d/conda.sh
    - conda activate flagscale-train
    - cd megatron
    - export PYTHONPATH=..:$PYTHONPATH
    - export NVTE_FLASH_ATTN=0
    - export NVTE_FUSED_ATTN=0
    - ulimit -n 65535
  coverage:
    core
  subset:
    data:
      deselect:
       - test_builder.py::test_builder
       - test_multimodal_dataset.py::test_multimodal_dataset
       - test_multimodal_dataset.py::test_mock_multimodal_dataset
       - test_gpt_dataset.py::test_mock_gpt_dataset
    dist_checkpointing:
      type: single
      ignore:
        - models/test_mamba.py
        - test_fp8.py
        - test_replication.py
      deselect:
        - models/test_retro_model.py::TestRetroModel::test_sharded_state_dict_save_load
        - test_fully_parallel.py::TestFullyParallelSaveAndLoad::test_memory_usage
    distributed:
      type: single
    models:
      ignore:
        - test_mamba_model.py
      deselect:
        - test_t5_model.py::TestT5Model::test_post_process_forward
        - test_t5_model.py::TestT5Model::test_forward_output_encoder_hidden_only
        - test_t5_model.py::TestT5Model::test_forward_with_encoder_hidden_states
        - test_llava_model.py::test_llava_model_parallelism
    inference:
      ignore:
        - engines/test_mcore_engine.py
        - test_modelopt_module_spec.py
        - text_generation_controllers/test_encoder_decoder_text_generation_controller.py
        - text_generation_controllers/test_simple_text_generation_controller.py
        - text_generation_controllers/test_vlm_text_generation_controller.py
    ssm:
      ignore:
        - test_mamba_block.py
        - test_mamba_hybrid_layer_allocation.py
        - test_mamba_layer.py
        - test_mamba_mixer.py
    transformer/moe:
      type: single
      ignore:
        - test_upcycling.py
      deselect:
        - test_a2a_token_dispatcher.py::TestAlltoAllDispatcher
        - test_grouped_mlp.py::TestTEGroupedMLP::test_gpu_forward_backward_with_no_tokens_allocated
        - test_moe_layer.py::TestMoELayerInit::test_moe_with_late_initialize
    transformer:
      depth: 1
      deselect:
        - test_attention.py::TestParallelAttention::test_gpu_forward
        - test_attention.py::TestParallelAttention::test_fused_rope_gpu_forward
        - test_attention.py::TestParallelAttention::test_checkpointed_gpu_forward
        - test_multi_latent_attention.py::TestParallelMLAAttention::test_gpu_forward
        - test_multi_latent_attention.py::TestParallelMLAAttention::test_fused_rope_gpu_forward
        - test_multi_latent_attention.py::TestParallelMLAAttention::test_checkpointed_gpu_forward
        - test_retro_attention.py::TestRetroAttention::test_gpu_forward
        - test_transformer_block.py::TestPipelineParallelTransformerBlock::test_layer_builder
    ./:
      depth: 1
      ignore:
        - test_utilities.py
      deselect:
        - test_parallel_state.py::test_different_initialize_order_unconsistency

flagscale:
  set_environment:
    - source /root/miniconda3/etc/profile.d/conda.sh
    - conda activate flagscale-train
    - export PYTHONPATH=./megatron:$PYTHONPATH
    - export NVTE_FLASH_ATTN=0
    - export NVTE_FUSED_ATTN=0
    - ulimit -n 65535
  subset:
    runner:
      type: batch
      depth: all
    ./:
      depth: 1
