defaults:
  - _self_
  - train: tp4_pp1_ep2

experiment:
  exp_name: tp4_pp1_ep2
  exp_dir: tests/functional_tests/test_cases/train/mixtral/results_test/tp4_pp1_ep2
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py
  runner:
    backend: torchrun
    ssh_port: null
  shell_cmds: null
  envs:
    HYDRA_FULL_ERROR: 1
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    CUBLAS_WORKSPACE_CONFIG: ":4096:8"
    NCCL_ALGO: "Tree"
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NVTE_FLASH_ATTN: 0
    NVTE_FUSED_ATTN: 0
    CUDNN_BENCHMARK: "false"
    CUDNN_DETERMINISTIC: "true"
  cmds:
    before_start:
      python tools/patch/unpatch.py --backend Megatron-LM;
      source /root/miniconda3/bin/activate flagscale-train
action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
