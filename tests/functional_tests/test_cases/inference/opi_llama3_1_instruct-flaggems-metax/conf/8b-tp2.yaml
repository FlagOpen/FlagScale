defaults:
  - _self_
  - inference: 8b-tp2

experiment:
  exp_name: opi_llama3_1_instruct-flaggems-metax
  exp_dir: tests/functional_tests/test_cases/inference/opi_llama3_1_instruct-flaggems-metax/results_test/8b-tp2
  task:
    type: inference
    backend: vllm
    entrypoint: flagscale/inference/inference_aquila.py
  runner:
    hostfile: null
  cmds:
    before_start:
      source /opt/conda/bin/activate flagscale-inference
  envs:
    HYDRA_FULL_ERROR: 1
    CUDA_VISIBLE_DEVICES: "0,1"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    CUBLAS_WORKSPACE_CONFIG: ":4096:8"
    NCCL_ALGO: "Ring"
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NVTE_FLASH_ATTN: 0
    NVTE_FUSED_ATTN: 0
    CUDNN_BENCHMARK: "false"
    CUDNN_DETERMINISTIC: "true"
    USE_FLAGGEMS: "true"
    GEMS_VENDOR: "metax"

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
