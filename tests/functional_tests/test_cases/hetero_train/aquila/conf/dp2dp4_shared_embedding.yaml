defaults:
  - _self_
  - train: dp2dp4_shared_embedding

experiment:
  exp_name: dp2dp4_shared_embedding
  exp_dir: tests/functional_tests/test_cases/hetero_train/aquila/results_test/dp2dp4_shared_embedding
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py
  runner:
    backend: torchrun
    ssh_port: null
  shell_cmds: null
  envs:
    HYDRA_FULL_ERROR: 1
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    CUBLAS_WORKSPACE_CONFIG: ":4096:8"
    NCCL_ALGO: "Tree"
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NVTE_FLASH_ATTN: 0
    NVTE_FUSED_ATTN: 0
    CUDNN_BENCHMARK: "false"
    CUDNN_DETERMINISTIC: "true"
  cmds:
    before_start: before_start: source /root/miniconda3/bin/activate flagscale-train
action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
