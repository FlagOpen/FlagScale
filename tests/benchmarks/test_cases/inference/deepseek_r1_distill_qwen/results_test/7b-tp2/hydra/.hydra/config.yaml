experiment:
  exp_name: deepseek_r1_distill_qwen
  exp_dir: tests/benchmarks/test_cases/inference/deepseek_r1_distill_qwen/results_test/7b-tp2
  task:
    type: inference
    backend: vllm
    entrypoint: flagscale/benchmarks/benchmark_throughput.py
  runner:
    hostfile: null
  cmds:
    before_start: source /root/miniconda3/bin/activate flagscale-inference
  envs:
    HYDRA_FULL_ERROR: 1
    CUBLAS_WORKSPACE_CONFIG: :4096:8
    CUDNN_BENCHMARK: 'false'
    CUDNN_DETERMINISTIC: 'true'
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NVTE_FLASH_ATTN: 0
    NVTE_FUSED_ATTN: 0
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NCCL_ALGO: Ring
    NCCL_PROTOCOL: LLC
    SEED: 1234
    PYTHONHASHSEED: 0
    MKL_NUM_THREADS: 1
    OMP_NUM_THREADS: 1
    NUMEXPR_NUM_THREADS: 1
    SCIPY_RDRANDOM: 0
    TF_DETERMINISTIC_OPS: 1
    TORCH_CUDNN_DETERMINISM: true
    CUDA_LAUNCH_BLOCKING: 1
    NCCL_DEBUG: INFO
    MAGIC_CACHE: disabled
action: test
inference:
  llm:
    model: /home/gitlab-runner/data/DeepSeek-R1-Distill-Qwen-7B
    tokenizer: /home/gitlab-runner/data/DeepSeek-R1-Distill-Qwen-7B
    trust_remote_code: true
    tensor_parallel_size: 2
    pipeline_parallel_size: 1
    gpu_memory_utilization: 0.9
    seed: 1234
    enforce_eager: true
  generate:
    prompts:
    - The president of the United States
    sampling:
      top_p: 0.1
      top_k: 1
      temperature: 0.0
      seed: 1234
    num_iters: 10
