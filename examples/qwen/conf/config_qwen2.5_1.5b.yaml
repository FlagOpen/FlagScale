defaults:
  - _self_
  - train: train_qwen_2.5_1.5b
  # - train: train_mixtral_1.8b

experiment:
  exp_name: train_qwen_2.5_1.5b
  exp_dir: ./outputs # outputs ## log„ÄÅcheckpoints output path
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/train_aquila.py
  runner:
    backend: torchrun
    nnodes: 2 
    nproc_per_node: 8
    hostfile: torchrun # Please replace with your actual hostfile path
  envs:
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NCCL_SOCKET_IFNAME: eth0
    NCCL_IB_DISABLE: 0
    NCCL_IB_CUDA_SUPPORT: 1
    NCCL_IB_GID_INDEX: 0
    NCCL_DEBUG: INFO
    OMP_NUM_THREADS: 4
    GLOO_SOCKET_IFNAME: eth0
    NCCL_IB_HCA: mlx5_2,mlx5_5

action: run
