defaults:
  - _self_
  - serve: serve_multiple_qwen2.5_7b

experiment:
  exp_name: qwen2.5_7b
  exp_dir: outputs/${experiment.exp_name}
  task:
    type: serve
    inference_engine: vllm
    entrypoint: null
  runner:
    nnodes: 1
    nproc_per_node: 8
  auto_tuner:
    space:
      tensor_model_parallel_size: [2,4]
      pipeline_model_parallel_size: [1,2,4]
      instance: [1,2,4]
      block_size: [16]
      max_num_batched_tokens: [512, 1024, 2048]
      max_num_seqs: [128, 256]
    control:
      interval: 20
      run_best: False
  cmds:
    before_start: export RAY_DEDUP_LOGS=0

action: auto_tune

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
