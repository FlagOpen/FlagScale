model_args:
  vllm_model:
    model: /models/Qwen2.5-32B-Instruct
    tensor_parallel_size: 2
    pipeline_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_model_len: 32768
    max_num_seqs: 256
    enforce_eager: True
    trust_remote_code: True
    enable_chunked_prefill: True

deploy:
  models:
    vllm_model:
      num_replicas: 4
      num_gpus: 1
  service:
    port: 6701
  command_line_mode: true
