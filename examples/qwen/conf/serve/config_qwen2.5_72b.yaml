command-line-mode: true

llm:
  model-tag: /models/qwen/weights/ 
  tensor-parallel-size: 4
  gpu-memory-utilization: 0.9
  max-model-len: 32768
  max-num-seqs: 256
  port: 4567
  action-args:
    - trust-remote-code
    - enable-chunked-prefill

deploy:
  - instance: qwen2.5-72b
    models:
      - model_name: vllm_serve
        num_gpus: 4
