# atmb: Auto Tuning for Multi-Backend
defaults:
  - _self_
  - serve: 0_6b

experiment:
  exp_name: qwen3_0.6b
  exp_dir: outputs/${experiment.exp_name}
  task:
    type: serve
  runner:
    hostfile: examples/qwen3/conf/hostfile.txt
    docker: ds
    nnodes: ${oc.env:LWS_GROUP_SIZE, 1}
    master_addr: ${oc.env:MASTER_ADDR, 127.0.0.1}
    master_port: ${oc.env:MASTER_PORT, 20036}
    deploy:
      port: 6706
      use_fs_serve: false
  cmds:
    before_start: source /root/miniconda3/bin/activate flagscale-inference
  envs:
    CUDA_VISIBLE_DEVICES: 0
    CUDA_DEVICE_MAX_CONNECTIONS: 1
  node_args:
    10.6.208.16:
      engine_args:
        model: /mnt/Qwen2.5-0.5B-Instruct/
    10.6.208.15:
        #model: /models/Qwen2.5-0.5B-Instruct/ 

  auto_tuner:
    engines: [vllm, sglang, llama_cpp]
    space:
      vllm:
        tensor_model_parallel_size: [1,2,4]
        pipeline_model_parallel_size: "auto"
        block_size: [32, 64]
        max_num_batched_tokens: [512, 1024, 2048]
        max_num_seqs: [128, 256]
      llama_cpp:
        threads: [32]
        split_mode: [none, layer, row]
      sglang:
        tensor_model_parallel_size: [4]
        # Error occured when PP with SGLang==main (v0.4.6 not support):
        # AttributeError: 'PPMissingLayer' object has no attribute 'quant_method'
        pipeline_model_parallel_size: [1]
        chunked_prefill_size: [128, 256, 512]
        page_size: [16, 32]
        max_running_requests: [32, 64, 128]
    cards: 4
    control:
      interval: 10
      run_best: False

action: auto_tune

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
