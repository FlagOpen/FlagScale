defaults:
  - _self_
  - train: 14b

experiment:
  exp_name: Qwen3-14b-Train
  seed: 42
  save_steps: 10000
  load: null
  exp_dir: /share/project/lixianduo/magi_attn/experiments/${experiment.exp_name}
  ckpt_format: torch
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py
  runner:
    per_node_task: false
    no_shared_fs: false
    rdzv_backend: static
    # hostfile: null
    hostfile: /share/project/lixianduo/magi_attn/4nodes_hostfile
  cmds:
    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate /share/project/lixianduo/envs/flagscale-magi && export https_proxy=http://10.8.48.6:2080 && export http_proxy=http://10.8.48.6:2080
  envs:
    LOGLEVEL: "INFO"
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NCCL_SOCKET_IFNAME: eth0
    GLOO_SOCKET_IFNAME: eth0
    NCCL_IB_DISABLE: 0
    NCCL_IB_CUDA_SUPPORT: 1
    NCCL_IB_GID_INDEX: 7
    NCCL_IB_TIMEOUT: 22
    NCCL_IB_RETRY_CNT: 13
    OMP_NUM_THREADS: 4
    TOKENIZERS_PARALLELISM: true
    TORCH_DISABLE_SHARE_RDZV_TCP_STORE: 1
    NCCL_IB_HCA: mlx5_100,mlx5_101,mlx5_102,mlx5_103,mlx5_104,mlx5_105,mlx5_106,mlx5_107

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
