- serve_id: llama_cpp_model
  engine: llama_cpp
  engine_args:
    model: /tmp/models/Qwen3-0.6B/ggml_model_f16.gguf
    host: 0.0.0.0
    tensor_parallel_size: 1
    pipeline_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_model_len: 32768
    max_num_seqs: 256
    enforce_eager: true
    trust_remote_code: true
    enable_chunked_prefill: true
