- serve_id: vllm_model
  engine: llama_cpp
  engine_args:
    model: /tmp/models/Qwen3-0.6B/ggml_model_f16.gguf
    host: 0.0.0.0
    port: 8080
    max_model_len: 1K
    max_num_seqs: 4
    kv_cache_dtype: f16
