defaults:
  - _self_
  - train: train_mixtral_8x7b

experiment:
  exp_name: mixtral-8x7b
  exp_dir: outputs
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py 
  runner:
    backend: torchrun
    hostfile: <xxxx>
  envs:
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra 
