action: run
defaults:
- train: train_llama3_70b_finetune
- _self_
experiment:
  cmds:
    before_start: source ~/.bashrc; export LD_LIBRARY_PATH=/share/project/PUBLIC/data/llama3-70b/xpu_output//xccl/3.0.0.4_20241107/xccl_rdma-ubuntu_x86_64/so/:/share/project/PUBLIC/data/llama3-70b/xpu_output//xhpc/20241107/xhpc-ubuntu2004_x86_64/xblas/so:/share/project/PUBLIC/data/llama3-70b/xpu_output//xhpc/20241107/xhpc-ubuntu2004_x86_64/xfa/so:/share/project/PUBLIC/data/llama3-70b/xpu_output//xhpc/20241107/xhpc-ubuntu2004_x86_64/xpudnn/so:/share/project/PUBLIC/data/llama3-70b/xpu_output//xre/5.0.21.5/xre-Linux-x86_64-5.0.21.5/so
  envs:
    ALLGATHER_ASYNC: false
    ALLREDUCE_ASYNC: false
    ALLREDUCE_FUSION: 0
    BKCL_CCIX_BUFFER_GM: 1
    BKCL_CCIX_RING: 1
    BKCL_ENABLE_XDR: 1
    BKCL_FLAT_RING: 1
    BKCL_KL3_TURBO_MODE: 1
    BKCL_RDMA_FORCE_TREE: 1
    BKCL_RDMA_NICS: ens11np0,ens11np0,ens13np0,ens13np0,ens15np0,ens15np0,ens17np0,ens17np0
    BKCL_RDMA_PROXY_DISABLE: 1
    BKCL_RING_BUFFER_GM: 1
    BKCL_TIMEOUT: 360000
    BKCL_TRANS_UNSUPPORTED_DATATYPE: 8
    BKCL_TREE_THRESHOLD: 1
    BKCL_XLINK_C2C: 1
    BKCL_XLINK_D2D: 0
    BKCL_XLINK_ETH: 0
    CUDART_DUMMY_REGISTER: 1
    CUDA_DEVICE_MAX_CONNECTIONS: 8
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    DIST_MULTI_STREAM: true
    FAST_SWIGLU_ENABLE: 1
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NVTE_APPLY_QK_LAYER_SCALING: 0
    USE_FAST_BF16_FC: true
    USE_L3: 1
    XBLAS_FC_HBM_VERSION: 40
    XDNN_USE_FAST_SWISH: true
    XMLIR_BATCH_PARALLEL: true
    XMLIR_DISABLE_CUDA_ALLOCATOR: true
    XMLIR_DIST_ASYNC_ISEND_IRECV: 1
    XMLIR_DIST_SINGLETON_STREAM: true
    XMLIR_DUMP_FALLBACK_OP_LIST_BOOL: true
    XMLIR_ENABLE_FALLBACK_TO_CPU_BOOL: false
    XMLIR_FA_GEMM_TYPE: float
    XMLIR_PARALLEL_SAVE_MEMORY: false
    XMLIR_XDNN_PYTORCH_CHECK_ENABLE_FALLBACK_BOOL: 0
    XPU_FORCE_USERMODE_LAUNCH: 1
    XPU_ZEBU_MODE: 1
  exp_dir: /share/project/PUBLIC/data/llama3-70b/FlagOpen/FlagPerf/training/result/run20241120200156/llama3_70B_continuetrain:flagscale_llama:R300p:4:8:1/round1/10.1.15.7_noderank0/outputs_llama3
  exp_name: llama3
  runner:
    backend: torchrun
    hostfile: /share/project/PUBLIC/data/llama3-70b/FlagOpen/FlagScale/hostfile
    nnodes: 4
    nproc_per_node: 8
    ssh_port: 3702
  task:
    backend: megatron
    entrypoint: ./flagscale/train/train_llama.py
    type: train
hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
