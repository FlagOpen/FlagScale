# defaults:
#   - _self_
#   - train: train_mistral

# experiment:
#   exp_name: mistral
#   exp_dir: output/${experiment.exp_name}
#   task:
#     type: train
#     backend: megatron
#     entrypoint: flagscale/train/train_mistral.py 
#   runner:
#     backend: torchrun
#     hostfile: hostfile
#   envs:
#     CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
#     CUDA_DEVICE_MAX_CONNECTIONS: 1

# action: run

# hydra:
#   run:
#     dir: ${experiment.exp_dir}/hydra



defaults:
  - _self_
  - train: train_mistral

experiment:
  exp_name: mistral-output-usp4
  exp_dir: /share/project/zhaoyingli/FlagScale/${experiment.exp_name}
  task:
    type: train
    backend: megatron # vllm
    entrypoint: flagscale/train/train_mistral.py 
  runner:
    backend: torchrun
    hostfile: /share/project/zhaoyingli/hostfile
  envs:
    NVTE_FLASH_ATTN: 1
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NCCL_ALGO: Ring
    CUDA_VISIBLE_DEVICES: "0,1,2,3"
    CUDA_DEVICE_MAX_CONNECTIONS: 1

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
