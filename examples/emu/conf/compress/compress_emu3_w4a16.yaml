system:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  use_flash_attn: True
  sequence_parallel: True
  logging:
    log_interval: 1
    tensorboard_log_interval: 1
    wandb_project: "compress-emu3-7B" 
    wandb_exp_name: "compress-test-7B" 
  save_dir: 


model: 
  model_cls: AutoModelForCausalLM
  model_path: BAAI/Emu3-Gen/
  device_map: cuda:0
  trust_remote_code: true
  torch_dtype: bfloat16

data:
  data_path: 
  num_calibration_samples: 16
  max_seq_length: 9216
  tokenzier_args:
    tokenizer_path: BAAI/Emu3-Gen/
    special_tokens_file: BAAI/Emu3-Gen/emu3_vision_tokens.txt
    trust_remote_code: true

compress_args:
  quantization:
    - algo:
        gptq: 
          blocksize: 128
          percdamp: 0.01
      ignore: ["lm_head"]
      targets: ["Linear"]
      scheme: W4A16


