defaults:
  - train: demo
  - _self_

experiment:
  exp_name: aquila2
  exp_dir: ./outputs
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/train_gpt.py
  runner:
    backend: torchrun
    nnodes: 1 
    nproc_per_node: 8 
  envs:
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1
  auto_tuner:
    space:
      data_parallel_size: "auto"
      use_distributed_optimizer: [true, false]
      tensor_model_parallel_size: [2, 4, 8]
      sequence_parallel: [true]
      pipeline_model_parallel_size: "auto"
      num_layers_per_virtual_pipeline_stage: [1]
      context_parallel_size: "auto"
      expert_model_parallel_size: [1]
      micro_batch_size: "auto"
      use_recompute: [true]
      recompute_method: "auto"
      recompute_granularity: "auto"
      recompute_num_layers: "auto"
    control:
      max_time_per_task: 300
      train_iters: 5
      max_time: 600

action: auto_tune

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra 
