#  Install FlagScale

Clone FlagScale code from github.

```sh
git clone https://github.com/FlagOpen/FlagScale.git
cd FlagScale/
```

If you don't have access to the international internet, import FlagScale project on [gitee](https://gitee.com/), then clone from gitee.

```sh
git clone https://gitee.com/flagopen/FlagScale.git
cd FlagScale/
```

Install train and inference env according to [README](https://github.com/FlagOpen/FlagScale/blob/main/README.md) 

# Download Model

Checkpoint is not publish yet.

Directory structure:
```sh
|-- action_model.pt
|-- backbone
|   |-- added_tokens.json
|   |-- chat_template.jinja
|   |-- config.json
|   |-- generation_config.json
|   |-- merges.txt
|   |-- model-00001-of-00002.safetensors
|   |-- model-00002-of-00002.safetensors
|   |-- model.safetensors.index.json
|   |-- preprocessor_config.json
|   |-- special_tokens_map.json
|   |-- tokenizer.json
|   |-- tokenizer_config.json
|   |-- video_preprocessor_config.json
|   `-- vocab.json
`-- config.yaml
```


# Serving

## Edit Config

```sh
cd FlagScale/
vim examples/robobrain_x0_5/conf/serve/libero_qwengroot.yaml
```

Change 2 fields:
- engine_args.model -> checkpoint path.
- engine_args.framework.qwenvl.base_vlm -> backbone checkpoint path.

## Run Serving

```sh
cd FlagScale/
python run.py --config-path ./examples/robobrain_x0_5/conf --config-name serve action=run
```

## Test Server with Client

Download test images:

```sh
cd FlagScale/
wget https://gitee.com/hchnr/flag-scale/blob/robotics_dataset/orbbec_0_latest.jpg
wget https://gitee.com/hchnr/flag-scale/blob/robotics_dataset/orbbec_1_latest.jpg
wget https://gitee.com/hchnr/flag-scale/blob/robotics_dataset/orbbec_2_latest.jpg
```

Run client:

```sh
python examples/robobrain_x0_5/client_libero.py  \
--host 127.0.0.1 \
--port 5001 \
--base-img orbbec_0_latest.jpg \
--left-wrist-img orbbec_1_latest.jpg \
--right-wrist-img orbbec_2_latest.jpg \
--num-steps 20
```


# Training

## Prepare Dataset

FlagScale uses WebDataset format and Megatraon.Energon data loader, you need process your data first.

For example, there is a dataset of 2 timesteps: [demo_0913_n2](https://gitee.com/hchnr/flag-scale/tree/robotics_dataset/demo_0913_n2/wds-2).

Download demo_0913_n2:

```sh
git archive --remote=git@gitee.com:hchnr/flag-scale.git robotics_dataset demo_0913_n2/ | tar -xv -C .
```

The directory structure of demo_0913_n2 is as follows:
- build_dep.sh: Copy .npy and .jpg files from production environment to ./deps
- demo_0913_n2.jsonl: A single timestep, including: task(str), images(.jpg), action(.npy), state(.npy)
- deps: .npy and .jpg files
- wds-2: Data in webdataset format (DP=2), generated by tools/datasets/vla/convert.py

Generate Data in webdataset format (DP=2) to ./demo_0913_n2/wds-2:

```sh
python tools/datasets/vla/convert.py \    
    --dataset-root=./demo_0913_n2 \
    --output-root=./demo_0913_n2 \
    --json=demo_0913_n2.jsonl \
    --train-split 1 \
    --val-split 0 \
    --images-key=image \
    --videos-key=video \
    --vision-root='' \
    --shuffle-tars \
    --num-workers=1 \
    --max-samples-per-tar 100000 \
    --dp-size 2
```

Move .jpg and .npy files from ./demo_0913_n2/deps to /:

```sh
mkdir -p /share/
cp -r ./demo_0913_n2/deps/* /
```

## Edit Config

```sh
cd FlagScale/
vim examples/robobrain_x0_5/conf/train/libero_qwengroot.yaml
```
Change 4 fields:
- checkpoint_dir: path to model checkpoint
- framework.qwenvl.base_vlm: path to backbone model (for example: qwenvl) checkpoint
- datasets.data_path: path to dataset, for example: ./demo_0913_n2/wds-2

## Start Training
```sh
cd FlagScale/
python run.py --config-path ./examples/robobrain_x0_5/conf --config-name train action=run
```
