- serve_id: vllm_model
  engine: vllm
  engine_args:
    model: /share/Qwen2.5-VL-32B-Instruct # should be customized
    served_model_name: qwenvl32-nv-flagos
    tensor_parallel_size: 8
    max_model_len: 32768
    pipeline_parallel_size: 1
    max_num_seqs: 8 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
    gpu_memory_utilization: 0.9
    limit_mm_per_prompt: image=18 # should be customized, 18 images/request is enough for most scenarios
    port: 9010
    trust_remote_code: true
    enforce_eager: true # better compare to FlagGems
    enable_chunked_prefill: true
