defaults:
  - train: 32b_stage2_D1
  - _self_

experiment:
  exp_name: train_qwen2_5_vl_32b_stage2_D1
  exp_dir: ./${experiment.exp_name}
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/train_qwen2_5_vl.py
  runner:
    backend: torchrun
    nnodes: 64
    nproc_per_node: 8
    rdzv_backend: static
    hostfile: /share/project/jiyuheng/32b_exp1/hostfile.64
    # ssh_port: 22
  cmds:  
    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train && export https_proxy=http://10.8.48.6:2080 && export http_proxy=http://10.8.48.6:2080
  envs:
    # NCCL_DEBUG: INFO
    # NCCL_DEBUG_SUBSYSTEM: ALL
    # RCH_DISABLE_SHARE_RDZV_TCP_STORE: 1
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NCCL_IB_DISABLE: 0
    GLOO_SOCKET_IFNAME: eth0
    NCCL_SOCKET_IFNAME: eth0
    NCCL_IB_HCA: mlx5_100,mlx5_101,mlx5_102,mlx5_103,mlx5_104,mlx5_105,mlx5_106,mlx5_107
    NCCL_IB_GID_INDEX: 7
    LIBRARY_PATH: /usr/local/cuda/lib64/stubs
    LD_LIBRARY_PATH: /usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/lib64:/usr/local/lib:/usr/local/mpi/lib:/usr/local/mpi/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
    PATH: /root/miniconda3/envs/flagscale-train/bin:/root/miniconda3/condabin:/client-tools:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
