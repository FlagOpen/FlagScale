defaults:
  - train: 3b_action_S6_subtask_agilex_eval5_demo
  - _self_

experiment:
  # exp_name: train_qwen2_5_vl_3b_group_lr_LLM_5e_6_ViT_5e_7_datav5_exp0_minlr_1_10
  exp_name: train_qwen2_5_vl_3b_action_S6_subtask_agilex_eval5_demo
  exp_dir: ./${experiment.exp_name}
  task:
    type: train
    backend: megatron
    # entrypoint: ./flagscale/train/pretrain_qwen2_5_vl_3b_unified.py
    entrypoint: ./flagscale/train/pretrain_qwen2_5_vl_3b_unified_sub.py
    
  runner: 
    backend: torchrun
    # nnodes: 32
    nnodes: 1
    nproc_per_node: 8
    rdzv_backend: static
    # hostfile: /share/project/jiyuheng/exp0/hostfile.32
    # ssh_port: 22
  cmds:  
    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train && export https_proxy=http://10.8.48.6:2080 && export http_proxy=http://10.8.48.6:2080
  envs:
    # NCCL_DEBUG: INFO
    # NCCL_DEBUG_SUBSYSTEM: ALL
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    NCCL_IB_DISABLE: 0
    GLOO_SOCKET_IFNAME: eth0
    NCCL_SOCKET_IFNAME: eth0
    NCCL_IB_HCA: mlx5_100,mlx5_101,mlx5_102,mlx5_103,mlx5_104,mlx5_105,mlx5_106,mlx5_107
    NCCL_IB_GID_INDEX: 7
    LIBRARY_PATH: /usr/local/cuda/lib64/stubs
    LD_LIBRARY_PATH: /usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/lib64:/usr/local/lib:/usr/local/mpi/lib:/usr/local/mpi/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
    PATH: /root/miniconda3/envs/flagscale-train/bin:/root/miniconda3/condabin:/client-tools:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
