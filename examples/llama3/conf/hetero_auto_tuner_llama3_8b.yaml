defaults:
  - train: 8b_hetero
  - _self_

experiment:
  exp_name: llama3_8b_hetero_auto_tuner
  exp_dir: /nfs/syq/test/pr1113/FlagScale/${experiment.exp_name}
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/train_gpt.py
  cmds:
    before_start: source /root/miniconda3/bin/activate flagscale-train
  runner:
    backend: torchrun
    ssh_port: 6008
    nnodes: 2
    nproc_per_node: 8
    rdzv_backend: static
    hostfile: ./hostfile_hetero
  envs:
    FLAGCX_IB_HCA: mlx5 #或者使用多网卡模糊匹配 mlx5
    FLAGCX_ENABLE_TOPO_DETECT: TRUE
    #FLAGCX_DEBUG: INFO
    #FLAGCX_DEBUG_SUBSYS: ALL
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1    
    device_type_specific:
      mlu590:
        MLU_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
        CNCL_IB_RELAXED_ORDERING_ENABLE: 1
        CNCL_MLULINK_OVER_ROCE_DISABLE: 1
        CNCL_MLU_DIRECT_LEVEL: 1
        CNCL_IB_GID_INDEX: 3
        LD_LIBRARY_PATH: "/workspace/ffmpeg-mlu-v4.4.0/install/lib:/usr/local/openmpi/lib:/usr/local/neuware/lib64"
        PATH: "/torch/venv3/pytorch/bin:/torch/venv3/pytorch/bin:/torch/venv3/pytorch_infer/bin:/workspace/ffmpeg-mlu-v4.4.0/install/bin:/usr/local/openmpi/bin:/usr/local/neuware/bin:/usr/local/openmpi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
      # A800:
        #  LIBRARY_PATH: "/usr/local/cuda/lib64/stubs"
          #  LD_LIBRARY_PATH: "/usr/local/cuda-12.4/lib64:/usr/local/lib:/usr/local/mpi/lib:/usr/local/mpi/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64"
      #  PATH: "/usr/local/cuda-12.4/bin:/root/miniconda3/condabin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
  
  auto_tuner:
    space:
      hetero_pipeline_layer_split: [[16,16], [18,14]]
      #hetero_pipeline_layer_split: [[12,12], [10,14]]
      hetero_process_meshes: ['auto', 1, 1, 'auto', 'auto', 'auto', 1, 1, 'auto', 'auto']
      #hetero_process_meshes: [1, 1, 1, 8,1,[1,2],1, 1, 'auto', 1]
      hetero_inter_mesh_max_layer_diff: 4
      hetero_intra_mesh_max_layer_diff: 2
      use_distributed_optimizer: [true]
      #sequence_parallel: [true]
      sequence_parallel: [true]  
      #num_layers_per_virtual_pipeline_stage: [1]
      #context_parallel_size: [1]
      #expert_model_parallel_size: [1]
      micro_batch_size: [1,2,4]
      use_recompute: [false]
      #recompute_method: "auto"
      #recompute_granularity: "auto"
      #recompute_num_layers: "auto"
    hetero_memory_model:
      gpu_memory: 
        A800: 60000  
        mlu590: 50000
        #gpu_utilization: [0.2, 0.8]
        
    control:
      max_time_per_task: 600
      train_iters: 3
      run_best: false

device_type_specific:
  mlu590:
    build_dir: /nfs/syq/test/FlagScale/build/Cambricon_MLU/FlagScale

action: auto_tune

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
