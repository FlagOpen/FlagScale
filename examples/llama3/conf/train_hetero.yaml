defaults:
  - train: 8b_hetero
  - _self_

experiment:
  exp_name: llama2_tp_hetero
  exp_dir: ./outputs_llama2_tp_hetero
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/hetero/train_gpt.py
  runner:
    backend: torchrun
    nnodes: 1
    nproc_per_node: 8
    hostfile: hostfile
  envs:
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
