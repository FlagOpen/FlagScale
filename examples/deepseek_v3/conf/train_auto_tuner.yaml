# DeepSeek 16B, 2.4A Model
defaults:
  - _self_
  - train: 16b_a3b

experiment:
  exp_name: DeepSeek-V3-auto-tune
  seed: 42
  save_steps: 10000
  load: null
  exp_dir: /xxx
  ckpt_format: torch
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py
  runner:
    per_node_task: false
    no_shared_fs: false
    rdzv_backend: static
    hostfile: null
  cmds:
    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
  envs:
    LOGLEVEL: "INFO"
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
  auto_tuner:
    space:
      data_parallel_size: [1, 2, 4]
      use_distributed_optimizer: [false, true]
      tensor_model_parallel_size: [1, 2, 4]
      expert_model_parallel_size: [1, 2, 4, 8]
      sequence_parallel: [true]
      pipeline_model_parallel_size: [1, 2, 4]
      num_layers_per_virtual_pipeline_stage: [0] # turn off vpp
      context_parallel_size: [1]
      micro_batch_size: [1]
      use_recompute: [false]
      # recompute_method: "auto"
      # recompute_granularity: "auto"
      # recompute_num_layers: "auto"
    control:
      max_time_per_task: 300
      train_iters: 5
      # max_time: 600
    memory_model:
      model_name: default
      gpu_memory: 80000
      gpu_utilization: [0.6, 0.9] # min-max

action: auto_tune

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
