defaults:
  - _self_
  - train: 135m

experiment:
  exp_name: SmolLM2-135m
  seed: 42
  save_steps: 512
  load: null
  exp_dir: checkpoints/${experiment.exp_name}
  ckpt_format: torch
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py
  runner:
    per_node_task: false
    no_shared_fs: false
    rdzv_backend: static
    # nnodes: 1
    nproc_per_node: 1
  cmds:
    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
  envs:
    NVTE_DEBUG: 1
    NVTE_DEBUG_LEVEL: 2
    # CUDNN_LOGERR_DBG: 1
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    # NCCL_IB_DISABLE: 0
    NCCL_IB_CUDA_SUPPORT: 1
    NCCL_IB_GID_INDEX: 0
    #TORCH_NCCL_USE_COMM_NONBLOCKING: 1
    OMP_NUM_THREADS: 4
    ENABLE_FLASH_ATTENTION_WITH_IXDNN: 0
    NCCL_NET_PLUGIN: none
    #NCCL_SHM_DISABLE=1
    NCCL_ALGO: Ring
    NCCL_P2P_NET_CHUNKSIZE: 1048576
    NCCL_CHUNK_SIZE: 1048576
    NCCL_BUFFSIZE: 8388608
    NCCL_MAX_NCHANNELS: 4
    NCCL_MIN_NCHANNELS: 4
    NCCL_MAX_P2P_NCHANNELS: 1
    NCCL_PROTO: Simple
    NCCL_NET_SHARED_BUFFERS: 0
    NCCL_P2P_LL_THRESHOLD: 0
    IXCCL_MIX_NV: 1
    IXCCL_FUSED_ENABLE: 0
    NCCL_IB_DISABLE: 0
    NCCL_IB_HCA: mlx5_2,mlx5_5

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
