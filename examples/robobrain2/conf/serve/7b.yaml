- serve_id: vllm_model
  engine: vllm
  engine_args:
    model: /share/project/lms/robobrain2_7b # should be customized
    served_model_name: robobrain2_7b-nv-flagos
    tensor_parallel_size: 1
    max_model_len: 32768
    pipeline_parallel_size: 1
    max_num_seqs: 8 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
    gpu_memory_utilization: 0.9
    limit_mm_per_prompt: image=18 # should be customized, 18 images/request is enough for most scenarios
    port: 9010
    trust_remote_code: true
    enforce_eager: false # set true if use FlagGems 
    enable_chunked_prefill: true
