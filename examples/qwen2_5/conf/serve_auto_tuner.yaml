defaults:
  - _self_
  - serve: 7b_tune

experiment:
  exp_name: qwen2.5_7b
  exp_dir: ./outputs
  task:
    type: serve
  deploy:
    port: 6701
    use_fs_serve: false
    prefill_decode_disaggregation: true
    prefill_num: 2
    decode_num: 2
    prefill_address: 10.6.208.15
    decode_address: 10.6.208.16
  runner:
    hostfile: examples/qwen2_5/conf/hostfile.txt
    docker: cz-dev
    ssh_port: 22
    nnodes: 1
    nproc_per_node: 4
  envs:
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    VLLM_USE_V1: 0
    FLAGCX_SOCKET_IFNAME: bond0.2208
    FLAGCX_PATH: /mine/ip16/project/FlagCX/
    FLAGCX_DEBUG: TRACE
    FLAGCX_DEBUG_SUBSYS: ALL
    USE_FLAGCX: true
  auto_tuner:
    engines: [vllm]
    space:
      vllm:
        tensor_model_parallel_size: [1, 2]
        pipeline_model_parallel_size: auto
        block_size: [16]
    performance:
      metric: total_token_throughput
      order: descend
    control:
      interval: 20
      run_best: False
  cmds:
    before_start: source /root/miniconda3/bin/activate flagscale-inference

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
