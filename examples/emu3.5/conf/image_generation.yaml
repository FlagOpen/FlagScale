defaults:
  - _self_
  - inference: x2i # t2i # x2i

experiment:
  exp_name: emu3p5_one_image_generation_x2i
  exp_dir: outputs/${experiment.exp_name}
  vq_model: /share/project/ly/workspace/code/release/Emu3.5/weights/Emu3.5-VisionTokenizer
  model: /share/project/ly/workspace/code/release/Emu3.5/weights/Emu3.5-image/
  tokenizer: src/tokenizer_emu3_ibq/
  hf_config_path: src/emu3p5/
  task:
    type: inference
    backend: vllm
    entrypoint: flagscale/inference/inference_emu3p5.py
  runner:
    hostfile: null
  cmds:
    before_start: source /root/miniconda3/bin/activate /share/project/zhaoyingli/envs/release-fs-vllm
  envs:
    CUDA_VISIBLE_DEVICES: 0,1
    VLLM_LOGGING_LEVEL: "DEBUG"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    VLLM_WORKER_MULTIPROC_METHOD: "spawn"
    NCCL_NVLS_ENABLE: 0
    NCCL_MAX_NCHANNELS: 16
    NCCL_MIN_NCHANNELS: 16

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
