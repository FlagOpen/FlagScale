llm:
  vq_model: ${experiment.vq_model}
  model: ${experiment.model}
  tokenizer: ${experiment.tokenizer}
  trust_remote_code: true
  dtype: auto
  tensor_parallel_size: 2
  gpu_memory_utilization: 0.7
  disable_log_stats: false
  enable_chunked_prefill: false
  enable_prefix_caching: false
  seed: 42

# prompt case is here: flagscale/inference/emu_utils/prompt_case.py
generate:
  task_type: x2i
  ratio: "auto"
  image_area: 1048576
  sampling:
    max_tokens: 5120
    detokenize: false
    top_k: 131072
    top_p: 1.0
    temperature: 1.0
    text_top_k: 1024
    text_top_p: 0.9
    text_temperature: 1.0
    image_top_k: 5120
    image_top_p: 1.0
    image_temperature: 1.0
    guidance_scale: 3.0
