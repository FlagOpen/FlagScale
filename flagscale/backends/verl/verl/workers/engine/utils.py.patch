diff --git a/verl/workers/engine/utils.py b/verl/workers/engine/utils.py
index cbb990c3..242a94a7 100644
--- a/verl/workers/engine/utils.py
+++ b/verl/workers/engine/utils.py
@@ -14,12 +14,19 @@
 
 
 import torch
+import torch.distributed as dist
 from tensordict import TensorDict
 
 from verl.utils import tensordict_utils as tu
 from verl.utils.dataset.dataset_utils import DatasetPadMode
 from verl.utils.py_functional import append_to_dict
-from verl.utils.seqlen_balancing import rearrange_micro_batches, restore_dynamic_batch
+from verl.utils.seqlen_balancing import (
+    rearrange_micro_batches,
+    restore_dynamic_batch,
+    get_packer,
+    roundup_divisible,
+)
+from megatron.core import parallel_state as mpu
 
 
 def prepare_micro_batches(
@@ -29,6 +36,7 @@ def prepare_micro_batches(
     same_micro_num_in_dp=True,
     min_num_micro_batch=None,
     use_dynamic_bsz_balance=True,
+    use_sequence_packing_balance=False,
 ):
     """
     Prepare micro batches from data.
@@ -49,6 +57,104 @@ def prepare_micro_batches(
             min_num_micro_batch=min_num_micro_batch,
             use_dynamic_bsz_balance=use_dynamic_bsz_balance,
         )
+    elif use_sequence_packing_balance:
+        # NeMo-aligned sequence packing: compute packable groups only; no truncation/concat here
+        algorithm = tu.get_non_tensor_data(data=data, key="algorithm", default="modified_first_fit_decreasing")
+        input_key = tu.get_non_tensor_data(data=data, key="input_key", default="input_ids")
+        input_lengths_key = tu.get_non_tensor_data(data=data, key="input_lengths_key", default="input_lengths")
+        sp_size = tu.get_non_tensor_data(data=data, key="sp_size", default=1)
+
+        assert (
+            "max_token_len_per_gpu" in data.keys()
+        ), "max_token_len_per_gpu must be set when use_sequence_packing is True"
+        max_token_len_per_gpu = int(data["max_token_len_per_gpu"])  
+        bin_capacity = max_token_len_per_gpu * int(sp_size)
+
+        # Build sequence lengths
+        if input_lengths_key in data.keys():
+            seqlens_tensor = data[input_lengths_key]
+            if isinstance(seqlens_tensor, torch.Tensor):
+                seqlens = seqlens_tensor.tolist()
+            else:
+                seqlens = list(map(int, seqlens_tensor))
+        else:
+            attn = data["attention_mask"]  # [B, S]
+            seqlens = attn.sum(dim=1).tolist()
+
+        # Compute pad multiple ~ 2 * TP * SP
+        tp_size = mpu.get_tensor_model_parallel_world_size() if mpu.is_initialized() else 1
+        pad_multiple = tu.get_non_tensor_data(
+            data=data,
+            key="sequence_length_pad_multiple",
+            default=2 * int(tp_size) * int(sp_size),
+        )
+
+        def _pad_to_multiple(x: int, m: int) -> int:
+            return ((int(x) + int(m) - 1) // int(m)) * int(m)
+
+        padded_seqlens = [_pad_to_multiple(l, pad_multiple) for l in seqlens]
+
+        # Initial packing
+        packer = get_packer(
+            algorithm=algorithm,
+            bin_capacity=bin_capacity,
+            min_bin_count=None,
+            bin_count_multiple=None,
+            equal_size=False,
+        )
+        bins = packer.pack(padded_seqlens)  # List[List[int]]
+
+        # Align with VPP if requested (num_batches_divided_by)
+        if num_batches_divided_by is not None and num_batches_divided_by > 1:
+            target_bins = roundup_divisible(len(bins), int(num_batches_divided_by))
+            if target_bins != len(bins):
+                packer = get_packer(
+                    algorithm=algorithm,
+                    bin_capacity=int(bin_capacity),
+                    min_bin_count=target_bins,
+                    bin_count_multiple=None,
+                    equal_size=False,
+                )
+                bins = packer.pack(padded_seqlens)
+
+        # Align micro-batch count across DP ranks if needed
+        if dist.is_initialized() and dp_group is not None and same_micro_num_in_dp:
+            device = data[input_key].device if isinstance(data.get(input_key, None), torch.Tensor) else "cpu"
+            local_bins = torch.tensor([len(bins)], device=device)
+            dist.all_reduce(local_bins, op=dist.ReduceOp.MAX, group=dp_group)
+            global_target = int(local_bins.item())
+            if global_target != len(bins):
+                packer = get_packer(
+                    algorithm=algorithm,
+                    bin_capacity=int(bin_capacity),
+                    min_bin_count=global_target,
+                    bin_count_multiple=None,
+                    equal_size=False,
+                )
+                bins = packer.pack(padded_seqlens)
+
+        # Materialize micro-batches (grouping only)
+        micro_batches = []
+        bin_packed_lengths = [sum(padded_seqlens[i] for i in part) for part in bins]
+        max_packed_len = max(bin_packed_lengths) if bin_packed_lengths else 0
+        # optional: if VPP provided, ensure max_packed_len is visible to all mbs for PP
+        for partition in bins:
+            curr_mb = tu.index_select_tensor_dict(data, partition)
+            tu.assign_non_tensor(curr_mb, pack_sequences=True)
+            # seq_length_key: key name holding per-sample true lengths (e.g., "input_lengths")
+            tu.assign_non_tensor(curr_mb, seq_length_key=input_lengths_key)
+            tu.assign_non_tensor(curr_mb, pad_individual_seqs_to_multiple_of=int(pad_multiple))
+            curr_packed_len = sum(padded_seqlens[i] for i in partition) 
+            # packed_seq_len: total packed tokens of this micro-batch (sum of per-seq aligned lengths)
+            tu.assign_non_tensor(curr_mb, packed_seq_len=int(curr_packed_len))
+            # for PP>1 cases, downstream may choose to pad all packed seqs to the same max within a global-batch
+            # pad_full_seq_to: global max packed length ensuring fixed [batch x seqlen] buffers for pipeline parallel
+            tu.assign_non_tensor(curr_mb, pad_full_seq_to=int(max_packed_len))
+            micro_batches.append(curr_mb)
+        batch_idx_list = bins
+        # Ensure downstream postprocess does not treat this as dynamic-bsz
+        tu.assign_non_tensor(data, use_dynamic_bsz=False)
+        tu.assign_non_tensor(data, use_sequence_packing=True)
     else:
         micro_batch_size_per_gpu = data["micro_batch_size_per_gpu"]
         micro_batches = data.split(micro_batch_size_per_gpu)
