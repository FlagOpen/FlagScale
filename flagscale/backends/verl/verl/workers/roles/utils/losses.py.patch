diff --git a/verl/workers/roles/utils/losses.py b/verl/workers/roles/utils/losses.py
index 3693e5eb..eed6348a 100644
--- a/verl/workers/roles/utils/losses.py
+++ b/verl/workers/roles/utils/losses.py
@@ -53,6 +53,51 @@ def ppo_loss(config: ActorConfig, model_output, data: TensorDict, dp_group=None)
     log_prob = model_output["log_probs"]
     entropy = model_output.get("entropy", None)
 
+    use_sequence_packing = tu.get_non_tensor_data(data, key="use_sequence_packing", default=False)
+    packed_seq_params = tu.get_non_tensor_data(data, key="packed_seq_params", default=None)
+
+
+    if use_sequence_packing and packed_seq_params is not None:
+        return _ppo_loss_with_sequence_packing(
+            config, log_prob, entropy, data, dp_group, packed_seq_params
+        )
+    else:
+        return _ppo_loss_standard(
+            config, log_prob, entropy, data, dp_group
+        )
+
+
+def value_loss(config: CriticConfig, model_output, data: TensorDict, dp_group=None):
+    vpreds = model_output["values"]
+    vpreds = no_padding_2_padding(vpreds, data)  # (bsz, response_length)
+
+    values = data["values"]
+    returns = data["returns"]
+    response_mask = data["response_mask"].to(bool)
+
+    vf_loss, vf_clipfrac = compute_value_loss(
+        vpreds=vpreds,
+        values=values,
+        returns=returns,
+        response_mask=response_mask,
+        cliprange_value=config.cliprange_value,
+        loss_agg_mode=config.loss_agg_mode,
+    )
+
+    metrics = {}
+
+    metrics.update(
+        {
+            "critic/vf_loss": vf_loss.detach().item(),
+            "critic/vf_clipfrac": vf_clipfrac.detach().item(),
+            "critic/vpred_mean": masked_mean(vpreds, response_mask).detach().item(),
+        }
+    )
+
+    return vf_loss, metrics
+
+
+def _ppo_loss_standard(config: ActorConfig, model_output, data: TensorDict, dp_group=None):
     log_prob = no_padding_2_padding(log_prob, data)  # (bsz, response_length)
     if entropy is not None:
         entropy = no_padding_2_padding(entropy, data)  # (bsz, response_length)
@@ -108,31 +153,78 @@ def ppo_loss(config: ActorConfig, model_output, data: TensorDict, dp_group=None)
     return policy_loss, metrics
 
 
-def value_loss(config: CriticConfig, model_output, data: TensorDict, dp_group=None):
-    vpreds = model_output["values"]
-    vpreds = no_padding_2_padding(vpreds, data)  # (bsz, response_length)
+def _ppo_loss_with_sequence_packing(config: ActorConfig, model_output, data: TensorDict, dp_group=None, packed_seq_params: PackedSeqParams = None):
+    from megatron.core import parallel_state as mpu
 
-    values = data["values"]
-    returns = data["returns"]
-    response_mask = data["response_mask"].to(bool)
+    cu_seqlens = packed_seq_params.cu_seqlens_q
+    cu_seqlens_padded = packed_seq_params.cu_seqlens_q_padded
 
-    vf_loss, vf_clipfrac = compute_value_loss(
-        vpreds=vpreds,
-        values=values,
-        returns=returns,
-        response_mask=response_mask,
-        cliprange_value=config.cliprange_value,
-        loss_agg_mode=config.loss_agg_mode,
-    )
+    if cu_seqlens_q_padded is not None:
+        padded_cu_seqlens = self.cu_seqlens_q_padded
+        padded_seq_lengths = (
+            self.cu_seqlens_q_padded[1:] - self.cu_seqlens_q_padded[:-1]
+        )
+    else:
+        padded_cu_seqlens = unpadded_cu_seqlens
+        padded_seq_lengths = unpadded_seq_lengths
 
-    metrics = {}
+    seq_starts = padded_cu_seqlens[:-1]
+    seq_ends = padded_cu_seqlens[1:]
 
-    metrics.update(
-        {
-            "critic/vf_loss": vf_loss.detach().item(),
-            "critic/vf_clipfrac": vf_clipfrac.detach().item(),
-            "critic/vpred_mean": masked_mean(vpreds, response_mask).detach().item(),
-        }
-    )
+    loss_accum = 0
 
-    return vf_loss, metrics
+    log_prob = model_output["log_probs"]
+    log_prob_values = log_prob.values()     # [total_tokens]
+    log_prob_offsets = log_prob.offsets()   # cu_seqlens
+
+    for seq_idx in range(len(seq_starts)):
+        seq_start = seq_starts[seq_idx].item()
+        seq_end = seq_ends[seq_idx].item()
+        unpadded_seq_len = unpadded_seq_lengths[seq_idx].item()
+
+        # get log_prob of the current sequence
+        seq_log_prob_start = log_prob_offsets[seq_idx].item()
+        seq_log_prob_end = log_prob_offsets[seq_idx + 1].item()
+        seq_log_prob = log_prob_values[seq_log_prob_start:seq_log_prob_end]
+        seq_log_prob = seq_log_prob.unsqueeze(0)    # [1, seq_len] 
+
+        # get other data
+        seq_data = {}
+        for key in ["old_log_probs", "advantages", "response_mask", "ref_log_prob"]:
+            if key in data:
+                tensor = data[key]
+                if isinstance(tensor, torch.Tensor):
+                    if tensor.ndim > 1 and tensor.shape[1] > 1:
+                        # 只保留有效长度
+                        seq_data[key] = tensor[seq_idx:seq_idx+1, :unpadded_seq_len]
+                    else:
+                        seq_data[key] = tensor[seq_idx:seq_idx+1]
+                else:
+                    seq_data[key] = tensor
+
+        # create model_output for the current sequence
+        seq_model_output = {"log_probs": seq_log_prob}
+        if entropy is not None:
+            entropy_values = entropy.values()
+            entropy_offsets = entropy.offsets()
+            seq_entropy_start = entropy_offsets[seq_idx].item()
+            seq_entropy_end = entropy_offsets[seq_idx + 1].item()
+            seq_entropy = entropy_values[seq_entropy_start:seq_entropy_end].unsqueeze(0)
+            seq_model_output["entropy"] = seq_entropy
+
+        # add data
+        seq_data_dict = TensorDict(seq_data, batch_size=[1])
+        for key in data.keys():
+            if key not in seq_data and not isinstance(data[key], torch.Tensor):
+                tu.assign_non_tensor(seq_data_dict, key, tu.get_non_tensor_data(data, key))
+
+        seq_loss, seq_metrics = _ppo_loss_standard(
+            config, seq_model_output, seq_data_dict, dp_group
+        )
+
+        # accumulate loss and metrics
+        loss_accum += seq_loss
+        for k, v in seq_metrics.items():
+            if k not in metrics_accum:
+                metrics_accum[k] = 0.0
+            metrics_accum[k] += v
\ No newline at end of file
