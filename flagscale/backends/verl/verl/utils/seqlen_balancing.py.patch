diff --git a/verl/utils/seqlen_balancing.py b/verl/utils/seqlen_balancing.py
index bc5588f7..1040ec11 100644
--- a/verl/utils/seqlen_balancing.py
+++ b/verl/utils/seqlen_balancing.py
@@ -15,7 +15,9 @@
 import copy
 import heapq
 from itertools import chain
-
+import enum
+from abc import ABC, abstractmethod
+from typing import Dict, List, Optional, Tuple, Type, Union
 import torch
 from torch import distributed as dist
 
@@ -34,162 +36,605 @@ def calculate_workload(seqlen_list: list[int]):
     return 24576 * seqlen_list + seqlen_list**2
 
 
-def karmarkar_karp(seqlen_list: list[int], k_partitions: int, equal_size: bool):
-    # see: https://en.wikipedia.org/wiki/Largest_differencing_method
-    class Set:
-        def __init__(self) -> None:
-            self.sum = 0
-            self.items = []
+class PackingAlgorithm(enum.Enum):
+    """Enum for supported sequence packing algorithms."""
+
+    KARMARKAR_KARP = "karmarkar_karp"
+    GREEDY_PARTITION = "greedy_partition"
+    CONCATENATIVE = "concatenative"
+    FIRST_FIT_DECREASING = "first_fit_decreasing"
+    FIRST_FIT_SHUFFLE = "first_fit_shuffle"
+    MODIFIED_FIRST_FIT_DECREASING = "modified_first_fit_decreasing"
+
+
+class SequencePacker(ABC):
+    def __init__(
+        self,
+        bin_capacity: int,
+        equal_size: bool = True,
+        min_bin_count: Optional[int] = 0,
+        bin_count_multiple: Optional[int] = None,
+    ):
+        """Initialize the sequence packer.
+
+        Args:
+            bin_capacity: The maximum capacity of each bin.
+            min_bin_count: Minimum number of bins to create, even if fewer would suffice.
+                          If None, no minimum is enforced.
+            bin_count_multiple: The total number of bins must be a multiple of this value.
+                               If None, no multiple constraint is enforced.
+            equal_size (bool): If True, ensures that each partition has the same number of items.
+                           Requires len(seqlen_list) to be divisible by k_partitions.
+                           If False, partitions can have varying numbers of items, focusing
+                           only on balancing the sum of sequence lengths.
 
-        def add(self, idx: int, val: int):
-            self.items.append((idx, val))
-            self.sum += val
+        Raises:
+            ValueError: If min_bin_count or bin_count_multiple are invalid.
+        """
+        self.bin_capacity = bin_capacity
+        self.equal_size = equal_size
+        self.min_bin_count = min_bin_count
+        self.bin_count_multiple = bin_count_multiple
+        self.metrics = None
+
+        if min_bin_count is not None and min_bin_count < 0:
+            raise ValueError("min_bin_count must be nonnegative")
+        if bin_count_multiple is not None and bin_count_multiple < 1:
+            raise ValueError("bin_count_multiple must be positive")
+
+    @abstractmethod
+    def _pack_implementation(self, sequence_lengths: list[int]) -> list[list[int]]:
+        pass
+
+    def _adjust_bin_count(self, bins: list[list[int]]) -> list[list[int]]:
+        """Adjust the number of bins to meet minimum and multiple constraints."""
+        current_bin_count = len(bins)
+        target_bin_count = current_bin_count
+
+        if self.min_bin_count is not None:
+            target_bin_count = max(target_bin_count, self.min_bin_count)
+
+        if self.bin_count_multiple is not None:
+            remainder = target_bin_count % self.bin_count_multiple
+            if remainder != 0:
+                target_bin_count += self.bin_count_multiple - remainder
+
+        if target_bin_count == current_bin_count:
+            return bins
+
+        # Count total sequences
+        total_sequences = sum(len(bin_contents) for bin_contents in bins)
+        if total_sequences < target_bin_count:
+            raise ValueError(
+                f"Cannot create {target_bin_count} bins with only {total_sequences} sequences. "
+                f"Each bin must contain at least one sequence. "
+                f"Either reduce min_bin_count/bin_count_multiple or provide more sequences."
+            )
 
-        def merge(self, other):
-            for idx, val in other.items:
+        adjusted_bins = [bin_contents.copy() for bin_contents in bins]
+        additional_bins_needed = target_bin_count - current_bin_count
+        for _ in range(additional_bins_needed):
+            adjusted_bins.append([])
+
+        # Move sequences from existing bins to new bins
+        bin_sizes = [
+            (len(bin_contents), i)
+            for i, bin_contents in enumerate(adjusted_bins[:current_bin_count])
+        ]
+        bin_sizes.sort(reverse=True)  # Sort by size, largest first
+
+        source_bin_idx = 0
+
+        for new_bin_idx in range(current_bin_count, target_bin_count):
+            # Find a bin with at least 2 sequences (so we can move one and leave at least one)
+            while source_bin_idx < len(bin_sizes):
+                bin_size, original_bin_idx = bin_sizes[source_bin_idx]
+                current_size = len(adjusted_bins[original_bin_idx])
+
+                if current_size > 1:
+                    # Move one sequence from this bin to the new bin
+                    sequence_to_move = adjusted_bins[original_bin_idx].pop()
+                    adjusted_bins[new_bin_idx].append(sequence_to_move)
+                    break
+                else:
+                    # This bin only has one sequence, try the next one
+                    source_bin_idx += 1
+            else:
+                # If we get here, we couldn't find any bin with more than 1 sequence
+                # This should not happen given our earlier validation, but let's handle it
+                raise ValueError(
+                    f"Cannot create additional bins: insufficient sequences to redistribute. "
+                    f"Need {additional_bins_needed} additional bins but cannot find enough "
+                    f"bins with multiple sequences to redistribute from."
+                    f"WARNING: Triggering this section of code is a bug. Please report it."
+                )
+
+        return adjusted_bins
+
+    def pack(self, seqlen_list: list[int]) -> list[list[int]]:
+        """Pack sequences into bins.
+
+        Args:
+            seqlen_list: A list of sequence lengths to pack.
+
+        Returns:
+            A list of bins, where each bin is a list of indices into the original
+            seqlen_list list. The number of bins will satisfy min_bin_count
+            and bin_count_multiple constraints if specified.
+        """
+        # Call the implementation
+        bins = self._pack_implementation(seqlen_list)
+
+        # Adjust bin count to meet constraints
+        bins = self._adjust_bin_count(bins)
+
+        return bins
+
+    def _validate_sequence_lengths(self, seqlen_list: list[int]) -> None:
+        """Validate that all sequence lengths are within bin capacity."""
+        for length in seqlen_list:
+            if length > self.bin_capacity:
+                raise ValueError(
+                    f"Sequence length {length} exceeds bin capacity {self.bin_capacity}"
+                )
+
+class KarmarkarKarpPacker(SequencePacker):
+    def _pack_implementation(self, seqlen_list: list[int]) -> list[list[int]]:
+        # see: https://en.wikipedia.org/wiki/Largest_differencing_method
+        k_partitions = self.min_bin_count
+        class Set:
+            def __init__(self) -> None:
+                self.sum = 0
+                self.items = []
+
+            def add(self, idx: int, val: int):
                 self.items.append((idx, val))
                 self.sum += val
 
-        def __lt__(self, other):
-            if self.sum != other.sum:
-                return self.sum < other.sum
-            if len(self.items) != len(other.items):
-                return len(self.items) < len(other.items)
-            return self.items < other.items
-
-    class State:
-        def __init__(self, items: list[tuple[int, int]], k: int) -> None:
-            self.k = k
-            # sets should always be decreasing order
-            self.sets = [Set() for _ in range(k)]
-            assert len(items) in [1, k], f"{len(items)} not in [1, {k}]"
-            for i, (idx, seqlen) in enumerate(items):
-                self.sets[i].add(idx=idx, val=seqlen)
-            self.sets = sorted(self.sets, reverse=True)
-
-        def get_partitions(self):
-            partitions = []
-            for i in range(len(self.sets)):
-                cur_partition = []
-                for idx, _ in self.sets[i].items:
-                    cur_partition.append(idx)
-                partitions.append(cur_partition)
-            return partitions
-
-        def merge(self, other):
-            for i in range(self.k):
-                self.sets[i].merge(other.sets[self.k - 1 - i])
-            self.sets = sorted(self.sets, reverse=True)
-
-        @property
-        def spread(self) -> int:
-            return self.sets[0].sum - self.sets[-1].sum
-
-        def __lt__(self, other):
-            # least heap, let the state with largest spread to be popped first,
-            # if the spread is the same, let the state who has the largest set
-            # to be popped first.
-            if self.spread != other.spread:
-                return self.spread > other.spread
-            return self.sets[0] > other.sets[0]
-
-        def __repr__(self) -> str:
-            repr_str = "["
-            for i in range(self.k):
-                if i > 0:
-                    repr_str += ","
-                repr_str += "{"
-                for j, (_, seqlen) in enumerate(self.sets[i].items):
-                    if j > 0:
+            def merge(self, other):
+                for idx, val in other.items:
+                    self.items.append((idx, val))
+                    self.sum += val
+
+            def __lt__(self, other):
+                if self.sum != other.sum:
+                    return self.sum < other.sum
+                if len(self.items) != len(other.items):
+                    return len(self.items) < len(other.items)
+                return self.items < other.items
+
+        class State:
+            def __init__(self, items: list[tuple[int, int]], k: int) -> None:
+                self.k = k
+                # sets should always be decreasing order
+                self.sets = [Set() for _ in range(k)]
+                assert len(items) in [1, k], f"{len(items)} not in [1, {k}]"
+                for i, (idx, seqlen) in enumerate(items):
+                    self.sets[i].add(idx=idx, val=seqlen)
+                self.sets = sorted(self.sets, reverse=True)
+
+            def get_partitions(self):
+                partitions = []
+                for i in range(len(self.sets)):
+                    cur_partition = []
+                    for idx, _ in self.sets[i].items:
+                        cur_partition.append(idx)
+                    partitions.append(cur_partition)
+                return partitions
+
+            def merge(self, other):
+                for i in range(self.k):
+                    self.sets[i].merge(other.sets[self.k - 1 - i])
+                self.sets = sorted(self.sets, reverse=True)
+
+            @property
+            def spread(self) -> int:
+                return self.sets[0].sum - self.sets[-1].sum
+
+            def __lt__(self, other):
+                # least heap, let the state with largest spread to be popped first,
+                # if the spread is the same, let the state who has the largest set
+                # to be popped first.
+                if self.spread != other.spread:
+                    return self.spread > other.spread
+                return self.sets[0] > other.sets[0]
+
+            def __repr__(self) -> str:
+                repr_str = "["
+                for i in range(self.k):
+                    if i > 0:
                         repr_str += ","
-                    repr_str += str(seqlen)
-                repr_str += "}"
-            repr_str += "]"
-            return repr_str
-
-    sorted_seqlen_list = sorted([(seqlen, i) for i, seqlen in enumerate(seqlen_list)])
-    states_pq = []
-    if equal_size:
-        assert len(seqlen_list) % k_partitions == 0, f"{len(seqlen_list)} % {k_partitions} != 0"
-        for offset in range(0, len(sorted_seqlen_list), k_partitions):
-            items = []
-            for i in range(k_partitions):
-                seqlen, idx = sorted_seqlen_list[offset + i]
-                items.append((idx, seqlen))
-            heapq.heappush(states_pq, State(items=items, k=k_partitions))
-    else:
-        for seqlen, idx in sorted_seqlen_list:
-            heapq.heappush(states_pq, State(items=[(idx, seqlen)], k=k_partitions))
-
-    while len(states_pq) > 1:
-        state0 = heapq.heappop(states_pq)
-        state1 = heapq.heappop(states_pq)
-        # merge states
-        state0.merge(state1)
-        heapq.heappush(states_pq, state0)
-
-    final_state = states_pq[0]
-    partitions = final_state.get_partitions()
-    if equal_size:
-        for i, partition in enumerate(partitions):
-            assert len(partition) * k_partitions == len(seqlen_list), (
-                f"{len(partition)} * {k_partitions} != {len(seqlen_list)}"
-            )
-    return partitions
-
-
-def greedy_partition(seqlen_list: list[int], k_partitions: int, equal_size: bool):
-    bias = sum(seqlen_list) + 1 if equal_size else 0
-    sorted_seqlen = [(seqlen + bias, i) for i, seqlen in enumerate(seqlen_list)]
-    partitions = [[] for _ in range(k_partitions)]
-    partition_sums = [0 for _ in range(k_partitions)]
-    for seqlen, i in sorted_seqlen:
-        min_idx = None
-        for j in range(k_partitions):
-            if min_idx is None or partition_sums[j] < partition_sums[min_idx]:
-                min_idx = j
-        partitions[min_idx].append(i)
-        partition_sums[min_idx] += seqlen
-    if equal_size:
-        for i, partition in enumerate(partitions):
-            assert len(partition) * k_partitions == len(seqlen_list), (
-                f"{len(partition)} * {k_partitions} != {len(seqlen_list)}"
+                    repr_str += "{"
+                    for j, (_, seqlen) in enumerate(self.sets[i].items):
+                        if j > 0:
+                            repr_str += ","
+                        repr_str += str(seqlen)
+                    repr_str += "}"
+                repr_str += "]"
+                return repr_str
+
+        sorted_seqlen_list = sorted([(seqlen, i) for i, seqlen in enumerate(seqlen_list)])
+        states_pq = []
+        if self.equal_size:
+            assert len(seqlen_list) % k_partitions == 0, f"{len(seqlen_list)} % {k_partitions} != 0"
+            for offset in range(0, len(sorted_seqlen_list), k_partitions):
+                items = []
+                for i in range(k_partitions):
+                    seqlen, idx = sorted_seqlen_list[offset + i]
+                    items.append((idx, seqlen))
+                heapq.heappush(states_pq, State(items=items, k=k_partitions))
+        else:
+            for seqlen, idx in sorted_seqlen_list:
+                heapq.heappush(states_pq, State(items=[(idx, seqlen)], k=k_partitions))
+
+        while len(states_pq) > 1:
+            state0 = heapq.heappop(states_pq)
+            state1 = heapq.heappop(states_pq)
+            # merge states
+            state0.merge(state1)
+            heapq.heappush(states_pq, state0)
+
+        final_state = states_pq[0]
+        partitions = final_state.get_partitions()
+        if self.equal_size:
+            for i, partition in enumerate(partitions):
+                assert len(partition) * k_partitions == len(seqlen_list), (
+                    f"{len(partition)} * {k_partitions} != {len(seqlen_list)}"
+                )
+        return partitions
+
+
+class GreedyPartitionPacker(SequencePacker):
+    def _pack_implementation(self, seqlen_list: list[int]) -> list[list[int]]:
+        k_partitions = self.min_bin_count
+
+        bias = sum(seqlen_list) + 1 if self.equal_size else 0
+        sorted_seqlen = [(seqlen + bias, i) for i, seqlen in enumerate(seqlen_list)]
+        partitions = [[] for _ in range(k_partitions)]
+        partition_sums = [0 for _ in range(k_partitions)]
+        for seqlen, i in sorted_seqlen:
+            min_idx = None
+            for j in range(k_partitions):
+                if min_idx is None or partition_sums[j] < partition_sums[min_idx]:
+                    min_idx = j
+            partitions[min_idx].append(i)
+            partition_sums[min_idx] += seqlen
+        if self.equal_size:
+            for i, partition in enumerate(partitions):
+                assert len(partition) * k_partitions == len(seqlen_list), (
+                    f"{len(partition)} * {k_partitions} != {len(seqlen_list)}"
+                )
+        return partitions
+
+
+class ConcatenativePacker(SequencePacker):
+    """Concatenative packing algorithm.
+
+    This algorithm simply concatenates sequences in order until reaching the bin capacity,
+    then starts a new bin. It doesn't try to optimize the packing in any way.
+
+    Time complexity: O(n) where n is the number of sequences.
+
+    Example:
+    ```python
+    >>> examples = {
+    ...     "sequence_lengths": [4, 1, 3, 2, 1, 3, 4, 5]
+    ... }
+    >>> # If packed with seq_length=5:
+    ... {"bins": [ [0, 1], [2, 3], [4, 5], [6], [7] ]}
+    >>> # If packed with seq_length=8:
+    ... {"bins": [ [0, 1, 2], [3, 4, 5], [6], [7] ]}
+    """
+
+    # Global class variable to limit the number of sequences packed in a unit
+    # -1 disables this limit
+    max_sequences_per_bin = -1  # Useful for debugging and testing
+
+    def _pack_implementation(self, seqlen_list: list[int]) -> list[list[int]]:
+        """Pack sequences using the concatenative algorithm."""
+        # Validate sequence lengths
+        self._validate_sequence_lengths(seqlen_list)
+
+        bins = []  # List of bins, each bin is a list of sequence indices
+        current_bin = []  # Current bin being filled
+        current_length = 0  # Current length of sequences in the bin
+
+        for i, length in enumerate(seqlen_list):
+            # Check if adding this sequence would exceed bin capacity or sequence limit
+            exceeds_capacity = current_length + length > self.bin_capacity
+            exceeds_sequence_limit = (
+                self.max_sequences_per_bin != -1
+                and len(current_bin) >= self.max_sequences_per_bin
             )
-    return partitions
 
+            # If adding this sequence would exceed constraints, start a new bin
+            if exceeds_capacity or exceeds_sequence_limit:
+                if current_bin:  # Only add the bin if it's not empty
+                    bins.append(current_bin)
+                current_bin = [i]
+                current_length = length
+            else:
+                # Add the sequence to the current bin
+                current_bin.append(i)
+                current_length += length
+
+        # Add the last bin if it's not empty
+        if current_bin:
+            bins.append(current_bin)
+
+        return bins
+
+
+class FirstFitPacker(SequencePacker):
+    """Base class for First-Fit algorithms.
+
+    First-Fit algorithms place each sequence into the first bin where it fits.
+    If no bin can fit the sequence, a new bin is created.
+
+    This is an abstract base class that provides the common implementation for
+    First-Fit variants. Subclasses must implement the _prepare_sequences method
+    to determine the order in which sequences are processed.
+    """
+
+    def _prepare_sequences(self, seqlen_list: list[int]) -> list[tuple[int, int]]:
+        raise NotImplementedError("Subclasses must implement _prepare_sequences")
+
+    def _pack_implementation(self, seqlen_list: list[int]) -> list[list[int]]:
+        # Prepare sequences for packing (order determined by subclass)
+        indexed_lengths = self._prepare_sequences(sequence_lengths)
+
+        bins = []  # List of bins, each bin is a list of sequence indices
+        bin_remaining = []  # Remaining capacity for each bin
+
+        for length, idx in indexed_lengths:
+            # If the sequence is larger than the bin capacity, it cannot be packed
+            if length > self.bin_capacity:
+                raise ValueError(
+                    f"Sequence length {length} exceeds bin capacity {self.bin_capacity}"
+                )
+
+            # Try to find a bin where the sequence fits
+            bin_found = False
+            for i, remaining in enumerate(bin_remaining):
+                if remaining >= length:
+                    # Add the sequence to this bin
+                    bins[i].append(idx)
+                    bin_remaining[i] -= length
+                    bin_found = True
+                    break
+
+            # If no suitable bin was found, create a new one
+            if not bin_found:
+                bins.append([idx])
+                bin_remaining.append(self.bin_capacity - length)
+
+        return bins
+
+
+class FirstFitDecreasingPacker(FirstFitPacker):
+    """First-Fit Decreasing (FFD) algorithm for sequence packing.
+
+    This algorithm sorts sequences by length in descending order and then
+    places each sequence into the first bin where it fits.
 
-def get_seqlen_balanced_partitions(seqlen_list: list[int], k_partitions: int, equal_size: bool):
+    Time complexity: O(n log n) for sorting + O(n * m) for packing,
+    where n is the number of sequences and m is the number of bins.
     """
-    Calculates partitions of indices from seqlen_list such that the sum of sequence lengths
-    in each partition is balanced. Uses the Karmarkar-Karp differencing method.
+    def _prepare_sequences(self, seqlen_list: list[int]) -> list[tuple[int, int]]:
+        # Create a list of (length, index) pairs
+        indexed_lengths = [(length, i) for i, length in enumerate(seqlen_list)]
 
-    This is useful for balancing workload across devices or batches, especially when
-    dealing with variable sequence lengths.
+        # Sort by length in descending order
+        indexed_lengths.sort(reverse=True)
+
+        return indexed_lengths
+
+class FirstFitShufflePacker(FirstFitPacker):
+    """First-Fit Shuffle algorithm for sequence packing.
+
+    This algorithm randomly shuffles the sequences and then places each
+    sequence into the first bin where it fits.
+
+    Time complexity: O(n * m) for packing, where n is the number of sequences
+    and m is the number of bins.
+    """
+
+    def _prepare_sequences(self, seqlen_list: list[int]) -> list[tuple[int, int]]:
+        # Create a list of (length, index) pairs
+        indexed_lengths = [(length, i) for i, length in enumerate(seqlen_list)]
+
+        # Shuffle the sequences
+        random.shuffle(indexed_lengths)
+
+        return indexed_lengths
+
+class ModifiedFirstFitDecreasingPacker(SequencePacker):
+    """Modified First-Fit Decreasing (MFFD) algorithm for sequence packing.
+
+    This algorithm implements the Johnson & Garey (1985) Modified First-Fit-Decreasing
+    heuristic. It classifies items into four categories (large, medium, small, tiny)
+    and uses a sophisticated 5-phase packing strategy to achieve better bin utilization
+    than standard First-Fit Decreasing.
+
+    The algorithm phases:
+    1. Classify items by size relative to bin capacity
+    2. Create one bin per large item
+    3. Add medium items to large bins (forward pass)
+    4. Add pairs of small items to bins with medium items (backward pass)
+    5. Greedily fit remaining items
+    6. Apply FFD to any leftovers
+
+    Time complexity: O(n log n) for sorting + O(n * m) for packing,
+    where n is the number of sequences and m is the number of bins.
+    """
+
+    def _classify_items(self, items: list[tuple[int, int]]) -> tuple[list[tuple[int, int]], list[tuple[int, int]], list[tuple[int, int]], list[tuple[int, int]]]:
+        """Split items into large / medium / small / tiny classes.
+
+        Follows the classification used by Johnson & Garey:
+            large   : (C/2, C]
+            medium  : (C/3, C/2]
+            small   : (C/6, C/3]
+            tiny    : (0  , C/6]
+
+        Args:
+            items: List of (index, size) tuples
+
+        Returns:
+            Tuple of four lists (large, medium, small, tiny) without additional sorting.
+        """
+        large, medium, small, tiny = [], [], [], []
+        for idx, size in items:
+            if size > self.bin_capacity / 2:
+                large.append((idx, size))
+            elif size > self.bin_capacity / 3:
+                medium.append((idx, size))
+            elif size > self.bin_capacity / 6:
+                small.append((idx, size))
+            else:
+                tiny.append((idx, size))
+        return large, medium, small, tiny
+    
+    def _pack_implementation(self, seqlen_list: list[int]) -> list[list[int]]:
+        # Validate sequence lengths
+        self._validate_sequence_lengths(seqlen_list)
+
+        items: list[tuple[int, int]] = [(i, l) for i, l in enumerate(seqlen_list)]
+        # Phase-0: classify
+        large, medium, small, tiny = self._classify_items(items)
+        # Sort according to the rules of MFFD
+
+        # Sort according to the rules of MFFD
+        large.sort(key=lambda x: x[1], reverse=True)  # descending size
+        medium.sort(key=lambda x: x[1], reverse=True)
+        small.sort(key=lambda x: x[1])  # ascending size
+        tiny.sort(key=lambda x: x[1])
+
+        # Phase-1: start one bin per large item
+        bins: list[list[tuple[int, int]]] = [[item] for item in large]
+
+        # Phase-2: try to add one medium item to each large bin (forward pass)
+        for b in bins:
+            remaining = self.bin_capacity - sum(size for _, size in b)
+            for i, (idx, size) in enumerate(medium):
+                if size <= remaining:
+                    b.append(medium.pop(i))
+                    break
+
+        # Phase-3: backward pass â€“ fill with two small items where possible 
+        for b in reversed(bins):
+            has_medium = any(
+                self.bin_capacity / 3 < size <= self.bin_capacity / 2 for _, size in b
+            )
+            if has_medium or len(small) < 2:
+                continue
+            remaining = self.bin_capacity - sum(size for _, size in b)
+            if small[0][1] + small[1][1] > remaining:
+                continue
+            first_small = small.pop(0)
+            # pick the *largest* small that fits with first_small (so iterate from end)
+            second_idx = None
+            for j in range(len(small) - 1, -1, -1):
+                if small[j][1] <= remaining - first_small[1]:
+                    second_idx = j
+                    break
+
+            if second_idx is not None:
+                second_small = small.pop(second_idx)
+                b.extend([first_small, second_small])
+
+        # Phase-4: forward greedy fit of remaining items
+        remaining_items = sorted(medium + small + tiny, key=lambda x: x[1], reverse=True)
+        for b in bins:
+            while remaining_items:
+                rem = self.bin_capacity - sum(size for _, size in b)
+                # if even the smallest remaining doesn't fit we break
+                if rem < remaining_items[-1][1]:
+                    break
+                # pick the first (largest) that fits
+                chosen_idx = None
+                for i, (_, size) in enumerate(remaining_items):
+                    if size <= rem:
+                        chosen_idx = i
+                        break
+                if chosen_idx is None:
+                    break
+                b.append(remaining_items.pop(chosen_idx))
+
+        # Phase-5: FFD on leftovers
+        leftovers = remaining_items  # renamed for clarity
+        ffd_bins: list[list[tuple[int, int]]] = []
+        for idx, size in sorted(leftovers, key=lambda x: x[1], reverse=True):
+            placed = False
+            for bin_ffd in ffd_bins:
+                if size <= self.bin_capacity - sum(s for _, s in bin_ffd):
+                    bin_ffd.append((idx, size))
+                    placed = True
+                    break
+            if not placed:
+                ffd_bins.append([(idx, size)])
+        bins.extend(ffd_bins)
+
+        # Convert to list of index lists (discard sizes)
+        return [[idx for idx, _ in b] for b in bins]
+
+
+def get_packer(
+    algorithm: Union[PackingAlgorithm, str],
+    bin_capacity: int,
+    min_bin_count: Optional[int] = None,
+    bin_count_multiple: Optional[int] = None,
+    equal_size: bool = True,
+) -> SequencePacker:
+    """Get a sequence packer based on the specified algorithm."""
+    packers: Dict[PackingAlgorithm, Type[SequencePacker]] = {
+        PackingAlgorithm.KARMARKAR_KARP: KarmarkarKarpPacker,
+        PackingAlgorithm.GREEDY_PARTITION: GreedyPartitionPacker,
+        PackingAlgorithm.CONCATENATIVE: ConcatenativePacker,
+        PackingAlgorithm.FIRST_FIT_DECREASING: FirstFitDecreasingPacker,
+        PackingAlgorithm.FIRST_FIT_SHUFFLE: FirstFitShufflePacker,
+        PackingAlgorithm.MODIFIED_FIRST_FIT_DECREASING: ModifiedFirstFitDecreasingPacker,
+    }
+    # Convert string to PackingAlgorithm enum if needed
+    if isinstance(algorithm, str):
+        algorithm = PackingAlgorithm(algorithm)
+    if algorithm not in packers:
+        raise ValueError(f"Unknown packing algorithm: {algorithm}")
+    return packers[algorithm](bin_capacity, min_bin_count, bin_count_multiple, equal_size)
+
+
+def get_seqlen_balanced_partitions(
+    seqlen_list: list[int],
+    algorithm: str,
+    bin_capacity: int,
+    equal_size: bool,
+    min_bin_count: Optional[int] = None,
+    bin_count_multiple: Optional[int] = None,
+):
+    """
+    Calculate balanced partitions of indices from seqlen_list.
+
+    Default behavior uses the Karmarkar-Karp differencing method (compatibility mode).
+    If an "algorithm" is provided, switches to the generic packing path via get_packer,
+    which supports multiple algorithms (FFD, KK, Greedy, etc.).
 
     Args:
-        seqlen_list (List[int]): A list of sequence lengths for each item.
-        k_partitions (int): The desired number of partitions.
-        equal_size (bool): If True, ensures that each partition has the same number of items.
-                           Requires len(seqlen_list) to be divisible by k_partitions.
-                           If False, partitions can have varying numbers of items, focusing
-                           only on balancing the sum of sequence lengths.
+        seqlen_list: Sequence lengths per item.
+        algorithm: Optional. When provided, choose a packing algorithm from PackingAlgorithm or its string name.
+        bin_capacity: Optional. Required when algorithm is provided; capacity per bin (micro-batch) in tokens.
+        min_bin_count: Optional. Minimum number of bins to create.
+        bin_count_multiple: Optional. Constrain total bins to be a multiple of this value.
+        equal_size: If True, try to ensure each partition has the same number of items.
 
     Returns:
-        List[List[int]]: A list containing k_partitions lists. Each inner list contains the
-                         original indices of the items assigned to that partition. The indices
-                         within each partition list are sorted.
-
-    Raises:
-        AssertionError: If len(seqlen_list) < k_partitions.
-        AssertionError: If equal_size is True and len(seqlen_list) is not divisible by k_partitions.
-        AssertionError: If any resulting partition is empty.
+        List of index lists (partitions). Indices in each partition are sorted.
     """
-    assert len(seqlen_list) >= k_partitions, f"number of items:[{len(seqlen_list)}] < k_partitions:[{k_partitions}]"
 
-    def _check_and_sort_partitions(partitions):
-        assert len(partitions) == k_partitions, f"{len(partitions)} != {k_partitions}"
+    def _check_and_sort_partitions(partitions: list[list[int]], expected_partitions: Optional[int]) -> list[list[int]]:
+        if expected_partitions is not None:
+            assert len(partitions) == expected_partitions, f"{len(partitions)} != {expected_partitions}"
         seen_idx = set()
-        sorted_partitions = [None] * k_partitions
+        sorted_partitions = [None] * len(partitions)
         for i, partition in enumerate(partitions):
             assert len(partition) > 0, f"the {i}-th partition is empty"
             for idx in partition:
@@ -198,8 +643,16 @@ def get_seqlen_balanced_partitions(seqlen_list: list[int], k_partitions: int, eq
         assert seen_idx == set(range(len(seqlen_list)))
         return sorted_partitions
 
-    partitions = karmarkar_karp(seqlen_list=seqlen_list, k_partitions=k_partitions, equal_size=equal_size)
-    return _check_and_sort_partitions(partitions)
+    packer = get_packer(
+        algorithm=PackingAlgorithm(algorithm),
+        bin_capacity=bin_capacity,
+        min_bin_count=min_bin_count,
+        bin_count_multiple=bin_count_multiple,
+        equal_size=equal_size,
+    )
+    partitions = packer.pack(seqlen_list)
+
+    return _check_and_sort_partitions(partitions, expected_partitions=max(min_bin_count, len(partitions)))
 
 
 def log_seqlen_unbalance(seqlen_list: list[int], partitions: list[list[int]], prefix):
@@ -266,23 +719,127 @@ def rearrange_micro_batches(
     same_micro_num_in_dp=True,
     min_num_micro_batch=None,
     use_dynamic_bsz_balance=True,
+    use_sequence_packing_balance=False,
+    algorithm="modified_first_fit_decreasing",
+    input_key="input_ids",
+    input_lengths_key="input_lengths",
+    sp_size=1,
+    tp_size=1,
+    pad_multiple=None,
 ):
     """
     Split a batch into micro-batches by total token count, with optional DP sync and padding.
+    Supports both dynamic batching (workload-based) and sequence packing (bin packing).
 
     Args:
         batch (TensorDict): must include "attention_mask" (B*S); other fields are sliced similarly.
-        max_token_len (int): max sum of attention_mask per micro-batch.
+        max_token_len (int): max sum of attention_mask per micro-batch (for dynamic batching)
+                            or bin capacity (for sequence packing).
         dp_group (optional): torch.distributed group for data-parallel sync.
         num_batches_divided_by (optional): virtual pipeline parallel size, for megatron.
         same_micro_num_in_dp (bool): if True and dp_group set, pad all ranks to the same count.
         min_num_micro_batch (int, optional): force at least this many splits (pads empty ones).
-        use_dynamic_bsz_balance (bool, optional): balance the computational workload between micro-batches
+        use_dynamic_bsz_balance (bool, optional): balance the computational workload between micro-batches.
+        use_sequence_packing_balance (bool, optional): if True, use sequence packing instead of dynamic batching.
+        algorithm (str, optional): packing algorithm for sequence packing (default: "modified_first_fit_decreasing").
+        input_key (str, optional): key for input tensor (default: "input_ids").
+        input_lengths_key (str, optional): key for sequence lengths (default: "input_lengths").
+        sp_size (int, optional): sequence parallel size (default: 1).
+        tp_size (int, optional): tensor parallel size (default: 1).
+        pad_multiple (int, optional): padding multiple for sequence packing. If None, defaults to 2*TP*SP.
 
     Returns:
         List[TensorDict]: the micro-batches.
         List[List[int]]: index lists mapping each micro-batch back to original positions.
     """
+    if use_sequence_packing_balance:
+        # NeMo-aligned sequence packing: compute packable groups only; no truncation/concat here
+        bin_capacity = max_token_len  # max_token_len is bin_capacity for sequence packing
+
+        # Build sequence lengths
+        if input_lengths_key in batch.keys():
+            seqlens_tensor = batch[input_lengths_key]
+            if isinstance(seqlens_tensor, torch.Tensor):
+                seqlens = seqlens_tensor.tolist()
+            else:
+                seqlens = list(map(int, seqlens_tensor))
+        else:
+            attn = batch["attention_mask"]  # [B, S]
+            seqlens = attn.sum(dim=1).tolist()
+
+        # Compute pad multiple ~ 2 * TP * SP
+        if pad_multiple is None:
+            pad_multiple = 2 * int(tp_size) * int(sp_size)
+
+        def _pad_to_multiple(x: int, m: int) -> int:
+            return ((int(x) + int(m) - 1) // int(m)) * int(m)
+
+        padded_seqlens = [_pad_to_multiple(l, pad_multiple) for l in seqlens]
+
+        # Determine target bin count considering VPP and DP alignment
+        # First, do an initial packing to get the base bin count
+        initial_bins = get_seqlen_balanced_partitions(
+            seqlen_list=padded_seqlens,
+            algorithm=algorithm,
+            bin_capacity=bin_capacity,
+            min_bin_count=None,
+            bin_count_multiple=None,
+            equal_size=False,
+        )
+        initial_bin_count = len(initial_bins)
+
+        # Align with VPP if requested (num_batches_divided_by)
+        target_bin_count = initial_bin_count
+        if num_batches_divided_by is not None and num_batches_divided_by > 1:
+            target_bin_count = roundup_divisible(initial_bin_count, int(num_batches_divided_by))
+
+        # Align micro-batch count across DP ranks if needed
+        if dist.is_initialized() and dp_group is not None and same_micro_num_in_dp:
+            device = batch[input_key].device if isinstance(batch.get(input_key, None), torch.Tensor) else "cpu"
+            local_bins = torch.tensor([target_bin_count], device=device)
+            dist.all_reduce(local_bins, op=dist.ReduceOp.MAX, group=dp_group)
+            target_bin_count = int(local_bins.item())
+            # Ensure DP-aligned count also satisfies VPP requirement
+            if num_batches_divided_by is not None and num_batches_divided_by > 1:
+                target_bin_count = roundup_divisible(target_bin_count, int(num_batches_divided_by))
+
+        # Use get_seqlen_balanced_partitions to do final packing with all constraints
+        # If target_bin_count differs from initial, we need to repack with min_bin_count constraint
+        if target_bin_count != initial_bin_count:
+            bins = get_seqlen_balanced_partitions(
+                seqlen_list=padded_seqlens,
+                algorithm=algorithm,
+                bin_capacity=bin_capacity,
+                min_bin_count=target_bin_count,
+                bin_count_multiple=None,
+                equal_size=False,
+            )
+        else:
+            # target_bin_count == initial_bin_count, so initial packing already satisfies all constraints
+            bins = initial_bins
+
+        # Materialize micro-batches (grouping only)
+        micro_batches = []
+        bin_packed_lengths = [sum(padded_seqlens[i] for i in part) for part in bins]
+        max_packed_len = max(bin_packed_lengths) if bin_packed_lengths else 0
+        # optional: if VPP provided, ensure max_packed_len is visible to all mbs for PP
+        for partition in bins:
+            curr_mb = tu.index_select_tensor_dict(batch, partition)
+            tu.assign_non_tensor(curr_mb, pack_sequences=True)
+            # seq_length_key: key name holding per-sample true lengths (e.g., "input_lengths")
+            tu.assign_non_tensor(curr_mb, seq_length_key=input_lengths_key)
+            tu.assign_non_tensor(curr_mb, pad_individual_seqs_to_multiple_of=int(pad_multiple))
+            curr_packed_len = sum(padded_seqlens[i] for i in partition) 
+            # packed_seq_len: total packed tokens of this micro-batch (sum of per-seq aligned lengths)
+            tu.assign_non_tensor(curr_mb, packed_seq_len=int(curr_packed_len))
+            # for PP>1 cases, downstream may choose to pad all packed seqs to the same max within a global-batch
+            # pad_full_seq_to: global max packed length ensuring fixed [batch x seqlen] buffers for pipeline parallel
+            tu.assign_non_tensor(curr_mb, pad_full_seq_to=int(max_packed_len))
+            micro_batches.append(curr_mb)
+        batch_idx_list = bins
+        return micro_batches, batch_idx_list
+
+    # Dynamic batching path (original implementation)
     # this is per local micro_bsz
     input_ids = batch["input_ids"]
     if input_ids.is_nested:
@@ -311,7 +868,15 @@ def rearrange_micro_batches(
     assert num_micro_batches <= len(seq_len_effective)
 
     workloads = calculate_workload(seq_len_effective)
-    micro_bsz_idx = get_seqlen_balanced_partitions(workloads, num_micro_batches, equal_size=False)
+    # Use karmarkar_karp algorithm for dynamic batching
+    micro_bsz_idx = get_seqlen_balanced_partitions(
+        seqlen_list=workloads.tolist() if isinstance(workloads, torch.Tensor) else workloads,
+        algorithm="karmarkar_karp",
+        bin_capacity=total_seqlen,  # Not used for karmarkar_karp when min_bin_count is set
+        min_bin_count=num_micro_batches,
+        bin_count_multiple=None,
+        equal_size=False,
+    )
 
     if use_dynamic_bsz_balance:
         # Use the sum of squared sequence lengths to approximate attention computation workload
