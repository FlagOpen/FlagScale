diff --git a/verl/models/mcore/saver.py b/verl/models/mcore/saver.py
index 2a954b24..0b572a36 100644
--- a/verl/models/mcore/saver.py
+++ b/verl/models/mcore/saver.py
@@ -19,6 +19,7 @@ import torch
 import torch.distributed as dist
 from megatron.core import mpu
 from megatron.core.distributed import DistributedDataParallel as LocalDDP
+from megatron.core.models.gpt.gpt_model import GPTModel
 from megatron.core.transformer.module import Float16Module
 from torch.nn.parallel import DistributedDataParallel as torchDDP
 
@@ -123,6 +124,7 @@ def merge_megatron_ckpt_gptmodel(wrapped_models, config, dtype, is_value_model=F
 
     for i, wrapped_model in enumerate(wrapped_models):
         models[i] = unwrap_model(wrapped_model, (torchDDP, LocalDDP, Float16Module))
+        gpt_model_module = _get_gpt_model(models[i])
         assert len(models[i].decoder.layers) == num_layers_per_model, (
             "len model layers {} not equal to num_layers_per_model {}".format(
                 len(models[i].decoder.layers), num_layers_per_model
