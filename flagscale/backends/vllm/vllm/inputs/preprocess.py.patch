diff --git a/vllm/inputs/preprocess.py b/vllm/inputs/preprocess.py
index 7518cd8fc..8ff83d805 100644
--- a/vllm/inputs/preprocess.py
+++ b/vllm/inputs/preprocess.py
@@ -311,6 +311,8 @@ class InputPreprocessor:
     ) -> Union[TokenInputs, MultiModalInputs]:
         prompt_token_ids = self._truncate_inputs(
             parsed_content["prompt_token_ids"], tokenization_kwargs)
+        uncond_prompt_token_ids = self._truncate_inputs(
+            parsed_content["uncond_prompt_token_ids"], tokenization_kwargs)
 
         inputs: Union[TokenInputs, MultiModalInputs]
         if multi_modal_data := parsed_content.get("multi_modal_data"):
@@ -322,7 +324,10 @@ class InputPreprocessor:
                 mm_uuids=mm_uuids,
             )
         else:
-            inputs = token_inputs(prompt_token_ids=prompt_token_ids)
+            inputs = token_inputs(
+                prompt_token_ids=prompt_token_ids,
+                uncond_prompt_token_ids=uncond_prompt_token_ids
+            )
 
         if cache_salt := parsed_content.get("cache_salt"):
             inputs["cache_salt"] = cache_salt
@@ -337,6 +342,7 @@ class InputPreprocessor:
         mm_uuids: Optional[MultiModalUUIDDict] = None,
     ) -> Union[TokenInputs, MultiModalInputs]:
         prompt_text = parsed_content["prompt"]
+        uncond_prompt_text = parsed_content.get("uncond_prompt")
 
         inputs: Union[TokenInputs, MultiModalInputs]
         if multi_modal_data := parsed_content.get("multi_modal_data"):
@@ -352,9 +358,17 @@ class InputPreprocessor:
                 prompt_text,
                 tokenization_kwargs=tokenization_kwargs,
             )
+            uncond_prompt_token_ids = None
+            if uncond_prompt_text:
+                uncond_prompt_token_ids = self._tokenize_prompt_async(
+                    uncond_prompt_text,
+                    tokenization_kwargs=tokenization_kwargs,
+                )
             inputs = token_inputs(
                 prompt=prompt_text,
                 prompt_token_ids=prompt_token_ids,
+                uncond_prompt=uncond_prompt_text,
+                uncond_prompt_token_ids=uncond_prompt_token_ids,
             )
 
         if cache_salt := parsed_content.get("cache_salt"):
