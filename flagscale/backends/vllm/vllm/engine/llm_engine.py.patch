diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index e7919d904..f10c094ba 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -573,9 +573,28 @@ class LLMEngine:
 
         encoder_inputs, decoder_inputs = split_enc_dec_inputs(processed_inputs)
 
+        # --- FLAGSCALE MODIFICATION BEG ---
+        negative_decoder_inputs = None
+        if "negative_prompt_token_ids" in processed_inputs \
+            and processed_inputs["negative_prompt_token_ids"] is not None:
+            positive_inputs = processed_inputs.copy()
+            positive_inputs.pop("negative_prompt_token_ids")
+            negative_inputs = processed_inputs.copy()
+            negative_inputs["prompt_token_ids"] = negative_inputs["negative_prompt_token_ids"]
+            negative_inputs.pop("negative_prompt_token_ids")
+            encoder_inputs = None
+            decoder_inputs = positive_inputs
+            negative_decoder_inputs = negative_inputs
+        # --- FLAGSCALE MODIFICATION END ---
+
         seq = Sequence(seq_id, decoder_inputs, block_size, eos_token_id,
                        lora_request)
 
+        # --- FLAGSCALE MODIFICATION BEG ---
+        negative_seq = (None if negative_decoder_inputs is None else Sequence(
+            seq_id, negative_decoder_inputs, block_size, eos_token_id, lora_request))
+        # --- FLAGSCALE MODIFICATION END ---
+
         encoder_seq = (None if encoder_inputs is None else Sequence(
             seq_id, encoder_inputs, block_size, eos_token_id, lora_request))
 
@@ -589,7 +608,8 @@ class LLMEngine:
                 lora_request=lora_request,
                 trace_headers=trace_headers,
                 encoder_seq=encoder_seq,
-                priority=priority)
+                priority=priority,
+                negative_seq=negative_seq) # --- FLAGSCALE MODIFICATION ---
         elif isinstance(params, PoolingParams):
             seq_group = self._create_sequence_group_with_pooling(
                 request_id,
@@ -727,6 +747,7 @@ class LLMEngine:
         trace_headers: Optional[Mapping[str, str]] = None,
         encoder_seq: Optional[Sequence] = None,
         priority: int = 0,
+        negative_seq: Optional[Sequence] = None, # --- FLAGSCALE MODIFICATION ---
     ) -> SequenceGroup:
         """Creates a SequenceGroup with SamplingParams."""
         max_logprobs = self.get_model_config().max_logprobs
@@ -760,7 +781,8 @@ class LLMEngine:
                                   trace_headers=trace_headers,
                                   encoder_seq=encoder_seq,
                                   priority=priority,
-                                  draft_size=draft_size)
+                                  draft_size=draft_size,
+                                  negative_seqs=[negative_seq] if negative_seq else None) # --- FLAGSCALE MODIFICATION ---
 
         return seq_group
 
@@ -1016,6 +1038,12 @@ class LLMEngine:
                 else:
                     seq_group.update_num_computed_tokens(
                         seq_group_meta.token_chunk_size or 0)
+                    # --- FLAGSCALE MODIFICATION BEG ---
+                    if seq_group.has_negative_seqs():
+                        seq_group.update_negative_num_computed_tokens(
+                            scheduled_seq_group.negative_token_chunk_size or 0
+                        )
+                    # --- FLAGSCALE MODIFICATION END ---
 
             if outputs:
                 for o in outputs:
@@ -1170,6 +1198,13 @@ class LLMEngine:
                                     if seq_group_metadata.token_chunk_size
                                     is not None else 0)
                 seq_group.update_num_computed_tokens(token_chunk_size)
+                # --- FLAGSCALE MODIFICATION BEG ---
+                if seq_group.has_negative_seqs():
+                    negative_token_chunk_size = (seq_group_metadata.negative_token_chunk_size
+                                                if seq_group_metadata.negative_token_chunk_size
+                                                is not None else 0)
+                    seq_group.update_negative_num_computed_tokens(negative_token_chunk_size)
+                # --- FLAGSCALE MODIFICATION END ---
 
             if seq_group_metadata.do_sample:
                 assert len(sequence_group_outputs.samples) == 1, (
@@ -1190,6 +1225,12 @@ class LLMEngine:
                 else:
                     seq.append_token_id(sample.output_token, sample.logprobs,
                                         sample.output_embed)
+                    # --- FLAGSCALE MODIFICATION BEG ---
+                    if seq_group.has_negative_seqs():
+                        negative_seq = seq_group.negative_seqs[0]
+                        negative_seq.append_token_id(sample.output_token, sample.logprobs,
+                                                     sample.output_embed)
+                    # --- FLAGSCALE MODIFICATION END ---
 
     def step(self) -> List[Union[RequestOutput, PoolingRequestOutput]]:
         """Performs one decoding iteration and returns newly generated results.
