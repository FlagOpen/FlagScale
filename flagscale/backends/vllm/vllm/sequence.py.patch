diff --git a/vllm/sequence.py b/vllm/sequence.py
index fe87b52f9..c4106dcc6 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -714,6 +714,7 @@ class SequenceGroup:
                  pooling_params: Optional[PoolingParams] = None,
                  pooled_data: Optional[torch.Tensor] = None,
                  encoder_seq: Optional[Sequence] = None,
+                 negative_seqs: Optional[list[Sequence]] = None, # --- FLAGSCALE MODIFICATION ---
                  trace_headers: Optional[Mapping[str, str]] = None,
                  priority: int = 0,
                  draft_size: int = 1) -> None:
@@ -742,6 +743,15 @@ class SequenceGroup:
 
         self.cached_request_output = None
 
+        # --- FLAGSCALE MODIFICATION BEG ---
+        self.negative_seqs = negative_seqs
+        self.negative_seqs_dict = {}
+        if negative_seqs:
+            self.negative_seqs_dict = {seq.seq_id: seq for seq in self.negative_seqs}
+            assert self.is_single_seq is True
+            assert self.seqs_dict.keys() == self.negative_seqs_dict.keys()
+        # --- FLAGSCALE MODIFICATION END ---
+
     @property
     def prompt(self) -> Optional[str]:
         return self.first_seq.prompt
@@ -875,6 +885,23 @@ class SequenceGroup:
 
         return [seq for seq in self.seqs if seq.status == status]
 
+    # --- FLAGSCALE MODIFICATION BEG ---
+    def has_negative_seqs(self) -> bool:
+        return self.negative_seqs is not None
+
+    def get_negative_seqs(
+        self,
+        status: Optional[SequenceStatus] = None,
+    ) -> list[Sequence]:
+        if status is None:
+            return self.negative_seqs
+
+        if self.is_single_seq:
+            return self.negative_seqs if self.first_seq.status == status else []
+
+        return [self.negative_seqs_dict[seq.seq_id] for seq in self.seqs if seq.status == status]
+    # --- FLAGSCALE MODIFICATION END ---
+
     def is_encoder_decoder(self) -> bool:
         return self.encoder_seq is not None
 
@@ -893,6 +920,14 @@ class SequenceGroup:
             if not seq.is_finished():
                 seq.data.update_num_computed_tokens(num_new_computed_tokens)
 
+    # --- FLAGSCALE MODIFICATION BEG ---
+    def update_negative_num_computed_tokens(self, num_new_computed_tokens: int):
+        for seq in self.seqs:
+            if not seq.is_finished():
+                negative_seq = self.negative_seqs_dict[seq.seq_id]
+                negative_seq.data.update_num_computed_tokens(num_new_computed_tokens)
+    # --- FLAGSCALE MODIFICATION END ---
+
     def get_num_uncomputed_tokens(self) -> int:
         num_uncomputed_tokens = 0
         for seq in self.seqs:
@@ -1009,6 +1044,12 @@ class SequenceGroupMetadata(
     cross_block_table: Optional[list[int]] = None
     token_chunk_size: Optional[int] = None
 
+    # --- FLAGSCALE MODIFICATION BEG ---
+    negative_seq_data: Optional[dict[int, SequenceData]] = None
+    negative_block_tables: Optional[dict[int, list[int]]] = None
+    negative_token_chunk_size: Optional[int] = None
+    # --- FLAGSCALE MODIFICATION END ---
+
     ### Stateful fields that are lazily defined. ###
     # The number of speculative tokens adopted in this request.
     # None means specuative decoding is not used.
@@ -1024,6 +1065,15 @@ class SequenceGroupMetadata(
             else:
                 self.token_chunk_size = 1
 
+        # --- FLAGSCALE MODIFICATION BEG ---
+        if self.negative_seq_data is not None and self.negative_token_chunk_size is None:
+            if self.is_prompt:
+                self.negative_token_chunk_size = next(iter(
+                    self.negative_seq_data.values())).get_len()
+            else:
+                self.negative_token_chunk_size = 1
+        # --- FLAGSCALE MODIFICATION END ---
+
     @property
     def lora_int_id(self) -> int:
         return self.lora_request.lora_int_id if self.lora_request else 0
