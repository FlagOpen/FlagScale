diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 5a185e745..8e84de4ed 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -74,6 +74,24 @@ torch._dynamo.config.cache_size_limit = 128
 torch._dynamo.config.accumulated_cache_size_limit = 128
 
 
+# --- FLAGSCALE MODIFICATION BEG ---
+# Know more about FlagGems: https://github.com/FlagOpen/FlagGems  
+import os
+if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
+    try:
+        print("Try to using FLAGGEMS...")
+        import flag_gems
+        flag_gems.enable(record=True, unused=["exponential_", "softmax"], path="/tmp/gems_oplist.log.txt")
+        logger.info("Successfully enabled flag_gems as default ops implementation.")
+    except ImportError as e:
+        # Throw an exception directly if failure occurs
+        raise ImportError("Failed to import 'flag_gems'. Please install flag_gems or set USE_FLAGGEMS=false to disable it.") from e
+    except Exception as e:
+        # Throw an exception directly if failure occurs
+        raise RuntimeError(f"Failed to enable 'flag_gems': {e}. Please check your flag_gems installation or set USE_FLAGGEMS=false to disable it.") from e
+# --- FLAGSCALE MODIFICATION END ---
+
+
 @dataclass(frozen=True)
 class ModelInputForGPU(ModelRunnerInputBase):
     """
@@ -493,12 +511,18 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
         self.attn_metadata_builder.prepare()
 
     def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,
-                      seq_group_metadata: SequenceGroupMetadata):
+                      seq_group_metadata: SequenceGroupMetadata, is_negative: bool = False): # --- FLAGSCALE MODIFICATION ---
         """Compute context length, sequence length and tokens
         for the given sequence data.
         """
-        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]
-        token_chunk_size = seq_group_metadata.token_chunk_size
+        # --- FLAGSCALE MODIFICATION BEG ---
+        if is_negative:
+            seq_data = seq_group_metadata.negative_seq_data[inter_data.seq_ids[seq_idx]]
+            token_chunk_size = seq_group_metadata.negative_token_chunk_size
+        # --- FLAGSCALE MODIFICATION END ---
+        else:
+            seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]
+            token_chunk_size = seq_group_metadata.token_chunk_size
 
         # Compute context length (the number of tokens that are
         # already computed) and sequence length (total number of tokens).
@@ -548,7 +572,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
 
     def _compute_for_prefix_cache_hit(
             self, inter_data: InterDataForSeqGroup, seq_idx: int,
-            seq_group_metadata: SequenceGroupMetadata):
+            seq_group_metadata: SequenceGroupMetadata, is_negative: bool = False): # --- FLAGSCALE MODIFICATION ---
         """Check if hit prefix cache (i.e., some blocks are already computed).
         If hit, update input tokens and positions to only compute the
         remaining blocks.
@@ -613,7 +637,8 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
 
     def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,
                                     seq_idx: int,
-                                    seq_group_metadata: SequenceGroupMetadata):
+                                    seq_group_metadata: SequenceGroupMetadata,
+                                    is_negative: bool = False): # --- FLAGSCALE MODIFICATION ---
         """Update seq_len and curr_sliding_window_block for the given
         sequence data (only required by decoding) if sliding window is enabled.
         """
@@ -637,7 +662,8 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
 
     def _compute_lora_input(self, inter_data: InterDataForSeqGroup,
                             seq_idx: int,
-                            seq_group_metadata: SequenceGroupMetadata):
+                            seq_group_metadata: SequenceGroupMetadata,
+                            is_negative: bool = False): # --- FLAGSCALE MODIFICATION ---
         """If LoRA is enabled, compute LoRA index and prompt mapping."""
         if not self.enable_lora:
             return
@@ -656,7 +682,8 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
             inter_data.lora_prompt_mapping.append([])
 
     def _compute_multi_modal_input(self, inter_data: InterDataForSeqGroup,
-                                   seq_group_metadata: SequenceGroupMetadata):
+                                   seq_group_metadata: SequenceGroupMetadata,
+                                   is_negative: bool = False): # --- FLAGSCALE MODIFICATION ---
         """If multi-modal data is given, add it to the input."""
         # NOTE: mm_kwargs only includes the subset of multi-modal items that
         # intersect with the current prefill positions.
@@ -740,6 +767,26 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
         for per_seq_group_fn in self.per_seq_group_compute_fns:
             per_seq_group_fn(inter_data, seq_group_metadata)
 
+        # --- FLAGSCALE MODIFICATION BEG ---
+        if seq_group_metadata.negative_seq_data:
+            negative_inter_data = self.init_cached_inter_data(
+                request_id=seq_group_metadata.request_id,
+                seq_ids=seq_ids,
+                is_prompt=is_prompt,
+                block_tables=seq_group_metadata.negative_block_tables,
+                computed_block_nums=[], # for prefix caching.
+                reinit=True,
+                reinit_use_defaults=True,
+                encoder_seq_len=encoder_seq_len)
+            self.inter_data_list.append(negative_inter_data)
+
+            for seq_idx in range(n_seqs):
+                for per_seq_fn in self.per_seq_compute_fns:
+                    per_seq_fn(negative_inter_data, seq_idx, seq_group_metadata, is_negative=True)
+            for per_seq_group_fn in self.per_seq_group_compute_fns:
+                per_seq_group_fn(negative_inter_data, seq_group_metadata)
+        # --- FLAGSCALE MODIFICATION END ---
+
     def _use_captured_graph(self,
                             batch_size: int,
                             decode_only: bool,
