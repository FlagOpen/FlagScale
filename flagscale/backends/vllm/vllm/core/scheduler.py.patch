diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py
index 61346da14..ae1d2cd3c 100644
--- a/vllm/core/scheduler.py
+++ b/vllm/core/scheduler.py
@@ -1,6 +1,7 @@
 # SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
+import contextlib # --- FLAGSCALE MODIFICATION ---
 import enum
 import os
 import random
@@ -130,6 +131,7 @@ class ScheduledSequenceGroup:
     # 1 for decoding. Same as prompt tokens for prefill, but if prefill is
     # chunked, it can be smaller than that.
     token_chunk_size: int
+    negative_token_chunk_size: int = 0 # --- FLAGSCALE MODIFICATION ---
 
 
 @dataclass
@@ -314,10 +316,61 @@ def scheduler_running_outputs_builder():
 
 def scheduled_seq_group_builder():
     return ScheduledSequenceGroup(SequenceGroup.__new__(SequenceGroup),
-                                  token_chunk_size=0)
+                                  token_chunk_size=0,
+                                  negative_token_chunk_size=0) # --- FLAGSCALE MODIFICATION ---
     # return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)
 
 
+# --- FLAGSCALE MODIFICATION BEG ---
+@contextlib.contextmanager
+def _switch_seq_group(seq_group: SequenceGroup, new_seqs: Sequence):
+    origin_seq = seq_group.seqs
+    origin_negative_seq = seq_group.negative_seqs
+
+    if new_seqs is None:
+        yield
+        return
+
+    for idx, new_seq in enumerate(new_seqs):
+        new_seq.status = origin_seq[idx].status
+
+    seq_group.seqs = new_seqs
+    seq_group.negative_seqs = None
+
+    try:
+        yield
+    finally:
+        seq_group.seqs = origin_seq
+        seq_group.negative_seqs = origin_negative_seq
+
+
+def _update_num_new_tokens(
+    budget: SchedulingBudget,
+    num_new_tokens: int = 0,
+    num_new_tokens_negative: int = 0,
+    enable_chunking: bool = False,
+):
+    if not enable_chunking:
+        return num_new_tokens, num_new_tokens_negative
+
+    remaining_token_budget = budget.remaining_token_budget()
+    if num_new_tokens + num_new_tokens_negative >= remaining_token_budget:
+        min_num_new_tokens = min(num_new_tokens, remaining_token_budget // 2)
+        min_num_new_tokens_negative = min(num_new_tokens_negative, remaining_token_budget // 2)
+
+        if min_num_new_tokens == num_new_tokens and \
+            min_num_new_tokens_negative != num_new_tokens_negative:
+            return 0, 0
+        elif min_num_new_tokens != num_new_tokens and \
+            min_num_new_tokens_negative == num_new_tokens_negative:
+            return 0, 0
+        else:
+            return min_num_new_tokens, min_num_new_tokens_negative
+
+    return num_new_tokens, num_new_tokens_negative
+# --- FLAGSCALE MODIFICATION END ---
+
+
 @dataclass
 class PartialPrefillMetadata:
     """Holds information about the partial prefills that are currently running
@@ -708,7 +761,23 @@ class Scheduler:
             )
 
             num_running_tokens = num_uncached_new_tokens
-            if num_running_tokens == 0:
+
+            # --- FLAGSCALE MODIFICATION BEG ---
+            with _switch_seq_group(seq_group, seq_group.negative_seqs):
+                num_uncached_new_tokens_negative, _ = \
+                    self._get_num_new_uncached_and_cached_tokens(
+                        seq_group,
+                        SequenceStatus.RUNNING,
+                        enable_chunking,
+                        budget,
+                        partial_prefill_metadata,
+                    )
+            num_running_tokens_negative = num_uncached_new_tokens_negative
+            num_running_tokens, num_running_tokens_negative = _update_num_new_tokens(
+                budget, num_running_tokens, num_running_tokens_negative, enable_chunking)
+            # --- FLAGSCALE MODIFICATION END ---
+
+            if num_running_tokens + num_running_tokens_negative == 0: # --- FLAGSCALE MODIFICATION ---
                 # No budget => Stop
                 break
 
@@ -727,7 +796,7 @@ class Scheduler:
             # slot to keep all the sequence groups in the RUNNING state.
             while not self._can_append_slots(seq_group, enable_chunking):
                 budget.subtract_num_batched_tokens(seq_group.request_id,
-                                                   num_running_tokens)
+                                                   num_running_tokens + num_running_tokens_negative) # --- FLAGSCALE MODIFICATION ---
                 num_running_seqs = seq_group.get_max_num_running_seqs()
                 budget.subtract_num_seqs(seq_group.request_id,
                                          num_running_seqs)
@@ -784,15 +853,17 @@ class Scheduler:
                 scheduled_seq_group.seq_group = seq_group
                 if is_prefill:
                     scheduled_seq_group.token_chunk_size = num_running_tokens
+                    scheduled_seq_group.negative_token_chunk_size = num_running_tokens_negative # --- FLAGSCALE MODIFICATION ---
                     prefill_seq_groups.append(scheduled_seq_group)
                     ret.prefill_seq_groups_list.append(seq_group)
                 else:
                     scheduled_seq_group.token_chunk_size = 1
+                    scheduled_seq_group.negative_token_chunk_size = 1 if seq_group.has_negative_seqs() else 0 # --- FLAGSCALE MODIFICATION ---
                     decode_seq_groups.append(scheduled_seq_group)
                     ret.decode_seq_groups_list.append(seq_group)
 
                 budget.add_num_batched_tokens(seq_group.request_id,
-                                              num_running_tokens)
+                                              num_running_tokens + num_running_tokens_negative) # --- FLAGSCALE MODIFICATION ---
                 # OPTIMIZATION:  Note that get_max_num_running_seqs is
                 # expensive. For the default scheduling chase where
                 # enable_chunking is False, num_seqs are updated before running
@@ -886,13 +957,21 @@ class Scheduler:
                     seq_group, SequenceStatus.SWAPPED, enable_chunking,
                     budget))
 
-            if num_new_tokens_uncached == 0 or not budget.can_schedule(
-                    num_new_tokens=num_new_tokens_uncached,
+            # --- FLAGSCALE MODIFICATION BEG ---
+            with _switch_seq_group(seq_group, seq_group.negative_seqs):
+                num_new_tokens_uncached_negative, num_new_tokens_cached_negative = (
+                    self._get_num_new_uncached_and_cached_tokens(
+                        seq_group, SequenceStatus.SWAPPED, enable_chunking,
+                        budget))
+
+            if num_new_tokens_uncached + num_new_tokens_uncached_negative == 0 or not budget.can_schedule(
+                    num_new_tokens=num_new_tokens_uncached + num_new_tokens_uncached_negative,
                     num_new_seqs=num_new_seqs,
             ):
                 self.remove_seq_from_computed_blocks_tracker(
                     seq_group, SequenceStatus.SWAPPED)
                 break
+            # --- FLAGSCALE MODIFICATION END ---
 
             if lora_int_id > 0 and curr_loras is not None:
                 curr_loras.add(lora_int_id)
@@ -905,14 +984,16 @@ class Scheduler:
                         seq_group,
                         token_chunk_size=num_new_tokens_uncached +
                         num_new_tokens_cached,
+                        negative_token_chunk_size=num_new_tokens_uncached_negative +
+                        num_new_tokens_cached_negative # --- FLAGSCALE MODIFICATION ---
                     ))
             else:
                 decode_seq_groups.append(
-                    ScheduledSequenceGroup(seq_group, token_chunk_size=1))
+                    ScheduledSequenceGroup(seq_group, token_chunk_size=1, negative_token_chunk_size=1 if seq_group.has_negative_seqs() else 0)) # --- FLAGSCALE MODIFICATION ---
             budget.add_num_batched_tokens(
                 seq_group.request_id,
-                num_batched_tokens=num_new_tokens_uncached,
-                num_cached_tokens=num_new_tokens_cached,
+                num_batched_tokens=num_new_tokens_uncached + num_new_tokens_uncached_negative, # --- FLAGSCALE MODIFICATION ---
+                num_cached_tokens=num_new_tokens_cached + num_new_tokens_cached_negative, # --- FLAGSCALE MODIFICATION ---
             )
             budget.add_num_seqs(seq_group.request_id, num_new_seqs)
 
@@ -1093,18 +1174,50 @@ class Scheduler:
                 ))
             num_new_tokens = num_new_tokens_uncached + num_new_tokens_cached
 
+            # --- FLAGSCALE MODIFICATION BEG ---
+            with _switch_seq_group(seq_group, seq_group.negative_seqs):
+                num_new_tokens_uncached_negative, num_new_tokens_cached_negative = (
+                    self._get_num_new_uncached_and_cached_tokens(
+                        seq_group,
+                        SequenceStatus.WAITING,
+                        enable_chunking,
+                        budget,
+                        partial_prefill_metadata=partial_prefill_metadata,
+                    ))
+
+            num_new_tokens_uncached, num_new_tokens_uncached_negative = _update_num_new_tokens(
+                budget, num_new_tokens_uncached, num_new_tokens_uncached_negative, enable_chunking)
+
+            num_new_tokens = num_new_tokens_uncached + num_new_tokens_cached
+            num_new_tokens_negative = num_new_tokens_uncached_negative + num_new_tokens_cached_negative
+            if num_new_tokens_negative > 0:
+                assert not self.scheduler_config.is_multi_step
+                assert not self.cache_config.enable_prefix_caching
+            # --- FLAGSCALE MODIFICATION END ---
+
             if not enable_chunking:
                 num_prompt_tokens = waiting_seqs[0].get_len()
-                assert num_new_tokens == num_prompt_tokens
+                if seq_group.has_negative_seqs():
+                    seq = waiting_seqs[0]
+                    neg_seq = seq_group.negative_seqs_dict[seq.seq_id]
+                    num_prompt_tokens += neg_seq.get_len()
+                assert num_new_tokens + num_new_tokens_negative == num_prompt_tokens
 
             prompt_limit = self._get_prompt_limit(seq_group)
-            if num_new_tokens > prompt_limit:
+            if num_new_tokens + num_new_tokens_negative > prompt_limit: # --- FLAGSCALE MODIFICATION ---
+                # --- FLAGSCALE MODIFICATION BEG ---
                 logger.warning(
-                    "Input prompt (%d tokens) is too long"
+                    "Input prompt (%d tokens) (%d negative tokens) is too long"
                     " and exceeds limit of %d",
                     num_new_tokens,
+                    num_new_tokens_negative,
                     prompt_limit,
                 )
+                if num_new_tokens_negative > 0:
+                    logger.warning(
+                        "CFG is enabled, setting enable_chunked_prefill to true can help."
+                    )
+                # --- FLAGSCALE MODIFICATION END ---
                 for seq in waiting_seqs:
                     seq.status = SequenceStatus.FINISHED_IGNORED
                 self.remove_seq_from_computed_blocks_tracker(
@@ -1127,11 +1240,12 @@ class Scheduler:
                 break
             elif can_allocate == AllocStatus.NEVER:
                 logger.warning(
-                    "Input prompt (%d tokens) + lookahead slots (%d) is "
+                    "Input prompt (%d tokens) (%d negative tokens) + lookahead slots (%d) is "
                     "too long and exceeds the capacity of block_manager",
                     num_new_tokens,
+                    num_new_tokens_negative,
                     num_lookahead_slots,
-                )
+                ) # --- FLAGSCALE MODIFICATION ---
                 for seq in waiting_seqs:
                     seq.status = SequenceStatus.FINISHED_IGNORED
                 self.remove_seq_from_computed_blocks_tracker(
@@ -1177,10 +1291,10 @@ class Scheduler:
                 break
 
             num_new_seqs = seq_group.get_max_num_running_seqs()
-            if num_new_tokens_uncached == 0 or not budget.can_schedule(
-                    num_new_tokens=num_new_tokens_uncached,
+            if num_new_tokens_uncached + num_new_tokens_uncached_negative == 0 or not budget.can_schedule(
+                    num_new_tokens=num_new_tokens_uncached + num_new_tokens_uncached_negative,
                     num_new_seqs=num_new_seqs,
-            ):
+            ): # --- FLAGSCALE MODIFICATION ---
                 self.remove_seq_from_computed_blocks_tracker(
                     seq_group, SequenceStatus.WAITING)
                 break
@@ -1215,11 +1329,12 @@ class Scheduler:
 
             seq_groups.append(
                 ScheduledSequenceGroup(seq_group=seq_group,
-                                       token_chunk_size=num_new_tokens))
+                                       token_chunk_size=num_new_tokens,
+                                       negative_token_chunk_size=num_new_tokens_negative)) # --- FLAGSCALE MODIFICATION ---
             budget.add_num_batched_tokens(
                 seq_group.request_id,
-                num_batched_tokens=num_new_tokens_uncached,
-                num_cached_tokens=num_new_tokens_cached,
+                num_batched_tokens=num_new_tokens_uncached + num_new_tokens_uncached_negative, # --- FLAGSCALE MODIFICATION ---
+                num_cached_tokens=num_new_tokens_cached + num_new_tokens_cached_negative, # --- FLAGSCALE MODIFICATION ---
             )
             budget.add_num_seqs(seq_group.request_id, num_new_seqs)
 
@@ -1553,6 +1668,7 @@ class Scheduler:
                 scheduler_outputs.scheduled_seq_groups):
             seq_group = scheduled_seq_group.seq_group
             token_chunk_size = scheduled_seq_group.token_chunk_size
+            negative_token_chunk_size = scheduled_seq_group.negative_token_chunk_size # --- FLAGSCALE MODIFICATION ---
             seq_group.maybe_set_first_scheduled_time(now)
 
             seq_group_metadata = self._seq_group_metadata_cache[
@@ -1578,12 +1694,30 @@ class Scheduler:
                 encoder_seq_data = None
                 cross_block_table = None
 
+            # --- FLAGSCALE MODIFICATION BEG ---
+            if seq_group.has_negative_seqs():
+                # seq_id -> SequenceData
+                negative_seq_data: Dict[int, SequenceData] = {}
+                # seq_id -> physical block numbers
+                negative_block_tables: Dict[int, List[int]] = {}
+            else:
+                negative_seq_data = None
+                negative_block_tables = None
+            # --- FLAGSCALE MODIFICATION END ---
+
             for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
                 seq_id = seq.seq_id
                 seq_data[seq_id] = seq.data
                 block_tables[seq_id] = self.block_manager.get_block_table(seq)
                 self.block_manager.access_all_blocks_in_seq(seq, now)
 
+                # --- FLAGSCALE MODIFICATION BEG ---
+                if seq_group.has_negative_seqs():
+                    negative_seq = seq_group.negative_seqs_dict[seq_id]
+                    negative_seq_data[seq_id] = negative_seq.data
+                    negative_block_tables[seq_id] = self.block_manager.get_negative_block_table(negative_seq)
+                # --- FLAGSCALE MODIFICATION END ---
+
             if self.cache_config.enable_prefix_caching:
                 common_computed_block_nums = (
                     self.block_manager.get_common_computed_block_ids(
@@ -1609,6 +1743,16 @@ class Scheduler:
                         < seqs[0].data.get_len()):
                     do_sample = False
 
+                # --- FLAGSCALE MODIFICATION BEG ---
+                if seq_group.has_negative_seqs():
+                    negative_seqs = seq_group.get_negative_seqs()
+                    assert len(negative_seqs) == 1
+                    negative_num_computed_tokens = negative_seqs[0].data.get_num_computed_tokens()
+                    if (negative_token_chunk_size + negative_num_computed_tokens <
+                            negative_seqs[0].data.get_len()):
+                        do_sample = False
+                # --- FLAGSCALE MODIFICATION END ---
+
             # It assumes the scheduled_seq_groups is ordered by
             # prefill < decoding.
             if is_first_prefill or not self.scheduler_config.send_delta_data:
@@ -1621,10 +1765,13 @@ class Scheduler:
                     do_sample=do_sample,
                     pooling_params=seq_group.pooling_params,
                     token_chunk_size=token_chunk_size,
+                    negative_token_chunk_size=negative_token_chunk_size, # --- FLAGSCALE MODIFICATION ---
                     lora_request=seq_group.lora_request,
                     computed_block_nums=common_computed_block_nums,
                     encoder_seq_data=encoder_seq_data,
                     cross_block_table=cross_block_table,
+                    negative_seq_data=negative_seq_data, # --- FLAGSCALE MODIFICATION ---
+                    negative_block_tables=negative_block_tables, # --- FLAGSCALE MODIFICATION ---
                     state=seq_group.state,
                     token_type_ids=seq_group.token_type_ids,
                     # `multi_modal_data` will only be present for the 1st comm
@@ -1790,8 +1937,9 @@ class Scheduler:
             seq_status = None
 
         for seq in seq_group.get_seqs(status=seq_status):
-            cows = self.block_manager.append_slots(seq, num_lookahead_slots)
+            cows = self.block_manager.append_slots(seq, num_lookahead_slots, seq_group) # --- FLAGSCALE MODIFICATION ---
             if len(cows) > 0:
+                assert not seq_group.has_negative_seqs() # --- FLAGSCALE MODIFICATION ---
                 blocks_to_copy.extend(cows)
 
     def _preempt(self, seq_group: SequenceGroup,
@@ -1831,6 +1979,11 @@ class Scheduler:
             )
         self.num_cumulative_preemption += 1
 
+        # --- FLAGSCALE MODIFICATION BEG ---
+        if seq_group.has_negative_seqs() and preemption_mode != PreemptionMode.RECOMPUTE:
+            raise NotImplementedError("Swap in/out does not support in CFG.")
+        # --- FLAGSCALE MODIFICATION END ---
+
         if preemption_mode == PreemptionMode.RECOMPUTE:
             self._preempt_by_recompute(seq_group)
         elif preemption_mode == PreemptionMode.SWAP:
@@ -1849,6 +2002,12 @@ class Scheduler:
             seq.status = SequenceStatus.WAITING
             self.free_seq(seq)
             seq.reset_state_for_recompute()
+            # --- FLAGSCALE MODIFICATION BEG ---
+            if seq_group.has_negative_seqs():
+                negative_seq = seq_group.negative_seqs_dict[seq.seq_id]
+                negative_seq.status = SequenceStatus.WAITING
+                negative_seq.reset_state_for_recompute()
+            # --- FLAGSCALE MODIFICATION END ---
         self._free_seq_group_cross_attn_blocks(seq_group)
 
     def _preempt_by_swap(
