diff --git a/gpt_builders.py b/gpt_builders.py
index 89b228815..75d1be325 100644
--- a/gpt_builders.py
+++ b/gpt_builders.py
@@ -5,6 +5,7 @@ from megatron.core.models.gpt.gpt_layer_specs import (
     get_gpt_decoder_block_spec,
     get_gpt_layer_local_spec,
     get_gpt_layer_with_transformer_engine_spec,
+    get_gpt_layer_with_flag_engine_spec,
     get_gpt_mtp_block_spec,
 )
 from megatron.core.models.gpt.heterogeneous.heterogeneous_layer_specs import (
@@ -26,7 +27,14 @@ def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):
         if args.yaml_cfg is not None:
             config = core_transformer_config_from_yaml(args, "language_model")
         else:
-            config = core_transformer_config_from_args(args)
+            from flagscale.train.global_vars import get_parallel_context
+            para_ctx = get_parallel_context()
+            if para_ctx is not None:
+                config = para_ctx.get_transformer_config()
+
+            if config is None:
+                config = core_transformer_config_from_args(args)
+
     if args.use_legacy_models:
         model = megatron.legacy.model.GPTModel(
             config,
@@ -40,12 +48,14 @@ def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):
             transformer_layer_spec = import_module(args.spec)
         else:
             use_te = args.transformer_impl == "transformer_engine"
+            use_fe = args.transformer_impl == "flag_engine"
 
             if args.num_experts:
                 # Define the decoder block spec
                 transformer_layer_spec = get_gpt_decoder_block_spec(
                     config,
                     use_transformer_engine=use_te,
+                    use_flag_engine=use_fe,
                     normalization=args.normalization,
                     qk_l2_norm=args.qk_l2_norm,
                     vp_stage=vp_stage,
@@ -54,7 +64,7 @@ def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):
                 transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)
             else:
                 # Define the decoder layer spec
-                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)
+                transformer_layer_spec = _get_transformer_layer_spec(use_te, use_fe, config)
         mtp_block_spec = None
         if args.mtp_num_layers is not None:
             if (
@@ -63,13 +73,14 @@ def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):
             ):
                 # Get the decoder layer spec explicitly if no decoder layer in the last stage,
                 # Only happens with block spec (TransformerBlockSubmodules) when using MoE.
-                transformer_layer_spec_for_mtp = _get_transformer_layer_spec(use_te, config)
+                transformer_layer_spec_for_mtp = _get_transformer_layer_spec(use_te, use_fe, config)
             else:
                 transformer_layer_spec_for_mtp = transformer_layer_spec
             mtp_block_spec = get_gpt_mtp_block_spec(
                 config,
                 transformer_layer_spec_for_mtp,
                 use_transformer_engine=use_te,
+                use_flag_engine=use_fe,
                 vp_stage=vp_stage,
             )
 
@@ -90,11 +101,12 @@ def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):
             mtp_block_spec=mtp_block_spec,
             vp_stage=vp_stage,
         )
-
+    
+    print(f"[gpt_builders.py], {model=}")
     return model
 
 
-def _get_transformer_layer_spec(use_te, config):
+def _get_transformer_layer_spec(use_te, use_fe, config):
     """Get transformer layer specification based on configuration.
 
     Args:
@@ -116,6 +128,16 @@ def _get_transformer_layer_spec(use_te, config):
             qk_l2_norm=args.qk_l2_norm,
             use_kitchen=config.use_kitchen,
         )
+    elif use_fe:
+        return get_gpt_layer_with_flag_engine_spec(
+            args.num_experts,
+            args.moe_grouped_gemm,
+            args.qk_layernorm,
+            args.multi_latent_attention,
+            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,
+            qk_l2_norm=args.qk_l2_norm,
+            use_kitchen=config.use_kitchen,
+        )
     else:
         return get_gpt_layer_local_spec(
             args.num_experts,
@@ -126,3 +148,4 @@ def _get_transformer_layer_spec(use_te, config):
             normalization=args.normalization,
             use_kitchen=config.use_kitchen,
         )
+
