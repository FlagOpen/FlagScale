diff --git a/gpt_builders.py b/gpt_builders.py
index 89b228815..9e48bace4 100644
--- a/gpt_builders.py
+++ b/gpt_builders.py
@@ -26,7 +26,14 @@ def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):
         if args.yaml_cfg is not None:
             config = core_transformer_config_from_yaml(args, "language_model")
         else:
-            config = core_transformer_config_from_args(args)
+            from flagscale.train.global_vars import get_parallel_context
+            para_ctx = get_parallel_context()
+            if para_ctx is not None:
+                config = para_ctx.get_transformer_config()
+
+            if config is None:
+                config = core_transformer_config_from_args(args)
+
     if args.use_legacy_models:
         model = megatron.legacy.model.GPTModel(
             config,
@@ -115,6 +122,7 @@ def _get_transformer_layer_spec(use_te, config):
             moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,
             qk_l2_norm=args.qk_l2_norm,
             use_kitchen=config.use_kitchen,
+            flex_attention=args.flex_attention,
         )
     else:
         return get_gpt_layer_local_spec(
