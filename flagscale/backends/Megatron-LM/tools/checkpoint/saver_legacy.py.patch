diff --git a/tools/checkpoint/saver_legacy.py b/tools/checkpoint/saver_legacy.py
index 50af6a57..ab8cfebb 100644
--- a/tools/checkpoint/saver_legacy.py
+++ b/tools/checkpoint/saver_legacy.py
@@ -132,6 +132,8 @@ def save_checkpoint(queue, args):
         sys.argv.append('--untie-embeddings-and-output-weights')
     if not md.linear_bias:
         sys.argv.append('--disable-bias-linear')
+    if not md.linear_bias_qkv:
+        sys.argv.append('--disable-bias-linear-qkv')
 
     if md.model_type == 'BERT' and not md.bert_binary_head:
         sys.argv.append('--bert-no-binary-head')
@@ -359,7 +361,35 @@ def save_checkpoint(queue, args):
                 if not hasattr(models[0].language_model, 'output_layer'):
                     print("ERROR: got an output layer, but model does not have one")
                     exit(1)
-                output_layer_weight = torch.chunk(msg.pop("weight"), args.target_tensor_parallel_size, dim=0)
+                # Deal with padding
+                orig_output_layer_weight = msg.pop("weight")
+                if md.true_vocab_size is not None:
+                    # figure out what our padded vocab size is
+                    orig_output_layer_size = orig_output_layer_weight.shape[0]
+                    margs.padded_vocab_size = _vocab_size_with_padding(md.true_vocab_size, margs)
+
+                    # Cut out extra padding we don't need
+                    if orig_output_layer_size > margs.padded_vocab_size:
+                        full_output_layer_weight = orig_output_layer_weight[0:margs.padded_vocab_size,:]
+
+                    # Expanding embedding to larger size by replicating final entry
+                    elif orig_output_layer_size < margs.padded_vocab_size:
+                        padding_size = margs.padded_vocab_size - orig_output_layer_size
+
+                        full_output_layer_weight = torch.cat((
+                            orig_output_layer_weight,
+                            orig_output_layer_weight[-1].unsqueeze(0).expand(padding_size, -1)))
+
+                    # Same size!
+                    else:
+                        full_output_layer_weight = orig_output_layer_weight
+                else:
+                    print("Original vocab size not specified, leaving embedding table as-is. "
+                          "If you've changed the tensor parallel size this could cause problems.")
+                    margs.padded_vocab_size = orig_output_layer_weight.shape[0]
+                    full_output_layer_weight = orig_output_layer_weight
+
+                output_layer_weight = torch.chunk(full_output_layer_weight, args.target_tensor_parallel_size, dim=0)
                 for tp_rank in range(args.target_tensor_parallel_size):
                     models[tp_rank].language_model.output_layer.weight.data.copy_(output_layer_weight[tp_rank])
                 del output_layer_weight
