diff --git a/megatron/core/models/common/language_module/language_module.py b/megatron/core/models/common/language_module/language_module.py
index 6000e35b..5272dbe6 100644
--- a/megatron/core/models/common/language_module/language_module.py
+++ b/megatron/core/models/common/language_module/language_module.py
@@ -171,9 +171,24 @@ class LanguageModule(MegatronModule):
             ):
                 weight = self.shared_embedding_or_output_weight()
                 weight.data = weight.data.cuda()
-                torch.distributed.all_reduce(
-                    weight.data, group=parallel_state.get_embedding_group()
-                )
+                embedding_group = parallel_state.get_embedding_group()
+                if not isinstance(embedding_group, list):
+                    torch.distributed.all_reduce(
+                        weight.data, group=parallel_state.get_embedding_group()
+                    )
+                else: # for multiple embedding groups in heterogeneous mode
+                    with torch.no_grad():
+                        original_dtype = weight.dtype
+                        if (original_dtype == torch.bfloat16) and torch.distributed.get_backend(group=embedding_group[0])=="cpu:gloo": # gloo backend doesn't support bfloat16
+                            weight = weight.to(torch.float32)
+                            weight.data = weight.data.cpu()
+                        original_weight = weight.clone().detach().data
+                        for group in embedding_group:
+                            weight.data.copy_(original_weight)
+                            torch.distributed.all_reduce(weight.data, group=group)
+                        if original_dtype != weight.dtype:
+                            weight = weight.to(original_dtype)
+                            weight.data = weight.data.cuda()
 
         elif not getattr(LanguageModule, "embedding_warning_printed", False):
             logging.getLogger(__name__).warning(
