diff --git a/megatron/core/models/common/language_module/language_module.py b/megatron/core/models/common/language_module/language_module.py
index d855322c..0d2cc80f 100644
--- a/megatron/core/models/common/language_module/language_module.py
+++ b/megatron/core/models/common/language_module/language_module.py
@@ -231,7 +231,22 @@ class LanguageModule(MegatronModule):
             if self._is_in_embd_group():
                 weight = self.shared_embedding_or_output_weight()
                 weight.data = weight.data.cuda()
-                torch.distributed.all_reduce(weight.data, group=self.embd_group)
+                embedding_group = self.embd_group
+                if not isinstance(embedding_group, list):
+                    torch.distributed.all_reduce(weight.data, group=self.embd_group)
+                else: # for multiple embedding groups in heterogeneous mode
+                    with torch.no_grad():
+                        original_dtype = weight.dtype
+                        if (original_dtype == torch.bfloat16) and torch.distributed.get_backend(group=embedding_group[0])=="cpu:gloo": # gloo backend doesn't support bfloat16
+                            weight = weight.to(torch.float32)
+                            weight.data = weight.data.cpu()
+                        original_weight = weight.clone().detach().data
+                        for group in embedding_group:
+                            weight.data.copy_(original_weight)
+                            torch.distributed.all_reduce(weight.data, group=group)
+                        if original_dtype != weight.dtype:
+                            weight = weight.to(original_dtype)
+                            weight.data = weight.data.cuda()
 
         elif not getattr(LanguageModule, "embedding_warning_printed", False):
             logging.getLogger(__name__).warning(
