diff --git a/megatron/core/pipeline_parallel/p2p_communication.py b/megatron/core/pipeline_parallel/p2p_communication.py
index 63ee9d1f5..910f8cc9a 100644
--- a/megatron/core/pipeline_parallel/p2p_communication.py
+++ b/megatron/core/pipeline_parallel/p2p_communication.py
@@ -2,6 +2,7 @@
 
 
 from typing import List, Optional, Tuple, Union
+from functools import partial
 
 import torch
 import torch.distributed as dist
@@ -9,6 +10,8 @@ import torch.distributed as dist
 from megatron.core.model_parallel_config import ModelParallelConfig
 from megatron.core.utils import nvtx_decorator
 
+from flagscale.train.hetero.p2p_communication import recv_forward_hetero, recv_backward_hetero, send_backward_hetero, send_forward_hetero, send_forward_recv_backward_hetero, send_backward_recv_forward_hetero
+
 # Types
 Shape = Union[List[int], torch.Size]
 
@@ -147,20 +150,20 @@ class P2PCommunicator:
         # Basic attrs
         self.pp_group = pp_group
         self.config = config
-
-        world_size = self.pp_group.size()
-        curr_rank_in_pg = self.pp_group.rank()
-
-        next_rank_pg = (curr_rank_in_pg + 1) % world_size
-        prev_rank_pg = (curr_rank_in_pg - 1) % world_size
-
-        self.next_rank: int | None = dist.get_global_rank(self.pp_group, next_rank_pg)
-        self.prev_rank: int | None = dist.get_global_rank(self.pp_group, prev_rank_pg)
-        self.virtual_pipeline_model_parallel_size = (
-            config.virtual_pipeline_model_parallel_size
-            if config.virtual_pipeline_model_parallel_size is not None
-            else None
-        )
+        if not isinstance(self.pp_group, list):
+            world_size = self.pp_group.size()
+            curr_rank_in_pg = self.pp_group.rank()
+
+            next_rank_pg = (curr_rank_in_pg + 1) % world_size
+            prev_rank_pg = (curr_rank_in_pg - 1) % world_size
+
+            self.next_rank: int | None = dist.get_global_rank(self.pp_group, next_rank_pg)
+            self.prev_rank: int | None = dist.get_global_rank(self.pp_group, prev_rank_pg)
+            self.virtual_pipeline_model_parallel_size = (
+                config.virtual_pipeline_model_parallel_size
+                if config.virtual_pipeline_model_parallel_size is not None
+                else None
+            )
 
     def _communicate_shapes(self, tensor_send_next, tensor_send_prev, recv_prev, recv_next):
         """Communicate tensor shapes between stages. Used to communicate
@@ -260,6 +263,7 @@ class P2PCommunicator:
         recv_next: bool,
         tensor_shape: Shape,
         wait_on_reqs: bool = True,
+        group = None, ######## FlagScale Add ########
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Communicate tensors between stages. Used as helper method in other
         communication methods that are used in megatron/schedules.py.
@@ -356,9 +360,20 @@ class P2PCommunicator:
         else:
             p2p_func = _p2p_ops
 
-        pp_group = self.pp_group
-        next_rank = self.next_rank
-        prev_rank = self.prev_rank
+        ######### FlagScale Begin #########
+        if group is not None:
+            pp_group = group
+            curr_rank_in_pg = pp_group.rank()
+            world_size = pp_group.size()
+            next_rank_pg = (curr_rank_in_pg + 1) % world_size
+            prev_rank_pg = (curr_rank_in_pg - 1) % world_size
+            next_rank: int | None = dist.get_global_rank(pp_group, next_rank_pg)
+            prev_rank: int | None = dist.get_global_rank(pp_group, prev_rank_pg)
+         ######### FlagScale End #########
+        else:
+            pp_group = self.pp_group
+            next_rank = self.next_rank
+            prev_rank = self.prev_rank
 
         if config.use_ring_exchange_p2p or config.batch_p2p_comm:
             reqs = []
@@ -404,6 +419,8 @@ class P2PCommunicator:
         self, tensor_shapes, is_first_stage: bool
     ) -> Union[torch.Tensor, list[torch.Tensor]]:
         """Receive tensor from previous rank in pipeline (forward receive)."""
+        if self.config.enable_hetero:
+            return recv_forward_hetero(tensor_shapes, is_first_stage, self.config, partial(self._communicate))
         unwrap_tensor_shapes = False
         if is_single_shape(tensor_shapes):
             unwrap_tensor_shapes = True
@@ -435,6 +452,8 @@ class P2PCommunicator:
         self, tensor_shapes, is_last_stage: bool
     ) -> Union[torch.Tensor, list[torch.Tensor]]:
         """Receive tensor from next rank in pipeline (backward receive)."""
+        if self.config.enable_hetero:
+            return recv_backward_hetero(tensor_shapes, is_last_stage, self.config, partial(self._communicate))
         unwrap_tensor_shapes = False
         if is_single_shape(tensor_shapes):
             unwrap_tensor_shapes = True
@@ -464,6 +483,8 @@ class P2PCommunicator:
     @nvtx_decorator()
     def send_forward(self, output_tensors, is_last_stage: bool) -> None:
         """Send tensor to next rank in pipeline (forward send)."""
+        if self.config.enable_hetero:
+            return send_forward_hetero(output_tensors, is_last_stage, self.config, partial(self._communicate))
         config = self.config
         if not isinstance(output_tensors, list):
             output_tensors = [output_tensors]
@@ -485,6 +506,8 @@ class P2PCommunicator:
     @nvtx_decorator()
     def send_backward(self, input_tensor_grads, is_first_stage: bool) -> None:
         """Send tensor to previous rank in pipeline (backward send)."""
+        if self.config.enable_hetero:
+            return send_backward_hetero(input_tensor_grads, is_first_stage, self.config, partial(self._communicate))
         if not isinstance(input_tensor_grads, list):
             input_tensor_grads = [input_tensor_grads]
         config = self.config
@@ -507,6 +530,8 @@ class P2PCommunicator:
         self, output_tensors, tensor_shapes, is_last_stage: bool
     ) -> Union[torch.Tensor, list[torch.Tensor]]:
         """Batched send and recv with next rank in pipeline."""
+        if self.config.enable_hetero:
+            return send_forward_recv_backward_hetero(output_tensors, tensor_shapes, is_last_stage, self.config, partial(self._communicate))
         config = self.config
         unwrap_output_tensors = False
         if not isinstance(output_tensors, list):
@@ -540,6 +565,8 @@ class P2PCommunicator:
         self, input_tensor_grads, tensor_shapes, is_first_stage: bool
     ) -> Union[torch.Tensor, list[torch.Tensor]]:
         """Batched send and recv with previous rank in pipeline."""
+        if self.config.enable_hetero:
+            return send_backward_recv_forward_hetero(input_tensor_grads, tensor_shapes, is_first_stage, self.config, partial(self._communicate))
         config = self.config
         unwrap_input_tensor_grads = False
         if not isinstance(input_tensor_grads, list):
@@ -643,3 +670,122 @@ class P2PCommunicator:
         if config.timers is not None:
             config.timers('forward-backward-send-forward-backward-recv').stop()
         return input_tensor, output_tensor_grad
+
+    ########## FlagScale Begin ##########
+    def warm_up_comm_group(self):
+        """Warm up the communication group by performing a dummy send and recv."""
+        if self.config.enable_hetero:
+            self.warm_up_comm_group_hetero()
+            return
+        # NOTE(lizhiyu): For enbale config.variable_seq_lengths and pp_size > 2
+        if not self.config.variable_seq_lengths or self.pp_group().size() <= 2:
+            return
+        self.config.variable_seq_lengths=False
+        rank = torch.distributed.get_rank()
+        # This is arbitrary because the shape of the recv tensor needs
+        # to be specified when communicating.
+        # It can be changed into any other shape.
+        tensor_shape = [1]
+        to_send_tensor= torch.empty(
+                tensor_shape,
+                requires_grad=True,
+                device=torch.cuda.current_device(),
+                dtype=self.config.pipeline_dtype,
+            )
+        to_recv_tensor= torch.empty(
+                tensor_shape,
+                requires_grad=True,
+                device=torch.cuda.current_device(),
+                dtype=self.config.pipeline_dtype,
+            )
+
+        group_ranks = torch.distributed.get_process_group_ranks(self.pp_group)
+        pipeline_rank = self.pp_group.rank()
+        if pipeline_rank == 0:
+            self._communicate(
+                tensor_send_next=to_send_tensor,
+                tensor_send_prev=None,
+                recv_prev=False,
+                recv_next=False,
+                tensor_shape=to_recv_tensor.shape,
+                group=self.pp_group,
+            )
+        elif pipeline_rank == len(group_ranks) - 1:
+            self._communicate(
+                tensor_send_next=None,
+                tensor_send_prev=None,
+                recv_prev=True,
+                recv_next=False,
+                tensor_shape=to_recv_tensor.shape,
+                group=self.pp_group,
+            )
+        elif rank in group_ranks:
+            self._communicate(
+                tensor_send_next=to_send_tensor,
+                tensor_send_prev=None,
+                recv_prev=True,
+                recv_next=False,
+                tensor_shape=to_recv_tensor.shape,
+                group=self.pp_group,
+            )
+        self.config.variable_seq_lengths = True
+
+
+    def warm_up_comm_group_hetero(self):
+        """ Warm up the communication for all PP groups, to avoid the hang issue.
+
+        P2P comm would call batch_isend_irecv API, which requires
+        all ranks of the group to participate if this API is the
+        first collective call in the group passed to `dist.P2POp`.
+
+        See batch_isend_irecv for more details.
+        """
+        rank = torch.distributed.get_rank()
+        # This is arbitrary because the shape of the recv tensor needs
+        # to be specified when communicating.
+        # It can be changed into any other shape.
+        tensor_shape = [1]
+        to_send_tensor= torch.empty(
+                tensor_shape,
+                requires_grad=True,
+                device=torch.cuda.current_device() if "cpu:gloo" != self.pp_group[0].name() else torch.device("cpu"),
+                dtype=self.config.pipeline_dtype,
+            )
+        to_recv_tensor= torch.empty(
+                tensor_shape,
+                requires_grad=True,
+                device=torch.cuda.current_device() if "cpu:gloo" !=  self.pp_group[0].name() else torch.device("cpu") ,
+                dtype=self.config.pipeline_dtype,
+            )
+
+        for pp_g in self.pp_group:
+            group_ranks = torch.distributed.get_process_group_ranks(pp_g)
+            pipeline_rank = pp_g.rank()
+            if pipeline_rank == 0:
+                self._communicate(
+                    tensor_send_next=to_send_tensor,
+                    tensor_send_prev=None,
+                    recv_prev=False,
+                    recv_next=False,
+                    tensor_shape=to_recv_tensor.shape,
+                    group=pp_g,
+                )
+            elif pipeline_rank == len(group_ranks) - 1:
+                self._communicate(
+                    tensor_send_next=None,
+                    tensor_send_prev=None,
+                    recv_prev=True,
+                    recv_next=False,
+                    tensor_shape=to_recv_tensor.shape,
+                    group=pp_g,
+                )
+            elif rank in group_ranks:
+                self._communicate(
+                    tensor_send_next=to_send_tensor,
+                    tensor_send_prev=None,
+                    recv_prev=True,
+                    recv_next=False,
+                    tensor_shape=to_recv_tensor.shape,
+                    group=pp_g,
+                )
+    ########## FlagScale Begin ##########
\ No newline at end of file
