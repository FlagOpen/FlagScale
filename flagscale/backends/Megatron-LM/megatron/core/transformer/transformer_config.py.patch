diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index fe5f7a72..34ef6636 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -283,6 +283,15 @@ class TransformerConfig(ModelParallelConfig):
     the number of transformer layers to recompute within each pipeline stage.  Must be None for
     'selective' activation checkpointing."""
 
+    recompute_granularity_per_stage_micro_batch: list = None
+    """Same as recompute_granularity but for each stage and each micro-batch."""
+
+    recompute_method_per_stage_micro_batch: list = None
+    """Same as recompute_method but for each stage and each micro-batch."""
+
+    recompute_num_layers_per_stage_micro_batch: list = None
+    """Same as recompute_num_layers but for each stage and each micro-batch."""
+
     distribute_saved_activations: Optional[bool] = None
     """If True, distribute recomputed activations across the model parallel group."""
 
@@ -366,6 +375,12 @@ class TransformerConfig(ModelParallelConfig):
     use_kitchen: bool = False
     """Use the kitchen extension for transformer quantization."""
 
+    ####################
+    # DualPipeV related
+    ####################
+    use_dualpipev: bool = False
+    moe_fb_overlap: bool = False
+
     ####################
     # MoE related
     ####################
@@ -577,6 +592,12 @@ class TransformerConfig(ModelParallelConfig):
     config_logger_dir: str = ""
     """When non-empty, dumps entry-point configs to config_logger_dir"""
 
+    qk_layernorm_hidden_dim: bool = False
+    """Whether to apply LayerNorm to the query and key embeddings on the hidden dimension rather than head dimension."""
+
+    use_partial_reduce_for_shared_embedding: bool = False
+    """Whether to use partional reduce for shared embedding."""
+
     flash_decode: bool = False
     """ Use the optimized flash decoding kernel during inference. """
 
@@ -1250,6 +1271,9 @@ class TransformerConfig(ModelParallelConfig):
                     f"the number of layers ({self.num_layers})"
                 )
 
+        if self.moe_fb_overlap:
+            self.delay_wgrad_compute = True
+
 
 @dataclass
 class MLATransformerConfig(TransformerConfig):
