diff --git a/megatron/core/transformer/dot_product_attention.py b/megatron/core/transformer/dot_product_attention.py
index 2a958722e..280f90e4b 100644
--- a/megatron/core/transformer/dot_product_attention.py
+++ b/megatron/core/transformer/dot_product_attention.py
@@ -180,6 +180,8 @@ class DotProductAttention(MegatronModule):
             (output_size[0] * output_size[1], output_size[2], output_size[3]), query.dtype, "mpu"
         )
 
+        
+        '''
         # Raw attention scores. [b * np, sq, sk]
         matmul_result = torch.baddbmm(
             matmul_input_buffer,
@@ -188,6 +190,13 @@ class DotProductAttention(MegatronModule):
             beta=0.0,
             alpha=self.softmax_scale,
         )
+        '''
+        # # replaced for gems
+        matmul_result = torch.bmm(
+            query.transpose(0, 1),                       # [b*np, sq, hn]
+            key.transpose(0, 1).transpose(1, 2)          # [b*np, hn, sk]
+        ) * self.softmax_scale                           # 手动乘 alpha
+
 
         # change view to [b, np, sq, sk]
         attention_scores = matmul_result.view(*output_size)
