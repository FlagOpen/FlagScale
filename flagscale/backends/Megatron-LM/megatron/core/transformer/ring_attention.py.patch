diff --git a/megatron/core/transformer/ring_attention.py b/megatron/core/transformer/ring_attention.py
new file mode 100644
index 000000000..f85bd9115
--- /dev/null
+++ b/megatron/core/transformer/ring_attention.py
@@ -0,0 +1,309 @@
+# Copyright (c) 2025, BAAI. All rights reserved.
+# 
+# Adopted from <https://github.com/malaysia-ai/context-parallelism>. Below is the original copyright:
+# https://github.com/malaysia-ai/context-parallelism/blob/master/context_parallelism/ring_flex.py
+
+import torch
+import torch.distributed as dist
+from megatron.core.transformer.ring_utils import RingComm, merge_attention
+from torch.nn.attention.flex_attention import (
+    flex_attention,
+    create_block_mask,
+    _identity, 
+    _apply_kernel_options,
+)
+from torch._higher_order_ops.flex_attention import (
+    sdpa_dense_backward, 
+    create_fw_bw_graph,
+)
+import math
+
+compiled_flex_attention = torch.compile(
+    flex_attention, dynamic=False, mode="max-autotune-no-cudagraphs"
+)
+
+
+def generate_mask_mod(attn_mask):
+    def mask_mod(b, h, q_idx, kv_idx):
+        return attn_mask[b][0][q_idx][kv_idx] == False
+    return mask_mod
+
+
+def _flex_forward(
+    q: torch.Tensor,
+    k: torch.Tensor,
+    v: torch.Tensor,
+    attn_mask: torch.Tensor,
+    scale: float,
+    rank: int,
+    iter: int,
+    world_size: int,
+    causal: bool = True,
+):
+    # create block mask
+    seq_chunk_id = (rank - iter) % world_size
+    seq_chunk_size = attn_mask.shape[-1] // world_size
+    start_seq_id = seq_chunk_id * seq_chunk_size
+    end_seq_id = (seq_chunk_id + 1) * seq_chunk_size
+    attn_mask_this_rank = attn_mask[:, :, :, start_seq_id:end_seq_id]
+    
+    mask_mod = generate_mask_mod(attn_mask=attn_mask_this_rank)
+    block_mask = create_block_mask(
+        mask_mod,
+        B=q.shape[0],
+        H=None,
+        Q_LEN=q.shape[2],
+        KV_LEN=k.shape[2],
+        device=q.device,
+    )
+
+    block_out, block_lse = compiled_flex_attention(q, k, v, block_mask=block_mask, scale=scale, enable_gqa=True, return_lse=True)
+    return block_out, block_lse    
+
+
+def _flex_backward(
+    dout: torch.Tensor,
+    dlse: torch.Tensor,
+    q: torch.Tensor,
+    k: torch.Tensor,
+    v: torch.Tensor,
+    attn_mask: torch.Tensor,
+    out: torch.Tensor,
+    lse: torch.Tensor,
+    scale: float,
+    rank: int,
+    iter: int,
+    world_size: int,
+    causal: bool = True,
+):
+    # create block mask
+    seq_chunk_id = (rank - iter) % world_size 
+    seq_chunk_size = attn_mask.shape[-1] // world_size
+    start_seq_id = seq_chunk_id * seq_chunk_size
+    end_seq_id = (seq_chunk_id + 1) * seq_chunk_size
+    attn_mask_this_rank = attn_mask[:, :, :, start_seq_id:end_seq_id]
+    
+    kernel_options = _apply_kernel_options(
+        q,
+        k,
+        v,
+        True,
+        None,
+    )
+
+    mask_mod = generate_mask_mod(attn_mask_this_rank)
+    block_mask = create_block_mask(
+        mask_mod,
+        B=q.shape[0],
+        H=None,
+        Q_LEN=q.shape[2],
+        KV_LEN=k.shape[2],
+        device=q.device,
+    )
+
+    block_mask = block_mask.as_tuple()
+    example_vals = (
+        q.new_zeros((), requires_grad=True),
+        q.new_zeros((), dtype=torch.int),
+        q.new_zeros((), dtype=torch.int),
+        q.new_zeros((), dtype=torch.int),
+        q.new_zeros((), dtype=torch.int),
+    )
+    fw_graph, bw_graph = create_fw_bw_graph(
+        _identity, example_vals, (),
+    )
+
+    # exp(score - lse)   â†’   exp2(score - lse2)
+    o = sdpa_dense_backward(
+        q,
+        k,
+        v,
+        out,
+        lse / math.log(2),
+        dout,
+        dlse,
+        fw_graph,
+        bw_graph,
+        block_mask, 
+        scale, 
+        kernel_options,
+        score_mod_other_buffers = (),
+        mask_mod_other_buffers = (),
+    )
+    return o[:-1][0], o[:-1][1], o[:-1][2], 
+
+
+def _forward(
+    process_group,
+    q: torch.Tensor,
+    k: torch.Tensor,
+    v: torch.Tensor,
+    attn_mask: torch.Tensor,
+    scale: float,
+    causal: bool = True,
+):
+    comm = RingComm(process_group)
+
+    out = None
+    lse = None
+    next_k, next_v = None, None
+
+    for step in range(comm.world_size):
+        if step + 1 != comm.world_size:
+            next_k, next_v = comm.send_recv_kv(k, v)
+
+        if not causal or step <= comm.rank:
+
+            with torch.no_grad():
+                attn_fwd_fn = _flex_forward
+                args = [
+                    q,
+                    k,
+                    v,
+                    attn_mask,
+                    scale,
+                    comm.rank,
+                    step,
+                    comm.world_size,
+                ]
+                block_out, block_lse = attn_fwd_fn(*args)
+                out, lse = merge_attention(out, lse, block_out, block_lse)
+                
+        if step + 1 != comm.world_size:
+            comm.wait()
+            k, v = next_k, next_v
+
+    out = out.to(q.dtype)
+    return out, lse
+
+
+def _backward(
+    process_group,
+    dout: torch.Tensor,
+    dlse: torch.Tensor,
+    q: torch.Tensor,
+    k: torch.Tensor,
+    v: torch.Tensor,
+    attn_mask: torch.Tensor,
+    out: torch.Tensor,
+    lse: torch.Tensor,
+    scale: float,
+    causal: bool = True,
+):
+    kv_comm = RingComm(process_group)
+    d_kv_comm = RingComm(process_group)
+    dq, dk, dv = None, None, None
+
+    next_dk, next_dv = None, None
+    next_k, next_v = None, None
+
+    for step in range(kv_comm.world_size):
+        if step + 1 != kv_comm.world_size:
+            next_k, next_v = kv_comm.send_recv_kv(k, v)
+
+        if not causal or step <= kv_comm.rank:
+            
+            attn_bwd_fn = _flex_backward
+            args = [
+                dout,
+                dlse,
+                q,
+                k,
+                v,
+                attn_mask,
+                out,
+                lse,
+                scale,
+                kv_comm.rank,
+                step,
+                kv_comm.world_size,
+                causal,
+            ]
+
+
+            (
+                block_dq_buffer,
+                block_dk_buffer,
+                block_dv_buffer,
+            ) = attn_bwd_fn(*args)
+            
+            if dq is None:
+                dq = block_dq_buffer.to(torch.float32)
+                dk = block_dk_buffer.to(torch.float32)
+                dv = block_dv_buffer.to(torch.float32)
+            else:
+                dq += block_dq_buffer
+                d_kv_comm.wait()
+                dk = block_dk_buffer + next_dk
+                dv = block_dv_buffer + next_dv
+        
+        elif step != 0:
+            d_kv_comm.wait()
+            dk, dv = next_dk, next_dv
+
+        if step + 1 != kv_comm.world_size:
+            kv_comm.wait()
+            k, v = next_k, next_v
+        
+        next_dk, next_dv = d_kv_comm.send_recv_kv(dk, dv)
+    
+    d_kv_comm.wait()
+    
+    return dq.to(q.dtype), next_dk.to(q.dtype), next_dv.to(q.dtype)
+
+
+class RingAttnFunc(torch.autograd.Function):
+    @staticmethod
+    def forward(
+        ctx,
+        q,
+        k,
+        v,
+        attn_mask,
+        scale,
+        causal,
+        group,
+    ):
+        if scale is None:
+            scale = q.shape[-1] ** (-0.5)
+        
+        q = q.contiguous()
+        k = k.contiguous()
+        v = v.contiguous()
+        attn_mask = attn_mask.contiguous()
+        out, lse = _forward(group, q, k, v, attn_mask, scale=scale, causal=causal)
+        ctx.save_for_backward(q, k, v, attn_mask, out, lse)
+        ctx.scale = scale
+        ctx.causal = causal
+        ctx.group = group
+        return out, lse
+
+    @staticmethod
+    def backward(ctx, dout, dlse, *args):
+        q, k, v, attn_mask, out, lse = ctx.saved_tensors
+        dq, dk, dv = _backward(
+            ctx.group,
+            dout,
+            dlse,
+            q,
+            k,
+            v,
+            attn_mask,
+            out,
+            lse,
+            scale=ctx.scale,
+            causal=ctx.causal,
+        )
+        return dq, dk, dv, None, None, None, None
+
+
+def ring_attn(
+    q,
+    k,
+    v,
+    attn_mask,
+    scale=None,
+    causal=False,
+    group=None,
+):      
+    return RingAttnFunc.apply(q, k, v, attn_mask, scale, causal, group)
