diff --git a/megatron/core/transformer/moe/moe_utils.py b/megatron/core/transformer/moe/moe_utils.py
index c049ed82d..b2ecc767d 100644
--- a/megatron/core/transformer/moe/moe_utils.py
+++ b/megatron/core/transformer/moe/moe_utils.py
@@ -744,6 +744,7 @@ def reduce_aux_losses_tracker_across_ranks(track_names: Optional[List[str]] = No
             torch.distributed.all_reduce(
                 values, group=tracker[name]['avg_group'], op=torch.distributed.ReduceOp.AVG
             )
+        
         # This ensures proper loss averaging across all ranks including CP ranks
         torch.distributed.all_reduce(
             values,
@@ -752,6 +753,72 @@ def reduce_aux_losses_tracker_across_ranks(track_names: Optional[List[str]] = No
         )
 
 
+def reduce_aux_losses_tracker_across_ranks_hetero(
+    track_names: Optional[List[str]] = None,
+):
+    """Collect and reduce the auxiliary losses across ranks."""
+    tracker = parallel_state.get_moe_layer_wise_logging_tracker()
+    if track_names is None:
+        track_names = tracker.keys()
+    for name in track_names:
+        values = tracker[name]["values"]
+        # Reduce aux losses across ranks.
+        if tracker[name].get("reduce_group") is not None:
+            torch.distributed.all_reduce(
+                values, group=tracker[name].get("reduce_group")
+            )
+        if tracker[name].get("avg_group") is not None:
+            torch.distributed.all_reduce(
+                values,
+                group=tracker[name]["avg_group"],
+                op=torch.distributed.ReduceOp.AVG,
+            )
+        pp_groups = parallel_state.get_pipeline_model_parallel_group()
+        if "cpu:gloo" == torch.distributed.get_backend(pp_groups[0]):
+            values = values.cpu()
+        assert isinstance(pp_groups, list), "pp_groups should be a list for hetero."
+        if len(pp_groups) > 1:
+            origin_values = values.clone().detach()
+            for pp_group in pp_groups:
+                values.copy_(origin_values)
+                torch.distributed.all_reduce(values, group=pp_group)
+        else:
+            torch.distributed.all_reduce(values, group=pp_groups[0])
+
+
+def reduce_aux_losses_tracker_across_ranks_hetero(
+    track_names: Optional[List[str]] = None,
+):
+    """Collect and reduce the auxiliary losses across ranks."""
+    tracker = parallel_state.get_moe_layer_wise_logging_tracker()
+    if track_names is None:
+        track_names = tracker.keys()
+    for name in track_names:
+        values = tracker[name]["values"]
+        # Reduce aux losses across ranks.
+        if tracker[name].get("reduce_group") is not None:
+            torch.distributed.all_reduce(
+                values, group=tracker[name].get("reduce_group")
+            )
+        if tracker[name].get("avg_group") is not None:
+            torch.distributed.all_reduce(
+                values,
+                group=tracker[name]["avg_group"],
+                op=torch.distributed.ReduceOp.AVG,
+            )
+        pp_groups = parallel_state.get_pipeline_model_parallel_group()
+        if "cpu:gloo" == torch.distributed.get_backend(pp_groups[0]):
+            values = values.cpu()
+        assert isinstance(pp_groups, list), "pp_groups should be a list for hetero."
+        if len(pp_groups) > 1:
+            origin_values = values.clone().detach()
+            for pp_group in pp_groups:
+                values.copy_(origin_values)
+                torch.distributed.all_reduce(values, group=pp_group)
+        else:
+            torch.distributed.all_reduce(values, group=pp_groups[0])
+
+
 def track_moe_metrics(
     loss_scale: float,
     iteration: int,
@@ -764,6 +831,7 @@ def track_moe_metrics(
     num_layers: Optional[int] = None,
     moe_layer_freq: Optional[Union[int, List[int]]] = None,
     mtp_num_layers: Optional[int] = None,
+    enable_hetero=False,
 ):
     """Track the MoE metrics for logging."""
     # Aux loss logging
@@ -777,7 +845,10 @@ def track_moe_metrics(
                     tracker[key]["values"] = torch.zeros(num_layers, device="cuda")
                     tracker[key]["reduce_group"] = None
                     tracker[key]["avg_group"] = None
-    reduce_aux_losses_tracker_across_ranks(track_names)
+    if not enable_hetero:
+        reduce_aux_losses_tracker_across_ranks(track_names)
+    else:
+        reduce_aux_losses_tracker_across_ranks_hetero(track_names)
 
     # Get number of MoE layers
     if moe_layer_freq is None:
