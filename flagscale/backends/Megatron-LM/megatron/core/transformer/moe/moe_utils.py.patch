diff --git a/megatron/core/transformer/moe/moe_utils.py b/megatron/core/transformer/moe/moe_utils.py
index b67324919..dfc06ca29 100644
--- a/megatron/core/transformer/moe/moe_utils.py
+++ b/megatron/core/transformer/moe/moe_utils.py
@@ -700,6 +700,39 @@ def reduce_aux_losses_tracker_across_ranks(track_names: Optional[List[str]] = No
             )
 
 
+def reduce_aux_losses_tracker_across_ranks_hetero(
+    track_names: Optional[List[str]] = None,
+):
+    """Collect and reduce the auxiliary losses across ranks."""
+    tracker = parallel_state.get_moe_layer_wise_logging_tracker()
+    if track_names is None:
+        track_names = tracker.keys()
+    for name in track_names:
+        values = tracker[name]["values"]
+        # Reduce aux losses across ranks.
+        if tracker[name].get("reduce_group") is not None:
+            torch.distributed.all_reduce(
+                values, group=tracker[name].get("reduce_group")
+            )
+        if tracker[name].get("avg_group") is not None:
+            torch.distributed.all_reduce(
+                values,
+                group=tracker[name]["avg_group"],
+                op=torch.distributed.ReduceOp.AVG,
+            )
+        pp_groups = parallel_state.get_pipeline_model_parallel_group()
+        if "cpu:gloo" == torch.distributed.get_backend(pp_groups[0]):
+            values = values.cpu()
+        assert isinstance(pp_groups, list), "pp_groups should be a list for hetero."
+        if len(pp_groups) > 1:
+            origin_values = values.clone().detach()
+            for pp_group in pp_groups:
+                values.copy_(origin_values)
+                torch.distributed.all_reduce(values, group=pp_group)
+        else:
+            torch.distributed.all_reduce(values, group=pp_groups[0])
+
+
 def track_moe_metrics(
     loss_scale: float,
     iteration: int,
@@ -711,6 +744,7 @@ def track_moe_metrics(
     track_names: Optional[List[str]] = None,
     num_layers: Optional[int] = None,
     moe_layer_freq: Optional[Union[int, List[int]]] = None,
+    enable_hetero=False,
 ):
     """Track the MoE metrics for logging."""
     # Aux loss logging
@@ -724,7 +758,10 @@ def track_moe_metrics(
                     tracker[key]["values"] = torch.zeros(num_layers, device="cuda")
                     tracker[key]["reduce_group"] = None
                     tracker[key]["avg_group"] = None
-    reduce_aux_losses_tracker_across_ranks(track_names)
+    if not enable_hetero:
+        reduce_aux_losses_tracker_across_ranks(track_names)
+    else:
+        reduce_aux_losses_tracker_across_ranks_hetero(track_names)
 
     # Get number of MoE layers
     if moe_layer_freq is None:
