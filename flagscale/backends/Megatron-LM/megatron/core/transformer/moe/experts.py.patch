diff --git a/megatron/core/transformer/moe/experts.py b/megatron/core/transformer/moe/experts.py
index fc741aa4..0a86aa60 100644
--- a/megatron/core/transformer/moe/experts.py
+++ b/megatron/core/transformer/moe/experts.py
@@ -232,6 +232,12 @@ class GroupedMLP(MegatronModule):
         setattr(self.weight1, 'allreduce', not self.expert_parallel)
         setattr(self.weight2, 'allreduce', not self.expert_parallel)
 
+        # NOTE(lizhiyu): The following code is for hetro-expert training when one of the expert parallel degree is 1.
+        #                 But there are other codes need to be modified to make it work.
+        # if config.enable_hetero:
+        #     setattr(self.weight1, 'allreduce', False)
+        #     setattr(self.weight2, 'allreduce', False)
+
         def remove_extra_states_check(self, incompatible_keys):
             """
             Remove _extra_state from unexpected keys.
