diff --git a/megatron/core/transformer/transformer_block.py b/megatron/core/transformer/transformer_block.py
old mode 100755
new mode 100644
index af0b3234..64fd6d60
--- a/megatron/core/transformer/transformer_block.py
+++ b/megatron/core/transformer/transformer_block.py
@@ -1,5 +1,5 @@
 # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
-
+import os
 from contextlib import nullcontext
 from dataclasses import dataclass
 from typing import List, Optional, Union
@@ -51,12 +51,13 @@ except ImportError:
         LayerNormImpl = WrappedTorchNorm
 
 
-def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] = None) -> int:
+def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] = None, is_dualpipev_first_chunk: Optional[bool] = False) -> int:
     """
     Determine the number of transformer layers to build for the current pipeline stage.
     Args:
         config (TransformerConfig): Configuration object containing transformer model parameters.
         vp_stage (Optional[int]): Virtual pipeline stage number.
+        is_dualpipev_first_chunk(Optional[bool]): Is dualpipev first model chunk or not
 
     Returns:
         int: The number of layers to be built for the current pipeline stage.
@@ -117,7 +118,7 @@ def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] =
             and config.num_layers_in_last_pipeline_stage is not None
         ):
             num_layers_per_pipeline_rank = config.num_layers_in_last_pipeline_stage
-    else:
+    elif not config.enable_hetero:
         # Include the embedding layer and loss layer into pipeline parallelism partition
         num_layers = config.num_layers
         if config.account_for_embedding_in_pipeline_split:
@@ -134,7 +135,7 @@ def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] =
     if (
         parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None
         and config.pipeline_model_parallel_size > 1
-    ):
+    ) and parallel_state.get_virtual_pipeline_model_parallel_world_size() > 1:
         # Interleaved pipeline parallelism:
         # Number of layers in each model chunk is the number of layers in the stage,
         # divided by the number of model chunks in a stage.
@@ -155,11 +156,26 @@ def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] =
         num_layers_per_virtual_stage = num_layers_per_pipeline_rank // vp_size
 
         num_layers_to_build = num_layers_per_virtual_stage
-
+    ######### FlagScale Begin ########
+    elif config.use_dualpipev:
+        num_layers_per_pipeline_rank_first_chunk = num_layers_per_pipeline_rank // 2
+        if num_layers_per_pipeline_rank % 2 != 0:
+            num_layers_per_pipeline_rank_first_chunk = num_layers_per_pipeline_rank_first_chunk + 1
+        num_layers_per_pipeline_rank_second_chunk = num_layers_per_pipeline_rank - num_layers_per_pipeline_rank_first_chunk
+        if is_dualpipev_first_chunk:
+            num_layers_to_build = num_layers_per_pipeline_rank_first_chunk
+        else:
+            num_layers_to_build = num_layers_per_pipeline_rank_second_chunk
+    ######### FlagScale End ########
     else:
         # Non-interleaved pipeline parallelism:
         # Each stage gets a contiguous set of layers.
-        num_layers_to_build = num_layers_per_pipeline_rank
+
+        if config.enable_hetero:
+            pipeline_rank = parallel_state.get_pipeline_model_parallel_rank()
+            num_layers_to_build = config.hetero_pipeline_layer_split[pipeline_rank]
+        else:
+            num_layers_to_build = num_layers_per_pipeline_rank
 
     # The embedding (or loss) layer cannot function as a standalone transformer layer
     # Reduce the number of layers to construct by 1 on the first (or last) stage if the
@@ -505,6 +521,14 @@ class TransformerBlock(MegatronModule):
             [s, b, h], and optionally the updated context tensor if cross-attention is used.
         """
 
+        ########## FlagScale Begin ##########
+        # for refined recompute
+        self.current_microbatch = -1
+        if len(self.layers) > 0: # some pp-stage has no layers in pipeline_model_parallel_layout,such as embedding stage
+            if hasattr(self.layers[0], 'current_microbatch'):
+                self.current_microbatch = self.layers[0].current_microbatch
+        ########## FlagScale End ##########
+
         inference_context = deprecate_inference_params(inference_context, inference_params)
 
         # Delete the obsolete reference to the initial input tensor if necessary
@@ -546,6 +570,71 @@ class TransformerBlock(MegatronModule):
         use_inner_fp8_context = self.config.fp8 and self.config.fp8_recipe != Fp8Recipe.delayed
         outer_fp8_context = get_fp8_context(self.config) if use_outer_fp8_context else nullcontext()
 
+        if self.config.recompute_method_per_stage_micro_batch != None:
+            if self.config.virtual_pipeline_model_parallel_size != None:
+                if (
+                    self.config.recompute_method_per_stage_micro_batch[
+                        parallel_state.get_virtual_pipeline_model_parallel_rank()
+                        * self.config.pipeline_model_parallel_size
+                        + parallel_state.get_pipeline_model_parallel_rank()
+                    ][self.current_microbatch]
+                    == 0
+                ):
+                    self.config.recompute_method = 'uniform'
+                elif (
+                    self.config.recompute_method_per_stage_micor_batch[
+                        parallel_state.get_virtual_pipeline_model_parallel_rank()
+                        * self.config.pipeline_model_parallel_size
+                        + parallel_state.get_pipeline_model_parallel_rank()
+                    ][self.current_microbatch]
+                    == 1
+                ):
+                    self.config.recompute_method = 'block'
+                else:
+                    raise ValueError("the item of recompute_method_per_stage_micor_batch must be '0' or '1' ")
+            else:
+                if (
+                    self.config.recompute_method_per_stage_micro_batch[
+                        parallel_state.get_pipeline_model_parallel_rank()
+                    ][self.current_microbatch]
+                    == 0
+                ):
+                    self.config.recompute_method = 'uniform'
+                elif (
+                    self.config.recompute_method_per_stage_micro_batch[
+                        parallel_state.get_pipeline_model_parallel_rank()
+                    ][self.current_microbatch]
+                    == 1
+                ):
+                    self.config.recompute_method = 'block'
+                else:
+                    raise ValueError("the item of recompute_method_per_stage_micor_batch must be '0' or '1' ")
+
+        if self.config.recompute_num_layers_per_stage_micro_batch != None:
+            if self.config.virtual_pipeline_model_parallel_size != None:
+                self.config.recompute_num_layers = self.config.recompute_num_layers_per_stage_micro_batch[
+                    parallel_state.get_virtual_pipeline_model_parallel_rank()
+                    * self.config.pipeline_model_parallel_size
+                    + parallel_state.get_pipeline_model_parallel_rank()
+                ][self.current_microbatch]
+            else:
+                self.config.recompute_num_layers = self.config.recompute_num_layers_per_stage_micro_batch[
+                    parallel_state.get_pipeline_model_parallel_rank()
+                ][self.current_microbatch]
+            if self.config.recompute_num_layers == 0:
+                self.config.recompute_method = None
+                self.config.recompute_granularity = None
+
+        if (
+            self.config.recompute_granularity_per_stage_micro_batch != None
+            and self.config.recompute_granularity_per_stage_micro_batch[
+                parallel_state.get_pipeline_model_parallel_rank()
+            ][self.current_microbatch]
+            == 0
+        ):
+            self.config.recompute_granularity = None
+            self.config.recompute_method = None
+
         with rng_context, outer_fp8_context:
             # Forward pass.
             if self.config.recompute_granularity == 'full' and self.training:
@@ -634,6 +723,16 @@ class TransformerBlock(MegatronModule):
         elif isinstance(self.config.moe_layer_freq, list):
             non_homogeneous_layers = True
 
+
+        # TODO: @aoyulong - This is a temporary solution to support single-file-per-tensor ckpt
+        non_homogeneous_layers_env = os.getenv('FS_NON_HOMOGENEOUS_LAYERS', 'False').lower() in (
+            'true',
+            '1',
+            't',
+        )
+        if non_homogeneous_layers_env:
+            non_homogeneous_layers = True
+
         if self.config.heterogeneous_block_specs:
             non_homogeneous_layers = True
 
