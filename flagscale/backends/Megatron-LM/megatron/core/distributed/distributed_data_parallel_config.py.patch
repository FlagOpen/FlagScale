diff --git a/megatron/core/distributed/distributed_data_parallel_config.py b/megatron/core/distributed/distributed_data_parallel_config.py
index 6c4cab7d..ebb19fe5 100644
--- a/megatron/core/distributed/distributed_data_parallel_config.py
+++ b/megatron/core/distributed/distributed_data_parallel_config.py
@@ -134,6 +134,9 @@ class DistributedDataParallelConfig:
 
     delay_wgrad_compute: bool = False
     """Delay the weight gradient computation to improve batch-level communication overlapping"""
+    
+    use_partial_reduce_for_shared_embedding: bool = False
+    """Whether to use partional reduce for shared embedding."""
 
     def __post_init__(self):
         import os
