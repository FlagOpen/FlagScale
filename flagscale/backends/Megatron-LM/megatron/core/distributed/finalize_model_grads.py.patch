diff --git a/megatron/core/distributed/finalize_model_grads.py b/megatron/core/distributed/finalize_model_grads.py
index ef1f0a149..2c22062b8 100644
--- a/megatron/core/distributed/finalize_model_grads.py
+++ b/megatron/core/distributed/finalize_model_grads.py
@@ -36,6 +36,18 @@ def _get_main_grad_attr(param: torch.nn.Parameter):
         return "main_grad"
     return "grad"
 
+def get_device_type_for_comm(model_parallel_group=None):
+    ''''Copy from flagscale/train/hetero/p2p_communication.py'''
+    device = 'cuda'
+    # "cpu:gloo": gloo only supports cpu tensor.
+    # "gloo" & "cpu:gloo,cuda:gloo": gloo supports both cpu and cuda tensor.
+    if isinstance(model_parallel_group, list):
+        if 'cpu:gloo' == torch.distributed.get_backend(model_parallel_group[0]):
+            device = 'cpu'
+    else:
+        if 'cpu:gloo' == torch.distributed.get_backend(model_parallel_group):
+            device = 'cpu'
+    return device
 
 def _unshard_if_dtensor(tensor: Union[torch.Tensor, "DTensor"]) -> torch.Tensor:
     """
@@ -101,6 +113,11 @@ def _allreduce_conditional_embedding_grads(
     if pp_group is None:
         pp_group = parallel_state.get_pipeline_model_parallel_group()
 
+    ######### FlagScale Begin #########
+    assert not(isinstance(pp_group, list) and getattr(config, "has_cond_embedder", False)), f"FlagScale does not support both pp_group is a list and has_cond_embedder is True."
+    if isinstance(pp_group, list):
+        return
+    ######### FlagScale End #########
     if pp_group.size() > 1 and getattr(config, "has_cond_embedder", False):
         grads_dict = {}
         for model_chunk in model:
@@ -217,8 +234,9 @@ def _allreduce_embedding_grad(
         skip_if_none (bool, optional): If True, quietly returns when the parameter or its
             gradient is ``None``. Defaults to True.
     """
-
+    embd_group_is_list = isinstance(embd_group, list)
     if (
+        not embd_group_is_list and
         # embd_group can be None in cases there is no embd_group
         # get_pg_size(embd_group) will return 1 and the all-reduce will be skipped.
         get_pg_size(embd_group) > 1
@@ -249,6 +267,67 @@ def _allreduce_embedding_grad(
             return
         torch.distributed.all_reduce(grad, group=embd_group)
         setattr(weight, grad_attr, _reshard_if_dtensor(grad, orig_grad))
+        
+    ######## FlagScale Begin ########
+    elif (embd_group_is_list and
+        get_pg_size(embd_group) > 1
+        and torch.distributed.get_rank() in torch.distributed.get_process_group_ranks(embd_group[0])):
+        if is_pp_first_stage(pp_group):
+            model_module = model[0]
+        elif is_pp_last_stage(pp_group):
+            model_module = model[-1]
+        else:  # We do not support an interleaved schedule for models with encoders yet.
+            model_module = model[0]
+
+        ddp_config = model_module.ddp_config
+        use_dist_opt = ddp_config.use_distributed_optimizer
+        model_module = get_attr_wrapped_model(model_module, 'pre_process', return_model_obj=True)
+
+        weight = weight_getter(model_module)
+        if weight is None and skip_if_none:
+            return
+
+        grad_attr = _get_main_grad_attr(weight)
+        orig_grad = getattr(weight, grad_attr)
+        if ddp_config.use_megatron_fsdp:
+            orig_grad = orig_grad._local_tensor if orig_grad is not None else None
+        grad = _unshard_if_dtensor(orig_grad)
+        # When the embedding is frozen, the grad is None.
+        if grad is None and skip_if_none:
+            return
+        com_device = get_device_type_for_comm(embd_group)
+        if com_device == "cpu":
+            grad = grad.cpu()
+        if use_dist_opt:
+            if ddp_config.use_partial_reduce_for_shared_embedding:
+                dp_world_size = parallel_state.get_data_parallel_world_size()
+                dp_rank = parallel_state.get_data_parallel_rank()
+                assert grad.shape[0] % dp_world_size == 0, f"grad shape: {grad.shape[0]}, dp_world_size: {dp_world_size}"
+                per_partion_size = grad.shape[0] // dp_world_size
+                if len(embd_group) == 1:
+                    offset = per_partion_size * dp_rank
+                    torch.distributed.all_reduce(grad[offset:offset+per_partion_size, :], group=embd_group[0])
+                else:
+                    group_idx = 0
+                    per_partion_size = per_partion_size // len(embd_group)
+                    for group in embd_group:
+                        offset = per_partion_size * (dp_rank * len(embd_group) + group_idx)
+                        torch.distributed.all_reduce(grad[offset : offset + per_partion_size, :], group=group)
+                        group_idx += 1
+            else: # megartron default method
+                torch.distributed.all_reduce(grad, group=embd_group[0])
+        else:
+            if len(embd_group) == 1: # megartron default method
+                torch.distributed.all_reduce(grad, group=embd_group[0])
+            else:
+                original_grad_data = grad.clone().detach().data
+                for group in embd_group:
+                    grad.data.copy_(original_grad_data)
+                    torch.distributed.all_reduce(grad, group=group)
+        if grad.device == torch.device('cpu'):
+            grad.to(torch.cuda.current_device())
+        setattr(weight, grad_attr, _reshard_if_dtensor(grad, orig_grad))
+    ######## FlagScale End ########
 
 
 def _allreduce_position_embedding_grads(
@@ -482,6 +561,9 @@ def finalize_model_grads(
         last_rank = get_pp_last_rank(pp_group)
         torch.distributed.broadcast(num_tokens, src=last_rank, group=pp_group)
 
+        if num_tokens.device == torch.device('cpu'):
+            num_tokens = num_tokens.cuda()
+
         # all-reduce across DP ranks.
         torch.distributed.all_reduce(num_tokens, group=dp_cp_group)
         for model_chunk in model:
