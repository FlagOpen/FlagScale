diff --git a/megatron/core/extensions/flag_engine_spec_provider.py b/megatron/core/extensions/flag_engine_spec_provider.py
new file mode 100644
index 000000000..e5bedacdd
--- /dev/null
+++ b/megatron/core/extensions/flag_engine_spec_provider.py
@@ -0,0 +1,98 @@
+# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
+
+import warnings
+from typing import Optional, Tuple
+
+from megatron.core.extensions.transformer_engine import (
+    TEActivationOp,
+    TEColumnParallelGroupedLinear,
+    TERowParallelGroupedLinear,
+)
+from megatron.core.extensions.flag_engine import (
+    FEColumnParallelLinear,
+    FEDotProductAttention,
+    FELayerNormColumnParallelLinear,
+    FELinear,
+    FENorm,
+    FERowParallelLinear,
+)
+from megatron.core.fusions.fused_layer_norm import FusedLayerNorm
+from megatron.core.models.backends import BackendSpecProvider
+from megatron.core.tensor_parallel.layers import ColumnParallelLinear, RowParallelLinear
+from megatron.core.transformer.mlp import MLPSubmodules
+from megatron.core.transformer.moe.experts import GroupedMLP, SequentialMLP, TEGroupedMLP
+from megatron.core.utils import get_te_version, is_te_min_version
+
+
+class FESpecProvider(BackendSpecProvider):
+    """A protocol for providing the submodules used in Spec building."""
+
+    def linear(self) -> type:
+        """Which linear module TE backend uses"""
+        return FELinear
+
+    def column_parallel_linear(self) -> type:
+        """Which column parallel linear module TE backend uses"""
+        return FEColumnParallelLinear
+
+    def row_parallel_linear(self) -> type:
+        """Which row parallel linear module TE backend uses"""
+        return FERowParallelLinear
+
+    def fuse_layernorm_and_linear(self) -> bool:
+        """TE backend chooses a single module for layernorm and linear"""
+        return True
+
+    def column_parallel_layer_norm_linear(self) -> Optional[type]:
+        """Which module for sequential layernorm and linear"""
+        return FELayerNormColumnParallelLinear
+
+    def layer_norm(self, rms_norm: bool = False, for_qk: bool = False) -> type:
+        """Which module to use for layer norm"""
+        if for_qk and not is_te_min_version("1.9.0"):
+            # FENorm significantly harms convergence when used
+            # for QKLayerNorm if TE Version < 1.9;
+            # we instead use the Apex implementation.
+            return FusedLayerNorm
+        return FENorm
+
+    def core_attention(self) -> type:
+        """Which module to use for attention"""
+        return FEDotProductAttention
+
+    def grouped_mlp_modules(
+        self, moe_use_grouped_gemm: bool, moe_use_legacy_grouped_gemm: bool
+    ) -> Tuple[type, Optional[MLPSubmodules]]:
+        """Which module and submodules to use for grouped mlp"""
+        if (
+            moe_use_grouped_gemm
+            and TEColumnParallelGroupedLinear is not None
+            and not moe_use_legacy_grouped_gemm
+        ):
+            return TEGroupedMLP, MLPSubmodules(
+                linear_fc1=TEColumnParallelGroupedLinear, linear_fc2=TERowParallelGroupedLinear
+            )
+        elif moe_use_grouped_gemm:
+            warnings.warn(
+                'The legacy GroupedMLP will be deprecated in Megatron-Core v0.12.0. '
+                'Please update the TransformerEngine to version>=1.7.0 and use TEGroupedMLP.'
+            )
+            return GroupedMLP, None
+        else:
+            if not is_te_min_version("1.7.0.dev0"):
+                warnings.warn(
+                    "Only transformer-engine>=1.7.0 supports MoE experts, "
+                    f"but your version is {get_te_version()}. "
+                    "Use local linear implementation instead."
+                )
+                return SequentialMLP, MLPSubmodules(
+                    linear_fc1=ColumnParallelLinear, linear_fc2=RowParallelLinear
+                )
+            return SequentialMLP, MLPSubmodules(
+                linear_fc1=FEColumnParallelLinear, linear_fc2=FERowParallelLinear
+            )
+
+    def activation_func(self) -> type:
+        """Which module to use for activation function"""
+        return TEActivationOp
+
