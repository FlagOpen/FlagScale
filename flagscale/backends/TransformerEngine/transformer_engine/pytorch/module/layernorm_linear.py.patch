diff --git a/transformer_engine/pytorch/module/layernorm_linear.py b/transformer_engine/pytorch/module/layernorm_linear.py
index 933c7cde..924a6bf3 100644
--- a/transformer_engine/pytorch/module/layernorm_linear.py
+++ b/transformer_engine/pytorch/module/layernorm_linear.py
@@ -73,8 +73,11 @@ from ..cpu_offload import is_cpu_offload_enabled, mark_activation_offload
 
 from ..cpp_extensions import (
     general_gemm,
+    gems_general_gemm,
 )
 
+from .gems_rms_norm import rms_norm_forward, rms_norm_backward
+
 __all__ = ["LayerNormLinear"]
 
 
@@ -205,18 +208,29 @@ class _LayerNormLinear(torch.autograd.Function):
 
         # Apply normalization
         nvtx_range_push(f"{nvtx_label}.norm")
-        ln_out, mu, rsigma = apply_normalization(
+        
+        # TODO(lixianduo): polish
+        # ln_out, mu, rsigma = apply_normalization(
+        #     inputmat,
+        #     None,  # ln_out
+        #     ln_weight,
+        #     ln_bias,
+        #     eps,
+        #     input_quantizer if with_quantized_norm else None,
+        #     inputmat.dtype,
+        #     normalization,
+        #     fwd_ln_sm_margin,
+        #     zero_centered_gamma,
+        # )
+
+        ln_out, rsigma = rms_norm_forward(
             inputmat,
-            None,  # ln_out
+            [inputmat.shape[-1]],
             ln_weight,
-            ln_bias,
             eps,
-            input_quantizer if with_quantized_norm else None,
-            inputmat.dtype,
-            normalization,
-            fwd_ln_sm_margin,
-            zero_centered_gamma,
         )
+        mu = None
+        
         nvtx_range_pop(f"{nvtx_label}.norm")
 
         # Store unquantized layer norm output if we need to return it
@@ -341,7 +355,20 @@ class _LayerNormLinear(torch.autograd.Function):
         # Note: y = x * w^T
         # ------------------------------------------------------
         nvtx_range_push(f"{nvtx_label}.gemm")
-        gemm_out, *_, reduce_scatter_out = general_gemm(
+        # TODO(lixianduo): polish
+        # gemm_out, *_, reduce_scatter_out = general_gemm(
+        #     weightmat,
+        #     ln_out_total,
+        #     get_workspace(),
+        #     quantization_params=output_quantizer,
+        #     out_dtype=activation_dtype,
+        #     bias=bias,
+        #     use_split_accumulator=use_split_accumulator,
+        #     ub=ub_obj,
+        #     ub_type=ub_type,
+        #     extra_output=reduce_scatter_out,
+        # )
+        gemm_out, *_, reduce_scatter_out = gems_general_gemm(
             weightmat,
             ln_out_total,
             get_workspace(),
@@ -492,6 +519,7 @@ class _LayerNormLinear(torch.autograd.Function):
             ctx.return_layernorm_output_gathered = return_layernorm_output_gathered
             ctx.bwd_ln_sm_margin = bwd_ln_sm_margin
             ctx.zero_centered_gamma = zero_centered_gamma
+            ctx.eps = eps
             ctx.ub_overlap_ag = ub_overlap_ag_dgrad
             ctx.ub_overlap_rs_dgrad = ub_overlap_rs_dgrad
             ctx.ub_bulk_wgrad = ub_bulk_wgrad
@@ -714,7 +742,23 @@ class _LayerNormLinear(torch.autograd.Function):
             # dgrad GEMM
             # Note: dx = dy * w
             nvtx_range_push(f"{nvtx_label}.dgrad_gemm")
-            gemm_out, *_, reduce_scatter_out = general_gemm(
+            # TODO(lixianduo): polish
+            # gemm_out, *_, reduce_scatter_out = general_gemm(
+            #     weight,
+            #     grad_output,
+            #     get_workspace(),
+            #     layout="NN",
+            #     grad=True,
+            #     quantization_params=ctx.grad_input_quantizer,
+            #     out=gemm_out,
+            #     out_dtype=ctx.activation_dtype,
+            #     use_split_accumulator=use_split_accumulator,
+            #     ub=ub_obj_dgrad,
+            #     ub_type=ub_type_dgrad,
+            #     extra_output=reduce_scatter_out,
+            #     bulk_overlap=ctx.ub_bulk_dgrad,
+            # )
+            gemm_out, *_, reduce_scatter_out = gems_general_gemm(
                 weight,
                 grad_output,
                 get_workspace(),
@@ -878,7 +922,9 @@ class _LayerNormLinear(torch.autograd.Function):
 
                     """
                     nvtx_range_push(f"{nvtx_label}.wgrad_gemm")
-                    dw, db, *_ = general_gemm(x, dy, **wgrad_gemm_kwargs)
+                    # TODO(lixianduo): polish
+                    # dw, db, *_ = general_gemm(x, dy, **wgrad_gemm_kwargs)
+                    dw, db, *_ = gems_general_gemm(x, dy, **wgrad_gemm_kwargs)
                     nvtx_range_pop(f"{nvtx_label}.wgrad_gemm")
                     return dw, db
 
@@ -963,14 +1009,26 @@ class _LayerNormLinear(torch.autograd.Function):
                 )
                 dgrad = dgrad.reshape(inputmat.size())
             elif ctx.normalization == "RMSNorm":
-                dgrad, dgamma = tex.rmsnorm_bwd(
+                
+                # TODO(lixianduo): polish
+                # dgrad, dgamma = tex.rmsnorm_bwd(
+                #     dgrad,
+                #     inputmat,
+                #     rsigma,
+                #     ln_weight,
+                #     ctx.bwd_ln_sm_margin,
+                #     ctx.zero_centered_gamma,
+                # )
+                
+                dgrad, dgamma = rms_norm_backward(
                     dgrad,
                     inputmat,
                     rsigma,
+                    [inputmat.shape[-1]],
                     ln_weight,
-                    ctx.bwd_ln_sm_margin,
-                    ctx.zero_centered_gamma,
+                    ctx.eps,
                 )
+
                 dgrad = dgrad.reshape(inputmat.size())
                 dbeta = None
             nvtx_range_pop(f"{nvtx_label}.norm")
@@ -1835,3 +1893,4 @@ class LayerNormLinear(TransformerEngineBaseModule):
                 self.quantizers["scaling_fwd"][
                     tex.FP8FwdTensors.GEMM1_INPUT
                 ].all_gather_usage = True
+
