diff --git a/transformer_engine/pytorch/module/layernorm_linear.py b/transformer_engine/pytorch/module/layernorm_linear.py
index 933c7cde..e1e2d17a 100644
--- a/transformer_engine/pytorch/module/layernorm_linear.py
+++ b/transformer_engine/pytorch/module/layernorm_linear.py
@@ -205,18 +205,31 @@ class _LayerNormLinear(torch.autograd.Function):
 
         # Apply normalization
         nvtx_range_push(f"{nvtx_label}.norm")
-        ln_out, mu, rsigma = apply_normalization(
+        
+        # ln_out, mu, rsigma = apply_normalization(
+        #     inputmat,
+        #     None,  # ln_out
+        #     ln_weight,
+        #     ln_bias,
+        #     eps,
+        #     input_quantizer if with_quantized_norm else None,
+        #     inputmat.dtype,
+        #     normalization,
+        #     fwd_ln_sm_margin,
+        #     zero_centered_gamma,
+        # )
+
+        from .gems_rms_norm import rms_norm_forward
+        ln_out, rsigma = rms_norm_forward(
             inputmat,
-            None,  # ln_out
+            [inputmat.shape[-1]],
             ln_weight,
-            ln_bias,
             eps,
-            input_quantizer if with_quantized_norm else None,
-            inputmat.dtype,
-            normalization,
-            fwd_ln_sm_margin,
-            zero_centered_gamma,
         )
+        mu = None        
+
+        
+        
         nvtx_range_pop(f"{nvtx_label}.norm")
 
         # Store unquantized layer norm output if we need to return it
@@ -492,6 +505,7 @@ class _LayerNormLinear(torch.autograd.Function):
             ctx.return_layernorm_output_gathered = return_layernorm_output_gathered
             ctx.bwd_ln_sm_margin = bwd_ln_sm_margin
             ctx.zero_centered_gamma = zero_centered_gamma
+            ctx.eps = eps
             ctx.ub_overlap_ag = ub_overlap_ag_dgrad
             ctx.ub_overlap_rs_dgrad = ub_overlap_rs_dgrad
             ctx.ub_bulk_wgrad = ub_bulk_wgrad
@@ -963,14 +977,26 @@ class _LayerNormLinear(torch.autograd.Function):
                 )
                 dgrad = dgrad.reshape(inputmat.size())
             elif ctx.normalization == "RMSNorm":
-                dgrad, dgamma = tex.rmsnorm_bwd(
+                # dgrad, dgamma = tex.rmsnorm_bwd(
+                #     dgrad,
+                #     inputmat,
+                #     rsigma,
+                #     ln_weight,
+                #     ctx.bwd_ln_sm_margin,
+                #     ctx.zero_centered_gamma,
+                # )
+                from .gems_rms_norm import rms_norm_backward
+                dgrad, dgamma = rms_norm_backward(
                     dgrad,
                     inputmat,
                     rsigma,
+                    [inputmat.shape[-1]],
                     ln_weight,
-                    ctx.bwd_ln_sm_margin,
-                    ctx.zero_centered_gamma,
+                    ctx.eps,
                 )
+
+
+
                 dgrad = dgrad.reshape(inputmat.size())
                 dbeta = None
             nvtx_range_pop(f"{nvtx_label}.norm")
@@ -1835,3 +1861,4 @@ class LayerNormLinear(TransformerEngineBaseModule):
                 self.quantizers["scaling_fwd"][
                     tex.FP8FwdTensors.GEMM1_INPUT
                 ].all_gather_usage = True
+
