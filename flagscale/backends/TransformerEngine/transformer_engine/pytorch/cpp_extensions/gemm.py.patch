diff --git a/transformer_engine/pytorch/cpp_extensions/gemm.py b/transformer_engine/pytorch/cpp_extensions/gemm.py
index dd041129..cccbdce9 100644
--- a/transformer_engine/pytorch/cpp_extensions/gemm.py
+++ b/transformer_engine/pytorch/cpp_extensions/gemm.py
@@ -19,9 +19,11 @@ from ...debug.pytorch.debug_quantization import DebugQuantizer
 
 __all__ = [
     "general_gemm",
+    "gems_general_gemm",
     "general_grouped_gemm",
 ]
 
+import flag_gems
 
 def validate_gemm_scale(scale: Optional[float], required: bool) -> float:
     """Validate whether a GEMM scaling factor is consistent with its usage"""
@@ -154,6 +156,208 @@ def general_gemm(
     return out, bias_grad, gelu_input, extra_output
 
 
+def gems_general_gemm(
+    A: torch.Tensor,
+    B: torch.Tensor,
+    workspace: torch.Tensor,
+    out_dtype: Optional[torch.dtype] = None,
+    quantization_params: Optional[Quantizer] = None,
+    gelu: bool = False,
+    gelu_in: torch.Tensor = None,
+    alpha: float = 1.0,
+    beta: Optional[float] = None,
+    accumulate: bool = False,
+    layout: str = "TN",
+    out: Optional[torch.Tensor] = None,
+    bias: Optional[torch.Tensor] = None,
+    use_split_accumulator: bool = False,
+    grad: bool = False,
+    ub: Union[tex.CommOverlap, tex.CommOverlapP2P] = None,
+    ub_type: tex.CommOverlapType = None,
+    extra_output: Optional[torch.Tensor] = None,
+    bulk_overlap: bool = False,
+) -> Iterable[Optional[torch.Tensor]]:
+    """GEMM supporting fp8 inputs."""
+    assert layout in ("TN", "NN", "NT"), f"GEMM layout {layout} not supported."
+    transa = layout[0] == "T"
+    transb = layout[1] == "T"
+
+    alpha = validate_gemm_scale(alpha, True)
+    beta = validate_gemm_scale(beta, accumulate)
+
+    if ub_type is not None:
+        assert ub is not None, (
+            f"{'AG+GEMM' if ub_type == tex.CommOverlapType.AG else 'GEMM+RS'} overlap requires"
+            + "a valid `ub` communicator object."
+        )
+
+    if ub is not None:
+        assert ub_type is not None, "Comm+GEMM overlap requires a valid `comm_type` argument."
+        if ub_type == tex.CommOverlapType.RS:
+            if not (bulk_overlap and not ub.is_fp8_ubuf()):
+                assert extra_output is not None, "GEMM+RS overlap requires extra output tensor."
+
+    if out is not None:
+        if not out.is_contiguous():
+            raise ValueError("Output tensor is not contiguous.")
+
+    # If A or B are custom tensors -> dispatch to quantizers's qgemm implementation
+    if is_custom(A) or is_custom(B):
+        return custom_gemm(
+            A,
+            B,
+            workspace,
+            out_dtype,
+            quantization_params,
+            gelu,
+            gelu_in,
+            accumulate,
+            layout,
+            out,
+            bias,
+            use_split_accumulator,
+            grad,
+        )
+
+    debug_quantizer = None
+    if isinstance(quantization_params, DebugQuantizer):
+        debug_quantizer = quantization_params
+        quantization_params = quantization_params.parent_quantizer
+        A = A.get_tensor(not transa)
+        B = B.get_tensor(transb)
+
+    # Use bfloat16 as default bias_dtype
+    bias_dtype = TE_DType[torch.bfloat16 if bias is None else bias.dtype]
+
+    if isinstance(A, Float8BlockwiseQTensorStorage) or isinstance(B, Float8BlockwiseQTensorStorage):
+        # There is not use_split_accumulator == False
+        # implementation for Float8BlockwiseQTensorStorage GEMM
+        use_split_accumulator = True
+
+        # Check that data format is supported
+        if (
+            A._data_format != tex.Float8BlockScaleTensorFormat.GEMM_READY
+            or B._data_format != tex.Float8BlockScaleTensorFormat.GEMM_READY
+        ):
+            raise RuntimeError("GEMM with Float8BlockwiseQTensor requires GEMM_READY format")
+
+
+    args = (
+        A,
+        transa,  # transa
+        B,
+        transb,  # transb
+        out,
+        quantization_params,
+        TE_DType[out_dtype] if out_dtype is not None else None,
+        bias,
+        bias_dtype,
+        gelu,
+        gelu_in,
+        grad,  # grad
+        workspace,
+        workspace.shape[0],
+        accumulate,
+        use_split_accumulator,
+    )
+    kwargs = {
+        "comm_overlap": ub,
+        "comm_type": ub_type,
+        "extra_output": extra_output,
+        "bulk_overlap": bulk_overlap,
+        "alpha": alpha,
+        "beta": beta,
+    }
+
+
+    if not transa and transb:
+
+        s = -1
+        b = -1
+        orig_A_shape = A.shape
+        orig_B_shape = B.shape
+        # shape_a_changed = False
+        shape_b_changed = False
+        
+        # A = A.to(torch.float32)
+        # B = B.to(torch.float32)
+        
+        if A.ndim == 3:
+            A = A.view(-1, A.shape[-1])
+            # shape_a_changed = True
+
+        if B.ndim == 3:
+            s, b, _ = B.shape
+            # B = B.view(-1, B.shape[-1])
+            shape_b_changed = True
+        
+        A_comp = A.T if transa else A
+        B_comp = B.T if transb else B
+        out1 = flag_gems.mm(B_comp, A_comp)
+
+        # if transa:
+        #     A = A.transpose(0, 1)
+        # if transb:
+        #     B = B.transpose(0, 1)
+        
+        # if shape_a_changed:
+        #     A = A.view(orig_A_shape)
+        if shape_b_changed:
+            out1 = out1.view(s, b, -1)
+            # B = B.view(orig_B_shape)
+        
+        # out1 = out.to(torch.float32) + out1
+        # out.copy_(out1)
+
+        # out.copy_(out.to(torch.float32).add_(out1))
+        out.add_(out1)
+
+
+    else:
+        s = -1
+        b = -1
+        orig_A_shape = A.shape
+        orig_B_shape = B.shape
+        shape_a_changed = False
+        shape_b_changed = False
+        
+        if A.ndim == 3:
+            A = A.view(-1, A.shape[-1])
+            shape_a_changed = True
+
+        if B.ndim == 3:
+            s, b, _ = B.shape
+            B = B.view(-1, B.shape[-1])
+            shape_b_changed = True
+        
+        if transa:
+            A = A.transpose(0, 1)
+        if transb:
+            B = B.transpose(0, 1)
+
+        out = flag_gems.mm(B, A)
+
+        if transa:
+            A = A.transpose(0, 1)
+        if transb:
+            B = B.transpose(0, 1)
+        
+        if shape_a_changed:
+            A = A.view(orig_A_shape)
+        if shape_b_changed:
+            out = out.view(s, b, -1)
+            B = B.view(orig_B_shape)
+
+    bias_grad = None
+    gelu_input = None
+    extra_output = None
+
+    if debug_quantizer is not None:
+        out = debug_quantizer.process_gemm_output(out)
+
+    return out, bias_grad, gelu_input, extra_output
+
+
 def general_grouped_gemm(
     A: List[torch.Tensor],
     B: List[torch.Tensor],
@@ -226,3 +430,4 @@ def general_grouped_gemm(
     )
 
     return out, bias, gelu_input
+
