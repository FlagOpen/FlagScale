Subject: [PATCH] little bug fix
---
Index: vllm/model_executor/layers/fused_moe/layer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
--- a/vllm/model_executor/layers/fused_moe/layer.py	(revision 65334ef3b9e4fd32ebc5c4e512debc25d5025488)
+++ b/vllm/model_executor/layers/fused_moe/layer.py	(revision 730e8fae53288bfc8e2b56b797940765c1673770)
@@ -43,6 +43,7 @@
         from .pplx_prepare_finalize import PplxPrepareAndFinalize
 else:
     fused_experts = None  # type: ignore
+    FusedMoEPrepareAndFinalize = None  # type: ignore
 if is_rocm_aiter_moe_enabled():
     from vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe import (  # noqa: E501
         rocm_aiter_biased_group_topk as grouped_topk)
Index: vllm/v1/attention/backends/mla/common.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py
--- a/vllm/v1/attention/backends/mla/common.py	(revision 65334ef3b9e4fd32ebc5c4e512debc25d5025488)
+++ b/vllm/v1/attention/backends/mla/common.py	(revision 730e8fae53288bfc8e2b56b797940765c1673770)
@@ -215,7 +215,7 @@
     is_vllm_fa = True
 except ImportError:
     # For rocm use upstream flash attention
-    from flash_attn import flash_attn_varlen_func
+    flash_attn_varlen_func = None
     is_vllm_fa = False
 
 if TYPE_CHECKING:
