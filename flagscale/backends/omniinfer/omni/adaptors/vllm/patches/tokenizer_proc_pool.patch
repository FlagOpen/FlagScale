diff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py
index 0b3bdf7d4..cc549d47e 100644
--- a/vllm/entrypoints/openai/serving_completion.py
+++ b/vllm/entrypoints/openai/serving_completion.py
@@ -84,6 +84,11 @@ class OpenAIServingCompletion(OpenAIServing):
         # success status before we actually start generating text :).
         if self.engine_client.errored:
             raise self.engine_client.dead_error
+        
+        # For decode side pre-process optimization.
+        # if exists, replace prompt with request.kv_transfer_params
+        if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+            request.prompt = request.kv_transfer_params["prompt_token_ids"]
 
         # Return error for unsupported features.
         if request.suffix is not None:
@@ -104,7 +109,7 @@ class OpenAIServingCompletion(OpenAIServing):
             ) = self._maybe_get_adapters(request)
 
             tokenizer = await self.engine_client.get_tokenizer(lora_request)
-
+            st = time.time()
             request_prompts, engine_prompts = await self._preprocess_completion(
                 request,
                 tokenizer,
@@ -112,6 +117,8 @@ class OpenAIServingCompletion(OpenAIServing):
                 truncate_prompt_tokens=request.truncate_prompt_tokens,
                 add_special_tokens=request.add_special_tokens,
             )
+            duration = time.time() - st
+            logger.debug(f"<<<Total time of _preprocess_completion:{duration*1000} ms")
         except ValueError as e:
             logger.exception("Error in preprocessing prompt inputs")
             return self.create_error_response(str(e))
@@ -216,6 +223,13 @@ class OpenAIServingCompletion(OpenAIServing):
             final_res_batch_checked = cast(list[RequestOutput],
                                            final_res_batch)
 
+            prompt_token_ids = []
+            for req_output in final_res_batch_checked:
+                prompt_token_ids.append(req_output.prompt_token_ids) 
+            if final_res_batch_checked[0].kv_transfer_params:
+                ## In Prefill node, the response will carry prompt_token_ids with kv_transfer_params
+                final_res_batch_checked[0].kv_transfer_params["prompt_token_ids"] = prompt_token_ids
+
             response = self.request_output_to_completion_response(
                 final_res_batch_checked,
                 request,
diff --git a/vllm/entrypoints/openai/serving_engine.py b/vllm/entrypoints/openai/serving_engine.py
index f9eebde37..8b3857fad 100644
--- a/vllm/entrypoints/openai/serving_engine.py
+++ b/vllm/entrypoints/openai/serving_engine.py
@@ -4,8 +4,10 @@ import json
 import sys
 import time
 from collections.abc import (AsyncGenerator, Iterable, Iterator, Mapping,
-                             Sequence)
+                             Sequence, Awaitable)
+import concurrent
 from concurrent.futures.thread import ThreadPoolExecutor
+from concurrent.futures import ProcessPoolExecutor
 from http import HTTPStatus
 from typing import (Annotated, Any, Callable, ClassVar, Generic, Optional,
                     TypeVar, Union)
@@ -13,6 +15,9 @@ from typing import (Annotated, Any, Callable, ClassVar, Generic, Optional,
 from fastapi import Request
 from pydantic import BaseModel, ConfigDict, Field
 from starlette.datastructures import Headers
+import asyncio
+from functools import partial
+import os
 
 if sys.version_info >= (3, 12):
     from typing import TypedDict
@@ -66,9 +71,9 @@ from vllm.sampling_params import BeamSearchParams, SamplingParams
 from vllm.sequence import Logprob, PromptLogprobs
 from vllm.tracing import (contains_trace_headers, extract_trace_headers,
                           log_tracing_disabled_warning)
-from vllm.transformers_utils.tokenizer import AnyTokenizer, MistralTokenizer
+from vllm.transformers_utils.tokenizer import AnyTokenizer, MistralTokenizer, get_tokenizer
 from vllm.utils import (is_list_of, make_async, merge_async_iterators,
-                        random_uuid)
+                        random_uuid, P, T)
 
 logger = init_logger(__name__)
 
@@ -196,14 +201,226 @@ class OpenAIServing:
         self.request_logger = request_logger
         self.return_tokens_as_token_ids = return_tokens_as_token_ids
 
-        self._tokenizer_executor = ThreadPoolExecutor(max_workers=1)
+        self.tokenizer_worker_num = max(1, int(os.getenv("TOKENIZER_WORKER_NUM", default=5)))
+        self.process_pool_threshold = max(1, int(os.getenv("TOKENIZER_PROC_POOL_THRES", 512)))
+
+        logger.info(f"Tokenizer worker num is {self.tokenizer_worker_num}.")
+        logger.info(f"Process pool threshold is {self.process_pool_threshold}.")
+        self.do_lower_case = False
+        if model_config and model_config.encoder_config:
+            self.do_lower_case = model_config.encoder_config.do_lower_case
 
+        ## Use thread pool tokenizer to process short requests
+        self._tokenizer_executor = ThreadPoolExecutor(max_workers=1)
         self._tokenize_prompt_input_async = make_async(
             self._tokenize_prompt_input, executor=self._tokenizer_executor)
         self._tokenize_prompt_input_or_inputs_async = make_async(
             self._tokenize_prompt_input_or_inputs,
             executor=self._tokenizer_executor)
 
+        ## Use process pool tokenizer to process long requests
+        self._tokenizer_proc_pool_executor = ProcessPoolExecutor(max_workers=self.tokenizer_worker_num,
+            initializer=self._init_proc_tokenizer,
+            initargs=(model_config.tokenizer, model_config.tokenizer_mode, model_config.trust_remote_code,
+                model_config.tokenizer_revision, model_config.truncation_side, model_config.max_model_len,
+                self.do_lower_case,))
+        self._tokenize_prompt_input_or_inputs_async_proc_pool = make_async(
+            self._tokenize_prompt_input_or_inputs_proc_pool,
+            executor=self._tokenizer_proc_pool_executor
+        )
+        self._initialize_process_pool()
+
+    def _initialize_process_pool(self):
+        """
+        Initializes the process pool executor by submitting and waiting for dummy tasks.
+        Ensures the process pool is properly set up before use.
+        """
+        if hasattr(self, '_tokenizer_proc_pool_executor'):
+            executor: Optional[concurrent.futures.ProcessPoolExecutor] = getattr(self, '_tokenizer_proc_pool_executor')
+            futures = []
+
+            try:
+                # Submit dummy tasks to all workers
+                for _ in range(executor._max_workers):
+                    future = executor.submit(OpenAIServing._proc_pool_dummy_task)
+                    futures.append(future)
+
+                # Wait for all dummy tasks to complete
+                for future in futures:
+                    future.result()
+            except concurrent.futures.process.BrokenProcessPool as e:
+                logger.error(f"Process pool initialization failed: {e}")
+                raise
+            except Exception as e:
+                logger.error(f"Unexpected error during process pool initialization: {e}")
+                raise
+    @staticmethod
+    def _proc_pool_dummy_task():
+        """A no-op task used to verify process pool initialization."""        
+        return
+
+    @staticmethod
+    def _init_proc_tokenizer(
+        tokenizer: str,
+        tokenizer_mode: str,
+        trust_remote_code: bool,
+        tokenizer_revision: str,
+        truncation_side: str,
+        max_model_len: int,
+        do_lower_case: bool
+    ) -> None:
+        """
+        Initializes a tokenizer in a subprocess for tokenization tasks.
+
+        Sets global configuration variables and loads a tokenizer instance for use in
+        process pool-based tokenization. Must be called in each subprocess before tokenization.
+
+        Args:
+            tokenizer (str): Name or path of the tokenizer.
+            tokenizer_mode (str): Tokenizer mode.
+            trust_remote_code (bool): Whether to allow loading remote code for the tokenizer.
+            tokenizer_revision (str): Revision of the tokenizer.
+            truncation_side (str): Side to truncate tokens.
+            max_model_len (int): Maximum model input length.
+            do_lower_case (bool): Whether to lowercase input text during tokenization.
+
+        Returns:
+            None
+
+        Raises:
+            ValueError: If input parameters are invalid or tokenizer initialization fails.
+        """
+        global _process_tokenizer, _process_tokenizer_name, _process_tokenizer_mode
+        global _process_trust_remote_code, _process_tokenizer_revision
+        global _process_truncation_side, _process_max_model_len, _process_do_lower_case
+
+        # Validate inputs
+        if not tokenizer:
+            raise ValueError("Tokenizer name or path cannot be empty")
+        if max_model_len <= 0:
+            raise ValueError(f"max_model_len must be positive, got {max_model_len}")
+
+        # Store configuration in global variables
+        _process_tokenizer_name = tokenizer
+        _process_tokenizer_mode = tokenizer_mode
+        _process_trust_remote_code = trust_remote_code
+        _process_tokenizer_revision = tokenizer_revision
+        _process_truncation_side = truncation_side
+        _process_max_model_len = max_model_len
+        _process_do_lower_case = do_lower_case
+
+        # Initialize tokenizer
+        try:
+            _process_tokenizer = get_tokenizer(
+                tokenizer_name=tokenizer,
+                tokenizer_mode=tokenizer_mode,
+                trust_remote_code=trust_remote_code,
+                revision=tokenizer_revision,
+                truncation_side=truncation_side
+            )
+        except Exception as e:
+            raise ValueError(f"Failed to initialize tokenizer: {e}")
+ 
+    @staticmethod
+    def _validate_input_static(
+        request: AnyRequest,
+        input_ids: list[int],
+        input_text: str,
+        max_model_len: int,
+    ) -> TextTokensPrompt:
+        token_num = len(input_ids)
+ 
+        # Note: EmbeddingRequest and ScoreRequest doesn't have max_tokens
+        if isinstance(request,
+                      (EmbeddingChatRequest, EmbeddingCompletionRequest,
+                       ScoreRequest, RerankRequest)):
+ 
+            operation = "score" if isinstance(request, ScoreRequest) \
+                else "embedding generation"
+            if token_num > max_model_len:
+                raise ValueError(
+                    f"This model's maximum context length is "
+                    f"{max_model_len} tokens. However, you requested "
+                    f"{token_num} tokens in the input for {operation}. "
+                    f"Please reduce the length of the input.")
+            return TextTokensPrompt(prompt=input_text,
+                                    prompt_token_ids=input_ids)
+ 
+        # Note: TokenizeRequest and DetokenizeRequest doesn't have max_tokens
+        # and does not require model context length validation
+        if isinstance(request, (TokenizeCompletionRequest, TokenizeChatRequest,
+                                DetokenizeRequest)):
+            return TextTokensPrompt(prompt=input_text,
+                                    prompt_token_ids=input_ids)
+ 
+        # chat completion endpoint supports max_completion_tokens
+        if isinstance(request, ChatCompletionRequest):
+            # TODO(#9845): remove max_tokens when field dropped from OpenAI API
+            max_tokens = request.max_completion_tokens or request.max_tokens
+        else:
+            max_tokens = request.max_tokens
+        if max_tokens is None:
+            if token_num >= max_model_len:
+                raise ValueError(
+                    f"This model's maximum context length is "
+                    f"{max_model_len} tokens. However, you requested "
+                    f"{token_num} tokens in the messages, "
+                    f"Please reduce the length of the messages.")
+        elif token_num + max_tokens > max_model_len:
+            raise ValueError(
+                f"This model's maximum context length is "
+                f"{max_model_len} tokens. However, you requested "
+                f"{max_tokens + token_num} tokens "
+                f"({token_num} in the messages, "
+                f"{max_tokens} in the completion). "
+                f"Please reduce the length of the messages or completion.")
+ 
+        return TextTokensPrompt(prompt=input_text, prompt_token_ids=input_ids)
+ 
+    @staticmethod
+    def _normalize_prompt_text_to_input_static(
+        request: AnyRequest,
+        tokenizer: AnyTokenizer,
+        prompt: str,
+        truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]],
+        add_special_tokens: bool,
+        max_model_len: int,
+        do_lower_case: bool,
+    ) -> TextTokensPrompt:
+        if do_lower_case:
+            prompt = prompt.lower()
+ 
+        if truncate_prompt_tokens is None:
+            encoded = tokenizer(prompt, add_special_tokens=add_special_tokens)
+        else:
+            encoded = tokenizer(prompt,
+                                add_special_tokens=add_special_tokens,
+                                truncation=True,
+                                max_length=truncate_prompt_tokens)
+ 
+        input_ids = encoded.input_ids
+ 
+        input_text = prompt
+ 
+        return OpenAIServing._validate_input_static(request, input_ids, input_text, max_model_len)
+ 
+    @staticmethod
+    def _normalize_prompt_tokens_to_input_static(
+        request: AnyRequest,
+        tokenizer: AnyTokenizer,
+        prompt_ids: list[int],
+        truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]],
+        max_model_len: int = 0,
+    ) -> TextTokensPrompt:
+        if truncate_prompt_tokens is None:
+            input_ids = prompt_ids
+        else:
+            input_ids = prompt_ids[-truncate_prompt_tokens:]
+ 
+        input_text = tokenizer.decode(input_ids)
+ 
+        return OpenAIServing._validate_input_static(request, input_ids, input_text, max_model_len)
+
     async def _preprocess(
         self,
         ctx: ServeContext,
@@ -441,9 +658,7 @@ class OpenAIServing:
         truncate_prompt_tokens: Optional[Annotated[int, Field(ge=-1)]],
         add_special_tokens: bool,
     ) -> TextTokensPrompt:
-        if (self.model_config.encoder_config is not None
-                and self.model_config.encoder_config.get(
-                    "do_lower_case", False)):
+        if self.do_lower_case:
             prompt = prompt.lower()
 
         if truncate_prompt_tokens is None:
@@ -627,6 +842,67 @@ class OpenAIServing:
             for prompt_input in parse_and_batch_prompt(input_or_inputs)
         ]
 
+    @staticmethod
+    def _tokenize_prompt_input_or_inputs_proc_pool(
+        request: AnyRequest,
+        input_or_inputs: Union[str, list[str], list[int], list[list[int]]],
+        truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]] = None,
+        add_special_tokens: bool = True,
+        # model_config: ModelConfig = None,
+    ) -> list[TextTokensPrompt]:
+        """
+        Tokenizes or detokenizes input prompts in a process pool, supporting multiple input formats.
+
+        According to the OpenAI API (https://platform.openai.com/docs/api-reference/embeddings/create),
+        each input can be a string or array of tokens. Each request can pass one or more inputs.
+
+        Args:
+            request (AnyRequest): The request object containing metadata or configuration.
+            input_or_inputs (Union[str, List[str], List[int], List[List[int]]]): Input prompt(s) as text or token IDs.
+            truncate_prompt_tokens (Optional[int]): Maximum number of tokens to keep, or None for no truncation.
+            add_special_tokens (bool): Whether to add special tokens (e.g., BOS, EOS) during tokenization.
+
+        Returns:
+            List[TextTokensPrompt]: A list of tokenized prompts with text and token IDs.
+
+        Raises:
+            ValueError: If the tokenizer or required global variables are not initialized.
+        """
+ 
+        global _process_tokenizer
+        global _process_max_model_len, _process_do_lower_case
+
+       # Validate global tokenizer state
+        if _process_tokenizer is None:
+            raise ValueError("Tokenizer not initialized. Call _init_proc_tokenizer first.")
+        if _process_max_model_len is None or _process_do_lower_case is None:
+            raise ValueError("Tokenizer configuration (max_model_len or do_lower_case) not initialized.")
+
+        # Process each prompt input
+        tokenized_prompts = []
+        for prompt_input in parse_and_batch_prompt(input_or_inputs):
+            if prompt_input["is_tokens"] is False:
+                result = OpenAIServing._normalize_prompt_text_to_input_static(
+                    request=request,
+                    tokenizer=_process_tokenizer,
+                    prompt=prompt_input["content"],
+                    truncate_prompt_tokens=truncate_prompt_tokens,
+                    add_special_tokens=add_special_tokens,
+                    max_model_len=_process_max_model_len,
+                    do_lower_case=_process_do_lower_case
+                )
+            else:
+                result = OpenAIServing._normalize_prompt_tokens_to_input_static(
+                    request=request,
+                    tokenizer=_process_tokenizer,
+                    prompt_ids=prompt_input["content"],
+                    truncate_prompt_tokens=truncate_prompt_tokens,
+                    max_model_len=_process_max_model_len
+                )
+            tokenized_prompts.append(result)
+
+        return tokenized_prompts
+
     async def _preprocess_completion(
         self,
         request: CompletionLikeRequest,
@@ -635,13 +911,23 @@ class OpenAIServing:
         truncate_prompt_tokens: Optional[Annotated[int, Field(ge=-1)]] = None,
         add_special_tokens: bool = True,
     ) -> tuple[list[TextTokensPrompt], list[TokensPrompt]]:
-        request_prompts = await self._tokenize_prompt_input_or_inputs_async(
-            request,
-            tokenizer,
-            input_or_inputs,
-            truncate_prompt_tokens=truncate_prompt_tokens,
-            add_special_tokens=add_special_tokens,
-        )
+        if len(input_or_inputs) < self.process_pool_threshold:
+            ## Use thread pool tokenizer to process short requests
+            request_prompts = await self._tokenize_prompt_input_or_inputs_async(
+                request,
+                tokenizer, 
+                input_or_inputs,
+                truncate_prompt_tokens=truncate_prompt_tokens,
+                add_special_tokens=add_special_tokens,
+            )
+        else:
+            ## Use process pool tokenizer to process long requests
+            request_prompts = await self._tokenize_prompt_input_or_inputs_async_proc_pool(
+                request,
+                input_or_inputs,
+                truncate_prompt_tokens=truncate_prompt_tokens,
+                add_special_tokens=add_special_tokens,
+            )
 
         engine_prompts = [
             TokensPrompt(prompt_token_ids=request_prompt["prompt_token_ids"])
