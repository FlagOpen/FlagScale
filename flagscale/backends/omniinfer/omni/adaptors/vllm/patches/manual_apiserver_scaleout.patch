diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 240142a1c..170db3449 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -283,6 +283,8 @@ class EngineArgs:
     pipeline_parallel_size: int = ParallelConfig.pipeline_parallel_size
     tensor_parallel_size: int = ParallelConfig.tensor_parallel_size
     data_parallel_size: int = ParallelConfig.data_parallel_size
+    data_parallel_rank: int = ParallelConfig.data_parallel_rank
+    data_parallel_rank_local: int = ParallelConfig.data_parallel_rank_local
     data_parallel_size_local: Optional[int] = None
     data_parallel_address: Optional[str] = None
     data_parallel_rpc_port: Optional[int] = None
@@ -599,6 +601,10 @@ class EngineArgs:
                                     **parallel_kwargs["tensor_parallel_size"])
         parallel_group.add_argument("--data-parallel-size", "-dp",
                                     **parallel_kwargs["data_parallel_size"])
+        parallel_group.add_argument("--data-parallel-rank", "-dp-rank",
+                                    **parallel_kwargs["data_parallel_rank"])
+        parallel_group.add_argument("--data-parallel-rank-local", "-dp-rank-local",
+                                    **parallel_kwargs["data_parallel_rank_local"])
         parallel_group.add_argument('--data-parallel-size-local',
                                     '-dpl',
                                     type=int,
@@ -1058,6 +1064,8 @@ class EngineArgs:
             pipeline_parallel_size=self.pipeline_parallel_size,
             tensor_parallel_size=self.tensor_parallel_size,
             data_parallel_size=self.data_parallel_size,
+            data_parallel_rank=self.data_parallel_rank,
+            data_parallel_rank_local=self.data_parallel_rank_local,
             data_parallel_size_local=data_parallel_size_local,
             data_parallel_master_ip=data_parallel_address,
             data_parallel_rpc_port=data_parallel_rpc_port,
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index 0d646d8dd..5628f0a11 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -110,10 +110,11 @@ class AsyncLLM(EngineClient):
         self.output_processor = OutputProcessor(self.tokenizer,
                                                 log_stats=self.log_stats)
 
-        # EngineCore (starts the engine in background process).
-        core_client_class = AsyncMPClient if (
-            vllm_config.parallel_config.data_parallel_size
-            == 1) else DPAsyncMPClient
+        # # EngineCore (starts the engine in background process).
+        # core_client_class = AsyncMPClient if (
+        #     vllm_config.parallel_config.data_parallel_size
+        #     == 1) else DPAsyncMPClient
+        core_client_class = AsyncMPClient # use AsyncMPClient only. a workaround for manual api-server scaleout
 
         self.engine_core = core_client_class(
             vllm_config=vllm_config,
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 0cf2383af..4d5f4d3d1 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -679,6 +679,8 @@ class DPEngineCoreProc(EngineCoreProc):
         super().__init__(vllm_config, on_head_node, input_address,
                          executor_class, log_stats, dp_rank)
 
+        self.engines_running = True # initialized with True, a workaround for manual api-server scaleout
+
     def _init_data_parallel(self, vllm_config: VllmConfig):
 
         # Configure GPUs and stateless process group for data parallel.
@@ -765,18 +767,19 @@ class DPEngineCoreProc(EngineCoreProc):
                 # dummy forward pass.
                 self.execute_dummy_batch()
 
-            # 3) All-reduce operation to determine global unfinished reqs.
-            self.engines_running = self._has_global_unfinished_reqs(
-                local_unfinished_reqs)
-
-            if not self.engines_running:
-                if self.local_dp_rank == 0:
-                    # Notify client that we are pausing the loop.
-                    logger.debug("Wave %d finished, pausing engine loop.",
-                                 self.current_wave)
-                    self.output_queue.put_nowait(
-                        EngineCoreOutputs(wave_complete=self.current_wave))
-                self.current_wave += 1
+            ### disable all-reduce operation, a workaround for manual api-server scale-out
+            # # 3) All-reduce operation to determine global unfinished reqs.
+            # self.engines_running = self._has_global_unfinished_reqs(
+            #     local_unfinished_reqs)
+
+            # if not self.engines_running:
+            #     if self.local_dp_rank == 0:
+            #         # Notify client that we are pausing the loop.
+            #         logger.debug("Wave %d finished, pausing engine loop.",
+            #                      self.current_wave)
+            #         self.output_queue.put_nowait(
+            #             EngineCoreOutputs(wave_complete=self.current_wave))
+            #     self.current_wave += 1
 
     def _has_global_unfinished_reqs(self, local_unfinished: bool) -> bool:
 
