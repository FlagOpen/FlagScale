diff --git a/vllm/config.py b/vllm/config.py
index 09e89c111..4b3ed9796 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -2427,6 +2427,19 @@ class SpeculativeConfig:
                 self.model = self.target_model_config.model
             elif self.method in ("ngram", "[ngram]"):
                 self.model = "ngram"
+            elif self.method == "mtp":
+                self.draft_model_config = self.target_model_config
+                self.draft_parallel_config = None
+                self.speculative_disable_mqa_scorer = None
+                self.speculative_disable_by_batch_size = None
+                self.prompt_lookup_max = None
+                self.prompt_lookup_min = None
+                self.acceptance_method = 'typical_acceptance_sampler'
+                self.posterior_threshold = 0.0
+                self.posterior_alpha = 0.0
+                self.disable_logprobs = False
+                self.disable_log_stats = False
+                self.num_speculative_tokens = 1   #zxp TODO: get from config            
             else:
                 raise ValueError("num_speculative_tokens was provided without "
                                  "speculative model.")
@@ -2501,7 +2514,7 @@ class SpeculativeConfig:
                 )
 
                 # Automatically detect the method
-                if self.method in ('eagle', 'eagle3'):
+                if self.method in ('eagle', 'eagle3', "mtp"):
                     pass
                 elif "eagle-" in self.draft_model_config.model.lower() or \
                         "eagle3-" in self.draft_model_config.model.lower():
@@ -2680,6 +2693,9 @@ class SpeculativeConfig:
             raise ValueError("Expected num_speculative_tokens to be greater "
                              f"than zero ({self.num_speculative_tokens}).")
 
+        if self.method == "mtp":
+            return
+
         if self.draft_model_config:
             self.draft_model_config.verify_with_parallel_config(
                 self.draft_parallel_config)
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 170db3449..1f9413751 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -1340,11 +1340,13 @@ class EngineArgs:
                     is_ngram_enabled = True
                 elif speculative_method in ("eagle", "eagle3"):
                     is_eagle_enabled = True
+                elif speculative_method in ("mtp"):
+                    is_fusion_spec_enabled = True
             else:
                 speculative_model = self.speculative_config.get("model")
                 if speculative_model in ("ngram", "[ngram]"):
                     is_ngram_enabled = True
-            if not (is_ngram_enabled or is_eagle_enabled):
+            if not (is_ngram_enabled or is_eagle_enabled or is_fusion_spec_enabled):
                 # Other speculative decoding methods are not supported yet.
                 _raise_or_fallback(feature_name="Speculative Decoding",
                                    recommend_to_remove=False)
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 0fa86985c..c599f04c8 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -378,7 +378,7 @@ class Scheduler(SchedulerInterface):
                     # We use `request.num_tokens` instead of
                     # `request.num_prompt_tokens` to consider the resumed
                     # requests, which have output tokens.
-                    num_new_tokens = request.num_tokens - num_computed_tokens
+                    num_new_tokens = request.num_tokens_with_spec - num_computed_tokens
                     if (0 < self.scheduler_config.long_prefill_token_threshold
                             < num_new_tokens):
                         num_new_tokens = (
@@ -433,6 +433,15 @@ class Scheduler(SchedulerInterface):
                     structured_output_request_ids[
                         request.request_id] = req_index
                 req_index += 1
+                if request.spec_token_ids:
+                    num_scheduled_spec_tokens = (num_new_tokens +
+                                                    request.num_computed_tokens -
+                                                    request.num_tokens)
+                    if num_scheduled_spec_tokens > 0:
+                        # Trim spec_token_ids list to num_scheduled_spec_tokens.
+                        del request.spec_token_ids[num_scheduled_spec_tokens:]
+                        scheduled_spec_decode_tokens[request.request_id] = (
+                            request.spec_token_ids)
                 self.running.append(request)
                 if self.log_stats:
                     request.record_event(EngineCoreEventType.SCHEDULED,
@@ -975,6 +984,10 @@ class Scheduler(SchedulerInterface):
         if num_computed_tokens == request.num_tokens:
             num_computed_tokens -= 1
 
+        # with spec
+        if self.vllm_config.speculative_config is not None:
+            request.spec_token_ids.append(0)
+
         # very dangerous changes! 
         # self.kv_cache_manager.single_type_manager.cache_blocks(
         #     request,
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 662a532c5..5ca3b5add 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -149,7 +149,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         if self.speculative_config:
             self.use_spec_decode = True
             if get_pp_group().is_last_rank:
-                if self.speculative_config.method == "ngram":
+                if self.speculative_config.method == "mtp":
+                    self.drafter = None
+                elif self.speculative_config.method == "ngram":
                     self.drafter = NgramProposer(self.vllm_config)
                 elif self.speculative_config.use_eagle():
                     self.drafter = EagleProposer(self.vllm_config,
