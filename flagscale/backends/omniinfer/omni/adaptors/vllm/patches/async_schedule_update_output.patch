diff --git a/vllm/config.py b/vllm/config.py
index 09e89c111..28984618d 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -1931,6 +1931,10 @@ class SchedulerConfig:
     This config has no static default. If left unspecified by the user, it will
     be set in `EngineArgs.create_engine_config` based on the usage context."""
 
+    async_schedule: bool = True
+    """enable async schedule and async update_from_output in EngineCore
+    """
+
     max_model_len: int = None  # type: ignore
     """Maximum length of a sequence (including prompt and generated text). This
     is primarily set in `ModelConfig` and that value should be manually
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 170db3449..5b86de679 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -308,6 +308,7 @@ class EngineArgs:
     long_prefill_token_threshold: int = \
         SchedulerConfig.long_prefill_token_threshold
     max_num_seqs: Optional[int] = SchedulerConfig.max_num_seqs
+    async_schedule: bool = SchedulerConfig.async_schedule
     max_logprobs: int = ModelConfig.max_logprobs
     disable_log_stats: bool = False
     revision: Optional[str] = ModelConfig.revision
@@ -788,6 +789,8 @@ class EngineArgs:
             **scheduler_kwargs["max_num_batched_tokens"])
         scheduler_group.add_argument("--max-num-seqs",
                                      **scheduler_kwargs["max_num_seqs"])
+        scheduler_group.add_argument("--async-schedule",
+                                     **scheduler_kwargs["async_schedule"])
         scheduler_group.add_argument(
             "--max-num-partial-prefills",
             **scheduler_kwargs["max_num_partial_prefills"])
@@ -1114,6 +1117,7 @@ class EngineArgs:
             runner_type=model_config.runner_type,
             max_num_batched_tokens=self.max_num_batched_tokens,
             max_num_seqs=self.max_num_seqs,
+            async_schedule=self.async_schedule,
             max_model_len=model_config.max_model_len,
             cuda_graph_sizes=self.cuda_graph_sizes,
             num_lookahead_slots=num_lookahead_slots,
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index c7c36f139..610b234bf 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -213,6 +213,11 @@ class Scheduler(SchedulerInterface):
                      request, request.num_computed_tokens, num_new_tokens,
                      encoder_budget)
 
+            # By wcd, spec need fixup
+            if self.scheduler_config.async_schedule == True:
+                if num_new_tokens == 0:
+                    num_new_tokens = 1 + self.num_spec_tokens
+
             if num_new_tokens == 0:
                 # The request cannot be scheduled because one of the following
                 # reasons:
@@ -1005,5 +1010,5 @@ class Scheduler(SchedulerInterface):
             self.finished_recving_kv_req_ids.add(req_id)
         for req_id in (model_runner_output.finished_sending or ()):
             logger.debug("Finished sending KV transfer for request %s", req_id)
-            if req_id in self.requests:
+            if req_id in self.requests and req_id not in self._cached_reqs_data:
                 self._free_blocks(self.requests[req_id])
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 4d5f4d3d1..d3ed2f782 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -38,6 +38,7 @@ from vllm.v1.request import Request, RequestStatus
 from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder
 from vllm.v1.structured_output import StructuredOutputManager
 from vllm.version import __version__ as VLLM_VERSION
+from concurrent.futures import ThreadPoolExecutor
 
 logger = init_logger(__name__)
 
@@ -94,6 +95,7 @@ class EngineCore:
                 "This scheduler interface is not public and "
                 "compatibility may not be maintained.",
                 vllm_config.scheduler_config.scheduler_cls)
+        self.async_schedule = vllm_config.scheduler_config.async_schedule
 
         self.scheduler: SchedulerInterface = Scheduler(
             vllm_config=vllm_config,
@@ -115,6 +117,12 @@ class EngineCore:
         self.batch_queue_size = self.model_executor.max_concurrent_batches
         self.batch_queue: Optional[queue.Queue[tuple[Future[ModelRunnerOutput],
                                                      SchedulerOutput]]] = None
+
+        self._slow_future = None
+        self._slow_executor = ThreadPoolExecutor(max_workers=1)
+        self.cur_batch = None
+        self.last_batch = None
+
         if self.batch_queue_size > 1:
             logger.info("Batch queue is enabled with size %d",
                         self.batch_queue_size)
@@ -225,6 +233,55 @@ class EngineCore:
 
         return engine_core_outputs
 
+    def step_async(self) -> EngineCoreOutputs:
+        """Schedule, execute, and make output."""
+
+        if not self.scheduler.has_requests():
+            self.cur_batch = None
+            self.last_batch = None
+            if self._slow_future is not None:
+                self._slow_future.cancel()
+            self._slow_future = None
+            return EngineCoreOutputs(
+                outputs=[],
+                scheduler_stats=self.scheduler.make_stats(),
+            )
+
+        engine_core_outputs = EngineCoreOutputs(
+                outputs=[],
+                scheduler_stats=self.scheduler.make_stats(),
+            )
+        self.cur_batch = self.scheduler.schedule()
+
+        output = None
+        if self._slow_future is not None:
+            output = self._slow_future.result()
+            for each_cached_req in self.cur_batch.scheduled_cached_reqs:
+                if each_cached_req.req_id in output.req_ids:
+                    req_id = output.req_id_to_index[each_cached_req.req_id]
+                    new_tokens = output.sampled_token_ids[req_id]
+                    spec_tokens = None
+                    len_spec_tokens = 0
+                    if output.spec_token_ids is not None:
+                        spec_tokens = output.spec_token_ids[req_id]
+                        len_spec_tokens = len(spec_tokens)
+                    if len(each_cached_req.new_token_ids) != self.cur_batch.num_scheduled_tokens[each_cached_req.req_id]:
+                        self.cur_batch.total_num_scheduled_tokens = self.cur_batch.total_num_scheduled_tokens - self.cur_batch.num_scheduled_tokens[each_cached_req.req_id] + len(new_tokens) + len_spec_tokens
+                        self.cur_batch.num_scheduled_tokens[each_cached_req.req_id] = len(new_tokens) + len_spec_tokens
+                        each_cached_req.new_token_ids = new_tokens[:]
+                        if spec_tokens is not None:
+                            self.cur_batch.scheduled_spec_decode_tokens[each_cached_req.req_id] = spec_tokens[:]
+
+        self._slow_future = self._slow_executor.submit(self.model_executor.execute_model, self.cur_batch)
+        time.sleep(0.005) # 5ms
+
+        if output is not None:
+            engine_core_outputs = self.scheduler.update_from_output(self.last_batch, output)
+
+        self.last_batch = self.cur_batch
+        # By wcd, return None or not?
+        return engine_core_outputs
+
     def step_with_batch_queue(self) -> Optional[EngineCoreOutputs]:
         """Schedule and execute batches with the batch queue.
         Note that if nothing to output in this step, None is returned.
@@ -381,6 +438,13 @@ class EngineCoreProc(EngineCore):
 
             self.step_fn = (self.step if self.batch_queue is None else
                             self.step_with_batch_queue)
+            if self.batch_queue is None:
+                if self.async_schedule is True:
+                    self.step_fn = self.step_async
+                else:
+                    self.step_fn = self.step
+            else:
+                self.step_with_batch_queue
             self.engines_running = False
 
             # Send ready message.
