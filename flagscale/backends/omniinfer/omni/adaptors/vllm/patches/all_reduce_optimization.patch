diff --git a/vllm/forward_context.py b/vllm/forward_context.py
index 5d2d95f18..7b92c0d93 100644
--- a/vllm/forward_context.py
+++ b/vllm/forward_context.py
@@ -75,24 +75,35 @@ def set_forward_context(attn_metadata: Any,
     dp_metadata: Optional[DPMetadata] = None
     if vllm_config.parallel_config.data_parallel_size > 1:
         dp_size = vllm_config.parallel_config.data_parallel_size
-        dp_rank = vllm_config.parallel_config.data_parallel_rank
-        if attn_metadata is not None and hasattr(attn_metadata,
-                                                 "num_prefill_tokens"):
-            # for v0 attention backends
-            batchsize = attn_metadata.num_prefill_tokens + \
-                attn_metadata.num_decode_tokens
+        if (not vllm_config.kv_transfer_config) or (not vllm_config.kv_transfer_config.kv_role == "kv_consumer"):
+            dp_rank = vllm_config.parallel_config.data_parallel_rank
+            if attn_metadata is not None and hasattr(attn_metadata,
+                                                    "num_prefill_tokens"):
+                # for v0 attention backends
+                batchsize = attn_metadata.num_prefill_tokens + \
+                    attn_metadata.num_decode_tokens
+            else:
+                # for v1 attention backends or no attn_metadata
+                batchsize = num_tokens
+            num_tokens_across_dp = [0] * dp_size
+            num_tokens_across_dp[dp_rank] = batchsize
+            num_tokens_tensor = torch.tensor(num_tokens_across_dp,
+                                            device="cpu",
+                                            dtype=torch.int32)
+            from vllm.distributed.parallel_state import get_dp_group
+            dist.all_reduce(num_tokens_tensor, group=get_dp_group().cpu_group)
+            max_tokens_across_dp_cpu = torch.max(num_tokens_tensor)
+            cu_tokens_across_dp_cpu = torch.cumsum(num_tokens_tensor, dim=0)
         else:
-            # for v1 attention backends or no attn_metadata
-            batchsize = num_tokens
-        num_tokens_across_dp = [0] * dp_size
-        num_tokens_across_dp[dp_rank] = batchsize
-        num_tokens_tensor = torch.tensor(num_tokens_across_dp,
-                                         device="cpu",
-                                         dtype=torch.int32)
-        from vllm.distributed.parallel_state import get_dp_group
-        dist.all_reduce(num_tokens_tensor, group=get_dp_group().cpu_group)
-        max_tokens_across_dp_cpu = torch.max(num_tokens_tensor)
-        cu_tokens_across_dp_cpu = torch.cumsum(num_tokens_tensor, dim=0)
+            max_num_seqs = vllm_config.scheduler_config.max_num_seqs
+            max_tokens_across_dp_cpu = torch.tensor([max_num_seqs],
+                                                    device="cpu",
+                                                    dtype=torch.int32)
+            cu_tokens_across_dp_cpu = max_num_seqs * torch.arange(1, 
+                                                                  dp_size + 1, 
+                                                                  device="cpu",
+                                                                  dtype=torch.int32)
+
         dp_metadata = DPMetadata(max_tokens_across_dp_cpu,
                                  cu_tokens_across_dp_cpu)
 
