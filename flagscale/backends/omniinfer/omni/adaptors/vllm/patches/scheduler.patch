diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index da18ece75..f13914de2 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -276,6 +276,88 @@ class KVCacheManager:
             num_computed_tokens + num_new_tokens - len(request.spec_token_ids))
 
         return KVCacheBlocks(new_blocks)
+    
+    def allocate_slots_running(
+        self,
+        request: Request,
+        num_new_tokens: int,
+        num_lookahead_tokens: int = 0,
+    ) -> Optional[KVCacheBlocks]:
+        """Add slots for a request with new tokens to append.
+
+        Args:
+            request: The request to allocate slots.
+            num_new_tokens: The number of tokens to allocate, including external
+                tokens. Note that this does not include tokens that have
+                already been computed locally (i.e. new_computed_blocks).
+            num_lookahead_tokens: The number of speculative tokens to allocate.
+                This is used by spec decode proposers with kv-cache such 
+                as eagle.
+
+        Blocks layout:
+        ```
+        -----------------------------------------------------------------------
+        | < computed > | < new computed > |    < new >    | < pre-allocated > |
+        -----------------------------------------------------------------------
+        |                  < required >                   |
+        --------------------------------------------------
+        |                    < full >                  |
+        ------------------------------------------------
+                                          | <new full> |
+                                          --------------
+        ```
+        The following *_blocks are illustrated in this layout.
+
+        Returns:
+            A list of new allocated blocks.
+        """
+        if num_new_tokens == 0:
+            raise ValueError("num_new_tokens must be greater than 0")
+
+        # Free the blocks that are skipped during the attention computation
+        # (e.g., tokens outside the sliding window).
+        # We can do this even if we cannot schedule this request due to
+        # insufficient free blocks.
+        # Should call this function before allocating new blocks to reduce
+        # the number of evicted blocks.
+        num_computed_tokens = request.num_computed_tokens
+        request_id = request.request_id
+
+        self.single_type_manager.remove_skipped_blocks(
+            request_id, num_computed_tokens)
+
+        num_tokens_need_slot = min(
+            num_computed_tokens + num_new_tokens + num_lookahead_tokens,
+            self.max_model_len)
+        num_blocks_to_allocate = (
+            self.single_type_manager.get_num_blocks_to_allocate(
+                request_id=request_id,
+                num_tokens=num_tokens_need_slot,
+                new_computed_blocks=[],
+            ))
+
+        if num_blocks_to_allocate > self.block_pool.get_num_free_blocks():
+            # Cannot allocate new blocks
+            return None
+        
+        if num_tokens_need_slot <= 0:
+            new_blocks = []
+        else:
+            new_blocks = self.single_type_manager.allocate_new_blocks(
+                request_id, num_tokens_need_slot)
+
+        # Update state for locally cached blocks.
+        if not self.enable_caching:
+            return KVCacheBlocks(new_blocks)
+
+        # Speculated tokens might be rejected in the future, so we does
+        # not cache any speculated tokens. We only cache blocks with
+        # generated (accepted) tokens.
+        self.single_type_manager.cache_blocks(
+            request, self.req_to_block_hashes[request_id],
+            num_computed_tokens + num_new_tokens - len(request.spec_token_ids))
+
+        return KVCacheBlocks(new_blocks)
 
     def free(self, request: Request) -> None:
         """Free the blocks allocated for the request.
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 5ad05485e..f7290b1a7 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -228,7 +228,8 @@ class Scheduler(SchedulerInterface):
                 continue
 
             while True:
-                new_blocks = self.kv_cache_manager.allocate_slots(
+                # new_blocks = self.kv_cache_manager.allocate_slots(
+                new_blocks = self.kv_cache_manager.allocate_slots_running(
                     request,
                     num_new_tokens,
                     num_lookahead_tokens=self.num_lookahead_tokens)
@@ -465,7 +466,8 @@ class Scheduler(SchedulerInterface):
             self.waiting.extendleft(skipped_waiting_requests)
 
         # Check if the scheduling constraints are satisfied.
-        total_num_scheduled_tokens = sum(num_scheduled_tokens.values())
+        # total_num_scheduled_tokens = sum(num_scheduled_tokens.values())
+        total_num_scheduled_tokens = self.max_num_scheduled_tokens - token_budget
         assert total_num_scheduled_tokens <= self.max_num_scheduled_tokens
         assert token_budget >= 0
         assert len(self.running) <= self.max_num_running_reqs
@@ -703,36 +705,38 @@ class Scheduler(SchedulerInterface):
             req_index = model_runner_output.req_id_to_index[req_id]
             generated_token_ids = sampled_token_ids[req_index]
 
-            scheduled_spec_token_ids = (
-                scheduler_output.scheduled_spec_decode_tokens.get(req_id))
-            if scheduled_spec_token_ids:
-                # num_computed_tokens represents the number of tokens
-                # processed in the current step, considering scheduled
-                # tokens and rejections. If some tokens are rejected,
-                # num_computed_tokens is decreased by the number of rejected
-                # tokens, where is given by:
-                # len(scheduled_spec_token_ids) + 1 - len(generated_token_ids).
-                num_tokens_rejected = (len(scheduled_spec_token_ids) + 1 -
-                                       len(generated_token_ids))
-                request.num_computed_tokens -= num_tokens_rejected
-                spec_decoding_stats = self.make_spec_decoding_stats(
-                    spec_decoding_stats,
-                    num_draft_tokens=len(scheduled_spec_token_ids),
-                    num_accepted_tokens=len(generated_token_ids) - 1)
-
-            cached_encoder_input_ids = (
-                self.encoder_cache_manager.get_cached_input_ids(request))
-            # OPTIMIZATION: Avoid list(set) if the set is empty.
-            if cached_encoder_input_ids:
-                for input_id in list(cached_encoder_input_ids):
-                    mm_positions = request.mm_positions[input_id]
-                    start_pos = mm_positions.offset
-                    num_tokens = mm_positions.length
-                    if start_pos + num_tokens <= request.num_computed_tokens:
-                        # The encoder output is already processed and stored
-                        # in the decoder's KV cache.
-                        self.encoder_cache_manager.free_encoder_input(
-                            request, input_id)
+            if self.num_spec_tokens:
+                scheduled_spec_token_ids = (
+                    scheduler_output.scheduled_spec_decode_tokens.get(req_id))
+                if scheduled_spec_token_ids:
+                    # num_computed_tokens represents the number of tokens
+                    # processed in the current step, considering scheduled
+                    # tokens and rejections. If some tokens are rejected,
+                    # num_computed_tokens is decreased by the number of rejected
+                    # tokens, where is given by:
+                    # len(scheduled_spec_token_ids) + 1 - len(generated_token_ids).
+                    num_tokens_rejected = (len(scheduled_spec_token_ids) + 1 -
+                                        len(generated_token_ids))
+                    request.num_computed_tokens -= num_tokens_rejected
+                    spec_decoding_stats = self.make_spec_decoding_stats(
+                        spec_decoding_stats,
+                        num_draft_tokens=len(scheduled_spec_token_ids),
+                        num_accepted_tokens=len(generated_token_ids) - 1)
+
+            if self.max_num_encoder_input_tokens:
+                cached_encoder_input_ids = (
+                    self.encoder_cache_manager.get_cached_input_ids(request))
+                # OPTIMIZATION: Avoid list(set) if the set is empty.
+                if cached_encoder_input_ids:
+                    for input_id in list(cached_encoder_input_ids):
+                        mm_positions = request.mm_positions[input_id]
+                        start_pos = mm_positions.offset
+                        num_tokens = mm_positions.length
+                        if start_pos + num_tokens <= request.num_computed_tokens:
+                            # The encoder output is already processed and stored
+                            # in the decoder's KV cache.
+                            self.encoder_cache_manager.free_encoder_input(
+                                request, input_id)
 
             stopped = False
             new_logprobs = None
@@ -743,11 +747,12 @@ class Scheduler(SchedulerInterface):
             # a request is still being prefilled, we expect the model runner
             # to return empty token ids for the request.
             for num_new, output_token_id in enumerate(new_token_ids, 1):
-                request.append_output_token_ids(output_token_id)
+                # request.append_output_token_ids(output_token_id)
 
                 # Check for stop and update request state.
                 # This must be called before we make the EngineCoreOutput.
-                stopped = check_stop(request, self.max_model_len)
+                # stopped = check_stop(request, self.max_model_len)
+                stopped = request.append_output_token_id_modify(output_token_id, self.max_model_len)
                 if stopped:
                     kv_transfer_params = self._free_request(request)
                     del new_token_ids[num_new:]  # Trim new tokens if needed.
diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index d2843b65a..cea3bc936 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -110,6 +110,27 @@ class Request:
             self._output_token_ids.extend(token_ids)
             self._all_token_ids.extend(token_ids)
 
+    def append_output_token_id_modify(
+        self,
+        token_id: int,
+        max_model_len: int
+    ) -> bool:
+        self._output_token_ids.append(token_id)
+        self._all_token_ids.append(token_id)
+        if (len(self._all_token_ids) >= max_model_len
+            or len(self._output_token_ids) >= self.max_tokens):
+                self.status = RequestStatus.FINISHED_LENGTH_CAPPED
+                return True
+        if (not self.sampling_params.ignore_eos
+            and token_id == self.eos_token_id):
+            self.status = RequestStatus.FINISHED_STOPPED
+            return True
+        if token_id in (self.sampling_params.stop_token_ids or ()):
+            self.status = RequestStatus.FINISHED_STOPPED
+            self.stop_reason = token_id
+            return True
+        return False
+
     @property
     def num_tokens(self) -> int:
         return len(self._all_token_ids)
