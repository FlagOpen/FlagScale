diff --git a/.gitignore b/.gitignore
index 2756c612b..3ef7c48ab 100644
--- a/.gitignore
+++ b/.gitignore
@@ -206,3 +206,5 @@ shellcheck*/
 
 # Ingore moe/marlin_moe gen code
 csrc/moe/marlin_moe_wna16/kernel_*
+
+*.rej
\ No newline at end of file
diff --git a/vllm/config.py b/vllm/config.py
index dd0791537..4dd7097ce 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -1803,6 +1803,7 @@ class ParallelConfig:
         if self.data_parallel_size > 1:
             # Data parallel was specified in the engine args.
             self.data_parallel_master_port = get_open_port()
+            self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP
             # TODO multi-node
         else:
             # Otherwise fall back to env vars (e.g. for offline SPMD case).
@@ -1822,7 +1823,8 @@ class ParallelConfig:
         ray_only_devices: list[str] = []
         from vllm.platforms import current_platform
         if (current_platform.device_type in ray_only_devices
-                and self.world_size > 1):
+                # and self.world_size > 1):
+                and self.world_size_across_dp > 1):
             if self.distributed_executor_backend is None:
                 self.distributed_executor_backend = "ray"
             if self.distributed_executor_backend != "ray":
@@ -1830,7 +1832,8 @@ class ParallelConfig:
                     f"{current_platform.device_type.upper()} backend only "
                     "supports Ray for distributed inference.")
 
-        if self.distributed_executor_backend is None and self.world_size > 1:
+        # if self.distributed_executor_backend is None and self.world_size > 1:
+        if self.distributed_executor_backend is None and self.world_size_across_dp > 1:
             # We use multiprocessing by default if world_size fits on the
             # current node and we aren't in a ray placement group.
 
@@ -1861,7 +1864,8 @@ class ParallelConfig:
             logger.info("Defaulting to use %s for distributed inference",
                         backend)
 
-        if self.distributed_executor_backend is None and self.world_size == 1:
+        # if self.distributed_executor_backend is None and self.world_size == 1:
+        if self.distributed_executor_backend is None and self.world_size_across_dp == 1:
             self.distributed_executor_backend = "uni"
 
         self._verify_args()
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index cb9658ce1..da1f34fdc 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -879,10 +879,13 @@ def init_distributed_environment(
         world_size = parallel_config.world_size_across_dp
         ip = parallel_config.data_parallel_master_ip
         port = parallel_config.get_next_dp_init_port()
+        # port = parallel_config.data_parallel_master_port
         distributed_init_method = f"tcp://{ip}:{port}"  # noqa
         logger.info(
             "Adjusting world_size=%d rank=%d distributed_init_method=%s for DP",
             world_size, rank, distributed_init_method)
+            # "Adjusting world_size=%d rank=%d local_rank=%d distributed_init_method=%s for DP",
+            # world_size, rank, local_rank, distributed_init_method)
     if not torch.distributed.is_initialized():
         assert distributed_init_method is not None, (
             "distributed_init_method must be provided when initializing "
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index 9b0b98731..56bb317fb 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -318,7 +318,8 @@ class RayDistributedExecutor(DistributedExecutorBase):
         n_ips = len(all_ips)
         n_nodes = len(node_workers)
 
-        if n_nodes != n_ips:
+        # if n_nodes != n_ips:
+        if self.parallel_config.data_parallel_size <= 1 and n_nodes != n_ips:
             raise RuntimeError(
                 f"Every node should have a unique IP address. Got {n_nodes}"
                 f" nodes with node ids {list(node_workers.keys())} and "
diff --git a/vllm/executor/ray_utils.py b/vllm/executor/ray_utils.py
index 37cc07bfb..386b95375 100644
--- a/vllm/executor/ray_utils.py
+++ b/vllm/executor/ray_utils.py
@@ -114,7 +114,9 @@ try:
                     pass
                 else:
                     import torch
-                    torch.cuda.set_device(self.worker.device)
+                    # torch.cuda.set_device(self.worker.device)
+                    import torch_npu
+                    torch.npu.set_device(self.worker.device)
 
                 self.compiled_dag_cuda_device_set = True
 
@@ -184,7 +186,8 @@ def _verify_bundles(placement_group: "PlacementGroup",
         node_id_to_bundle[node_id].append(bundles[bundle_idx])
     driver_node_id = ray.get_runtime_context().get_node_id()
 
-    if driver_node_id not in node_id_to_bundle:
+    # if driver_node_id not in node_id_to_bundle:
+    if parallel_config.data_parallel_size <= 1 and (driver_node_id not in node_id_to_bundle):
         raise RuntimeError(
             f"driver node id {driver_node_id} is not included in a placement "
             f"group {placement_group.id}. Node id -> bundles "
@@ -351,8 +354,46 @@ def initialize_ray_cluster(
         # vLLM engine is also a worker to execute model with an accelerator,
         # so it requires to have the device in a current node. Check if
         # the current node has at least one device.
-        current_ip = get_ip()
-        current_node_id = ray.get_runtime_context().get_node_id()
+        # current_ip = get_ip()
+        # current_node_id = ray.get_runtime_context().get_node_id()
+        head_ip = get_ip()
+        def sort_by_driver_then_worker_ip(ip_and_id):
+            ip = ip_and_id[0]
+            return (0 if ip == head_ip else 1, ip)
+
+        def _get_gpu_mapping():
+            nodes = ray.nodes()
+            node_gpu_mapping = []
+            for node_info in nodes:
+                if node_info.get("alive", False):
+                    node_id = node_info["NodeID"]
+                    ip = node_info["NodeManagerAddress"]
+                    num_gpus = node_info["Resources"].get("GPU", 0)
+                    node_gpu_mapping.append((ip, node_id, int(num_gpus)))
+            node_gpu_mapping = sorted(node_gpu_mapping, key=sort_by_driver_then_worker_ip)
+            return node_gpu_mapping
+        def _find_target_gpu(node_gpu_mapping, dp_rank):
+            accumulated_gpus = 0
+            world_size = parallel_config.world_size
+            logger.info(f"for initialize_ray_cluster, world_size = {world_size}")
+            for ip, node_id, num_gpus in node_gpu_mapping:
+                if dp_rank * world_size < accumulated_gpus + num_gpus:
+                    gpu_index = dp_rank * world_size - accumulated_gpus
+                    return (ip, node_id, gpu_index)
+                accumulated_gpus += num_gpus
+            raise ValueError(f"dp_rank {dp_rank} exceeds the num_gpus {accumulated_gpus}")
+        
+        node_gpu_mapping = _get_gpu_mapping()
+        current_dp_rank = parallel_config.data_parallel_rank
+        selected_node_ip, selected_node_id, gpu_index = _find_target_gpu(node_gpu_mapping, current_dp_rank)
+        logger.debug(f"selected_node_ip = {selected_node_ip}, selected_node_id = {selected_node_id}, gpu_index = {gpu_index}")
+
+        current_ip = selected_node_ip
+        current_node_id = selected_node_id
+
+        # current_ip = get_ip()
+        # current_node_id = ray.get(get_remote_node_id.remote())
+
         current_node_resource = available_resources_per_node()[current_node_id]
         if current_node_resource.get(device_str, 0) < 1:
             raise ValueError(
diff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py
index 0366895ef..1e07010ec 100644
--- a/vllm/model_executor/models/deepseek_v2.py
+++ b/vllm/model_executor/models/deepseek_v2.py
@@ -780,6 +780,8 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP):
                 if is_pp_missing_parameter(name, self):
                     continue
 
+                if name not in params_dict:
+                    continue
                 param = params_dict[name]
                 weight_loader = param.weight_loader
                 weight_loader(param, loaded_weight, shard_id)
@@ -794,6 +796,8 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP):
                     if is_pp_missing_parameter(name, self):
                         continue
 
+                    if name not in params_dict:
+                        continue
                     param = params_dict[name]
                     weight_loader = param.weight_loader
                     weight_loader(param,
@@ -815,6 +819,8 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP):
                     if is_pp_missing_parameter(name, self):
                         continue
 
+                    if name not in params_dict:
+                        continue
                     param = params_dict[name]
                     weight_loader = getattr(param, "weight_loader",
                                             default_weight_loader)
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 5a493db8a..55bbc2178 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -631,12 +631,18 @@ class DPEngineCoreProc(EngineCoreProc):
         assert 0 <= local_dp_rank <= dp_rank < dp_size
 
         from vllm.platforms import current_platform
-        device_control_env_var = current_platform.device_control_env_var
-        tp_size = vllm_config.parallel_config.tensor_parallel_size
-        os.environ[device_control_env_var] = ",".join(
-            str(current_platform.device_id_to_physical_device_id(i))
-            for i in range(local_dp_rank * tp_size, (local_dp_rank + 1) *
-                           tp_size))
+        if current_platform.is_cuda_alike():
+            tp_size = vllm_config.parallel_config.tensor_parallel_size
+            os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(
+                str(current_platform.device_id_to_physical_device_id(i))
+                for i in range(local_dp_rank * tp_size, (local_dp_rank + 1) *
+                               tp_size))
+        # device_control_env_var = current_platform.device_control_env_var
+        # tp_size = vllm_config.parallel_config.tensor_parallel_size
+        # os.environ[device_control_env_var] = ",".join(
+        #     str(current_platform.device_id_to_physical_device_id(i))
+        #     for i in range(local_dp_rank * tp_size, (local_dp_rank + 1) *
+        #                    tp_size))
 
         self.local_dp_rank = local_dp_rank
         self.dp_group = vllm_config.parallel_config.stateless_init_dp_group()
diff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py
index c33317edc..86999ae97 100644
--- a/vllm/v1/engine/core_client.py
+++ b/vllm/v1/engine/core_client.py
@@ -398,6 +398,7 @@ class MPClient(EngineCoreClient):
             self.resources.input_socket = self.input_socket
 
             new_core_engine = lambda index, local_dp_rank=None: CoreEngine(
+            # new_core_engine = lambda index, local_dp_rank: CoreEngine(
                 vllm_config, executor_class, log_stats, input_path, self.
                 output_path, index, local_dp_rank)
 
