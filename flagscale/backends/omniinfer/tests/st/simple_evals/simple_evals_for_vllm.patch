diff --git a/common.py b/common.py
index b6b4c0e..af7d01d 100644
--- a/common.py
+++ b/common.py
@@ -9,7 +9,7 @@ import numpy as np
 import requests
 from tqdm import tqdm
 
-from .types import EvalResult, Message, SamplerBase, SingleEvalResult
+from eval_types import EvalResult, Message, SamplerBase, SingleEvalResult
 
 QUERY_TEMPLATE_MULTICHOICE = """
 Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.
@@ -175,7 +175,7 @@ def _compute_stat(values: list, stat: str):
 def aggregate_results(
     single_eval_results: list[SingleEvalResult],
     default_stats: tuple[str] = ("mean", "std"),
-    name2stats: dict[str, tuple[str]] | None = None,
+    name2stats: dict[str, tuple[str]] = None,
 ) -> EvalResult:
     """
     Aggregate results from multiple evaluations into a single EvalResult.
diff --git a/drop_eval.py b/drop_eval.py
index 27918e5..4e5660a 100644
--- a/drop_eval.py
+++ b/drop_eval.py
@@ -3,7 +3,7 @@ DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragr
 Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner
 https://arxiv.org/abs/1903.00161
 """
-
+import os
 import gzip
 import json
 import random
@@ -14,9 +14,9 @@ from typing import Any, Dict, List, Optional, Set, Tuple, Union
 import numpy as np
 from scipy.optimize import linear_sum_assignment
 
-from . import common
-from .common import ANSWER_PATTERN, HTML_JINJA
-from .types import Eval, EvalResult, SamplerBase, SingleEvalResult
+import common
+from common import ANSWER_PATTERN, HTML_JINJA
+from eval_types import Eval, EvalResult, SamplerBase, SingleEvalResult
 
 """
 From here through _normalize_answer was originally copied from:
@@ -234,19 +234,16 @@ def drop_metric(sample: str, reference: list[str]) -> Tuple[float, float]:
 
 
 class DropEval(Eval):
-    def __init__(self, num_examples: int | None = None, train_samples_per_prompt: int = 3):
+    def __init__(self, num_examples: int = None, train_samples_per_prompt: int = 3, num_threads: int = 50, dataset_path: str=None):
         self.seed = 42
         self._num_examples = num_examples
+        self.num_threads = num_threads
         self._train_samples_per_prompt = train_samples_per_prompt
-        self.train_jsonl = (
-            "https://openaipublic.blob.core.windows.net/simple-evals/drop_v0_train.jsonl.gz"
-        )
-        self.test_jsonl = (
-            "https://openaipublic.blob.core.windows.net/simple-evals/drop_v0_dev.jsonl.gz"
-        )
-        with gzip.GzipFile(fileobj=common.url_to_fileobj(self.train_jsonl, binary=True), mode="rb") as f:
+        self.train_jsonl = os.path.join(dataset_path, "drop", "drop_v0_train.jsonl.gz")
+        self.test_jsonl = os.path.join(dataset_path, "drop", "drop_v0_dev.jsonl.gz")
+        with gzip.GzipFile(self.train_jsonl, mode="rb") as f:
             self.train_samples = list(map(json.loads, f.readlines()))
-        with gzip.GzipFile(fileobj=common.url_to_fileobj(self.test_jsonl, binary=True), mode="rb") as f:
+        with gzip.GzipFile(self.test_jsonl, mode="rb") as f:
             self.test_samples = list(map(json.loads, f.readlines()))
             if self._num_examples:
                 self.test_samples = random.Random(self.seed).sample(
@@ -307,5 +304,5 @@ Think step by step, then write a line of the form "Answer: $ANSWER" at the end o
                         metrics={"em_score": em_score, "f1_score": f1_score},
                     )
 
-        results = common.map_with_progress(fn, self.test_samples)
+        results = common.map_with_progress(fn, self.test_samples, num_threads=self.num_threads)
         return common.aggregate_results(results)
diff --git a/gpqa_eval.py b/gpqa_eval.py
index 21c717e..57ee8d1 100644
--- a/gpqa_eval.py
+++ b/gpqa_eval.py
@@ -6,12 +6,13 @@ https://arxiv.org/abs/2311.12022
 
 import random
 import re
+import os
 
 import pandas
 
-from . import common
-from .common import ANSWER_PATTERN_MULTICHOICE, HTML_JINJA, format_multichoice_question
-from .types import Eval, EvalResult, MessageList, SamplerBase, SingleEvalResult
+import common
+from common import ANSWER_PATTERN_MULTICHOICE, HTML_JINJA, format_multichoice_question
+from eval_types import Eval, EvalResult, MessageList, SamplerBase, SingleEvalResult
 
 
 class GPQAEval(Eval):
@@ -19,10 +20,13 @@ class GPQAEval(Eval):
         self,
         n_repeats: int = 4,
         variant: str = "diamond",
-        num_examples: int | None = None,  # restrict to a subset of the data for debugging
+        num_examples: int = None,  # restrict to a subset of the data for debugging,
+        num_threads: int = 50,
+        dataset_path: str = None
     ):
+        dataset_path = os.path.join(dataset_path, "gpqa", "gpqa_diamond.csv")
         df = pandas.read_csv(
-            f"https://openaipublic.blob.core.windows.net/simple-evals/gpqa_{variant}.csv"
+            dataset_path
         )
         examples = [row.to_dict() for _, row in df.iterrows()]
         rng = random.Random(0)
@@ -33,6 +37,7 @@ class GPQAEval(Eval):
         examples = [example | {"permutation": rng.sample(range(4), 4)} for example in examples]
         self.examples = examples
         self.n_repeats = n_repeats
+        self.num_threads = num_threads
 
     def __call__(self, sampler: SamplerBase) -> EvalResult:
         def fn(row: dict):
@@ -69,5 +74,5 @@ class GPQAEval(Eval):
                 html=html, score=score, convo=convo, metrics={"chars": len(response_text)}
             )
 
-        results = common.map_with_progress(fn, self.examples)
+        results = common.map_with_progress(fn, self.examples, num_threads=self.num_threads)
         return common.aggregate_results(results)
diff --git a/humaneval_eval.py b/humaneval_eval.py
index 75eab56..0be8812 100644
--- a/humaneval_eval.py
+++ b/humaneval_eval.py
@@ -18,9 +18,9 @@ from human_eval.data import HUMAN_EVAL, read_problems
 from human_eval.evaluation import estimate_pass_at_k
 from human_eval.execution import check_correctness  # , unsafe_execute
 
-from . import common
-from .common import HTML_JINJA
-from .types import Eval, EvalResult, SamplerBase, SingleEvalResult
+import common
+from common import HTML_JINJA
+from eval_types import Eval, EvalResult, SamplerBase, SingleEvalResult
 
 
 def evaluate_functional_correctness(
diff --git a/mgsm_eval.py b/mgsm_eval.py
index 81ee203..ad3ab53 100644
--- a/mgsm_eval.py
+++ b/mgsm_eval.py
@@ -4,31 +4,18 @@ Language Models are Multilingual Chain-of-Thought Reasoners
 Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei
 https://arxiv.org/abs/2210.03057 reference: https://github.com/google-research/url-nlp 
 """
-
+import os
 import re
 from typing import Optional
 
-from . import common
-from .mmlu_eval import HTML_JINJA
-from .types import Eval, EvalResult, SamplerBase, SingleEvalResult
+import common
+from mmlu_eval import HTML_JINJA
+from eval_types import Eval, EvalResult, SamplerBase, SingleEvalResult
 
 ALL_LANGUAGES = ["bn", "de", "en", "es", "fr", "ja", "ru", "sw", "te", "th", "zh"]
 LATIN_LANGUAGES = ["de", "en", "es", "fr", "sw"]
 NON_LATIN_LANGUAGES = ["bn", "ja", "ru", "te", "th", "zh"]
 
-LANG_TO_FPATH = {
-    "bn": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_bn.tsv",
-    "de": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_de.tsv",
-    "en": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_en.tsv",
-    "es": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_es.tsv",
-    "fr": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_fr.tsv",
-    "ja": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_ja.tsv",
-    "ru": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_ru.tsv",
-    "sw": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_sw.tsv",
-    "te": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_te.tsv",
-    "th": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_th.tsv",
-    "zh": "https://openaipublic.blob.core.windows.net/simple-evals/mgsm_zh.tsv",
-}
 LANG_TO_INSTRUCTIONS = {
     "en": """Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of "Answer:". Do not add anything other than the integer answer after "Answer:".
 
@@ -104,10 +91,24 @@ def score_mgsm(target: str, prediction: str) -> bool:
     return target == prediction
 
 
-def get_lang_examples(lang: str) -> list[dict[str, str]]:
+def get_lang_examples(lang: str, dataset_path: str) -> list[dict[str, str]]:
+
+    LANG_TO_FPATH = {
+        "bn": os.path.join(dataset_path, "mgsm", "mgsm_bn.tsv"),
+        "de": os.path.join(dataset_path, "mgsm", "mgsm_de.tsv"),
+        "en": os.path.join(dataset_path, "mgsm", "mgsm_en.tsv"),
+        "es": os.path.join(dataset_path, "mgsm", "mgsm_es.tsv"),
+        "fr": os.path.join(dataset_path, "mgsm", "mgsm_fr.tsv"),
+        "ja": os.path.join(dataset_path, "mgsm", "mgsm_ja.tsv"),
+        "ru": os.path.join(dataset_path, "mgsm", "mgsm_ru.tsv"),
+        "sw": os.path.join(dataset_path, "mgsm", "mgsm_sw.tsv"),
+        "te": os.path.join(dataset_path, "mgsm", "mgsm_te.tsv"),
+        "th": os.path.join(dataset_path, "mgsm", "mgsm_th.tsv"),
+        "zh": os.path.join(dataset_path, "mgsm", "mgsm_zh.tsv"),
+    }
     fpath = LANG_TO_FPATH[lang]
     examples = []
-    with common.url_to_fileobj(fpath, binary=False) as f:
+    with open(fpath, 'rt', encoding='utf-8') as f:
         for line in f:
             inputs, targets = line.strip().split("\t")
             if "." in targets:
@@ -131,6 +132,8 @@ class MGSMEval(Eval):
         self,
         num_examples_per_lang: int = 250,  # restrict to a subset of the data for debugging
         languages: Optional[list[str]] = ALL_LANGUAGES,
+        num_threads: int = 50,
+        dataset_path: str=None
     ):
         if languages is None:
             languages = ALL_LANGUAGES
@@ -146,9 +149,10 @@ class MGSMEval(Eval):
 
         examples = []
         for lang in self._languages:
-            lang_examples = get_lang_examples(lang)
+            lang_examples = get_lang_examples(lang, dataset_path)
             examples.extend(lang_examples[: self._num_examples_per_lang])
         self.examples = examples
+        self.num_threads = num_threads
 
     def __call__(self, sampler: SamplerBase) -> EvalResult:
         def fn(example: dict[str, str]):
@@ -185,5 +189,5 @@ class MGSMEval(Eval):
                 metrics={language: score, latin_language: score},
             )
 
-        results = common.map_with_progress(fn, self.examples)
+        results = common.map_with_progress(fn, self.examples, num_threads=self.num_threads)
         return common.aggregate_results(results, default_stats=("mean", "std"))
diff --git a/mmlu_eval.py b/mmlu_eval.py
index 9423c66..34bf133 100644
--- a/mmlu_eval.py
+++ b/mmlu_eval.py
@@ -6,11 +6,12 @@ https://arxiv.org/abs/2009.03300
 
 import random
 import re
+import os
 
 import pandas
 
-from . import common
-from .common import (
+import common
+from common import (
     HTML_JINJA,
     MULTILINGUAL_ANSWER_PATTERN_TEMPLATE,
     MULTILINGUAL_ANSWER_REGEXES,
@@ -18,7 +19,7 @@ from .common import (
     normalize_extracted_answer,
     normalize_response,
 )
-from .types import Eval, EvalResult, SamplerBase, SingleEvalResult
+from eval_types import Eval, EvalResult, SamplerBase, SingleEvalResult
 
 subject2category = {
     "abstract_algebra": "stem",
@@ -82,16 +83,14 @@ subject2category = {
 
 
 class MMLUEval(Eval):
-    def __init__(self, num_examples: int | None = None, language: str = "EN-US"):
-        if language != "EN-US":
-            url = f"https://openaipublic.blob.core.windows.net/simple-evals/mmlu_{language}.csv"
-        else:
-            url = "https://openaipublic.blob.core.windows.net/simple-evals/mmlu.csv"
-        df = pandas.read_csv(url)
+    def __init__(self, num_examples: int = None, language: str = "EN-US", num_threads: int = 50, dataset_path: str=None):
+        dataset_path = os.path.join(dataset_path, "mmlu", "mmlu.csv")
+        df = pandas.read_csv(dataset_path)
         examples = [row.to_dict() for _, row in df.iterrows()]
         if num_examples:
             examples = random.Random(0).sample(examples, num_examples)
         self.examples = examples
+        self.num_threads = num_threads
 
     def __call__(self, sampler: SamplerBase) -> EvalResult:
         def fn(row: dict):
@@ -122,5 +121,5 @@ class MMLUEval(Eval):
                 html=html, score=score, metrics={category: score}, convo=convo
             )
 
-        results = common.map_with_progress(fn, self.examples)
+        results = common.map_with_progress(fn, self.examples, num_threads=self.num_threads)
         return common.aggregate_results(results)
diff --git a/sampler/chat_completion_sampler.py b/sampler/chat_completion_sampler.py
index d75ce91..33c8c92 100644
--- a/sampler/chat_completion_sampler.py
+++ b/sampler/chat_completion_sampler.py
@@ -5,7 +5,7 @@ from typing import Any
 import openai
 from openai import OpenAI
 
-from ..types import MessageList, SamplerBase
+from eval_types import MessageList, SamplerBase
 
 OPENAI_SYSTEM_MESSAGE_API = "You are a helpful assistant."
 OPENAI_SYSTEM_MESSAGE_CHATGPT = (
@@ -21,13 +21,14 @@ class ChatCompletionSampler(SamplerBase):
 
     def __init__(
         self,
-        model: str = "gpt-3.5-turbo",
-        system_message: str | None = None,
+        model: str = "deepseek",
+        system_message: str = None,
         temperature: float = 0.5,
-        max_tokens: int = 1024,
+        max_tokens: int = 2048,
+        openai_api_base: str = None,
     ):
         self.api_key_name = "OPENAI_API_KEY"
-        self.client = OpenAI()
+        self.client = OpenAI(base_url=openai_api_base, api_key="EMPTY")
         # using api_key=os.environ.get("OPENAI_API_KEY")  # please set your API_KEY
         self.model = model
         self.system_message = system_message
diff --git a/simple_evals.py b/simple_evals.py
index eab807f..f4db626 100644
--- a/simple_evals.py
+++ b/simple_evals.py
@@ -1,21 +1,16 @@
 import json
 import argparse
 import pandas as pd
-from . import common
-from .drop_eval import DropEval
-from .gpqa_eval import GPQAEval
-from .humaneval_eval import HumanEval
-from .math_eval import MathEval
-from .mgsm_eval import MGSMEval
-from .mmlu_eval import MMLUEval
-from .simpleqa_eval import SimpleQAEval
-from .sampler.chat_completion_sampler import (
-    OPENAI_SYSTEM_MESSAGE_API,
-    OPENAI_SYSTEM_MESSAGE_CHATGPT,
+import common
+import datetime
+from drop_eval import DropEval
+from gpqa_eval import GPQAEval
+from humaneval_eval import HumanEval
+from mgsm_eval import MGSMEval
+from mmlu_eval import MMLUEval
+from sampler.chat_completion_sampler import (
     ChatCompletionSampler,
 )
-from .sampler.o_chat_completion_sampler import OChatCompletionSampler
-from .sampler.claude_sampler import ClaudeCompletionSampler, CLAUDE_SYSTEM_MESSAGE_LMSYS
 
 
 def main():
@@ -25,74 +20,27 @@ def main():
     parser.add_argument(
         "--list-models", action="store_true", help="List available models"
     )
-    parser.add_argument("--model", type=str, help="Select a model by name")
+    parser.add_argument("--model", type=str, help="Select a model by name",default="deepseek")
+    parser.add_argument("--served-model-name", type=str, help="Select a served model name", default="deepseek")
+    parser.add_argument("--max-tokens", type=int, help="max tokens", default=2048)
+    parser.add_argument("--temperature", type=float, help="temperature", default=0.5)
+    parser.add_argument("--num-threads", type=int, help="num threads", default=50)
+    parser.add_argument("--url", type=str, help="url", default="http://localhost:8192/v1")
     parser.add_argument("--debug", action="store_true", help="Run in debug mode")
+    parser.add_argument("--dataset", nargs='+', type=str, help="eval dataset")
     parser.add_argument(
         "--examples", type=int, help="Number of examples to use (overrides default)"
     )
+    parser.add_argument("--dataset-path", type=str, help="eval dataset path", default="./dataset")
 
     args = parser.parse_args()
 
     models = {
-        # chatgpt models:
-        "gpt-4o-2024-11-20_assistant": ChatCompletionSampler(
-            model="gpt-4o-2024-11-20",
-            system_message=OPENAI_SYSTEM_MESSAGE_API,
-            max_tokens=2048,
-        ),
-        "gpt-4o-2024-11-20_chatgpt": ChatCompletionSampler(
-            model="gpt-4o-2024-11-20",
-            system_message=OPENAI_SYSTEM_MESSAGE_CHATGPT,
-            max_tokens=2048,
-        ),
-        "o1": OChatCompletionSampler(
-            model="o1",
-        ),
-        "o1-preview": OChatCompletionSampler(
-            model="o1-preview",
-        ),
-        "o1-mini": OChatCompletionSampler(
-            model="o1-mini",
-        ),
-        # Default == Medium
-        "o3-mini": OChatCompletionSampler(
-            model="o3-mini",
-        ),
-        "o3-mini_high": OChatCompletionSampler(
-            model="o3-mini",
-            reasoning_effort="high",
-        ),
-        "o3-mini_low": OChatCompletionSampler(
-            model="o3-mini",
-            reasoning_effort="low",
-        ),
-        "gpt-4-turbo-2024-04-09_assistant": ChatCompletionSampler(
-            model="gpt-4-turbo-2024-04-09",
-            system_message=OPENAI_SYSTEM_MESSAGE_API,
-        ),
-        "gpt-4-turbo-2024-04-09_chatgpt": ChatCompletionSampler(
-            model="gpt-4-turbo-2024-04-09",
-            system_message=OPENAI_SYSTEM_MESSAGE_CHATGPT,
-        ),
-        "gpt-4o_assistant": ChatCompletionSampler(
-            model="gpt-4o",
-            system_message=OPENAI_SYSTEM_MESSAGE_API,
-            max_tokens=2048,
-        ),
-        "gpt-4o_chatgpt": ChatCompletionSampler(
-            model="gpt-4o",
-            system_message=OPENAI_SYSTEM_MESSAGE_CHATGPT,
-            max_tokens=2048,
-        ),
-        "gpt-4o-mini-2024-07-18": ChatCompletionSampler(
-            model="gpt-4o-mini-2024-07-18",
-            system_message=OPENAI_SYSTEM_MESSAGE_API,
-            max_tokens=2048,
-        ),
-        # claude models:
-        "claude-3-opus-20240229_empty": ClaudeCompletionSampler(
-            model="claude-3-opus-20240229",
-            system_message=CLAUDE_SYSTEM_MESSAGE_LMSYS,
+        "deepseek": ChatCompletionSampler(
+            model=args.served_model_name,
+            temperature=args.temperature,
+            max_tokens=args.max_tokens,
+            openai_api_base=args.url
         ),
     }
 
@@ -108,8 +56,8 @@ def main():
             return
         models = {args.model: models[args.model]}
 
-    grading_sampler = ChatCompletionSampler(model="gpt-4o")
-    equality_checker = ChatCompletionSampler(model="gpt-4-turbo-preview")
+    # grading_sampler = ChatCompletionSampler(model="gpt-4o")
+    # equality_checker = ChatCompletionSampler(model="gpt-4-turbo-preview")
     # ^^^ used for fuzzy matching, just for math
 
     def get_evals(eval_name, debug_mode):
@@ -117,39 +65,27 @@ def main():
             args.examples if args.examples is not None else (5 if debug_mode else None)
         )
         # Set num_examples = None to reproduce full evals
-        match eval_name:
-            case "mmlu":
-                return MMLUEval(num_examples=1 if debug_mode else num_examples)
-            case "math":
-                return MathEval(
-                    equality_checker=equality_checker,
-                    num_examples=num_examples,
-                    n_repeats=1 if debug_mode else 10,
-                )
-            case "gpqa":
-                return GPQAEval(
-                    n_repeats=1 if debug_mode else 10, num_examples=num_examples
-                )
-            case "mgsm":
-                return MGSMEval(num_examples_per_lang=10 if debug_mode else 250)
-            case "drop":
-                return DropEval(
-                    num_examples=10 if debug_mode else num_examples,
-                    train_samples_per_prompt=3,
-                )
-            case "humaneval":
-                return HumanEval(num_examples=10 if debug_mode else num_examples)
-            case "simpleqa":
-                return SimpleQAEval(
-                    grader_model=grading_sampler,
-                    num_examples=10 if debug_mode else num_examples,
-                )
-            case _:
-                raise Exception(f"Unrecognized eval type: {eval_name}")
+        if eval_name == "mmlu":
+            return MMLUEval(num_examples=1 if debug_mode else num_examples, num_threads=args.num_threads, dataset_path=args.dataset_path)
+        elif eval_name == "gpqa":
+            return GPQAEval(
+                n_repeats=1 if debug_mode else 10, num_examples=num_examples, num_threads=args.num_threads, dataset_path=args.dataset_path
+            )
+        elif eval_name == "mgsm":
+            return MGSMEval(num_examples_per_lang=10 if debug_mode else 250, num_threads=args.num_threads, dataset_path=args.dataset_path)
+        elif eval_name == "drop":
+            return DropEval(
+                num_examples=10 if debug_mode else num_examples,
+                train_samples_per_prompt=3, num_threads=args.num_threads, dataset_path=args.dataset_path
+            )
+        elif eval_name == "humaneval":
+            return HumanEval(num_examples=10 if debug_mode else num_examples)
+        else:
+            raise Exception(f"Unrecognized eval type: {eval_name}")
 
     evals = {
         eval_name: get_evals(eval_name, args.debug)
-        for eval_name in ["simpleqa", "mmlu", "math", "gpqa", "mgsm", "drop", "humaneval"]
+        for eval_name in args.dataset
     }
     print(evals)
     debug_suffix = "_DEBUG" if args.debug else ""
@@ -159,18 +95,20 @@ def main():
         for eval_name, eval_obj in evals.items():
             result = eval_obj(sampler)
             # ^^^ how to use a sampler
-            file_stem = f"{eval_name}_{model_name}"
-            report_filename = f"/tmp/{file_stem}{debug_suffix}.html"
+            now = datetime.datetime.now()
+            formatted_time = now.strftime("%Y%m%d_%H%M%S")
+            file_stem = f"{eval_name}_{model_name}_{formatted_time}"
+            report_filename = f"./results/{file_stem}{debug_suffix}.html"
             print(f"Writing report to {report_filename}")
             with open(report_filename, "w") as fh:
                 fh.write(common.make_report(result))
             metrics = result.metrics | {"score": result.score}
             print(metrics)
-            result_filename = f"/tmp/{file_stem}{debug_suffix}.json"
+            result_filename = f"./results/{file_stem}{debug_suffix}.json"
             with open(result_filename, "w") as f:
                 f.write(json.dumps(metrics, indent=2))
             print(f"Writing results to {result_filename}")
-            mergekey2resultpath[f"{file_stem}"] = result_filename
+            mergekey2resultpath[f"{eval_name}_{model_name}"] = result_filename
     merge_metrics = []
     for eval_model_name, result_filename in mergekey2resultpath.items():
         try:
diff --git a/types.py b/types.py
deleted file mode 100644
index 2d0e6ee..0000000
--- a/types.py
+++ /dev/null
@@ -1,48 +0,0 @@
-from dataclasses import dataclass, field
-from typing import Any
-
-Message = dict[str, Any]  # keys role, content
-MessageList = list[Message]
-
-
-class SamplerBase:
-    """
-    Base class for defining a sampling model, which can be evaluated,
-    or used as part of the grading process.
-    """
-
-    def __call__(self, message_list: MessageList) -> str:
-        raise NotImplementedError
-
-
-@dataclass
-class EvalResult:
-    """
-    Result of running an evaluation (usually consisting of many samples)
-    """
-
-    score: float | None  # top-line metric
-    metrics: dict[str, float] | None  # other metrics
-    htmls: list[str]  # strings of valid HTML
-    convos: list[MessageList]  # sampled conversations
-
-
-@dataclass
-class SingleEvalResult:
-    """
-    Result of evaluating a single sample
-    """
-
-    score: float | None
-    metrics: dict[str, float] = field(default_factory=dict)
-    html: str | None = None
-    convo: MessageList | None = None  # sampled conversation
-
-
-class Eval:
-    """
-    Base class for defining an evaluation.
-    """
-
-    def __call__(self, sampler: SamplerBase) -> EvalResult:
-        raise NotImplementedError
