name: Functional Tests Nvidia

on:
  workflow_call:
    inputs:
      type:
        required: true
        type: string
      task:
        required: true
        type: string
      image:
        required: true
        type: string

jobs:
  functional-test:
    runs-on: [self-hosted, Linux, X64, nvidia-0, gpus-8]
    container:
      image: ${{ inputs.image }}
      ports:
        - 80
      volumes:
        - /home/flagscale_cicd/flask/static:/workspace/report
        - /home/flagscale_cicd/docker/docker_build/docker_data:/home/gitlab-runner/data
        - /home/flagscale_cicd/docker/docker_build/docker_tokenizers:/home/gitlab-runner/tokenizers
      options: --gpus all --shm-size=500g --hostname flagscale_cicd --user root --ulimit nofile=65535:65535

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          ssh-strict: true
          ssh-user: git
          persist-credentials: true
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true

      - name: Run Functional Test
        run: |
          pip install gitpython # TODO: Remove after updating the image

          git config --global --add safe.directory /__w/FlagScale/FlagScale
          if [ "${{ inputs.type }}" = "train" ] || [ "${{ inputs.type }}" = "hetero_train" ]; then
            PYTHONPATH=./:$PYTHONPATH pip install . --no-build-isolation --verbose --config-settings=device="gpu" --config-settings=backend="Megatron-LM"
            if [ "${{ inputs.task }}" = "llava_onevision" ]; then
              python tools/patch/unpatch.py --backend Megatron-Energon
              # PYTHONPATH=./:$PYTHONPATH pip install . --no-build-isolation --verbose --config-settings=device="gpu" --config-settings=backend="Megatron-Energon"
              cp -r third_party/Megatron-Energon/src/megatron/energon third_party/Megatron-LM/megatron
            fi
          elif [ "${{ inputs.type }}" = "inference" ] || [ "${{ inputs.type }}" = "serve" ]; then
            source /root/miniconda3/bin/activate flagscale-inference
            pip install scikit-build scikit-build-core
            pip install git+https://github.com/FlagOpen/FlagGems.git@v3.0
            PYTHONPATH=./:$PYTHONPATH pip install . --config-settings=backend="vllm" --verbose --no-build-isolation
            conda deactivate
          elif [ "${{ inputs.type }}" = "rl" ]; then
            python tools/patch/unpatch.py --backend verl
            cd third_party/verl
            pip install --no-deps -e .
            cd ../..
          else
            echo "Unknown backend type: ${{ inputs.type }}"
            exit 1
          fi
          tests/scripts/functional_tests/test_task.sh --type ${{ inputs.type }} --task ${{ inputs.task }}
          exit_code=$?
          echo "Exit code: $exit_code"
        shell: bash
