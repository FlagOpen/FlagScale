name: Functional Tests Metax

on:
  workflow_call:
    inputs:
      type:
        required: true
        type: string
      task:
        required: true
        type: string
      image:
        required: true
        type: string

jobs:
  functional-test:
    runs-on: [self-hosted, Linux, X64, metax-0, gpus-2]
    container:
      image: ${{ inputs.image }}
      ports:
        - 80
      volumes:
        - /home/flagscale_cicd/docker/docker_build/docker_data:/home/gitlab-runner/data
      options: --ipc=host --privileged=true --group-add video --shm-size 100gb --hostname flagscale_cicd --user root --ulimit nofile=65535:65535 --ulimit memlock=-1 --device=/dev/dri --device=/dev/mxcd

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          fetch-depth: 0
          ssh-strict: true
          ssh-user: git
          persist-credentials: true
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true

      - name: Run Functional Test
        run: |
          echo "USER: $USER"
          echo "UID: $(id -u)"
          echo "GID: $(id -g)"
          echo "Home: $HOME"
          echo "PWD: ${PWD}"
          whoami
          git config --global --add safe.directory /__w/FlagScale/FlagScale
          if [ "${{ inputs.type }}" = "inference" ]; then
            source /opt/conda/bin/activate flagscale-inference
            python tools/patch/unpatch.py --backend vllm FlagScale --task inference --device-type Metax_C550 
            # TODO: Currently using the merged patch, the next step is to use the patch included in the current PR
            # --commit <flagscale_commit>
            cd build/Metax_C550/FlagScale/third_party/vllm
            source env.sh
            python setup.py bdist_wheel
            pip uninstall vllm -y
            pip install dist/vllm-0.8.5+maca2.33.0.12torch2.6-cp310-cp310-linux_x86_64.whl
          else
            echo "Unknown backend type: ${{ inputs.type }}"
            exit 1
          fi
          cd /__w/FlagScale/FlagScale
          tests/scripts/functional_tests/test_task.sh --type ${{ inputs.type }} --task ${{ inputs.task }} --hardware metax
          exit_code=$?
          echo "Exit code: $exit_code"
        shell: bash
