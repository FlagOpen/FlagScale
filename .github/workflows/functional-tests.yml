# .github/workflows/create-container-functional-tests.yml
name: Functional Tests

on:
  workflow_call:
    inputs:
      type:
        required: true
        type: string
      task:
        required: true
        type: string
      image:
        required: true
        type: string

jobs:
  functional-test:
    runs-on: self-hosted
    container:
      image: ${{ inputs.image }}
      ports:
        - 80
      volumes:
        - /home/flagscale_cicd/flask/static:/workspace/report
        - /home/flagscale_cicd/docker/docker_build/docker_data:/home/gitlab-runner/data
        - /home/flagscale_cicd/docker/docker_build/docker_tokenizers:/home/gitlab-runner/tokenizers
      options: --gpus all --shm-size=500g --hostname flagscale_cicd --user root --ulimit nofile=65535:65535

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          ssh-strict: true
          ssh-user: git
          persist-credentials: true
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true

      - name: Run Functional Test
        run: |
          pip install gitpython # TODO: Remove after updating the image

          git config --global --add safe.directory /__w/FlagScale/FlagScale
          if [ "${{ inputs.type }}" = "train" ] || [ "${{ inputs.type }}" = "hetero_train" ]; then
            python tools/patch/unpatch.py --backend Megatron-LM
            if [ "${{ inputs.task }}" = "llava_onevision" ]; then
              python tools/patch/unpatch.py --backend Megatron-Energon
              cp -r third_party/Megatron-Energon/src/megatron/energon third_party/Megatron-LM/megatron
            fi
          elif [ "${{ inputs.type }}" = "inference" ] || [ "${{ inputs.type }}" = "serve" ]; then
            python tools/patch/unpatch.py --backend vllm
            source /root/miniconda3/bin/activate flagscale-inference
            pip install scikit-build scikit-build-core
            pip install --no-build-isolation git+https://github.com/FlagOpen/FlagGems.git@release_v1.0.0
            MAX_JOBS=$(nproc) pip install --no-build-isolation -v ./third_party/vllm/.
          else
            echo "Unknown backend type: ${{ inputs.type }}"
            exit 1
          fi

          tests/scripts/functional_tests/test_task.sh --type ${{ inputs.type }} --task ${{ inputs.task }}
        shell: bash
