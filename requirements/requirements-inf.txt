# mirror from vllm repo with some modifications 
torch
fastapi
aiohttp
openai
uvicorn[standard]
pydantic >= 2.0  # Required for OpenAI server.
prometheus_client >= 0.18.0
prometheus-fastapi-instrumentator >= 7.0.0
lm-format-enforcer == 0.10.1
outlines == 0.0.34 # Requires torch >= 2.1.0
typing_extensions
filelock >= 3.10.4 # filelock starts to support `mode` argument from 3.10.4

# Uncomment the following lines if you don't compile them from source
# xformers
# vllm-flash-attn
# vllm-nccl-cu12
