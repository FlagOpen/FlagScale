diff --git a/megatron/core/fusions/fused_softmax.py b/megatron/core/fusions/fused_softmax.py
index c7bfbb768..ad0289a22 100644
--- a/megatron/core/fusions/fused_softmax.py
+++ b/megatron/core/fusions/fused_softmax.py
@@ -7,37 +7,18 @@ import torch.nn as nn
 
 from megatron.core.transformer.enums import AttnMaskType
 from megatron.core.transformer.utils import get_default_causal_mask
+import torch_npu
 
-
-class ScaledUpperTriangMaskedSoftmax(torch.autograd.Function):
-    """
-    Fused operation which performs following three operations in sequence
-    1. Scale the tensor.
-    2. Apply upper triangular mask (typically used in gpt models).
-    3. Perform softmax.
-    """
-
-    @staticmethod
-    def forward(ctx, inputs, scale):
-        import scaled_upper_triang_masked_softmax_cuda
-
-        scale_t = torch.tensor([scale])
-        softmax_results = scaled_upper_triang_masked_softmax_cuda.forward(inputs, scale_t[0])
-
-        ctx.save_for_backward(softmax_results, scale_t)
-        return softmax_results
-
+class ScaledUpperTriangMaskedSoftmax:
     @staticmethod
-    def backward(ctx, output_grads):
-        import scaled_upper_triang_masked_softmax_cuda
-
-        softmax_results, scale_t = ctx.saved_tensors
-        input_grads = scaled_upper_triang_masked_softmax_cuda.backward(
-            output_grads, softmax_results, scale_t[0]
-        )
-
-        return input_grads, None
-
+    def apply(input_, scale):
+        # npu_scaled_masked_softmax must be 4D
+        size = input_.size()
+        input_ = input_.view((1,) + tuple(size)).contiguous()
+        dummy_mask = torch.zeros(input_.size()).bool().npu()
+        #dummy_mask = torch.zeros(input_.size(), dtype=torch.uint8, device=input_.device)        
+        output = torch_npu.npu_scaled_masked_softmax(input_, dummy_mask, scale, True)
+        return output.view(size).contiguous()
 
 class ScaledMaskedSoftmax(torch.autograd.Function):
     """
@@ -148,26 +129,13 @@ class FusedScaleMaskSoftmax(nn.Module):
             return self.forward_torch_softmax(input, mask)
 
     def is_kernel_available(self, mask, b, np, sq, sk):
-        attn_batches = b * np
-
-        if (
-            self.scaled_masked_softmax_fusion  # user want to fuse
-            and self.input_in_float16  # input must be fp16
-            and 16 < sk <= 4096  # sk must be 16 ~ 2048
-            and sq % 4 == 0  # sq must be divisor of 4
-            and sk % 4 == 0  # sk must be divisor of 4
-            and attn_batches % 4 == 0  # np * b must be divisor of 4
-        ):
-            if 0 <= sk <= 4096:
-                batch_per_block = self.get_batch_per_block(sq, sk, b, np)
-
-                if self.attn_mask_type == AttnMaskType.causal:
-                    if attn_batches % batch_per_block == 0:
-                        return True
-                else:
-                    if sq % batch_per_block == 0:
-                        return True
-        return False
+        return (
+                self.scaled_masked_softmax_fusion  # user want to fuse
+                and self.input_in_float16  # input must be fp16
+                and 32 < sk <= 4096  # sk must be 32 ~ 4096
+                and sq % 16 == 0  # sq must be divisor of 16
+                and sk % 16 == 0  # sk must be divisor of 16
+        )
 
     def forward_fused_softmax(self, input, mask):
         b, np, sq, sk = input.size()

