diff --git a/megatron/core/distributed/distributed_data_parallel.py b/megatron/core/distributed/distributed_data_parallel.py
index b8a5c1358..43b69e3aa 100644
--- a/megatron/core/distributed/distributed_data_parallel.py
+++ b/megatron/core/distributed/distributed_data_parallel.py
@@ -524,42 +524,22 @@ class DistributedDataParallel(_BaseDataParallel):
             for bucket_group in self.bucket_groups + self.expert_parallel_bucket_groups:
                 bucket_group.is_last_microbatch = True
 
-    def start_param_sync(self, *unused, force_sync: bool = False, force_dispatch: bool = False):
-        """
-        Initiates param sync (all-gather) communication operations for all model parameters.
-
-        By default, when overlap_param_gather is set to True, dispatches asynchronous communication
-        calls; when overlap_param_gather is set to False, calls synchronous communication
-        ops. Can override this default behavior using flags below.
-
-        Args:
-            force_sync (bool, optional): force synchronous collective regardless of
-                other settings.
-            force_dispatch (bool, optional): force dispatch regardless of other settings.
-        """
+    def start_param_sync(self, *unused, force_sync: bool = False, force_dispatch: bool = False, dense_or_moe_group: str = None):
         if not force_sync:
             # If overlapping param AG with optimizer step, AG should not be dispatched again
             # in forward_backward_step.
             if self.overlap_param_gather_with_optimizer_step and not force_dispatch:
                 return
 
-        for bucket_group in self.bucket_groups + self.expert_parallel_bucket_groups:
+        if dense_or_moe_group is None:
+            bucket_groups = self.bucket_groups + self.expert_parallel_bucket_groups
+        elif dense_or_moe_group == 'dense':
+            bucket_groups = self.bucket_groups
+        elif dense_or_moe_group == 'moe':
+            bucket_groups = self.expert_parallel_bucket_groups
+
+        for bucket_group in bucket_groups:
             bucket_group.start_param_sync(force_sync=force_sync)
-            # For MXFP8 params, we need to copy the all-gathered param data from the buffer to
-            # the param.data, since param buffer is not mapped to model params for MXFP8 case.
-            # The paramaters are cast from bf16 to MXFP8 during copy.
-            if self.ddp_config.reuse_grad_buf_for_mxfp8_param_ag:
-                assert (
-                    not self.ddp_config.overlap_param_gather
-                ), "MXFP8 param currently does not support DP AG overlap."
-                for bucket in bucket_group.buckets:
-                    for param in bucket.params:
-                        param_start, param_end = bucket.param_to_index[param]
-                        param_slice = bucket.param_data.view(-1)[param_start:param_end]
-                        param.data.copy_(param_slice.view(param.data.shape))
-                    # All-gathered params are not needed after being copied to param.data.
-                    # Zero out the grad buffer (shared with param buffer) for gradient accumulation.
-                    bucket.grad_data.zero_()
 
     def start_grad_sync(self, *unused):
         """

