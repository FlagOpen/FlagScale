diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index fe5f7a725..56478a86b 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -1,16 +1,21 @@
 # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
 
+import inspect
 import warnings
 from dataclasses import dataclass
 from typing import Callable, List, Optional, Tuple, Union
+from functools import wraps
+from dataclasses import make_dataclass, field
 
 import torch
 import torch.nn.functional as F
+from .args_utils import get_full_args
 
 from megatron.core.enums import Fp8Recipe
 from megatron.core.quantization.quant_config import RecipeConfig
 from megatron.core.transformer.enums import AttnBackend
 from megatron.core.transformer.pipeline_parallel_layer_layout import PipelineParallelLayerLayout
+from .megatron_adaptor.features_manager import MindSpeedFeaturesManager
 
 from ..model_parallel_config import ModelParallelConfig
 from ..utils import (
@@ -283,6 +288,15 @@ class TransformerConfig(ModelParallelConfig):
     the number of transformer layers to recompute within each pipeline stage.  Must be None for
     'selective' activation checkpointing."""
 
+    recompute_granularity_per_stage_micro_batch: list = None
+    """Same as recompute_granularity but for each stage and each micro-batch."""
+
+    recompute_method_per_stage_micro_batch: list = None
+    """Same as recompute_method but for each stage and each micro-batch."""
+
+    recompute_num_layers_per_stage_micro_batch: list = None
+    """Same as recompute_num_layers but for each stage and each micro-batch."""
+
     distribute_saved_activations: Optional[bool] = None
     """If True, distribute recomputed activations across the model parallel group."""
 
@@ -366,6 +380,12 @@ class TransformerConfig(ModelParallelConfig):
     use_kitchen: bool = False
     """Use the kitchen extension for transformer quantization."""
 
+    ####################
+    # DualPipeV related
+    ####################
+    use_dualpipev: bool = False
+    moe_fb_overlap: bool = False
+
     ####################
     # MoE related
     ####################
@@ -577,6 +597,12 @@ class TransformerConfig(ModelParallelConfig):
     config_logger_dir: str = ""
     """When non-empty, dumps entry-point configs to config_logger_dir"""
 
+    qk_layernorm_hidden_dim: bool = False
+    """Whether to apply LayerNorm to the query and key embeddings on the hidden dimension rather than head dimension."""
+
+    use_partial_reduce_for_shared_embedding: bool = False
+    """Whether to use partional reduce for shared embedding."""
+
     flash_decode: bool = False
     """ Use the optimized flash decoding kernel during inference. """
 
@@ -628,6 +654,65 @@ class TransformerConfig(ModelParallelConfig):
     quant_recipe: Optional[RecipeConfig] = None
     """Configuration of any quantization to be applied to the model"""
 
+    ####################
+    # PEFT
+    ####################
+    peft_type: str = None
+    """Type for finetuning"""
+    lora_target_modules: Optional[List[str]] = None
+    """Lora target modules"""
+    lora_dim: Optional[int] = None
+    """Lora rank."""
+    lora_alpha: Optional[int] = None
+    """Lora scale alpha."""
+    lora_dropout: Optional[float] = None
+    """Lora dropout prob"""
+    lora_dropout_position: Optional[str] = None
+    """Lora dropout pos"""
+    lora_in_init_method: Optional[str] = None
+    """Lora a init method"""
+    lora_out_init_method: Optional[str] = None
+    """Lora b init method"""
+   
+    def transformer_config_post_init_wrapper(fn):
+        @wraps(fn)
+        def wrapper(self):
+            # make prev validation and copy some args.
+            MindSpeedFeaturesManager.pre_validate_features_args(self)
+            fn(self)
+            MindSpeedFeaturesManager.post_validate_features_args(args=self)
+            MindSpeedFeaturesManager.validate_features_args(args=self)
+
+        return wrapper
+ 
+    def transformer_config_init_wrapper(fn):
+        @wraps(fn)
+        def wrapper(self):
+            #Reset apply_rope_fusion to bypass Megatron core_r0.10.0 check.
+            ori_apply_rope_fusion = self.apply_rope_fusion
+            self.apply_rope_fusion = False
+            fn(self)
+            self.apply_rope_fusion = ori_apply_rope_fusion
+            del ori_apply_rope_fusion
+
+            from megatron.training import get_args
+            from dataclasses import make_dataclass, field
+            args = get_args()
+            fields = []
+            for key, value in vars(args).items():
+                field_name = str(key)
+                field_type = type(value)
+                if not hasattr(self, key):
+                    field_def = (field_name, field_type, field(init=False))
+                    fields.append(field_def)
+            self.__class__ = make_dataclass(self.__class__.__name__, fields=fields, bases=(self.__class__,))
+
+            for key, value in vars(args).items():
+                if not hasattr(self, key):
+                    setattr(self, key, value)
+        return wrapper
+
+    @transformer_config_init_wrapper    
     def __post_init__(self):
         """Python dataclass method that is used to modify attributes after initialization.
         See https://docs.python.org/3/library/dataclasses.html#post-init-processing for more
@@ -1250,6 +1335,9 @@ class TransformerConfig(ModelParallelConfig):
                     f"the number of layers ({self.num_layers})"
                 )
 
+        if self.moe_fb_overlap:
+            self.delay_wgrad_compute = True
+
 
 @dataclass
 class MLATransformerConfig(TransformerConfig):
@@ -1309,22 +1397,4 @@ class MLATransformerConfig(TransformerConfig):
 
     mscale_all_dim: float = 0.707
     """Mscale all dimensions for YaRN RoPE in Multi-Latent Attention, used by yarn."""
-
-    def __post_init__(self):
-        super().__post_init__()
-        if self.multi_latent_attention and self.apply_rope_fusion and self.rope_type != "yarn":
-            raise ValueError("apply_rope_fusion for MLA only works with YARN RoPE.")
-
-        # TODO(boxiangw): Deprecate this check
-        if self.max_position_embeddings != 4096:
-            if self.original_max_position_embeddings == 4096:
-                # only override the original_max_position_embeddings if it is not set
-                self.original_max_position_embeddings = self.max_position_embeddings
-            self.max_position_embeddings = 4096
-            warnings.warn(
-                "MLA config max_position_embeddings is overridden by customer input, "
-                "please use the original_max_position_embeddings instead for YaRN!"
-                "max_position_embeddings will be deprecated in the future."
-                "Assigned original_max_position_embeddings to max_position_embeddings if not set,"
-                "and assigned max_position_embeddings back to the original value."
-            )
+    

