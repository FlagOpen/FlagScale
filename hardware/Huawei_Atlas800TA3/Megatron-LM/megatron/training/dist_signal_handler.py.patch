diff --git a/megatron/training/dist_signal_handler.py b/megatron/training/dist_signal_handler.py
index f4b4fbf5c..91c2ab6ca 100644
--- a/megatron/training/dist_signal_handler.py
+++ b/megatron/training/dist_signal_handler.py
@@ -1,5 +1,6 @@
 # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
 import signal
+from functools import wraps
 
 import torch
 
@@ -11,7 +12,22 @@ def get_world_size():
         world_size = 1
     return world_size
 
+def get_device_wrapper(func):
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        backend = torch.distributed.get_backend()
+        local_rank = args[0]
+        if backend == 'hccl':
+            if local_rank is None:
+                device = torch.device('cuda')
+            else:
+                device = torch.device(f'cuda:{local_rank}')
+        else:
+            device = func(*args, **kwargs)
+        return device
+    return wrapper
 
+@get_device_wrapper
 def get_device(local_rank=None):
     backend = torch.distributed.get_backend()
     if backend == 'nccl':

