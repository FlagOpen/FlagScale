diff --git a/flagscale/train/mindspeed_patch/core/tensor_parallel/vocab_parallel/vocab_parallel.py b/flagscale/train/mindspeed_patch/core/tensor_parallel/vocab_parallel/vocab_parallel.py
new file mode 100644
index 00000000..a4a3c1e1
--- /dev/null
+++ b/flagscale/train/mindspeed_patch/core/tensor_parallel/vocab_parallel/vocab_parallel.py
@@ -0,0 +1,57 @@
+# coding=utf-8
+# Copyright (c) 2024, Huawei Technologies Co., Ltd. All rights reserved.
+# Copyright (c) 2024, Bytedance Inc. All rights reserved.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Callable
+
+import torch.nn.functional as F
+
+
+def vocab_parallel_embedding_forward_impl(
+    self,
+    input_,
+    reduce_from_tensor_model_parallel_region: Callable = None,
+    reduce_scatter_to_sequence_parallel_region: Callable = None,
+):
+    if self.tensor_model_parallel_size > 1:
+        # Build the mask.
+        input_mask = (input_ < self.vocab_start_index) | (input_ >= self.vocab_end_index)
+        # Mask the input.
+        masked_input = input_.clone() - self.vocab_start_index
+        masked_input *= ~input_mask
+    else:
+        masked_input = input_
+        # Get the embeddings.
+
+    if self.deterministic_mode:
+        output_parallel = self.weight[masked_input]
+    else:
+        # F.embedding currently has a non-deterministic backward function
+        # For higher accumulation accuracy for bf16 on NPU.
+        output_parallel = F.embedding(masked_input, self.weight)
+
+    # Mask the output embedding.
+    if self.tensor_model_parallel_size > 1:
+        output_parallel *= ~input_mask[..., None]
+    # Reduce across all the model parallel GPUs.
+    if self.reduce_scatter_embeddings:
+        # Data format change to avoid explicit tranposes : [b s h] --> [s b h].
+        output_parallel = output_parallel.transpose(0, 1).contiguous()
+        output = reduce_scatter_to_sequence_parallel_region(output_parallel)
+    else:
+        # Reduce across all the model parallel GPUs.
+        output = reduce_from_tensor_model_parallel_region(output_parallel)
+    return output

