diff --git a/flagscale/train/mindspeed_patch/core/tensor_parallel/mapping.py b/flagscale/train/mindspeed_patch/core/tensor_parallel/mapping.py
new file mode 100644
index 00000000..6602a39b
--- /dev/null
+++ b/flagscale/train/mindspeed_patch/core/tensor_parallel/mapping.py
@@ -0,0 +1,37 @@
+# coding=utf-8
+# Copyright (c) 2024, Huawei Technologies Co., Ltd. All rights reserved.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+
+from megatron.core.tensor_parallel.mappings import _reduce
+
+
+class _ReduceFromModelParallelRegion_Nd(torch.autograd.Function):
+    @staticmethod
+    def symbolic(graph, input_):
+        return _reduce(input_)
+
+    @staticmethod
+    def forward(ctx, input_):
+        return _reduce(input_)
+
+    @staticmethod
+    def backward(ctx, grad_output):
+        return _reduce(grad_output)
+
+
+def reduce_from_tensor_model_parallel_region_nd(input_):
+    return _ReduceFromModelParallelRegion_Nd.apply(input_)

