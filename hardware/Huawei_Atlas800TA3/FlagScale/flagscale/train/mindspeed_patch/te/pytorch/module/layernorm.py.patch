diff --git a/flagscale/train/mindspeed_patch/te/pytorch/module/layernorm.py b/flagscale/train/mindspeed_patch/te/pytorch/module/layernorm.py
new file mode 100644
index 00000000..24c46ed5
--- /dev/null
+++ b/flagscale/train/mindspeed_patch/te/pytorch/module/layernorm.py
@@ -0,0 +1,17 @@
+# Copyright (c) 2025, Huawei Technologies Co., Ltd. All rights reserved.
+import torch.nn as nn
+
+
+class MindSpeedTELayernorm(nn.LayerNorm):
+    def __init__(
+        self, hidden_size, eps=1e-5, sequence_parallel=False, zero_centered_gamma=False, **kwargs
+    ):
+        super(MindSpeedTELayernorm, self).__init__(hidden_size, eps=eps)
+        self.sequence_parallel = sequence_parallel
+        self.zero_centered_gamma = zero_centered_gamma
+        setattr(self.weight, 'sequence_parallel', sequence_parallel)
+        setattr(self.bias, 'sequence_parallel', sequence_parallel)
+        if self.zero_centered_gamma:
+            raise NotImplementedError(
+                "Zero-centered gamma is not supported in this dummy implementation."
+            )

