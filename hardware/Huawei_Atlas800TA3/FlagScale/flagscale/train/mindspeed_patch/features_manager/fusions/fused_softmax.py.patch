diff --git a/flagscale/train/mindspeed_patch/features_manager/fusions/fused_softmax.py b/flagscale/train/mindspeed_patch/features_manager/fusions/fused_softmax.py
new file mode 100644
index 00000000..3af10516
--- /dev/null
+++ b/flagscale/train/mindspeed_patch/features_manager/fusions/fused_softmax.py
@@ -0,0 +1,32 @@
+from flagscale.train.mindspeed_patch.features_manager.feature import MindSpeedFeature
+
+
+class FusedSoftmaxFeature(MindSpeedFeature):
+    def __init__(self):
+        super().__init__('fused-softmax', optimization_level=0)
+
+    def register_patches(self, pm, args):
+        from flagscale.train.mindspeed_patch.core.fusions.fused_softmax import (
+            ScaledMaskedSoftmax,
+            ScaledSoftmax,
+            ScaledUpperTriangMaskedSoftmax,
+            forward_fused_softmax,
+            is_kernel_available,
+        )
+
+        pm.register_patch(
+            'megatron.core.fusions.fused_softmax.ScaledUpperTriangMaskedSoftmax',
+            ScaledUpperTriangMaskedSoftmax,
+        )
+        pm.register_patch(
+            'megatron.core.fusions.fused_softmax.ScaledMaskedSoftmax', ScaledMaskedSoftmax
+        )
+        pm.register_patch('megatron.core.fusions.fused_softmax.ScaledSoftmax', ScaledSoftmax)
+        pm.register_patch(
+            'megatron.core.fusions.fused_softmax.FusedScaleMaskSoftmax.is_kernel_available',
+            is_kernel_available,
+        )
+        pm.register_patch(
+            'megatron.core.fusions.fused_softmax.FusedScaleMaskSoftmax.forward_fused_softmax',
+            forward_fused_softmax,
+        )

