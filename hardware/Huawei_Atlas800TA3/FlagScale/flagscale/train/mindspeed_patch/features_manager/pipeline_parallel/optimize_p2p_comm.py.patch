diff --git a/flagscale/train/mindspeed_patch/features_manager/pipeline_parallel/optimize_p2p_comm.py b/flagscale/train/mindspeed_patch/features_manager/pipeline_parallel/optimize_p2p_comm.py
new file mode 100644
index 00000000..c98c85c1
--- /dev/null
+++ b/flagscale/train/mindspeed_patch/features_manager/pipeline_parallel/optimize_p2p_comm.py
@@ -0,0 +1,30 @@
+# Copyright (c) 2025, Huawei Technologies Co., Ltd. All rights reserved.
+
+from argparse import ArgumentParser, Namespace
+
+from flagscale.train.mindspeed_patch.features_manager.feature import MindSpeedFeature
+from flagscale.train.mindspeed_patch.patch_utils import MindSpeedPatchesManager, is_megatron_training_available
+
+
+class OptimizeP2PCommFeature(MindSpeedFeature):
+    # Use isend/irecv instead of batch_isend_irecv on NPU for better performance for pipeline parallel.
+
+    def __init__(
+        self, feature_name: str = "pipeline-model-parallel-size", optimization_level: int = 2
+    ):
+        super().__init__(feature_name, optimization_level)
+
+    def register_patches(self, patch_manager: MindSpeedPatchesManager, args: Namespace):
+        # pylint: disable=import-outside-toplevel
+        from flagscale.train.mindspeed_patch.core.pipeline_parallel.optimize_p2p_comm.adaptor import (
+            core_transformer_config_from_args_wrapper,
+        )
+
+        if getattr(args, self.feature_name, None) and int(args.pipeline_model_parallel_size) >= 2:
+            if getattr(args, "num_layers_per_virtual_pipeline_stage", None) is None:  # noqa
+                megatron_training_available = is_megatron_training_available()
+                if megatron_training_available:
+                    patch_manager.register_patch(
+                        "megatron.training.arguments.core_transformer_config_from_args",  # noqa
+                        core_transformer_config_from_args_wrapper,
+                    )

