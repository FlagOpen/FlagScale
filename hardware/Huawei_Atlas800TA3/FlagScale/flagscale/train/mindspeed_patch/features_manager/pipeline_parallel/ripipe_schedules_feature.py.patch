diff --git a/flagscale/train/mindspeed_patch/features_manager/pipeline_parallel/ripipe_schedules_feature.py b/flagscale/train/mindspeed_patch/features_manager/pipeline_parallel/ripipe_schedules_feature.py
new file mode 100644
index 00000000..8560bceb
--- /dev/null
+++ b/flagscale/train/mindspeed_patch/features_manager/pipeline_parallel/ripipe_schedules_feature.py
@@ -0,0 +1,144 @@
+# coding=utf-8
+# Copyright (c) 2024, Huawei Technologies Co., Ltd. All rights reserved.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+from argparse import ArgumentParser
+
+from flagscale.train.mindspeed_patch.features_manager.feature import MindSpeedFeature
+
+
+class RiPipeSchedulesFeature(MindSpeedFeature):
+
+    def validate_args(self, args):
+        if getattr(args, "adaptive_recompute_device_size", None) is not None:
+            adaptive_recompute_enable = (
+                args.adaptive_recompute_device_size > 0 or args.adaptive_recompute_device_swap
+            )
+            if adaptive_recompute_enable:
+                if args.recompute_in_advance or args.recompute_in_bubble:
+                    raise AssertionError(
+                        'adaptive selective recompute is not compatible with ripipe schedule.'
+                    )
+        self.incompatible_check(args, "optimize_send_recv_comm")
+        if (
+            getattr(args, "ampipe_degree", None) is not None
+            and args.ampipe_degree > 1
+            and getattr(args, self.feature_name, None)
+        ):
+            raise AssertionError('{} and {} are incompatible.'.format(self.feature_name, "ampipe"))
+
+    def register_patches(self, patch_manager, args):
+        if not args.recompute_in_bubble and not args.recompute_in_advance:
+            return
+        from flagscale.train.mindspeed_patch.core.tensor_parallel.random import checkpoint_wrapper
+
+        # from flagscale.train.mindspeed_patch.core.memory.common import linear_forward_main_grad_wrapper, linear_backward_main_grad_wrapper
+        patch_manager.register_patch(
+            'megatron.core.tensor_parallel.random.checkpoint', checkpoint_wrapper
+        )
+        from flagscale.train.mindspeed_patch.core.pipeline_parallel.ripipe_schedules import (
+            get_forward_backward_func_ripipe_patch,
+        )
+
+        patch_manager.register_patch(
+            'megatron.core.pipeline_parallel.schedules.get_forward_backward_func',
+            get_forward_backward_func_ripipe_patch,
+        )
+        # patch_manager.register_patch(
+        #     'megatron.core.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication.forward',
+        #     linear_forward_main_grad_wrapper)
+        # patch_manager.register_patch(
+        #     'megatron.core.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication.backward',
+        #     linear_backward_main_grad_wrapper)
+
+
+class RiPipeSchedulesBubbleFeature(RiPipeSchedulesFeature):
+
+    def __init__(self):
+        super().__init__("recompute-in-bubble")
+
+    def register_args(self, parser: ArgumentParser):
+        group = parser.add_argument_group(title=self.feature_name)
+        group.add_argument(
+            "--recompute-in-bubble",
+            action="store_true",
+            help="use bubble to do recompute to reduce memory.",
+        )
+
+    def validate_args(self, args):
+        super().validate_args(args)
+        self.incompatible_check(args, "adaptive_memory_optimization")
+        if args.recompute_in_bubble:
+            # Check if recompute_num_layers is set to the value we expect
+            expected_recompute_num_layers = None
+            if getattr(args, "enable_recompute_layers_per_pp_rank", False):
+                expected_recompute_num_layers = args.num_layers // args.pipeline_model_parallel_size
+            else:
+                expected_recompute_num_layers = args.num_layers_per_virtual_pipeline_stage
+
+            # Only raise error when recompute_num_layers is set and not equal to our expected value
+            if (
+                args.recompute_num_layers
+                and args.recompute_num_layers != expected_recompute_num_layers
+            ):
+                raise AssertionError(
+                    'recompute_num_layers must be None or 0 when using recompute_in_bubble'
+                )
+            if (
+                args.pipeline_model_parallel_size <= 1
+                or args.num_layers_per_virtual_pipeline_stage is None
+            ):
+                raise AssertionError(
+                    'recompute_in_bubble only support pipelining with interleaving'
+                )
+            if not getattr(args, "swap_attention", False):
+                # Following is a trick to realize bubble recomputation. We first enable all recomputation,
+                # and then disable recomputation for all layers except the ones chosen for bubble recomputation.
+                # Only set these values if they haven't been set already to ensure idempotency
+                if getattr(args, 'recompute_granularity', None) != "full":
+                    args.recompute_granularity = "full"
+                if getattr(args, 'recompute_method', None) != "block":
+                    args.recompute_method = "block"
+            if (
+                expected_recompute_num_layers is not None
+                and args.recompute_num_layers != expected_recompute_num_layers
+            ):
+                args.recompute_num_layers = expected_recompute_num_layers
+
+
+class RiPipeSchedulesAdvanceFeature(RiPipeSchedulesFeature):
+
+    def __init__(self):
+        super().__init__("recompute-in-advance")
+
+    def register_args(self, parser: ArgumentParser):
+        group = parser.add_argument_group(title=self.feature_name)
+        group.add_argument(
+            '--recompute-in-advance',
+            action='store_true',
+            help='recompute early to reduce bubble and improve training.',
+        )
+
+    def validate_args(self, args):
+        super().validate_args(args)
+        args.reduce_recompute_for_last_chunk = False
+        if args.recompute_in_advance:
+            args.reduce_recompute_for_last_chunk = True
+            if args.recompute_method == "uniform":
+                raise AssertionError(
+                    'recompute_in_advance does not support uniform recompute_method'
+                )
+            if not args.recompute_num_layers and not getattr(
+                args, "adaptive_memory_optimization", None
+            ):
+                raise AssertionError(
+                    'recompute_num_layers can not be None or 0 when using recompute_in_advance'
+                )
+            if (
+                args.pipeline_model_parallel_size <= 1
+                or args.num_layers_per_virtual_pipeline_stage is None
+            ):
+                raise AssertionError(
+                    'recompute_in_advance only support pipelining with interleaving'
+                )
+            if args.num_layers_per_virtual_pipeline_stage != 1:
+                args.recompute_in_advance = False

