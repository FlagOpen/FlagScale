diff --git a/flagscale/train/train_qwen2_5_vl.py b/flagscale/train/train_qwen2_5_vl.py
index 9befeec7..20a17b4a 100644
--- a/flagscale/train/train_qwen2_5_vl.py
+++ b/flagscale/train/train_qwen2_5_vl.py
@@ -22,6 +22,9 @@ from copy import deepcopy
 from typing import List, Optional, Tuple, Union
 
 import torch
+######### For huawei NPU #########
+import flagscale.train.mindspeed_patch.megatron_adaptor
+
 import torch._dynamo
 
 from argparse import Namespace
@@ -94,8 +97,9 @@ from megatron.training.global_vars import get_tokenizer
 from flagscale.train.models.qwen2_5_vl.layer_specs import (
     get_gpt_layer_with_transformer_engine_spec,
     get_qwen2vl_vision_model_spec,
-    get_mlp_module_spec
-
+    get_mlp_module_spec,
+    get_qwen2vl_llm_layer_spec,
+    get_qwen2vl_layer_spec,
 )
 from flagscale.train.models.qwen2_5_vl.qwen2_5_vl_model import Qwen2_5VLModel
 from flagscale.train.models.qwen2_5_vl.tensor_parallel import broadcast_data
@@ -118,8 +122,8 @@ def model_provider(
     # Config of vit, llm and projector
     config = core_transformer_config_from_args(args)
     use_te = args.transformer_impl == "transformer_engine"
-    if not use_te:
-        raise NotImplementedError("The Qwen2-VL model is only implemented with TransformerEngine!")
+    if use_te:
+        raise NotImplementedError("The Qwen2-VL model is only implemented with Loacal transformer for huawei NPU!")
 
     if args.rotary_seq_len_interpolation_factor is not None or args.rotary_seq_len_interpolation_factor != 1:
         print_rank_0('Multimodal RoPE currently not support RoPE interpolation, set to None...')
@@ -132,9 +136,10 @@ def model_provider(
 
     print_rank_0("building Qwen2-5-VL model in TE...")
     # Layer Specs of vit, llm and projector
-    transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.qk_layernorm)
-    vision_model_spec = get_qwen2vl_vision_model_spec()
-    vision_projector_spec = get_mlp_module_spec(add_norm=False).submodules
+
+    transformer_layer_spec = get_qwen2vl_llm_layer_spec(args.qk_layernorm)
+    vision_model_spec = get_qwen2vl_layer_spec()
+    vision_projector_spec = get_mlp_module_spec(use_te=False, add_norm=False).submodules
     if args.enable_variable_seq_lengths:
         config.variable_seq_lengths = True
 
@@ -398,7 +403,7 @@ def get_batch(data_iterator):
     position_ids = None
 
     # Broadcast data.
-    torch.cuda.nvtx.range_push("get_data")
+    # torch.cuda.nvtx.range_push("get_data")
     if data_iterator is not None and get_tensor_model_parallel_rank() == 0:
         data = next(data_iterator)
         # pad_token_id = get_tokenizer().pad_token_id
@@ -441,23 +446,23 @@ def get_batch(data_iterator):
 
     image_input_mask = broadcast_data(["image_input_mask"], data, torch.bool)["image_input_mask"]
     video_input_mask = broadcast_data(["video_input_mask"], data, torch.bool)["video_input_mask"]
-    torch.cuda.nvtx.range_pop()
+    # torch.cuda.nvtx.range_pop()
 
-    torch.cuda.nvtx.range_push("index tokens")
+    # torch.cuda.nvtx.range_push("index tokens")
     tokenizer = get_tokenizer()
 
     tokens = data_text.long().contiguous()
     labels = target.contiguous()
 
     assert tokens.shape == labels.shape, f"tokens: {tokens.shape} != labels: {labels.shape}"
-    torch.cuda.nvtx.range_pop()
+    # torch.cuda.nvtx.range_pop()
 
     # NOTE: no sequence packing in LLM inputs
-    torch.cuda.nvtx.range_push("get_ltor_masks_and_position_ids")
+    # torch.cuda.nvtx.range_push("get_ltor_masks_and_position_ids")
     attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
         tokens, image_thw_grids, video_thw_grids, labels, IGNORE_IDX, second_per_grid_ts
     )
-    torch.cuda.nvtx.range_pop()
+    # torch.cuda.nvtx.range_pop()
 
     return (
         tokens,

