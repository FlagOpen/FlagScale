diff --git a/flagscale/train/models/qwen2_5_vl/layer_specs.py b/flagscale/train/models/qwen2_5_vl/layer_specs.py
index b74cb4ff..737c1d5d 100644
--- a/flagscale/train/models/qwen2_5_vl/layer_specs.py
+++ b/flagscale/train/models/qwen2_5_vl/layer_specs.py
@@ -28,10 +28,13 @@ from megatron.core.transformer.identity_op import IdentityOp
 
 from megatron.core.transformer.spec_utils import ModuleSpec
 from megatron.core.transformer.transformer_layer import TransformerLayer, TransformerLayerSubmodules
+from megatron.core.tensor_parallel.layers import ColumnParallelLinear, RowParallelLinear
+from megatron.core.transformer.dot_product_attention import DotProductAttention
 
 from megatron.core.transformer.mlp import MLP, MLPSubmodules
 from megatron.core.transformer.attention import (SelfAttentionSubmodules, SelfAttention)
 
+
 from .vision_attention import SelfAttentionVision
 
 # Use this spec to use lower level Transformer Engine modules (required for fp8 training)
@@ -121,3 +124,64 @@ def get_mlp_module_spec(
     else:
         # Mixture of experts with modules in megatron core.
         raise NotImplementedError()
+
+
+############### Qwen2.5-VL Layer Specs For huawei NPU ###############
+def get_qwen2vl_llm_layer_spec(qk_layernorm: bool = False) -> ModuleSpec:
+
+    mlp = get_mlp_module_spec(
+        use_te=False, num_experts=None, moe_grouped_gemm=False, add_norm=False
+    )
+    return ModuleSpec(
+        module=TransformerLayer,
+        submodules=TransformerLayerSubmodules(
+            input_layernorm=TENorm,
+            self_attention=ModuleSpec(
+                module=SelfAttention,
+                params={"attn_mask_type": AttnMaskType.causal},
+                submodules=SelfAttentionSubmodules(
+                    linear_qkv=ColumnParallelLinear,
+                    core_attention=DotProductAttention,
+                    linear_proj=RowParallelLinear,
+                   q_layernorm=TENorm if qk_layernorm else IdentityOp,
+                    k_layernorm=TENorm if qk_layernorm else IdentityOp,
+                ),
+            ),
+            self_attn_bda=get_bias_dropout_add,
+            pre_mlp_layernorm=TENorm,
+            mlp=mlp,
+            mlp_bda=get_bias_dropout_add,
+            sharded_state_dict_keys_map={
+                'input_layernorm.': 'self_attention.linear_qkv.layer_norm_',
+                'pre_mlp_layernorm.': 'mlp.linear_fc1.layer_norm_',
+            },
+        ),
+    )
+
+def get_qwen2vl_layer_spec(is_vit=True) -> ModuleSpec:
+    attn_mask_type = AttnMaskType.no_mask if is_vit else AttnMaskType.causal
+
+    mlp = get_mlp_module_spec(use_te=False, add_norm=False)
+    return ModuleSpec(
+        module=TransformerLayer,
+        submodules=TransformerLayerSubmodules(
+            input_layernorm=TENorm,
+            self_attention=ModuleSpec(
+                module=SelfAttentionVision,
+                params={
+                    "attn_mask_type": attn_mask_type
+                },
+                submodules=SelfAttentionSubmodules(
+                    linear_qkv=ColumnParallelLinear,
+                    core_attention=DotProductAttention,
+                    linear_proj=RowParallelLinear,
+                    q_layernorm=IdentityOp,
+                    k_layernorm=IdentityOp,
+                ),
+            ),
+            self_attn_bda=get_bias_dropout_add,
+            pre_mlp_layernorm=TENorm,
+            mlp=mlp,
+            mlp_bda=get_bias_dropout_add,
+        ),
+    )

