diff --git a/hardware/Huawei_Atlas800TA3/FlagScale/flagscale/train/megatron_basic_adaptor/layernorm.py.patch b/hardware/Huawei_Atlas800TA3/FlagScale/flagscale/train/megatron_basic_adaptor/layernorm.py.patch
new file mode 100644
index 00000000..05017e03
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/FlagScale/flagscale/train/megatron_basic_adaptor/layernorm.py.patch
@@ -0,0 +1,20 @@
+diff --git a/flagscale/train/megatron_basic_adaptor/layernorm.py b/flagscale/train/megatron_basic_adaptor/layernorm.py
+new file mode 100644
+index 00000000..2939b6eb
+--- /dev/null
++++ b/flagscale/train/megatron_basic_adaptor/layernorm.py
+@@ -0,0 +1,13 @@
++import torch.nn as nn
++
++
++class MindSpeedTELayernorm(nn.LayerNorm):
++    def __init__(self, hidden_size, eps=1e-5, sequence_parallel=False, zero_centered_gamma=False, **kwargs):
++        super(MindSpeedTELayernorm, self).__init__(hidden_size, eps=eps)
++        self.sequence_parallel = sequence_parallel
++        self.zero_centered_gamma = zero_centered_gamma
++        setattr(self.weight, 'sequence_parallel', sequence_parallel)
++        setattr(self.bias, 'sequence_parallel', sequence_parallel)
++        if self.zero_centered_gamma:
++            raise NotImplementedError("Zero-centered gamma is not supported in this dummy implementation.")
++
+

