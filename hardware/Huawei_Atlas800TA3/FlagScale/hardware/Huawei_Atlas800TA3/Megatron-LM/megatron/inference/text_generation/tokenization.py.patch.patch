diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/inference/text_generation/tokenization.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/inference/text_generation/tokenization.py.patch
new file mode 100644
index 00000000..a8709939
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/inference/text_generation/tokenization.py.patch
@@ -0,0 +1,21 @@
+diff --git a/megatron/inference/text_generation/tokenization.py b/megatron/inference/text_generation/tokenization.py
+index 541cc47b3..a41c7f592 100644
+--- a/megatron/inference/text_generation/tokenization.py
++++ b/megatron/inference/text_generation/tokenization.py
+@@ -41,6 +41,15 @@ def detokenize_generations(tokens_gpu_tensor,
+                     word = bytearray([tokenizer.tokenizer.byte_decoder[c] for c in word]).decode(
+                         "utf-8", errors="replace"
+                     )
++                    args = get_args()
++                    if args.tokenizer_type == 'AquilaTokenizer':
++                        if token in tokenizer.tokenizer.special_tokens_decoder:
++                            word = tokenizer.tokenizer.special_tokens_decoder[token]
++                        else :
++                            word = tokenizer.tokenizer.decoder[token]
++                            word = bytearray(
++                                [tokenizer.tokenizer.byte_decoder[c] for c in word]).decode(
++                                    'utf-8', errors='replace')
+                     words.append(word)
+ 
+             prompts_plus_generations_segments.append(words)
+

