diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/dist_checkpointing/exchange_utils.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/dist_checkpointing/exchange_utils.py.patch
new file mode 100644
index 00000000..63d88cd7
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/dist_checkpointing/exchange_utils.py.patch
@@ -0,0 +1,14 @@
+diff --git a/megatron/core/dist_checkpointing/exchange_utils.py b/megatron/core/dist_checkpointing/exchange_utils.py
+index def79fb77..2f7914490 100644
+--- a/megatron/core/dist_checkpointing/exchange_utils.py
++++ b/megatron/core/dist_checkpointing/exchange_utils.py
+@@ -63,7 +63,7 @@ class ShardDistribution(NamedTuple):
+ def _shard_size(sh_ten: ShardedTensor):
+     """Returns size in bytes of a given sharded tensor."""
+     if sh_ten.flattened_range is None:
+-        numel = np.product(sh_ten.local_shape)
++        numel = np.prod(sh_ten.local_shape)
+     else:
+         numel = sh_ten.flattened_range.stop - sh_ten.flattened_range.start
+     return numel * torch._utils._element_size(sh_ten.dtype)
+

