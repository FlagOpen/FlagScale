diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/pipeline_parallel/schedules.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/pipeline_parallel/schedules.py.patch
new file mode 100644
index 00000000..6587f89b
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/pipeline_parallel/schedules.py.patch
@@ -0,0 +1,26 @@
+diff --git a/megatron/core/pipeline_parallel/schedules.py b/megatron/core/pipeline_parallel/schedules.py
+index 317789ad6..dcc4f0c39 100644
+--- a/megatron/core/pipeline_parallel/schedules.py
++++ b/megatron/core/pipeline_parallel/schedules.py
+@@ -113,7 +113,11 @@ def get_forward_backward_func():
+     """
+     pipeline_model_parallel_size = parallel_state.get_pipeline_model_parallel_world_size()
+     if pipeline_model_parallel_size > 1:
+-        if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
++        ######### FlagScale Modify ########
++        if parallel_state.get_dualpipev_pipeline_model_parallel_world_size() is not None:
++            from flagscale.train.dualpipev.dualpipev_schedules import forward_backward_pipelining_with_dualpipev
++            forward_backward_func = forward_backward_pipelining_with_dualpipev
++        elif parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
+             forward_backward_func = forward_backward_pipelining_with_interleaving
+         else:
+             forward_backward_func = forward_backward_pipelining_without_interleaving
+@@ -1872,6 +1876,7 @@ def forward_backward_pipelining_without_interleaving(
+         output_tensors = []
+     forward_data_store = []
+ 
++    p2p_communication.warm_up_comm_group(config=config)
+     # Run warmup forward passes.
+     for i in range(num_warmup_microbatches):
+         # Decide to checkpoint all layers' activations of the current micro-batch
+

