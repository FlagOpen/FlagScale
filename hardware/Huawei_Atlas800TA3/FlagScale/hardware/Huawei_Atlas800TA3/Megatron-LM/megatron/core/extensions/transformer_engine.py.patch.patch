diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/extensions/transformer_engine.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/extensions/transformer_engine.py.patch
new file mode 100644
index 00000000..12ab3942
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/extensions/transformer_engine.py.patch
@@ -0,0 +1,108 @@
+diff --git a/megatron/core/extensions/transformer_engine.py b/megatron/core/extensions/transformer_engine.py
+index 79de39b7e..694471b16 100644
+--- a/megatron/core/extensions/transformer_engine.py
++++ b/megatron/core/extensions/transformer_engine.py
+@@ -12,6 +12,9 @@ import torch.nn.functional as F
+ from packaging.version import Version as PkgVersion
+ from torch import Tensor
+ from torch.nn.parameter import Parameter
++from megatron.core.transformer.megatron_adaptor.rms_norm_2d import TPYCollectiveComm
++from megatron.core.transformer.megatron_adaptor.layernorm_2d import LayerNorm2D
++from megatron.core.transformer.megatron_adaptor.rms_norm_2d import RMSNorm2D
+ 
+ from megatron.core.dist_checkpointing.utils import replace_prefix_for_sharding
+ from megatron.core.model_parallel_config import ModelParallelConfig
+@@ -23,6 +26,7 @@ from megatron.core.parallel_state import (
+     get_expert_model_parallel_world_size,
+     get_hierarchical_context_parallel_groups,
+     get_tensor_model_parallel_group,
++    get_tensor_model_parallel_world_size,
+ )
+ from megatron.core.process_groups_config import ModelCommProcessGroups
+ from megatron.core.tensor_parallel.layers import (
+@@ -87,30 +91,38 @@ class TENorm:
+             )
+ 
+         if config.normalization == "LayerNorm":
+-            instance = te.pytorch.LayerNorm(
+-                hidden_size=hidden_size,
+-                eps=eps,
+-                sequence_parallel=config.sequence_parallel,
+-                zero_centered_gamma=config.layernorm_zero_centered_gamma,
+-                **_get_extra_te_kwargs(config),
+-            )
++            if getattr(config, "tp_2d", False):
++                instance = LayerNorm2D(
++                    hidden_size,
++                    eps=eps,
++                    last_dim_split_comm_intf=TPYCollectiveComm(),
++                )
++            else:
++                try:
++                    # using apex implementation
++                    from megatron.core.fusions.fused_layer_norm import FusedLayerNorm
++                    instance = FusedLayerNorm(config=config, hidden_size=hidden_size, eps=eps)
++                except ImportError:
++                    # using torch implementation
++                    instance = torch.nn.LayerNorm(normalized_shape=hidden_size, eps=eps)
++                    add_layer_norm_sp_support(config, instance)
+         elif config.normalization == "RMSNorm":
+-            assert hasattr(
+-                te.pytorch, "RMSNorm"
+-            ), "Transformer-Engine >= v0.11 required to use this feature"
+-            instance = te.pytorch.RMSNorm(
+-                hidden_size=hidden_size,
+-                eps=eps,
+-                sequence_parallel=config.sequence_parallel,
+-                zero_centered_gamma=config.layernorm_zero_centered_gamma,
+-                **_get_extra_te_kwargs(config),
+-            )
++            if getattr(config, "tp_2d", False):
++                instance = RMSNorm2D(
++                    hidden_size,
++                    eps=eps,
++                    last_dim_split_comm_intf=TPYCollectiveComm(),
++                )
++                instance.use_fused_rmsnorm = False
++            else:
++                from ..transformer.megatron_adaptor.fused_rms_norm import RMSNorm
++                instance = RMSNorm(dim=hidden_size, eps=eps, sequence_parallel=config.sequence_parallel, config=config)
++                instance.config.use_fused_rmsnorm = True
+         else:
+-            raise Exception("Only LayerNorm and RMSNorm are curently supported")
++            raise Exception('Only LayerNorm and RMSNorm are curently supported')
+ 
+         return instance
+ 
+-
+ class TELinear(te.pytorch.Linear):
+     """Wrapper for the Transformer-Engine's `Linear` layer.
+ 
+@@ -439,7 +451,7 @@ class TELayerNormColumnParallelLinear(te.pytorch.LayerNormLinear):
+             sequence_parallel=self.config.sequence_parallel,
+             fuse_wgrad_accumulation=self.config.gradient_accumulation_fusion,
+             tp_group=tp_group if torch.distributed.is_initialized() else None,
+-            tp_size=self.config.tensor_model_parallel_size,
++            tp_size=get_tensor_model_parallel_world_size(),
+             get_rng_state_tracker=(
+                 get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None
+             ),
+@@ -863,7 +875,7 @@ class TEDotProductAttention(te.pytorch.DotProductAttention):
+             ),
+             attn_mask_type=attn_mask_type.name,
+             sequence_parallel=self.config.sequence_parallel,
+-            tp_size=self.config.tensor_model_parallel_size,
++            tp_size=get_tensor_model_parallel_world_size(),
+             get_rng_state_tracker=(
+                 get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None
+             ),
+@@ -1161,7 +1173,7 @@ if HAVE_TE and is_te_min_version("1.9.0.dev0"):
+                 return pickle.loads(state.detach().cpu().numpy().tobytes())
+             elif isinstance(state, io.BytesIO):
+                 state.seek(0)
+-                return torch.load(state, map_location="cuda")
++                return torch.load(state, map_location="cuda", weights_only=False)
+             else:
+                 raise RuntimeError("Unsupported checkpoint format.")
+ 
+

