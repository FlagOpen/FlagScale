diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/transformer/transformer_block.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/transformer/transformer_block.py.patch
new file mode 100644
index 00000000..ec829066
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/transformer/transformer_block.py.patch
@@ -0,0 +1,180 @@
+diff --git a/megatron/core/transformer/transformer_block.py b/megatron/core/transformer/transformer_block.py
+old mode 100755
+new mode 100644
+index af0b3234c..64fd6d606
+--- a/megatron/core/transformer/transformer_block.py
++++ b/megatron/core/transformer/transformer_block.py
+@@ -1,5 +1,5 @@
+ # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
+-
++import os
+ from contextlib import nullcontext
+ from dataclasses import dataclass
+ from typing import List, Optional, Union
+@@ -51,12 +51,13 @@ except ImportError:
+         LayerNormImpl = WrappedTorchNorm
+ 
+ 
+-def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] = None) -> int:
++def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] = None, is_dualpipev_first_chunk: Optional[bool] = False) -> int:
+     """
+     Determine the number of transformer layers to build for the current pipeline stage.
+     Args:
+         config (TransformerConfig): Configuration object containing transformer model parameters.
+         vp_stage (Optional[int]): Virtual pipeline stage number.
++        is_dualpipev_first_chunk(Optional[bool]): Is dualpipev first model chunk or not
+ 
+     Returns:
+         int: The number of layers to be built for the current pipeline stage.
+@@ -117,7 +118,7 @@ def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] =
+             and config.num_layers_in_last_pipeline_stage is not None
+         ):
+             num_layers_per_pipeline_rank = config.num_layers_in_last_pipeline_stage
+-    else:
++    elif not config.enable_hetero:
+         # Include the embedding layer and loss layer into pipeline parallelism partition
+         num_layers = config.num_layers
+         if config.account_for_embedding_in_pipeline_split:
+@@ -134,7 +135,7 @@ def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] =
+     if (
+         parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None
+         and config.pipeline_model_parallel_size > 1
+-    ):
++    ) and parallel_state.get_virtual_pipeline_model_parallel_world_size() > 1:
+         # Interleaved pipeline parallelism:
+         # Number of layers in each model chunk is the number of layers in the stage,
+         # divided by the number of model chunks in a stage.
+@@ -155,11 +156,26 @@ def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] =
+         num_layers_per_virtual_stage = num_layers_per_pipeline_rank // vp_size
+ 
+         num_layers_to_build = num_layers_per_virtual_stage
+-
++    ######### FlagScale Begin ########
++    elif config.use_dualpipev:
++        num_layers_per_pipeline_rank_first_chunk = num_layers_per_pipeline_rank // 2
++        if num_layers_per_pipeline_rank % 2 != 0:
++            num_layers_per_pipeline_rank_first_chunk = num_layers_per_pipeline_rank_first_chunk + 1
++        num_layers_per_pipeline_rank_second_chunk = num_layers_per_pipeline_rank - num_layers_per_pipeline_rank_first_chunk
++        if is_dualpipev_first_chunk:
++            num_layers_to_build = num_layers_per_pipeline_rank_first_chunk
++        else:
++            num_layers_to_build = num_layers_per_pipeline_rank_second_chunk
++    ######### FlagScale End ########
+     else:
+         # Non-interleaved pipeline parallelism:
+         # Each stage gets a contiguous set of layers.
+-        num_layers_to_build = num_layers_per_pipeline_rank
++
++        if config.enable_hetero:
++            pipeline_rank = parallel_state.get_pipeline_model_parallel_rank()
++            num_layers_to_build = config.hetero_pipeline_layer_split[pipeline_rank]
++        else:
++            num_layers_to_build = num_layers_per_pipeline_rank
+ 
+     # The embedding (or loss) layer cannot function as a standalone transformer layer
+     # Reduce the number of layers to construct by 1 on the first (or last) stage if the
+@@ -505,6 +521,14 @@ class TransformerBlock(MegatronModule):
+             [s, b, h], and optionally the updated context tensor if cross-attention is used.
+         """
+ 
++        ########## FlagScale Begin ##########
++        # for refined recompute
++        self.current_microbatch = -1
++        if len(self.layers) > 0: # some pp-stage has no layers in pipeline_model_parallel_layout,such as embedding stage
++            if hasattr(self.layers[0], 'current_microbatch'):
++                self.current_microbatch = self.layers[0].current_microbatch
++        ########## FlagScale End ##########
++
+         inference_context = deprecate_inference_params(inference_context, inference_params)
+ 
+         # Delete the obsolete reference to the initial input tensor if necessary
+@@ -546,6 +570,71 @@ class TransformerBlock(MegatronModule):
+         use_inner_fp8_context = self.config.fp8 and self.config.fp8_recipe != Fp8Recipe.delayed
+         outer_fp8_context = get_fp8_context(self.config) if use_outer_fp8_context else nullcontext()
+ 
++        if self.config.recompute_method_per_stage_micro_batch != None:
++            if self.config.virtual_pipeline_model_parallel_size != None:
++                if (
++                    self.config.recompute_method_per_stage_micro_batch[
++                        parallel_state.get_virtual_pipeline_model_parallel_rank()
++                        * self.config.pipeline_model_parallel_size
++                        + parallel_state.get_pipeline_model_parallel_rank()
++                    ][self.current_microbatch]
++                    == 0
++                ):
++                    self.config.recompute_method = 'uniform'
++                elif (
++                    self.config.recompute_method_per_stage_micor_batch[
++                        parallel_state.get_virtual_pipeline_model_parallel_rank()
++                        * self.config.pipeline_model_parallel_size
++                        + parallel_state.get_pipeline_model_parallel_rank()
++                    ][self.current_microbatch]
++                    == 1
++                ):
++                    self.config.recompute_method = 'block'
++                else:
++                    raise ValueError("the item of recompute_method_per_stage_micor_batch must be '0' or '1' ")
++            else:
++                if (
++                    self.config.recompute_method_per_stage_micro_batch[
++                        parallel_state.get_pipeline_model_parallel_rank()
++                    ][self.current_microbatch]
++                    == 0
++                ):
++                    self.config.recompute_method = 'uniform'
++                elif (
++                    self.config.recompute_method_per_stage_micro_batch[
++                        parallel_state.get_pipeline_model_parallel_rank()
++                    ][self.current_microbatch]
++                    == 1
++                ):
++                    self.config.recompute_method = 'block'
++                else:
++                    raise ValueError("the item of recompute_method_per_stage_micor_batch must be '0' or '1' ")
++
++        if self.config.recompute_num_layers_per_stage_micro_batch != None:
++            if self.config.virtual_pipeline_model_parallel_size != None:
++                self.config.recompute_num_layers = self.config.recompute_num_layers_per_stage_micro_batch[
++                    parallel_state.get_virtual_pipeline_model_parallel_rank()
++                    * self.config.pipeline_model_parallel_size
++                    + parallel_state.get_pipeline_model_parallel_rank()
++                ][self.current_microbatch]
++            else:
++                self.config.recompute_num_layers = self.config.recompute_num_layers_per_stage_micro_batch[
++                    parallel_state.get_pipeline_model_parallel_rank()
++                ][self.current_microbatch]
++            if self.config.recompute_num_layers == 0:
++                self.config.recompute_method = None
++                self.config.recompute_granularity = None
++
++        if (
++            self.config.recompute_granularity_per_stage_micro_batch != None
++            and self.config.recompute_granularity_per_stage_micro_batch[
++                parallel_state.get_pipeline_model_parallel_rank()
++            ][self.current_microbatch]
++            == 0
++        ):
++            self.config.recompute_granularity = None
++            self.config.recompute_method = None
++
+         with rng_context, outer_fp8_context:
+             # Forward pass.
+             if self.config.recompute_granularity == 'full' and self.training:
+@@ -634,6 +723,16 @@ class TransformerBlock(MegatronModule):
+         elif isinstance(self.config.moe_layer_freq, list):
+             non_homogeneous_layers = True
+ 
++
++        # TODO: @aoyulong - This is a temporary solution to support single-file-per-tensor ckpt
++        non_homogeneous_layers_env = os.getenv('FS_NON_HOMOGENEOUS_LAYERS', 'False').lower() in (
++            'true',
++            '1',
++            't',
++        )
++        if non_homogeneous_layers_env:
++            non_homogeneous_layers = True
++
+         if self.config.heterogeneous_block_specs:
+             non_homogeneous_layers = True
+ 
+

