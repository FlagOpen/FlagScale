diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py.patch
new file mode 100644
index 00000000..72a80625
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py.patch
@@ -0,0 +1,212 @@
+diff --git a/megatron/core/optimizer/distrib_optimizer.py b/megatron/core/optimizer/distrib_optimizer.py
+index 369f4747b..6ff5aa63d 100644
+--- a/megatron/core/optimizer/distrib_optimizer.py
++++ b/megatron/core/optimizer/distrib_optimizer.py
+@@ -7,7 +7,9 @@ import itertools
+ from dataclasses import replace
+ from logging import getLogger
+ from typing import Callable, Dict, List, Optional, Tuple
++from functools import wraps, partial
+ 
++import inspect
+ import torch
+ 
+ HAVE_APEX_OR_TE = True
+@@ -641,41 +643,13 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+         return state_dict
+ 
+     def load_state_dict(self, state_dict):
+-        """Load the state dict.
+-
+-        As detailed in state_dict(), the state dict contains all non-
+-        parameter-related variables. This method is notably longer than
+-        state_dict(), because the Torch optimizers state has yet to be
+-        allocated at this point, and so we must do a cross referencing between
+-        the optimizers state (and the ordering it expects for parameter state)
+-        and this DP rank's shards. The optimizer at this point does not contain
+-        any tensor dimension information, so we must get these dimensions from
+-        the DP shards mapped during DistributedOptimizer.__init__().
+-
+-        The tensor parameter state is loaded via load_parameter_state(), and
+-        so this method also must populate the loaded state dict with dummy
+-        tensor data (i.e., via torch.empty() below). This will be overwritten
+-        during load_parameter_state().
+-
+-        ** Note: Torch optimizer's state structure. **
+-        The Torch optimizer stores its state in two levels. The top level is a
+-        list of groups, where each group contains a list of integer indexes
+-        (corresponding to parameters) that index into a master parameter list
+-        that is shared by all groups. As such, three values are necessary for
+-        maintaining this ordering:
+-
+-        - group_index : The group to which a parameter belongs.
+-        - group_order : The index of a parameter within its group.
+-        - state_order : The index of a parameter within the shared parameter
+-            list.
+-        """
+         if len(self.optimizer.state) == 0:
+             if isinstance(self.optimizer, HybridDeviceOptimizer):
+                 self.optimizer.dummy_step()
+             elif self.ddp_config.use_custom_fsdp:
+                 # Initializes optimizer states with dummy values.
+ 
+-                # This step is necessary to ensure that the optimizer's states are
++                # This step is necessary to ensure that the optimizers' states are
+                 # initialized correctly. These dummy states will be replaced in-place
+                 # during the loading of distributed checkpoints.
+                 for group in self.optimizer.param_groups:
+@@ -687,16 +661,39 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+                 self.optimizer.step()
+                 self.optimizer.zero_grad()
+ 
+-        # Get the Torch optimizer's state dict.
++        # Get the Torch optimizers' state dict.
+         # - This 'inner' optimizer at this point is unallocated, and only
+         #   contains an integer ordering of parameters within each group, and
+         #   the ordering of parameters within its flattened parameter state
+         #   list.
++        def make_needed_groups(param_group):
++            needed_groups = []
++            for key in param_group_identifier_keys:
++                # NeMo changes these variable names from `lr_mult` and `wd_mult`
++                # to `pre_lr_mult` and `pre_wd_mult`, so we need to check both.
++                if key in param_group:
++                    pass
++                elif f"pre_{key}" in param_group:
++                    key = f"pre_{key}"
++                else:
++                    raise ValueError(
++                        f"Key {key} (or pre_{key}) not found in param_group {param_group}."
++                    )
++                needed_groups.append(param_group[key])
++            needed_groups = tuple(needed_groups)
++            return needed_groups
++
++        param_groups_map = {}
++        for param_group in state_dict["optimizer"]["param_groups"]:
++            needed_groups = make_needed_groups(param_group)
++            param_groups_map[needed_groups] = param_group
+         inner_state_dict = self.optimizer.state_dict()
+-        state_dict_param_groups = [
+-            {**group, "params": list(inner_state_dict["param_groups"][idx]["params"])}
+-            for idx, group in enumerate(state_dict["optimizer"]["param_groups"])
+-        ]
++        state_dict_param_groups = []
++        for inner_param_group in inner_state_dict["param_groups"]:
++            needed_groups = make_needed_groups(inner_param_group)
++            state_dict_param_groups.append(
++                {**param_groups_map[needed_groups], "params": inner_param_group['params']}
++            )
+ 
+         # Allocate or retrieve optimizer state (i.e., tensors).
+         if len(self.optimizer.state) == 0:
+@@ -722,23 +719,15 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+ 
+                             # Allocate dummy tensors.
+                             numel = len(param_range_map["gbuf_world"])
+-                            init_shard = lambda dtype=torch.float32: torch.empty(
+-                                (numel,), dtype=dtype, device=torch.cuda.current_device()
+-                            )
+ 
+-                            # For precision_aware_optimizer, the empty tensors should also be
+-                            #  initialized with the correct dtype.
+-                            tensors = {
+-                                "exp_avg": init_shard(self.config.exp_avg_dtype),
+-                                "exp_avg_sq": init_shard(self.config.exp_avg_sq_dtype),
+-                            }
+-                            if self.config.use_precision_aware_optimizer_no_fp8_or_ds_fp8:
+-                                if self.config.store_param_remainders and self.config.bf16:
+-                                    tensors["master_param"] = init_shard(torch.int16)
+-                                else:
+-                                    tensors["master_param"] = init_shard(
+-                                        self.config.main_params_dtype
+-                                    )
++                            def init_shard(elements_count):
++                                return torch.empty(
++                                    (elements_count,), dtype=torch.float32, device=torch.cuda.current_device()
++                                )
++
++                            tensors = {"exp_avg": init_shard(numel), "exp_avg_sq": init_shard(numel)}
++                            if self.config.use_precision_aware_optimizer:
++                                tensors["master_param"] = init_shard(numel)
+                             state_dict_state.append((state_order, tensors))
+ 
+             # Sort by state order (see method docstring for details).
+@@ -752,7 +741,8 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+         # Extract 'step', for non-Apex/TE support.
+         if not HAVE_APEX_OR_TE:
+             steps = list(set([g["step"] for g in state_dict["optimizer"]["param_groups"]]))
+-            assert len(steps) == 1
++            if len(steps) != 1:
++                raise AssertionError(f"Expect exactly one kind of step, but detect {len(steps)} kinds of steps")
+             step = torch.tensor(steps[0], dtype=torch.float)
+ 
+             for s in state_dict_state.values():
+@@ -760,12 +750,13 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+                 s["step"] = step
+         elif isinstance(self.optimizer, HybridDeviceOptimizer):
+             # Handle Torch AdamW special case, which, unlike FusedAdam, Torch AdamW
+-            # has an extra optimizer state "step".
++            # has an extra optimizer state “step”.
+             steps = list(
+                 set([g["step"] for g in state_dict["optimizer"]["param_groups"] if "step" in g])
+             )
+             if len(steps) != 0:
+-                assert len(steps) == 1, f"steps: {steps}"
++                if len(steps) != 1:
++                    raise AssertionError(f"steps: {steps}")
+                 step = torch.tensor(steps[0], dtype=torch.float32, device="cpu")
+                 for v in self.optimizer.state.values():
+                     v["step"] = step.detach().clone()
+@@ -786,19 +777,21 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+                 self.grad_scaler.load_state_dict(state_dict['grad_scaler'])
+             else:
+                 logger.info(
+-                    '***WARNING*** fould the grad scaler in the '
++                    '***WARNING*** found the grad scaler in the '
+                     'checkpoint but it is None in the class. '
+-                    'Skipping loading grad scaler ...'
++                    'Skip loading grad scaler ...'
+                 )
+ 
+         if 'param_state' in state_dict:
+-            assert 'param_state_sharding_type' in state_dict, state_dict.keys()
++            if 'param_state_sharding_type' not in state_dict:
++                raise AssertionError(
++                    f"Could not find 'param_state_sharding_type' in state_dict."
++                    f"Current state_dict.key(): {state_dict.key()}")
+             param_state = state_dict['param_state']
+             sharding_type = state_dict['param_state_sharding_type']
+             if self.ddp_config.use_custom_fsdp:
+-                assert (
+-                    sharding_type == "fully_sharded_model_space"
+-                ), "Only fully sharded model space is supported"
++                if sharding_type != "fully_sharded_model_space":
++                    raise AssertionError("Only fully sharded model space is supported")
+             logger.info(f'Loading distributed optimizer sharded state of type {sharding_type}')
+             if sharding_type == 'dp_zero_gather_scatter':
+                 self.load_parameter_state_from_dp_zero(param_state)
+@@ -2192,6 +2185,22 @@ class DistributedOptimizer(MixedPrecisionOptimizer):
+         copy_group_params(self.model_float16_groups, self.shard_fp32_from_float16_groups)
+         copy_group_params(self.model_fp32_groups, self.shard_fp32_groups)
+ 
++    def step_with_ready_grads_distrib_opti_wrapper(func):
++        @wraps(func)
++        def wrapper(*args, **kwargs):
++            self = args[0]
++            is_moe_param = getattr(self, 'is_moe_param', None)
++
++            # determine group type
++            for model_chunk in self.model_chunks:
++                if 'dense_or_moe_group' in inspect.signature(model_chunk.start_param_sync).parameters:
++                    model_chunk.start_param_sync = partial(model_chunk.start_param_sync, dense_or_moe_group=is_moe_param)
++
++            update_successful = func(*args, **kwargs)
++            return update_successful
++        return wrapper
++    
++    @step_with_ready_grads_distrib_opti_wrapper
+     @torch.no_grad()
+     def step_with_ready_grads(self) -> bool:
+         """Step the optimizer with ready gradients, return successful.
+

