diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/training/initialize.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/training/initialize.py.patch
new file mode 100644
index 00000000..4cd6a7c3
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/training/initialize.py.patch
@@ -0,0 +1,195 @@
+diff --git a/megatron/training/initialize.py b/megatron/training/initialize.py
+index b4b5aa027..41cd32685 100644
+--- a/megatron/training/initialize.py
++++ b/megatron/training/initialize.py
+@@ -29,9 +29,12 @@ from megatron.training import inprocess_restart
+ from megatron.training.arguments import parse_args, validate_args
+ from megatron.training.async_utils import init_persistent_async_worker
+ from megatron.training.checkpointing import load_args_from_checkpoint
+-from megatron.training.global_vars import set_global_variables
++from megatron.training.global_vars import set_global_variables, set_global_writers
+ from megatron.training.yaml_arguments import validate_yaml
+ 
++from flagscale.train import FSTrainArguments
++from flagscale.train import set_parallel_context, set_get_spiky_loss_detector
++
+ logger = logging.getLogger(__name__)
+ 
+ 
+@@ -82,11 +85,18 @@ def initialize_megatron(
+     if args.async_save and args.use_persistent_ckpt_worker:
+         init_persistent_async_worker()
+ 
++    if args.hetero_process_meshes is not None:
++        fs_argument = FSTrainArguments(args)
++        fs_argument.pre_validate_args()
++
+     if args.yaml_cfg is not None:
+         args = validate_yaml(args, args_defaults)
+     else:
+         validate_args(args, args_defaults)
+ 
++    if args.hetero_process_meshes is not None:
++        fs_argument.post_validate_args()
++
+     # set global args, build tokenizer, and set adlr-autoresume,
+     # tensorboard-writer, and timers.
+     set_global_variables(args)
+@@ -114,6 +124,9 @@ def initialize_megatron(
+         result_rejected_tracker_filename=args.result_rejected_tracker_filename,
+     )
+ 
++    if args.auto_skip_spiky_loss:
++        set_get_spiky_loss_detector(args=args)
++
+     # torch.distributed initialization
+     def finish_mpu_init():
+         args = get_args()
+@@ -137,6 +150,9 @@ def initialize_megatron(
+ 
+             MoEAuxLossAutoScaler.set_loss_scale(torch.ones(1, device=torch.cuda.current_device()))
+ 
++        # Set tensorboard writer and wandb writer.
++        set_global_writers(args)
++
+     if skip_mpu_initialization:
+         return None
+ 
+@@ -170,70 +186,13 @@ def initialize_megatron(
+ 
+ 
+ def _compile_dependencies():
+-
+-    args = get_args()
+-
+-    # =========================
+-    # Compile dataset C++ code.
+-    # =========================
+-    # TODO: move this to ninja
+     if torch.distributed.get_rank() == 0:
+         start_time = time.time()
+-        print("> compiling dataset index builder ...")
++        print('> compiling dataset index builder ...')
+         from megatron.core.datasets.utils import compile_helpers
+-
+         compile_helpers()
+-        print(
+-            ">>> done with dataset index builder. Compilation time: {:.3f} "
+-            "seconds".format(time.time() - start_time),
+-            flush=True,
+-        )
+-
+-    # ==================
+-    # Load fused kernels
+-    # ==================
+-
+-    # Custom kernel constraints check.
+-    seq_len = args.seq_length
+-    attn_batch_size = (
+-        args.num_attention_heads / args.tensor_model_parallel_size
+-    ) * args.micro_batch_size
+-    # Constraints on sequence length and attn_batch_size to enable warp based
+-    # optimization and upper triangular optimization (for causal mask)
+-    custom_kernel_constraint = (
+-        seq_len > 16 and seq_len <= 16384 and seq_len % 4 == 0 and attn_batch_size % 4 == 0
+-    )
+-    # Print a warning.
+-    if not ((args.fp16 or args.bf16) and custom_kernel_constraint and args.masked_softmax_fusion):
+-        if args.rank == 0:
+-            print(
+-                "WARNING: constraints for invoking optimized"
+-                " fused softmax kernel are not met. We default"
+-                " back to unfused kernel invocations.",
+-                flush=True,
+-            )
+-
+-    # Always build on rank zero first.
+-    if torch.distributed.get_rank() == 0:
+-        start_time = time.time()
+-        print("> compiling and loading fused kernels ...", flush=True)
+-        fused_kernels.load(args)
+-        torch.distributed.barrier()
+-    else:
+-        torch.distributed.barrier()
+-        fused_kernels.load(args)
+-    # Simple barrier to make sure all ranks have passed the
+-    # compilation phase successfully before moving on to the
+-    # rest of the program. We think this might ensure that
+-    # the lock is released.
+-    torch.distributed.barrier()
+-    if torch.distributed.get_rank() == 0:
+-        print(
+-            ">>> done with compiling and loading fused kernels. "
+-            "Compilation time: {:.3f} seconds".format(time.time() - start_time),
+-            flush=True,
+-        )
+-
++        print('>>> done with dataset index builder. Compilation time: {:.3f} '
++              'seconds'.format(time.time() - start_time), flush=True)
+ 
+ def _initialize_tp_communicators():
+     """initializing the communicators with user buffers for high-performance tensor-model-parallel
+@@ -333,12 +292,25 @@ def _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, s
+             'timeout': timedelta(minutes=args.distributed_timeout_minutes),
+         }
+ 
++        if args.enable_hetero and args.hetero_use_cpu_communication:
++            # if not all(device_type == args.hetero_device_types[0] for device_type in args.hetero_device_types):
++            #     init_process_group_kwargs['backend'] = 'gloo'
++            init_process_group_kwargs['backend'] = "cpu:gloo"
++        # TODO: @aoyulong the init_process_group will be hanging if the device_id is set
++        # if packaging.version.Version(torch.__version__) >= packaging.version.Version("2.3.0"):
++        #     init_process_group_kwargs['device_id'] = device_id
++
+         torch.distributed.init_process_group(**init_process_group_kwargs)
+         inprocess_restart.maybe_force_nccl_backend_init(device_id)
+ 
+     # Set the tensor model-parallel, pipeline model-parallel, and
+     # data-parallel communicators.
+     if device_count > 0:
++        # Set the parallel context.
++        if args.enable_hetero:
++            set_parallel_context(args)
++            return
++
+         if mpu.model_parallel_is_initialized():
+             print("model parallel is already initialized")
+         else:
+@@ -378,6 +350,7 @@ def _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, s
+                 get_position_embedding_ranks=get_position_embedding_ranks,
+                 create_gloo_process_groups=args.enable_gloo_process_groups,
+                 high_priority_stream_groups=args.high_priority_stream_groups,
++                create_dualpipev_parallel_size=args.use_dualpipev,
+             )
+             if args.rank == 0:
+                 print(
+@@ -433,6 +406,20 @@ def write_args_to_tensorboard():
+             writer.add_text(arg, str(getattr(args, arg)), global_step=args.iteration)
+ 
+ 
++def set_jit_fusion_options_wrapper(fn):
++    @wraps(fn)
++    def wrapper(*args, **kwargs):
++        def _jit_set_nvfuser_enabled(option):
++            pass
++        torch._C._jit_set_nvfuser_enabled = _jit_set_nvfuser_enabled
++        fn(*args, **kwargs)
++        args = get_args()
++        #if args.jit_compile:
++        #    torch_npu.npu.set_compile_mode(jit_compile=True)
++
++    return wrapper
++
++@set_jit_fusion_options_wrapper
+ def set_jit_fusion_options():
+     """Set PyTorch JIT layer fusion options."""
+     # flags required to enable jit fusion kernels
+@@ -456,7 +443,6 @@ def set_jit_fusion_options():
+ 
+     _warmup_jit_function()
+ 
+-
+ def _warmup_jit_function():
+     """Compilie JIT functions before the main training steps"""
+     args = get_args()
+

