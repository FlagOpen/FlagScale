diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/training/dist_signal_handler.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/training/dist_signal_handler.py.patch
new file mode 100644
index 00000000..3cf1a2d9
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/megatron/training/dist_signal_handler.py.patch
@@ -0,0 +1,35 @@
+diff --git a/megatron/training/dist_signal_handler.py b/megatron/training/dist_signal_handler.py
+index f4b4fbf5c..91c2ab6ca 100644
+--- a/megatron/training/dist_signal_handler.py
++++ b/megatron/training/dist_signal_handler.py
+@@ -1,5 +1,6 @@
+ # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
+ import signal
++from functools import wraps
+ 
+ import torch
+ 
+@@ -11,7 +12,22 @@ def get_world_size():
+         world_size = 1
+     return world_size
+ 
++def get_device_wrapper(func):
++    @wraps(func)
++    def wrapper(*args, **kwargs):
++        backend = torch.distributed.get_backend()
++        local_rank = args[0]
++        if backend == 'hccl':
++            if local_rank is None:
++                device = torch.device('cuda')
++            else:
++                device = torch.device(f'cuda:{local_rank}')
++        else:
++            device = func(*args, **kwargs)
++        return device
++    return wrapper
+ 
++@get_device_wrapper
+ def get_device(local_rank=None):
+     backend = torch.distributed.get_backend()
+     if backend == 'nccl':
+

