diff --git a/hardware/Huawei_Atlas800TA3/Megatron-LM/tools/checkpoint/saver_legacy.py.patch b/hardware/Huawei_Atlas800TA3/Megatron-LM/tools/checkpoint/saver_legacy.py.patch
new file mode 100644
index 00000000..a40ec09d
--- /dev/null
+++ b/hardware/Huawei_Atlas800TA3/Megatron-LM/tools/checkpoint/saver_legacy.py.patch
@@ -0,0 +1,51 @@
+diff --git a/tools/checkpoint/saver_legacy.py b/tools/checkpoint/saver_legacy.py
+index 50af6a571..ab8cfebb2 100644
+--- a/tools/checkpoint/saver_legacy.py
++++ b/tools/checkpoint/saver_legacy.py
+@@ -132,6 +132,8 @@ def save_checkpoint(queue, args):
+         sys.argv.append('--untie-embeddings-and-output-weights')
+     if not md.linear_bias:
+         sys.argv.append('--disable-bias-linear')
++    if not md.linear_bias_qkv:
++        sys.argv.append('--disable-bias-linear-qkv')
+ 
+     if md.model_type == 'BERT' and not md.bert_binary_head:
+         sys.argv.append('--bert-no-binary-head')
+@@ -359,7 +361,35 @@ def save_checkpoint(queue, args):
+                 if not hasattr(models[0].language_model, 'output_layer'):
+                     print("ERROR: got an output layer, but model does not have one")
+                     exit(1)
+-                output_layer_weight = torch.chunk(msg.pop("weight"), args.target_tensor_parallel_size, dim=0)
++                # Deal with padding
++                orig_output_layer_weight = msg.pop("weight")
++                if md.true_vocab_size is not None:
++                    # figure out what our padded vocab size is
++                    orig_output_layer_size = orig_output_layer_weight.shape[0]
++                    margs.padded_vocab_size = _vocab_size_with_padding(md.true_vocab_size, margs)
++
++                    # Cut out extra padding we don't need
++                    if orig_output_layer_size > margs.padded_vocab_size:
++                        full_output_layer_weight = orig_output_layer_weight[0:margs.padded_vocab_size,:]
++
++                    # Expanding embedding to larger size by replicating final entry
++                    elif orig_output_layer_size < margs.padded_vocab_size:
++                        padding_size = margs.padded_vocab_size - orig_output_layer_size
++
++                        full_output_layer_weight = torch.cat((
++                            orig_output_layer_weight,
++                            orig_output_layer_weight[-1].unsqueeze(0).expand(padding_size, -1)))
++
++                    # Same size!
++                    else:
++                        full_output_layer_weight = orig_output_layer_weight
++                else:
++                    print("Original vocab size not specified, leaving embedding table as-is. "
++                          "If you've changed the tensor parallel size this could cause problems.")
++                    margs.padded_vocab_size = orig_output_layer_weight.shape[0]
++                    full_output_layer_weight = orig_output_layer_weight
++
++                output_layer_weight = torch.chunk(full_output_layer_weight, args.target_tensor_parallel_size, dim=0)
+                 for tp_rank in range(args.target_tensor_parallel_size):
+                     models[tp_rank].language_model.output_layer.weight.data.copy_(output_layer_weight[tp_rank])
+                 del output_layer_weight
+

