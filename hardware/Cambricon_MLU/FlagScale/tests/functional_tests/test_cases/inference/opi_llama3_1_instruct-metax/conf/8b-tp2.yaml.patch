diff --git a/tests/functional_tests/test_cases/inference/deepseek_flaggems/conf/tp2.yaml b/tests/functional_tests/test_cases/inference/opi_llama3_1_instruct-metax/conf/8b-tp2.yaml
similarity index 68%
rename from tests/functional_tests/test_cases/inference/deepseek_flaggems/conf/tp2.yaml
rename to tests/functional_tests/test_cases/inference/opi_llama3_1_instruct-metax/conf/8b-tp2.yaml
index d830b7da..67ee9fd7 100644
--- a/tests/functional_tests/test_cases/inference/deepseek_flaggems/conf/tp2.yaml
+++ b/tests/functional_tests/test_cases/inference/opi_llama3_1_instruct-metax/conf/8b-tp2.yaml
@@ -1,10 +1,10 @@
 defaults:
   - _self_
-  - inference: tp2
+  - inference: 8b-tp2
 
 experiment:
-  exp_name: deepseek_flaggems
-  exp_dir: tests/functional_tests/test_cases/inference/deepseek_flaggems/results_test/tp2
+  exp_name: opi_llama3_1_instruct-metax
+  exp_dir: tests/functional_tests/test_cases/inference/opi_llama3_1_instruct-metax/results_test/8b-tp2
   task:
     type: inference
     backend: vllm
@@ -13,20 +13,20 @@ experiment:
     hostfile: null
   cmds:
     before_start:
-      source /root/miniconda3/bin/activate flagscale-inference
+      source /opt/conda/bin/activate flagscale-inference
   envs:
     HYDRA_FULL_ERROR: 1
     CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
     CUDA_DEVICE_MAX_CONNECTIONS: 1
     CUBLAS_WORKSPACE_CONFIG: ":4096:8"
-    NCCL_ALGO: "Tree"
+    NCCL_ALGO: "Ring"
     NVTE_APPLY_QK_LAYER_SCALING: 0
     NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
     NVTE_FLASH_ATTN: 0
     NVTE_FUSED_ATTN: 0
     CUDNN_BENCHMARK: "false"
     CUDNN_DETERMINISTIC: "true"
-    USE_FLAGGEMS: "true"
+    GEMS_VENDOR: "metax"
 
 action: run
 

