diff --git a/hardware/Metax_C550/FlagScale/diff.patch b/hardware/Metax_C550/FlagScale/diff.patch
index b6858c1f..e55789a8 100644
--- a/hardware/Metax_C550/FlagScale/diff.patch
+++ b/hardware/Metax_C550/FlagScale/diff.patch
@@ -56,19 +56,18 @@ index 719a6726..bb2a28fd 100644
 \ No newline at end of file
 diff --git a/examples/deepseek_r1_w8a8/conf/hostfile.txt b/examples/deepseek_r1_w8a8/conf/hostfile.txt
 new file mode 100644
-index 00000000..544e3d37
+index 00000000..1410176e
 --- /dev/null
 +++ b/examples/deepseek_r1_w8a8/conf/hostfile.txt
 @@ -0,0 +1,5 @@
-+# ip slots type=xxx[optional]
++# ip slots type=x.x.x.x
 +# master node
 +x.x.x.x slots=8 type=gpu
 +# worker nodes
 +x.x.x.x slots=8 type=gpu
-\ No newline at end of file
 diff --git a/examples/deepseek_r1_w8a8/conf/serve.yaml b/examples/deepseek_r1_w8a8/conf/serve.yaml
 new file mode 100644
-index 00000000..32cbf6a0
+index 00000000..370bc3ca
 --- /dev/null
 +++ b/examples/deepseek_r1_w8a8/conf/serve.yaml
 @@ -0,0 +1,20 @@
@@ -84,7 +83,7 @@ index 00000000..32cbf6a0
 +    use_fs_serve: false
 +  runner:
 +    hostfile: examples/deepseek_r1_w8a8/conf/hostfile.txt
-+    docker: flagrelease_metax
++    docker: ds_flagscale3
 +    ssh_port: 22
 +  cmds:
 +    before_start:
@@ -94,43 +93,46 @@ index 00000000..32cbf6a0
 +    dir: ${experiment.exp_dir}/hydra
 diff --git a/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
 new file mode 100644
-index 00000000..1f4911e0
+index 00000000..ee3171e8
 --- /dev/null
 +++ b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
-@@ -0,0 +1,12 @@
+@@ -0,0 +1,14 @@
 +- serve_id: vllm_model
 +  engine: vllm
 +  engine_args:
-+    model: /nfs/deepseek_r1_bf16_w8a8/vllm_quant_model # path of weight of deepseek r1
++    model: /share/project/zhangyu.d/deepseek_r1_bf16_w8a8/vllm_quant_model # path of weight of deepseek r1
 +    tensor_parallel_size: 8
 +    pipeline_parallel_size: 2
-+    gpu_memory_utilization: 0.9
-+    max_model_len: 4096
++    gpu_memory_utilization: 0.95
++    max_model_len: 32768
++    max_seq_len_to_capture: 32768
++    max_num_batched_tokens: 32768
 +    swap_space: 16
 +    dtype: bfloat16
 +    trust_remote_code: true
 +    distributed_executor_backend: ray
-\ No newline at end of file
 diff --git a/examples/qwen3/conf/serve.yaml b/examples/qwen3/conf/serve.yaml
-index d8006ef8..7859ad53 100644
+index f3e09dca..0a098508 100644
 --- a/examples/qwen3/conf/serve.yaml
 +++ b/examples/qwen3/conf/serve.yaml
-@@ -1,9 +1,9 @@
+@@ -1,19 +1,16 @@
  defaults:
  - _self_
 -- serve: 0_6b
-+- serve: 32b
++- serve: 8b
  
  experiment:
 -  exp_name: qwen3_0.6b
-+  exp_name: qwen3_32b
++  exp_name: qwen3_8b
    exp_dir: outputs/${experiment.exp_name}
    task:
      type: serve
-@@ -11,9 +11,6 @@ experiment:
-     use_fs_serve: false
++  deploy:
++    use_fs_serve: false
    runner:
      hostfile: null
+-    deploy:
+-      use_fs_serve: false
 -  envs:
 -    CUDA_VISIBLE_DEVICES: 0
 -    CUDA_DEVICE_MAX_CONNECTIONS: 1
@@ -160,3 +162,166 @@ index 00000000..991ac232
 +      trust_remote_code: true
 +      distributed_executor_backend: ray
 \ No newline at end of file
+diff --git a/examples/qwen3/conf/serve/8b.yaml b/examples/qwen3/conf/serve/8b.yaml
+new file mode 100644
+index 00000000..f031034c
+--- /dev/null
++++ b/examples/qwen3/conf/serve/8b.yaml
+@@ -0,0 +1,15 @@
++- serve_id: vllm_model
++  engine: vllm
++  engine_args:
++    model: /nfs/models/Qwen3-8B/
++    host: 0.0.0.0
++    max_model_len: 4096
++    uvicorn_log_level: warning
++  engine_args_specific:
++    vllm:
++      tensor_parallel_size: 1
++      pipeline_parallel_size: 1
++      gpu_memory_utilization: 0.9
++      swap_space: 16
++      dtype: bfloat16
++      trust_remote_code: true
+diff --git a/examples/qwen3/conf/serve_atmb.yaml b/examples/qwen3/conf/serve_atmb.yaml
+index 34683dae..11c82b4e 100644
+--- a/examples/qwen3/conf/serve_atmb.yaml
++++ b/examples/qwen3/conf/serve_atmb.yaml
+@@ -8,12 +8,12 @@ experiment:
+   exp_dir: outputs/${experiment.exp_name}
+   task:
+     type: serve
++  deploy:
++    port: 6701
++    use_fs_serve: false
+   runner:
+     nnodes: 1
+     nproc_per_node: 4
+-    deploy:
+-      port: 6701
+-      use_fs_serve: false
+   envs:
+     CUDA_VISIBLE_DEVICES: 0,1,2,3
+     CUDA_DEVICE_MAX_CONNECTIONS: 1
+diff --git a/examples/qwen3/conf/train.yaml b/examples/qwen3/conf/train.yaml
+index fb0ec63b..f741f52f 100644
+--- a/examples/qwen3/conf/train.yaml
++++ b/examples/qwen3/conf/train.yaml
+@@ -1,15 +1,13 @@
+ defaults:
+   - _self_
+-  # - train: 30b_a3b
+-  - train: 32b
++  - train: 30b_a3b
+ 
+ experiment:
+-  # exp_name: Qwen3-30b-a3b-Train
+-  exp_name: Qwen3-32b-Train
++  exp_name: Qwen3-Test
+   seed: 42
+   save_steps: 10000
+-  load: null
+-  exp_dir: xxx
++  load: None
++  exp_dir: /xxx
+   ckpt_format: torch
+   task:
+     type: train
+@@ -21,7 +19,7 @@ experiment:
+     rdzv_backend: static
+     hostfile: null
+   cmds:
+-    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
++    before_start: "ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale"
+   envs:
+     LOGLEVEL: "INFO"
+     CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
+diff --git a/examples/qwen3/conf/train/30b_a3b.yaml b/examples/qwen3/conf/train/30b_a3b.yaml
+index c7fdf487..642c2cb4 100644
+--- a/examples/qwen3/conf/train/30b_a3b.yaml
++++ b/examples/qwen3/conf/train/30b_a3b.yaml
+@@ -1,14 +1,19 @@
+ system:
+   no_shared_fs: ${experiment.runner.no_shared_fs}
+-  num_workers: 2
+-  tensor_model_parallel_size: 1
++  num_workers: 16
++  tensor_model_parallel_size: 2
+   pipeline_model_parallel_size: 2
+-  expert_model_parallel_size: 4
++  expert_model_parallel_size: 2
+   context_parallel_size: 1
++  disable_bias_linear: true
++  reset_position_ids: True
++  reset_attention_mask: True
++  qk_layernorm: true
+   sequence_parallel: true
+   use_distributed_optimizer: true
+-  # overlap_grad_reduce: true
+-  # overlap_param_gather: true
++  overlap_grad_reduce: true
++  overlap_param_gather: true
++  finetune: false
+   precision:
+     bf16: true
+     attention_softmax_in_fp32: true
+@@ -33,9 +38,8 @@ model:
+   transformer_impl: transformer_engine
+   num_layers: 48
+   hidden_size: 2048
+-  num_attention_heads: 32
+   kv_channels: 128
+-  group_query_attention: true
++  num_attention_heads: 32
+   num_query_groups: 4 # num_key_value_heads
+   seq_length: 4096
+   max_position_embeddings: 40960
+@@ -44,14 +48,14 @@ model:
+   rotary_base: 1000000
+   swiglu: true
+   normalization: RMSNorm
+-  qk_layernorm: true
+   init_method_std: 0.02
+   attention_dropout: 0.0
+   hidden_dropout: 0.0
++  clip_grad: 1.0
++  position_embedding_type: rope
+   untie_embeddings_and_output_weights: true
+   no_position_embedding: true
+   no_rope_fusion: true
+-  disable_bias_linear: true
+ 
+   # moe args ===================
+   ffn_hidden_size: 6144
+@@ -67,26 +71,24 @@ model:
+   seed: ${experiment.seed}
+   # finetune: false
+   micro_batch_size: 1
+-  global_batch_size: 128 #2048
++  global_batch_size: 2048
+   eval_iters: 0
+-  train_iters: 102400
++  train_samples: 244141056 #1T #29297664 #120B tokens
+ 
+   optimizer:
+-    clip_grad: 1.0
+     weight_decay: 0.1
+     adam_beta1: 0.9
+     adam_beta2: 0.95
+     lr_scheduler:
+       lr: 3.0e-3
+       min_lr: 3.0e-4
+-      lr_warmup_fraction: 0.01
++      lr_warmup_samples: 2048000
+       lr_decay_style: WSD
+       lr_wsd_decay_style: cosine
+-      lr_wsd_decay_iters: 10
++      lr_wsd_decay_samples: 2048
++
+ 
+ data:
+-  reset_position_ids: True
+-  reset_attention_mask: True
+   data_path: /path
+   split: 1
+   no_mmap_bin_files: true

