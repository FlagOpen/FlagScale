diff --git a/hardware/Metax_C550/vllm/diff.patch b/hardware/Metax_C550/vllm/diff.patch
index e7a43def..96c84fba 100644
--- a/hardware/Metax_C550/vllm/diff.patch
+++ b/hardware/Metax_C550/vllm/diff.patch
@@ -1,39 +1,3106 @@
+diff --git a/.buildkite/check-wheel-size.py b/.buildkite/check-wheel-size.py
+index 68aff793a..a378bc6ba 100644
+--- a/.buildkite/check-wheel-size.py
++++ b/.buildkite/check-wheel-size.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ import os
+ import sys
+@@ -9,12 +8,12 @@ import zipfile
+ # Note that we have 400 MiB quota, please use it wisely.
+ # See https://github.com/pypi/support/issues/3792 .
+ # Please also sync the value with the one in Dockerfile.
+-VLLM_MAX_SIZE_MB = int(os.environ.get("VLLM_MAX_SIZE_MB", 400))
++VLLM_MAX_SIZE_MB = int(os.environ.get('VLLM_MAX_SIZE_MB', 400))
+ 
+ 
+ def print_top_10_largest_files(zip_file):
+     """Print the top 10 largest files in the given zip file."""
+-    with zipfile.ZipFile(zip_file, "r") as z:
++    with zipfile.ZipFile(zip_file, 'r') as z:
+         file_sizes = [(f, z.getinfo(f).file_size) for f in z.namelist()]
+         file_sizes.sort(key=lambda x: x[1], reverse=True)
+         for f, size in file_sizes[:10]:
+@@ -29,18 +28,14 @@ def check_wheel_size(directory):
+                 wheel_path = os.path.join(root, file_name)
+                 wheel_size_mb = os.path.getsize(wheel_path) / (1024 * 1024)
+                 if wheel_size_mb > VLLM_MAX_SIZE_MB:
+-                    print(
+-                        f"Not allowed: Wheel {wheel_path} is larger "
+-                        f"({wheel_size_mb:.2f} MB) than the limit "
+-                        f"({VLLM_MAX_SIZE_MB} MB)."
+-                    )
++                    print(f"Not allowed: Wheel {wheel_path} is larger "
++                          f"({wheel_size_mb:.2f} MB) than the limit "
++                          f"({VLLM_MAX_SIZE_MB} MB).")
+                     print_top_10_largest_files(wheel_path)
+                     return 1
+                 else:
+-                    print(
+-                        f"Wheel {wheel_path} is within the allowed size "
+-                        f"({wheel_size_mb:.2f} MB)."
+-                    )
++                    print(f"Wheel {wheel_path} is within the allowed size "
++                          f"({wheel_size_mb:.2f} MB).")
+     return 0
+ 
+ 
+@@ -50,4 +45,4 @@ if __name__ == "__main__":
+         sys.exit(1)
+ 
+     directory = sys.argv[1]
+-    sys.exit(check_wheel_size(directory))
++    sys.exit(check_wheel_size(directory))
+\ No newline at end of file
+diff --git a/.buildkite/generate_index.py b/.buildkite/generate_index.py
+index 7045d8810..36e1b6c01 100644
+--- a/.buildkite/generate_index.py
++++ b/.buildkite/generate_index.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ import argparse
+ import os
+@@ -23,5 +22,5 @@ with open("index.html", "w") as f:
+     print(f"Generated index.html for {args.wheel}")
+     # cloudfront requires escaping the '+' character
+     f.write(
+-        template.format(wheel=filename, wheel_html_escaped=filename.replace("+", "%2B"))
+-    )
++        template.format(wheel=filename,
++                        wheel_html_escaped=filename.replace("+", "%2B")))
+diff --git a/.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-FP8-compressed-tensors.yaml b/.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-FP8-compressed-tensors.yaml
+deleted file mode 100644
+index cca58097e..000000000
+--- a/.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-FP8-compressed-tensors.yaml
++++ /dev/null
+@@ -1,11 +0,0 @@
+-# bash .buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh -m RedHatAI/Llama-3.2-1B-Instruct-FP8 -b "auto" -l 1319 -f 5 -t 1
+-model_name: "RedHatAI/Llama-3.2-1B-Instruct-FP8"
+-tasks:
+-- name: "gsm8k"
+-  metrics:
+-  - name: "exact_match,strict-match"
+-    value: 0.335
+-  - name: "exact_match,flexible-extract"
+-    value: 0.323
+-limit: 1319
+-num_fewshot: 5
+diff --git a/.buildkite/lm-eval-harness/configs/Qwen2.5-1.5B-Instruct.yaml b/.buildkite/lm-eval-harness/configs/Qwen2.5-1.5B-Instruct.yaml
+deleted file mode 100644
+index 54579a63a..000000000
+--- a/.buildkite/lm-eval-harness/configs/Qwen2.5-1.5B-Instruct.yaml
++++ /dev/null
+@@ -1,11 +0,0 @@
+-# bash .buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh -m Qwen/Qwen2.5-1.5B-Instruct -b auto -l 1319 -f 5 -t 1
+-model_name: "Qwen/Qwen2.5-1.5B-Instruct"
+-tasks:
+-- name: "gsm8k"
+-  metrics:
+-  - name: "exact_match,strict-match"
+-    value: 0.54
+-  - name: "exact_match,flexible-extract"
+-    value: 0.59
+-limit: 1319
+-num_fewshot: 5
+diff --git a/.buildkite/lm-eval-harness/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml b/.buildkite/lm-eval-harness/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
+deleted file mode 100644
+index a2f235f48..000000000
+--- a/.buildkite/lm-eval-harness/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
++++ /dev/null
+@@ -1,11 +0,0 @@
+-# bash .buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh -m RedHatAI/Qwen2.5-VL-3B-Instruct-FP8-Dynamic -b auto -l 1319 -f 5 -t 1
+-model_name: "RedHatAI/Qwen2.5-VL-3B-Instruct-FP8-Dynamic"
+-tasks:
+-- name: "gsm8k"
+-  metrics:
+-  - name: "exact_match,strict-match"
+-    value: 0.47
+-  - name: "exact_match,flexible-extract"
+-    value: 0.64
+-limit: 1319
+-num_fewshot: 5
+diff --git a/.buildkite/lm-eval-harness/configs/models-large.txt b/.buildkite/lm-eval-harness/configs/models-large.txt
+index 27a1a9a82..37eeac85c 100644
+--- a/.buildkite/lm-eval-harness/configs/models-large.txt
++++ b/.buildkite/lm-eval-harness/configs/models-large.txt
+@@ -3,4 +3,3 @@ Meta-Llama-3-70B-Instruct.yaml
+ Mixtral-8x7B-Instruct-v0.1.yaml
+ Qwen2-57B-A14-Instruct.yaml
+ DeepSeek-V2-Lite-Chat.yaml
+-Meta-Llama-3-8B-QQQ.yaml
+diff --git a/.buildkite/lm-eval-harness/configs/models-small.txt b/.buildkite/lm-eval-harness/configs/models-small.txt
+index 36e054387..254d01edf 100644
+--- a/.buildkite/lm-eval-harness/configs/models-small.txt
++++ b/.buildkite/lm-eval-harness/configs/models-small.txt
+@@ -1,6 +1,10 @@
+-Qwen2.5-1.5B-Instruct.yaml
++Meta-Llama-3-8B-Instruct.yaml
++Meta-Llama-3-8B-Instruct-FP8-compressed-tensors.yaml
+ Meta-Llama-3.2-1B-Instruct-INT8-compressed-tensors.yaml
+ Meta-Llama-3-8B-Instruct-INT8-compressed-tensors-asym.yaml
+ Meta-Llama-3-8B-Instruct-nonuniform-compressed-tensors.yaml
+-Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
++Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml
+ Qwen1.5-MoE-W4A16-compressed-tensors.yaml
++Qwen2-1.5B-Instruct-INT8-compressed-tensors.yaml
++Qwen2-1.5B-Instruct-FP8W8.yaml
++Meta-Llama-3-8B-QQQ.yaml
+diff --git a/.buildkite/lm-eval-harness/conftest.py b/.buildkite/lm-eval-harness/conftest.py
+deleted file mode 100644
+index c0d60dd53..000000000
+--- a/.buildkite/lm-eval-harness/conftest.py
++++ /dev/null
+@@ -1,44 +0,0 @@
+-# SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+-from pathlib import Path
+-
+-import pytest
+-
+-
+-def pytest_addoption(parser):
+-    parser.addoption(
+-        "--config-list-file",
+-        action="store",
+-        help="Path to the file listing model config YAMLs (one per line)",
+-    )
+-    parser.addoption(
+-        "--tp-size",
+-        action="store",
+-        default="1",
+-        help="Tensor parallel size to use for evaluation",
+-    )
+-
+-
+-@pytest.fixture(scope="session")
+-def config_list_file(pytestconfig, config_dir):
+-    rel_path = pytestconfig.getoption("--config-list-file")
+-    return config_dir / rel_path
+-
+-
+-@pytest.fixture(scope="session")
+-def tp_size(pytestconfig):
+-    return pytestconfig.getoption("--tp-size")
+-
+-
+-def pytest_generate_tests(metafunc):
+-    if "config_filename" in metafunc.fixturenames:
+-        rel_path = metafunc.config.getoption("--config-list-file")
+-        config_list_file = Path(rel_path).resolve()
+-        config_dir = config_list_file.parent
+-        with open(config_list_file, encoding="utf-8") as f:
+-            configs = [
+-                config_dir / line.strip()
+-                for line in f
+-                if line.strip() and not line.startswith("#")
+-            ]
+-        metafunc.parametrize("config_filename", configs)
+diff --git a/.buildkite/lm-eval-harness/run-tests.sh b/.buildkite/lm-eval-harness/run-tests.sh
+new file mode 100644
+index 000000000..26f33b744
+--- /dev/null
++++ b/.buildkite/lm-eval-harness/run-tests.sh
+@@ -0,0 +1,59 @@
++#!/bin/bash
++
++usage() {
++    echo``
++    echo "Runs lm eval harness on GSM8k using vllm and compares to "
++    echo "precomputed baseline (measured by HF transformers.)"
++    echo
++    echo "usage: ${0} <options>"
++    echo
++    echo "  -c    - path to the test data config (e.g. configs/small-models.txt)"
++    echo "  -t    - tensor parallel size"
++    echo
++}
++
++SUCCESS=0
++
++while getopts "c:t:" OPT; do
++  case ${OPT} in
++    c ) 
++        CONFIG="$OPTARG"
++        ;;
++    t )
++        TP_SIZE="$OPTARG"
++        ;;
++    \? )
++        usage
++        exit 1
++        ;;
++  esac
++done
++
++# Parse list of configs.
++IFS=$'\n' read -d '' -r -a MODEL_CONFIGS < "$CONFIG"
++
++for MODEL_CONFIG in "${MODEL_CONFIGS[@]}"
++do
++    LOCAL_SUCCESS=0
++    
++    echo "=== RUNNING MODEL: $MODEL_CONFIG WITH TP SIZE: $TP_SIZE==="
++
++    export LM_EVAL_TEST_DATA_FILE=$PWD/configs/${MODEL_CONFIG}
++    export LM_EVAL_TP_SIZE=$TP_SIZE
++    pytest -s test_lm_eval_correctness.py || LOCAL_SUCCESS=$?
++
++    if [[ $LOCAL_SUCCESS == 0 ]]; then
++        echo "=== PASSED MODEL: ${MODEL_CONFIG} ==="
++    else
++        echo "=== FAILED MODEL: ${MODEL_CONFIG} ==="
++    fi
++
++    SUCCESS=$((SUCCESS + LOCAL_SUCCESS))
++
++done
++
++if [ "${SUCCESS}" -eq "0" ]; then
++    exit 0
++else
++    exit 1
++fi
+diff --git a/.buildkite/lm-eval-harness/test_lm_eval_correctness.py b/.buildkite/lm-eval-harness/test_lm_eval_correctness.py
+index 930adfaf3..6015a83e8 100644
+--- a/.buildkite/lm-eval-harness/test_lm_eval_correctness.py
++++ b/.buildkite/lm-eval-harness/test_lm_eval_correctness.py
+@@ -1,55 +1,69 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ """
+ LM eval harness on model to compare vs HF baseline computed offline.
+ Configs are found in configs/$MODEL.yaml
+ 
+-pytest -s -v test_lm_eval_correctness.py \
+-    --config-list-file=configs/models-small.txt \
+-    --tp-size=1
++* export LM_EVAL_TEST_DATA_FILE=configs/Meta-Llama-3-70B-Instruct.yaml
++* export LM_EVAL_TP_SIZE=4 
++* pytest -s test_lm_eval_correctness.py
+ """
+ 
++import os
++from pathlib import Path
++
+ import lm_eval
+-import numpy as np
++import numpy
++import pytest
+ import yaml
+ 
+ RTOL = 0.08
++TEST_DATA_FILE = os.environ.get(
++    "LM_EVAL_TEST_DATA_FILE",
++    ".buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct.yaml")
++
++TP_SIZE = os.environ.get("LM_EVAL_TP_SIZE", 1)
++
+ 
++def launch_lm_eval(eval_config):
++    trust_remote_code = eval_config.get('trust_remote_code', False)
++
++    model_args = f"pretrained={eval_config['model_name']}," \
++                 f"tensor_parallel_size={TP_SIZE}," \
++                 f"add_bos_token=true," \
++                 f"trust_remote_code={trust_remote_code}"
+ 
+-def launch_lm_eval(eval_config, tp_size):
+-    trust_remote_code = eval_config.get("trust_remote_code", False)
+-    model_args = (
+-        f"pretrained={eval_config['model_name']},"
+-        f"tensor_parallel_size={tp_size},"
+-        f"enforce_eager=true,"
+-        f"add_bos_token=true,"
+-        f"trust_remote_code={trust_remote_code}"
+-    )
+     results = lm_eval.simple_evaluate(
+         model="vllm",
+         model_args=model_args,
+         tasks=[task["name"] for task in eval_config["tasks"]],
+         num_fewshot=eval_config["num_fewshot"],
+         limit=eval_config["limit"],
+-        batch_size="auto",
+-    )
++        batch_size="auto")
++
+     return results
+ 
+ 
+-def test_lm_eval_correctness_param(config_filename, tp_size):
+-    eval_config = yaml.safe_load(config_filename.read_text(encoding="utf-8"))
++def test_lm_eval_correctness():
++    eval_config = yaml.safe_load(
++        Path(TEST_DATA_FILE).read_text(encoding="utf-8"))
++
++    if eval_config[
++            "model_name"] == "nm-testing/Meta-Llama-3-70B-Instruct-FBGEMM-nonuniform":  #noqa: E501
++        pytest.skip("FBGEMM is currently failing on main.")
+ 
+-    results = launch_lm_eval(eval_config, tp_size)
++    # Launch eval requests.
++    results = launch_lm_eval(eval_config)
+ 
++    # Confirm scores match ground truth.
+     success = True
+     for task in eval_config["tasks"]:
+         for metric in task["metrics"]:
+             ground_truth = metric["value"]
+             measured_value = results["results"][task["name"]][metric["name"]]
+-            print(
+-                f"{task['name']} | {metric['name']}: "
+-                f"ground_truth={ground_truth} | measured={measured_value}"
+-            )
+-            success = success and np.isclose(ground_truth, measured_value, rtol=RTOL)
++            print(f'{task["name"]} | {metric["name"]}: '
++                  f'ground_truth={ground_truth} | measured={measured_value}')
++            success = success and numpy.isclose(
++                ground_truth, measured_value, rtol=RTOL)
+ 
++    # Assert at the end, print all scores even on failure for debugging.
+     assert success
+diff --git a/.buildkite/nightly-benchmarks/README.md b/.buildkite/nightly-benchmarks/README.md
+index 72c52d5bb..d3f5fc5cd 100644
+--- a/.buildkite/nightly-benchmarks/README.md
++++ b/.buildkite/nightly-benchmarks/README.md
+@@ -113,7 +113,7 @@ WARNING: The benchmarking script will save json results by itself, so please do
+ 
+ ### Visualizing the results
+ 
+-The `convert-results-json-to-markdown.py` helps you put the benchmarking results inside a markdown table, by formatting [descriptions.md](performance-benchmarks-descriptions.md) with real benchmarking results.
++The `convert-results-json-to-markdown.py` helps you put the benchmarking results inside a markdown table, by formatting [descriptions.md](tests/descriptions.md) with real benchmarking results.
+ You can find the result presented as a table inside the `buildkite/performance-benchmark` job page.
+ If you do not see the table, please wait till the benchmark finish running.
+ The json version of the table (together with the json version of the benchmark) will be also attached to the markdown file.
+diff --git a/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py b/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
+index a4f1638c1..1030ec24e 100644
+--- a/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
++++ b/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ import json
+ import os
+@@ -66,18 +65,18 @@ def read_markdown(file):
+ 
+ 
+ def results_to_json(latency, throughput, serving):
+-    return json.dumps(
+-        {
+-            "latency": latency.to_dict(),
+-            "throughput": throughput.to_dict(),
+-            "serving": serving.to_dict(),
+-        }
+-    )
++    return json.dumps({
++        'latency': latency.to_dict(),
++        'throughput': throughput.to_dict(),
++        'serving': serving.to_dict()
++    })
+ 
+ 
+ if __name__ == "__main__":
++
+     # collect results
+     for test_file in results_folder.glob("*.json"):
++
+         with open(test_file) as f:
+             raw_result = json.loads(f.read())
+ 
+@@ -121,8 +120,7 @@ if __name__ == "__main__":
+             for perc in [10, 25, 50, 75, 90, 99]:
+                 # Multiply 1000 to convert the time unit from s to ms
+                 raw_result.update(
+-                    {f"P{perc}": 1000 * raw_result["percentiles"][str(perc)]}
+-                )
++                    {f"P{perc}": 1000 * raw_result["percentiles"][str(perc)]})
+             raw_result["avg_latency"] = raw_result["avg_latency"] * 1000
+ 
+             # add the result to raw_result
+@@ -155,27 +153,26 @@ if __name__ == "__main__":
+     serving_results = pd.DataFrame.from_dict(serving_results)
+     throughput_results = pd.DataFrame.from_dict(throughput_results)
+ 
+-    raw_results_json = results_to_json(
+-        latency_results, throughput_results, serving_results
+-    )
++    raw_results_json = results_to_json(latency_results, throughput_results,
++                                       serving_results)
+ 
+     # remapping the key, for visualization purpose
+     if not latency_results.empty:
+-        latency_results = latency_results[list(latency_column_mapping.keys())].rename(
+-            columns=latency_column_mapping
+-        )
++        latency_results = latency_results[list(
++            latency_column_mapping.keys())].rename(
++                columns=latency_column_mapping)
+     if not serving_results.empty:
+-        serving_results = serving_results[list(serving_column_mapping.keys())].rename(
+-            columns=serving_column_mapping
+-        )
++        serving_results = serving_results[list(
++            serving_column_mapping.keys())].rename(
++                columns=serving_column_mapping)
+     if not throughput_results.empty:
+-        throughput_results = throughput_results[
+-            list(throughput_results_column_mapping.keys())
+-        ].rename(columns=throughput_results_column_mapping)
++        throughput_results = throughput_results[list(
++            throughput_results_column_mapping.keys())].rename(
++                columns=throughput_results_column_mapping)
+ 
+-    processed_results_json = results_to_json(
+-        latency_results, throughput_results, serving_results
+-    )
++    processed_results_json = results_to_json(latency_results,
++                                             throughput_results,
++                                             serving_results)
+ 
+     for df in [latency_results, serving_results, throughput_results]:
+         if df.empty:
+@@ -187,39 +184,38 @@ if __name__ == "__main__":
+         # The GPUs sometimes come in format of "GPUTYPE\nGPUTYPE\n...",
+         # we want to turn it into "8xGPUTYPE"
+         df["GPU"] = df["GPU"].apply(
+-            lambda x: f"{len(x.split('\n'))}x{x.split('\n')[0]}"
+-        )
++            lambda x: f"{len(x.split('\n'))}x{x.split('\n')[0]}")
+ 
+     # get markdown tables
+-    latency_md_table = tabulate(
+-        latency_results, headers="keys", tablefmt="pipe", showindex=False
+-    )
+-    serving_md_table = tabulate(
+-        serving_results, headers="keys", tablefmt="pipe", showindex=False
+-    )
+-    throughput_md_table = tabulate(
+-        throughput_results, headers="keys", tablefmt="pipe", showindex=False
+-    )
++    latency_md_table = tabulate(latency_results,
++                                headers='keys',
++                                tablefmt='pipe',
++                                showindex=False)
++    serving_md_table = tabulate(serving_results,
++                                headers='keys',
++                                tablefmt='pipe',
++                                showindex=False)
++    throughput_md_table = tabulate(throughput_results,
++                                   headers='keys',
++                                   tablefmt='pipe',
++                                   showindex=False)
+ 
+     # document the result
+     with open(results_folder / "benchmark_results.md", "w") as f:
+-        results = read_markdown(
+-            "../.buildkite/nightly-benchmarks/"
+-            + "performance-benchmarks-descriptions.md"
+-        )
++
++        results = read_markdown("../.buildkite/nightly-benchmarks/" +
++                                "performance-benchmarks-descriptions.md")
+         results = results.format(
+             latency_tests_markdown_table=latency_md_table,
+             throughput_tests_markdown_table=throughput_md_table,
+             serving_tests_markdown_table=serving_md_table,
+-            benchmarking_results_in_json_string=processed_results_json,
+-        )
++            benchmarking_results_in_json_string=processed_results_json)
+         f.write(results)
+ 
+     # document benchmarking results in json
+     with open(results_folder / "benchmark_results.json", "w") as f:
+-        results = (
+-            latency_results.to_dict(orient="records")
+-            + throughput_results.to_dict(orient="records")
+-            + serving_results.to_dict(orient="records")
+-        )
++
++        results = latency_results.to_dict(
++            orient='records') + throughput_results.to_dict(
++                orient='records') + serving_results.to_dict(orient='records')
+         f.write(json.dumps(results))
+diff --git a/.buildkite/nightly-benchmarks/scripts/download-tokenizer.py b/.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
+index 8532ff7ef..5e17b79d2 100644
+--- a/.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
++++ b/.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ import argparse
+ 
+@@ -15,12 +14,15 @@ def main(model, cachedir):
+ 
+ if __name__ == "__main__":
+     parser = argparse.ArgumentParser(
+-        description="Download and save Hugging Face tokenizer"
+-    )
+-    parser.add_argument("--model", type=str, required=True, help="Name of the model")
+-    parser.add_argument(
+-        "--cachedir", type=str, required=True, help="Directory to save the tokenizer"
+-    )
++        description="Download and save Hugging Face tokenizer")
++    parser.add_argument("--model",
++                        type=str,
++                        required=True,
++                        help="Name of the model")
++    parser.add_argument("--cachedir",
++                        type=str,
++                        required=True,
++                        help="Directory to save the tokenizer")
+ 
+     args = parser.parse_args()
+     main(args.model, args.cachedir)
+diff --git a/.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py b/.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
+index 053fd52c3..0ff95a091 100644
+--- a/.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
++++ b/.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ import argparse
+ import json
+@@ -12,33 +11,33 @@ from tabulate import tabulate
+ 
+ def parse_arguments():
+     parser = argparse.ArgumentParser(
+-        description="Parse command line arguments for summary-nightly-results script."
+-    )
+-    parser.add_argument(
+-        "--results-folder",
+-        type=str,
+-        required=True,
+-        help="The folder where the results are stored.",
+-    )
+-    parser.add_argument(
+-        "--description", type=str, required=True, help="Description of the results."
+-    )
++        description=
++        'Parse command line arguments for summary-nightly-results script.')
++    parser.add_argument('--results-folder',
++                        type=str,
++                        required=True,
++                        help='The folder where the results are stored.')
++    parser.add_argument('--description',
++                        type=str,
++                        required=True,
++                        help='Description of the results.')
+ 
+     args = parser.parse_args()
+     return args
+ 
+ 
+ def get_perf(df, method, model, metric):
++
+     means = []
+ 
+     for qps in [2, 4, 8, 16, "inf"]:
+-        target = df["Test name"].str.contains(model)
+-        target = target & df["Engine"].str.contains(method)
+-        target = target & df["Test name"].str.contains("qps_" + str(qps))
++        target = df['Test name'].str.contains(model)
++        target = target & df['Engine'].str.contains(method)
++        target = target & df['Test name'].str.contains("qps_" + str(qps))
+         filtered_df = df[target]
+ 
+         if filtered_df.empty:
+-            means.append(0.0)
++            means.append(0.)
+         else:
+             means.append(filtered_df[metric].values[0])
+ 
+@@ -46,6 +45,7 @@ def get_perf(df, method, model, metric):
+ 
+ 
+ def get_perf_w_std(df, method, model, metric):
++
+     if metric in ["TTFT", "ITL"]:
+         mean = get_perf(df, method, model, "Mean " + metric + " (ms)")
+         mean = mean.tolist()
+@@ -60,8 +60,7 @@ def get_perf_w_std(df, method, model, metric):
+     else:
+         assert metric == "Tput"
+         mean = get_perf(df, method, model, "Input Tput (tok/s)") + get_perf(
+-            df, method, model, "Output Tput (tok/s)"
+-        )
++            df, method, model, "Output Tput (tok/s)")
+         mean = mean.tolist()
+         std = None
+ 
+@@ -81,17 +80,18 @@ def main(args):
+     # generate markdown table
+     df = pd.DataFrame.from_dict(results)
+ 
+-    md_table = tabulate(df, headers="keys", tablefmt="pipe", showindex=False)
++    md_table = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
+ 
+     with open(args.description) as f:
+         description = f.read()
+ 
+-    description = description.format(nightly_results_benchmarking_table=md_table)
++    description = description.format(
++        nightly_results_benchmarking_table=md_table)
+ 
+     with open("nightly_results.md", "w") as f:
+         f.write(description)
+ 
+ 
+-if __name__ == "__main__":
++if __name__ == '__main__':
+     args = parse_arguments()
+     main(args)
+diff --git a/.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py b/.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
+index ddea1d2b1..e5f179a0f 100644
+--- a/.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
++++ b/.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ from lmdeploy.serve.openai.api_client import APIClient
+ 
+diff --git a/.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py b/.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
+index fb3b9d5e3..62ee5e10b 100644
+--- a/.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
++++ b/.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
+@@ -1,5 +1,4 @@
+ # SPDX-License-Identifier: Apache-2.0
+-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ import datetime
+ import json
+@@ -35,8 +34,10 @@ serving_column_mapping = {
+ }
+ 
+ if __name__ == "__main__":
++
+     # collect results
+     for test_file in results_folder.glob("*.json"):
++
+         with open(test_file) as f:
+             raw_result = json.loads(f.read())
+ 
+@@ -55,16 +56,17 @@ if __name__ == "__main__":
+     serving_results = pd.DataFrame.from_dict(serving_results)
+ 
+     if not serving_results.empty:
+-        serving_results = serving_results[list(serving_column_mapping.keys())].rename(
+-            columns=serving_column_mapping
+-        )
++        serving_results = serving_results[list(
++            serving_column_mapping.keys())].rename(
++                columns=serving_column_mapping)
+ 
+-    serving_md_table_with_headers = tabulate(
+-        serving_results, headers="keys", tablefmt="pipe", showindex=False
+-    )
++    serving_md_table_with_headers = tabulate(serving_results,
++                                             headers='keys',
++                                             tablefmt='pipe',
++                                             showindex=False)
+     # remove the first line of header
+-    serving_md_table_lines = serving_md_table_with_headers.split("\n")
+-    serving_md_table_without_header = "\n".join(serving_md_table_lines[2:])
++    serving_md_table_lines = serving_md_table_with_headers.split('\n')
++    serving_md_table_without_header = '\n'.join(serving_md_table_lines[2:])
+ 
+     prefix = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+     prefix = prefix + "_" + os.environ.get("CURRENT_LLM_SERVING_ENGINE")
+@@ -74,9 +76,10 @@ if __name__ == "__main__":
+         # document results with header.
+         # for those who wants to reproduce our benchmark.
+         f.write(serving_md_table_with_headers)
+-        f.write("\n")
++        f.write('\n')
+ 
+     # document benchmarking results in json
+     with open(results_folder / f"{prefix}_nightly_results.json", "w") as f:
+-        results = serving_results.to_dict(orient="records")
++
++        results = serving_results.to_dict(orient='records')
+         f.write(json.dumps(results))
+diff --git a/.buildkite/pyproject.toml b/.buildkite/pyproject.toml
+deleted file mode 100644
+index d5cad1c73..000000000
+--- a/.buildkite/pyproject.toml
++++ /dev/null
+@@ -1,46 +0,0 @@
+-# This local pyproject file is part of the migration from yapf to ruff format.
+-# It uses the same core rules as the main pyproject.toml file, but with the
+-# following differences:
+-# - ruff line length is overridden to 88
+-# - deprecated typing ignores (UP006, UP035) have been removed
+-
+-[tool.ruff]
+-line-length = 88
+-
+-[tool.ruff.lint.per-file-ignores]
+-"vllm/third_party/**" = ["ALL"]
+-"vllm/version.py" = ["F401"]
+-"vllm/_version.py" = ["ALL"]
+-
+-[tool.ruff.lint]
+-select = [
+-    # pycodestyle
+-    "E",
+-    # Pyflakes
+-    "F",
+-    # pyupgrade
+-    "UP",
+-    # flake8-bugbear
+-    "B",
+-    # flake8-simplify
+-    "SIM",
+-    # isort
+-    "I",
+-    # flake8-logging-format
+-    "G",
+-]
+-ignore = [
+-    # star imports
+-    "F405", "F403",
+-    # lambda expression assignment
+-    "E731",
+-    # Loop control variable not used within loop body
+-    "B007",
+-    # f-string format
+-    "UP032",
+-    # Can remove once 3.10+ is the minimum Python version
+-    "UP007",
+-]
+-
+-[tool.ruff.format]
+-docstring-code-format = true
+diff --git a/.buildkite/release-pipeline.yaml b/.buildkite/release-pipeline.yaml
+index 16b5ad029..a21a657c4 100644
+--- a/.buildkite/release-pipeline.yaml
++++ b/.buildkite/release-pipeline.yaml
+@@ -1,22 +1,20 @@
+ steps:
+-  - label: "Build wheel - CUDA 12.8"
+-    id: build-wheel-cuda-12-8
++  - label: "Build wheel - CUDA 12.4"
+     agents:
+       queue: cpu_queue_postmerge
+     commands:
+-      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=12.8.1 --tag vllm-ci:build-image --target build --progress plain -f docker/Dockerfile ."
++      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=12.4.0 --tag vllm-ci:build-image --target build --progress plain -f docker/Dockerfile ."
+       - "mkdir artifacts"
+       - "docker run --rm -v $(pwd)/artifacts:/artifacts_host vllm-ci:build-image bash -c 'cp -r dist /artifacts_host && chmod -R a+rw /artifacts_host'"
+       - "bash .buildkite/scripts/upload-wheels.sh"
+     env:
+       DOCKER_BUILDKIT: "1"
+ 
+-  - label: "Build wheel - CUDA 12.6"
+-    id: build-wheel-cuda-12-6
++  - label: "Build wheel - CUDA 12.1"
+     agents:
+       queue: cpu_queue_postmerge
+     commands:
+-      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=12.6.3 --build-arg torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0+PTX' --tag vllm-ci:build-image --target build --progress plain -f docker/Dockerfile ."
++      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=12.1.0 --tag vllm-ci:build-image --target build --progress plain -f docker/Dockerfile ."
+       - "mkdir artifacts"
+       - "docker run --rm -v $(pwd)/artifacts:/artifacts_host vllm-ci:build-image bash -c 'cp -r dist /artifacts_host && chmod -R a+rw /artifacts_host'"
+       - "bash .buildkite/scripts/upload-wheels.sh"
+@@ -30,11 +28,10 @@ steps:
+ 
+   - label: "Build wheel - CUDA 11.8"
+     # depends_on: block-build-cu118-wheel
+-    id: build-wheel-cuda-11-8
+     agents:
+       queue: cpu_queue_postmerge
+     commands:
+-      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=11.8.0 --build-arg torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0+PTX' --tag vllm-ci:build-image --target build --progress plain -f docker/Dockerfile ."
++      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=11.8.0 --tag vllm-ci:build-image --target build --progress plain -f docker/Dockerfile ."
+       - "mkdir artifacts"
+       - "docker run --rm -v $(pwd)/artifacts:/artifacts_host vllm-ci:build-image bash -c 'cp -r dist /artifacts_host && chmod -R a+rw /artifacts_host'"
+       - "bash .buildkite/scripts/upload-wheels.sh"
+@@ -47,49 +44,33 @@ steps:
+ 
+   - label: "Build release image"
+     depends_on: block-release-image-build
+-    id: build-release-image
+     agents:
+       queue: cpu_queue_postmerge
+     commands:
+       - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
+-      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=12.8.1 --tag public.ecr.aws/q9t5s3a7/vllm-release-repo:$BUILDKITE_COMMIT --target vllm-openai --progress plain -f docker/Dockerfile ."
++      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --build-arg CUDA_VERSION=12.4.0 --tag public.ecr.aws/q9t5s3a7/vllm-release-repo:$BUILDKITE_COMMIT --target vllm-openai --progress plain -f docker/Dockerfile ."
+       - "docker push public.ecr.aws/q9t5s3a7/vllm-release-repo:$BUILDKITE_COMMIT"
+ 
+-  - label: "Annotate release workflow"
+-    depends_on:
+-      - build-release-image
+-      - build-wheel-cuda-12-8
+-      - build-wheel-cuda-12-6
+-      - build-wheel-cuda-11-8
+-    id: annotate-release-workflow
+-    agents:
+-      queue: cpu_queue_postmerge
+-    commands:
+-      - "bash .buildkite/scripts/annotate-release.sh"
+-
+   - label: "Build and publish TPU release image"
+     depends_on: ~
+     if: build.env("NIGHTLY") == "1"
+     agents:
+       queue: tpu_queue_postmerge
+     commands:
+-      - "yes | docker system prune -a"
+-      - "git fetch --all"
+       - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --build-arg GIT_REPO_CHECK=1 --tag vllm/vllm-tpu:nightly --tag vllm/vllm-tpu:$BUILDKITE_COMMIT --progress plain -f docker/Dockerfile.tpu ."
+       - "docker push vllm/vllm-tpu:nightly"
+       - "docker push vllm/vllm-tpu:$BUILDKITE_COMMIT"
+     plugins:
+       - docker-login#v3.0.0:
+-          username: vllmbot
++          username: vllm
+           password-env: DOCKERHUB_TOKEN
+     env:
+       DOCKER_BUILDKIT: "1"
+ 
+   - input: "Provide Release version here"
+-    id: input-release-version
+     fields:
+       - text: "What is the release version?"
+-        key: release-version
++        key: "release-version"
+ 
+   - block: "Build CPU release image"
+     key: block-cpu-release-image-build
+diff --git a/.buildkite/scripts/annotate-release.sh b/.buildkite/scripts/annotate-release.sh
+deleted file mode 100755
+index 94e0ac239..000000000
+--- a/.buildkite/scripts/annotate-release.sh
++++ /dev/null
+@@ -1,31 +0,0 @@
+-#!/bin/bash
+-
+-set -ex
+-
+-# Get release version and strip leading 'v' if present
+-RELEASE_VERSION=$(buildkite-agent meta-data get release-version | sed 's/^v//')
+-
+-if [ -z "$RELEASE_VERSION" ]; then
+-  echo "Error: RELEASE_VERSION is empty. 'release-version' metadata might not be set or is invalid."
+-  exit 1
+-fi
+-
+-buildkite-agent annotate --style 'info' --context 'release-workflow' << EOF
+-To download the wheel:
+-\`\`\`
+-aws s3 cp s3://vllm-wheels/${RELEASE_VERSION}/vllm-${RELEASE_VERSION}-cp38-abi3-manylinux1_x86_64.whl .
+-aws s3 cp s3://vllm-wheels/${RELEASE_VERSION}+cu126/vllm-${RELEASE_VERSION}+cu126-cp38-abi3-manylinux1_x86_64.whl .
+-aws s3 cp s3://vllm-wheels/${RELEASE_VERSION}+cu118/vllm-${RELEASE_VERSION}+cu118-cp38-abi3-manylinux1_x86_64.whl . 
+-\`\`\`
+-
+-To download and upload the image:
+-
+-\`\`\`
+-docker pull public.ecr.aws/q9t5s3a7/vllm-release-repo:${BUILDKITE_COMMIT}
+-docker tag public.ecr.aws/q9t5s3a7/vllm-release-repo:${BUILDKITE_COMMIT} vllm/vllm-openai
+-docker tag vllm/vllm-openai vllm/vllm-openai:latest
+-docker tag vllm/vllm-openai vllm/vllm-openai:v${RELEASE_VERSION}
+-docker push vllm/vllm-openai:latest
+-docker push vllm/vllm-openai:v${RELEASE_VERSION}
+-\`\`\`
+-EOF 
+\ No newline at end of file
+diff --git a/.buildkite/scripts/ci-clean-log.sh b/.buildkite/scripts/ci-clean-log.sh
+deleted file mode 100644
+index 69d8a3a28..000000000
+--- a/.buildkite/scripts/ci-clean-log.sh
++++ /dev/null
+@@ -1,17 +0,0 @@
+-#!/bin/bash
+-# Usage: ./ci_clean_log.sh ci.log
+-# This script strips timestamps and color codes from CI log files.
+-
+-# Check if argument is given
+-if [ $# -lt 1 ]; then
+-    echo "Usage: $0 ci.log"
+-    exit 1
+-fi
+-
+-INPUT_FILE="$1"
+-
+-# Strip timestamps
+-sed -i 's/^\[[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}T[0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}Z\] //' "$INPUT_FILE"
+-
+-# Strip colorization
+-sed -i -r 's/\x1B\[[0-9;]*[mK]//g' "$INPUT_FILE"
+diff --git a/.buildkite/scripts/hardware_ci/run-amd-test.sh b/.buildkite/scripts/hardware_ci/run-amd-test.sh
+index 6e9af1e72..368f30434 100755
+--- a/.buildkite/scripts/hardware_ci/run-amd-test.sh
++++ b/.buildkite/scripts/hardware_ci/run-amd-test.sh
+@@ -3,9 +3,6 @@
+ # This script runs test inside the corresponding ROCm docker container.
+ set -o pipefail
+ 
+-# Export Python path
+-export PYTHONPATH=".."
+-
+ # Print ROCm version
+ echo "--- Confirming Clean Initial State"
+ while true; do
+@@ -77,73 +74,38 @@ HF_MOUNT="/root/.cache/huggingface"
+ 
+ commands=$@
+ echo "Commands:$commands"
+-
+-if [[ $commands == *"pytest -v -s basic_correctness/test_basic_correctness.py"* ]]; then
+-  commands=${commands//"pytest -v -s basic_correctness/test_basic_correctness.py"/"VLLM_USE_TRITON_FLASH_ATTN=0 pytest -v -s basic_correctness/test_basic_correctness.py"}
+-fi
+-
+-if [[ $commands == *"pytest -v -s models/test_registry.py"* ]]; then
+-  commands=${commands//"pytest -v -s models/test_registry.py"/"pytest -v -s models/test_registry.py -k 'not BambaForCausalLM and not GritLM and not Mamba2ForCausalLM and not Zamba2ForCausalLM'"}
+-fi
+-
+-if [[ $commands == *"VLLM_USE_V1=0 pytest -v -s models/test_initialization.py -k 'not llama4 and not plamo2'"* ]]; then
+-  commands=${commands//"VLLM_USE_V1=0 pytest -v -s models/test_initialization.py -k 'not llama4 and not plamo2'"/"VLLM_USE_V1=0 pytest -v -s models/test_initialization.py -k 'not llama4 and not plamo2 and not BambaForCausalLM and not Gemma2ForCausalLM and not Grok1ModelForCausalLM and not Zamba2ForCausalLM and not Gemma2Model and not GritLM'"}
+-fi
+-
+-if [[ $commands == *"pytest -v -s compile/test_basic_correctness.py"* ]]; then
+-  commands=${commands//"pytest -v -s compile/test_basic_correctness.py"/"VLLM_USE_TRITON_FLASH_ATTN=0 pytest -v -s compile/test_basic_correctness.py"}
+-fi
+-
+-if [[ $commands == *"pytest -v -s lora"* ]]; then
+-  commands=${commands//"pytest -v -s lora"/"VLLM_ROCM_CUSTOM_PAGED_ATTN=0 pytest -v -s lora"}
+-fi
+-
+ #ignore certain kernels tests
+-if [[ $commands == *" kernels/core"* ]]; then
++if [[ $commands == *" kernels "* ]]; then
+   commands="${commands} \
+-  --ignore=kernels/core/test_fused_quant_layernorm.py \
+-  --ignore=kernels/core/test_permute_cols.py"
+-fi
+-
+-if [[ $commands == *" kernels/attention"* ]]; then
+-  commands="${commands} \
+-  --ignore=kernels/attention/stest_attention_selector.py \
+-  --ignore=kernels/attention/test_blocksparse_attention.py \
+-  --ignore=kernels/attention/test_encoder_decoder_attn.py \
+-  --ignore=kernels/attention/test_attention_selector.py \
+-  --ignore=kernels/attention/test_flash_attn.py \
+-  --ignore=kernels/attention/test_flashinfer.py \
+-  --ignore=kernels/attention/test_prefix_prefill.py \
+-  --ignore=kernels/attention/test_cascade_flash_attn.py \
+-  --ignore=kernels/attention/test_mha_attn.py \
+-  --ignore=kernels/attention/test_lightning_attn.py \
+-  --ignore=kernels/attention/test_attention.py"
+-fi
+-
+-if [[ $commands == *" kernels/quantization"* ]]; then
+-  commands="${commands} \
+-  --ignore=kernels/quantization/test_int8_quant.py \
+-  --ignore=kernels/quantization/test_aqlm.py \
+-  --ignore=kernels/quantization/test_machete_mm.py \
+-  --ignore=kernels/quantization/test_block_fp8.py \
+-  --ignore=kernels/quantization/test_block_int8.py \
+-  --ignore=kernels/quantization/test_marlin_gemm.py \
+-  --ignore=kernels/quantization/test_cutlass_scaled_mm.py \
+-  --ignore=kernels/quantization/test_int8_kernel.py"
+-fi
+-
+-if [[ $commands == *" kernels/mamba"* ]]; then
+-  commands="${commands} \
+-  --ignore=kernels/mamba/test_mamba_mixer2.py \
+-  --ignore=kernels/mamba/test_causal_conv1d.py \
+-  --ignore=kernels/mamba/test_mamba_ssm_ssd.py"
+-fi
+-
+-if [[ $commands == *" kernels/moe"* ]]; then
+-  commands="${commands} \
+-  --ignore=kernels/moe/test_moe.py \
+-  --ignore=kernels/moe/test_cutlass_moe.py \
+-  --ignore=kernels/moe/test_triton_moe_ptpc_fp8.py"
++  --ignore=kernels/test_attention_selector.py \
++  --ignore=kernels/test_blocksparse_attention.py \
++  --ignore=kernels/test_causal_conv1d.py \
++  --ignore=kernels/test_cutlass.py \
++  --ignore=kernels/test_encoder_decoder_attn.py \
++  --ignore=kernels/test_flash_attn.py \
++  --ignore=kernels/test_flashinfer.py \
++  --ignore=kernels/test_int8_quant.py \
++  --ignore=kernels/test_machete_gemm.py \
++  --ignore=kernels/test_mamba_ssm.py \
++  --ignore=kernels/test_marlin_gemm.py \
++  --ignore=kernels/test_moe.py \
++  --ignore=kernels/test_prefix_prefill.py \
++  --ignore=kernels/test_rand.py \
++  --ignore=kernels/test_sampler.py \
++  --ignore=kernels/test_cascade_flash_attn.py \
++  --ignore=kernels/test_mamba_mixer2.py \
++  --ignore=kernels/test_aqlm.py \
++  --ignore=kernels/test_machete_mm.py \
++  --ignore=kernels/test_mha_attn.py \
++  --ignore=kernels/test_block_fp8.py \
++  --ignore=kernels/test_cutlass_moe.py \
++  --ignore=kernels/test_mamba_ssm_ssd.py \
++  --ignore=kernels/test_attention.py \
++  --ignore=kernels/test_block_int8.py \
++  --ignore=kernels/test_fused_quant_layernorm.py \
++  --ignore=kernels/test_int8_kernel.py \
++  --ignore=kernels/test_triton_moe_ptpc_fp8.py \
++  --ignore=kernels/test_permute_cols.py"
+ fi
+ 
+ #ignore certain Entrypoints/openai tests
+@@ -185,8 +147,6 @@ fi
+ 
+ 
+ PARALLEL_JOB_COUNT=8
+-MYPYTHONPATH=".."
+-
+ # check if the command contains shard flag, we will run all shards in parallel because the host have 8 GPUs. 
+ if [[ $commands == *"--shard-id="* ]]; then
+   # assign job count as the number of shards used   
+@@ -207,7 +167,6 @@ if [[ $commands == *"--shard-id="* ]]; then
+         -e AWS_SECRET_ACCESS_KEY \
+         -v "${HF_CACHE}:${HF_MOUNT}" \
+         -e "HF_HOME=${HF_MOUNT}" \
+-        -e "PYTHONPATH=${MYPYTHONPATH}" \
+         --name "${container_name}_${GPU}" \
+         "${image_name}" \
+         /bin/bash -c "${commands_gpu}" \
+@@ -238,7 +197,6 @@ else
+           -e AWS_SECRET_ACCESS_KEY \
+           -v "${HF_CACHE}:${HF_MOUNT}" \
+           -e "HF_HOME=${HF_MOUNT}" \
+-          -e "PYTHONPATH=${MYPYTHONPATH}" \
+           --name "${container_name}" \
+           "${image_name}" \
+           /bin/bash -c "${commands}"
+diff --git a/.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh b/.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
+index 36bcb015d..5d863dd82 100755
+--- a/.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
++++ b/.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
+@@ -7,7 +7,6 @@ set -ex
+ # Setup cleanup
+ remove_docker_container() {
+   if [[ -n "$container_id" ]]; then
+-      podman stop --all -t0
+       podman rm -f "$container_id" || true
+   fi
+   podman system prune -f
+@@ -33,12 +32,9 @@ function cpu_tests() {
+     set -e
+     pip install pytest pytest-asyncio einops peft Pillow soundfile transformers_stream_generator matplotlib
+     pip install sentence-transformers datamodel_code_generator
+-    pytest -v -s tests/models/language/generation/test_bart.py -m cpu_model
+-    pytest -v -s tests/models/language/generation/test_common.py::test_models[False-5-32-openai-community/gpt2]
+-    pytest -v -s tests/models/language/generation/test_common.py::test_models[False-5-32-facebook/opt-125m]
+-    pytest -v -s tests/models/language/generation/test_common.py::test_models[False-5-32-google/gemma-1.1-2b-it]
+-    pytest -v -s tests/models/language/pooling/test_classification.py::test_models[float-jason9693/Qwen2.5-1.5B-apeach]
+-    pytest -v -s tests/models/language/pooling/test_embedding.py -m cpu_model"
++    pytest -v -s tests/models/embedding/language/test_cls_models.py::test_classification_models[float-jason9693/Qwen2.5-1.5B-apeach]
++    pytest -v -s tests/models/embedding/language/test_embedding.py::test_models[half-BAAI/bge-base-en-v1.5]
++    pytest -v -s tests/models/encoder_decoder/language -m cpu_model"
+ }
+ 
+ # All of CPU tests are expected to be finished less than 40 mins.
+diff --git a/.buildkite/scripts/hardware_ci/run-cpu-test.sh b/.buildkite/scripts/hardware_ci/run-cpu-test.sh
+index bbcde4009..40f3df960 100644
+--- a/.buildkite/scripts/hardware_ci/run-cpu-test.sh
++++ b/.buildkite/scripts/hardware_ci/run-cpu-test.sh
+@@ -6,70 +6,72 @@ set -ex
+ 
+ # allow to bind to different cores
+ CORE_RANGE=${CORE_RANGE:-48-95}
+-OMP_CORE_RANGE=${OMP_CORE_RANGE:-48-95}
+ NUMA_NODE=${NUMA_NODE:-1}
+ 
+-export CMAKE_BUILD_PARALLEL_LEVEL=32
+-
+ # Setup cleanup
+ remove_docker_container() { 
+     set -e; 
+-    docker rm -f cpu-test-"$NUMA_NODE" cpu-test-"$NUMA_NODE"-avx2 || true; 
++    docker rm -f cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" cpu-test-"$BUILDKITE_BUILD_NUMBER"-avx2-"$NUMA_NODE" || true; 
++    docker image rm cpu-test-"$BUILDKITE_BUILD_NUMBER" cpu-test-"$BUILDKITE_BUILD_NUMBER"-avx2 || true; 
+ }
+ trap remove_docker_container EXIT
+ remove_docker_container
+ 
+ # Try building the docker image
+-numactl -C "$CORE_RANGE" -N "$NUMA_NODE" docker build --tag cpu-test-"$NUMA_NODE" --target vllm-test -f docker/Dockerfile.cpu .
+-numactl -C "$CORE_RANGE" -N "$NUMA_NODE" docker build --build-arg VLLM_CPU_DISABLE_AVX512="true" --tag cpu-test-"$NUMA_NODE"-avx2 --target vllm-test -f docker/Dockerfile.cpu .
++numactl -C "$CORE_RANGE" -N "$NUMA_NODE" docker build --tag cpu-test-"$BUILDKITE_BUILD_NUMBER" --target vllm-test -f docker/Dockerfile.cpu .
++numactl -C "$CORE_RANGE" -N "$NUMA_NODE" docker build --build-arg VLLM_CPU_DISABLE_AVX512="true" --tag cpu-test-"$BUILDKITE_BUILD_NUMBER"-avx2 --target vllm-test -f docker/Dockerfile.cpu .
+ 
+ # Run the image, setting --shm-size=4g for tensor parallel.
+-docker run -itd --cpuset-cpus="$CORE_RANGE" --cpuset-mems="$NUMA_NODE" --entrypoint /bin/bash -v ~/.cache/huggingface:/root/.cache/huggingface --privileged=true -e HF_TOKEN --env VLLM_CPU_KVCACHE_SPACE=4 --env VLLM_CPU_OMP_THREADS_BIND="$OMP_CORE_RANGE" --shm-size=4g --name cpu-test-"$NUMA_NODE" cpu-test-"$NUMA_NODE"
+-docker run -itd --cpuset-cpus="$CORE_RANGE" --cpuset-mems="$NUMA_NODE" --entrypoint /bin/bash -v ~/.cache/huggingface:/root/.cache/huggingface --privileged=true -e HF_TOKEN --env VLLM_CPU_KVCACHE_SPACE=4 --env VLLM_CPU_OMP_THREADS_BIND="$OMP_CORE_RANGE" --shm-size=4g --name cpu-test-"$NUMA_NODE"-avx2 cpu-test-"$NUMA_NODE"-avx2
++docker run -itd --entrypoint /bin/bash -v ~/.cache/huggingface:/root/.cache/huggingface --cpuset-cpus="$CORE_RANGE"  \
++ --cpuset-mems="$NUMA_NODE" --privileged=true -e HF_TOKEN --env VLLM_CPU_KVCACHE_SPACE=4 --shm-size=4g --name cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" cpu-test-"$BUILDKITE_BUILD_NUMBER"
++docker run -itd --entrypoint /bin/bash -v ~/.cache/huggingface:/root/.cache/huggingface --cpuset-cpus="$CORE_RANGE" \
++ --cpuset-mems="$NUMA_NODE" --privileged=true -e HF_TOKEN --env VLLM_CPU_KVCACHE_SPACE=4 --shm-size=4g --name cpu-test-"$BUILDKITE_BUILD_NUMBER"-avx2-"$NUMA_NODE" cpu-test-"$BUILDKITE_BUILD_NUMBER"-avx2
+ 
+ function cpu_tests() {
+   set -e
+   export NUMA_NODE=$2
++  export BUILDKITE_BUILD_NUMBER=$3
+ 
+   # offline inference
+-  docker exec cpu-test-"$NUMA_NODE"-avx2 bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-avx2-"$NUMA_NODE" bash -c "
+     set -e
+     python3 examples/offline_inference/basic/generate.py --model facebook/opt-125m"
+ 
+   # Run basic model test
+-  docker exec cpu-test-"$NUMA_NODE" bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" bash -c "
+     set -e
+-    pytest -v -s tests/kernels/attention/test_cache.py -m cpu_model
+-    pytest -v -s tests/kernels/attention/test_mla_decode_cpu.py -m cpu_model
+-    pytest -v -s tests/models/language/generation -m cpu_model
+-    pytest -v -s tests/models/language/pooling -m cpu_model
+-    pytest -v -s tests/models/multimodal/generation \
+-                --ignore=tests/models/multimodal/generation/test_mllama.py \
+-                --ignore=tests/models/multimodal/generation/test_pixtral.py \
+-                -m cpu_model"
++    pytest -v -s tests/kernels/test_cache.py -m cpu_model
++    pytest -v -s tests/kernels/test_mla_decode_cpu.py -m cpu_model
++    pytest -v -s tests/models/decoder_only/language -m cpu_model
++    pytest -v -s tests/models/embedding/language -m cpu_model
++    pytest -v -s tests/models/encoder_decoder/language -m cpu_model
++    pytest -v -s tests/models/decoder_only/audio_language -m cpu_model
++    pytest -v -s tests/models/decoder_only/vision_language -m cpu_model"
+ 
+   # Run compressed-tensor test
+-  docker exec cpu-test-"$NUMA_NODE" bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" bash -c "
+     set -e
+     pytest -s -v \
+     tests/quantization/test_compressed_tensors.py::test_compressed_tensors_w8a8_static_setup \
+     tests/quantization/test_compressed_tensors.py::test_compressed_tensors_w8a8_dynamic_per_token"
+ 
+   # Run AWQ test
+-  docker exec cpu-test-"$NUMA_NODE" bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" bash -c "
+     set -e
+-    VLLM_USE_V1=0 pytest -s -v \
++    pytest -s -v \
+     tests/quantization/test_ipex_quant.py"
+ 
+   # Run chunked-prefill and prefix-cache test
+-  docker exec cpu-test-"$NUMA_NODE" bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" bash -c "
+     set -e
+     pytest -s -v -k cpu_model \
+     tests/basic_correctness/test_chunked_prefill.py"  
+ 
+   # online serving
+-  docker exec cpu-test-"$NUMA_NODE" bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" bash -c "
+     set -e
++    export VLLM_CPU_KVCACHE_SPACE=10 
++    export VLLM_CPU_OMP_THREADS_BIND=$1
+     python3 -m vllm.entrypoints.openai.api_server --model facebook/opt-125m --dtype half & 
+     timeout 600 bash -c 'until curl localhost:8000/v1/models; do sleep 1; done' || exit 1
+     python3 benchmarks/benchmark_serving.py \
+@@ -81,7 +83,7 @@ function cpu_tests() {
+       --tokenizer facebook/opt-125m"
+ 
+   # Run multi-lora tests
+-  docker exec cpu-test-"$NUMA_NODE" bash -c "
++  docker exec cpu-test-"$BUILDKITE_BUILD_NUMBER"-"$NUMA_NODE" bash -c "
+     set -e
+     pytest -s -v \
+     tests/lora/test_qwen2vl.py"
+@@ -89,4 +91,4 @@ function cpu_tests() {
+ 
+ # All of CPU tests are expected to be finished less than 40 mins.
+ export -f cpu_tests
+-timeout 1h bash -c "cpu_tests $CORE_RANGE $NUMA_NODE"
++timeout 40m bash -c "cpu_tests $CORE_RANGE $NUMA_NODE $BUILDKITE_BUILD_NUMBER"
+diff --git a/.buildkite/scripts/hardware_ci/run-hpu-test.sh b/.buildkite/scripts/hardware_ci/run-hpu-test.sh
+index 5efac3ddf..95b6ac37f 100644
+--- a/.buildkite/scripts/hardware_ci/run-hpu-test.sh
++++ b/.buildkite/scripts/hardware_ci/run-hpu-test.sh
+@@ -10,17 +10,15 @@ docker build -t hpu-test-env -f docker/Dockerfile.hpu .
+ # Setup cleanup
+ # certain versions of HPU software stack have a bug that can
+ # override the exit code of the script, so we need to use
+-# separate remove_docker_containers and remove_docker_containers_and_exit
++# separate remove_docker_container and remove_docker_container_and_exit
+ # functions, while other platforms only need one remove_docker_container
+ # function.
+ EXITCODE=1
+-remove_docker_containers() { docker rm -f hpu-test || true; docker rm -f hpu-test-tp2 || true; }
+-remove_docker_containers_and_exit() { remove_docker_containers; exit $EXITCODE; }
+-trap remove_docker_containers_and_exit EXIT
+-remove_docker_containers
++remove_docker_container() { docker rm -f hpu-test || true; }
++remove_docker_container_and_exit() { remove_docker_container; exit $EXITCODE; }
++trap remove_docker_container_and_exit EXIT
++remove_docker_container
+ 
+ # Run the image and launch offline inference
+ docker run --runtime=habana --name=hpu-test --network=host -e HABANA_VISIBLE_DEVICES=all -e VLLM_SKIP_WARMUP=true --entrypoint="" hpu-test-env python3 examples/offline_inference/basic/generate.py --model facebook/opt-125m
+-docker run --runtime=habana --name=hpu-test-tp2 --network=host -e HABANA_VISIBLE_DEVICES=all -e VLLM_SKIP_WARMUP=true --entrypoint="" hpu-test-env python3 examples/offline_inference/basic/generate.py --model facebook/opt-125m --tensor-parallel-size 2
+-
+ EXITCODE=$?
+diff --git a/.buildkite/scripts/hardware_ci/run-neuron-test.sh b/.buildkite/scripts/hardware_ci/run-neuron-test.sh
+index 3d294ea5f..ec6a080eb 100644
+--- a/.buildkite/scripts/hardware_ci/run-neuron-test.sh
++++ b/.buildkite/scripts/hardware_ci/run-neuron-test.sh
+@@ -11,14 +11,13 @@ container_name="neuron_$(tr -dc A-Za-z0-9 < /dev/urandom | head -c 10; echo)"
+ HF_CACHE="$(realpath ~)/huggingface"
+ mkdir -p "${HF_CACHE}"
+ HF_MOUNT="/root/.cache/huggingface"
+-HF_TOKEN=$(aws secretsmanager get-secret-value  --secret-id "ci/vllm-neuron/hf-token" --region us-west-2 --query 'SecretString' --output text | jq -r .VLLM_NEURON_CI_HF_TOKEN)
+ 
+ NEURON_COMPILE_CACHE_URL="$(realpath ~)/neuron_compile_cache"
+ mkdir -p "${NEURON_COMPILE_CACHE_URL}"
+ NEURON_COMPILE_CACHE_MOUNT="/root/.cache/neuron_compile_cache"
+ 
+ # Try building the docker image
+-aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws
++aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com
+ 
+ # prune old image and containers to save disk space, and only once a day
+ # by using a timestamp file in tmp.
+@@ -48,16 +47,8 @@ trap remove_docker_container EXIT
+ docker run --rm -it --device=/dev/neuron0 --network bridge \
+        -v "${HF_CACHE}:${HF_MOUNT}" \
+        -e "HF_HOME=${HF_MOUNT}" \
+-       -e "HF_TOKEN=${HF_TOKEN}" \
+        -v "${NEURON_COMPILE_CACHE_URL}:${NEURON_COMPILE_CACHE_MOUNT}" \
+        -e "NEURON_COMPILE_CACHE_URL=${NEURON_COMPILE_CACHE_MOUNT}" \
+        --name "${container_name}" \
+        ${image_name} \
+-       /bin/bash -c "
+-            python3 /workspace/vllm/examples/offline_inference/neuron.py;
+-            python3 -m pytest /workspace/vllm/tests/neuron/1_core/ -v --capture=tee-sys;
+-            for f in /workspace/vllm/tests/neuron/2_core/*.py; do
+-                echo 'Running test file: '$f;
+-                python3 -m pytest \$f -v --capture=tee-sys;
+-            done
+-       "
+\ No newline at end of file
++       /bin/bash -c "python3 /workspace/vllm/examples/offline_inference/neuron.py && python3 -m pytest /workspace/vllm/tests/neuron/1_core/ -v --capture=tee-sys && python3 -m pytest /workspace/vllm/tests/neuron/2_core/ -v --capture=tee-sys"
+diff --git a/.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh b/.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
+index a2a5c2a02..21982b01b 100755
+--- a/.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
++++ b/.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
+@@ -1,185 +1,54 @@
+ #!/bin/bash
+ 
+-set -xu
+-
+-
+-remove_docker_container() { 
+-    docker rm -f tpu-test || true; 
+-    docker rm -f vllm-tpu || true;
+-}
+-
+-trap remove_docker_container EXIT
+-
+-# Remove the container that might not be cleaned up in the previous run.
+-remove_docker_container
++set -xue
+ 
+ # Build the docker image.
+ docker build -f docker/Dockerfile.tpu -t vllm-tpu .
+ 
+ # Set up cleanup.
+-cleanup_docker() {
+-  # Get Docker's root directory
+-  docker_root=$(docker info -f '{{.DockerRootDir}}')
+-  if [ -z "$docker_root" ]; then
+-    echo "Failed to determine Docker root directory."
+-    exit 1
+-  fi
+-  echo "Docker root directory: $docker_root"
+-  # Check disk usage of the filesystem where Docker's root directory is located
+-  disk_usage=$(df "$docker_root" | tail -1 | awk '{print $5}' | sed 's/%//')
+-  # Define the threshold
+-  threshold=70
+-  if [ "$disk_usage" -gt "$threshold" ]; then
+-    echo "Disk usage is above $threshold%. Cleaning up Docker images and volumes..."
+-    # Remove dangling images (those that are not tagged and not used by any container)
+-    docker image prune -f
+-    # Remove unused volumes / force the system prune for old images as well.
+-    docker volume prune -f && docker system prune --force --filter "until=72h" --all
+-    echo "Docker images and volumes cleanup completed."
+-  else
+-    echo "Disk usage is below $threshold%. No cleanup needed."
+-  fi
+-}
+-cleanup_docker
++remove_docker_container() { docker rm -f tpu-test || true; }
++trap remove_docker_container EXIT
++# Remove the container that might not be cleaned up in the previous run.
++remove_docker_container
+ 
+ # For HF_TOKEN.
+ source /etc/environment
+-
++# Run a simple end-to-end example.
+ docker run --privileged --net host --shm-size=16G -it \
+     -e "HF_TOKEN=$HF_TOKEN" --name tpu-test \
+-    vllm-tpu /bin/bash -c '
+-set -e # Exit immediately if a command exits with a non-zero status.
+-set -u # Treat unset variables as an error.
+-
+-echo "--- Starting script inside Docker container ---"
+-
+-# Create results directory
+-RESULTS_DIR=$(mktemp -d)
+-# If mktemp fails, set -e will cause the script to exit.
+-echo "Results will be stored in: $RESULTS_DIR"
+-
+-# Install dependencies
+-echo "--- Installing Python dependencies ---"
+-python3 -m pip install --progress-bar off git+https://github.com/thuml/depyf.git \
+-    && python3 -m pip install --progress-bar off pytest pytest-asyncio tpu-info \
+-    && python3 -m pip install --progress-bar off lm_eval[api]==0.4.4
+-echo "--- Python dependencies installed ---"
+-export VLLM_USE_V1=1
+-export VLLM_XLA_CHECK_RECOMPILATION=1
+-export VLLM_XLA_CACHE_PATH=
+-echo "Using VLLM V1"
+-
+-echo "--- Hardware Information ---"
+-tpu-info
+-echo "--- Starting Tests ---"
+-set +e
+-overall_script_exit_code=0
+-
+-# --- Test Definitions ---
+-# If a test fails, this function will print logs and will not cause the main script to exit.
+-run_test() {
+-    local test_num=$1
+-    local test_name=$2
+-    local test_command=$3
+-    local log_file="$RESULTS_DIR/test_${test_num}.log"
+-    local actual_exit_code
+-
+-    echo "--- TEST_$test_num: Running $test_name ---"
+-    
+-    # Execute the test command.
+-    eval "$test_command" > >(tee -a "$log_file") 2> >(tee -a "$log_file" >&2)
+-    actual_exit_code=$?
+-
+-    echo "TEST_${test_num}_COMMAND_EXIT_CODE: $actual_exit_code" # This goes to main log
+-    echo "TEST_${test_num}_COMMAND_EXIT_CODE: $actual_exit_code" >> "$log_file" # Also to per-test log
+-
+-    if [ "$actual_exit_code" -ne 0 ]; then
+-        echo "TEST_$test_num ($test_name) FAILED with exit code $actual_exit_code." >&2
+-        echo "--- Log for failed TEST_$test_num ($test_name) ---" >&2
+-        if [ -f "$log_file" ]; then
+-            cat "$log_file" >&2
+-        else
+-            echo "Log file $log_file not found for TEST_$test_num ($test_name)." >&2
+-        fi
+-        echo "--- End of log for TEST_$test_num ($test_name) ---" >&2
+-        return "$actual_exit_code" # Return the failure code
+-    else
+-        echo "TEST_$test_num ($test_name) PASSED."
+-        return 0 # Return success
+-    fi
+-}
+-
+-# Helper function to call run_test and update the overall script exit code
+-run_and_track_test() {
+-    local test_num_arg="$1"
+-    local test_name_arg="$2"
+-    local test_command_arg="$3"
+-
+-    # Run the test
+-    run_test "$test_num_arg" "$test_name_arg" "$test_command_arg"
+-    local test_specific_exit_code=$?
+-
+-    # If the test failed, set the overall script exit code to 1
+-    if [ "$test_specific_exit_code" -ne 0 ]; then
+-        # No need for extra echo here, run_test already logged the failure.
+-        overall_script_exit_code=1
+-    fi
+-}
+-
+-# --- Actual Test Execution ---
+-run_and_track_test 0 "test_perf.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_perf.py"
+-run_and_track_test 1 "test_compilation.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/tpu/test_compilation.py"
+-run_and_track_test 2 "test_basic.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_basic.py"
+-run_and_track_test 3 "test_accuracy.py::test_lm_eval_accuracy_v1_engine" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/entrypoints/llm/test_accuracy.py::test_lm_eval_accuracy_v1_engine"
+-run_and_track_test 4 "test_quantization_accuracy.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/tpu/test_quantization_accuracy.py"
+-run_and_track_test 5 "examples/offline_inference/tpu.py" \
+-    "python3 /workspace/vllm/examples/offline_inference/tpu.py"
+-run_and_track_test 6 "test_tpu_model_runner.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/worker/test_tpu_model_runner.py"
+-run_and_track_test 7 "test_sampler.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_sampler.py"
+-run_and_track_test 8 "test_topk_topp_sampler.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_topk_topp_sampler.py"
+-run_and_track_test 9 "test_multimodal.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_multimodal.py"
+-run_and_track_test 10 "test_pallas.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_pallas.py"
+-run_and_track_test 11 "test_struct_output_generate.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/entrypoints/llm/test_struct_output_generate.py -k \"not test_structured_output_with_reasoning_matrices\""
+-run_and_track_test 12 "test_moe_pallas.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/tpu/test_moe_pallas.py"
+-run_and_track_test 13 "test_lora.py" \
+-    "VLLM_XLA_CHECK_RECOMPILATION=0 python3 -m pytest -s -v /workspace/vllm/tests/tpu/lora/test_lora.py"
+-run_and_track_test 14 "test_tpu_qkv_linear.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_tpu_qkv_linear.py"
+-run_and_track_test 15 "test_spmd_model_weight_loading.py" \
+-    "python3 -m pytest -s -v /workspace/vllm/tests/v1/tpu/test_spmd_model_weight_loading.py"
+-
+-# After all tests have been attempted, exit with the overall status.
+-if [ "$overall_script_exit_code" -ne 0 ]; then
+-    echo "--- One or more tests FAILED. Overall script exiting with failure code 1. ---"
+-else
+-    echo "--- All tests have completed and PASSED. Overall script exiting with success code 0. ---"
+-fi
+-exit "$overall_script_exit_code"
+-' # IMPORTANT: This is the closing single quote for the bash -c "..." command. Ensure it is present and correct.
++    vllm-tpu /bin/bash -c "python3 -m pip install git+https://github.com/thuml/depyf.git \
++    && python3 -m pip install pytest pytest-asyncio tpu-info \
++    && python3 -m pip install lm_eval[api]==0.4.4 \
++    && export VLLM_XLA_CACHE_PATH= \
++    && export VLLM_USE_V1=1 \
++    && export VLLM_XLA_CHECK_RECOMPILATION=1 \
++    && echo HARDWARE \
++    && tpu-info \
++    && echo TEST_0 \
++    && pytest -v -s /workspace/vllm/tests/v1/tpu/test_perf.py \
++    && echo TEST_1 \
++    && pytest -v -s /workspace/vllm/tests/tpu/test_compilation.py \
++    && echo TEST_2 \
++    && pytest -v -s /workspace/vllm/tests/v1/tpu/test_basic.py \
++    && echo TEST_3 \
++    && pytest -v -s /workspace/vllm/tests/entrypoints/llm/test_accuracy.py::test_lm_eval_accuracy_v1_engine \
++    && echo TEST_4 \
++    && pytest -s -v /workspace/vllm/tests/tpu/test_quantization_accuracy.py \
++    && echo TEST_5 \
++    && python3 /workspace/vllm/examples/offline_inference/tpu.py \
++    && echo TEST_6 \
++    && pytest -s -v /workspace/vllm/tests/v1/tpu/worker/test_tpu_model_runner.py \
++    && echo TEST_7 \
++    && pytest -s -v /workspace/vllm/tests/v1/tpu/test_sampler.py \
++    && echo TEST_8 \
++    && pytest -s -v /workspace/vllm/tests/v1/tpu/test_topk_topp_sampler.py \
++    && echo TEST_9 \
++    && pytest -s -v /workspace/vllm/tests/v1/tpu/test_multimodal.py \
++    && echo TEST_10 \
++    && pytest -s -v /workspace/vllm/tests/v1/tpu/test_pallas.py \
++    && echo TEST_11 \
++    && pytest -s -v /workspace/vllm/tests/v1/entrypoints/llm/test_struct_output_generate.py" \
+ 
+-# Capture the exit code of the docker run command
+-DOCKER_RUN_EXIT_CODE=$?
+ 
+-# The trap will run for cleanup.
+-# Exit the main script with the Docker run command's exit code.
+-if [ "$DOCKER_RUN_EXIT_CODE" -ne 0 ]; then
+-    echo "Docker run command failed with exit code $DOCKER_RUN_EXIT_CODE."
+-    exit "$DOCKER_RUN_EXIT_CODE"
+-else
+-    echo "Docker run command completed successfully."
+-    exit 0
+-fi
+ # TODO: This test fails because it uses RANDOM_SEED sampling
+-# pytest -v -s /workspace/vllm/tests/tpu/test_custom_dispatcher.py \
++# && VLLM_USE_V1=1 pytest -v -s /workspace/vllm/tests/tpu/test_custom_dispatcher.py \
+diff --git a/.buildkite/scripts/rerun-test.sh b/.buildkite/scripts/rerun-test.sh
+deleted file mode 100644
+index d79c0d5f3..000000000
+--- a/.buildkite/scripts/rerun-test.sh
++++ /dev/null
+@@ -1,18 +0,0 @@
+-#!/bin/bash
+-
+-# Usage: ./rerun_test.sh path/to/test.py::test_name
+-
+-# Check if argument is given
+-if [ $# -lt 1 ]; then
+-    echo "Usage: $0 path/to/test.py::test_name"
+-    echo "Example: $0 tests/v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp]"
+-    exit 1
+-fi
+-
+-TEST=$1
+-COUNT=1
+-
+-while pytest -sv "$TEST"; do
+-    COUNT=$((COUNT + 1))
+-    echo "RUN NUMBER ${COUNT}"
+-done
+diff --git a/.buildkite/scripts/tpu/cleanup_docker.sh b/.buildkite/scripts/tpu/cleanup_docker.sh
+deleted file mode 100755
+index 209d9c434..000000000
+--- a/.buildkite/scripts/tpu/cleanup_docker.sh
++++ /dev/null
+@@ -1,24 +0,0 @@
+-#!/bin/bash
+-
+-set -euo pipefail
+-
+-docker_root=$(docker info -f '{{.DockerRootDir}}')
+-if [ -z "$docker_root" ]; then
+-  echo "Failed to determine Docker root directory."
+-  exit 1
+-fi
+-echo "Docker root directory: $docker_root"
+-# Check disk usage of the filesystem where Docker's root directory is located
+-disk_usage=$(df "$docker_root" | tail -1 | awk '{print $5}' | sed 's/%//')
+-# Define the threshold
+-threshold=70
+-if [ "$disk_usage" -gt "$threshold" ]; then
+-  echo "Disk usage is above $threshold%. Cleaning up Docker images and volumes..."
+-  # Remove dangling images (those that are not tagged and not used by any container)
+-  docker image prune -f
+-  # Remove unused volumes / force the system prune for old images as well.
+-  docker volume prune -f && docker system prune --force --filter "until=72h" --all
+-  echo "Docker images and volumes cleanup completed."
+-else
+-  echo "Disk usage is below $threshold%. No cleanup needed."
+-fi
+diff --git a/.buildkite/scripts/tpu/config_v6e_1.env b/.buildkite/scripts/tpu/config_v6e_1.env
+deleted file mode 100644
+index 441758647..000000000
+--- a/.buildkite/scripts/tpu/config_v6e_1.env
++++ /dev/null
+@@ -1,14 +0,0 @@
+-# Environment config
+-TEST_NAME=llama8b
+-CONTAINER_NAME=vllm-tpu
+-
+-# vllm config
+-MODEL=meta-llama/Llama-3.1-8B-Instruct
+-MAX_NUM_SEQS=512
+-MAX_NUM_BATCHED_TOKENS=512
+-TENSOR_PARALLEL_SIZE=1
+-MAX_MODEL_LEN=2048
+-DOWNLOAD_DIR=/mnt/disks/persist
+-EXPECTED_THROUGHPUT=8.0
+-INPUT_LEN=1800
+-OUTPUT_LEN=128
+diff --git a/.buildkite/scripts/tpu/docker_run_bm.sh b/.buildkite/scripts/tpu/docker_run_bm.sh
+deleted file mode 100755
+index 6705da03e..000000000
+--- a/.buildkite/scripts/tpu/docker_run_bm.sh
++++ /dev/null
+@@ -1,102 +0,0 @@
+-#!/bin/bash
+-
+-if [ ! -f "$1" ]; then
+-  echo "Error: The env file '$1' does not exist."
+-  exit 1  # Exit the script with a non-zero status to indicate an error
+-fi
+-
+-ENV_FILE=$1
+-
+-# For testing on local vm, use `set -a` to export all variables
+-source /etc/environment
+-source $ENV_FILE
+-
+-remove_docker_container() { 
+-    docker rm -f tpu-test || true; 
+-    docker rm -f vllm-tpu || true;
+-    docker rm -f $CONTAINER_NAME || true;
+-}
+-
+-trap remove_docker_container EXIT
+-
+-# Remove the container that might not be cleaned up in the previous run.
+-remove_docker_container
+-
+-# Build docker image.
+-# TODO: build the image outside the script and share the image with other
+-# tpu test if building time is too long.
+-DOCKER_BUILDKIT=1 docker build \
+-  --build-arg max_jobs=16 \
+-  --build-arg USE_SCCACHE=1 \
+-  --build-arg GIT_REPO_CHECK=0 \
+-  --tag vllm/vllm-tpu-bm \
+-  --progress plain -f docker/Dockerfile.tpu .
+-
+-LOG_ROOT=$(mktemp -d)
+-# If mktemp fails, set -e will cause the script to exit.
+-echo "Results will be stored in: $LOG_ROOT"
+-
+-if [ -z "$HF_TOKEN" ]; then
+-  echo "Error: HF_TOKEN is not set or is empty."  
+-  exit 1
+-fi
+-
+-# Make sure mounted disk or dir exists
+-if [ ! -d "$DOWNLOAD_DIR" ]; then
+-    echo "Error: Folder $DOWNLOAD_DIR does not exist. This is useually a mounted drive. If no mounted drive, just create a folder."
+-    exit 1
+-fi
+-
+-echo "Run model $MODEL"
+-echo
+-
+-echo "starting docker...$CONTAINER_NAME"
+-echo    
+-docker run \
+- -v $DOWNLOAD_DIR:$DOWNLOAD_DIR \
+- --env-file $ENV_FILE \
+- -e HF_TOKEN="$HF_TOKEN" \
+- -e TARGET_COMMIT=$BUILDKITE_COMMIT \
+- -e MODEL=$MODEL \
+- -e WORKSPACE=/workspace \
+- --name $CONTAINER_NAME \
+- -d \
+- --privileged \
+- --network host \
+- -v /dev/shm:/dev/shm \
+- vllm/vllm-tpu-bm tail -f /dev/null
+-
+-echo "run script..."
+-echo
+-docker exec "$CONTAINER_NAME" /bin/bash -c ".buildkite/scripts/hardware_ci/run_bm.sh"
+-
+-echo "copy result back..."
+-VLLM_LOG="$LOG_ROOT/$TEST_NAME"_vllm_log.txt
+-BM_LOG="$LOG_ROOT/$TEST_NAME"_bm_log.txt
+-docker cp "$CONTAINER_NAME:/workspace/vllm_log.txt" "$VLLM_LOG" 
+-docker cp "$CONTAINER_NAME:/workspace/bm_log.txt" "$BM_LOG"
+-
+-throughput=$(grep "Request throughput (req/s):" "$BM_LOG" | sed 's/[^0-9.]//g')
+-echo "throughput for $TEST_NAME at $BUILDKITE_COMMIT: $throughput"
+-
+-if [ "$BUILDKITE" = "true" ]; then
+-  echo "Running inside Buildkite"
+-  buildkite-agent artifact upload "$VLLM_LOG" 
+-  buildkite-agent artifact upload "$BM_LOG"
+-else
+-  echo "Not running inside Buildkite"
+-fi
+-
+-#
+-# compare the throughput with EXPECTED_THROUGHPUT 
+-# and assert meeting the expectation
+-# 
+-if [[ -z "$throughput" || ! "$throughput" =~ ^[0-9]+([.][0-9]+)?$ ]]; then
+-  echo "Failed to get the throughput"
+-  exit 1
+-fi
+-
+-if (( $(echo "$throughput < $EXPECTED_THROUGHPUT" | bc -l) )); then
+-  echo "Error: throughput($throughput) is less than expected($EXPECTED_THROUGHPUT)"
+-  exit 1
+-fi
+diff --git a/.buildkite/scripts/tpu/run_bm.sh b/.buildkite/scripts/tpu/run_bm.sh
+deleted file mode 100755
+index 877669cd9..000000000
+--- a/.buildkite/scripts/tpu/run_bm.sh
++++ /dev/null
+@@ -1,94 +0,0 @@
+-#!/bin/bash
+-
+-set -euo pipefail
+-
+-VLLM_LOG="$WORKSPACE/vllm_log.txt"
+-BM_LOG="$WORKSPACE/bm_log.txt"
+-
+-if [ -n "$TARGET_COMMIT" ]; then
+-  head_hash=$(git rev-parse HEAD)
+-  if [ "$TARGET_COMMIT" != "$head_hash" ]; then
+-    echo "Error: target commit $TARGET_COMMIT does not match HEAD: $head_hash"
+-    exit 1
+-  fi
+-fi
+-
+-echo "model: $MODEL"
+-echo
+-
+-#
+-# create a log folder
+-#
+-mkdir "$WORKSPACE/log"
+-
+-# TODO: Move to image building.
+-pip install pandas
+-pip install datasets
+-
+-#
+-# create sonnet_4x
+-#
+-echo "Create sonnet_4x.txt"
+-echo "" > benchmarks/sonnet_4x.txt
+-for _ in {1..4}
+- do
+-  cat benchmarks/sonnet.txt >> benchmarks/sonnet_4x.txt
+-done
+-
+-#
+-# start vllm service in backend
+-#
+-echo "lanching vllm..."
+-echo "logging to $VLLM_LOG"
+-echo
+-
+-VLLM_USE_V1=1 vllm serve $MODEL \
+- --seed 42 \
+- --disable-log-requests \
+- --max-num-seqs $MAX_NUM_SEQS \
+- --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \
+- --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
+- --no-enable-prefix-caching \
+- --download_dir $DOWNLOAD_DIR \
+- --max-model-len $MAX_MODEL_LEN > "$VLLM_LOG" 2>&1 &
+-
+-
+-echo "wait for 20 minutes.."
+-echo
+-# sleep 1200
+-# wait for 10 minutes...
+-for i in {1..120}; do
+-    # TODO: detect other type of errors.
+-    if grep -Fq "raise RuntimeError" "$VLLM_LOG"; then
+-        echo "Detected RuntimeError, exiting."
+-        exit 1
+-    elif grep -Fq "Application startup complete" "$VLLM_LOG"; then
+-        echo "Application started"
+-        break
+-    else
+-        echo "wait for 10 seconds..."
+-        sleep 10
+-    fi
+-done
+-
+-#
+-# run test
+-#
+-echo "run benchmark test..."
+-echo "logging to $BM_LOG"
+-echo
+-python benchmarks/benchmark_serving.py \
+-    --backend vllm \
+-    --model $MODEL  \
+-    --dataset-name sonnet \
+-    --dataset-path benchmarks/sonnet_4x.txt \
+-    --sonnet-input-len $INPUT_LEN \
+-    --sonnet-output-len $OUTPUT_LEN \
+-    --ignore-eos > "$BM_LOG"
+-
+-echo "completed..."
+-echo
+-
+-throughput=$(grep "Request throughput (req/s):" "$BM_LOG" | sed 's/[^0-9.]//g')
+-echo "throughput: $throughput"
+-echo
+diff --git a/.buildkite/scripts/upload-wheels.sh b/.buildkite/scripts/upload-wheels.sh
+index 037897e53..a681f8927 100644
+--- a/.buildkite/scripts/upload-wheels.sh
++++ b/.buildkite/scripts/upload-wheels.sh
+@@ -50,11 +50,11 @@ aws s3 cp "$normal_wheel" "s3://vllm-wheels/$BUILDKITE_COMMIT/"
+ if [[ $normal_wheel == *"cu118"* ]]; then
+     # if $normal_wheel matches cu118, do not upload the index.html
+     echo "Skipping index files for cu118 wheels"
+-elif [[ $normal_wheel == *"cu126"* ]]; then
+-    # if $normal_wheel matches cu126, do not upload the index.html
+-    echo "Skipping index files for cu126 wheels"
++elif [[ $normal_wheel == *"cu121"* ]]; then
++    # if $normal_wheel matches cu121, do not upload the index.html
++    echo "Skipping index files for cu121 wheels"
+ else
+-    # only upload index.html for cu128 wheels (default wheels)
++    # only upload index.html for cu124 wheels (default wheels)
+     aws s3 cp index.html "s3://vllm-wheels/$BUILDKITE_COMMIT/vllm/index.html"
+     aws s3 cp "s3://vllm-wheels/nightly/index.html" "s3://vllm-wheels/$BUILDKITE_COMMIT/index.html"
+ fi
+@@ -66,13 +66,12 @@ aws s3 cp "$normal_wheel" "s3://vllm-wheels/nightly/"
+ if [[ $normal_wheel == *"cu118"* ]]; then
+     # if $normal_wheel matches cu118, do not upload the index.html
+     echo "Skipping index files for cu118 wheels"
+-elif [[ $normal_wheel == *"cu126"* ]]; then
+-    # if $normal_wheel matches cu126, do not upload the index.html
+-    echo "Skipping index files for cu126 wheels"
++elif [[ $normal_wheel == *"cu121"* ]]; then
++    # if $normal_wheel matches cu121, do not upload the index.html
++    echo "Skipping index files for cu121 wheels"
+ else
+-    # only upload index.html for cu128 wheels (default wheels)
++    # only upload index.html for cu124 wheels (default wheels)
+     aws s3 cp index.html "s3://vllm-wheels/nightly/vllm/index.html"
+ fi
+ 
+-aws s3 cp "$wheel" "s3://vllm-wheels/$version/"
+-aws s3 cp index.html "s3://vllm-wheels/$version/vllm/index.html"
++aws s3 cp "$wheel" "s3://vllm-wheels/$version/"
+\ No newline at end of file
+diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml
+index b739851cb..20d858cb1 100644
+--- a/.buildkite/test-pipeline.yaml
++++ b/.buildkite/test-pipeline.yaml
+@@ -32,17 +32,16 @@ steps:
+ ##### fast check tests  #####
+ 
+ - label: Documentation Build # 2min
+-  mirror_hardwares: [amdexperimental]
+-  working_dir: "/vllm-workspace/test_docs"
++  working_dir: "/vllm-workspace/test_docs/docs"
+   fast_check: true
+   no_gpu: True
+   commands:
+-  - pip install -r ../requirements/docs.txt
+-  # TODO: add `--strict` once warnings in docstrings are fixed
+-  - mkdocs build
++  - pip install -r ../../requirements/docs.txt
++  - SPHINXOPTS=\"-W\" make html
++  # Check API reference (if it fails, you may have missing mock imports)
++  - grep \"sig sig-object py\" build/html/api/inference_params.html
+ 
+ - label: Async Engine, Inputs, Utils, Worker Test # 24min
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - vllm/
+   - tests/mq_llm_engine
+@@ -58,13 +57,11 @@ steps:
+   - pytest -v -s async_engine # AsyncLLMEngine
+   - NUM_SCHEDULER_STEPS=4 pytest -v -s async_engine/test_async_llm_engine.py
+   - pytest -v -s test_inputs.py
+-  - pytest -v -s test_outputs.py
+   - pytest -v -s multimodal
+   - pytest -v -s test_utils.py # Utils
+   - pytest -v -s worker # Worker
+ 
+ - label: Python-only Installation Test
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - tests/standalone_tests/python_only_compile.sh
+   - setup.py
+@@ -72,7 +69,7 @@ steps:
+   - bash standalone_tests/python_only_compile.sh
+ 
+ - label: Basic Correctness Test # 30min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  #mirror_hardwares: [amd]
+   fast_check: true
+   torch_nightly: true
+   source_file_dependencies:
+@@ -89,7 +86,6 @@ steps:
+   - VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT=1 pytest -v -s basic_correctness/test_preemption.py
+ 
+ - label: Chunked Prefill Test
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - vllm/
+   - tests/basic_correctness/test_chunked_prefill
+@@ -98,7 +94,7 @@ steps:
+   - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py
+ 
+ - label: Core Test # 10min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  mirror_hardwares: [amd]
+   fast_check: true
+   source_file_dependencies:
+   - vllm/core
+@@ -108,10 +104,10 @@ steps:
+   - pytest -v -s core
+ 
+ - label: Entrypoints Test # 40min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/tests"
+   fast_check: true
+   torch_nightly: true
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/
+   - tests/entrypoints/llm
+@@ -125,12 +121,11 @@ steps:
+   - pytest -v -s entrypoints/llm/test_generate.py # it needs a clean process
+   - pytest -v -s entrypoints/llm/test_generate_multiple_loras.py # it needs a clean process
+   - VLLM_USE_V1=0 pytest -v -s entrypoints/llm/test_guided_generate.py # it needs a clean process
+-  - pytest -v -s entrypoints/openai --ignore=entrypoints/openai/test_chat_with_tool_reasoning.py --ignore=entrypoints/openai/test_oot_registration.py --ignore=entrypoints/openai/test_tensorizer_entrypoint.py --ignore=entrypoints/openai/correctness/
++  - pytest -v -s entrypoints/openai --ignore=entrypoints/openai/test_oot_registration.py  --ignore=entrypoints/openai/test_chat_with_tool_reasoning.py --ignore=entrypoints/openai/correctness/ --ignore=entrypoints/openai/test_openai_schema.py
+   - pytest -v -s entrypoints/test_chat_utils.py
+   - VLLM_USE_V1=0 pytest -v -s entrypoints/offline_mode # Needs to avoid interference with other tests
+ 
+ - label: Distributed Tests (4 GPUs) # 10min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 4
+   source_file_dependencies:
+@@ -138,38 +133,32 @@ steps:
+   - vllm/core/
+   - tests/distributed/test_utils
+   - tests/distributed/test_pynccl
+-  - tests/distributed/test_events
+   - tests/spec_decode/e2e/test_integration_dist_tp4
+   - tests/compile/test_basic_correctness
+   - examples/offline_inference/rlhf.py
+   - examples/offline_inference/rlhf_colocate.py
+   - tests/examples/offline_inference/data_parallel.py
+   - tests/v1/test_async_llm_dp.py
+-  - tests/v1/engine/test_engine_core_client.py
+   commands:
+   # test with tp=2 and external_dp=2
+   - VLLM_USE_V1=0 torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
+   - torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
+-  # test with tp=2 and pp=2
+-  - PP_SIZE=2 torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
+   # test with internal dp
+   - python3 ../examples/offline_inference/data_parallel.py
+   - TP_SIZE=2 DP_SIZE=2 pytest -v -s v1/test_async_llm_dp.py
+-  - pytest -v -s v1/engine/test_engine_core_client.py::test_kv_cache_events_dp
+   - pytest -v -s distributed/test_utils.py
+   - pytest -v -s compile/test_basic_correctness.py
+   - pytest -v -s distributed/test_pynccl.py
+-  - pytest -v -s distributed/test_events.py
+   - pytest -v -s spec_decode/e2e/test_integration_dist_tp4.py
+   # TODO: create a dedicated test section for multi-GPU example tests
+   # when we have multiple distributed example tests
+   - pushd ../examples/offline_inference
+-  - VLLM_ALLOW_INSECURE_SERIALIZATION=1 python3 rlhf.py
+-  - VLLM_ALLOW_INSECURE_SERIALIZATION=1 RAY_DEDUP_LOGS=0 python3 rlhf_colocate.py
++  - python3 rlhf.py
++  - RAY_DEDUP_LOGS=0 python3 rlhf_colocate.py
+   - popd
+ 
+ - label: Metrics, Tracing Test # 10min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  mirror_hardwares: [amd]
+   num_gpus: 2
+   source_file_dependencies:
+   - vllm/
+@@ -183,7 +172,7 @@ steps:
+ #####  1 GPU test  #####
+ 
+ - label: Regression Test # 5min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/
+   - tests/test_regression
+@@ -193,7 +182,7 @@ steps:
+   working_dir: "/vllm-workspace/tests" # optional
+ 
+ - label: Engine Test # 10min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/
+   - tests/engine
+@@ -201,14 +190,13 @@ steps:
+   - tests/test_sequence
+   - tests/test_config
+   - tests/test_logger
+-  - tests/test_vllm_port
+   commands:
+-  - pytest -v -s engine test_sequence.py test_config.py test_logger.py test_vllm_port.py
++  - pytest -v -s engine test_sequence.py test_config.py test_logger.py
+   # OOM in the CI unless we run this separately
+   - pytest -v -s tokenization
+ 
+ - label: V1 Test
+-  mirror_hardwares: [amdexperimental]
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+     - vllm/
+     - tests/v1
+@@ -221,11 +209,10 @@ steps:
+     - pytest -v -s v1/worker
+     - pytest -v -s v1/structured_output
+     - pytest -v -s v1/spec_decode
+-    - pytest -v -s v1/kv_connector/unit
+     - pytest -v -s v1/test_serial_utils.py
++    - pytest -v -s v1/test_stats.py
+     - pytest -v -s v1/test_utils.py
+     - pytest -v -s v1/test_oracle.py
+-    - pytest -v -s v1/test_metrics_reader.py
+     # TODO: accuracy does not match, whether setting
+     # VLLM_USE_FLASHINFER_SAMPLER or not on H100.
+     - pytest -v -s v1/e2e
+@@ -234,8 +221,8 @@ steps:
+     - pytest -v -s entrypoints/openai/correctness/test_lmeval.py::test_lm_eval_accuracy_v1_engine
+ 
+ - label: Examples Test # 25min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/examples"
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/entrypoints
+   - examples/
+@@ -250,7 +237,7 @@ steps:
+     - python3 offline_inference/vision_language.py --seed 0
+     - python3 offline_inference/vision_language_embedding.py --seed 0
+     - python3 offline_inference/vision_language_multi_image.py --seed 0
+-    - VLLM_USE_V1=0 python3 others/tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 others/tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors
++    - VLLM_USE_V1=0 python3 other/tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 other/tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors
+     - python3 offline_inference/encoder_decoder.py
+     - python3 offline_inference/encoder_decoder_multimodal.py --model-type whisper --seed 0
+     - python3 offline_inference/basic/classify.py
+@@ -259,7 +246,7 @@ steps:
+     - VLLM_USE_V1=0 python3 offline_inference/profiling.py --model facebook/opt-125m run_num_steps --num-steps 2
+ 
+ - label: Prefix Caching Test # 9min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/
+   - tests/prefix_caching
+@@ -267,7 +254,6 @@ steps:
+     - pytest -v -s prefix_caching
+ 
+ - label: Samplers Test # 36min
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - vllm/model_executor/layers
+   - vllm/sampling_metadata.py
+@@ -277,8 +263,18 @@ steps:
+     - pytest -v -s samplers
+     - VLLM_USE_FLASHINFER_SAMPLER=1 pytest -v -s samplers
+ 
++- label: LogitsProcessor Test # 5min
++  mirror_hardwares: [amd]
++  source_file_dependencies:
++  - vllm/model_executor/layers
++  - vllm/model_executor/guided_decoding
++  - tests/test_logits_processor
++  - tests/model_executor/test_guided_processors
++  commands:
++    - pytest -v -s test_logits_processor.py
++    - pytest -v -s model_executor/test_guided_processors.py
++
+ - label: Speculative decoding tests # 40min
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - vllm/spec_decode
+   - tests/spec_decode
+@@ -289,7 +285,7 @@ steps:
+     - pytest -v -s spec_decode/e2e/test_eagle_correctness.py
+ 
+ - label: LoRA Test %N # 15min each
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/lora
+   - tests/lora
+@@ -297,21 +293,15 @@ steps:
+   parallelism: 4
+ 
+ - label: PyTorch Compilation Unit Tests
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  torch_nightly: true
+   source_file_dependencies:
+     - vllm/
+     - tests/compile
+   commands:
+     - pytest -v -s compile/test_pass_manager.py
+     - pytest -v -s compile/test_fusion.py
+-    - pytest -v -s compile/test_silu_mul_quant_fusion.py
+     - pytest -v -s compile/test_sequence_parallelism.py
+-    - pytest -v -s compile/test_async_tp.py
+ 
+ - label: PyTorch Fullgraph Smoke Test # 9min
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  torch_nightly: true
+   source_file_dependencies:
+   - vllm/
+   - tests/compile
+@@ -320,11 +310,8 @@ steps:
+   # these tests need to be separated, cannot combine
+   - pytest -v -s compile/piecewise/test_simple.py
+   - pytest -v -s compile/piecewise/test_toy_llama.py
+-  - pytest -v -s compile/piecewise/test_full_cudagraph.py
+ 
+ - label: PyTorch Fullgraph Test # 18min
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  torch_nightly: true
+   source_file_dependencies:
+   - vllm/
+   - tests/compile
+@@ -332,7 +319,6 @@ steps:
+   - pytest -v -s compile/test_full_graph.py
+ 
+ - label: Kernels Core Operation Test
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   source_file_dependencies:
+   - csrc/
+   - tests/kernels/core
+@@ -340,7 +326,6 @@ steps:
+     - pytest -v -s kernels/core
+ 
+ - label: Kernels Attention Test %N
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   source_file_dependencies:
+   - csrc/attention/
+   - vllm/attention
+@@ -351,7 +336,6 @@ steps:
+   parallelism: 2
+ 
+ - label: Kernels Quantization Test %N
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   source_file_dependencies:
+   - csrc/quantization/
+   - vllm/model_executor/layers/quantization
+@@ -361,7 +345,6 @@ steps:
+   parallelism: 2
+ 
+ - label: Kernels MoE Test
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - csrc/moe/
+   - tests/kernels/moe
+@@ -370,7 +353,6 @@ steps:
+     - pytest -v -s kernels/moe
+ 
+ - label: Kernels Mamba Test
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - csrc/mamba/
+   - tests/kernels/mamba
+@@ -378,69 +360,48 @@ steps:
+     - pytest -v -s kernels/mamba
+ 
+ - label: Tensorizer Test # 11min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  # mirror_hardwares: [amd]
+   soft_fail: true
+   source_file_dependencies:
+   - vllm/model_executor/model_loader
+   - tests/tensorizer_loader
+-  - tests/entrypoints/openai/test_tensorizer_entrypoint.py
+   commands:
+     - apt-get update && apt-get install -y curl libsodium23
+     - export VLLM_WORKER_MULTIPROC_METHOD=spawn
+     - pytest -v -s tensorizer_loader
+-    - pytest -v -s entrypoints/openai/test_tensorizer_entrypoint.py
+-
+-- label: Model Executor Test
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  soft_fail: true
+-  source_file_dependencies:
+-  - vllm/model_executor
+-  - tests/model_executor
+-  commands:
+-    - apt-get update && apt-get install -y curl libsodium23
+-    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
+-    - pytest -v -s model_executor
+ 
+ - label: Benchmarks # 9min
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   working_dir: "/vllm-workspace/.buildkite"
++  mirror_hardwares: [amd]
+   source_file_dependencies:
+   - benchmarks/
+   commands:
+   - bash scripts/run-benchmarks.sh
+ 
+ - label: Benchmarks CLI Test # 10min
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   source_file_dependencies:
+   - vllm/
+   - tests/benchmarks/
+   commands:
+   - pytest -v -s benchmarks/
+ 
+-- label: Quantization Test
+-  mirror_hardwares: [amdexperimental]
++- label: Quantization Test # 33min
+   source_file_dependencies:
+   - csrc/
+   - vllm/model_executor/layers/quantization
+   - tests/quantization
+-  commands:
+-  # temporary install here since we need nightly, will move to requirements/test.in
+-  # after torchao 0.12 release
+-  - pip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu126
+-  - VLLM_TEST_FORCE_LOAD_FORMAT=auto pytest -v -s quantization
++  command: VLLM_TEST_FORCE_LOAD_FORMAT=auto pytest -v -s quantization
+ 
+ - label: LM Eval Small Models # 53min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/.buildkite/lm-eval-harness"
+   source_file_dependencies:
+   - csrc/
+   - vllm/model_executor/layers/quantization
+   commands:
+   - export VLLM_WORKER_MULTIPROC_METHOD=spawn
+-  - pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-small.txt --tp-size=1
++  - bash ./run-tests.sh -c configs/models-small.txt -t 1
+ 
+ - label: OpenAI API correctness
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - csrc/
+   - vllm/entrypoints/openai/
+@@ -449,7 +410,6 @@ steps:
+   - pytest -s entrypoints/openai/correctness/
+ 
+ - label: Encoder Decoder tests # 5min
+-  mirror_hardwares: [amdexperimental]
+   source_file_dependencies:
+   - vllm/
+   - tests/encoder_decoder
+@@ -457,8 +417,8 @@ steps:
+     - pytest -v -s encoder_decoder
+ 
+ - label: OpenAI-Compatible Tool Use # 20 min
+-  mirror_hardwares: [amdexperimental]
+   fast_check: false
++  #mirror_hardwares: [ amd ]
+   source_file_dependencies:
+     - vllm/
+     - tests/tool_use
+@@ -470,104 +430,92 @@ steps:
+ #####  models test  #####
+ 
+ - label: Basic Models Test # 24min
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  torch_nightly: true
+   source_file_dependencies:
+   - vllm/
+   - tests/models
+   commands:
+     - pytest -v -s models/test_transformers.py
+     - pytest -v -s models/test_registry.py
+-    - pytest -v -s models/test_utils.py
+-    - pytest -v -s models/test_vision.py
+-    - pytest -v -s models/test_initialization.py
++    # V1 Test: https://github.com/vllm-project/vllm/issues/14531
++    - VLLM_USE_V1=0 pytest -v -s models/test_initialization.py -k 'not llama4 and not plamo2'
++    - VLLM_USE_V1=0 pytest -v -s models/test_initialization.py -k 'llama4'
++    - VLLM_USE_V1=0 pytest -v -s models/test_initialization.py -k 'plamo2'
+ 
+-- label: Language Models Test (Standard)
+-  mirror_hardwares: [amdexperimental]
+-  torch_nightly: true
++- label: Language Models Test (Standard) # 32min
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/
+-  - tests/models/language
++  - tests/models/decoder_only/language
++  - tests/models/embedding/language
++  - tests/models/encoder_decoder/language
+   commands:
+     # Install causal-conv1d for plamo2 models here, as it is not compatible with pip-compile.
+-    - pip install 'git+https://github.com/Dao-AILab/causal-conv1d@v1.5.0.post8'
+-    - pip freeze | grep -E 'torch'
+-    - pytest -v -s models/language -m core_model
++    - pip install causal-conv1d
++    - pytest -v -s models/decoder_only/language -m 'core_model or quant_model'
++    - pytest -v -s models/embedding/language -m core_model
+ 
+-- label: Language Models Test (Extended Generation) # 1hr20min
+-  mirror_hardwares: [amdexperimental]
++- label: Language Models Test (Extended) # 1h10min
+   optional: true
+   source_file_dependencies:
+   - vllm/
+-  - tests/models/language/generation
++  - tests/models/decoder_only/language
++  - tests/models/embedding/language
++  - tests/models/encoder_decoder/language
+   commands:
+     # Install causal-conv1d for plamo2 models here, as it is not compatible with pip-compile.
+-    - pip install 'git+https://github.com/Dao-AILab/causal-conv1d@v1.5.0.post8'
+-    - pytest -v -s models/language/generation -m 'not core_model'
+-
+-- label: Language Models Test (Extended Pooling)  # 36min
+-  mirror_hardwares: [amdexperimental]
+-  optional: true
+-  source_file_dependencies:
+-  - vllm/
+-  - tests/models/language/pooling
+-  commands:
+-    - pytest -v -s models/language/pooling -m 'not core_model'
++    - pip install causal-conv1d
++    - pytest -v -s models/decoder_only/language -m 'not core_model and not quant_model'
++    - pytest -v -s models/embedding/language -m 'not core_model'
+ 
+-- label: Multi-Modal Models Test (Standard)
+-  mirror_hardwares: [amdexperimental]
+-  torch_nightly: true
++- label: Multi-Modal Models Test (Standard) # 40min
++  #mirror_hardwares: [amd]
+   source_file_dependencies:
+   - vllm/
+-  - tests/models/multimodal
++  - tests/models/decoder_only/audio_language
++  - tests/models/decoder_only/vision_language
++  - tests/models/embedding/vision_language
++  - tests/models/encoder_decoder/audio_language
++  - tests/models/encoder_decoder/vision_language
+   commands:
+     - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
+-    - pip freeze | grep -E 'torch'
+-    - pytest -v -s models/multimodal/processing
+-    - pytest -v -s --ignore models/multimodal/generation/test_whisper.py models/multimodal -m core_model
+-    - cd .. && pytest -v -s tests/models/multimodal/generation/test_whisper.py -m core_model  # Otherwise, mp_method="spawn" doesn't work
+-
+-- label: Multi-Modal Models Test (Extended) 1
+-  mirror_hardwares: [amdexperimental]
++    - pytest -v -s models/multimodal
++    - pytest -v -s models/decoder_only/audio_language -m 'core_model or quant_model'
++    - pytest -v -s models/decoder_only/vision_language -m 'core_model or quant_model'
++    - pytest -v -s models/embedding/vision_language -m core_model
++    - pytest -v -s models/encoder_decoder/audio_language -m core_model
++    - pytest -v -s models/encoder_decoder/language -m core_model
++    - pytest -v -s models/encoder_decoder/vision_language -m core_model
++    - pytest -v -s models/decoder_only/vision_language/test_interleaved.py
++
++- label: Multi-Modal Models Test (Extended) 1 # 48m
+   optional: true
+   source_file_dependencies:
+   - vllm/
+-  - tests/models/multimodal
++  - tests/models/decoder_only/audio_language
++  - tests/models/decoder_only/vision_language
++  - tests/models/embedding/vision_language
++  - tests/models/encoder_decoder/vision_language
+   commands:
+     - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
+-    - pytest -v -s --ignore models/multimodal/generation/test_common.py --ignore models/multimodal/processing models/multimodal -m 'not core_model'
+-
+-- label: Multi-Modal Models Test (Extended) 2
+-  mirror_hardwares: [amdexperimental]
++    - pytest -v -s models/decoder_only/audio_language -m 'not core_model and not quant_model'
++    - pytest -v -s models/decoder_only/vision_language/test_models.py -m 'split(group=0) and not core_model and not quant_model'
++    - pytest -v -s --ignore models/decoder_only/vision_language/test_models.py models/decoder_only/vision_language -m 'not core_model and not quant_model'
++    - pytest -v -s models/embedding/vision_language -m 'not core_model'
++    - pytest -v -s models/encoder_decoder/language -m 'not core_model'
++    - pytest -v -s models/encoder_decoder/vision_language -m 'not core_model'
++
++- label: Multi-Modal Models Test (Extended) 2 # 38m
+   optional: true
+   source_file_dependencies:
+   - vllm/
+-  - tests/models/multimodal
++  - tests/models/decoder_only/vision_language
+   commands:
+     - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
+-    - pytest -v -s models/multimodal/generation/test_common.py -m 'split(group=0) and not core_model'
+-
+-- label: Multi-Modal Models Test (Extended) 3
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  optional: true
+-  source_file_dependencies:
+-  - vllm/
+-  - tests/models/multimodal
+-  commands:
+-    - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
+-    - pytest -v -s models/multimodal/generation/test_common.py -m 'split(group=1) and not core_model'
+-
+-- label: Quantized Models Test
+-  mirror_hardwares: [amdexperimental, amdproduction]
+-  source_file_dependencies:
+-  - vllm/model_executor/layers/quantization
+-  - tests/models/quantization
+-  commands:
+-    - pytest -v -s models/quantization
++    - pytest -v -s models/decoder_only/vision_language/test_models.py -m 'split(group=1) and not core_model and not quant_model'
+ 
+ # This test is used only in PR development phase to test individual models and should never run on main
+ - label: Custom Models Test
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  mirror_hardwares: [amd]
+   optional: true
+   commands:
+     - echo 'Testing custom models...'
+@@ -579,7 +527,7 @@ steps:
+ #####  multi gpus test  #####
+ 
+ - label: Distributed Comm Ops Test # 7min
+-  mirror_hardwares: [amdexperimental, amdproduction]
++  mirror_hardwares: [amd]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 2
+   source_file_dependencies:
+@@ -590,7 +538,6 @@ steps:
+   - pytest -v -s distributed/test_shm_broadcast.py
+ 
+ - label: 2 Node Tests (4 GPUs in total) # 16min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 2
+   num_nodes: 2
+@@ -609,7 +556,7 @@ steps:
+     - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py | grep 'Same node test passed'
+ 
+ - label: Distributed Tests (2 GPUs) # 40min
+-  mirror_hardwares: [amdexperimental]
++  #mirror_hardwares: [amd]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 2
+   source_file_dependencies:
+@@ -624,11 +571,9 @@ steps:
+   - vllm/worker/model_runner.py
+   - entrypoints/llm/test_collective_rpc.py
+   - tests/v1/test_async_llm_dp.py
+-  - tests/v1/entrypoints/openai/test_multi_api_servers.py
+   - vllm/v1/engine/
+   commands:
+   - TP_SIZE=1 DP_SIZE=2 pytest -v -s v1/test_async_llm_dp.py
+-  - DP_SIZE=2 pytest -v -s v1/entrypoints/openai/test_multi_api_servers.py
+   - pytest -v -s entrypoints/llm/test_collective_rpc.py
+   - pytest -v -s ./compile/test_basic_correctness.py
+   - pytest -v -s ./compile/test_wrapper.py
+@@ -636,8 +581,9 @@ steps:
+   - TARGET_TEST_SUITE=L4 pytest basic_correctness/ -v -s -m 'distributed(num_gpus=2)'
+   # Avoid importing model tests that cause CUDA reinitialization error
+   - pytest models/test_transformers.py -v -s -m 'distributed(num_gpus=2)'
+-  - pytest models/language -v -s -m 'distributed(num_gpus=2)'
+-  - pytest models/multimodal -v -s -m 'distributed(num_gpus=2)'
++  - pytest models/encoder_decoder/language/test_bart.py -v -s -m 'distributed(num_gpus=2)'
++  - pytest models/encoder_decoder/vision_language/test_broadcast.py -v -s -m 'distributed(num_gpus=2)'
++  - pytest models/decoder_only/vision_language/test_models.py -v -s -m 'distributed(num_gpus=2)'
+   # test sequence parallel
+   - pytest -v -s distributed/test_sequence_parallel.py
+   # this test fails consistently.
+@@ -648,14 +594,13 @@ steps:
+   - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s v1/shutdown
+ 
+ - label: Plugin Tests (2 GPUs) # 40min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 2
+   source_file_dependencies:
+   - vllm/plugins/
+   - tests/plugins/
+   commands:
+-  # begin platform plugin and general plugin tests, all the code in-between runs on dummy platform
++  # begin platform plugin tests, all the code in-between runs on dummy platform
+   - pip install -e ./plugins/vllm_add_dummy_platform
+   - pytest -v -s plugins_tests/test_platform_plugins.py
+   - pip uninstall vllm_add_dummy_platform -y
+@@ -666,10 +611,8 @@ steps:
+   - pytest -v -s distributed/test_distributed_oot.py
+   - pytest -v -s entrypoints/openai/test_oot_registration.py # it needs a clean process
+   - pytest -v -s models/test_oot_registration.py # it needs a clean process
+-  - pytest -v -s plugins/lora_resolvers # unit tests for in-tree lora resolver plugins
+ 
+ - label: Multi-step Tests (4 GPUs) # 36min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 4
+   source_file_dependencies:
+@@ -690,7 +633,6 @@ steps:
+   - pytest -v -s multi_step/test_correctness_llm.py
+ 
+ - label: Pipeline Parallelism Test # 45min
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 4
+   source_file_dependencies:
+@@ -704,7 +646,6 @@ steps:
+   - pytest -v -s distributed/test_pipeline_parallel.py
+ 
+ - label: LoRA TP Test (Distributed)
+-  mirror_hardwares: [amdexperimental, amdproduction]
+   num_gpus: 4
+   source_file_dependencies:
+   - vllm/lora
+@@ -720,7 +661,6 @@ steps:
+ 
+ 
+ - label: Weight Loading Multiple GPU Test  # 33min
+-  mirror_hardwares: [amdexperimental]
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 2
+   source_file_dependencies:
+@@ -730,7 +670,6 @@ steps:
+     - bash weight_loading/run_model_weight_loading_test.sh -c weight_loading/models.txt
+ 
+ - label: Weight Loading Multiple GPU Test - Large Models # optional
+-  mirror_hardwares: [amdexperimental] 
+   working_dir: "/vllm-workspace/tests"
+   num_gpus: 2
+   gpu: a100
+@@ -769,4 +708,4 @@ steps:
+   - vllm/model_executor/layers/quantization
+   commands:
+   - export VLLM_WORKER_MULTIPROC_METHOD=spawn
+-  - pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large.txt --tp-size=4
++  - bash ./run-tests.sh -c configs/models-large.txt -t 4
+diff --git a/.github/CODEOWNERS b/.github/CODEOWNERS
+index e98ccd035..76aa5f7a3 100644
+--- a/.github/CODEOWNERS
++++ b/.github/CODEOWNERS
+@@ -10,17 +10,14 @@
+ /vllm/worker/worker.py @zhuohan123 @youkaichao @alexm-redhat @comaniac @njhill
+ /vllm/model_executor/layers/sampler.py @zhuohan123 @youkaichao @alexm-redhat @comaniac @njhill
+ /vllm/model_executor/layers/quantization @mgoin @robertgshaw2-redhat @tlrmchlsmth
+-/vllm/model_executor/guided_decoding @mgoin @russellb @aarnphm
++/vllm/model_executor/guided_decoding @mgoin @russellb
+ /vllm/multimodal @DarkLight1337 @ywang96
+ /vllm/vllm_flash_attn @LucasWilkinson
+-/vllm/lora @jeejeelee
+-/vllm/reasoning @aarnphm
+-/vllm/entrypoints @aarnphm
+ CMakeLists.txt @tlrmchlsmth
+ 
+ # vLLM V1
+ /vllm/v1 @WoosukKwon @robertgshaw2-redhat @njhill @ywang96 @comaniac @alexm-redhat
+-/vllm/v1/structured_output @mgoin @russellb @aarnphm
++/vllm/v1/structured_output @mgoin @russellb
+ 
+ # Test ownership
+ /.buildkite/lm-eval-harness @mgoin @simon-mo
+@@ -29,8 +26,8 @@ CMakeLists.txt @tlrmchlsmth
+ /tests/distributed/test_multi_node_assignment.py @youkaichao
+ /tests/distributed/test_pipeline_parallel.py @youkaichao
+ /tests/distributed/test_same_node.py @youkaichao
+-/tests/entrypoints @DarkLight1337 @robertgshaw2-redhat @simon-mo @aarnphm
+-/tests/entrypoints/llm/test_guided_generate.py @mgoin @russellb @aarnphm
++/tests/entrypoints @DarkLight1337 @robertgshaw2-redhat @simon-mo
++/tests/entrypoints/llm/test_guided_generate.py @mgoin @russellb
+ /tests/kernels @tlrmchlsmth @WoosukKwon
+ /tests/model_executor/test_guided_processors.py @mgoin @russellb
+ /tests/models @DarkLight1337 @ywang96
+@@ -40,11 +37,6 @@ CMakeLists.txt @tlrmchlsmth
+ /tests/quantization @mgoin @robertgshaw2-redhat
+ /tests/spec_decode @njhill @LiuXiaoxuanPKU
+ /tests/test_inputs.py @DarkLight1337 @ywang96
+-/tests/v1/entrypoints/llm/test_struct_output_generate.py @mgoin @russellb @aarnphm
+-/tests/v1/structured_output @mgoin @russellb @aarnphm
++/tests/v1/entrypoints/llm/test_struct_output_generate.py @mgoin @russellb
++/tests/v1/structured_output @mgoin @russellb
+ /tests/weight_loading @mgoin @youkaichao
+-/tests/lora @jeejeelee
+-
+-# Docs
+-/docs @hmellor
+-mkdocs.yaml @hmellor
+diff --git a/.github/ISSUE_TEMPLATE/400-bug-report.yml b/.github/ISSUE_TEMPLATE/400-bug-report.yml
+index 8c5c28cd7..b96ab4074 100644
+--- a/.github/ISSUE_TEMPLATE/400-bug-report.yml
++++ b/.github/ISSUE_TEMPLATE/400-bug-report.yml
+@@ -8,16 +8,6 @@ body:
+   attributes:
+     value: >
+       #### Before submitting an issue, please make sure the issue hasn't been already addressed by searching through [the existing and past issues](https://github.com/vllm-project/vllm/issues?q=is%3Aissue+sort%3Acreated-desc+).
+-- type: markdown
+-  attributes:
+-    value: |
+-       **SECURITY WARNING:** Please review any text you paste to ensure it does not contain sensitive information such as:
+-      - API tokens or keys (e.g., Hugging Face tokens, OpenAI API keys)
+-      - Passwords or authentication credentials
+-      - Private URLs or endpoints
+-      - Personal or confidential data
+-      
+-      Consider redacting or replacing sensitive values with placeholders like `<YOUR_TOKEN_HERE>` when sharing configuration or code examples.
+ - type: textarea
+   attributes:
+     label: Your current environment
+@@ -31,12 +21,12 @@ body:
+       It is suggested to download and execute the latest script, as vllm might frequently update the diagnosis information needed for accurately and quickly responding to issues.
+     value: |
+       <details>
+-      <summary>The output of <code>python collect_env.py</code></summary>
++      <summary>The output of `python collect_env.py`</summary>
+ 
+       ```text
+       Your output of `python collect_env.py` here
+       ```
+-
++      
+       </details>
+   validations:
+     required: true
+@@ -85,20 +75,20 @@ body:
+       ```
+ 
+       ```
+-      The error message you got, with the full traceback and the error logs with [dump_input.py:##] if present.
++      The error message you got, with the full traceback.
+       ```
+   validations:
+     required: true
+ - type: markdown
+   attributes:
+-    value: |
+-       Please separate bugs of `transformers` implementation or usage from bugs of `vllm`. If you think anything is wrong with the model's output:
++    value: >
++       Please separate bugs of `transformers` implementation or usage from bugs of `vllm`. If you think anything is wrong with the models' output:
+ 
+       - Try the counterpart of `transformers` first. If the error appears, please go to [their issues](https://github.com/huggingface/transformers/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc).
+ 
+       - If the error only appears in vllm, please provide the detailed script of how you run `transformers` and `vllm`, also highlight the difference and what you expect.
+ 
+-      Thanks for reporting !
++      Thanks for contributing !
+ - type: checkboxes
+   id: askllm
+   attributes:
+diff --git a/.github/ISSUE_TEMPLATE/450-ci-failure.yml b/.github/ISSUE_TEMPLATE/450-ci-failure.yml
+deleted file mode 100644
+index 7af0e0673..000000000
+--- a/.github/ISSUE_TEMPLATE/450-ci-failure.yml
++++ /dev/null
+@@ -1,69 +0,0 @@
+-name:  CI failure report
+-description: Report a failing test.
+-title: "[CI Failure]: "
+-labels: ["ci-failure"]
+-
+-body:
+-- type: markdown
+-  attributes:
+-    value: >
+-      #### Include the name of the failing Buildkite step and test file in the title.
+-- type: input
+-  attributes:
+-    label: Name of failing test
+-    description: |
+-      Paste in the fully-qualified name of the failing test from the logs.
+-    placeholder: |
+-      `path/to/test_file.py::test_name[params]`
+-  validations:
+-    required: true
+-- type: checkboxes
+-  attributes:
+-    label: Basic information
+-    description: Select all items that apply to the failing test.
+-    options:
+-      - label: Flaky test
+-      - label: Can reproduce locally
+-      - label: Caused by external libraries (e.g. bug in `transformers`)
+-- type: textarea
+-  attributes:
+-    label:  Describe the failing test
+-    description: |
+-      Please provide a clear and concise description of the failing test.
+-    placeholder: |
+-      A clear and concise description of the failing test.
+-  
+-      ```
+-      The error message you got, with the full traceback and the error logs with [dump_input.py:##] if present.
+-      ```
+-  validations:
+-    required: true
+-- type: textarea
+-  attributes:
+-    label:  History of failing test
+-    description: |
+-      Since when did the test start to fail?
+-      You can look up its history via [Buildkite Test Suites](https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests?branch=main).
+-
+-      If you have time, identify the PR that caused the test to fail on main. You can do so via the following methods:
+-
+-      - Use Buildkite Test Suites to find the PR where the test failure first occurred, and reproduce the failure locally.
+-
+-      - Run [`git bisect`](https://git-scm.com/docs/git-bisect) locally.
+-
+-      - Manually unblock Buildkite steps for suspected PRs on main and check the results. (authorized users only)
+-    placeholder: |
+-      Approximate timeline and/or problematic PRs
+-
+-      A link to the Buildkite analytics of the failing test (if available)
+-  validations:
+-    required: true
+-- type: textarea
+-  attributes:
+-    label: CC List.
+-    description: >
+-      The list of people you want to CC. Usually, this includes those who worked on the PR that failed the test.
+-- type: markdown
+-  attributes:
+-    value: >
+-      Thanks for reporting !
+diff --git a/.github/PULL_REQUEST_TEMPLATE.md b/.github/PULL_REQUEST_TEMPLATE.md
+index 017ec7ca8..7042e81a8 100644
+--- a/.github/PULL_REQUEST_TEMPLATE.md
++++ b/.github/PULL_REQUEST_TEMPLATE.md
+@@ -1,18 +1,6 @@
+-## Essential Elements of an Effective PR Description Checklist
+-- [ ] The purpose of the PR, such as "Fix some issue (link existing issues this PR will resolve)".
+-- [ ] The test plan, such as providing test command.
+-- [ ] The test results, such as pasting the results comparison before and after, or e2e results
+-- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
++FILL IN THE PR DESCRIPTION HERE
+ 
+-PLEASE FILL IN THE PR DESCRIPTION HERE ENSURING ALL CHECKLIST ITEMS ABOVE HAVE BEEN CONSIDERED.
+-
+-## Purpose
+-
+-## Test Plan
+-
+-## Test Result
+-
+-## (Optional) Documentation Update
++FIX #xxxx (*link existing issues this PR will resolve*)
+ 
+ <!--- pyml disable-next-line no-emphasis-as-heading -->
+-**BEFORE SUBMITTING, PLEASE READ <https://docs.vllm.ai/en/latest/contributing>** (anything written below this line will be removed by GitHub Actions)
++**BEFORE SUBMITTING, PLEASE READ <https://docs.vllm.ai/en/latest/contributing/overview.html>** (anything written below this line will be removed by GitHub Actions)
+diff --git a/.github/mergify.yml b/.github/mergify.yml
+index 5692bb5d3..15fa3660a 100644
+--- a/.github/mergify.yml
++++ b/.github/mergify.yml
+@@ -36,20 +36,6 @@ pull_request_rules:
+       add:
+         - frontend
+ 
+-- name: label-llama
+-  description: Automatically apply llama label
+-  conditions:
+-    - or:
+-      - files~=^examples/.*llama.*\.py
+-      - files~=^tests/.*llama.*\.py
+-      - files~=^vllm/entrypoints/openai/tool_parsers/llama.*\.py
+-      - files~=^vllm/model_executor/models/.*llama.*\.py
+-      - files~=^vllm/transformers_utils/configs/.*llama.*\.py
+-  actions:
+-    label:
+-      add:
+-        - llama
+-
+ - name: label-multi-modality
+   description: Automatically apply multi-modality label
+   conditions:
+@@ -72,7 +58,7 @@ pull_request_rules:
+       - files~=^benchmarks/structured_schemas/
+       - files=benchmarks/benchmark_serving_structured_output.py
+       - files=benchmarks/run_structured_output_benchmark.sh
+-      - files=docs/features/structured_outputs.md
++      - files=docs/source/features/structured_outputs.md
+       - files=examples/offline_inference/structured_outputs.py
+       - files=examples/online_serving/openai_chat_completion_structured_outputs.py
+       - files=examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
+@@ -149,7 +135,9 @@ pull_request_rules:
+       - files~=^tests/entrypoints/openai/tool_parsers/
+       - files=tests/entrypoints/openai/test_chat_with_tool_reasoning.py
+       - files~=^vllm/entrypoints/openai/tool_parsers/
+-      - files=docs/features/tool_calling.md
++      - files=docs/source/features/tool_calling.md
++      - files=docs/source/getting_started/examples/openai_chat_completion_client_with_tools.md
++      - files=docs/source/getting_started/examples/chat_with_tools.md
+       - files~=^examples/tool_chat_*
+       - files=examples/offline_inference/chat_with_tools.py
+       - files=examples/online_serving/openai_chat_completion_client_with_tools_required.py
+@@ -175,17 +163,6 @@ pull_request_rules:
+ 
+        https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork
+ 
+-- name: assign reviewer for tensorizer changes
+-  conditions:
+-      - files~=^vllm/model_executor/model_loader/tensorizer.py
+-      - files~=^vllm/model_executor/model_loader/tensorizer_loader.py
+-      - files~=^tests/entrypoints/openai/test_tensorizer_entrypoint.py
+-      - files~=^tests/tensorizer_loader/
+-  actions:
+-    assign:
+-      users:
+-        - "sangstar"
+-
+ - name: remove 'needs-rebase' label when conflict is resolved
+   conditions:
+       - -conflict
+diff --git a/.github/scripts/cleanup_pr_body.sh b/.github/scripts/cleanup_pr_body.sh
+index 8d65936fb..3246c6f9b 100755
+--- a/.github/scripts/cleanup_pr_body.sh
++++ b/.github/scripts/cleanup_pr_body.sh
+@@ -26,7 +26,7 @@ sed -i '/\*\*BEFORE SUBMITTING, PLEASE READ.*\*\*/,$d' "${NEW}"
+ 
+ # Remove HTML <details> section that includes <summary> text of "PR Checklist (Click to Expand)"
+ python3 - <<EOF
+-import regex as re
++import re
+ 
+ with open("${NEW}", "r") as file:
+     content = file.read()
+diff --git a/.github/workflows/add_label_automerge.yml b/.github/workflows/add_label_automerge.yml
+index 315042fbf..c9d6d4259 100644
+--- a/.github/workflows/add_label_automerge.yml
++++ b/.github/workflows/add_label_automerge.yml
+@@ -1,6 +1,4 @@
+ name: Add label on auto-merge enabled
+-permissions:
+-    pull-requests: write
+ on:
+     pull_request_target:
+         types:
+diff --git a/.github/workflows/cleanup_pr_body.yml b/.github/workflows/cleanup_pr_body.yml
+index d5c6b8d43..50fea0c43 100644
+--- a/.github/workflows/cleanup_pr_body.yml
++++ b/.github/workflows/cleanup_pr_body.yml
+@@ -20,12 +20,7 @@ jobs:
+         with:
+           python-version: '3.12'
+ 
+-      - name: Install Python dependencies
+-        run: |
+-          python3 -m pip install --upgrade pip
+-          python3 -m pip install regex
+-
+       - name: Update PR description
+         env:
+           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+-        run: bash .github/scripts/cleanup_pr_body.sh "${{ github.event.number }}"
++        run: .github/scripts/cleanup_pr_body.sh "${{ github.event.number }}"
+diff --git a/.github/workflows/lint-and-deploy.yaml b/.github/workflows/lint-and-deploy.yaml
+index 64011922a..7b1d9f699 100644
+--- a/.github/workflows/lint-and-deploy.yaml
++++ b/.github/workflows/lint-and-deploy.yaml
+@@ -2,9 +2,6 @@ name: Lint and Deploy Charts
+ 
+ on: pull_request
+ 
+-permissions:
+-  contents: read
+-
+ jobs:
+   lint-and-deploy:
+     runs-on: ubuntu-latest
+@@ -69,7 +66,7 @@ jobs:
+           export AWS_SECRET_ACCESS_KEY=minioadmin
+           sleep 30 && kubectl -n ns-vllm logs -f "$(kubectl -n ns-vllm get pods | awk '/deployment/ {print $1;exit}')" &
+           helm install --wait --wait-for-jobs --timeout 5m0s --debug --create-namespace --namespace=ns-vllm test-vllm examples/online_serving/chart-helm -f examples/online_serving/chart-helm/values.yaml --set secrets.s3endpoint=http://minio:9000 --set secrets.s3bucketname=testbucket --set secrets.s3accesskeyid=$AWS_ACCESS_KEY_ID --set secrets.s3accesskey=$AWS_SECRET_ACCESS_KEY --set resources.requests.cpu=1 --set resources.requests.memory=4Gi --set resources.limits.cpu=2 --set resources.limits.memory=5Gi --set image.env[0].name=VLLM_CPU_KVCACHE_SPACE --set image.env[1].name=VLLM_LOGGING_LEVEL --set-string image.env[0].value="1" --set-string image.env[1].value="DEBUG" --set-string extraInit.s3modelpath="opt-125m/" --set-string 'resources.limits.nvidia\.com/gpu=0' --set-string 'resources.requests.nvidia\.com/gpu=0' --set-string image.repository="vllm-cpu-env"
+-
++    
+       - name: curl test
+         run: |
+           kubectl -n ns-vllm port-forward service/test-vllm-service 8001:80 &
+@@ -82,4 +79,4 @@ jobs:
+                           "max_tokens": 7,
+                           "temperature": 0
+                   }'):$CODE"
+-          echo "$CODE"
++          echo "$CODE"
+\ No newline at end of file
+diff --git a/.github/workflows/pre-commit.yml b/.github/workflows/pre-commit.yml
+index 8e694d181..6ab63a402 100644
+--- a/.github/workflows/pre-commit.yml
++++ b/.github/workflows/pre-commit.yml
+@@ -5,9 +5,6 @@ on:
+   push:
+     branches: [main]
+ 
+-permissions:
+-  contents: read
+-
+ jobs:
+   pre-commit:
+     runs-on: ubuntu-latest
+diff --git a/.github/workflows/reminder_comment.yml b/.github/workflows/reminder_comment.yml
+index 16ae1aadb..27318c2fd 100644
+--- a/.github/workflows/reminder_comment.yml
++++ b/.github/workflows/reminder_comment.yml
+@@ -1,6 +1,4 @@
+ name: PR Reminder Comment Bot
+-permissions:
+-  pull-requests: write
+ on:
+   pull_request_target:
+     types: [opened]
+diff --git a/.gitignore b/.gitignore
+index e49d1d6ba..728213ceb 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -77,6 +77,10 @@ instance/
+ # Scrapy stuff:
+ .scrapy
+ 
++# Sphinx documentation
++docs/_build/
++docs/source/getting_started/examples/
++
+ # PyBuilder
+ .pybuilder/
+ target/
+@@ -146,7 +150,6 @@ venv.bak/
+ 
+ # mkdocs documentation
+ /site
+-docs/examples
+ 
+ # mypy
+ .mypy_cache/
+diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
+index a105b0e14..f76b24c02 100644
+--- a/.pre-commit-config.yaml
++++ b/.pre-commit-config.yaml
+@@ -11,47 +11,42 @@ repos:
+   hooks:
+   - id: yapf
+     args: [--in-place, --verbose]
+-    # Keep the same list from yapfignore here to avoid yapf failing without any inputs
+-    exclude: '(.buildkite|benchmarks|build|examples)/.*'
+ - repo: https://github.com/astral-sh/ruff-pre-commit
+-  rev: v0.11.7
++  rev: v0.9.3
+   hooks:
+   - id: ruff
+     args: [--output-format, github, --fix]
+-  - id: ruff-format
+-    files: ^(.buildkite|benchmarks|examples)/.*
+ - repo: https://github.com/codespell-project/codespell
+-  rev: v2.4.1
++  rev: v2.4.0
+   hooks:
+   - id: codespell
+     additional_dependencies: ['tomli']
+     args: ['--toml', 'pyproject.toml']
+ - repo: https://github.com/PyCQA/isort
+-  rev: 6.0.1
++  rev: 0a0b7a830386ba6a31c2ec8316849ae4d1b8240d # 6.0.0
+   hooks:
+   - id: isort
+ - repo: https://github.com/pre-commit/mirrors-clang-format
+-  rev: v20.1.3
++  rev: v19.1.7
+   hooks:
+   - id: clang-format
+     exclude: 'csrc/(moe/topk_softmax_kernels.cu|quantization/gguf/(ggml-common.h|dequantize.cuh|vecdotq.cuh|mmq.cuh|mmvq.cuh))|vllm/third_party/.*'
+     types_or: [c++, cuda]
+     args: [--style=file, --verbose]
+ - repo: https://github.com/jackdewinter/pymarkdown
+-  rev: v0.9.29
++  rev: v0.9.27
+   hooks:
+   - id: pymarkdown
+-    exclude: '.*\.inc\.md'
+     args: [fix]
+ - repo: https://github.com/rhysd/actionlint
+   rev: v1.7.7
+   hooks:
+   - id: actionlint
+ - repo: https://github.com/astral-sh/uv-pre-commit
+-  rev: 0.6.17
++  rev: 0.6.2
+   hooks:
+     - id: pip-compile
+-      args: [requirements/test.in, -o, requirements/test.txt, --index-strategy, unsafe-best-match, --torch-backend, cu128]
++      args: [requirements/test.in, -o, requirements/test.txt]
+       files: ^requirements/test\.(in|txt)$
+ - repo: local
+   hooks:
+@@ -60,7 +55,7 @@ repos:
+     entry: tools/mypy.sh 0 "local"
+     language: python
+     types: [python]
+-    additional_dependencies: &mypy_deps [mypy==1.11.1, types-cachetools, types-setuptools, types-PyYAML, types-requests, pydantic]
++    additional_dependencies: &mypy_deps [mypy==1.11.1, types-cachetools, types-setuptools, types-PyYAML, types-requests]
+     stages: [pre-commit] # Don't run in CI
+   - id: mypy-3.9 # TODO: Use https://github.com/pre-commit/mirrors-mypy when mypy setup is less awkward
+     name: Run mypy for Python 3.9
+@@ -106,8 +101,8 @@ repos:
+     args:
+       - -c
+       - |
+-        if ! grep -q "^Signed-off-by: $(git config user.name) <$(git config user.email)>" "$(git rev-parse --git-path COMMIT_EDITMSG)"; then
+-          printf "\nSigned-off-by: $(git config user.name) <$(git config user.email)>\n" >> "$(git rev-parse --git-path COMMIT_EDITMSG)"
++        if ! grep -q "^Signed-off-by: $(git config user.name) <$(git config user.email)>" .git/COMMIT_EDITMSG; then
++          printf "\nSigned-off-by: $(git config user.name) <$(git config user.email)>\n" >> .git/COMMIT_EDITMSG
+         fi
+     language: system
+     verbose: true
+@@ -130,21 +125,8 @@ repos:
+     name: Update Dockerfile dependency graph
+     entry: tools/update-dockerfile-graph.sh
+     language: script
+-  - id: enforce-import-regex-instead-of-re
+-    name: Enforce import regex as re
+-    entry: python tools/enforce_regex_import.py
+-    language: python
+-    types: [python]
+-    pass_filenames: false
+-    additional_dependencies: [regex]
+-  # forbid directly import triton
+-  - id: forbid-direct-triton-import
+-    name: "Forbid direct 'import triton'"
+-    entry: python tools/check_triton_import.py
+-    language: python
+-    types: [python]
++    files: ^docker/Dockerfile$
+     pass_filenames: false
+-    additional_dependencies: [regex]
+   # Keep `suggestion` last
+   - id: suggestion
+     name: Suggestion
+diff --git a/.readthedocs.yaml b/.readthedocs.yaml
+index 98c3be25f..2781ec223 100644
+--- a/.readthedocs.yaml
++++ b/.readthedocs.yaml
+@@ -8,8 +8,12 @@ build:
+   tools:
+     python: "3.12"
+ 
+-mkdocs:
+-  configuration: mkdocs.yaml
++sphinx:
++  configuration: docs/source/conf.py
++  fail_on_warning: true
++
++# If using Sphinx, optionally build your docs in additional formats such as PDF
++formats: []
+ 
+ # Optionally declare the Python requirements required to build your docs
+ python:
 diff --git a/CMakeLists.txt b/CMakeLists.txt
-old mode 100755
-new mode 100644
-index c823c9ff8..9b1c192e8
+index bd389823f..f199d66eb 100644
 --- a/CMakeLists.txt
 +++ b/CMakeLists.txt
-@@ -19,6 +19,8 @@ set(VLLM_TARGET_DEVICE "cuda" CACHE STRING "Target device backend for vLLM")
+@@ -18,6 +18,32 @@ set(VLLM_TARGET_DEVICE "cuda" CACHE STRING "Target device backend for vLLM")
  message(STATUS "Build type: ${CMAKE_BUILD_TYPE}")
  message(STATUS "Target device: ${VLLM_TARGET_DEVICE}")
  
-+option(USE_MACA          "enable MACA"  ON)
++option(USE_MACA "Enable MACA Support" ON)
++option(ENABLE_BLAS_API "Enable blas api for fused moe" ON)
++
++if (USE_MACA)
++  message(STATUS "###########################")
++  message(STATUS "Detected Using MACA")
++  message(STATUS "###########################")
++
++  add_compile_definitions(
++    MACA_VERSION_MAJOR=${MACA_VERSION_MAJOR}
++    MACA_VERSION_MINOR=${MACA_VERSION_MINOR}
++    MACA_VERSION_PATCH=${MACA_VERSION_PATCH}
++    MACA_VERSION_BUILD=${MACA_VERSION_BUILD}
++  )
++  message(STATUS "MACA version: ${MACA_VERSION_MAJOR}.${MACA_VERSION_MINOR}.${MACA_VERSION_PATCH}.${MACA_VERSION_BUILD}")
++
++  set(MACA_PATH "$ENV{MACA_PATH}")
++  add_compile_definitions(USE_MACA)
++
++  if (MACA_PATH AND EXISTS ${MACA_PATH})
++    message(STATUS "MACA found at ${MACA_PATH}")
++  else()
++    message(FATAL_ERROR "MACA not found or invalid path, please check your MACA_PATH")
++  endif()
++endif()
 +
  include(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)
  
  # Suppress potential warnings about unused manually-specified variables
-@@ -46,7 +48,8 @@ set(HIP_SUPPORTED_ARCHS "gfx906;gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx11
+@@ -45,8 +71,13 @@ set(HIP_SUPPORTED_ARCHS "gfx906;gfx908;gfx90a;gfx942;gfx950;gfx1030;gfx1100;gfx1
  # requirements.txt files and should be kept consistent.  The ROCm torch
- # versions are derived from Dockerfile.rocm
+ # versions are derived from docker/Dockerfile.rocm
  #
--set(TORCH_SUPPORTED_VERSION_CUDA "2.5.1")
-+set(TORCH_SUPPORTED_VERSION_CUDA "2.1.2")
-+#set(TORCH_SUPPORTED_VERSION_CUDA "2.5.1")
- set(TORCH_SUPPORTED_VERSION_ROCM "2.5.1")
++if (USE_MACA)
++set(TORCH_SUPPORTED_VERSION_CUDA "2.6.0")
++set(TORCH_SUPPORTED_VERSION_ROCM "2.6.0")
++else()
+ set(TORCH_SUPPORTED_VERSION_CUDA "2.7.0")
+ set(TORCH_SUPPORTED_VERSION_ROCM "2.7.0")
++endif()
  
  #
-@@ -80,6 +83,8 @@ endif()
+ # Try to find python package with an executable that exactly matches
+@@ -162,7 +193,7 @@ endif()
+ # `VLLM_GPU_LANG`.
+ # The final set of arches is stored in `VLLM_GPU_FLAGS`.
  #
- find_package(Torch REQUIRED)
+-get_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG})
++get_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG} USE_MACA)
  
-+add_compile_definitions(USE_MACA)
-+
  #
- # Forward the non-CUDA device extensions to external CMake scripts.
- #
-@@ -224,6 +229,13 @@ set(VLLM_EXT_SRC
-   "csrc/prepare_inputs/advance_step.cu"
+ # Set nvcc parallelism.
+@@ -255,6 +286,13 @@ set(VLLM_EXT_SRC
+   "csrc/custom_all_reduce.cu"
    "csrc/torch_bindings.cpp")
  
 +# support opt of gptq-marlin
@@ -46,27 +3113,40 @@ index c823c9ff8..9b1c192e8
  if(VLLM_GPU_LANG STREQUAL "CUDA")
    SET(CUTLASS_ENABLE_HEADERS_ONLY ON CACHE BOOL "Enable only the header library")
  
-@@ -235,6 +247,10 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+@@ -266,6 +304,12 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
      set(VLLM_CUTLASS_SRC_DIR $ENV{VLLM_CUTLASS_SRC_DIR})
    endif()
  
++  # Substitue CUTLASS with MATLASS 
 +  if (USE_MACA)
-+    set(VLLM_CUTLASS_SRC_DIR $ENV{MACA_PATH}/include) # For MACA cutlass header files.
++    message(WARNING "Use MACA, Overwrite VLLM_CUTLASS_SRC_DIR.")
++    set(VLLM_CUTLASS_SRC_DIR $ENV{MACA_PATH}/include)
 +  endif()
 +
    if(VLLM_CUTLASS_SRC_DIR)
      if(NOT IS_ABSOLUTE VLLM_CUTLASS_SRC_DIR)
        get_filename_component(VLLM_CUTLASS_SRC_DIR "${VLLM_CUTLASS_SRC_DIR}" ABSOLUTE)
-@@ -259,7 +275,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+@@ -286,16 +330,16 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+         GIT_SHALLOW TRUE
+     )
+   endif()
+-  FetchContent_MakeAvailable(cutlass)
++  #FetchContent_MakeAvailable(cutlass)
+ 
    list(APPEND VLLM_EXT_SRC
-     "csrc/mamba/mamba_ssm/selective_scan_fwd.cu"
-     "csrc/mamba/causal_conv1d/causal_conv1d.cu"
 -    "csrc/quantization/aqlm/gemm_kernels.cu"
-+    #"csrc/quantization/aqlm/gemm_kernels.cu"
++    # "csrc/quantization/aqlm/gemm_kernels.cu"
      "csrc/quantization/awq/gemm_kernels.cu"
-     "csrc/custom_all_reduce.cu"
      "csrc/permute_cols.cu"
-@@ -272,10 +288,17 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+     "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu"
+     "csrc/quantization/fp4/nvfp4_quant_entry.cu"
+     "csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu"
+-    "csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu"
++    # "csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu"
+     "csrc/sparse/cutlass/sparse_scaled_mm_entry.cu"
+     "csrc/cutlass_extensions/common.cpp"
+     "csrc/attention/mla/cutlass_mla_entry.cu")
+@@ -304,12 +348,26 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
      SRCS "${VLLM_EXT_SRC}"
      CUDA_ARCHS "${CUDA_ARCHS}")
  
@@ -76,1614 +3156,223 @@ index c823c9ff8..9b1c192e8
 +    PROPERTIES
 +    COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128"
 +  )
++
++  # support opt of cutlass w8a8 scale mm
++  set_source_files_properties(
++    "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu"
++    PROPERTIES
++    COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -misched-postra=true"
++  )  
 +
    # Only build Marlin kernels if we are building for at least some compatible archs.
    # Keep building Marlin for 9.0 as there are some group sizes and shapes that
    # are not supported by Machete yet.
--  cuda_archs_loose_intersection(MARLIN_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}")
-+  # cuda_archs_loose_intersection(MARLIN_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}") # Not support
-   if (MARLIN_ARCHS)
-     set(MARLIN_SRCS
-        "csrc/quantization/fp8/fp8_marlin.cu"
-@@ -297,6 +320,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+   # 9.0 for latest bf16 atomicAdd PTX
+   cuda_archs_loose_intersection(MARLIN_ARCHS "8.0;8.7;9.0+PTX" "${CUDA_ARCHS}")
+-  if (MARLIN_ARCHS)
++  if (MARLIN_ARCHS AND NOT USE_MACA)
  
-   # The cutlass_scaled_mm kernels for Hopper (c3x, i.e. CUTLASS 3.x) require
-   # CUDA 12.0 or later (and only work on Hopper, 9.0a for now).
-+  if(0)
-   cuda_archs_loose_intersection(SCALED_MM_3X_ARCHS "9.0a" "${CUDA_ARCHS}")
-   if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_3X_ARCHS)
-     set(SRCS 
-@@ -326,6 +350,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
-     # build any 3x kernels
-     set(SCALED_MM_3X_ARCHS)
+     #
+     # For the Marlin kernels we automatically generate sources for various
+@@ -368,13 +426,14 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+     list(APPEND VLLM_EXT_SRC "${MARLIN_SRCS}")
+     message(STATUS "Building Marlin kernels for archs: ${MARLIN_ARCHS}")
+   else()
++    message(WARNING "Use MACA, Skip MARLIN_ARCHS.")
+     message(STATUS "Not building Marlin kernels as no compatible archs found"
+                    " in CUDA target architectures")
    endif()
-+  endif()
  
-   #
-   # For the cutlass_scaled_mm kernels we want to build the c2x (CUTLASS 2.x)
-@@ -475,14 +500,19 @@ target_compile_definitions(_C PRIVATE CUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1)
- set(VLLM_MOE_EXT_SRC
-   "csrc/moe/torch_bindings.cpp"
+   # Only build AllSpark kernels if we are building for at least some compatible archs.
+   cuda_archs_loose_intersection(ALLSPARK_ARCHS "8.0;8.6;8.7;8.9" "${CUDA_ARCHS}")
+-  if (ALLSPARK_ARCHS)
++  if (ALLSPARK_ARCHS AND NOT USE_MACA)
+     set(ALLSPARK_SRCS
+        "csrc/quantization/gptq_allspark/allspark_repack.cu"
+        "csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu")
+@@ -384,6 +443,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+     list(APPEND VLLM_EXT_SRC "${ALLSPARK_SRCS}")
+     message(STATUS "Building AllSpark kernels for archs: ${ALLSPARK_ARCHS}")
+   else()
++    message(WARNING "Use MACA, Skip ALLSPARK_ARCHS.")
+     message(STATUS "Not building AllSpark kernels as no compatible archs found"
+                    " in CUDA target architectures")
+   endif()
+@@ -393,7 +453,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+   # The cutlass_scaled_mm kernels for Hopper (c3x, i.e. CUTLASS 3.x) require
+   # CUDA 12.0 or later
+   cuda_archs_loose_intersection(SCALED_MM_ARCHS "9.0a;" "${CUDA_ARCHS}")
+-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS)
++  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS AND NOT USE_MACA)
+     set(SRCS
+        "csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90.cu"
+        "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu"
+@@ -409,6 +469,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+     list(APPEND SCALED_MM_3X_ARCHS "${SCALED_MM_ARCHS}")
+     message(STATUS "Building scaled_mm_c3x_sm90 for archs: ${SCALED_MM_ARCHS}")
+   else()
++    message(WARNING "Use MACA, Skip SCALED_MM_ARCHS.")
+     if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS)
+       message(STATUS "Not building scaled_mm_c3x_sm90 as CUDA Compiler version is "
+                      "not >= 12.0, we recommend upgrading to CUDA 12.0 or "
+@@ -464,6 +525,13 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+       CUDA_ARCHS "${SCALED_MM_2X_ARCHS}")
+     list(APPEND VLLM_EXT_SRC "${SRCS}")
+     list(APPEND VLLM_GPU_FLAGS "-DENABLE_SCALED_MM_C2X=1")
++    
++    # support opt of cutlass w8a8 scale mm
++    set_source_files_properties(
++      "csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu"
++      PROPERTIES
++      COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -misched-postra=true"
++    )
+     message(STATUS "Building scaled_mm_c2x for archs: ${SCALED_MM_2X_ARCHS}")
+   else()
+     if (SCALED_MM_3X_ARCHS)
+@@ -502,7 +570,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+ 
+   # FP4 Archs and flags
+   cuda_archs_loose_intersection(FP4_ARCHS "10.0a" "${CUDA_ARCHS}")
+-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND FP4_ARCHS)
++  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND FP4_ARCHS AND NOT USE_MACA)
+     set(SRCS
+       "csrc/quantization/fp4/nvfp4_quant_kernels.cu"
+       "csrc/quantization/fp4/nvfp4_experts_quant.cu"
+@@ -515,6 +583,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+     list(APPEND VLLM_GPU_FLAGS "-DENABLE_NVFP4=1")
+     message(STATUS "Building NVFP4 for archs: ${FP4_ARCHS}")
+   else()
++    message(WARNING "Use MACA, Skip FP4_ARCHS.")
+     message(STATUS "Not building NVFP4 as no compatible archs were found.")
+     # clear FP4_ARCHS
+     set(FP4_ARCHS)
+@@ -522,7 +591,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+ 
+   # CUTLASS MLA Archs and flags
+   cuda_archs_loose_intersection(MLA_ARCHS "10.0a" "${CUDA_ARCHS}")
+-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND MLA_ARCHS)
++  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND MLA_ARCHS AND NOT USE_MACA)
+     set(SRCS
+       "csrc/attention/mla/cutlass_mla_kernels.cu")
+     set_gencode_flags_for_srcs(
+@@ -535,6 +604,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+       PROPERTIES INCLUDE_DIRECTORIES "${CUTLASS_DIR}/examples/77_blackwell_fmha;${CUTLASS_DIR}/examples/common")
+     message(STATUS "Building CUTLASS MLA for archs: ${MLA_ARCHS}")
+   else()
++    message(WARNING "Use MACA, Skip MLA_ARCHS.")
+     message(STATUS "Not building CUTLASS MLA as no compatible archs were found.")
+     # clear MLA_ARCHS
+     set(MLA_ARCHS)
+@@ -666,6 +736,13 @@ set(VLLM_MOE_EXT_SRC
    "csrc/moe/moe_align_sum_kernels.cu"
--  "csrc/moe/topk_softmax_kernels.cu")
-+  "csrc/moe/topk_softmax_kernels.cu"
-+  "csrc/moe/moe_ops.cpp")
+   "csrc/moe/topk_softmax_kernels.cu")
+ 
++if (USE_MACA AND ENABLE_BLAS_API)
++  list(APPEND VLLM_MOE_EXT_SRC "csrc/moe/moe_ops.cpp")
 +
++  set(MCBLAS_INCLUDE_DIR $ENV{MACA_PATH}/include/mcblas)
++  set(MCBLAS_LIB $ENV{MACA_PATH}/lib/libmcblas.so)
++endif()
 +
-+set(MCBLAS_INCLUDE_DIR $ENV{MACA_PATH}/include/mcblas)
-+set(MCBLAS_LIB $ENV{MACA_PATH}/lib/libmcblas.so)
- 
- set_gencode_flags_for_srcs(
-   SRCS "${VLLM_MOE_EXT_SRC}"
-   CUDA_ARCHS "${CUDA_ARCHS}")
- 
  if(VLLM_GPU_LANG STREQUAL "CUDA")
--  cuda_archs_loose_intersection(MARLIN_MOE_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}")
-+  # cuda_archs_loose_intersection(MARLIN_MOE_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}") # Not support
-   if (MARLIN_MOE_ARCHS)
-     set(MARLIN_MOE_SRC
-         "csrc/moe/marlin_kernels/marlin_moe_kernel.h"
-@@ -514,6 +544,8 @@ define_gpu_extension_target(
-   SOURCES ${VLLM_MOE_EXT_SRC}
-   COMPILE_FLAGS ${VLLM_GPU_FLAGS}
+   list(APPEND VLLM_MOE_EXT_SRC "csrc/moe/moe_wna16.cu")
+ endif()
+@@ -685,7 +762,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+   list(APPEND VLLM_MOE_EXT_SRC "${VLLM_MOE_WNA16_SRC}")
+   # 9.0 for latest bf16 atomicAdd PTX
+   cuda_archs_loose_intersection(MARLIN_MOE_ARCHS "8.0;8.7;9.0+PTX" "${CUDA_ARCHS}")
+-  if (MARLIN_MOE_ARCHS)
++  if (MARLIN_MOE_ARCHS AND NOT USE_MACA)
+ 
+     #
+     # For the Marlin MOE kernels we automatically generate sources for various
+@@ -733,6 +810,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+ 
+     message(STATUS "Building Marlin MOE kernels for archs: ${MARLIN_MOE_ARCHS}")
+   else()
++    message(WARNING "Use MACA, Skip MARLIN_MOE_ARCHS.")
+     message(STATUS "Not building Marlin MOE kernels as no compatible archs found"
+                    " in CUDA target architectures")
+   endif()
+@@ -750,6 +828,15 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
+   list(APPEND VLLM_MOE_EXT_SRC "${MOE_PERMUTE_SRC}")
+ endif()
+ message(STATUS "Enabling moe extension.")
++
++set(BLAS_API_ARGS "")
++if(USE_MACA AND ENABLE_BLAS_API)
++  message(STATUS "Blas API for fused moe enabled")
++  list(APPEND BLAS_API_ARGS 
++      INCLUDE_DIRECTORIES ${MCBLAS_INCLUDE_DIR}
++      LIBRARIES ${MCBLAS_LIB})
++endif()
++
+ define_gpu_extension_target(
+   _moe_C
+   DESTINATION vllm
+@@ -759,6 +846,7 @@ define_gpu_extension_target(
    ARCHITECTURES ${VLLM_GPU_ARCHES}
-+  INCLUDE_DIRECTORIES ${MCBLAS_INCLUDE_DIR}
-+  LIBRARIES ${MCBLAS_LIB}
+   INCLUDE_DIRECTORIES ${CUTLASS_INCLUDE_DIR}
+   INCLUDE_DIRECTORIES ${CUTLASS_TOOLS_UTIL_INCLUDE_DIR}
++  ${BLAS_API_ARGS}
    USE_SABI 3
    WITH_SOABI)
  
-@@ -567,46 +599,46 @@ endif()
- # This is to enable local development of vllm-flash-attn within vLLM.
- # It can be set as an environment variable or passed as a cmake argument.
- # The environment variable takes precedence.
--if (DEFINED ENV{VLLM_FLASH_ATTN_SRC_DIR})
--  set(VLLM_FLASH_ATTN_SRC_DIR $ENV{VLLM_FLASH_ATTN_SRC_DIR})
--endif()
--
--if(VLLM_FLASH_ATTN_SRC_DIR)
--  FetchContent_Declare(
--          vllm-flash-attn SOURCE_DIR 
--          ${VLLM_FLASH_ATTN_SRC_DIR}
--          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
--  )
--else()
--  FetchContent_Declare(
--          vllm-flash-attn
--          GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git
--          GIT_TAG d4e09037abf588af1ec47d0e966b237ee376876c
--          GIT_PROGRESS TRUE
--          # Don't share the vllm-flash-attn build between build types
--          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
--  )
--endif()
-+#if (DEFINED ENV{VLLM_FLASH_ATTN_SRC_DIR})
-+#  set(VLLM_FLASH_ATTN_SRC_DIR $ENV{VLLM_FLASH_ATTN_SRC_DIR})
-+#endif()
-+
-+#if(VLLM_FLASH_ATTN_SRC_DIR)
-+#  FetchContent_Declare(
-+#          vllm-flash-attn SOURCE_DIR 
-+#          ${VLLM_FLASH_ATTN_SRC_DIR}
-+#          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
-+#  )
-+#else()
-+#  FetchContent_Declare(
-+#          vllm-flash-attn
-+#          GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git
-+#          GIT_TAG d4e09037abf588af1ec47d0e966b237ee376876c
-+#          GIT_PROGRESS TRUE
-+#          # Don't share the vllm-flash-attn build between build types
-+#          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
-+#  )
-+#endif()
- 
- 
- # Fetch the vllm-flash-attn library
--FetchContent_MakeAvailable(vllm-flash-attn)
--message(STATUS "vllm-flash-attn is available at ${vllm-flash-attn_SOURCE_DIR}")
-+# FetchContent_MakeAvailable(vllm-flash-attn)
-+# message(STATUS "vllm-flash-attn is available at ${vllm-flash-attn_SOURCE_DIR}")
- 
- # Copy over the vllm-flash-attn python files (duplicated for fa2 and fa3, in
- # case only one is built, in the case both are built redundant work is done)
--install(
--  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
--  DESTINATION vllm_flash_attn
--  COMPONENT _vllm_fa2_C
--  FILES_MATCHING PATTERN "*.py"
--)
--
--install(
--  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
--  DESTINATION vllm_flash_attn
--  COMPONENT _vllm_fa3_C
--  FILES_MATCHING PATTERN "*.py"
--)
-+#install(
-+#  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
-+#  DESTINATION vllm_flash_attn
-+#  COMPONENT _vllm_fa2_C
-+#  FILES_MATCHING PATTERN "*.py"
-+#)
-+
-+#install(
-+#  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
-+#  DESTINATION vllm_flash_attn
-+#  COMPONENT _vllm_fa3_C
-+#  FILES_MATCHING PATTERN "*.py"
-+#)
- 
- # Nothing after vllm-flash-attn, see comment about macros above
-diff --git a/NOTICE b/NOTICE
-new file mode 100644
-index 000000000..aa8efcedb
---- /dev/null
-+++ b/NOTICE
-@@ -0,0 +1,130 @@
-+The following files may have been Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. in 2025. 
-+
-+CMakeLists.txt
-+benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
-+benchmarks/kernels/benchmark_moe.py
-+cmake/utils.cmake
-+csrc/activation_kernels.cu
-+csrc/attention/attention_kernels.cuh
-+csrc/attention/attention_utils.cuh
-+csrc/attention/dtype_float16.cuh
-+csrc/attention/paged_attention_v1.cu
-+csrc/attention/paged_attention_v2.cu
-+csrc/cache.h
-+csrc/cache_kernels.cu
-+csrc/core/scalar_type.hpp
-+csrc/cuda_compat.h
-+csrc/custom_all_reduce.cuh
-+csrc/cutlass_extensions/common.hpp
-+csrc/mamba/causal_conv1d/causal_conv1d.cu
-+csrc/mamba/mamba_ssm/selective_scan_fwd.cu
-+csrc/moe/moe_align_sum_kernels.cu
-+csrc/moe/moe_ops.h
-+csrc/ops.h
-+csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
-+csrc/quantization/awq/awq_4bits.cuh
-+csrc/quantization/awq/dequant.cuh
-+csrc/quantization/awq/dequantize.cuh
-+csrc/quantization/awq/gemm_kernels.cu
-+csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
-+csrc/quantization/awq/hgemv_selector.hpp
-+csrc/quantization/compressed_tensors/int8_quant_kernels.cu
-+csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
-+csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
-+csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
-+csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
-+csrc/quantization/fp8/common.cu
-+csrc/quantization/fp8/nvidia/quant_utils.cuh
-+csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
-+csrc/quantization/fused_kernels/quant_conversions.cuh
-+csrc/quantization/gguf/ggml-common.h
-+csrc/quantization/gptq/Hgemm_common.cuh
-+csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
-+csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
-+csrc/quantization/gptq/dequant.cuh
-+csrc/quantization/gptq/gptq.cuh
-+csrc/quantization/gptq/hgemm_gptq.h
-+csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
-+csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
-+csrc/quantization/gptq/hgemv_selector.hpp
-+csrc/quantization/gptq/q_gemm.cu
-+csrc/quantization/gptq/qdq_4.cuh
-+csrc/quantization/gptq/scalar_type.hpp
-+csrc/quantization/vectorization.cuh
-+csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
-+examples/llm_engine_example_bytemlperf.py
-+requirements-cuda.txt
-+setup.py
-+tests/distributed/test_pynccl.py
-+tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
-+tests/kernels/test_awq.py
-+tests/kernels/test_causal_conv1d.py
-+tests/kernels/test_cutlass.py
-+tests/kernels/test_fused_quant_layernorm.py
-+tests/kernels/test_int8_quant.py
-+tests/kernels/test_mamba_ssm.py
-+tests/kernels/test_marlin_gemm.py
-+tests/kernels/utils.py
-+vllm/__init__.py
-+vllm/_custom_ops.py
-+vllm/attention/backends/flash_attn.py
-+vllm/attention/backends/flash_attn_pg.py
-+vllm/attention/backends/mla/utils.py
-+vllm/attention/backends/triton_mla.py
-+vllm/attention/layer.py
-+vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
-+vllm/attention/ops/blocksparse_attention/interface.py
-+vllm/attention/ops/paged_attn.py
-+vllm/attention/ops/triton_decode_attention.py
-+vllm/config.py
-+vllm/distributed/device_communicators/pynccl_wrapper.py
-+vllm/distributed/kv_transfer/kv_connector/base.py
-+vllm/distributed/kv_transfer/kv_connector/factory.py
-+vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
-+vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-+vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
-+vllm/distributed/kv_transfer/kv_transfer_agent.py
-+vllm/distributed/kv_transfer/kv_transfer_utils.py
-+vllm/distributed/parallel_state.py
-+vllm/distributed/utils.py
-+vllm/engine/arg_utils.py
-+vllm/engine/llm_engine.py
-+vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
-+vllm/envs.py
-+vllm/executor/executor_base.py
-+vllm/executor/mp_distributed_executor.py
-+vllm/executor/ray_distributed_executor.py
-+vllm/model_executor/layers/fused_moe/fused_moe.py
-+vllm/model_executor/layers/linear.py
-+vllm/model_executor/layers/logits_processor.py
-+vllm/model_executor/layers/quantization/__init__.py
-+vllm/model_executor/layers/quantization/awq.py
-+vllm/model_executor/layers/quantization/base_config.py
-+vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
-+vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
-+vllm/model_executor/layers/quantization/gptq.py
-+vllm/model_executor/layers/quantization/moe_wna16.py
-+vllm/model_executor/layers/quantization/utils/fp8_utils.py
-+vllm/model_executor/layers/vocab_parallel_embedding.py
-+vllm/model_executor/model_loader/__init__.py
-+vllm/model_executor/model_loader/loader.py
-+vllm/model_executor/models/baichuan_moe.py
-+vllm/model_executor/models/deepseek.py
-+vllm/model_executor/models/deepseek_v2.py
-+vllm/model_executor/models/qwen.py
-+vllm/model_executor/models/registry.py
-+vllm/model_executor/models/telechat.py
-+vllm/platforms/__init__.py
-+vllm/platforms/cuda.py
-+vllm/platforms/pynvml.py
-+vllm/triton_utils/custom_cache_manager.py
-+vllm/utils.py
-+vllm/v1/attention/backends/flash_attn.py
-+vllm/v1/core/scheduler.py
-+vllm/v1/engine/core.py
-+vllm/v1/executor/abstract.py
-+vllm/v1/executor/ray_distributed_executor.py
-+vllm/version.py
-+vllm/worker/model_runner.py
-+vllm/worker/multi_step_model_runner.py
-+vllm/worker/worker_base.py
-diff --git a/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh b/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
-old mode 100644
-new mode 100755
-index eb5d891d0..bd1c1e1b5
---- a/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
-+++ b/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
-@@ -3,7 +3,7 @@
- # Requirement: 2x GPUs.
- 
- 
--# Model: meta-llama/Meta-Llama-3.1-8B-Instruct
-+# Model: /pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct
- # Query: 1024 input tokens, 6 output tokens, QPS 2/4/6/8, 100 requests
- # Resource: 2x GPU
- # Approaches:
-@@ -34,7 +34,7 @@ wait_for_server() {
- 
- 
- launch_chunked_prefill() {
--  model="meta-llama/Meta-Llama-3.1-8B-Instruct"
-+  model="/pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct"
-   # disagg prefill
-   CUDA_VISIBLE_DEVICES=0 python3 \
-     -m vllm.entrypoints.openai.api_server \
-@@ -58,7 +58,7 @@ launch_chunked_prefill() {
- 
- 
- launch_disagg_prefill() {
--  model="meta-llama/Meta-Llama-3.1-8B-Instruct" 
-+  model="/pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct" 
-   # disagg prefill
-   CUDA_VISIBLE_DEVICES=0 python3 \
-     -m vllm.entrypoints.openai.api_server \
-@@ -66,8 +66,9 @@ launch_disagg_prefill() {
-     --port 8100 \
-     --max-model-len 10000 \
-     --gpu-memory-utilization 0.6 \
-+    --enforce-eager \
-     --kv-transfer-config \
--    '{"kv_connector":"PyNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
-+    '{"kv_connector":"LayerwisePyNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
- 
-   CUDA_VISIBLE_DEVICES=1 python3 \
-     -m vllm.entrypoints.openai.api_server \
-@@ -76,7 +77,7 @@ launch_disagg_prefill() {
-     --max-model-len 10000 \
-     --gpu-memory-utilization 0.6 \
-     --kv-transfer-config \
--    '{"kv_connector":"PyNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
-+    '{"kv_connector":"LayerwisePyNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
- 
-   wait_for_server 8100
-   wait_for_server 8200
-@@ -87,7 +88,7 @@ launch_disagg_prefill() {
- 
- benchmark() {
-   results_folder="./results"
--  model="meta-llama/Meta-Llama-3.1-8B-Instruct"
-+  model="/pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct"
-   dataset_name="sonnet"
-   dataset_path="../sonnet_4x.txt"
-   num_prompts=100
-@@ -143,11 +144,11 @@ main() {
- 
-   export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')
- 
--  launch_chunked_prefill
--  for qps in 2 4 6 8; do
--  benchmark $qps $default_output_len chunked_prefill
--  done
--  kill_gpu_processes
-+  #launch_chunked_prefill
-+  #for qps in 2 4 6 8; do
-+  #benchmark $qps $default_output_len chunked_prefill
-+  #done
-+  #kill_gpu_processes
- 
-   launch_disagg_prefill
-   for qps in 2 4 6 8; do
-diff --git a/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py b/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
-index 980e68668..d067a9b5a 100644
---- a/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
-+++ b/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -783,7 +871,7 @@ if(VLLM_GPU_LANG STREQUAL "HIP")
+ endif()
  
- import os
-@@ -58,6 +59,34 @@ async def handle_request():
-         print(e)
-         print("".join(traceback.format_exception(*exc_info)))
+ # For CUDA we also build and ship some external projects.
+-if (VLLM_GPU_LANG STREQUAL "CUDA")
++if (VLLM_GPU_LANG STREQUAL "CUDA" AND NOT USE_MACA)
+     include(cmake/external_projects/flashmla.cmake)
  
-+@app.route('/metrics', methods=['GET'])
-+async def handle_request_metrics():
-+    try:
-+        original_request_data = await request.get_json()
-+
-+        generator = forward_request('http://localhost:8100/metrics',
-+                                    original_request_data)
-+        print("getting prefill metrics...")
-+        response = await make_response(generator)
-+        #if response.success:
-+        print("metrics prefill getted")
-+        # return decode
-+        generator = forward_request('http://localhost:8200/metrics',
-+                                    original_request_data)
-+        print("getting decode metrics...")
-+        response = await make_response(generator)
-+        #if response.success:
-+        print("metrics decode getted")
-+
-+        return response
+     # vllm-flash-attn should be last as it overwrites some CMake functions
+diff --git a/cmake/utils.cmake b/cmake/utils.cmake
+index 6d90555f2..79a0105e5 100644
+--- a/cmake/utils.cmake
++++ b/cmake/utils.cmake
+@@ -89,7 +89,7 @@ endfunction()
+ #
+ # Get additional GPU compiler flags from torch.
+ #
+-function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
++function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG USE_MACA)
+   if (${GPU_LANG} STREQUAL "CUDA")
+     #
+     # Get common NVCC flags from torch.
+@@ -98,6 +98,18 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
+       "from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))"
+       "Failed to determine torch nvcc compiler flags")
+ 
++    if (USE_MACA)
++      message(WARNING "Use MACA, Overwrite GPU_FLAGS.")
++      set(GPU_FLAGS 
++        "-D__CUDA_NO_HALF_OPERATORS__"
++        "-D__CUDA_NO_HALF_CONVERSIONS__"
++        "-D__CUDA_NO_HALF2_OPERATORS__"
++        "--expt-relaxed-constexpr")
++      list(APPEND GPU_FLAGS 
++        "-mllvm" 
++        "-metaxgpu-GridDim-UseLdu")
++    endif()
 +
-+    except Exception as e:
-+        import sys
-+        import traceback
-+        exc_info = sys.exc_info()
-+        print("Error occurred in disagg prefill metrics start proxy server")
-+        print(e)
-+        print("".join(traceback.format_exception(*exc_info)))
- 
- if __name__ == '__main__':
-     app.run(port=8000)
-diff --git a/benchmarks/kernels/benchmark_moe.py b/benchmarks/kernels/benchmark_moe.py
-index a4a45c9cb..990f77291 100644
---- a/benchmarks/kernels/benchmark_moe.py
-+++ b/benchmarks/kernels/benchmark_moe.py
-@@ -1,560 +1,618 @@
--# SPDX-License-Identifier: Apache-2.0
--
--import argparse
--import time
--from datetime import datetime
--from itertools import product
--from typing import Any, Dict, List, Tuple, TypedDict
--
--import ray
--import torch
--import triton
--from ray.experimental.tqdm_ray import tqdm
--from transformers import AutoConfig
--
--from vllm.model_executor.layers.fused_moe.fused_moe import *
--from vllm.platforms import current_platform
--from vllm.utils import FlexibleArgumentParser
--
--FP8_DTYPE = torch.float8_e4m3fnuz if current_platform.is_rocm(
--) else torch.float8_e4m3fn
--
--
--class BenchmarkConfig(TypedDict):
--    BLOCK_SIZE_M: int
--    BLOCK_SIZE_N: int
--    BLOCK_SIZE_K: int
--    GROUP_SIZE_M: int
--    num_warps: int
--    num_stages: int
--
--
--def benchmark_config(
--    config: BenchmarkConfig,
--    num_tokens: int,
--    num_experts: int,
--    shard_intermediate_size: int,
--    hidden_size: int,
--    topk: int,
--    dtype: torch.dtype,
--    use_fp8_w8a8: bool,
--    use_int8_w8a16: bool,
--    num_iters: int = 100,
--) -> float:
--    init_dtype = torch.float16 if use_fp8_w8a8 else dtype
--    x = torch.randn(num_tokens, hidden_size, dtype=dtype)
--    if use_int8_w8a16:
--        w1 = torch.randint(-127,
--                           127, (
--                               num_experts,
--                               shard_intermediate_size,
--                               hidden_size,
--                           ),
--                           dtype=torch.int8)
--        w2 = torch.randint(-127,
--                           127, (
--                               num_experts,
--                               hidden_size,
--                               shard_intermediate_size // 2,
--                           ),
--                           dtype=torch.int8)
--    else:
--        w1 = torch.randn(num_experts,
--                         shard_intermediate_size,
--                         hidden_size,
--                         dtype=init_dtype)
--        w2 = torch.randn(num_experts,
--                         hidden_size,
--                         shard_intermediate_size // 2,
--                         dtype=init_dtype)
--    gating_output = torch.randn(num_iters,
--                                num_tokens,
--                                num_experts,
--                                dtype=torch.float32)
--
--    w1_scale = None
--    w2_scale = None
--    a1_scale = None
--    a2_scale = None
--    if use_int8_w8a16:
--        w1_scale = torch.randn((num_experts, 2 * shard_intermediate_size),
--                               dtype=torch.float32)
--        w2_scale = torch.randn((hidden_size, num_experts), dtype=torch.float32)
--    if use_fp8_w8a8:
--        w1_scale = torch.randn(num_experts, dtype=torch.float32)
--        w2_scale = torch.randn(num_experts, dtype=torch.float32)
--        a1_scale = torch.randn(1, dtype=torch.float32)
--        a2_scale = torch.randn(1, dtype=torch.float32)
--
--        w1 = w1.to(FP8_DTYPE)
--        w2 = w2.to(FP8_DTYPE)
--
--    input_gating = torch.empty(num_tokens, num_experts, dtype=torch.float32)
--
--    def prepare(i: int):
--        input_gating.copy_(gating_output[i])
--
--    def run():
--        from vllm.model_executor.layers.fused_moe import override_config
--        with override_config(config):
--            fused_moe(
--                x,
--                w1,
--                w2,
--                input_gating,
--                topk,
--                renormalize=True,
--                inplace=True,
--                use_fp8_w8a8=use_fp8_w8a8,
--                use_int8_w8a16=use_int8_w8a16,
--                w1_scale=w1_scale,
--                w2_scale=w2_scale,
--                a1_scale=a1_scale,
--                a2_scale=a2_scale,
--            )
--
--    # JIT compilation & warmup
--    run()
--    torch.cuda.synchronize()
--
--    # Capture 10 invocations with CUDA graph
--    graph = torch.cuda.CUDAGraph()
--    with torch.cuda.graph(graph):
--        for _ in range(10):
--            run()
--    torch.cuda.synchronize()
--
--    # Warmup
--    for _ in range(5):
--        graph.replay()
--    torch.cuda.synchronize()
--
--    start_event = torch.cuda.Event(enable_timing=True)
--    end_event = torch.cuda.Event(enable_timing=True)
--
--    latencies: List[float] = []
--    for i in range(num_iters):
--        prepare(i)
--        torch.cuda.synchronize()
--
--        start_event.record()
--        graph.replay()
--        end_event.record()
--        end_event.synchronize()
--        latencies.append(start_event.elapsed_time(end_event))
--    avg = sum(latencies) / (num_iters * 10) * 1000  # us
--    graph.reset()
--    return avg
--
--
--def get_rocm_tuning_space(use_fp16):
--    block_mn_range = [16, 32, 64, 128, 256]
--    block_k_range = [16, 32, 64, 128, 256]
--    if not use_fp16:
--        block_k_range.remove(16)  # BLOCK_K=16 not supported for fp8
--    num_warps_range = [1, 2, 4, 8]
--    group_m_range = [1, 4, 8, 16, 32]
--    num_stage_range = [2]
--    waves_per_eu_range = [0]
--    matrix_instr_nonkdim_range = [16, 32] if use_fp16 else []
--    kpack_range = [1, 2] if use_fp16 else []
--
--    param_ranges = {
--        "BLOCK_SIZE_M": block_mn_range,
--        "BLOCK_SIZE_N": block_mn_range,
--        "BLOCK_SIZE_K": block_k_range,
--        "GROUP_SIZE_M": group_m_range,
--        "num_warps": num_warps_range,
--        "num_stages": num_stage_range,
--        "waves_per_eu": waves_per_eu_range,
--    }
--    if use_fp16:
--        param_ranges["matrix_instr_nonkdim"] = matrix_instr_nonkdim_range
--        param_ranges["kpack"] = kpack_range
--
--    return param_ranges
--
--
--def get_configs_compute_bound(use_fp16) -> List[Dict[str, int]]:
--    configs: List[BenchmarkConfig] = []
--
--    if current_platform.is_rocm():
--        param_ranges = get_rocm_tuning_space(use_fp16)
--    else:
--        # Reduced search space for faster tuning.
--        # TODO(woosuk): Increase the search space and use a performance model to
--        # prune the search space.
--        block_m_range = [16, 32, 64, 128, 256]
--        block_n_range = [32, 64, 128, 256]
--        block_k_range = [64, 128, 256]
--        num_warps_range = [4, 8]
--        group_m_range = [1, 16, 32, 64]
--        num_stage_range = [2, 3, 4, 5]
--
--        param_ranges = {
--            "BLOCK_SIZE_M": block_m_range,
--            "BLOCK_SIZE_N": block_n_range,
--            "BLOCK_SIZE_K": block_k_range,
--            "GROUP_SIZE_M": group_m_range,
--            "num_warps": num_warps_range,
--            "num_stages": num_stage_range,
--        }
--
--    keys, values = zip(*param_ranges.items())
--    for config_values in product(*values):
--        config = dict(zip(keys, config_values))
--        configs.append(config)
--    return configs
--
--
--def prune_rocm_search_space(num_tokens, shard_intermediate_size, hidden_size,
--                            search_space, is_fp16):
--    N1, K1 = shard_intermediate_size, hidden_size
--    N2, K2 = hidden_size, shard_intermediate_size // 2
--    pruned_space_1 = prune_rocm_configs(num_tokens * 2, N1, K1, search_space,
--                                        is_fp16)
--    pruned_space_2 = prune_rocm_configs(num_tokens * 2, N2, K2, search_space,
--                                        is_fp16)
--    search_space = merge_unique_dicts(pruned_space_1, pruned_space_2)
--    return search_space
--
--
--# The following code is inspired by ROCm/Triton GEMM tuning script:
--# https://github.com/ROCm/triton/blob/triton-mlir/scripts/amd/gemm/tune_gemm.py#L89
--def prune_rocm_configs(M, N, K, configs, is_fp16=True):
--    pruned_configs = []
--    elemBytes_a = 2 if is_fp16 else 1
--    elemBytes_b = 2 if is_fp16 else 1
--
--    mfma = 16 if M < 32 or N < 32 else 32
--
--    # TODO (zhanglx): figure out the boundary between large and small gemms
--    large_gemm = False
--    if M >= 2048 and N >= 2048:
--        large_gemm = True
--
--    for config in configs:
--        BLOCK_SIZE_M = config.get("BLOCK_SIZE_M")
--        BLOCK_SIZE_N = config.get("BLOCK_SIZE_N")
--        BLOCK_SIZE_K = config.get("BLOCK_SIZE_K")
--        num_warps = config.get("num_warps")
--
--        if is_fp16:
--            matrix_instr_nonkdim = config.get("matrix_instr_nonkdim")
--            if matrix_instr_nonkdim > mfma:
--                continue
--        if mfma == 4 and BLOCK_SIZE_K < 64:
--            continue
--        # some layouts could not work properly in case
--        # number elements per thread is less 1
--        if BLOCK_SIZE_M * BLOCK_SIZE_N < 64:
--            continue
--        SPLIT_K = config.get("SPLIT_K", 1)
--        GROUP_M = config.get("GROUP_SIZE_M")
--        if is_fp16:
--            if (matrix_instr_nonkdim > BLOCK_SIZE_M
--                    or matrix_instr_nonkdim > BLOCK_SIZE_N):
--                continue
--            if (matrix_instr_nonkdim >= M
--                    and matrix_instr_nonkdim != BLOCK_SIZE_M):
--                continue
--            if (matrix_instr_nonkdim >= N
--                    and matrix_instr_nonkdim != BLOCK_SIZE_N):
--                continue
--        # Skip BLOCK_SIZE that is too large compare to M/N
--        # unless BLOCK_SIZE is already small enough
--        if M * 2 < BLOCK_SIZE_M and BLOCK_SIZE_M != 16:
--            continue
--        if N * 2 < BLOCK_SIZE_N and BLOCK_SIZE_N != 16:
--            continue
--        # skip large split_k when not necessary
--        if SPLIT_K != 1 and not need_split_k(M, N, K):
--            continue
--        # skip split_k that leads to EVEN_K = false
--        leap = SPLIT_K * BLOCK_SIZE_K
--        modv = K % leap
--        if modv != 0:
--            continue
--        # skip large GROUP_M
--        if GROUP_M * BLOCK_SIZE_M > M and GROUP_M != 1:
--            continue
--        # out of shared memory resource
--        # TODO (zhanglx): This does not consider the LDS usage in the epilogue
--        LDS = (BLOCK_SIZE_K * BLOCK_SIZE_M * elemBytes_a +
--               BLOCK_SIZE_K * BLOCK_SIZE_N * elemBytes_b)
--        if LDS > 65536:
--            continue
--        # Skip small block sizes and num_warps for large gemm
--        # For fp16 and f8, we want to only use BLOCK_SIZE >= 64
--        if large_gemm:
--            if BLOCK_SIZE_M < 64 or BLOCK_SIZE_N < 64:
--                continue
--            if BLOCK_SIZE_K < 64:
--                continue
--            if num_warps < 4:
--                continue
--
--        pruned_configs.append(config)
--
--    return pruned_configs
--
--
--def need_split_k(SIZE_M, SIZE_N, SIZE_K):
--    return (SIZE_M < 64 or SIZE_N < 64) and SIZE_K > 1024
--
--
--def merge_unique_dicts(list1, list2):
--    result = []
--    combined_list = list1.copy()
--    combined_list.extend(list2)
--    for dictionary in combined_list:
--        if dictionary not in result:
--            result.append(dictionary)
--    return result
--
--
--@ray.remote(num_gpus=1)
--class BenchmarkWorker:
--
--    def __init__(self, seed: int) -> None:
--        torch.set_default_device("cuda")
--        current_platform.seed_everything(seed)
--        self.seed = seed
--        # Get the device ID to allocate tensors and kernels
--        # on the respective GPU. This is required for Ray to work
--        # correctly with multi-GPU tuning on the ROCm platform.
--        self.device_id = int(ray.get_gpu_ids()[0])
--
--    def benchmark(
--        self,
--        num_tokens: int,
--        num_experts: int,
--        shard_intermediate_size: int,
--        hidden_size: int,
--        topk: int,
--        dtype: torch.dtype,
--        use_fp8_w8a8: bool,
--        use_int8_w8a16: bool,
--    ) -> Tuple[Dict[str, int], float]:
--        current_platform.seed_everything(self.seed)
--        dtype_str = get_config_dtype_str(dtype,
--                                         use_int8_w8a16=use_int8_w8a16,
--                                         use_fp8_w8a8=use_fp8_w8a8)
--        # NOTE(woosuk): The current naming convention uses w2.shape[2], which
--        # is the intermediate size after silu_and_mul.
--        op_config = get_moe_configs(num_experts, shard_intermediate_size // 2,
--                                    dtype_str)
--        if op_config is None:
--            config = get_default_config(num_tokens,
--                                        num_experts,
--                                        shard_intermediate_size,
--                                        hidden_size,
--                                        topk,
--                                        dtype_str,
--                                        is_marlin=False)
--        else:
--            config = op_config[min(op_config.keys(),
--                                   key=lambda x: abs(x - num_tokens))]
--        kernel_time = benchmark_config(config, num_tokens, num_experts,
--                                       shard_intermediate_size, hidden_size,
--                                       topk, dtype, use_fp8_w8a8,
--                                       use_int8_w8a16)
--        return config, kernel_time
--
--    def tune(
--        self,
--        num_tokens: int,
--        num_experts: int,
--        shard_intermediate_size: int,
--        hidden_size: int,
--        topk: int,
--        dtype: torch.dtype,
--        use_fp8_w8a8: bool,
--        use_int8_w8a16: bool,
--        search_space: List[Dict[str, int]],
--    ) -> Dict[str, int]:
--        best_config = None
--        best_time = float("inf")
--        if current_platform.is_rocm():
--            is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16)
--            search_space = prune_rocm_search_space(num_tokens,
--                                                   shard_intermediate_size,
--                                                   hidden_size, search_space,
--                                                   is_fp16)
--
--        with torch.cuda.device(self.device_id):
--            for config in tqdm(search_space):
--                try:
--                    kernel_time = benchmark_config(config,
--                                                   num_tokens,
--                                                   num_experts,
--                                                   shard_intermediate_size,
--                                                   hidden_size,
--                                                   topk,
--                                                   dtype,
--                                                   use_fp8_w8a8,
--                                                   use_int8_w8a16,
--                                                   num_iters=20)
--                except triton.runtime.autotuner.OutOfResources:
--                    # Some configurations may be invalid and fail to compile.
--                    continue
--
--                if kernel_time < best_time:
--                    best_time = kernel_time
--                    best_config = config
--        now = datetime.now()
--        print(f"{now.ctime()}] Completed tuning for batch_size={num_tokens}")
--        assert best_config is not None
--        return best_config
--
--
--def sort_config(config: BenchmarkConfig) -> BenchmarkConfig:
--    return {
--        "BLOCK_SIZE_M":
--        config["BLOCK_SIZE_M"],
--        "BLOCK_SIZE_N":
--        config["BLOCK_SIZE_N"],
--        "BLOCK_SIZE_K":
--        config["BLOCK_SIZE_K"],
--        "GROUP_SIZE_M":
--        config["GROUP_SIZE_M"],
--        "num_warps":
--        config["num_warps"],
--        "num_stages":
--        config["num_stages"],
--        **({
--            "waves_per_eu": config["waves_per_eu"]
--        } if "waves_per_eu" in config else {}),
--        **({
--            "matrix_instr_nonkdim": config["matrix_instr_nonkdim"]
--        } if "matrix_instr_nonkdim" in config else {}),
--        **({
--            "kpack": config["kpack"]
--        } if "kpack" in config else {}),
--    }
--
--
--def save_configs(configs: Dict[int, BenchmarkConfig], num_experts: int,
--                 shard_intermediate_size: int, hidden_size: int, topk: int,
--                 dtype: torch.dtype, use_fp8_w8a8: bool,
--                 use_int8_w8a16: bool) -> None:
--    dtype_str = get_config_dtype_str(dtype,
--                                     use_int8_w8a16=use_int8_w8a16,
--                                     use_fp8_w8a8=use_fp8_w8a8)
--
--    # NOTE(woosuk): The current naming convention uses w2.shape[2], which
--    # is the intermediate size after silu_and_mul.
--    filename = get_config_file_name(num_experts, shard_intermediate_size // 2,
--                                    dtype_str)
--
--    print(f"Writing best config to {filename}...")
--    with open(filename, "w") as f:
--        json.dump(configs, f, indent=4)
--        f.write("\n")
--
--
--def main(args: argparse.Namespace):
--    print(args)
--
--    config = AutoConfig.from_pretrained(
--        args.model, trust_remote_code=args.trust_remote_code)
--    if config.architectures[0] == "DbrxForCausalLM":
--        E = config.ffn_config.moe_num_experts
--        topk = config.ffn_config.moe_top_k
--        intermediate_size = config.ffn_config.ffn_hidden_size
--        shard_intermediate_size = 2 * intermediate_size // args.tp_size
--    elif config.architectures[0] == "JambaForCausalLM":
--        E = config.num_experts
--        topk = config.num_experts_per_tok
--        intermediate_size = config.intermediate_size
--        shard_intermediate_size = 2 * intermediate_size // args.tp_size
--    elif config.architectures[0] == "DeepseekV3ForCausalLM":
--        E = config.n_routed_experts
--        topk = config.num_experts_per_tok
--        intermediate_size = config.moe_intermediate_size
--        shard_intermediate_size = 2 * intermediate_size // args.tp_size
--    else:
--        # Default: Mixtral.
--        E = config.num_local_experts
--        topk = config.num_experts_per_tok
--        intermediate_size = config.intermediate_size
--        shard_intermediate_size = 2 * intermediate_size // args.tp_size
--
--    hidden_size = config.hidden_size
--    dtype = torch.float16 if current_platform.is_rocm() else config.torch_dtype
--    use_fp8_w8a8 = args.dtype == "fp8_w8a8"
--    use_int8_w8a16 = args.dtype == "int8_w8a16"
--
--    if args.batch_size is None:
--        batch_sizes = [
--            1, 2, 4, 8, 16, 24, 32, 48, 64, 96, 128, 256, 512, 1024, 1536,
--            2048, 3072, 4096
--        ]
--    else:
--        batch_sizes = [args.batch_size]
--
--    ray.init()
--    num_gpus = int(ray.available_resources()["GPU"])
--    workers = [BenchmarkWorker.remote(args.seed) for _ in range(num_gpus)]
--
--    def _distribute(method: str, inputs: List[Any]) -> List[Any]:
--        outputs = []
--        worker_idx = 0
--        for input_args in inputs:
--            worker = workers[worker_idx]
--            worker_method = getattr(worker, method)
--            output = worker_method.remote(*input_args)
--            outputs.append(output)
--            worker_idx = (worker_idx + 1) % num_gpus
--        return ray.get(outputs)
--
--    if args.tune:
--        is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16)
--        search_space = get_configs_compute_bound(is_fp16)
--        print(f"Start tuning over {len(search_space)} configurations...")
--
--        start = time.time()
--        configs = _distribute(
--            "tune", [(batch_size, E, shard_intermediate_size, hidden_size,
--                      topk, dtype, use_fp8_w8a8, use_int8_w8a16, search_space)
--                     for batch_size in batch_sizes])
--        best_configs = {
--            M: sort_config(config)
--            for M, config in zip(batch_sizes, configs)
--        }
--        save_configs(best_configs, E, shard_intermediate_size, hidden_size,
--                     topk, dtype, use_fp8_w8a8, use_int8_w8a16)
--        end = time.time()
--        print(f"Tuning took {end - start:.2f} seconds")
--    else:
--        outputs = _distribute(
--            "benchmark", [(batch_size, E, shard_intermediate_size, hidden_size,
--                           topk, dtype, use_fp8_w8a8, use_int8_w8a16)
--                          for batch_size in batch_sizes])
--
--        for batch_size, (config, kernel_time) in zip(batch_sizes, outputs):
--            print(f"Batch size: {batch_size}, config: {config}")
--            print(f"Kernel time: {kernel_time:.2f} us")
--
--
--if __name__ == "__main__":
--    parser = FlexibleArgumentParser()
--    parser.add_argument("--model",
--                        type=str,
--                        default="mistralai/Mixtral-8x7B-Instruct-v0.1")
--    parser.add_argument("--tp-size",
--                        "-tp",
--                        "--tensor-parallel-size",
--                        type=int,
--                        default=2)
--    parser.add_argument("--dtype",
--                        type=str,
--                        choices=["auto", "fp8_w8a8", "int8_w8a16"],
--                        default="auto")
--    parser.add_argument("--seed", type=int, default=0)
--    parser.add_argument("--batch-size", type=int, required=False)
--    parser.add_argument("--tune", action="store_true")
--    parser.add_argument("--trust-remote-code", action="store_true")
--    args = parser.parse_args()
--
--    main(args)
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# SPDX-License-Identifier: Apache-2.0
-+
-+import argparse
-+import time
-+from datetime import datetime
-+from itertools import product
-+from typing import Any, Dict, List, Tuple, TypedDict
-+
-+import ray
-+import torch
-+import triton
-+from ray.experimental.tqdm_ray import tqdm
-+from transformers import AutoConfig
-+
-+from vllm.model_executor.layers.fused_moe.fused_moe import *
-+from vllm.platforms import current_platform
-+from vllm.utils import FlexibleArgumentParser
-+
-+FP8_DTYPE = torch.float8_e4m3fnuz if current_platform.is_rocm(
-+) else torch.float8_e4m3fn
-+
-+
-+class BenchmarkConfig(TypedDict):
-+    BLOCK_SIZE_M: int
-+    BLOCK_SIZE_N: int
-+    BLOCK_SIZE_K: int
-+    GROUP_SIZE_M: int
-+    num_warps: int
-+    num_stages: int
-+
-+
-+def benchmark_config(
-+    config: BenchmarkConfig,
-+    num_tokens: int,
-+    num_experts: int,
-+    shard_intermediate_size: int,
-+    hidden_size: int,
-+    topk: int,
-+    dtype: torch.dtype,
-+    use_fp8_w8a8: bool,
-+    use_int8_w8a16: bool,
-+    use_int4_w4a16: bool, 
-+    use_int8_w8a8: bool,
-+    num_iters: int = 100,
-+) -> float:
-+    init_dtype = torch.float16 if use_fp8_w8a8 else dtype
-+    x = torch.randn(num_tokens, hidden_size, dtype=dtype)
-+    if use_int8_w8a16 or use_int8_w8a8:
-+        w1 = torch.randint(-127,
-+                           127, (
-+                               num_experts,
-+                               shard_intermediate_size,
-+                               hidden_size,
-+                           ),
-+                           dtype=torch.int8)
-+        w2 = torch.randint(-127,
-+                           127, (
-+                               num_experts,
-+                               hidden_size,
-+                               shard_intermediate_size // 2,
-+                           ),
-+                           dtype=torch.int8)
-+    elif use_int4_w4a16:
-+        w1 = torch.randint(-127,
-+                           127, (
-+                               num_experts,
-+                               shard_intermediate_size,
-+                               hidden_size//2,
-+                           ),
-+                           dtype=torch.int8)
-+        w2 = torch.randint(-127,
-+                           127, (
-+                               num_experts,
-+                               hidden_size,
-+                               shard_intermediate_size // 4,
-+                           ),
-+                           dtype=torch.int8)
-+
-+    else:
-+        w1 = torch.randn(num_experts,
-+                         shard_intermediate_size,
-+                         hidden_size,
-+                         dtype=init_dtype)
-+        w2 = torch.randn(num_experts,
-+                         hidden_size,
-+                         shard_intermediate_size // 2,
-+                         dtype=init_dtype)
-+    gating_output = torch.randn(num_iters,
-+                                num_tokens,
-+                                num_experts,
-+                                dtype=torch.float32)
-+
-+    w1_scale = None
-+    w2_scale = None
-+    a1_scale = None
-+    a2_scale = None
-+    w1_zp = None
-+    w2_zp = None
-+    block_shape = None
-+
-+    if use_int8_w8a16 :
-+        w1_scale = torch.randn((num_experts, 2 * shard_intermediate_size),
-+                               dtype=torch.float32)
-+        w2_scale = torch.randn((hidden_size, num_experts), dtype=torch.float32)
-+
-+    if use_fp8_w8a8:
-+        w1_scale = torch.randn(num_experts, dtype=torch.float32)
-+        w2_scale = torch.randn(num_experts, dtype=torch.float32)
-+        a1_scale = torch.randn(1, dtype=torch.float32)
-+        a2_scale = torch.randn(1, dtype=torch.float32)
-+
-+        w1 = w1.to(FP8_DTYPE)
-+        w2 = w2.to(FP8_DTYPE)
-+
-+    if use_int4_w4a16:
-+        w1_scale = torch.randn(num_experts, shard_intermediate_size, hidden_size // 128, dtype=torch.float16)
-+        w2_scale = torch.randn((num_experts,hidden_size,shard_intermediate_size // 256,), dtype=torch.float16)
-+        w1_zp = torch.randint(-127,127, (num_experts, shard_intermediate_size//2, hidden_size // 128),dtype=torch.int8)
-+        w2_zp = torch.randint(-127,127, (num_experts,hidden_size//2,shard_intermediate_size // 256,),dtype=torch.int8)
-+        block_shape=[0, 128]
-+
-+    if use_int8_w8a8:
-+        w1_scale = torch.randn((num_experts, shard_intermediate_size, 1),dtype=torch.float32) 
-+        w2_scale = torch.randn((num_experts, hidden_size,  1), dtype=torch.float32) 
-+        
-+    input_gating = torch.empty(num_tokens, num_experts, dtype=torch.float32)
-+
-+    def prepare(i: int):
-+        input_gating.copy_(gating_output[i])
-+
-+    def run():
-+        from vllm.model_executor.layers.fused_moe import override_config
-+        with override_config(config):
-+            fused_moe(
-+                x,
-+                w1,
-+                w2,
-+                input_gating,
-+                topk,
-+                renormalize=True,
-+                inplace=True,
-+                use_fp8_w8a8=use_fp8_w8a8,
-+                use_int8_w8a16=use_int8_w8a16,
-+                use_int4_w4a16=use_int4_w4a16, 
-+                use_int8_w8a8=use_int8_w8a8,
-+                w1_scale=w1_scale,
-+                w2_scale=w2_scale,
-+                a1_scale=a1_scale,
-+                a2_scale=a2_scale,
-+                w1_zp=w1_zp, 
-+                w2_zp=w2_zp, 
-+                block_shape=block_shape
-+            )
-+
-+    # JIT compilation & warmup
-+    run()
-+    torch.cuda.synchronize()
-+
-+    # Capture 10 invocations with CUDA graph
-+    graph = torch.cuda.CUDAGraph()
-+    with torch.cuda.graph(graph):
-+        for _ in range(10):
-+            run()
-+    torch.cuda.synchronize()
-+    
-+
-+    # Warmup
-+    for _ in range(5):
-+        graph.replay()
-+    torch.cuda.synchronize()
-+
-+    start_event = torch.cuda.Event(enable_timing=True)
-+    end_event = torch.cuda.Event(enable_timing=True)
-+
-+    latencies: List[float] = []
-+    for i in range(num_iters):
-+        prepare(i)
-+        torch.cuda.synchronize()
-+
-+        start_event.record()
-+        graph.replay()
-+        end_event.record()
-+        end_event.synchronize()
-+        latencies.append(start_event.elapsed_time(end_event))
-+    avg = sum(latencies) / (num_iters * 10) * 1000  # us
-+    # graph.reset()
-+    return avg
-+
-+
-+def get_rocm_tuning_space(use_fp16):
-+    block_mn_range = [16, 32, 64, 128, 256]
-+    block_k_range = [16, 32, 64, 128, 256]
-+    if not use_fp16:
-+        block_k_range.remove(16)  # BLOCK_K=16 not supported for fp8
-+    num_warps_range = [1, 2, 4, 8]
-+    group_m_range = [1, 4, 8, 16, 32]
-+    num_stage_range = [2]
-+    waves_per_eu_range = [0]
-+    matrix_instr_nonkdim_range = [16, 32] if use_fp16 else []
-+    kpack_range = [1, 2] if use_fp16 else []
-+
-+    param_ranges = {
-+        "BLOCK_SIZE_M": block_mn_range,
-+        "BLOCK_SIZE_N": block_mn_range,
-+        "BLOCK_SIZE_K": block_k_range,
-+        "GROUP_SIZE_M": group_m_range,
-+        "num_warps": num_warps_range,
-+        "num_stages": num_stage_range,
-+        "waves_per_eu": waves_per_eu_range,
-+    }
-+    if use_fp16:
-+        param_ranges["matrix_instr_nonkdim"] = matrix_instr_nonkdim_range
-+        param_ranges["kpack"] = kpack_range
-+
-+    return param_ranges
-+
-+
-+def get_configs_compute_bound(use_fp16) -> List[Dict[str, int]]:
-+    configs: List[BenchmarkConfig] = []
-+
-+    if current_platform.is_rocm():
-+        param_ranges = get_rocm_tuning_space(use_fp16)
-+    else:
-+        # Reduced search space for faster tuning.
-+        # TODO(woosuk): Increase the search space and use a performance model to
-+        # prune the search space.
-+        block_m_range = [16, 32, 64, 128, 256]
-+        block_n_range = [32, 64, 128, 256]
-+        block_k_range = [64, 128, 256]
-+        num_warps_range = [4, 8]
-+        group_m_range = [1, 16, 32, 64]
-+        num_stage_range = [2, 3, 4, 5]
-+
-+        param_ranges = {
-+            "BLOCK_SIZE_M": block_m_range,
-+            "BLOCK_SIZE_N": block_n_range,
-+            "BLOCK_SIZE_K": block_k_range,
-+            "GROUP_SIZE_M": group_m_range,
-+            "num_warps": num_warps_range,
-+            "num_stages": num_stage_range,
-+        }
-+
-+    keys, values = zip(*param_ranges.items())
-+    for config_values in product(*values):
-+        config = dict(zip(keys, config_values))
-+        configs.append(config)
-+    return configs
-+
-+
-+def prune_rocm_search_space(num_tokens, shard_intermediate_size, hidden_size,
-+                            search_space, is_fp16):
-+    N1, K1 = shard_intermediate_size, hidden_size
-+    N2, K2 = hidden_size, shard_intermediate_size // 2
-+    pruned_space_1 = prune_rocm_configs(num_tokens * 2, N1, K1, search_space,
-+                                        is_fp16)
-+    pruned_space_2 = prune_rocm_configs(num_tokens * 2, N2, K2, search_space,
-+                                        is_fp16)
-+    search_space = merge_unique_dicts(pruned_space_1, pruned_space_2)
-+    return search_space
-+
-+
-+# The following code is inspired by ROCm/Triton GEMM tuning script:
-+# https://github.com/ROCm/triton/blob/triton-mlir/scripts/amd/gemm/tune_gemm.py#L89
-+def prune_rocm_configs(M, N, K, configs, is_fp16=True):
-+    pruned_configs = []
-+    elemBytes_a = 2 if is_fp16 else 1
-+    elemBytes_b = 2 if is_fp16 else 1
-+
-+    mfma = 16 if M < 32 or N < 32 else 32
-+
-+    # TODO (zhanglx): figure out the boundary between large and small gemms
-+    large_gemm = False
-+    if M >= 2048 and N >= 2048:
-+        large_gemm = True
-+
-+    for config in configs:
-+        BLOCK_SIZE_M = config.get("BLOCK_SIZE_M")
-+        BLOCK_SIZE_N = config.get("BLOCK_SIZE_N")
-+        BLOCK_SIZE_K = config.get("BLOCK_SIZE_K")
-+        num_warps = config.get("num_warps")
-+
-+        if is_fp16:
-+            matrix_instr_nonkdim = config.get("matrix_instr_nonkdim")
-+            if matrix_instr_nonkdim > mfma:
-+                continue
-+        if mfma == 4 and BLOCK_SIZE_K < 64:
-+            continue
-+        # some layouts could not work properly in case
-+        # number elements per thread is less 1
-+        if BLOCK_SIZE_M * BLOCK_SIZE_N < 64:
-+            continue
-+        SPLIT_K = config.get("SPLIT_K", 1)
-+        GROUP_M = config.get("GROUP_SIZE_M")
-+        if is_fp16:
-+            if (matrix_instr_nonkdim > BLOCK_SIZE_M
-+                    or matrix_instr_nonkdim > BLOCK_SIZE_N):
-+                continue
-+            if (matrix_instr_nonkdim >= M
-+                    and matrix_instr_nonkdim != BLOCK_SIZE_M):
-+                continue
-+            if (matrix_instr_nonkdim >= N
-+                    and matrix_instr_nonkdim != BLOCK_SIZE_N):
-+                continue
-+        # Skip BLOCK_SIZE that is too large compare to M/N
-+        # unless BLOCK_SIZE is already small enough
-+        if M * 2 < BLOCK_SIZE_M and BLOCK_SIZE_M != 16:
-+            continue
-+        if N * 2 < BLOCK_SIZE_N and BLOCK_SIZE_N != 16:
-+            continue
-+        # skip large split_k when not necessary
-+        if SPLIT_K != 1 and not need_split_k(M, N, K):
-+            continue
-+        # skip split_k that leads to EVEN_K = false
-+        leap = SPLIT_K * BLOCK_SIZE_K
-+        modv = K % leap
-+        if modv != 0:
-+            continue
-+        # skip large GROUP_M
-+        if GROUP_M * BLOCK_SIZE_M > M and GROUP_M != 1:
-+            continue
-+        # out of shared memory resource
-+        # TODO (zhanglx): This does not consider the LDS usage in the epilogue
-+        LDS = (BLOCK_SIZE_K * BLOCK_SIZE_M * elemBytes_a +
-+               BLOCK_SIZE_K * BLOCK_SIZE_N * elemBytes_b)
-+        if LDS > 65536:
-+            continue
-+        # Skip small block sizes and num_warps for large gemm
-+        # For fp16 and f8, we want to only use BLOCK_SIZE >= 64
-+        if large_gemm:
-+            if BLOCK_SIZE_M < 64 or BLOCK_SIZE_N < 64:
-+                continue
-+            if BLOCK_SIZE_K < 64:
-+                continue
-+            if num_warps < 4:
-+                continue
-+
-+        pruned_configs.append(config)
-+
-+    return pruned_configs
-+
-+
-+def need_split_k(SIZE_M, SIZE_N, SIZE_K):
-+    return (SIZE_M < 64 or SIZE_N < 64) and SIZE_K > 1024
-+
-+
-+def merge_unique_dicts(list1, list2):
-+    result = []
-+    combined_list = list1.copy()
-+    combined_list.extend(list2)
-+    for dictionary in combined_list:
-+        if dictionary not in result:
-+            result.append(dictionary)
-+    return result
-+
-+
-+@ray.remote(num_gpus=1)
-+class BenchmarkWorker:
-+
-+    def __init__(self, seed: int) -> None:
-+        torch.set_default_device("cuda")
-+        current_platform.seed_everything(seed)
-+        self.seed = seed
-+        # Get the device ID to allocate tensors and kernels
-+        # on the respective GPU. This is required for Ray to work
-+        # correctly with multi-GPU tuning on the ROCm platform.
-+        self.device_id = int(ray.get_gpu_ids()[0])
-+
-+    def benchmark(
-+        self,
-+        num_tokens: int,
-+        num_experts: int,
-+        shard_intermediate_size: int,
-+        hidden_size: int,
-+        topk: int,
-+        dtype: torch.dtype,
-+        use_fp8_w8a8: bool,
-+        use_int8_w8a16: bool,
-+    ) -> Tuple[Dict[str, int], float]:
-+        current_platform.seed_everything(self.seed)
-+        dtype_str = get_config_dtype_str(dtype,
-+                                         use_int8_w8a16=use_int8_w8a16,
-+                                         use_fp8_w8a8=use_fp8_w8a8)
-+        # NOTE(woosuk): The current naming convention uses w2.shape[2], which
-+        # is the intermediate size after silu_and_mul.
-+        op_config = get_moe_configs(num_experts, shard_intermediate_size // 2,
-+                                    dtype_str)
-+        if op_config is None:
-+            config = get_default_config(num_tokens,
-+                                        num_experts,
-+                                        shard_intermediate_size,
-+                                        hidden_size,
-+                                        topk,
-+                                        dtype_str,
-+                                        is_marlin=False)
-+        else:
-+            config = op_config[min(op_config.keys(),
-+                                   key=lambda x: abs(x - num_tokens))]
-+        kernel_time = benchmark_config(config, num_tokens, num_experts,
-+                                       shard_intermediate_size, hidden_size,
-+                                       topk, dtype, use_fp8_w8a8,
-+                                       use_int8_w8a16)
-+        return config, kernel_time
-+
-+    def tune(
-+        self,
-+        num_tokens: int,
-+        num_experts: int,
-+        shard_intermediate_size: int,
-+        hidden_size: int,
-+        topk: int,
-+        dtype: torch.dtype,
-+        use_fp8_w8a8: bool,
-+        use_int8_w8a16: bool,
-+        use_int4_w4a16: bool, 
-+        use_int8_w8a8: bool,
-+        search_space: List[Dict[str, int]],
-+    ) -> Dict[str, int]:
-+        best_config = None
-+        best_time = float("inf")
-+        if current_platform.is_rocm():
-+            is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16)
-+            search_space = prune_rocm_search_space(num_tokens,
-+                                                   shard_intermediate_size,
-+                                                   hidden_size, search_space,
-+                                                   is_fp16)
-+        with torch.cuda.device(self.device_id):
-+            for config in tqdm(search_space):
-+                try:
-+                    kernel_time = benchmark_config(config,
-+                                                   num_tokens,
-+                                                   num_experts,
-+                                                   shard_intermediate_size,
-+                                                   hidden_size,
-+                                                   topk,
-+                                                   dtype,
-+                                                   use_fp8_w8a8,
-+                                                   use_int8_w8a16,
-+                                                   use_int4_w4a16, 
-+                                                   use_int8_w8a8,
-+                                                   num_iters=20)
-+                except triton.runtime.autotuner.OutOfResources:
-+                    # Some configurations may be invalid and fail to compile.
-+                    continue
-+
-+                if kernel_time < best_time:
-+                    best_time = kernel_time
-+                    best_config = config
-+        now = datetime.now()
-+        print(f"{now.ctime()}] Completed tuning for batch_size={num_tokens}")
-+        assert best_config is not None
-+        return best_config
-+
-+
-+def sort_config(config: BenchmarkConfig) -> BenchmarkConfig:
-+    return {
-+        "BLOCK_SIZE_M":
-+        config["BLOCK_SIZE_M"],
-+        "BLOCK_SIZE_N":
-+        config["BLOCK_SIZE_N"],
-+        "BLOCK_SIZE_K":
-+        config["BLOCK_SIZE_K"],
-+        "GROUP_SIZE_M":
-+        config["GROUP_SIZE_M"],
-+        "num_warps":
-+        config["num_warps"],
-+        "num_stages":
-+        config["num_stages"],
-+        **({
-+            "waves_per_eu": config["waves_per_eu"]
-+        } if "waves_per_eu" in config else {}),
-+        **({
-+            "matrix_instr_nonkdim": config["matrix_instr_nonkdim"]
-+        } if "matrix_instr_nonkdim" in config else {}),
-+        **({
-+            "kpack": config["kpack"]
-+        } if "kpack" in config else {}),
-+    }
-+
-+
-+def save_configs(configs: Dict[int, BenchmarkConfig], num_experts: int,
-+                 shard_intermediate_size: int, hidden_size: int, topk: int,
-+                 dtype: torch.dtype, use_fp8_w8a8: bool,
-+                 use_int8_w8a16: bool, use_int4_w4a16: bool, use_int8_w8a8: bool) -> None:
-+    dtype_str = get_config_dtype_str(dtype,
-+                                     use_int8_w8a16=use_int8_w8a16,
-+                                     use_fp8_w8a8=use_fp8_w8a8, 
-+                                     use_int4_w4a16 = use_int4_w4a16,
-+                                     use_int8_w8a8 = use_int8_w8a8)
-+
-+    # NOTE(woosuk): The current naming convention uses w2.shape[2], which
-+    # is the intermediate size after silu_and_mul.
-+    filename = get_config_file_name(num_experts, shard_intermediate_size // 2,
-+                                    dtype_str)
-+
-+    print(f"Writing best config to {filename}...")
-+    with open(filename, "w") as f:
-+        json.dump(configs, f, indent=4)
-+        f.write("\n")
-+
-+
-+def main(args: argparse.Namespace):
-+    print(args)
-+
-+    config = AutoConfig.from_pretrained(
-+        args.model, trust_remote_code=args.trust_remote_code)
-+    if config.architectures[0] == "DbrxForCausalLM":
-+        E = config.ffn_config.moe_num_experts
-+        topk = config.ffn_config.moe_top_k
-+        intermediate_size = config.ffn_config.ffn_hidden_size
-+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-+    elif config.architectures[0] == "JambaForCausalLM":
-+        E = config.num_experts
-+        topk = config.num_experts_per_tok
-+        intermediate_size = config.intermediate_size
-+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-+    elif config.architectures[0] == "DeepseekV3ForCausalLM":
-+        E = config.n_routed_experts
-+        topk = config.num_experts_per_tok
-+        intermediate_size = config.moe_intermediate_size
-+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-+    elif config.architectures[0] in [
-+            "Qwen2MoeForCausalLM", "Qwen3MoeForCausalLM"
-+    ]:
-+        E = config.num_experts
-+        topk = config.num_experts_per_tok
-+        intermediate_size = config.moe_intermediate_size
-+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-+ 
-+    else:
-+        # Default: Mixtral.
-+        E = config.num_local_experts
-+        topk = config.num_experts_per_tok
-+        intermediate_size = config.intermediate_size
-+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-+
-+    hidden_size = config.hidden_size
-+    dtype = torch.float16 if current_platform.is_rocm() else config.torch_dtype
-+    use_fp8_w8a8 = args.dtype == "fp8_w8a8"
-+    use_int8_w8a16 = args.dtype == "int8_w8a16"
-+    use_int4_w4a16 = args.dtype == "int4_w4a16"
-+    use_int8_w8a8 = args.dtype == "int8_w8a8"
-+
-+    if args.batch_size is None:
-+        batch_sizes = [
-+            1, 2, 4, 8, 16, 24, 32, 48, 64, 96, 128, 256, 512, 1024, 1536,
-+            2048, 3072, 4096
-+        ]
-+    else:
-+        batch_sizes = [args.batch_size]
-+
-+    ray.init()
-+    num_gpus = int(ray.available_resources()["GPU"])
-+    workers = [BenchmarkWorker.remote(args.seed) for _ in range(num_gpus)]
-+
-+    def _distribute(method: str, inputs: List[Any]) -> List[Any]:
-+        outputs = []
-+        worker_idx = 0
-+        for input_args in inputs:
-+            worker = workers[worker_idx]
-+            worker_method = getattr(worker, method)
-+            output = worker_method.remote(*input_args)
-+            outputs.append(output)
-+            worker_idx = (worker_idx + 1) % num_gpus
-+        return ray.get(outputs)
-+
-+    if args.tune:
-+        is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16 or use_int4_w4a16)
-+        search_space = get_configs_compute_bound(is_fp16)
-+        print(f"Start tuning over {len(search_space)} configurations...")
-+
-+        start = time.time()
-+        configs = _distribute(
-+            "tune", [(batch_size, E, shard_intermediate_size, hidden_size,
-+                      topk, dtype, use_fp8_w8a8, use_int8_w8a16, use_int4_w4a16, use_int8_w8a8, search_space)
-+                     for batch_size in batch_sizes])
-+        best_configs = {
-+            M: sort_config(config)
-+            for M, config in zip(batch_sizes, configs)
-+        }
-+        save_configs(best_configs, E, shard_intermediate_size, hidden_size,
-+                     topk, dtype, use_fp8_w8a8, use_int8_w8a16, use_int4_w4a16, use_int8_w8a8)
-+        end = time.time()
-+        print(f"Tuning took {end - start:.2f} seconds")
-+    else:
-+        outputs = _distribute(
-+            "benchmark", [(batch_size, E, shard_intermediate_size, hidden_size,
-+                           topk, dtype, use_fp8_w8a8, use_int8_w8a16)
-+                          for batch_size in batch_sizes])
-+
-+        for batch_size, (config, kernel_time) in zip(batch_sizes, outputs):
-+            print(f"Batch size: {batch_size}, config: {config}")
-+            print(f"Kernel time: {kernel_time:.2f} us")
-+
-+
-+if __name__ == "__main__":
-+    parser = FlexibleArgumentParser()
-+    parser.add_argument("--model",
-+                        type=str,
-+                        default="mistralai/Mixtral-8x7B-Instruct-v0.1")
-+    parser.add_argument("--tp-size",
-+                        "-tp",
-+                        "--tensor-parallel-size",
-+                        type=int,
-+                        default=2)
-+    parser.add_argument("--dtype",
-+                        type=str,
-+                        choices=["auto", "fp8_w8a8", "int8_w8a16", "int4_w4a16", "int8_w8a8"],
-+                        default="auto")
-+    parser.add_argument("--seed", type=int, default=0)
-+
-+    parser.add_argument("--batch-size", type=int, required=False)
-+    parser.add_argument("--tune", action="store_true")
-+    parser.add_argument("--trust-remote-code", action="store_true")
-+    args = parser.parse_args()
-+
-+    main(args)
-+
-diff --git a/cmake/utils.cmake b/cmake/utils.cmake
-index 1c1c53981..724930ab1 100644
---- a/cmake/utils.cmake
-+++ b/cmake/utils.cmake
-@@ -94,9 +94,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
-     #
-     # Get common NVCC flags from torch.
-     #
--    run_python(GPU_FLAGS
--      "from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))"
--      "Failed to determine torch nvcc compiler flags")
-+    set(GPU_FLAGS "-D__CUDA_NO_HALF_OPERATORS__;-D__CUDA_NO_HALF_CONVERSIONS__;-D__CUDA_NO_HALF2_OPERATORS__;--expt-relaxed-constexpr")
-+    list(APPEND GPU_FLAGS -mllvm -metaxgpu-GridDim-UseLdu)
-+    #run_python(GPU_FLAGS
-+    #  "from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))"
-+    #  "Failed to determine torch nvcc compiler flags")
- 
      if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)
        list(APPEND GPU_FLAGS "-DENABLE_FP8")
-@@ -105,7 +107,6 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
+     endif()
+@@ -105,8 +117,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
        list(REMOVE_ITEM GPU_FLAGS
          "-D__CUDA_NO_HALF_OPERATORS__"
          "-D__CUDA_NO_HALF_CONVERSIONS__"
 -        "-D__CUDA_NO_BFLOAT16_CONVERSIONS__"
          "-D__CUDA_NO_HALF2_OPERATORS__")
++      if (not USE_MACA)
++        list(REMOVE_ITEM GPU_FLAGS
++        "-D__CUDA_NO_BFLOAT16_CONVERSIONS__")
++      endif()
      endif()
  
-@@ -148,6 +149,15 @@ macro(clear_cuda_arches CUDA_ARCH_FLAGS)
+   elseif(${GPU_LANG} STREQUAL "HIP")
+@@ -148,6 +163,13 @@ macro(clear_cuda_arches CUDA_ARCH_FLAGS)
      string(REGEX MATCHALL "-gencode arch=[^ ]+" CUDA_ARCH_FLAGS
        ${CMAKE_CUDA_FLAGS})
  
@@ -1691,8 +3380,6 @@ index 1c1c53981..724930ab1 100644
 +    string(REPLACE "-metaxgpu-disable-bsm-offset=1" "-metaxgpu-disable-bsm-offset=0"
 +            CMAKE_CUDA_FLAGS ${CMAKE_CUDA_FLAGS})
 +
-+    # support opt of gptq-marlin
-+    # string(APPEND CMAKE_CUDA_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128")
 +    # opt of cutlass w8a8
 +    string(APPEND CMAKE_CUDA_FLAGS " -mllvm -structurizecfg-skip-uniform-regions=true")
 +
@@ -1700,7 +3387,7 @@ index 1c1c53981..724930ab1 100644
      # and passed back via the `CUDA_ARCHITECTURES` property.
      string(REGEX REPLACE "-gencode arch=[^ ]+ *" "" CMAKE_CUDA_FLAGS
 diff --git a/csrc/activation_kernels.cu b/csrc/activation_kernels.cu
-index 88275dbdd..fc416fb78 100644
+index 55e659679..d39c87209 100644
 --- a/csrc/activation_kernels.cu
 +++ b/csrc/activation_kernels.cu
 @@ -1,3 +1,4 @@
@@ -1830,7 +3517,7 @@ index 88275dbdd..fc416fb78 100644
  }
  
  template <typename T>
-@@ -68,16 +179,68 @@ __device__ __forceinline__ T gelu_tanh_kernel(const T& x) {
+@@ -68,6 +179,54 @@ __device__ __forceinline__ T gelu_tanh_kernel(const T& x) {
  #define LAUNCH_ACTIVATION_GATE_KERNEL(KERNEL, ACT_FIRST)                 \
    int d = input.size(-1) / 2;                                            \
    int64_t num_tokens = input.numel() / input.size(-1);                   \
@@ -1884,6 +3571,8 @@ index 88275dbdd..fc416fb78 100644
 +  } else {                                                                                  \
    dim3 grid(num_tokens);                                                 \
    dim3 block(std::min(d, 1024));                                         \
+   if (num_tokens == 0) {                                                 \
+@@ -76,11 +235,15 @@ __device__ __forceinline__ T gelu_tanh_kernel(const T& x) {
    const at::cuda::OptionalCUDAGuard device_guard(device_of(input));      \
    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();          \
    VLLM_DISPATCH_FLOATING_TYPES(                                          \
@@ -1904,6953 +3593,4951 @@ index 88275dbdd..fc416fb78 100644
  
  void silu_and_mul(torch::Tensor& out,    // [..., d]
                    torch::Tensor& input)  // [..., 2 * d]
-diff --git a/csrc/attention/attention_kernels.cuh b/csrc/attention/attention_kernels.cuh
-index eb216dc8b..5033890ed 100644
---- a/csrc/attention/attention_kernels.cuh
-+++ b/csrc/attention/attention_kernels.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
-  * Adapted from
-  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
-@@ -43,6 +44,10 @@ typedef __hip_bfloat16 __nv_bfloat16;
- #define MIN(a, b) ((a) < (b) ? (a) : (b))
- #define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))
+diff --git a/csrc/attention/dtype_float16.cuh b/csrc/attention/dtype_float16.cuh
+index 3a1815f0e..05ab38d71 100644
+--- a/csrc/attention/dtype_float16.cuh
++++ b/csrc/attention/dtype_float16.cuh
+@@ -22,6 +22,7 @@
  
-+typedef __NATIVE_VECTOR__(2, float) v2f;
-+typedef __NATIVE_VECTOR__(2, _Float16) v2h;
-+typedef __NATIVE_VECTOR__(4, float) v4f;
-+
- namespace vllm {
+ #include "attention_generic.cuh"
+ #include "dtype_float32.cuh"
++#include "cuda_fp16.h"
  
- // Utility function for attention softmax.
-@@ -50,7 +55,7 @@ template <int NUM_WARPS>
- inline __device__ float block_sum(float* red_smem, float sum) {
-   // Decompose the thread index into warp / lane.
-   int warp = threadIdx.x / WARP_SIZE;
--  int lane = threadIdx.x % WARP_SIZE;
-+  int lane = threadIdx.x & (WARP_SIZE - 1);
+ #ifdef USE_ROCM
+   #include <hip/hip_fp16.h>
+@@ -69,6 +70,12 @@ struct FloatVec<uint4> {
  
-   // Compute the sum per warp.
- #pragma unroll
-@@ -81,6 +86,229 @@ inline __device__ float block_sum(float* red_smem, float sum) {
-   return VLLM_SHFL_SYNC(sum, 0);
+ // Utility functions for type conversions.
+ inline __device__ uint32_t h0_h0(uint16_t a) {
++#ifdef USE_MACA
++  uint32_t b;
++  b = a;
++  b = b << 16 | b;
++  return b;
++#else
+ #ifndef USE_ROCM
+   uint32_t b;
+   asm volatile("mov.b32 %0, {%1, %1};" : "=r"(b) : "h"(a));
+@@ -82,19 +89,35 @@ inline __device__ uint32_t h0_h0(uint16_t a) {
+   tmp.u16[1] = a;
+   return tmp.u32;
+ #endif
++#endif  // USE_MACA
  }
  
-+template<int NUM_WARPS>
-+inline __device__ float mxblock_sum(float* red_smem, float sum) {
-+  // Decompose the thread index into warp / lane.
-+  int warp = threadIdx.x >> 6;
-+  int lane = threadIdx.x & (MXWARP_SIZE - 1);
-+
-+  // Compute the sum per warp.
-+#pragma unroll
-+  for (int mask = MXWARP_SIZE / 2; mask >= 1; mask /= 2) {
-+    sum += MXVLLM_SHFL_XOR_SYNC(sum, mask);
-+  }
-+
-+  // Warp leaders store the data to shared memory.
-+  if (lane == 0) {
-+    red_smem[warp] = sum;
-+  }
-+
-+  // Make sure the data is in shared memory.
-+  __syncthreads();
-+
-+  // The warps compute the final sums.
-+  if (lane < NUM_WARPS) {
-+    sum = red_smem[lane];
-+  }
-+ // Parallel reduction inside the warp.
-+#pragma unroll
-+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
-+    sum += MXVLLM_SHFL_XOR_SYNC(sum, mask);
-+  }
-+
-+  // Broadcast to other threads.
-+  return MXVLLM_SHFL_SYNC(sum, 0);
-+}
-+template<typename scalar_t>
-+__device__  float __forceinline__ atten_mul(scalar_t *a, float b, int j) {
-+  printf("not support\n");
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_mul(uint16_t *a, float b, int j) {
-+    return __half2float(*((half*)a + j)) * __half2float(__float2half(b));
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_mul(__nv_bfloat16 *a, float b, int j) {
-+    return __bfloat162float(*(a + j)) * __bfloat162float(__float2bfloat16(b));
-+}
-+
-+template<typename scalar_t>
-+__device__  float __forceinline__ atten_mul_opt(scalar_t *a, float b, int j) {
-+  printf("not support\n");
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_mul_opt(uint16_t *a, float b, int j) {
-+    return __half2float(*((half*)a + j)) * b;
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_mul_opt(__nv_bfloat16 *a, float b, int j) {
-+    return __bfloat162float(*(a + j)) * b;
-+}
-+
-+template<typename scalar_t>
-+__device__  void __forceinline__ atten_mul_opt2(scalar_t *a, float b, int j, float &r0, float &r1) {
-+  printf("not support\n");
-+}
-+template<>
-+__device__ void __forceinline__ atten_mul_opt2(uint16_t *a, float b, int j, float &r0, float &r1) {
-+    v2f vacc; vacc[0] = r0; vacc[1] = r1;
-+    v2f vb; vb[0] = b; vb[1] = b;
-+    v2h a_2h = *(v2h*)(a + j);
-+    v2f va = __builtin_mxc_cvt_pk_f16tof32(a_2h);
-+    vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
-+    r0 = vacc[0]; r1 = vacc[1];
-+}
-+
-+template<>
-+__device__ void __forceinline__ atten_mul_opt2(__nv_bfloat16 *a, float b, int j, float &r0, float &r1) {
-+    v2f vacc; vacc[0] = r0; vacc[1] = r1;
-+    v2f vb; vb[0] = b; vb[1] = b;
-+    v2f va; va[0] = __bfloat162float(*(a + j)); va[1] = __bfloat162float(*(a + j + 1));
-+    vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
-+    r0 = vacc[0]; r1 = vacc[1];
-+}
-+
-+template<typename scalar_t, typename cache_t>
-+__device__ float __forceinline__ atten_dot(scalar_t* a, cache_t *b ,int i){
-+  printf("not support\n");
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_dot(uint16_t* a, uint16_t *b ,int i){
-+  return __half2float(*((half*)a + i)) * __half2float(*((half*)b + i));
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_dot(float* a, uint16_t *b ,int i) {
-+  return *(a + i) * __half2float(*((half*)b + i));
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_dot(__nv_bfloat16* a, __nv_bfloat16 *b ,int i){
-+  return __bfloat162float(a[i]) * __bfloat162float(b[i]);
-+}
-+
-+template<>
-+__device__ float __forceinline__ atten_dot(float *a, __nv_bfloat16* b, int i) {
-+  return *(a + i) * __bfloat162float(b[i]);
-+}
-+
-+template<typename scalar_t, typename cache_t, typename T, int Vec_size>
-+__device__ void __forceinline__ atten_dot(scalar_t &v1, cache_t &v2, T&qk) {
-+  printf("not support\n");
-+}
-+
-+template<>
-+__device__  void __forceinline__ atten_dot<Float8_, uint4,v2f, 8>(Float8_ &v1, uint4 &v2,v2f &vdst) {
-+    v2h *ptr_v2 = (v2h*)&v2;
-+    v4f* ptr_v1 = (v4f*)&v1;
-+    v4f reg_v1_0 = ptr_v1[0], reg_v1_1 = ptr_v1[1];
-+    v2f v1_2f, v2_2f;
-+    v2h v2_2h;
-+    v1_2f[0] = reg_v1_0[0];                  v1_2f[1] = reg_v1_0[1];
-+    v2_2h = ptr_v2[0];
-+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+    v1_2f[0] = reg_v1_0[2];                  v1_2f[1] = reg_v1_0[3];
-+    v2_2h = ptr_v2[1];
-+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+    v1_2f[0] = reg_v1_1[0];                  v1_2f[1] = reg_v1_1[1];
-+    v2_2h = ptr_v2[2];
-+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+    v1_2f[0] = reg_v1_1[2];                  v1_2f[1] = reg_v1_1[3];
-+    v2_2h = ptr_v2[3];
-+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+}
-+template<>
-+__device__ void __forceinline__ atten_dot<Float8_, bf16_8_t, v2f, 8>(Float8_ &v1, bf16_8_t &v2, v2f &vdst) {
-+    __nv_bfloat16 * ptr_v2 = (__nv_bfloat16*)&v2;
-+    v2f v1_2f, v2_2f;
-+    v1_2f[0] = v1.x.x;                  v1_2f[1] = v1.x.y;
-+    v2_2f[0] = __bfloat162float(ptr_v2[0]); v2_2f[1] = __bfloat162float(ptr_v2[1]);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+    v1_2f[0] = v1.y.x;                  v1_2f[1] = v1.y.y;
-+    v2_2f[0] = __bfloat162float(ptr_v2[2]); v2_2f[1] = __bfloat162float(ptr_v2[3]);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+    v1_2f[0] = v1.z.x;                  v1_2f[1] = v1.z.y;
-+    v2_2f[0] = __bfloat162float(ptr_v2[4]); v2_2f[1] = __bfloat162float(ptr_v2[5]);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+    v1_2f[0] = v1.w.x;                  v1_2f[1] = v1.w.y;
-+    v2_2f[0] = __bfloat162float(ptr_v2[6]); v2_2f[1] = __bfloat162float(ptr_v2[7]);
-+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
-+}
-+
-+template<typename T, typename Vec_T0, typename Vec_T1, int Vec_size>
-+__device__ __forceinline__ void convert(Vec_T0 & src , Vec_T1 &dst){
-+    printf("not support\n");
-+}
-+
-+template<>
-+__device__ __forceinline__ void convert<uint16_t, uint4, Float8_, 8>(uint4 & src , Float8_ &dst) {
-+  half * ptr_src = (half *)&src;
-+  dst.x.x = __half2float(ptr_src[0]);
-+  dst.x.y = __half2float(ptr_src[1]);
-+  dst.y.x = __half2float(ptr_src[2]);
-+  dst.y.y = __half2float(ptr_src[3]);
-+  dst.z.x = __half2float(ptr_src[4]);
-+  dst.z.y = __half2float(ptr_src[5]);
-+  dst.w.x = __half2float(ptr_src[6]);
-+  dst.w.y = __half2float(ptr_src[7]);
-+}
-+template<>
-+__device__ __forceinline__ void convert<__nv_bfloat16, bf16_8_t, Float8_, 8>(bf16_8_t & src , Float8_ &dst) {
-+  __nv_bfloat16 * ptr_src = (__nv_bfloat16 *)&src;
-+  dst.x.x = __bfloat162float(ptr_src[0]);
-+  dst.x.y = __bfloat162float(ptr_src[1]);
-+  dst.y.x = __bfloat162float(ptr_src[2]);
-+  dst.y.y = __bfloat162float(ptr_src[3]);
-+  dst.z.x = __bfloat162float(ptr_src[4]);
-+  dst.z.y = __bfloat162float(ptr_src[5]);
-+  dst.w.x = __bfloat162float(ptr_src[6]);
-+  dst.w.y = __bfloat162float(ptr_src[7]);
-+}
-+
-+template<typename T>
-+__device__ __forceinline__ float convert(float src){
-+  printf("not support\n");
-+}
-+
-+template<>
-+__device__ __forceinline__ float convert<uint16_t>(float src) {
-+   return __half2float(__float2half(src));
-+}
-+
-+template<>
-+__device__ __forceinline__ float convert<__nv_bfloat16>(float src) {
-+   return __bfloat162float(__float2bfloat16(src));
-+}
-+
-+template<typename scalar_t>
-+__device__ void to_v2f(scalar_t& a, scalar_t& b, v2f& vdst){
-+printf("not support\n");
-+}
-+
-+template<>
-+__device__ void to_v2f(uint16_t& a, uint16_t& b, v2f &vdst) {
-+  v2h f_d;
-+  _Float16 * ptr_a = (_Float16*)&f_d;
-+  ptr_a[0] = *(_Float16*)&a;
-+  ptr_a[1] = *(_Float16*)&b;
-+  vdst = __builtin_mxc_cvt_pk_f16tof32(f_d);
-+}
-+
-+template<>
-+__device__ void to_v2f(__nv_bfloat16& a, __nv_bfloat16& b, v2f &vdst) {
-+  vdst[0] = __bfloat162float(a);
-+  vdst[1] = __bfloat162float(b);
-+}
-+
- // TODO(woosuk): Merge the last two dimensions of the grid.
- // Grid: (num_heads, num_seqs, max_num_partitions).
- template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-@@ -111,37 +339,30 @@ __device__ void paged_attention_kernel(
-   const int seq_idx = blockIdx.y;
-   const int partition_idx = blockIdx.z;
-   const int max_num_partitions = gridDim.z;
-+  const int blockDim_x = blockDim.x;
-   constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
-   const int seq_len = seq_lens[seq_idx];
-   if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
-     // No work to do. Terminate the thread block.
-     return;
-   }
--
-   const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
--  const int num_blocks_per_partition =
--      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
-+  const int num_blocks_per_partition = USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
- 
-   // [start_block_idx, end_block_idx) is the range of blocks to process.
--  const int start_block_idx =
--      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
--  const int end_block_idx =
--      MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
-+  const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
-+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
-   const int num_blocks = end_block_idx - start_block_idx;
- 
-   // [start_token_idx, end_token_idx) is the range of tokens to process.
-   const int start_token_idx = start_block_idx * BLOCK_SIZE;
--  const int end_token_idx =
--      MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
-+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
-   const int num_tokens = end_token_idx - start_token_idx;
- 
-   constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
--  constexpr int NUM_THREAD_GROUPS =
--      NUM_THREADS / THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
--                                        // divides NUM_THREADS
-+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
-   assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
--  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
--      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
-+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
-   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
-   const int thread_idx = threadIdx.x;
-   const int warp_idx = thread_idx / WARP_SIZE;
-@@ -151,18 +372,14 @@ __device__ void paged_attention_kernel(
-   const int num_heads = gridDim.x;
-   const int num_queries_per_kv = num_heads / num_kv_heads;
-   const int kv_head_idx = head_idx / num_queries_per_kv;
--  const float alibi_slope =
--      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
--
--  // A vector type to store a part of a key or a query.
--  // The vector size is configured in such a way that the threads in a thread
--  // group fetch or compute 16 bytes at a time. For example, if the size of a
--  // thread group is 4 and the data type is half, then the vector size is 16 /
--  // (4 * sizeof(half)) == 2.
--  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
-+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
-+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
-   using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
-   using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
-+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
-+#ifdef ENABLE_FP8_E5M2
-   using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
-+#endif
+ inline __device__ float half_to_float(uint16_t h) {
+   float f;
++#ifdef USE_MACA
++  f = __half2float(*(__half*)&h);
++#else
+ #ifndef USE_ROCM
+   asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
+ #else
+   asm volatile("v_cvt_f32_f16 %0, %1;" : "=v"(f) : "v"(h));
+ #endif
++#endif // USE_MACA
+   return f;
+ }
  
-   constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
-   constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
-@@ -172,21 +389,20 @@ __device__ void paged_attention_kernel(
- 
-   // Load the query to registers.
-   // Each thread in a thread group has a different part of the query.
--  // For example, if the the thread group size is 4, then the first thread in
--  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
--  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
--  // q is split from a qkv tensor, it may not be contiguous.
-+  // For example, if the the thread group size is 4, then the first thread in the group
-+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
-+  // th vectors of the query, and so on.
-+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
-   const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
--  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
-+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
- #pragma unroll
--  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
--       i += NUM_THREAD_GROUPS) {
-+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
-+    Q_vec_l dst;
-     const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
--    q_vecs[thread_group_offset][i] =
--        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
-+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
-+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
-   }
--  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
--                    // memory wall right before we use q_vecs
-+  __syncthreads(); // TODO(naed90): possible speedup if this is replaced with a memory wall right before we use q_vecs
- 
-   // Memory planning.
-   extern __shared__ char shared_mem[];
-@@ -195,9 +411,8 @@ __device__ void paged_attention_kernel(
-   // Workspace for reduction.
-   __shared__ float red_smem[2 * NUM_WARPS];
- 
--  // x == THREAD_GROUP_SIZE * VEC_SIZE
-   // Each thread group fetches x elements from the key at a time.
--  constexpr int x = 16 / sizeof(cache_t);
-+  constexpr int x = 16;
-   float qk_max = -FLT_MAX;
- 
-   // Iterate over the key blocks.
-@@ -224,13 +439,96 @@ __device__ void paged_attention_kernel(
-                         1;
-   }
+ inline __device__ float2 half2_to_float2(uint32_t v) {
++#ifdef USE_MACA
++  uint16_t lo, hi;
++  union {
++    uint32_t u32;
++    uint16_t u16[2];
++  } tmp;
++  tmp.u32 = v;
++  lo = tmp.u16[0];
++  hi = tmp.u16[1];
++  return make_float2(half_to_float(lo), half_to_float(hi));
++#else
+ #ifndef USE_ROCM
+   uint16_t lo, hi;
+   asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo), "=h"(hi) : "r"(v));
+@@ -110,6 +133,7 @@ inline __device__ float2 half2_to_float2(uint32_t v) {
+   ret.y = half_to_float(tmp.u16[1]);
+   return ret;
+ #endif
++#endif // USE_MACA
+ }
  
--  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
--       block_idx += NUM_WARPS) {
--    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
--    // int64 because int32 can lead to overflow when this variable is multiplied
--    // by large numbers (e.g., kv_block_stride).
--    // For blocksparse attention: skip computation on blocks that are not
--    // attended
-+  int block_idx0 = start_block_idx + warp_idx;
-+  int kv_offset0, kv_offset1;
-+  K_vec load_k[NUM_VECS_PER_THREAD];
-+  K_vec compute_k[NUM_VECS_PER_THREAD];
-+  
-+  int k_offset[NUM_VECS_PER_THREAD];
-+  kv_offset0 = block_table[block_idx0];
-+  if(block_idx0 + NUM_WARPS < end_block_idx) {
-+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
-+  }
-+  #pragma unroll
-+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
-+    const int offset1 = vec_idx >> 4;
-+    const int offset2 = vec_idx & 15;
-+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
-+  }
-+  if constexpr (IS_BLOCK_SPARSE) {
-+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
-+      const bool is_remote =
-+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+      const bool is_local =
-+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+      if(is_remote || is_local) {
-+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+            const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
-+            const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
-+            const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
-+                                              + kv_head_idx * kv_head_stride
-+                                              + physical_block_offset * x;
-+            
-+        #pragma unroll
-+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+            }
-+          }    
-+      } 
-+  } else {
-+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
-+      const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
-+      const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
-+                                        + kv_head_idx * kv_head_stride
-+                                        + physical_block_offset * x;
-+      
-+  #pragma unroll
-+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+      }
-+    }
-+  } 
-+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
-+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
-+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+      #pragma unroll
-+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        compute_k[j] = load_k[j];
-+      }
-+      if(block_idx < end_block_idx - NUM_WARPS) {
-+          kv_offset0 = kv_offset1;
-+          int nblock_idx = block_idx + NUM_WARPS;
-+          if(block_idx < end_block_idx - (NUM_WARPS << 1)) {
-+            kv_offset1 = block_table[block_idx + (NUM_WARPS<<1)];
-+          }
-+          if constexpr (IS_BLOCK_SPARSE) {
-+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
-+            const bool is_remote =
-+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+            const bool is_local =
-+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+            if(is_remote || is_local) {
-+              const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
-+                                      + kv_head_idx * kv_head_stride
-+                                      + physical_block_offset * x;
-+              #pragma unroll NUM_VECS_PER_THREAD
-+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+              }  
-+            }
-+          } else {
-+            const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
-+                                      + kv_head_idx * kv_head_stride
-+                                      + physical_block_offset * x;
-+            #pragma unroll NUM_VECS_PER_THREAD
-+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+            }
-+          } 
-+    }
-     if constexpr (IS_BLOCK_SPARSE) {
-       const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-       const bool is_remote =
-@@ -254,48 +552,26 @@ __device__ void paged_attention_kernel(
-         continue;
-       }
-     }
--    const int64_t physical_block_number =
--        static_cast<int64_t>(block_table[block_idx]);
--
--    // Load a key to registers.
--    // Each thread in a thread group has a different part of the key.
--    // For example, if the the thread group size is 4, then the first thread in
--    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
--    // has 1, 5, 9, ... th vectors of the key, and so on.
--    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
--      const int physical_block_offset =
--          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
--      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
--      K_vec k_vecs[NUM_VECS_PER_THREAD];
--
--#pragma unroll
--      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
--        const cache_t* k_ptr =
--            k_cache + physical_block_number * kv_block_stride +
--            kv_head_idx * kv_head_stride + physical_block_offset * x;
--        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
--        const int offset1 = (vec_idx * VEC_SIZE) / x;
--        const int offset2 = (vec_idx * VEC_SIZE) % x;
--
--        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
--          k_vecs[j] = *reinterpret_cast<const K_vec*>(
--              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
--        } else {
--          // Vector conversion from Quant_vec to K_vec.
--          Quant_vec k_vec_quant = *reinterpret_cast<const Quant_vec*>(
--              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
--          k_vecs[j] = fp8::scaled_convert<K_vec, Quant_vec, KV_DTYPE>(
--              k_vec_quant, *k_scale);
--        }
--      }
- 
-       // Compute dot product.
-       // This includes a reduction across the threads in the same thread group.
--      float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
--                             q_vecs[thread_group_offset], k_vecs);
-+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
-+      float qk = 0.0f;
-+      v2f f2_qk = {0,0};
-+      #pragma unroll
-+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+	atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
-+      }
-+      qk = f2_qk[0] + f2_qk[1];
-+  
-+      #pragma unroll
-+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
-+        qk += VLLM_SHFL_XOR_SYNC(qk, mask);
-+      }
-+      qk = scale * qk;
-       // Add the ALiBi bias if slopes are given.
-       qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
--
-+      
-       if (thread_group_offset == 0) {
-         // Store the partial reductions to shared memory.
-         // NOTE(woosuk): It is required to zero out the masked logits.
-@@ -318,7 +594,6 @@ __device__ void paged_attention_kernel(
-     red_smem[warp_idx] = qk_max;
-   }
-   __syncthreads();
--
-   // TODO(woosuk): Refactor this part.
-   // Get the max qk value for the sequence.
-   qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
-@@ -332,14 +607,14 @@ __device__ void paged_attention_kernel(
-   // Get the sum of the exp values.
-   float exp_sum = 0.f;
-   for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
--    float val = __expf(logits[i] - qk_max);
-+    float val = __builtin_expf(logits[i] - qk_max);
-     logits[i] = val;
-     exp_sum += val;
-   }
-   exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
+ inline __device__ uint16_t float_to_half(float f) {
+@@ -117,11 +141,16 @@ inline __device__ uint16_t float_to_half(float f) {
+     uint32_t u32;
+     uint16_t u16[2];
+   } tmp;
++#ifdef USE_MACA
++  __half __tmp = __float2half(f);
++  tmp.u16[0] = *(uint16_t*)&__tmp;
++#else
+ #ifndef USE_ROCM
+   asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f));
+ #else
+   asm volatile("v_cvt_f16_f32 %0, %1;\n" : "=v"(tmp.u32) : "v"(f));
+ #endif
++#endif // USE MACA
+   return tmp.u16[0];
+ }
  
-   // Compute softmax.
--  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
-+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
-   for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-     logits[i] *= inv_sum;
-   }
-@@ -347,201 +622,216 @@ __device__ void paged_attention_kernel(
- 
-   // If partitioning is enabled, store the max logit and exp_sum.
-   if (USE_PARTITIONING && thread_idx == 0) {
--    float* max_logits_ptr = max_logits +
--                            seq_idx * num_heads * max_num_partitions +
--                            head_idx * max_num_partitions + partition_idx;
-+    float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
-+                                       + head_idx * max_num_partitions
-+                                       + partition_idx;
-     *max_logits_ptr = qk_max;
--    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
--                          head_idx * max_num_partitions + partition_idx;
-+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
-+                                   + head_idx * max_num_partitions
-+                                   + partition_idx;
-     *exp_sums_ptr = exp_sum;
-   }
--
--  // Each thread will fetch 16 bytes from the value cache at a time.
--  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
-+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
-+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
-+  constexpr int NUM_COLS_PER_ITER = MAX(WARP_SIZE / NUM_V_VECS_PER_THREAD,1);
-+  constexpr int NUM_VALID_THREAD = NUM_COLS_PER_ITER * NUM_V_VECS_PER_THREAD;
-+  constexpr int NUM_LGT_PER_COL = (BLOCK_SIZE + NUM_COLS_PER_ITER - 1) / NUM_COLS_PER_ITER;
-+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
-   using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-   using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
--  using V_quant_vec = typename Vec<cache_t, V_VEC_SIZE>::Type;
-   using Float_L_vec = typename FloatVec<L_vec>::Type;
--
--  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
--  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
--  constexpr int NUM_ROWS_PER_THREAD =
--      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
--
--  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
--  float accs[NUM_ROWS_PER_THREAD];
--#pragma unroll
--  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
--    accs[i] = 0.f;
--  }
--
--  scalar_t zero_value;
--  zero(zero_value);
--  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
--       block_idx += NUM_WARPS) {
--    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
--    // int64 because int32 can lead to overflow when this variable is multiplied
--    // by large numbers (e.g., kv_block_stride).
--    // For blocksparse attention: skip computation on blocks that are not
--    // attended
-+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
-+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
-+  V_vec v_vecs[NUM_LGT_PER_COL];
-+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
-+  float accs[V_VEC_SIZE];
-+  float reg_log[NUM_LGT_PER_COL];
-+  float reg_prev_log[NUM_LGT_PER_COL];
-+  #pragma unroll
-+  for(int i = 0; i < V_VEC_SIZE; i++) {
-+    accs[i] = 0.0f;
-+  }
-+  int token_idx, kv_stride, block_offset;
-+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
-+  kv_offset0 = block_table[block_idx0];
-+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
-+  if(block_idx0 + NUM_WARPS < end_block_idx) {
-+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
-+  }
-+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
-+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE; 
-+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
-+  float *ptr_logits = logits + token_idx - start_token_idx;
-+  if(lane < NUM_VALID_THREAD) {
-     if constexpr (IS_BLOCK_SPARSE) {
--      int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
--      if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
--          !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
--        continue;
--      }
-+          int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
-+          if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
-+              ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+              if(block_idx0 == num_seq_blocks - 1) {
-+              #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                  if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                  }
-+                }
-+              } else {
-+                #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                  if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                  }
-+                }
-+              }
-+          }
-     }
--    const int64_t physical_block_number =
--        static_cast<int64_t>(block_table[block_idx]);
--    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
--    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
--    L_vec logits_vec;
--    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
--                                                           start_token_idx));
--
--    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
--                           kv_head_idx * kv_head_stride;
--#pragma unroll
--    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
--      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
--      if (row_idx < HEAD_SIZE) {
--        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
--        V_vec v_vec;
--
--        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
--          v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
-+    else {
-+      if(block_idx0 == num_seq_blocks - 1) {
-+        #pragma unroll
-+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+	    if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+            }
-+          }
-         } else {
--          V_quant_vec v_quant_vec =
--              *reinterpret_cast<const V_quant_vec*>(v_ptr + offset);
--          // Vector conversion from V_quant_vec to V_vec.
--          v_vec = fp8::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec,
--                                                                    *v_scale);
--        }
--        if (block_idx == num_seq_blocks - 1) {
--          // NOTE(woosuk): When v_vec contains the tokens that are out of the
--          // context, we should explicitly zero out the values since they may
--          // contain NaNs. See
--          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
--          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
--#pragma unroll
--          for (int j = 0; j < V_VEC_SIZE; j++) {
--            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;
-+          #pragma unroll
-+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+	    if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+            }
-           }
-         }
--        accs[i] += dot(logits_vec, v_vec);
--      }
-     }
--  }
--
--  // Perform reduction within each warp.
--#pragma unroll
--  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
--    float acc = accs[i];
--#pragma unroll
--    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
--      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
-+    
-+    for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
-+        int next_block = block_idx + NUM_WARPS;
-+        int nnext_block = next_block + NUM_WARPS;
-+        for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+            v_vecs[i] = v_prev_vecs[i];
-+            reg_log[i] = reg_prev_log[i];
-+        }
-+        if(next_block < end_block_idx) {
-+            kv_offset0 = kv_offset1;
-+            if(nnext_block < end_block_idx) {
-+            kv_offset1 = block_table[nnext_block];
-+            }
-+            token_idx = next_block * BLOCK_SIZE + physical_block_offset;
-+            const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
-+            ptr_logits = logits + token_idx - start_token_idx;
-+            if constexpr (IS_BLOCK_SPARSE) {
-+            int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
-+            if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
-+                ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+                    if(next_block == num_seq_blocks - 1) {
-+                    #pragma unroll
-+                    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
-+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                    }
-+                    }
-+                } else {
-+                    #pragma unroll
-+                    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
-+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                    }
-+                    }
-+                }
-+            }
-+            } else {
-+            if(next_block == num_seq_blocks - 1) {
-+                #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
-+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                }
-+                }
-+            } else {
-+                #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
-+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                }
-+                }
-+            }
-+            }
-+        }
-+        
-+      if constexpr (IS_BLOCK_SPARSE) {
-+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
-+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+          continue;
-+        }
-+      }
-+      
-+      token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+      for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+        if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
-+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
-+	  #pragma unroll
-+          for(int j = 0; j < V_VEC_SIZE; j+=2) {
-+            atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
-+          }
-+        }
-+      } 
-     }
--    accs[i] = acc;
-   }
--
--  // NOTE(woosuk): A barrier is required because the shared memory space for
--  // logits is reused for the output.
-   __syncthreads();
+@@ -130,6 +159,15 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
+     uint32_t u32;
+     uint16_t u16[2];
+   } tmp;
++#ifdef USE_MACA
++  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
++  __half2 __tmp = __half2(__float2half(f.x), __float2half(f.y));
++  tmp.u32 = *(uint32_t*)&__tmp;
++  #else
++  tmp.u16[0] = float_to_half(f.x);
++  tmp.u16[1] = float_to_half(f.y);
++  #endif
++#else
+ #ifndef USE_ROCM
+   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
+   asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n"
+@@ -143,27 +181,42 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
+   tmp.u16[0] = float_to_half(f.x);
+   tmp.u16[1] = float_to_half(f.y);
+ #endif
++#endif // USE_MACA
+   return tmp.u32;
+ }
+ 
+ // Vector addition.
+ inline __device__ uint16_t add(uint16_t a, uint16_t b) {
+   uint16_t c;
++#ifdef USE_MACA
++  unsigned short __a=(a);
++  unsigned short __b=(b);
++  __half __d=__hadd(*(__half*)&__a,*(__half*)&__b);
++  (c)=*(unsigned short*)&__d;
++#else
+ #ifndef USE_ROCM
+   asm volatile("add.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
+ #else
+   asm volatile("v_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+ #endif
++#endif // USE_MACA
+   return c;
+ }
+ 
+ inline __device__ uint32_t add(uint32_t a, uint32_t b) {
+   uint32_t c;
++#ifdef USE_MACA
++  unsigned int __a=(a);
++  unsigned int __b=(b);
++  __half2 __d=__hadd2(*(__half2*)&__a,*(__half2*)&__b);
++  (c)=*(unsigned int*)&__d;
++#else
+ #ifndef USE_ROCM
+   asm volatile("add.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
+ #else
+   asm volatile("v_pk_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+ #endif
++#endif // USE_MACA
+   return c;
+ }
+ 
+@@ -208,22 +261,36 @@ inline __device__ Float8_ add(uint4 a, Float8_ fb) {
+ template <>
+ inline __device__ uint16_t mul(uint16_t a, uint16_t b) {
+   uint16_t c;
++#ifdef USE_MACA
++  unsigned short __a=(a);
++  unsigned short __b=(b);
++  __half __d=__hmul(*(__half*)&__a,*(__half*)&__b);
++  (c)=*(unsigned short*)&__d;
++#else
+ #ifndef USE_ROCM
+   asm volatile("mul.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
+ #else
+   asm volatile("v_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+ #endif
++#endif // USE_MACA
+   return c;
+ }
+ 
+ template <>
+ inline __device__ uint32_t mul(uint32_t a, uint32_t b) {
+   uint32_t c;
++#ifdef USE_MACA
++  unsigned int __a=(a);
++  unsigned int __b=(b);
++  __half2 __d=__hmul2(*(__half2*)&__a,*(__half2*)&__b);
++  (c)=*(unsigned int*)&__d;
++#else
+ #ifndef USE_ROCM
+   asm volatile("mul.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
+ #else
+   asm volatile("v_pk_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+ #endif
++#endif //USE_MACA
+   return c;
+ }
+ 
+@@ -330,6 +397,13 @@ inline __device__ Float8_ mul(uint16_t a, uint4 b) {
+ // Vector fused multiply-add.
+ inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {
+   uint32_t d;
++#ifdef USE_MACA
++  unsigned int __a=(a);
++  unsigned int __b=(b);
++  unsigned int __c=(c);
++  __half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
++  (d)=*(unsigned int*)&__d;
++#else
+ #ifndef USE_ROCM
+   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
+                : "=r"(d)
+@@ -339,6 +413,7 @@ inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {
+                : "=v"(d)
+                : "v"(a), "v"(b), "v"(c));
+ #endif
++#endif // USE_MACA
+   return d;
+ }
+ 
+diff --git a/csrc/cumem_allocator.cpp b/csrc/cumem_allocator.cpp
+index fab6ca36d..41015355d 100644
+--- a/csrc/cumem_allocator.cpp
++++ b/csrc/cumem_allocator.cpp
+@@ -3,8 +3,6 @@
+ // need to be unsigned long long
+ #include <iostream>
+ 
+-extern "C" {
 -
--  // Perform reduction across warps.
-+  //need move
-   float* out_smem = reinterpret_cast<float*>(shared_mem);
--#pragma unroll
--  for (int i = NUM_WARPS; i > 1; i /= 2) {
--    int mid = i / 2;
--    // Upper warps write to shared memory.
--    if (warp_idx >= mid && warp_idx < i) {
--      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
--#pragma unroll
--      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
--        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
--        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
--          dst[row_idx] = accs[i];
--        }
--      }
--    }
--    __syncthreads();
-+  for(int i = threadIdx.x; i < NUM_WARPS * NUM_COLS_PER_ITER * HEAD_SIZE; i += blockDim_x) {
-+    out_smem[i] = 0.0f;
-+  }
-+  __syncthreads();
+ #define PY_SSIZE_T_CLEAN
+ #include <Python.h>
  
--    // Lower warps update the output.
--    if (warp_idx < mid) {
--      const float* src = &out_smem[warp_idx * HEAD_SIZE];
--#pragma unroll
--      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
--        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
--        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
--          accs[i] += src[row_idx];
--        }
--      }
-+  if(lane < NUM_VALID_THREAD) {
-+    float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
-+    for(int i = 0; i < V_VEC_SIZE; i++) {
-+      ptr_out_smem[i] = accs[i];
-     }
--    __syncthreads();
+@@ -12,6 +10,8 @@ extern "C" {
+ #include <cuda_runtime_api.h>
+ #include <cuda.h>
+ 
++extern "C" {
++
+ char error_msg[10240];  // 10KB buffer to store error messages
+ CUresult no_error = CUresult(0);
+ CUresult error_code = no_error;  // store error code
+diff --git a/csrc/custom_all_reduce.cuh b/csrc/custom_all_reduce.cuh
+index 44709b459..18c38855f 100644
+--- a/csrc/custom_all_reduce.cuh
++++ b/csrc/custom_all_reduce.cuh
+@@ -56,9 +56,15 @@ struct Signal {
+   alignas(128) FlagType _flag[kMaxBlocks];  // incremental flags for each rank
+ };
+ 
++#ifdef USE_MACA
++struct __align__(16) RankData { 
++  const void* ptrs[8]; 
++};
++#else
+ struct __align__(16) RankData {
+   const void* ptrs[8];
+ };
++#endif // USE_MACA
+ 
+ struct __align__(16) RankSignals {
+   Signal* signals[8];
+@@ -155,6 +161,7 @@ DINLINE O downcast(array_t<float, O::size> val) {
+ #if !defined(USE_ROCM)
+ 
+ static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
++  #ifndef USE_MACA
+   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
+   asm volatile("st.release.sys.global.u32 [%1], %0;" ::"r"(flag),
+                "l"(flag_addr));
+@@ -162,10 +169,12 @@ static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
+   asm volatile("membar.sys; st.volatile.global.u32 [%1], %0;" ::"r"(flag),
+                "l"(flag_addr));
+   #endif
++  #endif // USE_MACA
+ }
+ 
+ static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
+   FlagType flag;
++  #ifndef USE_MACA
+   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
+   asm volatile("ld.acquire.sys.global.u32 %0, [%1];"
+                : "=r"(flag)
+@@ -175,18 +184,23 @@ static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
+                : "=r"(flag)
+                : "l"(flag_addr));
+   #endif
++  #endif // USE_MACA
+   return flag;
+ }
+ 
+ static DINLINE void st_flag_volatile(FlagType* flag_addr, FlagType flag) {
++#ifndef USE_MACA
+   asm volatile("st.volatile.global.u32 [%1], %0;" ::"r"(flag), "l"(flag_addr));
++#endif // USE_MACA
+ }
+ 
+ static DINLINE FlagType ld_flag_volatile(FlagType* flag_addr) {
+   FlagType flag;
++#ifndef USE_MACA
+   asm volatile("ld.volatile.global.u32 %0, [%1];"
+                : "=r"(flag)
+                : "l"(flag_addr));
++#endif // USE_MACA
+   return flag;
+ }
+ 
+@@ -365,7 +379,9 @@ __global__ void __launch_bounds__(512, 1)
+ 
+ using IPC_KEY = std::array<uint8_t, sizeof(cudaIpcMemHandle_t)>;
+ static_assert(sizeof(IPC_KEY) == sizeof(cudaIpcMemHandle_t));
++#ifndef USE_MACA
+ static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));
++#endif // USE_MACA
+ 
+ class CustomAllreduce {
+  public:
+diff --git a/csrc/cutlass_extensions/common.hpp b/csrc/cutlass_extensions/common.hpp
+index 195872e8e..858cecd2c 100644
+--- a/csrc/cutlass_extensions/common.hpp
++++ b/csrc/cutlass_extensions/common.hpp
+@@ -1,6 +1,10 @@
+ #pragma once
+ 
++#ifndef USE_MACA
+ #include "cutlass/cutlass.h"
++#else
++#include "mctlass/mctlass.h"
++#endif // USE_MACA
+ #include <climits>
+ #include "cuda_runtime.h"
+ #include <iostream>
+@@ -8,12 +12,21 @@
+ /**
+  * Helper function for checking CUTLASS errors
+  */
++#ifndef USE_MACA
+ #define CUTLASS_CHECK(status)                       \
+   {                                                 \
+     cutlass::Status error = status;                 \
+     TORCH_CHECK(error == cutlass::Status::kSuccess, \
+                 cutlassGetStatusString(error));     \
    }
++#else
++#define CUTLASS_CHECK(status)                       \
++  {                                                 \
++    mctlass::Status error = status;                 \
++    TORCH_CHECK(error == mctlass::Status::kSuccess, \
++                mctlassGetStatusString(error));     \
++  }
++#endif // USE_MACA
+ 
+ inline int get_cuda_max_shared_memory_per_block_opt_in(int const device) {
+   int max_shared_mem_per_block_opt_in = 0;
+@@ -34,7 +47,11 @@ int32_t get_sm_version_num();
+ template <typename Kernel>
+ struct enable_sm90_or_later : Kernel {
+   template <typename... Args>
++#ifndef USE_MACA
+   CUTLASS_DEVICE void operator()(Args&&... args) {
++#else
++  MCTLASS_DEVICE void operator()(Args&&... args) {
++#endif // USE_MACA
+ #if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 900
+     Kernel::operator()(std::forward<Args>(args)...);
+ #endif
+@@ -44,7 +61,11 @@ struct enable_sm90_or_later : Kernel {
+ template <typename Kernel>
+ struct enable_sm90_only : Kernel {
+   template <typename... Args>
++#ifndef USE_MACA
+   CUTLASS_DEVICE void operator()(Args&&... args) {
++#else
++  MCTLASS_DEVICE void operator()(Args&&... args) {
++#endif // USE_MACA
+ #if defined __CUDA_ARCH__ && __CUDA_ARCH__ == 900
+     Kernel::operator()(std::forward<Args>(args)...);
+ #endif
+@@ -54,7 +75,11 @@ struct enable_sm90_only : Kernel {
+ template <typename Kernel>
+ struct enable_sm100_only : Kernel {
+   template <typename... Args>
++#ifndef USE_MACA
+   CUTLASS_DEVICE void operator()(Args&&... args) {
++#else
++  MCTLASS_DEVICE void operator()(Args&&... args) {
++#endif // USE_MACA
+ #if defined __CUDA_ARCH__ && __CUDA_ARCH__ == 1000
+     Kernel::operator()(std::forward<Args>(args)...);
+ #endif
+diff --git a/csrc/mamba/causal_conv1d/causal_conv1d.cu b/csrc/mamba/causal_conv1d/causal_conv1d.cu
+index f62d08c17..b1f15930e 100644
+--- a/csrc/mamba/causal_conv1d/causal_conv1d.cu
++++ b/csrc/mamba/causal_conv1d/causal_conv1d.cu
+@@ -187,7 +187,7 @@ void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
+ 
+     // Otherwise the kernel will be launched from cuda:0 device
+     // Cast to char to avoid compiler warning about narrowing
+-    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
++    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
+     auto stream = at::cuda::getCurrentCUDAStream().stream();
+     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_fwd", [&] {
+             causal_conv1d_fwd_cuda<input_t, weight_t>(params, stream);
+@@ -280,7 +280,7 @@ void causal_conv1d_update(const at::Tensor &x,
+ 
+     // Otherwise the kernel will be launched from cuda:0 device
+     // Cast to char to avoid compiler warning about narrowing
+-    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
++    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
+     auto stream = at::cuda::getCurrentCUDAStream().stream();
+     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_update", [&] {
+             causal_conv1d_update_cuda<input_t, weight_t>(params, stream);
+diff --git a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
+index 0c9df925b..093f7f284 100644
+--- a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
++++ b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
+@@ -331,7 +331,17 @@ void selective_scan_fwd_launch(SSMParamsBase &params, cudaStream_t stream) {
+ 
+ template<typename input_t, typename weight_t>
+ void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {
 -
--  // Write the final output.
--  if (warp_idx == 0) {
--    scalar_t* out_ptr =
--        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
--        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
--#pragma unroll
--    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
--      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
--      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
--        from_float(*(out_ptr + row_idx), accs[i]);
-+  __syncthreads();
-+  scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
-+                        + head_idx * max_num_partitions * HEAD_SIZE
-+                        + partition_idx * HEAD_SIZE;
-+  if(threadIdx.x < HEAD_SIZE) {
-+    int length = NUM_LANE * HEAD_SIZE;
-+      float r = 0;
-+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
-+        r += out_smem[i];
-       }
--    }
-+      from_float(*(out_ptr + threadIdx.x), r);
++    #ifdef USE_MACA
++        if (params.seqlen <= 256) {
++            selective_scan_fwd_launch<64, 4, input_t, weight_t>(params, stream);
++        } else if (params.seqlen <= 512) {
++            selective_scan_fwd_launch<64, 8, input_t, weight_t>(params, stream);
++        } else if (params.seqlen <= 1024) {
++            selective_scan_fwd_launch<64, 16, input_t, weight_t>(params, stream);
++        } else {
++            selective_scan_fwd_launch<128, 16, input_t, weight_t>(params, stream);
++        }
++    #else
+     #ifndef USE_ROCM
+         if (params.seqlen <= 128) {           
+             selective_scan_fwd_launch<32, 4, input_t, weight_t>(params, stream);
+@@ -355,6 +365,7 @@ void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {
+             selective_scan_fwd_launch<128, 16, input_t, weight_t>(params, stream);
+         }
+     #endif
++    #endif // USE_MACA
+ }
+ 
+ template void selective_scan_fwd_cuda<at::BFloat16, float>(SSMParamsBase &params, cudaStream_t stream);
+@@ -649,7 +660,7 @@ void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,
+     
+     // Otherwise the kernel will be launched from cuda:0 device
+     // Cast to char to avoid compiler warning about narrowing
+-    at::cuda::CUDAGuard device_guard{(char)u.get_device()};
++    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(u.get_device())};
+     auto stream = at::cuda::getCurrentCUDAStream().stream();
+     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(u.scalar_type(), "selective_scan_fwd", [&] {
+         selective_scan_fwd_cuda<input_t, weight_t>(params, stream);
+diff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu
+index 6b6a9d04a..80fb62430 100644
+--- a/csrc/moe/moe_align_sum_kernels.cu
++++ b/csrc/moe/moe_align_sum_kernels.cu
+@@ -197,6 +197,240 @@ __global__ void moe_align_block_size_global_mem_kernel(
    }
  }
  
--// Grid: (num_heads, num_seqs, 1).
--template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
--          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
--          bool IS_BLOCK_SPARSE>
--__global__ void paged_attention_v1_kernel(
--    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
--    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
--    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
--                                          // head_size/x, block_size, x]
--    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
--                                          // head_size, block_size]
--    const int num_kv_heads,               // [num_heads]
--    const float scale,
--    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
--    const int* __restrict__ seq_lens,      // [num_seqs]
--    const int max_num_blocks_per_seq,
--    const float* __restrict__ alibi_slopes,  // [num_heads]
--    const int q_stride, const int kv_block_stride, const int kv_head_stride,
--    const float* k_scale, const float* v_scale, const int tp_rank,
--    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
--    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
--  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
--                         KV_DTYPE, IS_BLOCK_SPARSE>(
--      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
--      v_cache, num_kv_heads, scale, block_tables, seq_lens,
--      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
--      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
--      blocksparse_vert_stride, blocksparse_block_size,
--      blocksparse_head_sliding_step);
--}
--
--// Grid: (num_heads, num_seqs, max_num_partitions).
- template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-           int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-           bool IS_BLOCK_SPARSE,
--          int PARTITION_SIZE>
--__global__ void paged_attention_v2_kernel(
-+          int PARTITION_SIZE = 0>  // Zero means no partitioning.
-+__device__ __forceinline__ void paged_attention_kernel_32N(
-     float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
--    float* __restrict__ max_logits,       // [num_seqs, num_heads,
--                                          // max_num_partitions]
--    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
--                                          // max_num_partitions, head_size]
-+    float* __restrict__ max_logits,  // [num_seqs, num_heads,
-+                                     // max_num_partitions]
-+    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,head_size]
-     const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
--    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
--                                          // head_size/x, block_size, x]
--    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
--                                          // head_size, block_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads, head_size/x, block_size, x]->[num_blocks, num_kv_heads, head_size/16, block_size, 16]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads, head_size, block_size]->[num_blocks, num_kv_heads, block_size, head_size]
-     const int num_kv_heads,               // [num_heads]
-     const float scale,
-     const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-@@ -549,16 +839,1186 @@ __global__ void paged_attention_v2_kernel(
-     const int max_num_blocks_per_seq,
-     const float* __restrict__ alibi_slopes,  // [num_heads]
-     const int q_stride, const int kv_block_stride, const int kv_head_stride,
--    const float* k_scale, const float* v_scale, const int tp_rank,
--    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
--    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
--  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
--                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
--      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
--      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
--      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
--      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
--      blocksparse_head_sliding_step);
-+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
-+  const int seq_idx = blockIdx.y;
-+  const int partition_idx = blockIdx.z;
-+  // const int max_num_partitions = gridDim.z;
-+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
-+  const int seq_len = seq_lens[seq_idx];
-+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
-+    // No work to do. Terminate the thread block.
-+    return;
++
++
++__device__ __forceinline__ int32_t ScanWarp2(int32_t val, int32_t mask = 8) {
++  int32_t lane = threadIdx.x & 31;
++  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1, mask);
++  if (lane >= 1) {
++    val += tmp;
 +  }
-+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
-+  const int num_blocks_per_partition = USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
-+
-+  // [start_block_idx, end_block_idx) is the range of blocks to process.
-+  const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
-+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
-+  const int num_blocks = end_block_idx - start_block_idx;
-+
-+  // [start_token_idx, end_token_idx) is the range of tokens to process.
-+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
-+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
-+  const int num_tokens = end_token_idx - start_token_idx;
-+  constexpr int THREAD_GROUP_SIZE = MAX(MXWARP_SIZE / BLOCK_SIZE, 1);
-+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
-+  // assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
-+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, MXWARP_SIZE);
-+  constexpr int NUM_WARPS = NUM_THREADS >> 6;
-+  const int thread_idx = threadIdx.x;
-+  const int warp_idx = thread_idx >> 6;
-+  const int lane = thread_idx & 63;
-+
-+  const int head_idx = blockIdx.x;
-+  //const int num_heads = gridDim.x;
-+  const int num_queries_per_kv = num_heads / num_kv_heads;
-+  const int kv_head_idx = head_idx / num_queries_per_kv;
-+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
-+  // A vector type to store a part of a key or a query.
-+  // The vector size is configured in such a way that the threads in a thread group
-+  // fetch or compute 16 bytes at a time.
-+  // For example, if the size of a thread group is 4 and the data type is half,
-+  // then the vector size is 16 / (4 * sizeof(half)) == 2.
-+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
-+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
-+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
-+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
-+#ifdef ENABLE_FP8_E5M2
-+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
-+#endif
-+  
-+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
-+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
-+
-+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
-+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
-+
-+  // Load the query to registers.
-+  // Each thread in a thread group has a different part of the query.
-+  // For example, if the the thread group size is 4, then the first thread in the group
-+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
-+  // th vectors of the query, and so on.
-+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
-+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
-+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
-+#pragma unroll
-+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
-+    Q_vec_l dst;
-+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
-+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE); 
-+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
++  tmp = __shfl_up_sync(0xffffffff, val, 2, mask);
++  if (lane >= 2) {
++    val += tmp;
 +  }
-+  __syncthreads(); // TODO(naed90): possible speedup if this is replaced with a memory wall right before we use q_vecs
-+  // Memory planning.
-+  extern __shared__ char shared_mem[];
-+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
-+  float* logits = reinterpret_cast<float*>(shared_mem);
-+  // Workspace for reduction.
-+  __shared__ float red_smem[2 * NUM_WARPS];
-+
-+  // Each thread group fetches x elements from the key at a time.
-+  constexpr int x = 16;
-+  float qk_max = -FLT_MAX;
-+
-+  // Iterate over the key blocks.
-+  // Each warp fetches a block of keys for each iteration.
-+  // Each thread group in a warp fetches a key from the block, and computes
-+  // dot product with the query.
-+  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
-+  // blocksparse specific vars
-+  int bs_block_offset;
-+  int q_bs_block_id;
-+  if constexpr (IS_BLOCK_SPARSE) {
-+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
-+    // blocksparse_block_size);
-+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
-+    if (blocksparse_head_sliding_step >= 0)
-+      // sliding on q heads
-+      bs_block_offset =
-+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
-+    else
-+      // sliding on kv heads
-+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
-+                            (-blocksparse_head_sliding_step) +
-+                        1;
++  tmp = __shfl_up_sync(0xffffffff, val, 4, mask);
++  if (lane >= 4) {
++    val += tmp;
 +  }
-+  int block_idx0 = start_block_idx + warp_idx;
-+  int kv_offset0, kv_offset1;
-+  K_vec load_k[NUM_VECS_PER_THREAD];
-+  K_vec compute_k[NUM_VECS_PER_THREAD];
-+  int k_offset[NUM_VECS_PER_THREAD];
-+  kv_offset0 = block_table[block_idx0];
-+  if(block_idx0 + NUM_WARPS < end_block_idx) {
-+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
++  return val;
++}
++
++__device__ __forceinline__ int32_t ScanWarp(int32_t val) {
++  int32_t lane = threadIdx.x & 31;
++  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1);
++  if (lane >= 1) {
++    val += tmp;
 +  }
-+  #pragma unroll
-+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
-+    const int offset1 = vec_idx >> 4;
-+    const int offset2 = vec_idx & 15;
-+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
++  tmp = __shfl_up_sync(0xffffffff, val, 2);
++  if (lane >= 2) {
++    val += tmp;
 +  }
-+  const cache_t* ptr_k_cache = k_cache + kv_head_idx * kv_head_stride;
-+  if constexpr (IS_BLOCK_SPARSE) {
-+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
-+      const bool is_remote =
-+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+      const bool is_local =
-+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+      if(is_remote || is_local) {
-+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+            const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+        #pragma unroll
-+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+            }
-+          }    
-+      } 
-+  } else {
-+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+        const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1) ;
-+        const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
-+        const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+    #pragma unroll
-+        for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+          load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+        } 
-+    }
++  tmp = __shfl_up_sync(0xffffffff, val, 4);
++  if (lane >= 4) {
++    val += tmp;
 +  }
-+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {  
-+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+      #pragma unroll
-+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        compute_k[j] = load_k[j];
-+      }
-+      if(block_idx < end_block_idx - NUM_WARPS) {
-+          kv_offset0 = kv_offset1;
-+	  int nblock_idx = block_idx + NUM_WARPS;
-+          if(block_idx < end_block_idx - (NUM_WARPS << 1)) {
-+            kv_offset1 = block_table[block_idx + (NUM_WARPS<<1)];
-+          }
-+          if constexpr (IS_BLOCK_SPARSE) {
-+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
-+            const bool is_remote =
-+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+            const bool is_local =
-+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+            if(is_remote || is_local) {
-+              const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+              #pragma unroll NUM_VECS_PER_THREAD
-+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+              }  
-+            }
-+          } else {
-+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+            #pragma unroll NUM_VECS_PER_THREAD
-+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+            }
-+          }
-+      }
-+      if constexpr (IS_BLOCK_SPARSE) {
-+        const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-+        const bool is_remote =
-+            ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+        const bool is_local =
-+            (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+        if (!is_remote && !is_local) {
-+            for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+            const int physical_block_offset =
-+                (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+            const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+
-+            if (thread_group_offset == 0) {
-+                // NOTE(linxihui): assign very large number to skipped tokens to
-+                // avoid contribution to the sumexp softmax normalizer. This will
-+                // not be used at computing sum(softmax*v) as the blocks will be
-+                // skipped.
-+                logits[token_idx - start_token_idx] = -FLT_MAX;
-+            }
-+            }
-+            continue;
-+        }
-+    }
-+      // Compute dot product.
-+      // This includes a reduction across the threads in the same thread group.
-+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
-+      float qk = 0.0f;
-+      v2f f2_qk = {0,0};
-+      #pragma unroll
-+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
-+      }
-+      qk = f2_qk[0] + f2_qk[1];
-+      #pragma unroll
-+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
-+        qk += MXVLLM_SHFL_XOR_SYNC(qk, mask);
-+      }
-+      qk = scale * qk;
-+      // Add the ALiBi bias if slopes are given.
-+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
-+
-+      if (thread_group_offset == 0) {
-+        // Store the partial reductions to shared memory.
-+        // NOTE(woosuk): It is required to zero out the masked logits.
-+        const bool mask = token_idx >= seq_len;
-+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
-+        // Update the max value.
-+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
-+      }
-+    }
-+  }
-+  // Perform reduction across the threads in the same warp to get the
-+  // max qk value for each "warp" (not across the thread block yet).
-+  // The 0-th thread of each thread group already has its max qk value.
-+  #pragma unroll
-+  for (int mask = MXWARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
-+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
++  tmp = __shfl_up_sync(0xffffffff, val, 8);
++  if (lane >= 8) {
++    val += tmp;
 +  }
-+  if (lane == 0) {
-+    red_smem[warp_idx] = qk_max;
++  tmp = __shfl_up_sync(0xffffffff, val, 16);
++  if (lane >= 16) {
++    val += tmp;
 +  }
-+  __syncthreads();
++  return val;
++}
 +
-+  // TODO(woosuk): Refactor this part.
-+  // Get the max qk value for the sequence.
-+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
-+  #pragma unroll
-+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
-+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
-+  }
-+  // Broadcast the max qk value to all threads.
-+  qk_max = MXVLLM_SHFL_SYNC(qk_max, 0);
-+
-+  // Get the sum of the exp values.
-+  float exp_sum = 0.f;
-+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-+    float val = __builtin_expf(logits[i] - qk_max);
-+    logits[i] = val;
-+    exp_sum += val;
++
++__device__ inline void copy(const void* local, void* data)
++{
++    const float4* in = static_cast<const float4*>(local);
++    float4* out = static_cast<float4*>(data);
++    *out = *in;
++}
++
++template <typename scalar_t, int VPT>
++__global__ void opt_sgl_moe_align_block_size_kernel_stage_2(size_t numel, int32_t* local_offsets, scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids){
++  int idx = blockIdx.x * blockDim.x + threadIdx.x;
++  if (idx * VPT >= numel) return;
++  int32_t expert_local[VPT];
++  copy(topk_ids + idx * VPT, expert_local);
++  for(int i = 0; i < VPT && i + idx * VPT < numel; ++i) {
++    int32_t expert_id = expert_local[i];
++    int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);
++    sorted_token_ids[rank_post_pad] = i + idx * VPT;
 +  }
-+  exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
-+
-+  // Compute softmax.
-+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
-+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-+    float val = logits[i];
-+    val *= inv_sum;
-+    logits[i] = convert<cache_t>(val);
++}
++
++template <typename scalar_t>
++__global__ void opt_sgl_moe_align_block_size_kernel_stage_1(
++    scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids,
++    int32_t* expert_ids, int32_t* total_tokens_post_pad, int32_t num_experts,
++    int32_t block_size, size_t numel, int32_t* offsets) {
++  __shared__ int32_t shared_counts[32][8];
++  __shared__ int32_t local_offsets[256];
++  __shared__ int32_t cum_test[257];
++  __shared__ int32_t blocksum[32];
++
++  const int warp_id = threadIdx.x / 32;
++  const int lane_id = threadIdx.x % 32;
++  const int experts_per_warp = 8;
++  const int my_expert_start = warp_id * experts_per_warp;
++
++  int32_t idx = threadIdx.x;
++  if(idx < num_experts){
++    shared_counts[idx / 8][idx % 8] = 0;
++    cum_test[0] = 0;
 +  }
 +  __syncthreads();
 +
-+  // If partitioning is enabled, store the max logit and exp_sum.
-+  if (USE_PARTITIONING && thread_idx == 0) {
-+    float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
-+                                       + head_idx * max_num_partitions
-+                                       + partition_idx;
-+    *max_logits_ptr = qk_max;
-+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
-+                                   + head_idx * max_num_partitions
-+                                   + partition_idx;
-+    *exp_sums_ptr = exp_sum;
-+  }
-+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
-+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
-+  constexpr int NUM_COLS_PER_ITER = MAX(MXWARP_SIZE / NUM_V_VECS_PER_THREAD , 1); 
-+  constexpr int NUM_LGT_PER_COL = BLOCK_SIZE / NUM_COLS_PER_ITER; 
-+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
-+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
-+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
-+  V_vec v_vecs[NUM_LGT_PER_COL];
-+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
-+  float accs[V_VEC_SIZE];
-+  float reg_log[NUM_LGT_PER_COL];
-+  float reg_prev_log[NUM_LGT_PER_COL];
-+  #pragma unroll
-+  for(int i = 0; i < V_VEC_SIZE; i++) {
-+    accs[i] = 0.0f;
++  const size_t tokens_per_thread = CEILDIV(numel, blockDim.x);
++  const size_t start_idx = threadIdx.x * tokens_per_thread;
++
++  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
++    int expert_id = topk_ids[i];
++    int warp_idx = expert_id / experts_per_warp;
++    int expert_offset = expert_id % experts_per_warp;
++    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
 +  }
-+  int token_idx, kv_stride, block_offset;
-+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
-+  kv_offset0 = block_table[block_idx0];
-+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
-+  if(block_idx0 + NUM_WARPS < end_block_idx) {
-+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
++  __syncthreads();
++
++  int val = 0;
++  if (threadIdx.x < 256) {
++    int32_t final_val = 0;
++    int row = idx / 8;
++    int line = idx % 8;
++    val = shared_counts[row][line];
++    val = CEILDIV(val, block_size) * block_size;
 +  }
-+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
-+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE; 
-+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
-+  float *ptr_logits = logits + token_idx - start_token_idx;
-+  if constexpr (IS_BLOCK_SPARSE) {
-+        int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
-+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
-+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+            #pragma unroll
-+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+                const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                }
-+            }
-+        }
-+    } else {
-+          #pragma unroll
-+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+            if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+            }
-+          }
-+    }
-+  for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
-+    int next_block = block_idx + NUM_WARPS;
-+    int nnext_block = next_block + NUM_WARPS;
-+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+      v_vecs[i] = v_prev_vecs[i];
-+      reg_log[i] = reg_prev_log[i];
-+    }
-+    if(next_block < end_block_idx) {
-+      kv_offset0 = kv_offset1;
-+      if(nnext_block < end_block_idx) {
-+        kv_offset1 = block_table[nnext_block];
-+      }
-+      token_idx = next_block * BLOCK_SIZE + physical_block_offset;
-+      const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
-+      ptr_logits = logits + token_idx - start_token_idx;
-+      if constexpr (IS_BLOCK_SPARSE) {
-+        int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
-+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
-+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+                #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
-+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                }
-+            }
-+        }
-+        } else  {
-+            #pragma unroll
-+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+            }
-+        }
-+    }
-+    if constexpr (IS_BLOCK_SPARSE) {
-+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
-+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+          continue;
-+        }
++  __syncthreads();
++
++  if(idx < 256) {
++    int tmp = 0;
++    val = ScanWarp(val);
++    if(lane_id == 31) {
++      blocksum[warp_id] = val;
 +    }
-+    
-+    token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+    float *ptr_logits = logits + token_idx - start_token_idx;
-+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+      if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
-+        scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
-+        #pragma unroll
-+        for(int j = 0; j < V_VEC_SIZE; j+=2) {
-+          atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
-+        }
-+      }
-+    } 
 +  }
 +  __syncthreads();
-+  float* out_smem = reinterpret_cast<float*>(shared_mem);
-+  float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
-+  #pragma unroll
-+  for(int i = 0; i < V_VEC_SIZE; i++) {
-+    ptr_out_smem[i] = accs[i];
-+  }
-+   __syncthreads();
-+  
-+  scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
-+                        + head_idx * max_num_partitions * HEAD_SIZE
-+                        + partition_idx * HEAD_SIZE;
-+  if(threadIdx.x < HEAD_SIZE) {
-+    int length = NUM_LANE * HEAD_SIZE;
-+      float r = 0;
-+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
-+        r += out_smem[i];
-+      }
-+      from_float(*(out_ptr + threadIdx.x), r);
-+  }
-+}
-+
-+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-+          bool IS_BLOCK_SPARSE,
-+	  int PARTITION_SIZE = 512>  // Zero means no partitioning.
-+__device__ __forceinline__ void paged_attention_kernel_32N_final(
-+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
-+    float* __restrict__ max_logits,  // [num_seqs, num_heads,
-+                                     // max_num_partitions]
-+    int* __restrict__ block_count,          // [num_seqs, num_heads]
-+    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,
-+                                 // head_size]
-+    scalar_t* __restrict__ final_out,      // [num_seqs, num_heads, head_size]
-+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads, head_size/x, block_size, x]->[num_blocks, num_kv_heads, head_size/16, block_size, 16]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads, head_size, block_size]->[num_blocks, num_kv_heads, block_size, head_size]
-+    const int num_kv_heads,               // [num_heads]
-+    const float scale,
-+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-+    const int* __restrict__ seq_lens,      // [num_seqs]
-+    const int max_num_blocks_per_seq,
-+    const float* __restrict__ alibi_slopes,  // [num_heads]
-+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step, const int max_num_partitions,
-+    const int blockDim_x,
-+    const int num_heads,
-+    const int grid_dim_y,
-+    const bool count_init_once) {
-+  const int seq_idx = blockIdx.y;
-+  const int partition_idx = blockIdx.z;
-+  // const int max_num_partitions = gridDim.z;
-+  // const int blockDim_x = blockDim.x;
-+  const int head_idx = blockIdx.x;
-+  // const int num_heads = gridDim.x;
-+  // const int grid_dim_y = gridDim.y;
-+  const int seq_len = seq_lens[seq_idx];
-+
-+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
-+  const int num_blocks_per_partition = PARTITION_SIZE / BLOCK_SIZE;
-+
-+  // [start_block_idx, end_block_idx) is the range of blocks to process.
-+  const int start_block_idx = partition_idx * num_blocks_per_partition;
-+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
-+  const int num_blocks = end_block_idx - start_block_idx;
-+
-+  // [start_token_idx, end_token_idx) is the range of tokens to process.
-+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
-+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
-+  const int num_tokens = end_token_idx - start_token_idx;
-+
-+  constexpr int THREAD_GROUP_SIZE = MAX(MXWARP_SIZE / BLOCK_SIZE, 1);
-+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
-+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, MXWARP_SIZE);
-+  constexpr int NUM_WARPS = NUM_THREADS >> 6;
-+  const int thread_idx = threadIdx.x;
-+  const int warp_idx = thread_idx >> 6;
-+  const int lane = thread_idx & 63;
-+
-+  const int num_queries_per_kv = num_heads / num_kv_heads;
-+  const int kv_head_idx = head_idx / num_queries_per_kv;
-+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
-+  int offset0 = seq_idx * num_heads;
-+  int offset1 = offset0 * HEAD_SIZE;
-+  int offset2 = head_idx * HEAD_SIZE;
-+  int offset3 = head_idx * max_num_partitions;
-+  int offset4 = offset0 * max_num_partitions;
-+  int offset5 = seq_idx * max_num_blocks_per_seq;
-+  int num_warps2 = NUM_WARPS << 1;
-+
-+  // The vector size is configured in such a way that the threads in a thread group
-+  // fetch or compute 16 bytes at a time.
-+  // For example, if the size of a thread group is 4 and the data type is half,
-+  // then the vector size is 16 / (4 * sizeof(half)) == 2.
-+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
-+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
-+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
-+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
-+#ifdef ENABLE_FP8_E5M2
-+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
-+#endif
 +
-+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
-+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
-+
-+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
-+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
-+
-+  // Load the query to registers.
-+  // Each thread in a thread group has a different part of the query.
-+  // For example, if the the thread group size is 4, then the first thread in the group
-+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
-+  // th vectors of the query, and so on.
-+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
-+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
-+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
-+#pragma unroll
-+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
-+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
-+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
-+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
-+  }
-+  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
-+                    // memory wall right before we use q_vecs
-+
-+  // Memory planning.
-+  extern __shared__ char shared_mem[];
-+  float* red_smem = reinterpret_cast<float*>(shared_mem);
-+  int * block_table_smem = reinterpret_cast<int*>(red_smem + num_warps2);
-+  float * logits = reinterpret_cast<float*>(block_table_smem + 512 + num_warps2);
-+
-+  // Each thread group fetches x elements from the key at a time.
-+  constexpr int x = 16;
-+  float qk_max = -FLT_MAX;
-+
-+  // blocksparse specific vars
-+  int bs_block_offset;
-+  int q_bs_block_id;
-+  if constexpr (IS_BLOCK_SPARSE) {
-+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
-+    // blocksparse_block_size);
-+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
-+    if (blocksparse_head_sliding_step >= 0)
-+      // sliding on q heads
-+      bs_block_offset =
-+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
-+    else
-+      // sliding on kv heads
-+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
-+                            (-blocksparse_head_sliding_step) +
-+                        1;
++  if(warp_id == 0 && lane_id < 8) {
++    int res = blocksum[lane_id];
++    blocksum[lane_id] = ScanWarp2(res);
 +  }
++  __syncthreads();
 +
-+  // Iterate over the key blocks.
-+  // Each warp fetches a block of keys for each iteration.
-+  // Each thread group in a warp fetches a key from the block, and computes
-+  // dot product with the query.
-+  const int* block_table = block_tables + offset5 + start_block_idx;
-+  //load block_table to share memory
-+  for(int i = threadIdx.x; i < num_blocks; i += blockDim_x){
-+    block_table_smem[i] = block_table[i];
++  if(threadIdx.x < 256 && warp_id > 0){
++    val += blocksum[warp_id - 1];
 +  }
-+  int block_idx0 = start_block_idx + warp_idx;
 +  __syncthreads();
-+  int kv_offset0, kv_offset1;
-+  K_vec load_k[NUM_VECS_PER_THREAD];
-+  K_vec compute_k[NUM_VECS_PER_THREAD];
-+  int k_offset[NUM_VECS_PER_THREAD];
-+  kv_offset0 = block_table_smem[block_idx0 - start_block_idx];
-+  kv_offset1 = block_table_smem[block_idx0 + NUM_WARPS - start_block_idx];
-+  #pragma unroll
-+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
-+    const int offset1 = vec_idx >> 4;
-+    const int offset2 = vec_idx & 15;
-+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
++
++  if(idx < 256){
++    cum_test[idx + 1] = val;
 +  }
++  __syncthreads();
 +
-+  const cache_t* ptr_k_cache = k_cache + kv_head_idx * kv_head_stride;
-+  if constexpr (IS_BLOCK_SPARSE) {
-+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
-+      const bool is_remote =
-+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+      const bool is_local =
-+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+      if(is_remote || is_local) {
-+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+            const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+        #pragma unroll
-+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+            }
-+          }    
-+      } 
-+  } else {
-+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+      const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+    #pragma unroll
-+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+      }
++  if (threadIdx.x < num_experts) {
++    for (int i = cum_test[threadIdx.x]; i < cum_test[threadIdx.x + 1];
++         i += block_size) {
++      expert_ids[i / block_size] = threadIdx.x;
 +    }
-+  }
-+  
-+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
-+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+      #pragma unroll
-+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        compute_k[j] = load_k[j];
-+      }
-+      if(block_idx < end_block_idx - NUM_WARPS) {
-+          kv_offset0 = kv_offset1;
-+	  int nblock_idx = block_idx + NUM_WARPS;
-+          kv_offset1 = block_table_smem[block_idx + num_warps2 - start_block_idx];
-+          if constexpr (IS_BLOCK_SPARSE) {
-+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
-+            const bool is_remote =
-+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+            const bool is_local =
-+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+            if(is_remote || is_local) {
-+              const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+              #pragma unroll NUM_VECS_PER_THREAD
-+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+              }  
-+            }
-+          } else {
-+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
-+            #pragma unroll NUM_VECS_PER_THREAD
-+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
-+            }
-+          }
-+      }
-+      if constexpr (IS_BLOCK_SPARSE) {
-+      const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-+      const bool is_remote =
-+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
-+      const bool is_local =
-+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
-+      if (!is_remote && !is_local) {
-+        for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-+          const int physical_block_offset =
-+              (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
-+          const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+
-+          if (thread_group_offset == 0) {
-+            // NOTE(linxihui): assign very large number to skipped tokens to
-+            // avoid contribution to the sumexp softmax normalizer. This will
-+            // not be used at computing sum(softmax*v) as the blocks will be
-+            // skipped.
-+            logits[token_idx - start_token_idx] = -FLT_MAX;
-+          }
-+        }
-+        continue;
-+      }
++    local_offsets[threadIdx.x] = cum_test[threadIdx.x];
++    offsets[threadIdx.x] = cum_test[threadIdx.x];
++    if(threadIdx.x == 0){
++      *total_tokens_post_pad = cum_test[num_experts];
 +    }
++  }
++}
 +
-+      // Compute dot product.
-+      // This includes a reduction across the threads in the same thread group.
-+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
-+      float qk = 0.0f;
-+      v2f f2_qk = {0,0};
-+      #pragma unroll
-+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-+        atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
-+      }
-+      qk = f2_qk[0] + f2_qk[1];
-+      #pragma unroll
-+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
-+        qk += MXVLLM_SHFL_XOR_SYNC(qk, mask);
-+      }
 +
-+      qk = scale * qk;
-+      // Add the ALiBi bias if slopes are given.
-+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
-+
-+      if (thread_group_offset == 0) {
-+        // Store the partial reductions to shared memory.
-+        // NOTE(woosuk): It is required to zero out the masked logits.
-+        const bool mask = token_idx >= seq_len;
-+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
-+        // Update the max value.
-+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
-+      }
-+    }
-+  }
++template <typename scalar_t>
++__global__ void opt_sgl_moe_align_block_size_kernel(
++    scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids,
++    int32_t* expert_ids, int32_t* total_tokens_post_pad, int32_t num_experts,
++    int32_t block_size, size_t numel, int32_t* cumsum) {
++  __shared__ int32_t shared_counts[32][8];
++  __shared__ int32_t local_offsets[256];
++  __shared__ int32_t cum_test[257];
++  __shared__ int32_t blocksum[32];
++
++  const int warp_id = threadIdx.x / 32;
++  const int lane_id = threadIdx.x % 32;
++  const int experts_per_warp = 8;
++  const int my_expert_start = warp_id * experts_per_warp;
 +
-+  // Perform reduction across the threads in the same warp to get the
-+  // max qk value for each "warp" (not across the thread block yet).
-+  // The 0-th thread of each thread group already has its max qk value.
-+#pragma unroll
-+  for (int mask = MXWARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
-+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
++  int32_t idx = threadIdx.x;
++  if(idx < num_experts){
++    shared_counts[idx / 8][idx % 8] = 0;
++    cum_test[0] = 0;
 +  }
-+  if (lane == 0) {
-+    red_smem[warp_idx] = qk_max;
++  __syncthreads();
++
++  const size_t tokens_per_thread = CEILDIV(numel, blockDim.x);
++  const size_t start_idx = threadIdx.x * tokens_per_thread;
++
++  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
++    int expert_id = topk_ids[i];
++    int warp_idx = expert_id / experts_per_warp;
++    int expert_offset = expert_id % experts_per_warp;
++    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
 +  }
 +  __syncthreads();
 +
-+  // TODO(woosuk): Refactor this part.
-+  // Get the max qk value for the sequence.
-+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
-+#pragma unroll
-+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
-+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
++  int val = 0;
++  if (threadIdx.x < 256) {
++    int32_t final_val = 0;
++    int row = idx / 8;
++    int line = idx % 8;
++    val = shared_counts[row][line];
++    val = CEILDIV(val, block_size) * block_size;
 +  }
-+  // Broadcast the max qk value to all threads.
-+  qk_max = MXVLLM_SHFL_SYNC(qk_max, 0);
-+
-+  // Get the sum of the exp values.
-+  float exp_sum = 0.f;
-+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-+    float val = __builtin_expf(logits[i] - qk_max);
-+    logits[i] = val;
-+    exp_sum += val;
++  __syncthreads();
++
++  if(idx < 256) {
++    int tmp = 0;
++    val = ScanWarp(val);
++    if(lane_id == 31) {
++      blocksum[warp_id] = val;
++    }
 +  }
-+  exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
-+
-+  // Compute softmax.
-+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
-+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-+    float val = logits[i];
-+    val *= inv_sum;
-+    logits[i] = convert<cache_t>(val);
++  __syncthreads();
++
++  if(warp_id == 0 && lane_id < 8) {
++    int res = blocksum[lane_id];
++    blocksum[lane_id] = ScanWarp2(res);
 +  }
 +  __syncthreads();
 +
-+  // If partitioning is enabled, store the max logit and exp_sum.
-+  if (thread_idx == 0) {
-+    float* max_logits_ptr = max_logits + offset4
-+                                       + offset3
-+                                       + partition_idx;
-+    *max_logits_ptr = qk_max;
-+    float* exp_sums_ptr = exp_sums + offset4
-+                                   + offset3
-+                                   + partition_idx;
-+    *exp_sums_ptr = exp_sum;
++  if(threadIdx.x < 256 && warp_id > 0){
++    val += blocksum[warp_id - 1];
 +  }
++  __syncthreads();
 +
-+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
-+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
-+  constexpr int NUM_COLS_PER_ITER = MAX(MXWARP_SIZE / NUM_V_VECS_PER_THREAD , 1); 
-+  constexpr int NUM_LGT_PER_COL = BLOCK_SIZE / NUM_COLS_PER_ITER; 
-+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
-+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
-+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
-+  V_vec v_vecs[NUM_LGT_PER_COL];
-+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
-+  float accs[V_VEC_SIZE];
-+  float reg_log[NUM_LGT_PER_COL];
-+  float reg_prev_log[NUM_LGT_PER_COL];
-+
-+  #pragma unroll
-+  for(int i = 0; i < V_VEC_SIZE; i++) {
-+    accs[i] = 0.0f;
++  if(idx < 256){
++    cum_test[idx + 1] = val;
 +  }
-+  int token_idx, kv_stride, block_offset;
-+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
-+  kv_offset0 = block_table_smem[block_idx0 - start_block_idx];
-+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
-+  kv_offset1 = block_table_smem[block_idx0 + NUM_WARPS - start_block_idx];
-+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
-+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE;
-+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
-+  float *ptr_logits = logits + token_idx - start_token_idx;
-+
-+  if constexpr (IS_BLOCK_SPARSE) {
-+      int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
-+      if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
-+          ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+          #pragma unroll
-+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+              if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+              }
-+          }
-+      }
-+  } else {
-+        #pragma unroll
-+        for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+          if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
-+            const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+            v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+            reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+          }
-+        }
-+    }
++  __syncthreads();
 +
-+  for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
-+    int next_block = block_idx + NUM_WARPS;
-+    int nnext_block = next_block + NUM_WARPS;
-+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+      v_vecs[i] = v_prev_vecs[i]; reg_log[i] = reg_prev_log[i];
-+    }
-+    if(next_block < end_block_idx) {
-+      kv_offset0 = kv_offset1;
-+      kv_offset1 = block_table_smem[nnext_block - start_block_idx];
-+      token_idx = next_block * BLOCK_SIZE + physical_block_offset;
-+      const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
-+      ptr_logits = logits + token_idx - start_token_idx;
-+      if constexpr (IS_BLOCK_SPARSE) {
-+        int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
-+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
-+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+              if(next_block == num_seq_blocks - 1) {
-+                #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
-+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                    }
-+                }
-+              } else {
-+                #pragma unroll
-+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+                }
-+              }
-+        }
-+        } else {
-+          if(next_block == num_seq_blocks - 1) {
-+            #pragma unroll
-+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
-+                const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+                v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+                reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+              }
-+            }
-+          } else {
-+            #pragma unroll
-+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
-+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
-+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
-+            }
-+          }
-+        }
++  if (threadIdx.x < num_experts) {
++    for (int i = cum_test[threadIdx.x]; i < cum_test[threadIdx.x + 1];
++         i += block_size) {
++      expert_ids[i / block_size] = threadIdx.x;
 +    }
-+    if constexpr (IS_BLOCK_SPARSE) {
-+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
-+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-+          continue;
-+        }
++    local_offsets[threadIdx.x] = cum_test[threadIdx.x];
++    if(threadIdx.x == 0){
++      *total_tokens_post_pad = cum_test[num_experts];
 +    }
++  }
++  __syncthreads();
 +
-+    token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-+    float *ptr_logits = logits + token_idx - start_token_idx;
-+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
-+      if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
-+        scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
-+        #pragma unroll
-+        for(int j = 0; j < V_VEC_SIZE; j+=2) {
-+          atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
-+        }
-+      }
-+    } 
++  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
++    int32_t expert_id = topk_ids[i];
++    int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);
++    sorted_token_ids[rank_post_pad] = i;
 +  }
++}
++
+ // taken from
+ // https://github.com/sgl-project/sglang/commit/cdae77b03dfc6fec3863630550b45bbfc789f957
+ template <typename scalar_t>
+@@ -290,6 +524,81 @@ __global__ void moe_sum_kernel(
+   }
+ }
+ 
++template<typename scalar_t, typename token_cnts_t, int32_t NUM_EXPS, int32_t BLOCK_DIM>
++__global__ void moe_align_block_size_kernel_opt(scalar_t* __restrict__ topk_ids,
++                            int32_t* sorted_token_ids,
++                            int32_t* expert_ids,
++                            int32_t* total_tokens_post_pad,
++                            int32_t num_experts,
++                            int32_t block_size, size_t numel) {
++  __shared__ int32_t shared_counts[32][4];
++  __shared__ int32_t cumsum[NUM_EXPS + 1];
++  __shared__ int32_t blocksum[BLOCK_DIM / WARP_SIZE + 1];
++
++  const size_t tokens_per_thread = CEILDIV(numel, BLOCK_DIM);
++  const size_t start_idx = threadIdx.x * tokens_per_thread;
++  static constexpr int32_t experts_per_warp = 4;
 +
++  const int32_t warp_id = threadIdx.x / 32;
++  const int32_t lane_id = threadIdx.x % 32;
++  const int32_t my_expert_start = warp_id * experts_per_warp;
++
++  if (threadIdx.x < NUM_EXPS) {
++    shared_counts[threadIdx.x / experts_per_warp][threadIdx.x % experts_per_warp] = 0;
++    cumsum[0] = 0;
++    blocksum[0] = 0;
++  }
 +  __syncthreads();
-+  float* out_smem = reinterpret_cast<float*>(shared_mem);
-+  float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
-+  #pragma unroll
-+  for(int i = 0; i < V_VEC_SIZE; i++) {
-+    ptr_out_smem[i] = accs[i];
++
++  for (int32_t i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
++    int32_t expert_id = topk_ids[i];
++    int32_t warp_idx = expert_id / experts_per_warp;
++    int32_t expert_offset = expert_id % experts_per_warp;
++    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
 +  }
-+   __syncthreads();
-+  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);
-+
-+  if(partition_idx * PARTITION_SIZE < seq_len) {
-+    scalar_t* out_ptr = out + (offset4 + offset3 + partition_idx) * HEAD_SIZE;
-+    if(threadIdx.x < HEAD_SIZE) {
-+      int length = NUM_LANE * HEAD_SIZE;
-+      float r = 0;
-+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
-+        r += out_smem[i];
-+      }
-+      from_float(*(out_ptr + threadIdx.x), r);
++  __syncthreads();
++
++  int32_t val = 0;
++  if (threadIdx.x < NUM_EXPS) {
++    int32_t row = threadIdx.x / experts_per_warp;
++    int32_t line = threadIdx.x % experts_per_warp;
++    val = CEILDIV(shared_counts[row][line], block_size) * block_size;
++    val = ScanWarp(val);
++    if(lane_id == 31) {
++      blocksum[warp_id + 1] = val;
 +    }
 +  }
++  __syncthreads();
++
++  if(threadIdx.x < BLOCK_DIM / WARP_SIZE) {
++    int32_t res = blocksum[lane_id + 1];
++    blocksum[lane_id + 1] = ScanWarp2(res, BLOCK_DIM / WARP_SIZE);
++  }
++  __syncthreads();
 +
++  if (threadIdx.x < NUM_EXPS) {
++    val += blocksum[warp_id];
++    cumsum[threadIdx.x + 1] = val;
++  }
 +  __syncthreads();
-+  bool last_block = false;
-+  if(threadIdx.x == blockDim_x - 1) {
-+    if(atomicAdd(block_count + head_idx * grid_dim_y + seq_idx, 1) == max_num_partitions - 1) {
-+      last_block = true;
++
++  if (threadIdx.x < NUM_EXPS) {
++    for (int32_t i = cumsum[threadIdx.x]; i < cumsum[threadIdx.x + 1]; i += block_size) {
++      expert_ids[i / block_size] = threadIdx.x;
++    }
++    if (threadIdx.x == 0) {
++      *total_tokens_post_pad = cumsum[NUM_EXPS];
 +    }
 +  }
-+  if (__syncthreads_or(last_block)) {
-+      if(count_init_once) {
-+        if(threadIdx.x == blockDim_x - 2){
-+          *(block_count + head_idx * grid_dim_y + seq_idx) = 0;
-+        }
-+      }
-+      if(num_partitions == 1) {
-+        scalar_t* out_ptr = final_out + offset1 + offset2;
-+        const scalar_t* tmp_out_ptr = out + (offset4 + offset3) * HEAD_SIZE;
-+        V_vec* ptr_vec_out = (V_vec*)out_ptr;
-+        V_vec* ptr_vec_in = (V_vec*)tmp_out_ptr;
-+        int num = HEAD_SIZE / V_VEC_SIZE;
-+        for (int i = threadIdx.x; i < num; i += blockDim_x) {
-+          ptr_vec_out[i] = ptr_vec_in[i];
++  __syncthreads();
++
++  for (int32_t i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
++    int32_t expert_id = topk_ids[i];
++    int32_t rank_post_pad = atomicAdd(&cumsum[expert_id], 1);
++    sorted_token_ids[rank_post_pad] = i;
++  }
++}
++
+ }  // namespace moe
+ }  // namespace vllm
+ 
+@@ -351,7 +660,24 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
+               cumsum_buffer.data_ptr<int32_t>());
+         });
+   } else if (use_i16) {
+-    VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(
++    if (num_experts == 128){
++      VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(
++        topk_ids.scalar_type(), "moe_align_block_size_kernel_opt", [&] {
++          // set dynamic shared mem
++          constexpr int32_t tids = 512;
++          auto kernel =
++              vllm::moe::moe_align_block_size_kernel_opt<scalar_t, uint16_t, 128, tids>;
++          // AT_CUDA_CHECK(VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(
++          //     (void*)kernel, shared_mem_i16));
++          kernel<<<1, tids, 0, stream>>>(
++              topk_ids.data_ptr<scalar_t>(),
++              sorted_token_ids.data_ptr<int32_t>(),
++              experts_ids.data_ptr<int32_t>(),
++              num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
++              topk_ids.numel());
++        });
++    } else {
++      VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(
+         topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
+           // set dynamic shared mem
+           auto kernel =
+@@ -365,6 +691,7 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
+               num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
+               topk_ids.numel());
+         });
++    }
+   } else {
+     VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(
+         topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
+@@ -396,27 +723,44 @@ void sgl_moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
+         // calc needed amount of shared mem for `cumsum` tensors
+         auto options_int =
+             torch::TensorOptions().dtype(torch::kInt).device(topk_ids.device());
+-        torch::Tensor cumsum_buffer =
++        torch::Tensor offsets_buffer =
+             torch::zeros({num_experts + 1}, options_int);
+ 
+-        auto align_kernel =
+-            vllm::moe::sgl_moe_align_block_size_kernel<scalar_t>;
+-        align_kernel<<<1, 1024, 0, stream>>>(
+-            topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
+-            experts_ids.data_ptr<int32_t>(),
+-            num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
+-            topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>());
+-
+-        const int block_threads = 256;
+-        const int num_blocks =
+-            (topk_ids.numel() + block_threads - 1) / block_threads;
+-        const int max_blocks = 65535;
+-        const int actual_blocks = std::min(num_blocks, max_blocks);
+-        auto sort_kernel = vllm::moe::sgl_moe_token_sort_kernel<scalar_t>;
+-        sort_kernel<<<actual_blocks, block_threads, 0, stream>>>(
+-            topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
+-            cumsum_buffer.data_ptr<int32_t>(), topk_ids.numel());
+-      });
++        if(topk_ids.numel() <= 4096) {
++          auto align_kernel =
++              vllm::moe::opt_sgl_moe_align_block_size_kernel<scalar_t>;
++          align_kernel<<<1, 1024, 0, stream>>>(
++              topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
++              experts_ids.data_ptr<int32_t>(),
++              num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
++                topk_ids.numel(), offsets_buffer.data_ptr<int32_t>());
++        } else {
++           auto kernel = vllm::moe::opt_sgl_moe_align_block_size_kernel_stage_1<scalar_t>;
++          kernel<<<1, 1024, 0, stream>>>(
++              topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
++              experts_ids.data_ptr<int32_t>(),
++              num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
++              topk_ids.numel(), offsets_buffer.data_ptr<int32_t>());
++
++          auto kernel_2 = vllm::moe::opt_sgl_moe_align_block_size_kernel_stage_2<scalar_t, 16 / sizeof(scalar_t)>;
++          int block_size = 256;
++          int grid_size = (topk_ids.numel() + block_size - 1) / block_size;
++          kernel_2<<<grid_size, block_size, 0, stream>>>(
++              topk_ids.numel(), offsets_buffer.data_ptr<int32_t>(), 
++              topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>());
 +        }
-+	return;
-+      }
-+      // Load max logits to shared memory.
-+      float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
-+      float * red_smem = shared_max_logits + (num_partitions << 1 );
-+      const float* max_logits_ptr = max_logits + offset4 + offset3;
-+      float max_logit = -FLT_MAX;
-+      for (int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
-+        const float l = max_logits_ptr[i];
-+        shared_max_logits[i] = l;
-+      }
-+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
-+        max_logit = fmaxf(max_logit, shared_max_logits[i]);
-+      }
-+      __syncthreads();
-+      // Get the global max logit.
-+      // Reduce within the warp.
-+      #pragma unroll
-+      for (int mask = MXWARP_SIZE / 2; mask >= 1; mask /= 2) {
-+        max_logit = fmaxf(max_logit, MXVLLM_SHFL_XOR_SYNC(max_logit, mask));
-+      }
-+      
-+      if (lane == 0) {
-+        red_smem[warp_idx] = max_logit;
-+      }
-+      __syncthreads();
-+      // Reduce across warps.
-+      max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
-+    #pragma unroll
-+      for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
-+        max_logit = fmaxf(max_logit, MXVLLM_SHFL_XOR_SYNC(max_logit, mask));
-+      }
-+      // Broadcast the max value to all threads.
-+      max_logit = MXVLLM_SHFL_SYNC(max_logit, 0);
 +
-+      // Load rescaled exp sums to shared memory.
-+      float* shared_exp_sums = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
-+      const float* exp_sums_ptr = exp_sums + offset4 + offset3;
++      //   const int block_threads = 256;
++      //   const int num_blocks =
++      //       (topk_ids.numel() + block_threads - 1) / block_threads;
++      //   const int max_blocks = 65535;
++      //   const int actual_blocks = std::min(num_blocks, max_blocks);
++      //   auto sort_kernel = vllm::moe::sgl_moe_token_sort_kernel<scalar_t>;
++      //   sort_kernel<<<actual_blocks, block_threads, 0, stream>>>(
++      //       topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
++      //       offsets_buffer.data_ptr<int32_t>(), topk_ids.numel());
++    }
++);
+ }
+ 
+ void moe_sum(torch::Tensor& input,   // [num_tokens, topk, hidden_size]
+diff --git a/csrc/moe/moe_ops.cpp b/csrc/moe/moe_ops.cpp
+new file mode 100644
+index 000000000..a12aeff32
+--- /dev/null
++++ b/csrc/moe/moe_ops.cpp
+@@ -0,0 +1,64 @@
++#include "moe_ops.h"
 +
-+      float global_exp_sum = 0.0f;  
-+      float * out_sm_ptr = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions * 2);
-+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
-+        out_sm_ptr[i] = exp_sums_ptr[i];
-+      }
++#include <ATen/cuda/CUDAContext.h>
++#include <mcblas.h>
++#include <maca_fp16.h>
 +
-+      for (int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
-+        float l = shared_max_logits[i];
-+        float rescaled_exp_sum = out_sm_ptr[i] * __builtin_expf(l - max_logit);
-+        global_exp_sum += rescaled_exp_sum;
-+        shared_exp_sums[i] = rescaled_exp_sum;
-+      }
-+      __syncthreads();
++mcblasStatus_t mcblasFusedMoe(mcStream_t stream,
++                              const void *a_ptr,
++                              const void *b_ptr,
++                              void *c_ptr,
++                              const int *sorted_token_ids_ptr,
++                              const int *expert_ids_ptr,
++                              const int *num_tokens_post_padded,
++                              int N,
++                              int K,
++                              int num_valid_tokens,
++                              int sorted_token_ids_len,
++                              int stride_am,
++                              int stride_ak,
++                              int stride_be,
++                              int stride_bk,
++                              int stride_bn,
++                              int stride_cm,
++                              int stride_cn,
++                              int top_k,
++                              bool mul_routed_weight,
++                              const float *topk_weights_ptr,
++                              macaDataType compute_type,
++                              int tileConfig = 0);
 +
-+      global_exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum);
-+      const float inv_global_exp_sum = __builtin_mxc_rcpf(global_exp_sum + 1e-6f);
-+      
-+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
-+          shared_exp_sums[i] = shared_exp_sums[i] * inv_global_exp_sum;
-+      }
++void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
++                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
++                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
++                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig) {
++    
++    assert(topk_weights.stride(1) == 1);
++    assert(sorted_token_ids.stride(0) == 1);
 +
-+      // Aggregate tmp_out to out.
-+      scalar_t* out_ptr = final_out + offset1 + offset2;
-+      const scalar_t* tmp_out_ptr = out + (offset1 + offset2) * max_num_partitions;
-+      scalar_t * out_sm_ptr_2 = reinterpret_cast<scalar_t*>(out_sm_ptr);
-+      int buffer_size = num_partitions * HEAD_SIZE / V_VEC_SIZE;
-+      for(int i = threadIdx.x; i < buffer_size; i += blockDim_x) {
-+          int offset = i * V_VEC_SIZE;
-+          V_vec reg = *reinterpret_cast<const V_vec*>(tmp_out_ptr + offset);
-+          *(V_vec *)(out_sm_ptr_2  + offset) = reg;
-+      }
-+      __syncthreads();
-+      
-+      if(threadIdx.x < HEAD_SIZE) {
-+        scalar_t * ptr_sm_out_ptr = out_sm_ptr_2 + threadIdx.x;
-+        float acc = 0.0f;
-+        int num_partitions2 = num_partitions >> 1 << 1;
-+        int j = 0;
-+        v2f vacc; vacc[0] = 0.0f; vacc[1] = 0.0f;
-+        for(; j < num_partitions2; j += 2) {
-+          v2f va; 
-+          scalar_t a0, a1;
-+          a0 = *(ptr_sm_out_ptr); ptr_sm_out_ptr += HEAD_SIZE;
-+          a1 = *(ptr_sm_out_ptr); ptr_sm_out_ptr += HEAD_SIZE;
-+          to_v2f(a0,a1,va);
-+          v2f vb; 
-+          vb[0] = shared_exp_sums[j]; vb[1] = shared_exp_sums[j + 1];
-+          vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
-+        }
-+        acc = vacc[0] + vacc[1];
-+        for (; j < num_partitions; ++j) {
-+          acc += to_float(*ptr_sm_out_ptr) * shared_exp_sums[j];
-+          ptr_sm_out_ptr += HEAD_SIZE;
-+        }
-+        from_float(out_ptr[threadIdx.x], acc);
-+      }
-+  }
++    auto stream = at::cuda::getCurrentCUDAStream();
++    macaDataType compute_type = (A.dtype() == at::ScalarType::BFloat16) ? MACA_R_16BF : MACA_R_16F;
++    mcblasFusedMoe(stream,
++                  A.data_ptr(),
++                  B.data_ptr(),
++                  C.data_ptr(),
++                  sorted_token_ids.data_ptr<int>(),
++                  expert_ids.data_ptr<int>(),
++                  num_tokens_post_padded.data_ptr<int>(),
++                  B.size(1),
++                  B.size(2),
++                  topk_ids.numel(),
++                  sorted_token_ids.size(0),
++                  A.stride(0),
++                  A.stride(1),
++                  B.stride(0),
++                  B.stride(2),
++                  B.stride(1),
++                  C.stride(1),
++                  C.stride(2),
++                  static_cast<int>(top_k),
++                  mul_routed_weight,
++                  topk_weights.data_ptr<float>(),
++                  compute_type,
++                  static_cast<int>(tileConfig));
 +}
+\ No newline at end of file
+diff --git a/csrc/moe/moe_ops.h b/csrc/moe/moe_ops.h
+index c4faef731..9dc55541a 100644
+--- a/csrc/moe/moe_ops.h
++++ b/csrc/moe/moe_ops.h
+@@ -30,6 +30,15 @@ torch::Tensor moe_wna16_gemm(torch::Tensor input, torch::Tensor output,
+                              int64_t BLOCK_SIZE_K, int64_t bit);
+ #endif
+ 
++#ifdef USE_MACA
 +
-+// Grid: (num_heads, num_seqs, 1).
-+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-+          bool IS_BLOCK_SPARSE>
-+__global__ void paged_attention_v1_kernel(
-+    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
-+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size/x, block_size, x]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size, block_size]
-+    const int num_kv_heads,               // [num_heads]
-+    const float scale,
-+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-+    const int* __restrict__ seq_lens,      // [num_seqs]
-+    const int max_num_blocks_per_seq,
-+    const float* __restrict__ alibi_slopes,  // [num_heads]
-+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-+    const float* k_scale, const float* v_scale, const int tp_rank,
-+    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-+    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-+  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-+                         KV_DTYPE, IS_BLOCK_SPARSE>(
-+      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
-+      v_cache, num_kv_heads, scale, block_tables, seq_lens,
-+      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
-+      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
-+      blocksparse_vert_stride, blocksparse_block_size,
-+      blocksparse_head_sliding_step);
-+}
++void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
++                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
++                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
++                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig);
++
++#endif
 +
-+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-+          bool IS_BLOCK_SPARSE>
-+__global__ void paged_attention_v1_32N_kernel(
-+    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
-+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size/x, block_size, x]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size, block_size]
-+    const int num_kv_heads,               // [num_heads]
-+    const float scale,
-+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-+    const int* __restrict__ seq_lens,      // [num_seqs]
-+    const int max_num_blocks_per_seq,
-+    const float* __restrict__ alibi_slopes,  // [num_heads]
-+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
-+  paged_attention_kernel_32N<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-+                         KV_DTYPE, IS_BLOCK_SPARSE>(
-+      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
-+      v_cache, num_kv_heads, scale, block_tables, seq_lens,
-+      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
-+      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
-+      blocksparse_vert_stride, blocksparse_block_size,
-+      blocksparse_head_sliding_step,max_num_partitions,num_heads);
+ bool moe_permute_unpermute_supported();
+ 
+ void shuffle_rows(const torch::Tensor& input_tensor,
+diff --git a/csrc/moe/moe_permute_unpermute_op.cu b/csrc/moe/moe_permute_unpermute_op.cu
+index 68f429fac..86aaef251 100644
+--- a/csrc/moe/moe_permute_unpermute_op.cu
++++ b/csrc/moe/moe_permute_unpermute_op.cu
+@@ -1,12 +1,13 @@
+ #include <c10/core/ScalarType.h>
+ #include <torch/all.h>
+ #include <ATen/cuda/CUDAContext.h>
+-#include "permute_unpermute_kernels/moe_permute_unpermute_kernel.h"
+-#include "permute_unpermute_kernels/dispatch.h"
+ #include "core/registration.h"
+ 
++#ifndef USE_MACA
+ // moe_permute kernels require at least CUDA 12.0
+ #if defined(CUDA_VERSION) && (CUDA_VERSION >= 12000)
++#include "permute_unpermute_kernels/moe_permute_unpermute_kernel.h"
++#include "permute_unpermute_kernels/dispatch.h"
+ 
+ void moe_permute(
+     const torch::Tensor& input,                      // [n_token, hidden]
+@@ -216,12 +217,50 @@ void moe_unpermute(const torch::Tensor& input,
+ 
+ #endif
+ 
++#else
++void moe_permute(const torch::Tensor& input, const torch::Tensor& topk_weights,
++                 torch::Tensor& topk_ids,
++                 const torch::Tensor& token_expert_indicies,
++                 const std::optional<torch::Tensor>& expert_map,
++                 int64_t n_expert, int64_t n_local_expert, int64_t topk,
++                 const std::optional<int64_t>& align_block_size,
++                 torch::Tensor& permuted_input,
++                 torch::Tensor& expert_first_token_offset,
++                 torch::Tensor& src_row_id2dst_row_id_map,
++                 torch::Tensor& m_indices) {
++  TORCH_CHECK(false, "moe_unpermute is not supported on MACA");
 +}
 +
-+// Grid: (num_heads, num_seqs, max_num_partitions).
-+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-+          bool IS_BLOCK_SPARSE,
-+          int PARTITION_SIZE>
-+__global__ void paged_attention_v2_kernel(
-+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
-+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
-+                                          // max_num_partitions]
-+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
-+                                          // max_num_partitions, head_size]
-+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size/x, block_size, x]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size, block_size]
-+    const int num_kv_heads,               // [num_heads]
-+    const float scale,
-+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-+    const int* __restrict__ seq_lens,      // [num_seqs]
-+    const int max_num_blocks_per_seq,
-+    const float* __restrict__ alibi_slopes,  // [num_heads]
-+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-+    const float* k_scale, const float* v_scale, const int tp_rank,
-+    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-+    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-+  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
-+      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
-+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
-+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
-+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
-+      blocksparse_head_sliding_step);
++void moe_unpermute(
++    const torch::Tensor& permuted_hidden_states,     // [n_token * topk, hidden]
++    const torch::Tensor& topk_weights,               //[n_token, topk]
++    const torch::Tensor& topk_ids,                   // [n_token, topk]
++    const torch::Tensor& src_row_id2dst_row_id_map,  // [n_token, topk]
++    const torch::Tensor& expert_first_token_offset,  // [n_local_expert+1]
++    int64_t n_expert, int64_t n_local_expert, int64_t topk,
++    torch::Tensor& hidden_states  // [n_token, hidden]
++) {
++  TORCH_CHECK(false, "moe_unpermute is not supported on MACA");
 +}
 +
-+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-+          bool IS_BLOCK_SPARSE,
-+          int PARTITION_SIZE>
-+__global__ void paged_attention_v2_32N_kernel(
-+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
-+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
-+                                          // max_num_partitions]
-+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
-+                                          // max_num_partitions, head_size]
-+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size/x, block_size, x]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size, block_size]
-+    const int num_kv_heads,               // [num_heads]
-+    const float scale,
-+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-+    const int* __restrict__ seq_lens,      // [num_seqs]
-+    const int max_num_blocks_per_seq,
-+    const float* __restrict__ alibi_slopes,  // [num_heads]
-+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
-+  paged_attention_kernel_32N<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
-+      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
-+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
-+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
-+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
-+      blocksparse_head_sliding_step,max_num_partitions,num_heads);
++void shuffle_rows(const torch::Tensor& input_tensor,
++                  const torch::Tensor& dst2src_map,
++                  torch::Tensor& output_tensor) {
++  TORCH_CHECK(false, "shuffle_rows is not supported on MACA");
 +}
 +
-+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-+          bool IS_BLOCK_SPARSE,
-+          int PARTITION_SIZE>
-+__global__ void paged_attention_v2_kernel_final(
-+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
-+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
-+                                          // max_num_partitions]
-+    int* __restrict__ block_count,          // [num_seqs, num_heads]
-+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
-+                                          // max_num_partitions, head_size]
-+    scalar_t* __restrict__ final_out,      // [num_seqs, num_heads, head_size]
-+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size/x, block_size, x]
-+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-+                                          // head_size, block_size]
-+    const int num_kv_heads,               // [num_heads]
-+    const float scale,
-+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-+    const int* __restrict__ seq_lens,      // [num_seqs]
-+    const int max_num_blocks_per_seq,
-+    const float* __restrict__ alibi_slopes,  // [num_heads]
-+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step,const int max_num_partitions,
-+    const int blockDim_x,
-+    const int num_heads,
-+    const int grid_dim_y, const bool count_init_once) {
-+  paged_attention_kernel_32N_final<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
-+      exp_sums, max_logits, block_count, tmp_out, final_out, q, k_cache, v_cache, num_kv_heads, scale,
-+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
-+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
-+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
-+      blocksparse_head_sliding_step,max_num_partitions,blockDim_x, num_heads,grid_dim_y,count_init_once);
++#endif // USE_MACA
++
+ bool moe_permute_unpermute_supported() {
++#ifndef USE_MACA
+ #if defined(CUDA_VERSION) && (CUDA_VERSION >= 12000)
+   return true;
+ #else
+   return false;
+ #endif
++#else
++  return false;
++#endif
  }
  
- // Grid: (num_heads, num_seqs).
-@@ -581,11 +2041,9 @@ __global__ void paged_attention_v2_reduce_kernel(
-   const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);
-   if (num_partitions == 1) {
-     // No need to reduce. Only copy tmp_out to out.
--    scalar_t* out_ptr =
--        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
--    const scalar_t* tmp_out_ptr =
--        tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
--        head_idx * max_num_partitions * HEAD_SIZE;
-+    scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
-+    const scalar_t* tmp_out_ptr = tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
-+                                          + head_idx * max_num_partitions * HEAD_SIZE;
-     for (int i = threadIdx.x; i < HEAD_SIZE; i += blockDim.x) {
-       out_ptr[i] = tmp_out_ptr[i];
-     }
-@@ -604,9 +2062,8 @@ __global__ void paged_attention_v2_reduce_kernel(
- 
-   // Load max logits to shared memory.
-   float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
--  const float* max_logits_ptr = max_logits +
--                                seq_idx * num_heads * max_num_partitions +
--                                head_idx * max_num_partitions;
-+  const float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
-+                                           + head_idx * max_num_partitions;
-   float max_logit = -FLT_MAX;
-   for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {
-     const float l = max_logits_ptr[i];
-@@ -635,11 +2092,9 @@ __global__ void paged_attention_v2_reduce_kernel(
-   max_logit = VLLM_SHFL_SYNC(max_logit, 0);
- 
-   // Load rescaled exp sums to shared memory.
--  float* shared_exp_sums =
--      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
--  const float* exp_sums_ptr = exp_sums +
--                              seq_idx * num_heads * max_num_partitions +
--                              head_idx * max_num_partitions;
-+  float* shared_exp_sums = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
-+  const float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
-+                                       + head_idx * max_num_partitions;
-   float global_exp_sum = 0.0f;
-   for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {
-     float l = shared_max_logits[i];
-@@ -652,17 +2107,14 @@ __global__ void paged_attention_v2_reduce_kernel(
-   const float inv_global_exp_sum = __fdividef(1.0f, global_exp_sum + 1e-6f);
- 
-   // Aggregate tmp_out to out.
--  const scalar_t* tmp_out_ptr =
--      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
--      head_idx * max_num_partitions * HEAD_SIZE;
--  scalar_t* out_ptr =
--      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
-+  const scalar_t* tmp_out_ptr = tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
-+                                        + head_idx * max_num_partitions * HEAD_SIZE;
-+  scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
- #pragma unroll
-   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {
-     float acc = 0.0f;
-     for (int j = 0; j < num_partitions; ++j) {
--      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *
--             inv_global_exp_sum;
-+      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] * inv_global_exp_sum;
-     }
-     from_float(out_ptr[i], acc);
-   }
-diff --git a/csrc/attention/attention_utils.cuh b/csrc/attention/attention_utils.cuh
-index 826b0edff..b56e7f67b 100644
---- a/csrc/attention/attention_utils.cuh
-+++ b/csrc/attention/attention_utils.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
-  * Adapted from
-  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
-@@ -28,7 +29,8 @@ namespace vllm {
- 
- // Q*K^T operation.
- template <int THREAD_GROUP_SIZE, typename Vec, int N>
--inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
-+//inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
-+__forceinline__ __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
-   using A_vec = typename FloatVec<Vec>::Type;
-   // Compute the parallel products for Q*K^T (treat vector lanes separately).
-   A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);
-@@ -49,7 +51,8 @@ inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
- template <typename T, int THREAD_GROUP_SIZE>
- struct Qk_dot {
-   template <typename Vec, int N>
--  static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
-+  //static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
-+  static __forceinline__ __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
-     return qk_dot_<THREAD_GROUP_SIZE>(q, k);
-   }
- };
-diff --git a/csrc/attention/dtype_float16.cuh b/csrc/attention/dtype_float16.cuh
-index 3a1815f0e..bbcf457b4 100644
---- a/csrc/attention/dtype_float16.cuh
-+++ b/csrc/attention/dtype_float16.cuh
-@@ -1,8 +1,8 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
-- * Adapted from
-- * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
-- * and
-- * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
-+ * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
-+ * and https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
-  * Copyright (c) 2023, The vLLM team.
-  * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
-  *
-@@ -22,6 +22,7 @@
+ TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {
+diff --git a/csrc/moe/moe_wna16.cu b/csrc/moe/moe_wna16.cu
+index 7b6a111c0..f1badfe23 100644
+--- a/csrc/moe/moe_wna16.cu
++++ b/csrc/moe/moe_wna16.cu
+@@ -85,9 +85,15 @@ __global__ void moe_wna16_gemm_kernel(
+     if (threadIdx.x >= BLOCK_SIZE_N || offset_n >= size_n) return;
+ 
+     float res[64];  // assume BLOCK_SIZE_M <= 64
++#ifndef USE_MACA
+     scalar_t2 res2;
+     scalar_t2 scale_f2;
+     scalar_t2 qzero_f2;
++#else
++    scalar_t2 res2{};
++    scalar_t2 scale_f2{};
++    scalar_t2 qzero_f2{};
++#endif
  
- #include "attention_generic.cuh"
- #include "dtype_float32.cuh"
-+#include "cuda_fp16.h"
+     // note that (size_n * size_k * expert_id) may greater than 2 ** 31
+     constexpr int8_t pack_factor = 32 / bit;
+@@ -190,7 +196,9 @@ __global__ void moe_wna16_gemm_kernel(
+       dequant<scalar_t2, bit>(expert_qweight_tmp[tmp_k % 4], weight_half2);
  
- #ifdef USE_ROCM
-   #include <hip/hip_fp16.h>
-@@ -32,37 +33,37 @@
- namespace vllm {
+       for (int m = 0; m < num_valid_tokens; m++) {
++#ifndef USE_MACA
+         res2 = {};
++#endif
  
- // FP16 vector types for Q, K, V.
--template <>
-+template<>
- struct Vec<uint16_t, 1> {
-   using Type = uint16_t;
- };
--template <>
-+template<>
- struct Vec<uint16_t, 2> {
-   using Type = uint32_t;
- };
--template <>
-+template<>
- struct Vec<uint16_t, 4> {
-   using Type = uint2;
- };
--template <>
-+template<>
- struct Vec<uint16_t, 8> {
-   using Type = uint4;
- };
+ #pragma unroll
+         for (int i = 0; i < 16 / bit; i++) {
+diff --git a/csrc/moe/moe_wna16_utils.h b/csrc/moe/moe_wna16_utils.h
+index 8ef03f0e6..1b74926de 100644
+--- a/csrc/moe/moe_wna16_utils.h
++++ b/csrc/moe/moe_wna16_utils.h
+@@ -81,18 +81,22 @@ class ScalarType<nv_bfloat16> {
+ template <int lut>
+ __device__ inline int lop3(int a, int b, int c) {
+   int res;
++#ifndef USE_MACA
+   asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
+                : "=r"(res)
+                : "r"(a), "r"(b), "r"(c), "n"(lut));
++#endif
+   return res;
+ }
  
- // FP32 accumulator vector types corresponding to Vec.
--template <>
-+template<>
- struct FloatVec<uint16_t> {
-   using Type = float;
- };
--template <>
-+template<>
- struct FloatVec<uint32_t> {
-   using Type = float2;
- };
--template <>
-+template<>
- struct FloatVec<uint2> {
-   using Type = Float4_;
- };
--template <>
-+template<>
- struct FloatVec<uint4> {
-   using Type = Float8_;
- };
-@@ -71,12 +72,16 @@ struct FloatVec<uint4> {
- inline __device__ uint32_t h0_h0(uint16_t a) {
- #ifndef USE_ROCM
-   uint32_t b;
--  asm volatile("mov.b32 %0, {%1, %1};" : "=r"(b) : "h"(a));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("mov.b32 %0, {%1, %1};" : "=r"(b) : "h"(a));
-+  b = a;
-+  b = b << 16 | b;
-   return b;
- #else
-   union {
--    uint32_t u32;
--    uint16_t u16[2];
-+   uint32_t u32;
-+   uint16_t u16[2];
-   } tmp;
-   tmp.u16[0] = a;
-   tmp.u16[1] = a;
-@@ -87,9 +92,14 @@ inline __device__ uint32_t h0_h0(uint16_t a) {
- inline __device__ float half_to_float(uint16_t h) {
-   float f;
- #ifndef USE_ROCM
--  asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
-+  f = __half2float(*(__half*)&h);
- #else
--  asm volatile("v_cvt_f32_f16 %0, %1;" : "=v"(f) : "v"(h));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_cvt_f32_f16 %0, %1;" : "=v"(f) : "v"(h));
- #endif
-   return f;
+ template <int start_byte, int mask>
+ __device__ inline uint32_t prmt(uint32_t a) {
+   uint32_t res;
++#ifndef USE_MACA
+   asm volatile("prmt.b32 %0, %1, %2, %3;\n"
+                : "=r"(res)
+                : "r"(a), "n"(start_byte), "n"(mask));
++#endif
+   return res;
  }
-@@ -97,7 +107,16 @@ inline __device__ float half_to_float(uint16_t h) {
- inline __device__ float2 half2_to_float2(uint32_t v) {
- #ifndef USE_ROCM
-   uint16_t lo, hi;
--  asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo), "=h"(hi) : "r"(v));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo), "=h"(hi) : "r"(v));
-+  union {
-+    uint32_t u32;
-+    uint16_t u16[2];
-+  } tmp;
-+  tmp.u32 = v;
-+  lo = tmp.u16[0];
-+  hi = tmp.u16[1];
-   return make_float2(half_to_float(lo), half_to_float(hi));
- #else
-   union {
-@@ -118,9 +137,15 @@ inline __device__ uint16_t float_to_half(float f) {
-     uint16_t u16[2];
-   } tmp;
- #ifndef USE_ROCM
--  asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f));
-+  __half __tmp = __float2half(f);
-+  tmp.u16[0] = *(uint16_t*)&__tmp;
- #else
--  asm volatile("v_cvt_f16_f32 %0, %1;\n" : "=v"(tmp.u32) : "v"(f));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_cvt_f16_f32 %0, %1;\n" : "=v"(tmp.u32) : "v"(f));
+ 
+diff --git a/csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu b/csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
+index de2c15388..9b3b055f4 100644
+--- a/csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
++++ b/csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
+@@ -1,8 +1,7 @@
+-
+-#include "moe_permute_unpermute_kernel.h"
+-
++#ifndef USE_MACA
+ // moe_permute kernels require at least CUDA 12.0
+ #if defined(CUDA_VERSION) && (CUDA_VERSION >= 12000)
++#include "moe_permute_unpermute_kernel.h"
+ 
+ // CubKeyValueSorter definition begin
+ CubKeyValueSorter::CubKeyValueSorter()
+@@ -229,3 +228,4 @@ void getMIndices(int64_t* expert_first_token_offset,
+ }
+ 
  #endif
-   return tmp.u16[0];
++#endif // USE_MACA
+diff --git a/csrc/moe/topk_softmax_kernels.cu b/csrc/moe/topk_softmax_kernels.cu
+index 10be47966..6f378e8be 100644
+--- a/csrc/moe/topk_softmax_kernels.cu
++++ b/csrc/moe/topk_softmax_kernels.cu
+@@ -390,6 +390,129 @@ __launch_bounds__(WARPS_PER_CTA* WARP_SIZE) __global__
+     }
  }
-@@ -132,12 +157,19 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
-   } tmp;
- #ifndef USE_ROCM
-   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
--  asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n"
--               : "=r"(tmp.u32)
--               : "f"(f.y), "f"(f.x));
-+    
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n" : "=r"(tmp.u32) : "f"(f.y), "f"(f.x));
-+  __half2 __tmp = __half2(__float2half(f.x), __float2half(f.y));
-+  tmp.u32 = *(uint32_t*)&__tmp;
-   #else
--  asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f.x));
--  asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[1]) : "f"(f.y));
-+    
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f.x));
-+  tmp.u16[0] = float_to_half(f.x);
-+// >>>> PTX Rejuvenate Failed <<<<
-+// asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[1]) : "f"(f.y));
-+  tmp.u16[1] = float_to_half(f.y);
-   #endif
- #else
-   tmp.u16[0] = float_to_half(f.x);
-@@ -150,9 +182,21 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
- inline __device__ uint16_t add(uint16_t a, uint16_t b) {
-   uint16_t c;
- #ifndef USE_ROCM
--  asm volatile("add.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
-+  
-+// >>>> PTX Rejuvenate Success <<<<
-+{
-+{
-+unsigned short __a=(a);
-+unsigned short __b=(b);
-+__half __d=__hadd(*(__half*)&__a,*(__half*)&__b);
-+(c)=*(unsigned short*)&__d;
-+}
+ 
++__device__ __forceinline__ float get_weight(const int64_t& v) {
++    const float* idx_and_weight = (const float*)&v;
++    return idx_and_weight[0];
 +}
 +
- #else
--  asm volatile("v_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
- #endif
-   return c;
- }
-@@ -160,9 +204,21 @@ inline __device__ uint16_t add(uint16_t a, uint16_t b) {
- inline __device__ uint32_t add(uint32_t a, uint32_t b) {
-   uint32_t c;
- #ifndef USE_ROCM
--  asm volatile("add.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
-+  
-+// >>>> PTX Rejuvenate Success <<<<
-+{
-+{
-+unsigned int __a=(a);
-+unsigned int __b=(b);
-+__half2 __d=__hadd2(*(__half2*)&__a,*(__half2*)&__b);
-+(c)=*(unsigned int*)&__d;
-+}
++template<uint32_t MASK=0xffffffff>
++__device__ __forceinline__ void warpSortDescending(float (&idx_and_weight)[2], int tid) {
++
++    int64_t val = *(int64_t*)idx_and_weight;
++    for (int width = 2; width <= WARP_SIZE; width <<=1) {
++        for (int step = width >> 1; step > 0; step >>=1) {
++            const bool is_not_final_phase = (width != WARP_SIZE);
++            const uint32_t bitmask = (tid & width);
++            const bool direction = is_not_final_phase & (bitmask == 0);
++            int64_t other_temp_val = __shfl_xor_sync(MASK, val, step);
++            int other_tid = tid ^ step;
++
++            float current_weight_bits = get_weight(val);
++            float other_weight_bits = get_weight(other_temp_val);
++            int current_index = val >> 32;
++            int other_index = other_temp_val >> 32;
++
++            bool weight_gt = other_weight_bits > current_weight_bits;
++            bool weight_eq = other_weight_bits == current_weight_bits;
++            bool index_lt = other_index < current_index;
++            bool cond = (tid < other_tid) ^ direction;
++
++            bool swap = (cond & (weight_gt | (weight_eq & index_lt))) |
++                        (!cond & ((other_weight_bits < current_weight_bits) | (weight_eq & (other_index > current_index))));
++
++            val = swap ? other_temp_val : val;
++        }
++    }
++    *(int64_t*)idx_and_weight = val;
 +}
 +
- #else
--  asm volatile("v_pk_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_pk_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
- #endif
-   return c;
- }
-@@ -205,34 +261,58 @@ inline __device__ Float8_ add(uint4 a, Float8_ fb) {
- }
- 
- // Vector multiplication.
--template <>
-+template<>
- inline __device__ uint16_t mul(uint16_t a, uint16_t b) {
-   uint16_t c;
- #ifndef USE_ROCM
--  asm volatile("mul.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
-+  
-+// >>>> PTX Rejuvenate Success <<<<
++template <typename IndType, int VPT = 4, int NUM_EXPERTS = 128, int WARPS_PER_CTA = 8, int BYTES_PER_LDG = 4, int TOPK = 8, int ROWS_PER_CTA = 2, int WARPS_PER_ROW = 4>
++__launch_bounds__(WARPS_PER_CTA* WARP_SIZE) __global__
++    void topkGatingSoftmaxDecode(const float* input, const bool* finished, float* output, const int num_rows, IndType* indices,
++        int* source_rows, const int k, const int start_expert, const int end_expert)
 +{
-+{
-+unsigned short __a=(a);
-+unsigned short __b=(b);
-+__half __d=__hmul(*(__half*)&__a,*(__half*)&__b);
-+(c)=*(unsigned short*)&__d;
-+}
++    static int THREADS_PER_ROW = WARPS_PER_ROW * WARP_SIZE;
++    const int thread_row = blockIdx.x * ROWS_PER_CTA + threadIdx.x / 128;
++    if (thread_row >= num_rows)
++    {
++        return;
++    }
++    const int warp_id = (threadIdx.x / WARP_SIZE);
++    const int tid_in_row = threadIdx.x % THREADS_PER_ROW;
++    const int row_in_block = threadIdx.x / THREADS_PER_ROW;
++    float idx_and_weight[2];
++    float idx_and_weight_2[2];
++    const bool row_is_active = finished ? !finished[thread_row] : true;
++    __shared__ float shared_experts[64][2];
++    __shared__ float max_val[8];
++    __shared__ float sum_val[8];
++    __shared__ float max_val_final[ROWS_PER_CTA];
++    __shared__ float rep[ROWS_PER_CTA];
++
++    // We finally start setting up the read pointers for each thread. First, each thread jumps to the start of the
++    // row it will read.
++    const float* thread_row_ptr = input + thread_row * NUM_EXPERTS;
++
++    float row_val = input[thread_row * NUM_EXPERTS + tid_in_row];
++    float thread_max = row_val;
++    float sum_data = expf(row_val);
++    for (int step = 1; step < WARP_SIZE; step <<= 1)
++    {
++        thread_max = max(thread_max, __shfl_xor_sync(0x0000000f, thread_max, step));
++        sum_data += __shfl_xor_sync(0x0000000f, sum_data, step);
++    }
++    if(tid_in_row % WARP_SIZE == 0) {
++        max_val[warp_id] = thread_max;
++        sum_val[warp_id] = sum_data;
++    }
++    __syncthreads();
++    if(tid_in_row < 4) {
++        float mx_v = max_val[tid_in_row + row_in_block * 4];
++        float sm_v = sum_val[tid_in_row + row_in_block * 4];
++        for(int step = 1; step < TOPK; step <<= 1 ) {
++            mx_v = max(mx_v, __shfl_xor_sync(0x0000000f, mx_v, step));
++            sm_v += __shfl_xor_sync(0x0000000f, sm_v, step);
++        }
++        if(tid_in_row == 0) {
++            max_val_final[row_in_block] = mx_v;
++            rep[row_in_block] = 1.0f / (sm_v * expf(-mx_v));
++        }
++    }
++    __syncthreads();
++    row_val = expf(row_val - max_val_final[row_in_block]) * rep[row_in_block];
++    
++    int64_t expert_id_opt = static_cast<int64_t>(tid_in_row);
++    idx_and_weight[0] = row_val;
++    idx_and_weight[1] = 0.0f;
++    *((int64_t*)idx_and_weight) |= (expert_id_opt << 32);
++    warpSortDescending<0xffffffff>(idx_and_weight, threadIdx.x);
++    if (tid_in_row % WARP_SIZE < TOPK) {
++        *((int64_t*)(shared_experts) + warp_id * TOPK + tid_in_row % WARP_SIZE) = *(int64_t*)idx_and_weight;
++    }
++    __syncthreads();
++    if(tid_in_row < WARP_SIZE){
++        *(int64_t*)idx_and_weight_2 = *((int64_t*)shared_experts + (warp_id / WARPS_PER_ROW) * WARP_SIZE + tid_in_row);
++    }
++    warpSortDescending<0xffffffff>(idx_and_weight_2, threadIdx.x);
++    if (tid_in_row < TOPK) {
++        int64_t res = *(int64_t*)idx_and_weight_2;
++        float max_val_ordered = get_weight(res);
++        int expert_id_ordered = (res >> 32);
++        max_val_ordered = max_val_ordered;//expf(max_val_ordered - thread_max_opt[threadIdx.y]) * reciprocal_row_sum;
++
++        // Add a guard to ignore experts not included by this node
++        const bool node_uses_expert = expert_id_ordered >= start_expert && expert_id_ordered < end_expert;
++        const bool should_process_row = row_is_active && node_uses_expert;
++
++        // The lead thread from each sub-group will write out the final results to global memory. (This will be a
++        // single) thread per row of the input/output matrices.
++        const int idx = TOPK * thread_row + tid_in_row;
++        output[idx] = max_val_ordered;
++        indices[idx] = should_process_row ? (expert_id_ordered - start_expert) : NUM_EXPERTS;
++        source_rows[idx] = tid_in_row * num_rows + thread_row;
++    }
 +}
 +
- #else
--  asm volatile("v_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
- #endif
-   return c;
+ namespace detail
+ {
+ // Constructs some constants needed to partition the work across threads at compile time.
+@@ -423,12 +546,50 @@ void topkGatingSoftmaxLauncherHelper(const float* input, const bool* finished, f
+         input, finished, output, num_rows, indices, source_row, k, start_expert, end_expert);
  }
  
--template <>
-+template<>
- inline __device__ uint32_t mul(uint32_t a, uint32_t b) {
-   uint32_t c;
- #ifndef USE_ROCM
--  asm volatile("mul.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
-+  
-+// >>>> PTX Rejuvenate Success <<<<
-+{
-+{
-+unsigned int __a=(a);
-+unsigned int __b=(b);
-+__half2 __d=__hmul2(*(__half2*)&__a,*(__half2*)&__b);
-+(c)=*(unsigned int*)&__d;
-+}
++template <int EXPERTS, int WARPS_PER_TB, typename IndType>
++void topkDecodeGatingSoftmaxLauncherHelper(const float* input, const bool* finished, float* output, IndType* indices,
++    int* source_row, const int num_rows, const int k, const int start_expert, const int end_expert, cudaStream_t stream) {
++    if (k == 8 and num_rows < 1024){
++        static constexpr int MAX_BYTES_PER_LDG = 16;
++        static constexpr int BYTES_PER_LDG = MIN(MAX_BYTES_PER_LDG, sizeof(float) * EXPERTS);
++        using Constants = detail::TopkConstants<EXPERTS, BYTES_PER_LDG>;
++        static constexpr int VPT = Constants::VPT;
++        static constexpr int ROWS_PER_WARP = Constants::ROWS_PER_WARP;
++
++        const int num_warps = (num_rows + ROWS_PER_WARP - 1) / ROWS_PER_WARP;
++        const int num_blocks = num_rows + 1 / 2;
++
++        dim3 block_dim(WARP_SIZE, WARPS_PER_TB);
++        topkGatingSoftmaxDecode<<<num_blocks, 256, 0, stream>>>(
++            input, finished, output, num_rows, indices, source_row, k, start_expert, end_expert);
++    } else {
++        static constexpr std::size_t MAX_BYTES_PER_LDG = 16;
++
++        static constexpr int BYTES_PER_LDG = MIN(MAX_BYTES_PER_LDG, sizeof(float) * EXPERTS);
++        using Constants = detail::TopkConstants<EXPERTS, BYTES_PER_LDG>;
++        static constexpr int VPT = Constants::VPT;
++        static constexpr int ROWS_PER_WARP = Constants::ROWS_PER_WARP;
++        const int num_warps = (num_rows + ROWS_PER_WARP - 1) / ROWS_PER_WARP;
++        const int num_blocks = (num_warps + WARPS_PER_TB - 1) / WARPS_PER_TB;
++
++        dim3 block_dim(WARP_SIZE, WARPS_PER_TB);
++        topkGatingSoftmax<VPT, EXPERTS, WARPS_PER_TB, BYTES_PER_LDG><<<num_blocks, block_dim, 0, stream>>>(
++            input, finished, output, num_rows, indices, source_row, k, start_expert, end_expert);
++    }
 +}
 +
- #else
--  asm volatile("v_pk_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_pk_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
- #endif
-   return c;
- }
- 
--template <>
-+template<>
- inline __device__ uint32_t mul(uint16_t a, uint32_t b) {
-   return mul<uint32_t, uint32_t, uint32_t>(h0_h0(a), b);
- }
- 
--template <>
-+template<>
- inline __device__ uint2 mul(uint2 a, uint2 b) {
-   uint2 c;
-   c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);
-@@ -240,7 +320,7 @@ inline __device__ uint2 mul(uint2 a, uint2 b) {
-   return c;
- }
- 
--template <>
-+template<>
- inline __device__ uint2 mul(uint16_t a, uint2 b) {
-   uint32_t s = h0_h0(a);
-   uint2 c;
-@@ -249,7 +329,7 @@ inline __device__ uint2 mul(uint16_t a, uint2 b) {
-   return c;
- }
- 
--template <>
-+template<>
- inline __device__ uint4 mul(uint4 a, uint4 b) {
-   uint4 c;
-   c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);
-@@ -259,7 +339,7 @@ inline __device__ uint4 mul(uint4 a, uint4 b) {
-   return c;
- }
- 
--template <>
-+template<>
- inline __device__ uint4 mul(uint16_t a, uint4 b) {
-   uint32_t s = h0_h0(a);
-   uint4 c;
-@@ -270,26 +350,26 @@ inline __device__ uint4 mul(uint16_t a, uint4 b) {
-   return c;
- }
- 
--template <>
-+template<>
- inline __device__ float mul(uint16_t a, uint16_t b) {
-   float fa = half_to_float(a);
-   float fb = half_to_float(b);
-   return fa * fb;
- }
- 
--template <>
-+template<>
- inline __device__ float2 mul(uint32_t a, uint32_t b) {
-   float2 fa = half2_to_float2(a);
-   float2 fb = half2_to_float2(b);
-   return mul<float2, float2, float2>(fa, fb);
- }
+ #define LAUNCH_SOFTMAX(NUM_EXPERTS, WARPS_PER_TB)                       \
+     topkGatingSoftmaxLauncherHelper<NUM_EXPERTS, WARPS_PER_TB>(         \
+         gating_output, nullptr, topk_weights, topk_indicies,            \
+         token_expert_indices, num_tokens, topk, 0, num_experts,         \
+         stream);
+ 
++#define LAUNCH_SOFTMAX_OPT(NUM_EXPERTS, WARPS_PER_TB)                   \
++    topkDecodeGatingSoftmaxLauncherHelper<NUM_EXPERTS, WARPS_PER_TB>(  \
++        gating_output, nullptr, topk_weights, topk_indicies,            \
++        token_expert_indices, num_tokens, topk, 0, num_experts,         \
++        stream);
++
+ template <typename IndType>
+ void topkGatingSoftmaxKernelLauncher(
+     const float* gating_output,
+@@ -464,7 +625,7 @@ void topkGatingSoftmaxKernelLauncher(
+             LAUNCH_SOFTMAX(64, WARPS_PER_TB);
+             break;
+         case 128:
+-            LAUNCH_SOFTMAX(128, WARPS_PER_TB);
++            LAUNCH_SOFTMAX_OPT(128, WARPS_PER_TB);
+             break;
+         case 256:
+             LAUNCH_SOFTMAX(256, WARPS_PER_TB);
+diff --git a/csrc/moe/torch_bindings.cpp b/csrc/moe/torch_bindings.cpp
+index a74eb3720..ca37d3a0d 100644
+--- a/csrc/moe/torch_bindings.cpp
++++ b/csrc/moe/torch_bindings.cpp
+@@ -88,6 +88,16 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, m) {
+   m.impl("shuffle_rows", torch::kCUDA, &shuffle_rows);
  
--template <>
-+template<>
- inline __device__ float2 mul(uint16_t a, uint32_t b) {
-   return mul<float2, uint32_t, uint32_t>(h0_h0(a), b);
+ #endif
++
++#ifdef USE_MACA
++// Fused moe in mcblas
++  m.def(
++      "fused_moe_kernel(Tensor! A, Tensor! B, Tensor! C,"
++      "Tensor! topk_weights, Tensor! topk_ids,"
++      "Tensor! sorted_token_ids, Tensor! expert_ids,"
++      "Tensor! num_tokens_post_padded, bool mul_routed_weight, int top_k, int tileConfig) -> ()");
++  m.impl("fused_moe_kernel", torch::kCUDA, &fused_moe_kernel);
++#endif
  }
  
--template <>
-+template<>
- inline __device__ Float4_ mul(uint2 a, uint2 b) {
-   Float4_ fc;
-   fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);
-@@ -297,7 +377,7 @@ inline __device__ Float4_ mul(uint2 a, uint2 b) {
-   return fc;
- }
+ REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
+diff --git a/csrc/ops.h b/csrc/ops.h
+index f02f5083a..a202a046c 100644
+--- a/csrc/ops.h
++++ b/csrc/ops.h
+@@ -182,13 +182,16 @@ torch::Tensor aqlm_dequant(
  
--template <>
-+template<>
- inline __device__ Float4_ mul(uint16_t a, uint2 b) {
-   uint32_t s = h0_h0(a);
-   Float4_ fc;
-@@ -306,7 +386,7 @@ inline __device__ Float4_ mul(uint16_t a, uint2 b) {
-   return fc;
- }
+ torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
+                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
+-                       int64_t split_k_iters);
++                       int64_t split_k_iters, torch::Tensor _temp_space, 
++                       bool dtype_bf16);
  
--template <>
-+template<>
- inline __device__ Float8_ mul(uint4 a, uint4 b) {
-   Float8_ fc;
-   fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);
-@@ -316,7 +396,7 @@ inline __device__ Float8_ mul(uint4 a, uint4 b) {
-   return fc;
- }
+ torch::Tensor awq_dequantize(torch::Tensor _kernel,
+                              torch::Tensor _scaling_factors,
+                              torch::Tensor _zeros, int64_t split_k_iters,
+                              int64_t thx, int64_t thy);
  
--template <>
-+template<>
- inline __device__ Float8_ mul(uint16_t a, uint4 b) {
-   uint32_t s = h0_h0(a);
-   Float8_ fc;
-@@ -331,13 +411,22 @@ inline __device__ Float8_ mul(uint16_t a, uint4 b) {
- inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {
-   uint32_t d;
- #ifndef USE_ROCM
--  asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
--               : "=r"(d)
--               : "r"(a), "r"(b), "r"(c));
-+  
-+// >>>> PTX Rejuvenate Success <<<<
-+{
-+{
-+unsigned int __a=(a);
-+unsigned int __b=(b);
-+unsigned int __c=(c);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(d)=*(unsigned int*)&__d;
-+}
-+}
++torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight);
 +
- #else
--  asm volatile("v_pk_fma_f16 %0, %1, %2, %3;\n"
--               : "=v"(d)
--               : "v"(a), "v"(b), "v"(c));
-+  
-+// >>>> PTX Rejuvenate Failed <<<<
-+asm volatile("v_pk_fma_f16 %0, %1, %2, %3;\n" : "=v"(d) : "v"(a), "v"(b), "v"(c));
+ torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm);
  #endif
-   return d;
- }
-@@ -431,24 +520,24 @@ inline __device__ Float8_ fma(uint16_t a, uint4 b, Float8_ fc) {
- }
- 
- // Vector sum.
--template <>
-+template<>
- inline __device__ float sum(uint16_t v) {
-   return half_to_float(v);
- }
- 
--template <>
-+template<>
- inline __device__ float sum(uint32_t v) {
-   float2 tmp = half2_to_float2(v);
-   return tmp.x + tmp.y;
- }
- 
--template <>
-+template<>
- inline __device__ float sum(uint2 v) {
-   uint32_t c = add(v.x, v.y);
-   return sum(c);
- }
- 
--template <>
-+template<>
- inline __device__ float sum(uint4 v) {
-   uint32_t c = add(v.x, v.y);
-   c = add(c, v.z);
-@@ -478,9 +567,13 @@ inline __device__ void from_float(uint4& dst, Float8_ src) {
- }
- 
- // From float16 to float32.
--inline __device__ float to_float(uint16_t u) { return half_to_float(u); }
-+inline __device__ float to_float(uint16_t u) {
-+  return half_to_float(u);
-+}
- 
--inline __device__ float2 to_float(uint32_t u) { return half2_to_float2(u); }
-+inline __device__ float2 to_float(uint32_t u) {
-+  return half2_to_float2(u);
-+}
- 
- inline __device__ Float4_ to_float(uint2 u) {
-   Float4_ tmp;
-@@ -499,6 +592,8 @@ inline __device__ Float8_ to_float(uint4 u) {
- }
  
- // Zero-out a variable.
--inline __device__ void zero(uint16_t& dst) { dst = uint16_t(0); }
-+inline __device__ void zero(uint16_t& dst) {
-+  dst = uint16_t(0);
-+}
+@@ -296,11 +299,21 @@ void static_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
+ void dynamic_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
+                                torch::Tensor& scales,
+                                std::optional<torch::Tensor> const& azp);
++                               
++void fused_silu_mul_dq_mask_quant_pack(
++    torch::Tensor& out,          
++    torch::Tensor const& input, 
++    torch::Tensor const &mask);
++
++void dynamic_scaled_int8_mask_quant( torch::Tensor& out,  torch::Tensor const& input, torch::Tensor const &mask, 
++    torch::Tensor& scales, c10::optional<torch::Tensor> const& azp);
  
--}  // namespace vllm
-+} // namespace vllm
-diff --git a/csrc/attention/dtype_fp8.cuh b/csrc/attention/dtype_fp8.cuh
-index e714e321b..c91a1bdeb 100644
---- a/csrc/attention/dtype_fp8.cuh
-+++ b/csrc/attention/dtype_fp8.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
+ torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
+                         torch::Tensor b_gptq_qzeros,
+                         torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
+-                        bool use_exllama, int64_t bit);
++                        bool use_exllama, int64_t bit, int64_t group_size, 
++                        torch::Tensor perm_space, torch::Tensor temp_space,
++			            bool dtype_bf16);
  
- #include "attention_generic.cuh"
-@@ -15,6 +16,7 @@ enum class Fp8KVCacheDataType {
-   kAuto = 0,
-   kFp8E4M3 = 1,
-   kFp8E5M2 = 2,
-+  kInt8 = 3,
- };
+ void gptq_shuffle(torch::Tensor q_weight, torch::Tensor q_perm, int64_t bit);
  
- // fp8 vector types for quantization of kv cache
-diff --git a/csrc/attention/paged_attention_v1.cu b/csrc/attention/paged_attention_v1.cu
-index 9b3a5c4b1..36dcdc47d 100644
---- a/csrc/attention/paged_attention_v1.cu
-+++ b/csrc/attention/paged_attention_v1.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
-  * Adapted from
-  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
-@@ -41,38 +42,164 @@
-           out_ptr, query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, \
-           scale, block_tables_ptr, seq_lens_ptr, max_num_blocks_per_seq,    \
-           alibi_slopes_ptr, q_stride, kv_block_stride, kv_head_stride,      \
--          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,      \
-+          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,              \
-           blocksparse_vert_stride, blocksparse_block_size,                  \
-           blocksparse_head_sliding_step);
- 
-+#define LAUNCH_PAGED_ATTENTION_V1_32N(HEAD_SIZE)                                \
-+  VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \
-+      ((void*)vllm::paged_attention_v1_32N_kernel<T, CACHE_T, HEAD_SIZE,        \
-+                                              BLOCK_SIZE, NUM_THREADS,      \
-+                                              KV_DTYPE, IS_BLOCK_SPARSE>),  \
-+      shared_mem_size);                                                     \
-+  vllm::paged_attention_v1_32N_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,        \
-+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE>   \
-+      <<<grid, block, shared_mem_size, stream>>>(                           \
-+          out_ptr, query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, \
-+          scale, block_tables_ptr, seq_lens_ptr, max_num_blocks_per_seq,    \
-+          alibi_slopes_ptr, q_stride, kv_block_stride, kv_head_stride,      \
-+          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,                      \
-+          blocksparse_vert_stride, blocksparse_block_size,                  \
-+          blocksparse_head_sliding_step, 1, num_heads);
-+
-+template< typename scalar_t>
-+__global__ void reshape_k_layout_new(scalar_t * __restrict__ k_buffer, scalar_t* k_output,int num_blocks,int num_kv_heads, int head_size,int block_size, int x,int dst_x) {
-+  int k_head_stride = head_size * block_size;
-+  scalar_t *ptr_k_buffer = k_buffer + blockIdx.x * k_head_stride;
-+  scalar_t *ptr_output = k_output + blockIdx.x * k_head_stride;
-+  for(int t = threadIdx.x; t < k_head_stride; t += blockDim.x) {
-+    int heightId = t / (block_size * dst_x);
-+    int remain = t % (block_size * dst_x);
-+    int blockId = remain / dst_x;
-+    int wId = remain % dst_x;
-+    int inId = heightId * dst_x + wId;
-+    int in_y = inId / x;
-+    int in_x = inId % x;
-+    int inIndex = in_y  * block_size * x + blockId * x + in_x;
-+    ptr_output[t] = ptr_k_buffer[inIndex];
-+  }
-+}
-+// [num_blocks, num_kv_heads, head_size, block_size] -->   [num_blocks,  num_kv_heads, block_size,head_size]
-+template<typename scalar_t>
-+__global__ void reshape_v_layout(scalar_t * __restrict__ v_buffer, scalar_t* v_output,int num_blocks,int num_kv_heads, int head_size,int block_size) {
-+      int v_block_stride = head_size * block_size * num_kv_heads;
-+      int v_head_stride = head_size * block_size;
-+      scalar_t *ptr_in = v_buffer + blockIdx.x * v_block_stride;
-+      scalar_t *ptr_output = v_output + blockIdx.x * v_block_stride;
-+      for(int t = threadIdx.x; t < v_block_stride; t += blockDim.x) {
-+        int num_kv_headIdx = t / v_head_stride;
-+        int remain = t % v_head_stride;
-+        int headId_H = remain / block_size;
-+        remain = remain % block_size;
-+        int out_idx = num_kv_headIdx * head_size * block_size + remain * head_size + headId_H;
-+        ptr_output[out_idx] = ptr_in[t];
-+      }
-+}
-+
-+template<
-+  typename CACHE_T,
-+  int BLOCK_SIZE>
-+void reshape_kv_cache(
-+  torch::Tensor& key_cache,
-+  torch::Tensor& value_cache,
-+  torch::Tensor& key_cache_new_layer,
-+  torch::Tensor& value_cache_new_layer,
-+  int num_seqs,
-+  int num_heads,
-+  int head_size,
-+  int num_kv_heads) {
-+  int kv_block_stride = key_cache.stride(0); // NU ,BLC ,HEAD, HEAD_DIM
-+  int kv_head_stride = key_cache.stride(1);
-+
-+  CACHE_T* key_cache_ptr = reinterpret_cast<CACHE_T*>(key_cache.data_ptr());
-+  CACHE_T* value_cache_ptr = reinterpret_cast<CACHE_T*>(value_cache.data_ptr());
-+  CACHE_T* key_cache_tmp = reinterpret_cast<CACHE_T*>(key_cache_new_layer.data_ptr());
-+  CACHE_T* value_cache_tmp = reinterpret_cast<CACHE_T*>(value_cache_new_layer.data_ptr());
-+
-+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+diff --git a/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
+new file mode 100644
+index 000000000..614a6251e
+--- /dev/null
++++ b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
+@@ -0,0 +1,473 @@
++// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
++#pragma once
++#include "../gptq/Hgemm_common.cuh"
++#include "awq_4bits.cuh"
++#include "maca_fp16.h"
 +
-+  reshape_k_layout_new<CACHE_T><<<dim3(key_cache.size(0)*num_kv_heads,1,1),dim3(256,1,1),0,stream>>>(key_cache_ptr,key_cache_tmp,key_cache.size(0),num_kv_heads,head_size,BLOCK_SIZE,8,16);
-+  reshape_v_layout<CACHE_T><<<dim3(key_cache.size(0),1,1),dim3(256,1,1),0,stream>>>(value_cache_ptr,value_cache_tmp,key_cache.size(0),num_kv_heads,head_size,BLOCK_SIZE);
-+}
-+#define CALL_RESHAPE_LAUNCHER(CACHE_T, BLOCK_SIZE)       \
-+  reshape_kv_cache<CACHE_T, BLOCK_SIZE>( \
-+    key_cache,                                                               \
-+    value_cache,                                                             \
-+    key_cache_new_layer,                                                     \
-+    value_cache_new_layer,                                                   \
-+    num_seqs,\
-+    num_heads,\
-+    head_size,\
-+    num_kv_heads);
-+
-+#define CALL_RESHAPE_BLOCK_SIZE(CACHE_T) \
-+  switch (block_size) {                                               \
-+    case 8:                                                           \
-+      CALL_RESHAPE_LAUNCHER(CACHE_T, 8);          \
-+      break;                                                          \
-+    case 16:                                                          \
-+      CALL_RESHAPE_LAUNCHER(CACHE_T, 16);         \
-+      break;                                                          \
-+    case 32:                                                          \
-+      CALL_RESHAPE_LAUNCHER(CACHE_T, 32);         \
-+      break;                                                          \
-+    default:                                                          \
-+      TORCH_CHECK(false, "Unsupported block size: ", block_size);     \
-+      break;                                                          \
-+  }
-+void page_reshape_kv_cache(
-+  torch::Tensor& key_cache,       // [num_blocks, num_heads, head_size/x, block_size, x]
-+  torch::Tensor& value_cache,     // [num_blocks, num_heads, head_size, block_size]
-+  torch::Tensor& key_cache_new_layer, //[num_blocks, num_heads, head_size/16, block_size, 16]
-+  torch::Tensor& value_cache_new_layer,//[num_blocks, num_heads, block_size, head_size]
-+  int64_t num_seqs,
-+  int64_t num_heads,
-+  int64_t head_size,
-+  int64_t num_kv_heads,               // [num_heads]
-+  int64_t block_size,
-+  const std::string& kv_cache_dtype) {
-+  if (kv_cache_dtype == "auto") {
-+    if (sizeof(key_cache.dtype())==4) {
-+      CALL_RESHAPE_BLOCK_SIZE(float);
-+    } else if (sizeof(key_cache.dtype()) == 2) {
-+      CALL_RESHAPE_BLOCK_SIZE(uint16_t);
-+    } else {
-+      TORCH_CHECK(false, "Unsupported data type: ", key_cache.dtype());
-+    }
-+  }  else {
-+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
-+  }
-+}
++#define input_type __half
++#define output_type __half
++#define scalar_type float
++#define acc_type float
 +
++#define SEL0 0x01000504
++#define SEL1 0x03020706
 +
- // TODO(woosuk): Tune NUM_THREADS.
- template <typename T, typename CACHE_T, int BLOCK_SIZE,
-           vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,
--          int NUM_THREADS = 128>
-+          int NUM_THREADS = 256>
- void paged_attention_v1_launcher(
-     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int num_kv_heads, float scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
--    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
--    torch::Tensor& v_scale, const int tp_rank,
--    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
--    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-+    const c10::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
-+    torch::Tensor& v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step) {
-   int num_seqs = query.size(0);
-   int num_heads = query.size(1);
-   int head_size = query.size(2);
-   int max_num_blocks_per_seq = block_tables.size(1);
--  int q_stride = query.stride(0);
--  int kv_block_stride = key_cache.stride(0);
-+  int q_stride = query.stride(0);  //num head head_dim 
-+  int kv_block_stride = key_cache.stride(0);   // NU ,BLC ,HEAD, HEAD_DIM
-   int kv_head_stride = key_cache.stride(1);
- 
--  [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
-+  int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
-   assert(head_size % thread_group_size == 0);
-+  assert((head_size & 7) == 0);
- 
-   // NOTE: alibi_slopes is optional.
--  const float* alibi_slopes_ptr =
--      alibi_slopes
--          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
--          : nullptr;
-+  const float* alibi_slopes_ptr = alibi_slopes ?
-+    reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
-+    : nullptr;
- 
-   T* out_ptr = reinterpret_cast<T*>(out.data_ptr());
-   T* query_ptr = reinterpret_cast<T*>(query.data_ptr());
-@@ -84,10 +211,12 @@ void paged_attention_v1_launcher(
-   const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());
- 
-   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
--  int padded_max_seq_len =
--      DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;
-+  int padded_max_seq_len = DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;
-   int logits_size = padded_max_seq_len * sizeof(float);
--  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
-+  int V_VEC_SIZE = 16 / sizeof(CACHE_T); 
-+  int NUM_V_VECS_PER_THREAD = head_size / V_VEC_SIZE; 
-+  int NUM_COLS_PER_ITER = MAX(WARP_SIZE / NUM_V_VECS_PER_THREAD, 1); 
-+  int outputs_size = NUM_WARPS * head_size * sizeof(float) * NUM_COLS_PER_ITER;
-   // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len
-   // Keep that in sync with the logic here!
-   int shared_mem_size = std::max(logits_size, outputs_size);
-@@ -104,7 +233,7 @@ void paged_attention_v1_launcher(
-       LAUNCH_PAGED_ATTENTION_V1(32);
-       break;
-     case 64:
--      LAUNCH_PAGED_ATTENTION_V1(64);
-+      LAUNCH_PAGED_ATTENTION_V1_32N(64);
-       break;
-     case 80:
-       LAUNCH_PAGED_ATTENTION_V1(80);
-@@ -119,13 +248,15 @@ void paged_attention_v1_launcher(
-       LAUNCH_PAGED_ATTENTION_V1(120);
-       break;
-     case 128:
--      LAUNCH_PAGED_ATTENTION_V1(128);
-+      LAUNCH_PAGED_ATTENTION_V1_32N(128);
-       break;
-+    case 160:
-+      LAUNCH_PAGED_ATTENTION_V1(160);
-     case 192:
-       LAUNCH_PAGED_ATTENTION_V1(192);
-       break;
-     case 256:
--      LAUNCH_PAGED_ATTENTION_V1(256);
-+      LAUNCH_PAGED_ATTENTION_V1_32N(256);
-       break;
-     default:
-       TORCH_CHECK(false, "Unsupported head size: ", head_size);
-@@ -178,10 +309,9 @@ void paged_attention_v1(
-     torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]
-     torch::Tensor& seq_lens,      // [num_seqs]
-     int64_t block_size, int64_t max_seq_len,
--    const std::optional<torch::Tensor>& alibi_slopes,
--    const std::string& kv_cache_dtype, torch::Tensor& k_scale,
--    torch::Tensor& v_scale, const int64_t tp_rank,
--    const int64_t blocksparse_local_blocks,
-+    const c10::optional<torch::Tensor>& alibi_slopes,
-+    const std::string& kv_cache_dtype, torch::Tensor& k_scale, torch::Tensor& v_scale,
-+    const int64_t tp_rank, const int64_t blocksparse_local_blocks,
-     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
-     const int64_t blocksparse_head_sliding_step) {
-   const bool is_block_sparse = (blocksparse_vert_stride > 1);
-@@ -193,4 +323,4 @@ void paged_attention_v1(
- #undef WARP_SIZE
- #undef MAX
- #undef MIN
--#undef DIVIDE_ROUND_UP
-\ No newline at end of file
-+#undef DIVIDE_ROUND_UP
-diff --git a/csrc/attention/paged_attention_v2.cu b/csrc/attention/paged_attention_v2.cu
-index 9935359e0..ebacff6b2 100644
---- a/csrc/attention/paged_attention_v2.cu
-+++ b/csrc/attention/paged_attention_v2.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
-  * Adapted from
-  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
-@@ -37,7 +38,7 @@
-           exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \
-           value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \
-           seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \
--          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,  \
-+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,          \
-           blocksparse_local_blocks, blocksparse_vert_stride,                   \
-           blocksparse_block_size, blocksparse_head_sliding_step);              \
-   vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \
-@@ -46,18 +47,52 @@
-           out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
-           max_num_partitions);
- 
-+#define LAUNCH_PAGED_ATTENTION_V2_32N(HEAD_SIZE)                                   \
-+  vllm::paged_attention_v2_32N_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
-+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \
-+                                  PARTITION_SIZE>                              \
-+      <<<grid, block, shared_mem_size, stream>>>(                              \
-+          exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \
-+          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \
-+          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \
-+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,                  \
-+          blocksparse_local_blocks, blocksparse_vert_stride,                   \
-+          blocksparse_block_size, blocksparse_head_sliding_step, max_num_partitions, num_heads);              \
-+  vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \
-+                                         PARTITION_SIZE>                       \
-+      <<<reduce_grid, block, reduce_shared_mem_size, stream>>>(                \
-+          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
-+          max_num_partitions);
-+
-+#define LAUNCH_PAGED_ATTENTION_V2_FINAL(HEAD_SIZE)                                   \
-+  vllm::paged_attention_v2_kernel_final<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
-+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \
-+                                  PARTITION_SIZE>                              \
-+      <<<grid, block, shared_mem_size, stream>>>(                              \
-+          exp_sums_ptr, max_logits_ptr, block_count_ptr, tmp_out_ptr, out_ptr, query_ptr, key_cache_ptr, \
-+          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,                   \
-+          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,         \
-+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,                        \
-+          blocksparse_local_blocks, blocksparse_vert_stride,                        \
-+          blocksparse_block_size, blocksparse_head_sliding_step,max_num_partitions, \
-+          NUM_THREADS,                                                              \
-+          num_heads,                                                                \
-+          num_seqs,                                                                 \
-+          count_init_once                                                           \
-+	  );
-+
- template <typename T, typename CACHE_T, int BLOCK_SIZE,
-           vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,
--          int NUM_THREADS = 128, int PARTITION_SIZE = 512>
-+          int NUM_THREADS = 256, int PARTITION_SIZE = 512>
- void paged_attention_v2_launcher(
-     torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,
--    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
-+    torch::Tensor& tmp_out, torch::Tensor& block_count, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int num_kv_heads, float scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
--    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
--    torch::Tensor& v_scale, const int tp_rank,
--    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
--    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-+    const c10::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
-+    torch::Tensor& v_scale, const int tp_rank, const int blocksparse_local_blocks,
-+    const int blocksparse_vert_stride, const int blocksparse_block_size,
-+    const int blocksparse_head_sliding_step, const bool count_init_once) {
-   int num_seqs = query.size(0);
-   int num_heads = query.size(1);
-   int head_size = query.size(2);
-@@ -66,14 +101,14 @@ void paged_attention_v2_launcher(
-   int kv_block_stride = key_cache.stride(0);
-   int kv_head_stride = key_cache.stride(1);
- 
--  [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
-+  int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
-   assert(head_size % thread_group_size == 0);
-+  assert((head_size & 7) == 0);
- 
-   // NOTE: alibi_slopes is optional.
--  const float* alibi_slopes_ptr =
--      alibi_slopes
--          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
--          : nullptr;
-+  const float* alibi_slopes_ptr = alibi_slopes ?
-+    reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
-+    : nullptr;
- 
-   T* out_ptr = reinterpret_cast<T*>(out.data_ptr());
-   float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());
-@@ -86,15 +121,20 @@ void paged_attention_v2_launcher(
-   int* seq_lens_ptr = seq_lens.data_ptr<int>();
-   const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());
-   const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());
-+  int* block_count_ptr = block_count.data_ptr<int>();
- 
-   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
-   int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);
-   int logits_size = PARTITION_SIZE * sizeof(float);
--  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
-+  int V_VEC_SIZE = 16 / sizeof(CACHE_T);
-+  int NUM_V_VECS_PER_THREAD = head_size / V_VEC_SIZE;
-+  int NUM_COLS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_THREAD;
-+  int outputs_size = NUM_WARPS * head_size * sizeof(float) * NUM_COLS_PER_ITER;
- 
-   // For paged attention v2 kernel.
-   dim3 grid(num_heads, num_seqs, max_num_partitions);
--  int shared_mem_size = std::max(logits_size, outputs_size);
-+  int shared_mem_size = std::max(std::max(logits_size, outputs_size) + (2 * NUM_WARPS + 512 + (NUM_WARPS<<1))*sizeof(float),
-+     (2 * max_num_partitions + 2 * NUM_WARPS + max_num_partitions * head_size)*sizeof(float));
-   // For paged attention v2 reduce kernel.
-   dim3 reduce_grid(num_heads, num_seqs);
-   int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
-@@ -110,7 +150,7 @@ void paged_attention_v2_launcher(
-       LAUNCH_PAGED_ATTENTION_V2(32);
-       break;
-     case 64:
--      LAUNCH_PAGED_ATTENTION_V2(64);
-+      LAUNCH_PAGED_ATTENTION_V2_FINAL(64);
-       break;
-     case 80:
-       LAUNCH_PAGED_ATTENTION_V2(80);
-@@ -125,13 +165,15 @@ void paged_attention_v2_launcher(
-       LAUNCH_PAGED_ATTENTION_V2(120);
-       break;
-     case 128:
--      LAUNCH_PAGED_ATTENTION_V2(128);
-+      LAUNCH_PAGED_ATTENTION_V2_FINAL(128);
-       break;
-+    case 160:
-+      LAUNCH_PAGED_ATTENTION_V2(160);
-     case 192:
-       LAUNCH_PAGED_ATTENTION_V2(192);
-       break;
-     case 256:
--      LAUNCH_PAGED_ATTENTION_V2(256);
-+      LAUNCH_PAGED_ATTENTION_V2_FINAL(256);
-       break;
-     default:
-       TORCH_CHECK(false, "Unsupported head size: ", head_size);
-@@ -142,11 +184,11 @@ void paged_attention_v2_launcher(
- #define CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, KV_DTYPE, IS_BLOCK_SPARSE)   \
-   paged_attention_v2_launcher<T, CACHE_T, BLOCK_SIZE, KV_DTYPE,               \
-                               IS_BLOCK_SPARSE>(                               \
--      out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,      \
-+      out, exp_sums, max_logits, tmp_out, block_count, query, key_cache, value_cache,      \
-       num_kv_heads, scale, block_tables, seq_lens, max_seq_len, alibi_slopes, \
-       k_scale, v_scale, tp_rank, blocksparse_local_blocks,                    \
-       blocksparse_vert_stride, blocksparse_block_size,                        \
--      blocksparse_head_sliding_step);
-+      blocksparse_head_sliding_step, count_init_once);
- 
- #define CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE) \
-   if (is_block_sparse) {                                                   \
-@@ -179,6 +221,7 @@ void paged_attention_v2(
-     torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]
-     torch::Tensor&
-         tmp_out,  // [num_seqs, num_heads, max_num_partitions, head_size]
-+    torch::Tensor& block_count,     // [num_seqs, num_heads]
-     torch::Tensor& query,  // [num_seqs, num_heads, head_size]
-     torch::Tensor&
-         key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]
-@@ -189,18 +232,16 @@ void paged_attention_v2(
-     torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]
-     torch::Tensor& seq_lens,      // [num_seqs]
-     int64_t block_size, int64_t max_seq_len,
--    const std::optional<torch::Tensor>& alibi_slopes,
--    const std::string& kv_cache_dtype, torch::Tensor& k_scale,
--    torch::Tensor& v_scale, const int64_t tp_rank,
--    const int64_t blocksparse_local_blocks,
-+    const c10::optional<torch::Tensor>& alibi_slopes,
-+    const std::string& kv_cache_dtype, torch::Tensor& k_scale, torch::Tensor& v_scale,
-+    const int64_t tp_rank, const int64_t blocksparse_local_blocks,
-     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
--    const int64_t blocksparse_head_sliding_step) {
-+    const int64_t blocksparse_head_sliding_step, bool count_init_once) {
-   const bool is_block_sparse = (blocksparse_vert_stride > 1);
--  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,
--                             CALL_V2_LAUNCHER_BLOCK_SIZE)
-+  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype, CALL_V2_LAUNCHER_BLOCK_SIZE)
- }
- 
- #undef WARP_SIZE
- #undef MAX
- #undef MIN
--#undef DIVIDE_ROUND_UP
-\ No newline at end of file
-+#undef DIVIDE_ROUND_UP
-diff --git a/csrc/cache.h b/csrc/cache.h
-index cf4a65c29..7e157c0fc 100644
---- a/csrc/cache.h
-+++ b/csrc/cache.h
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #include <torch/all.h>
-@@ -24,6 +25,16 @@ void reshape_and_cache(torch::Tensor& key, torch::Tensor& value,
-                        const std::string& kv_cache_dtype,
-                        torch::Tensor& k_scale, torch::Tensor& v_scale);
- 
-+void reshape_and_cache_new(
-+		torch::Tensor& key,
-+		torch::Tensor& value,
-+                torch::Tensor& key_cache,
-+		torch::Tensor& value_cache,
-+                torch::Tensor& slot_mapping,
-+                const std::string& kv_cache_dtype,
-+                const double k_scale,
-+		const double v_scale);
-+
- void reshape_and_cache_flash(torch::Tensor& key, torch::Tensor& value,
-                              torch::Tensor& key_cache,
-                              torch::Tensor& value_cache,
-diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu
-index 0960888d1..98a5858e1 100644
---- a/csrc/cache_kernels.cu
-+++ b/csrc/cache_kernels.cu
-@@ -1,9 +1,11 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include <torch/all.h>
- #include <ATen/cuda/CUDAContext.h>
- #include <c10/cuda/CUDAGuard.h>
- 
- #include "cuda_compat.h"
- #include "dispatch_utils.h"
-+#include "quantization/fused_kernels/quant_conversions.cuh"
- 
- #ifdef USE_ROCM
-   #include "quantization/fp8/amd/quant_utils.cuh"
-@@ -260,6 +262,118 @@ __global__ void reshape_and_cache_kernel(
-   }
- }
- 
-+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
-+__global__ void reshape_and_cache_kernel_layout(
-+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
-+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
-+    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,
-+                                         // block_size, x]
-+    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,
-+                                         // block_size]
-+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
-+    const int key_stride, const int value_stride, const int num_heads,
-+    const int head_size, const int block_size, const int x,
-+    const float kv_scale) {
-+  const int64_t token_idx = blockIdx.x;
-+  const int64_t slot_idx = slot_mapping[token_idx];
-+  if (slot_idx < 0) {
-+    // Padding token that should be ignored.
-+    return;
-+  }
++#define SWIZZLE_INDEX(index, phase) (index ^ phase)
++#define ONE_DIM_INDEX(x, y, lda) (y * lda + x)
++#define CLAMP(value, bound, align) (value < bound ? value : bound - align)
 +
-+  const int64_t block_idx = slot_idx / block_size;
-+  const int64_t block_offset = slot_idx % block_size;
-+
-+  const int n = num_heads * head_size;
-+  for (int i = threadIdx.x; i < n; i += blockDim.x) {
-+    const int64_t src_key_idx = token_idx * key_stride + i;
-+    const int64_t src_value_idx = token_idx * value_stride + i;
-+
-+    const int head_idx = i / head_size;
-+    const int head_offset = i % head_size;
-+    const int x_idx = head_offset / x;
-+    const int x_offset = head_offset % x;
-+    const int64_t tgt_key_idx =
-+        block_idx * num_heads * (head_size / x) * block_size * x +
-+        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
-+        block_offset * x + x_offset;
-+    const int64_t tgt_value_idx =
-+        block_idx * num_heads * head_size * block_size +
-+        head_idx * head_size * block_size + block_offset * head_size +
-+        head_offset;
-+    scalar_t tgt_key = key[src_key_idx];
-+    scalar_t tgt_value = value[src_value_idx];
-+    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
-+      key_cache[tgt_key_idx] = tgt_key;
-+      value_cache[tgt_value_idx] = tgt_value;
-+    } else {
-+      key_cache[tgt_key_idx] =
-+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, kv_scale);
-+      value_cache[tgt_value_idx] =
-+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, kv_scale);
++#define LDG_B                                                       \
++    {                                                               \
++        ldg_b128_bsm_async(b_sts, dB[0], pred_n0, true);            \
++        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1, true); \
 +    }
-+  }
-+}
-+
-+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
-+__global__ void reshape_and_cache_kernel_layout_opt(
-+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
-+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
-+    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,
-+                                         // block_size, x]
-+    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,
-+                                         // block_size]
-+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
-+    const int key_stride, const int value_stride, const int num_heads,
-+    const int head_size, const int block_size, const int x,
-+    const float kv_scale) {
-+  const int64_t token_idx = blockIdx.x;
-+  const int64_t slot_idx = slot_mapping[token_idx];
-+  if (slot_idx < 0) {
-+    // Padding token that should be ignored.
-+    return;
-+  }
 +
-+  const int64_t block_idx = slot_idx / block_size;
-+  const int64_t block_offset = slot_idx % block_size;
-+  const int n = num_heads * head_size / 8;
-+  for (int t = threadIdx.x; t < n; t += blockDim.x) {
-+    int i = t << 3;
-+    const int64_t src_key_idx = token_idx * key_stride + i;
-+    const int64_t src_value_idx = token_idx * value_stride + i;
-+
-+    const int head_idx = i / head_size;
-+    const int head_offset = i % head_size;
-+    const int x_idx = head_offset / x;
-+    const int x_offset = head_offset % x;
-+
-+    const int64_t tgt_key_idx =
-+        block_idx * num_heads * head_size * block_size +
-+        head_idx * head_size * block_size + x_idx * block_size * x +
-+        block_offset * x + x_offset;
-+    const int64_t tgt_value_idx =
-+        block_idx * num_heads * head_size * block_size +
-+        head_idx * head_size * block_size + block_offset * head_size +
-+        head_offset;
-+
-+    *(float4*)(key_cache + tgt_key_idx) = *(float4*)(key + src_key_idx);
-+    *(float4*)(value_cache + tgt_value_idx) = *(float4*)(value + src_value_idx);
-+   /*
-+    scalar_t tgt_key = key[src_key_idx];
-+    scalar_t tgt_value = value[src_value_idx];
-+    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
-+      key_cache[tgt_key_idx] = tgt_key;
-+      value_cache[tgt_value_idx] = tgt_value;
-+    } else {
-+      key_cache[tgt_key_idx] =
-+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, kv_scale);
-+      value_cache[tgt_value_idx] =
-+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, kv_scale);
++#define LDG_B_HEAD                                                            \
++    {                                                                         \
++        bool pred_k = rowB_swizzle < ktail;                                   \
++        ldg_b128_bsm_async(b_sts, dB[0], pred_n0 &&pred_k, true);             \
++        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1 && pred_k, true); \
 +    }
-+    */
-+  }
-+}
-+
- template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
- __global__ void reshape_and_cache_flash_kernel(
-     const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
-@@ -336,6 +450,8 @@ __global__ void concat_and_cache_mla_kernel(
-           block_idx * block_stride + block_offset * entry_stride + i + offset;
-       if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
-         dst[dst_idx] = src[src_idx];
-+      } else if (kv_dt == Fp8KVCacheDataType::kInt8) {
-+        dst[dst_idx] = scaled_quant_to_int8<scalar_t>(src[src_idx], scale);
-       } else {
-         dst[dst_idx] =
-             fp8::scaled_convert<cache_t, scalar_t, kv_dt>(src[src_idx], *scale);
-@@ -392,6 +508,83 @@ void reshape_and_cache(
-                              CALL_RESHAPE_AND_CACHE)
- }
- 
-+#define CALL_RESHAPE_AND_CACHE_LAYOUT(KV_T, CACHE_T, KV_DTYPE)               \
-+  vllm::reshape_and_cache_kernel_layout<KV_T, CACHE_T, KV_DTYPE>             \
-+      <<<grid, block, 0, stream>>>(                                   \
-+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
-+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
-+          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \
-+          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \
-+          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \
-+          num_heads, head_size, block_size, x, kv_scale);
-+
-+#define CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(KV_T, CACHE_T, KV_DTYPE)               \
-+  vllm::reshape_and_cache_kernel_layout_opt<KV_T, CACHE_T, KV_DTYPE>             \
-+      <<<grid, block, 0, stream>>>(                                   \
-+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
-+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
-+          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \
-+          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \
-+          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \
-+          num_heads, head_size, block_size, x, kv_scale);
-+
-+void reshape_and_cache_new(
-+    torch::Tensor& key,    // [num_tokens, num_heads, head_size]
-+    torch::Tensor& value,  // [num_tokens, num_heads, head_size]
-+    torch::Tensor&
-+        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]
-+    torch::Tensor&
-+        value_cache,  // [num_blocks, num_heads, head_size, block_size]
-+    torch::Tensor& slot_mapping,  // [num_tokens]
-+    const std::string& kv_cache_dtype, const double kv_scale, const double v_scale) {
-+  int num_tokens = key.size(0);
-+  int num_heads = key.size(1);
-+  int head_size = key.size(2);
-+  int block_size = key_cache.size(3);
-+  int x = key_cache.size(4);
-+
-+  int key_stride = key.stride(0);
-+  int value_stride = value.stride(0);
-+
-+  dim3 grid(num_tokens);
-+  dim3 block(std::min(num_heads * head_size, 512));
-+  const at::cuda::OptionalCUDAGuard device_guard(device_of(key));
-+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 +
-+  if (kv_cache_dtype == "auto") {
-+    if (key.dtype() == at::ScalarType::Float) {
-+      CALL_RESHAPE_AND_CACHE_LAYOUT(float, float, vllm::Fp8KVCacheDataType::kAuto);
-+    } else if (key.dtype() == at::ScalarType::Half) {
-+      if((x & 7) == 0 && (head_size & 7) == 0) {
-+        CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(half, half, vllm::Fp8KVCacheDataType::kAuto);
-+      } else {
-+        CALL_RESHAPE_AND_CACHE_LAYOUT(half, half, vllm::Fp8KVCacheDataType::kAuto);
-+      }
-+    }
-+    else if (key.dtype() == at::ScalarType::BFloat16) {
-+      if((x & 7) == 0 && (head_size & 7) == 0) {
-+        CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);
-+      } else {
-+        CALL_RESHAPE_AND_CACHE_LAYOUT(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);
-+      }
++#define MMA_ABC1                                                             \
++    {                                                                        \
++        for (int index_n = 0; index_n < 2; ++index_n)                        \
++        {                                                                    \
++            for (int index_m = 0; index_m < 8; ++index_m)                    \
++            {                                                                \
++                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
++                                cast_b64(rgB)[ONE_DIM_INDEX(0, index_n, 2)], \
++                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
++            }                                                                \
++        }                                                                    \
 +    }
-+  }
-+ /*
-+  else if (kv_cache_dtype == "fp8_e5m2") {
-+    if (key.dtype() == at::ScalarType::Float) {
-+      CALL_RESHAPE_AND_CACHE_LAYOUT(float, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
-+    } else if (key.dtype() == at::ScalarType::Half) {
-+      CALL_RESHAPE_AND_CACHE_LAYOUT(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
-+    } else if (key.dtype() == at::ScalarType::BFloat16) {
-+      CALL_RESHAPE_AND_CACHE_LAYOUT(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
++
++#define MMA_ABC2                                                             \
++    {                                                                        \
++        for (int index_n = 0; index_n < 2; ++index_n)                        \
++        {                                                                    \
++            for (int index_m = 0; index_m < 8; ++index_m)                    \
++            {                                                                \
++                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
++                                cast_b64(rgB)[ONE_DIM_INDEX(1, index_n, 2)], \
++                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
++            }                                                                \
++        }                                                                    \
 +    }
-+  }
-+  */
-+  else {
-+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
-+  }
-+}
 +
- // KV_T is the stored data type of kv-cache.
- // CACHE_T is the data type of key and value tensors.
- // KV_DTYPE is the real data type of kv-cache.
-diff --git a/csrc/core/scalar_type.hpp b/csrc/core/scalar_type.hpp
-index c2ae554c9..f743d3d31 100644
---- a/csrc/core/scalar_type.hpp
-+++ b/csrc/core/scalar_type.hpp
-@@ -1,6 +1,8 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- // For TORCH_CHECK
-+#include <variant>
- #include <torch/library.h>
- 
- namespace vllm {
-diff --git a/csrc/cpu/attention.cpp b/csrc/cpu/attention.cpp
-index b9764056e..0f75e6657 100644
---- a/csrc/cpu/attention.cpp
-+++ b/csrc/cpu/attention.cpp
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include "cpu_types.hpp"
- 
- namespace {
-@@ -386,7 +387,7 @@ void paged_attention_v1_impl_launcher(
-     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int num_kv_heads, float scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
--    const std::optional<torch::Tensor>& alibi_slopes) {
-+    const c10::optional<torch::Tensor>& alibi_slopes) {
-   int num_seqs = query.size(0);
-   int num_heads = query.size(1);
-   int head_size = query.size(2);
-@@ -459,7 +460,7 @@ void paged_attention_v1(
-     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
--    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
-+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
-     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
-     torch::Tensor& v_scale, const int64_t tp_rank,
-     const int64_t blocksparse_local_blocks,
-@@ -702,7 +703,7 @@ void paged_attention_v2_impl_launcher(
-     torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int num_kv_heads, float scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int block_size,
--    int max_seq_len, const std::optional<torch::Tensor>& alibi_slopes) {
-+    int max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes) {
-   int num_seqs = query.size(0);
-   int num_heads = query.size(1);
-   int head_size = query.size(2);
-@@ -781,7 +782,7 @@ void paged_attention_v2(
-     torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
--    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
-+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
-     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
-     torch::Tensor& v_scale, const int64_t tp_rank,
-     const int64_t blocksparse_local_blocks,
-@@ -795,4 +796,4 @@ void paged_attention_v2(
-                                  CALL_V2_KERNEL_LAUNCHER_BLOCK_SIZE(scalar_t);
-                                  CPU_KERNEL_GUARD_OUT(paged_attention_v2_impl)
-                                });
--}
-\ No newline at end of file
-+}
-diff --git a/csrc/cpu/quant.cpp b/csrc/cpu/quant.cpp
-index 33b163783..27f300897 100644
---- a/csrc/cpu/quant.cpp
-+++ b/csrc/cpu/quant.cpp
-@@ -1,3 +1,5 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
++#define LDS_B                                          \
++    {                                                  \
++        cast_b128(rgB)[0] = cast_b128(b_lds)[0];       \
++        cast_b128(rgB)[1] = cast_b128(b_lds)[16 * 16]; \
++    }
 +
- #include "cpu_types.hpp"
- #include "dnnl_helper.hpp"
- 
-@@ -561,7 +563,7 @@ void int8_scaled_mm_azp(torch::Tensor& c,        // [M, OC], row-major
- void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
-                               const torch::Tensor& input,  // [..., hidden_size]
-                               const torch::Tensor& scale,
--                              std::optional<torch::Tensor> const& azp) {
-+                              c10::optional<torch::Tensor> const& azp) {
-   CPU_KERNEL_GUARD_IN(static_scaled_int8_quant)
-   TORCH_CHECK(input.is_contiguous());
-   TORCH_CHECK(out.is_contiguous());
-@@ -590,7 +592,7 @@ void dynamic_scaled_int8_quant(
-     torch::Tensor& out,          // [..., hidden_size]
-     const torch::Tensor& input,  // [..., hidden_size]
-     torch::Tensor& scale,        // [..., 1]
--    std::optional<torch::Tensor> const& azp) {
-+    c10::optional<torch::Tensor> const& azp) {
-   CPU_KERNEL_GUARD_IN(dynamic_scaled_int8_quant)
-   TORCH_CHECK(input.is_contiguous());
-   TORCH_CHECK(out.is_contiguous());
-diff --git a/csrc/cuda_compat.h b/csrc/cuda_compat.h
-index 82e55613d..753909903 100644
---- a/csrc/cuda_compat.h
-+++ b/csrc/cuda_compat.h
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #ifdef USE_ROCM
-@@ -47,3 +48,17 @@
-   #define VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(FUNC, VAL) \
-     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)
- #endif
++#define PERM_C(C2perm)                             \
++    {                                              \
++        Float4VecType C_tmp[8];                    \
++        float *ptri = (float *)C2perm;             \
++        float *ptro = (float *)C_tmp;              \
++        for (int j = 0; j < 4; ++j)                \
++        {                                          \
++            for (int i = 0; i < 8; ++i)            \
++            {                                      \
++                ptro[j * 8 + i] = ptri[j + i * 4]; \
++            }                                      \
++        }                                          \
++        for (int i = 0; i < 8; ++i)                \
++        {                                          \
++            C2perm[i] = C_tmp[i];                  \
++        }                                          \
++    }
 +
-+#define MXWARP_SIZE 64
-+#ifndef USE_ROCM
-+  #define MXVLLM_SHFL_SYNC(var, src_lane) __shfl_sync(uint64_t(-1), var, src_lane)
-+#else
-+  #define MXVLLM_SHFL_SYNC(var, src_lane) __shfl(var, src_lane)
-+#endif
++#define STS_C(phase)                                            \
++    {                                                           \
++        cast_b128(c_sts[0])[0] = cast_b128(rgC)[0 + phase * 8]; \
++        cast_b128(c_sts[1])[0] = cast_b128(rgC)[1 + phase * 8]; \
++        cast_b128(c_sts[2])[0] = cast_b128(rgC)[2 + phase * 8]; \
++        cast_b128(c_sts[3])[0] = cast_b128(rgC)[3 + phase * 8]; \
++        cast_b128(c_sts[4])[0] = cast_b128(rgC)[4 + phase * 8]; \
++        cast_b128(c_sts[5])[0] = cast_b128(rgC)[5 + phase * 8]; \
++        cast_b128(c_sts[6])[0] = cast_b128(rgC)[6 + phase * 8]; \
++        cast_b128(c_sts[7])[0] = cast_b128(rgC)[7 + phase * 8]; \
++    }
 +
-+#ifndef USE_ROCM
-+  #define MXVLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor_sync(uint64_t(-1), var, lane_mask)
-+#else
-+  #define MXVLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor(var, lane_mask)
-+#endif
++#define REDUCE_C(phase)                                                   \
++    {                                                                     \
++        cast_b128(rgC)[0 + phase * 8] = cast_b128(c_lds[0])[0];           \
++        cast_b128(rgC)[1 + phase * 8] = cast_b128(c_lds[1])[0];           \
++        cast_b128(rgC)[2 + phase * 8] = cast_b128(c_lds[0])[1 * 16 * 32]; \
++        cast_b128(rgC)[3 + phase * 8] = cast_b128(c_lds[1])[1 * 16 * 32]; \
++        cast_b128(rgC)[4 + phase * 8] = cast_b128(c_lds[0])[2 * 16 * 32]; \
++        cast_b128(rgC)[5 + phase * 8] = cast_b128(c_lds[1])[2 * 16 * 32]; \
++        cast_b128(rgC)[6 + phase * 8] = cast_b128(c_lds[0])[3 * 16 * 32]; \
++        cast_b128(rgC)[7 + phase * 8] = cast_b128(c_lds[1])[3 * 16 * 32]; \
++        float *reduc_c = reinterpret_cast<float *>(rgC);                  \
++        for (int loop = 0; loop < 8; ++loop)                              \
++        {                                                                 \
++            float acc = 0;                                                \
++            acc += reduc_c[loop + phase * 32 + 0 * 8];                    \
++            acc += reduc_c[loop + phase * 32 + 1 * 8];                    \
++            acc += reduc_c[loop + phase * 32 + 2 * 8];                    \
++            acc += reduc_c[loop + phase * 32 + 3 * 8];                    \
++            reduc_c[loop + phase * 8] = acc;                              \
++        }                                                                 \
++    }
 +
-diff --git a/csrc/cumem_allocator.cpp b/csrc/cumem_allocator.cpp
-index e8555d853..3297fdb36 100644
---- a/csrc/cumem_allocator.cpp
-+++ b/csrc/cumem_allocator.cpp
-@@ -1,10 +1,9 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- // A CUDAPluggableAllocator based on cumem* APIs.
- // Important: allocation size, CUdeviceptr and CUmemGenericAllocationHandle*
- // need to be unsigned long long
- #include <iostream>
- 
--extern "C" {
--
- #define PY_SSIZE_T_CLEAN
- #include <Python.h>
- 
-@@ -12,6 +11,8 @@ extern "C" {
- #include <cuda_runtime_api.h>
- #include <cuda.h>
- 
-+extern "C" {
++template <Operation_t TransA,
++          Operation_t TransB,
++          bool IsBetaZero,
++          int tileM,
++          int tileN,
++          int tileK,
++          bool splitk = false,
++          bool Swap = false>
++__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_awq_4bit(int m,
++                                                                           int n,
++                                                                           int k,
++                                                                           const scalar_type alpha,
++                                                                           const scalar_type beta,
++                                                                           const quant_packed_type *dA_input,
++                                                                           int lda,
++                                                                           const input_type *dB_input,
++                                                                           int ldb,
++                                                                           output_type *dC_input,
++                                                                           output_type *dC_output,
++                                                                           int ldc,
++                                                                           quant_packed_type *d_zeros,
++                                                                           input_type *d_scales,
++                                                                           int splitk_iters = 1,
++                                                                           acc_type * d_acc_tmp=nullptr)
++{
++    __shared__ uint8_t smem_base[0x8000];
++    int bidx = Swap ? blockIdx.y : blockIdx.x;
++    int bidy = Swap ? blockIdx.x : blockIdx.y;
++    int bidz = blockIdx.z;
++    int tid = threadIdx.x;
 +
- #define CUDA_CHECK(condition)                                                  \
-   do {                                                                         \
-     CUresult error = condition;                                                \
-diff --git a/csrc/custom_all_reduce.cuh b/csrc/custom_all_reduce.cuh
-index b9df4ed16..acfe506e9 100644
---- a/csrc/custom_all_reduce.cuh
-+++ b/csrc/custom_all_reduce.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #include <cuda.h>
-@@ -38,9 +39,10 @@ struct Signal {
-   alignas(128) FlagType peer_counter[2][kMaxBlocks][8];
- };
- 
--struct __align__(16) RankData {
--  const void* __restrict__ ptrs[8];
--};
-+//struct __align__(16) RankData {
-+//  const void* __restrict__ ptrs[8];
-+//};
-+struct __align__(16) RankData { const void* ptrs[8]; };
- 
- struct __align__(16) RankSignals {
-   Signal* signals[8];
-@@ -135,6 +137,7 @@ DINLINE O downcast(array_t<float, O::size> val) {
- }
- 
- static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
-+#ifdef MX_MACA
- #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
-   asm volatile("st.release.sys.global.u32 [%1], %0;" ::"r"(flag),
-                "l"(flag_addr));
-@@ -142,10 +145,12 @@ static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
-   asm volatile("membar.sys; st.volatile.global.u32 [%1], %0;" ::"r"(flag),
-                "l"(flag_addr));
- #endif
-+#endif
- }
- 
- static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
-   FlagType flag;
-+#ifdef MX_MACA
- #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
-   asm volatile("ld.acquire.sys.global.u32 %0, [%1];"
-                : "=r"(flag)
-@@ -156,17 +161,22 @@ static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
-                : "l"(flag_addr));
- #endif
-   return flag;
-+#endif
- }
- 
- static DINLINE void st_flag_volatile(FlagType* flag_addr, FlagType flag) {
-+#ifdef MX_MACA
-   asm volatile("st.volatile.global.u32 [%1], %0;" ::"r"(flag), "l"(flag_addr));
-+#endif
- }
- 
- static DINLINE FlagType ld_flag_volatile(FlagType* flag_addr) {
-   FlagType flag;
-+#ifdef MX_MACA
-   asm volatile("ld.volatile.global.u32 %0, [%1];"
-                : "=r"(flag)
-                : "l"(flag_addr));
-+#endif
-   return flag;
- }
- 
-@@ -281,7 +291,7 @@ __global__ void __launch_bounds__(512, 1)
- 
- using IPC_KEY = std::array<uint8_t, sizeof(cudaIpcMemHandle_t)>;
- static_assert(sizeof(IPC_KEY) == sizeof(cudaIpcMemHandle_t));
--static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));
-+//static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));
- 
- class CustomAllreduce {
-  public:
-diff --git a/csrc/cutlass_extensions/common.hpp b/csrc/cutlass_extensions/common.hpp
-index febc4eccd..c0a9dd531 100644
---- a/csrc/cutlass_extensions/common.hpp
-+++ b/csrc/cutlass_extensions/common.hpp
-@@ -1,6 +1,7 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
--#include "cutlass/cutlass.h"
-+#include "mctlass/mctlass.h"
- #include <climits>
- #include "cuda_runtime.h"
- #include <iostream>
-@@ -10,9 +11,9 @@
-  */
- #define CUTLASS_CHECK(status)                       \
-   {                                                 \
--    cutlass::Status error = status;                 \
--    TORCH_CHECK(error == cutlass::Status::kSuccess, \
--                cutlassGetStatusString(error));     \
-+    mctlass::Status error = status;                 \
-+    TORCH_CHECK(error == mctlass::Status::kSuccess, \
-+                mctlassGetStatusString(error));     \
-   }
- 
- /**
-@@ -43,9 +44,9 @@ int32_t get_sm_version_num();
- template <typename Kernel>
- struct enable_sm90_or_later : Kernel {
-   template <typename... Args>
--  CUTLASS_DEVICE void operator()(Args&&... args) {
-+  MCTLASS_DEVICE void operator()(Args&&... args) {
- #if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 900
-     Kernel::operator()(std::forward<Args>(args)...);
- #endif
-   }
--};
-\ No newline at end of file
-+};
-diff --git a/csrc/mamba/causal_conv1d/causal_conv1d.cu b/csrc/mamba/causal_conv1d/causal_conv1d.cu
-index f0e5533bc..beeea22a4 100644
---- a/csrc/mamba/causal_conv1d/causal_conv1d.cu
-+++ b/csrc/mamba/causal_conv1d/causal_conv1d.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- // clang-format off
- // adapted from https://github.com/Dao-AILab/causal-conv1d/blob/main/csrc/causal_conv1d_fwd.cu 
- // and https://github.com/Dao-AILab/causal-conv1d/blob/main/csrc/causal_conv1d_update.cu
-@@ -53,12 +54,12 @@ void set_conv_params_fwd(ConvParamsBase &params,
-                          const at::Tensor x,
-                          const at::Tensor weight,
-                          const at::Tensor out,
--                         const std::optional<at::Tensor>& bias,
-+                         const c10::optional<at::Tensor>& bias,
-                          bool silu_activation,
-                          int64_t pad_slot_id,
--                         const std::optional<at::Tensor>& query_start_loc = std::nullopt,
--                         const std::optional<at::Tensor>& cache_indices = std::nullopt,
--                         const std::optional<at::Tensor>& has_initial_state = std::nullopt) {
-+                         const c10::optional<at::Tensor>& query_start_loc = at::nullopt,
-+                         const c10::optional<at::Tensor>& cache_indices = at::nullopt,
-+                         const c10::optional<at::Tensor>& has_initial_state = at::nullopt) {
- 
-     // Reset the parameters
-     memset(&params, 0, sizeof(params));
-@@ -93,11 +94,11 @@ void set_conv_params_fwd(ConvParamsBase &params,
- 
- 
- void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
--                  const std::optional<at::Tensor> &bias_,
--                  const std::optional<at::Tensor> &conv_states,
--                  const std::optional<at::Tensor> &query_start_loc,
--                  const std::optional<at::Tensor> &cache_indices,
--                  const std::optional<at::Tensor> &has_initial_state,
-+                  const c10::optional<at::Tensor> &bias_,
-+                  const c10::optional<at::Tensor> &conv_states,
-+                  const c10::optional<at::Tensor> &query_start_loc,
-+                  const c10::optional<at::Tensor> &cache_indices,
-+                  const c10::optional<at::Tensor> &has_initial_state,
-                   bool silu_activation,
-                  // used to identify padding entries if cache_indices provided
-                  // in case of padding, the kernel will return early
-@@ -183,7 +184,7 @@ void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
- 
-     // Otherwise the kernel will be launched from cuda:0 device
-     // Cast to char to avoid compiler warning about narrowing
--    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
-+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
-     auto stream = at::cuda::getCurrentCUDAStream().stream();
-     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_fwd", [&] {
-             causal_conv1d_fwd_cuda<input_t, weight_t>(params, stream);
-@@ -194,10 +195,10 @@ void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
- void causal_conv1d_update(const at::Tensor &x,
-                      const at::Tensor &conv_state,
-                      const at::Tensor &weight,
--                     const std::optional<at::Tensor> &bias_,
-+                     const c10::optional<at::Tensor> &bias_,
-                      bool silu_activation,
--                     const std::optional<at::Tensor> &cache_seqlens_,
--                     const std::optional<at::Tensor> &conv_state_indices_,
-+                     const c10::optional<at::Tensor> &cache_seqlens_,
-+                     const c10::optional<at::Tensor> &conv_state_indices_,
-                      // used to identify padding entries if cache_indices provided
-                      // in case of padding, the kernel will return early
-                      int64_t pad_slot_id) {
-@@ -276,7 +277,7 @@ void causal_conv1d_update(const at::Tensor &x,
- 
-     // Otherwise the kernel will be launched from cuda:0 device
-     // Cast to char to avoid compiler warning about narrowing
--    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
-+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
-     auto stream = at::cuda::getCurrentCUDAStream().stream();
-     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_update", [&] {
-             causal_conv1d_update_cuda<input_t, weight_t>(params, stream);
-diff --git a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
-index bd0a34119..a479369fb 100644
---- a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
-+++ b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- // clang-format off
- // adapted from https://github.com/state-spaces/mamba/blob/main/csrc/selective_scan/selective_scan_fwd_kernel.cuh
- #include <torch/all.h>
-@@ -332,7 +333,7 @@ void selective_scan_fwd_launch(SSMParamsBase &params, cudaStream_t stream) {
- template<typename input_t, typename weight_t>
- void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {
- 
--    #ifndef USE_ROCM
-+    #ifdef MX_MACA
-         if (params.seqlen <= 128) {           
-             selective_scan_fwd_launch<32, 4, input_t, weight_t>(params, stream);
-         } else if (params.seqlen <= 256) {
-@@ -402,14 +403,14 @@ void set_ssm_params_fwd(SSMParamsBase &params,
-                         const torch::Tensor out,
-                         const torch::Tensor z,
-                         const torch::Tensor out_z,
--                        const std::optional<at::Tensor>& D,
--                        const std::optional<at::Tensor>& delta_bias,
-+                        const c10::optional<at::Tensor>& D,
-+                        const c10::optional<at::Tensor>& delta_bias,
-                         const torch::Tensor ssm_states,
-                         bool has_z, 
-                         bool delta_softplus,
--                        const std::optional<at::Tensor>& query_start_loc,
--                        const std::optional<at::Tensor>& cache_indices,
--                        const std::optional<at::Tensor>& has_initial_state,
-+                        const c10::optional<at::Tensor>& query_start_loc,
-+                        const c10::optional<at::Tensor>& cache_indices,
-+                        const c10::optional<at::Tensor>& has_initial_state,
-                         bool varlen,
-                         int64_t pad_slot_id) {
- 
-@@ -504,13 +505,13 @@ void set_ssm_params_fwd(SSMParamsBase &params,
- 
- void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,
-                   const torch::Tensor &A, const torch::Tensor &B, const torch::Tensor &C,
--                  const std::optional<torch::Tensor> &D_,
--                  const std::optional<torch::Tensor> &z_,
--                  const std::optional<torch::Tensor> &delta_bias_,
-+                  const c10::optional<torch::Tensor> &D_,
-+                  const c10::optional<torch::Tensor> &z_,
-+                  const c10::optional<torch::Tensor> &delta_bias_,
-                   bool delta_softplus,
--                  const std::optional<torch::Tensor> &query_start_loc,
--                  const std::optional<torch::Tensor> &cache_indices,
--                  const std::optional<torch::Tensor> &has_initial_state,
-+                  const c10::optional<torch::Tensor> &query_start_loc,
-+                  const c10::optional<torch::Tensor> &cache_indices,
-+                  const c10::optional<torch::Tensor> &has_initial_state,
-                   const torch::Tensor &ssm_states,
-                   // used to identify padding entries if cache_indices provided
-                   // in case of padding, the kernel will return early
-@@ -649,7 +650,7 @@ void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,
-     
-     // Otherwise the kernel will be launched from cuda:0 device
-     // Cast to char to avoid compiler warning about narrowing
--    at::cuda::CUDAGuard device_guard{(char)u.get_device()};
-+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(u.get_device())};
-     auto stream = at::cuda::getCurrentCUDAStream().stream();
-     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(u.scalar_type(), "selective_scan_fwd", [&] {
-         selective_scan_fwd_cuda<input_t, weight_t>(params, stream);
-diff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu
-index 01dac4044..66ef3c4bf 100644
---- a/csrc/moe/moe_align_sum_kernels.cu
-+++ b/csrc/moe/moe_align_sum_kernels.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include <torch/all.h>
- #include <ATen/cuda/CUDAContext.h>
- #include <c10/cuda/CUDAGuard.h>
-@@ -197,6 +198,153 @@ __global__ void moe_align_block_size_global_mem_kernel(
-   }
- }
- 
++    uint64_t arowstride = TransA == OP_N ? 1 : lda;
++    uint64_t acolstride = TransA == OP_N ? lda : 1;
++    uint64_t browstride = TransB == OP_N ? 1 : ldb;
++    uint64_t bcolstride = TransB == OP_N ? ldb : 1;
 +
-+__device__ __forceinline__ int32_t ScanWarp2(int32_t val) {
-+  int32_t lane = threadIdx.x & 31;
-+  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1, 8);
-+  if (lane >= 1) {
-+    val += tmp;
-+  }
-+  tmp = __shfl_up_sync(0xffffffff, val, 2, 8);
-+  if (lane >= 2) {
-+    val += tmp;
-+  }
-+  tmp = __shfl_up_sync(0xffffffff, val, 4, 8);
-+  if (lane >= 4) {
-+    val += tmp;
-+  }
-+  return val;
-+}
++    const int malign = 8;
++    const int nalign = 1;
++    const int kalign = 8;
++    int align_m = (m + malign - 1) / malign * malign;
++    int align_n = (n + nalign - 1) / nalign * nalign;
++    int align_k = (k + kalign - 1) / kalign * kalign;
 +
-+__device__ __forceinline__ int32_t ScanWarp(int32_t val) {
-+  int32_t lane = threadIdx.x & 31;
-+  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1);
-+  if (lane >= 1) {
-+    val += tmp;
-+  }
-+  tmp = __shfl_up_sync(0xffffffff, val, 2);
-+  if (lane >= 2) {
-+    val += tmp;
-+  }
-+  tmp = __shfl_up_sync(0xffffffff, val, 4);
-+  if (lane >= 4) {
-+    val += tmp;
-+  }
-+  tmp = __shfl_up_sync(0xffffffff, val, 8);
-+  if (lane >= 8) {
-+    val += tmp;
-+  }
-+  tmp = __shfl_up_sync(0xffffffff, val, 16);
-+  if (lane >= 16) {
-+    val += tmp;
-+  }
-+  return val;
-+}
++    int k_num = (align_k + splitk_iters - 1) / splitk_iters;
++    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
++    int k_begin = bidz * k_num;
++    int k_end = (bidz + 1) * k_num;
++    k_end = k_end > align_k ? align_k : k_end;
++    k_num = k_end - k_begin;
++    int ktail = k_num % tileK;
++    int kloop = k_begin;
++    if (k_begin > align_k)
++    {
++        return;
++    }    
 +
 +
-+template <typename scalar_t>
-+__global__ void opt_sgl_moe_align_block_size_kernel(
-+    scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids,
-+    int32_t* expert_ids, int32_t* total_tokens_post_pad, int32_t num_experts,
-+    int32_t block_size, size_t numel, int32_t* cumsum) {
-+  __shared__ int32_t shared_counts[32][8];
-+  __shared__ int32_t local_offsets[256];
-+  __shared__ int32_t cum_test[257];
-+  __shared__ int32_t blocksum[32];
++    int slot = tid / 64;
++    int lane = tid & 63;
++    int m64d16 = lane / 16;
++    int m64m16 = lane % 16;
++    output_type *c_base_i = dC_input;
++    output_type *c_base_o = dC_output;
 +
-+  const int warp_id = threadIdx.x / 32;
-+  const int lane_id = threadIdx.x % 32;
-+  const int experts_per_warp = 8;
-+  const int my_expert_start = warp_id * experts_per_warp;
 +
-+  int32_t idx = threadIdx.x;
-+  if(idx < num_experts){
-+    shared_counts[idx / 8][idx % 8] = 0;
-+    cum_test[0] = 0;
-+  }
-+  __syncthreads();
++    quant_packed_type rga[8];
++    b128VecType rgb[2];
 +
-+  const size_t tokens_per_thread = CEILDIV(numel, blockDim.x);
-+  const size_t start_idx = threadIdx.x * tokens_per_thread;
++    b64VecType rgA[16], rgB[4], rgCi[4];
++    Float4VecType rgC[16];
++    uint32_t rgZeros[1];
++    b128VecType rgScales[1];
 +
-+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
-+    int expert_id = topk_ids[i];
-+    int warp_idx = expert_id / experts_per_warp;
-+    int expert_offset = expert_id % experts_per_warp;
-+    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
-+  }
-+  __syncthreads();
++    quant_packed_type *dA[1];
++    input_type *dB[2];
 +
-+  int val = 0;
-+  if (threadIdx.x < 256) {
-+    int32_t final_val = 0;
-+    int row = idx / 8;
-+    int line = idx % 8;
-+    val = shared_counts[row][line];
-+    val = CEILDIV(val, block_size) * block_size;
-+  }
-+  __syncthreads();
++    // ldg A/B head
 +
-+  if(idx < 256) {
-+    int tmp = 0;
-+    val = ScanWarp(val);
-+    if(lane_id == 31) {
-+      blocksum[warp_id] = val;
-+    }
-+  }
-+  __syncthreads();
++    int rowA = m64m16 * 8;
++    int colA = m64d16 * 8 + slot * 32;
++    uint current_m = bidx * tileM + rowA;
++    bool pred_m = current_m < align_m;
++    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m / PACK_RATIO) * (uint64_t)(arowstride) +
++            (uint64_t)(colA + k_begin) * (uint64_t)(acolstride / PACK_RATIO);
 +
-+  if(warp_id == 0 && lane_id < 8) {
-+    int res = blocksum[lane_id];
-+    blocksum[lane_id] = ScanWarp2(res);
-+  }
-+  __syncthreads();
++    int colB = m64d16 + slot * 4;
++    int rowB = m64m16 * 8;
++    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
++    int current_n = bidy * tileN + colB;
++    bool pred_n0 = current_n < align_n;
++    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
++            (uint64_t)(current_n) * (uint64_t)bcolstride;
 +
-+  if(threadIdx.x < 256 && warp_id > 0){
-+    val += blocksum[warp_id - 1];
-+  }
-+  __syncthreads();
++    current_n += 16;
++    bool pred_n1 = current_n < align_n;
++    dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
++            (uint64_t)(current_n) * (uint64_t)bcolstride;
 +
-+  if(idx < 256){
-+    cum_test[idx + 1] = val;
-+  }
-+  __syncthreads();
++    // ldgB to BSM need swizzle
++    input_type *b_sts = (input_type *)smem_base;
++    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
 +
-+  if (threadIdx.x < num_experts) {
-+    for (int i = cum_test[threadIdx.x]; i < cum_test[threadIdx.x + 1];
-+         i += block_size) {
-+      expert_ids[i / block_size] = threadIdx.x;
-+    }
-+    local_offsets[threadIdx.x] = cum_test[threadIdx.x];
-+    if(threadIdx.x == 0){
-+      *total_tokens_post_pad = cum_test[num_experts];
-+    }
-+  }
-+  __syncthreads();
++    // lds need swizzle
++    input_type *b_lds = (input_type *)smem_base;
++    int colB_lds = m64m16;
++    int rowB_lds = m64d16 * 8 + slot * 32;
++    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
 +
-+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
-+    int32_t expert_id = topk_ids[i];
-+    int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);
-+    sorted_token_ids[rank_post_pad] = i;
-+  }
-+}
++    // ldg zeros and scales
++    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO) * (kloop / tileK) + bidx * (tileM / PACK_RATIO) + tid;
++    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
++    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
++    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + tid * 2;
++    quant_packed_type *lds_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
++    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + m64m16 * 8;
 +
-+template <typename scalar_t, int TOPK>
-+__global__ void moe_sum_kernel(
-+    scalar_t* __restrict__ out,          // [..., d]
-+    const scalar_t* __restrict__ input,  // [..., topk, d]
-+    const int d) {
-+  const int64_t token_idx = blockIdx.x;
-+  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {
-+    scalar_t x = 0.0;
-+#pragma unroll
-+    for (int k = 0; k < TOPK; ++k) {
-+      x += VLLM_LDG(&input[token_idx * TOPK * d + k * d + idx]);
-+    }
-+    out[token_idx * d + idx] = x;
-+  }
-+}
++    // clear C registers
++    rgC[0] = 0;
++    rgC[1] = 0;
++    rgC[2] = 0;
++    rgC[3] = 0;
++    rgC[4] = 0;
++    rgC[5] = 0;
++    rgC[6] = 0;
++    rgC[7] = 0;
++    rgC[8] = 0;
++    rgC[9] = 0;
++    rgC[10] = 0;
++    rgC[11] = 0;
++    rgC[12] = 0;
++    rgC[13] = 0;
++    rgC[14] = 0;
++    rgC[15] = 0;
 +
- // taken from
- // https://github.com/sgl-project/sglang/commit/ded9fcd09a43d5e7d5bb31a2bc3e9fc21bf65d2a
- template <typename scalar_t>
-@@ -263,21 +411,6 @@ __global__ void sgl_moe_align_block_size_kernel(
-   }
- }
- 
--template <typename scalar_t, int TOPK>
--__global__ void moe_sum_kernel(
--    scalar_t* __restrict__ out,          // [..., d]
--    const scalar_t* __restrict__ input,  // [..., topk, d]
--    const int d) {
--  const int64_t token_idx = blockIdx.x;
--  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {
--    scalar_t x = 0.0;
--#pragma unroll
--    for (int k = 0; k < TOPK; ++k) {
--      x += VLLM_LDG(&input[token_idx * TOPK * d + k * d + idx]);
--    }
--    out[token_idx * d + idx] = x;
--  }
--}
- 
- }  // namespace moe
- }  // namespace vllm
-@@ -388,7 +521,7 @@ void sgl_moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
-         torch::Tensor cumsum_buffer =
-             torch::empty({num_experts + 1}, options_int);
- 
--        auto kernel = vllm::moe::sgl_moe_align_block_size_kernel<scalar_t>;
-+        auto kernel = vllm::moe::opt_sgl_moe_align_block_size_kernel<scalar_t>;
-         kernel<<<1, 1024, 0, stream>>>(
-             topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
-             experts_ids.data_ptr<int32_t>(),
-diff --git a/csrc/moe/moe_ops.cpp b/csrc/moe/moe_ops.cpp
-new file mode 100644
-index 000000000..a4a44e075
---- /dev/null
-+++ b/csrc/moe/moe_ops.cpp
-@@ -0,0 +1,66 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
++    int aincr = tileK * acolstride / PACK_RATIO;
++    int bincr = tileK * browstride;
 +
-+#include "moe_ops.h"
++    if (k_num > ktail)
++    {
++        LDG_B;
++        LDG_ZEROS;
++        LDG_SCALES;
++        LDG_A;
 +
-+#include <ATen/cuda/CUDAContext.h>
-+#include <mcblas.h>
-+#include <maca_fp16.h>
++        dA[0] += aincr;
++        dB[0] += bincr;
++        dB[1] += bincr;
 +
-+mcblasStatus_t mcblasFusedMoe(mcStream_t stream,
-+                              const void *a_ptr,
-+                              const void *b_ptr,
-+                              void *c_ptr,
-+                              const int *sorted_token_ids_ptr,
-+                              const int *expert_ids_ptr,
-+                              const int *num_tokens_post_padded,
-+                              int N,
-+                              int K,
-+                              int num_valid_tokens,
-+                              int sorted_token_ids_len,
-+                              int stride_am,
-+                              int stride_ak,
-+                              int stride_be,
-+                              int stride_bk,
-+                              int stride_bn,
-+                              int stride_cm,
-+                              int stride_cn,
-+                              int top_k,
-+                              bool mul_routed_weight,
-+                              const float *topk_weights_ptr,
-+                              macaDataType compute_type,
-+                              int tileConfig = 0);
++        // lds_A, need swizzle
++        arrive_gvmcnt(8);
++        barrier();
++        LDS_B;
++        LDS_ZEROS;
++        LDS_SCALES;
++        arrive_bsmcnt(0);
++        barrier();
 +
-+void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
-+                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
-+                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
-+                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig) {
-+    
-+    assert(topk_weights.stride(1) == 1);
-+    assert(sorted_token_ids.stride(0) == 1);
++        ldg_zeros_offset += lda / PACK_RATIO;
++        ldg_scales_offset += lda;
 +
-+    auto stream = at::cuda::getCurrentCUDAStream();
-+    macaDataType compute_type = (A.dtype() == at::ScalarType::BFloat16) ? MACA_R_16BF : MACA_R_16F;
-+    mcblasFusedMoe(stream,
-+                  A.data_ptr(),
-+                  B.data_ptr(),
-+                  C.data_ptr(),
-+                  sorted_token_ids.data_ptr<int>(),
-+                  expert_ids.data_ptr<int>(),
-+                  num_tokens_post_padded.data_ptr<int>(),
-+                  B.size(1),
-+                  B.size(2),
-+                  topk_ids.numel(),
-+                  sorted_token_ids.size(0),
-+                  A.stride(0),
-+                  A.stride(1),
-+                  B.stride(0),
-+                  B.stride(2),
-+                  B.stride(1),
-+                  C.stride(1),
-+                  C.stride(2),
-+                  static_cast<int>(top_k),
-+                  mul_routed_weight,
-+                  topk_weights.data_ptr<float>(),
-+                  compute_type,
-+                  static_cast<int>(tileConfig));
-+}
-diff --git a/csrc/moe/moe_ops.h b/csrc/moe/moe_ops.h
-index 66bb5f41b..702c03f5f 100644
---- a/csrc/moe/moe_ops.h
-+++ b/csrc/moe/moe_ops.h
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #include <torch/all.h>
-@@ -18,3 +19,8 @@ void sgl_moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
-                               torch::Tensor sorted_token_ids,
-                               torch::Tensor experts_ids,
-                               torch::Tensor num_tokens_post_pad);
++// KLOOP
++#pragma unroll 1
++        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
++        {
++            // load next B
++            LDG_B;
++            LDG_ZEROS;
++            LDG_SCALES;
 +
-+void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
-+                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
-+                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
-+                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig);
-diff --git a/csrc/moe/torch_bindings.cpp b/csrc/moe/torch_bindings.cpp
-index 8540633dc..5d379755f 100644
---- a/csrc/moe/torch_bindings.cpp
-+++ b/csrc/moe/torch_bindings.cpp
-@@ -1,3 +1,5 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
++            // MMA_ABC1;
++            arrive_gvmcnt(4);
++            PERM_A;
++            LDG_A;
++            MMA;
 +
- #include "core/registration.h"
- #include "moe_ops.h"
- 
-@@ -43,6 +45,14 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, m) {
-       " -> Tensor");
-   // conditionally compiled so impl registration is in source file
- #endif
++            // sts && lds
++            arrive_gvmcnt(8);
++            barrier();
++            LDS_B;
++            LDS_ZEROS;
++            LDS_SCALES;
++            arrive_bsmcnt(0);
++            barrier();
 +
-+// Fused moe in mcblas
-+  m.def(
-+      "fused_moe_kernel(Tensor! A, Tensor! B, Tensor! C,"
-+      "Tensor! topk_weights, Tensor! topk_ids,"
-+      "Tensor! sorted_token_ids, Tensor! expert_ids,"
-+      "Tensor! num_tokens_post_padded, bool mul_routed_weight, int top_k, int tileConfig) -> ()");
-+  m.impl("fused_moe_kernel", torch::kCUDA, &fused_moe_kernel);
- }
- 
- REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
-diff --git a/csrc/ops.h b/csrc/ops.h
-index e39d4ef31..44d1d5d0b 100644
---- a/csrc/ops.h
-+++ b/csrc/ops.h
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #include <optional>
-@@ -33,7 +34,7 @@ void paged_attention_v1(
-     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
--    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
-+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
-     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
-     torch::Tensor& v_scale, const int64_t tp_rank,
-     const int64_t blocksparse_local_blocks,
-@@ -42,15 +43,27 @@ void paged_attention_v1(
- 
- void paged_attention_v2(
-     torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,
--    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
-+    torch::Tensor& tmp_out, torch::Tensor& block_count, torch::Tensor& query, torch::Tensor& key_cache,
-     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
-     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
--    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
-+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
-     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
-     torch::Tensor& v_scale, const int64_t tp_rank,
-     const int64_t blocksparse_local_blocks,
-     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
--    const int64_t blocksparse_head_sliding_step);
-+    const int64_t blocksparse_head_sliding_step, bool count_init_once);
-+
-+void page_reshape_kv_cache(
-+    torch::Tensor& key_cache,
-+    torch::Tensor& value_cache,
-+    torch::Tensor& key_cache_new_layer,
-+    torch::Tensor& value_cache_new_layer,
-+    int64_t num_seqs,
-+    int64_t num_heads,
-+    int64_t head_size,
-+    int64_t num_kv_heads,
-+    int64_t block_size,
-+    const std::string& kv_cache_dtype);
- 
- void rms_norm(torch::Tensor& out, torch::Tensor& input, torch::Tensor& weight,
-               double epsilon);
-@@ -73,8 +86,8 @@ void rms_norm_dynamic_per_token_quant(torch::Tensor& out,
-                                       torch::Tensor const& weight,
-                                       torch::Tensor& scales,
-                                       double const epsilon,
--                                      std::optional<torch::Tensor> scale_ub,
--                                      std::optional<torch::Tensor> residual);
-+                                      c10::optional<torch::Tensor> scale_ub,
-+                                      c10::optional<torch::Tensor> residual);
- 
- void rotary_embedding(torch::Tensor& positions, torch::Tensor& query,
-                       torch::Tensor& key, int64_t head_size,
-@@ -130,17 +143,20 @@ torch::Tensor aqlm_dequant(
-     const torch::Tensor& codes, const torch::Tensor& codebooks,
-     const std::vector<int64_t>& codebook_partition_sizes);
- 
-+torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm);
-+#endif
++            dA[0] += aincr;
++            dB[0] += bincr;
++            dB[1] += bincr;
++            ldg_zeros_offset += lda / PACK_RATIO;
++            ldg_scales_offset += lda;
++        }
 +
- torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
-                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
--                       int64_t split_k_iters);
-+                       int64_t split_k_iters, torch::Tensor _temp_space, 
-+                       bool dtype_bf16);
- 
- torch::Tensor awq_dequantize(torch::Tensor _kernel,
-                              torch::Tensor _scaling_factors,
-                              torch::Tensor _zeros, int64_t split_k_iters,
-                              int64_t thx, int64_t thy);
- 
--torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm);
--#endif
-+torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight);
- 
- torch::Tensor ggml_dequantize(torch::Tensor W, int64_t type, int64_t m,
-                               int64_t n);
-@@ -158,15 +174,15 @@ bool cutlass_scaled_mm_supports_block_fp8(int64_t cuda_device_capability);
- void cutlass_scaled_mm(torch::Tensor& out, torch::Tensor const& a,
-                        torch::Tensor const& b, torch::Tensor const& a_scales,
-                        torch::Tensor const& b_scales,
--                       std::optional<torch::Tensor> const& bias);
-+                       c10::optional<torch::Tensor> const& bias);
- 
- void cutlass_scaled_mm_azp(torch::Tensor& out, torch::Tensor const& a,
-                            torch::Tensor const& b,
-                            torch::Tensor const& a_scales,
-                            torch::Tensor const& b_scales,
-                            torch::Tensor const& azp_adj,
--                           std::optional<torch::Tensor> const& azp,
--                           std::optional<torch::Tensor> const& bias);
-+                           c10::optional<torch::Tensor> const& azp,
-+                           c10::optional<torch::Tensor> const& bias);
- 
- bool cutlass_sparse_scaled_mm_supported(int64_t cuda_device_capability);
- 
-@@ -174,7 +190,7 @@ void cutlass_scaled_sparse_mm(torch::Tensor& out, torch::Tensor const& a,
-                               torch::Tensor const& b, torch::Tensor const& e,
-                               torch::Tensor const& a_scales,
-                               torch::Tensor const& b_scales,
--                              std::optional<torch::Tensor> const& bias);
-+                              c10::optional<torch::Tensor> const& bias);
- 
- bool cutlass_sparse_compress_entry(torch::Tensor& a_compressed,
-                                    torch::Tensor& e, torch::Tensor const& a);
-@@ -182,16 +198,18 @@ bool cutlass_sparse_compress_entry(torch::Tensor& a_compressed,
- 
- void static_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
-                               torch::Tensor const& scale,
--                              std::optional<torch::Tensor> const& azp);
-+                              c10::optional<torch::Tensor> const& azp);
- 
- void dynamic_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
-                                torch::Tensor& scales,
--                               std::optional<torch::Tensor> const& azp);
-+                               c10::optional<torch::Tensor> const& azp);
- 
- torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
-                         torch::Tensor b_gptq_qzeros,
-                         torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
--                        bool use_exllama, int64_t bit);
-+                        bool use_exllama, int64_t bit, int64_t group_size, 
-+			torch::Tensor perm_space, torch::Tensor temp_space,
-+			bool dtype_bf16);
- 
- void gptq_shuffle(torch::Tensor q_weight, torch::Tensor q_perm, int64_t bit);
- 
-@@ -203,34 +221,34 @@ void dynamic_scaled_fp8_quant(torch::Tensor& out, torch::Tensor const& input,
- 
- void dynamic_per_token_scaled_fp8_quant(
-     torch::Tensor& out, torch::Tensor const& input, torch::Tensor& scale,
--    std::optional<torch::Tensor> const& scale_ub);
-+    c10::optional<torch::Tensor> const& scale_ub);
- 
- void selective_scan_fwd(const torch::Tensor& u, const torch::Tensor& delta,
-                         const torch::Tensor& A, const torch::Tensor& B,
-                         const torch::Tensor& C,
--                        const std::optional<torch::Tensor>& D_,
--                        const std::optional<torch::Tensor>& z_,
--                        const std::optional<torch::Tensor>& delta_bias_,
-+                        const c10::optional<torch::Tensor>& D_,
-+                        const c10::optional<torch::Tensor>& z_,
-+                        const c10::optional<torch::Tensor>& delta_bias_,
-                         bool delta_softplus,
--                        const std::optional<torch::Tensor>& query_start_loc,
--                        const std::optional<torch::Tensor>& cache_indices,
--                        const std::optional<torch::Tensor>& has_initial_state,
-+                        const c10::optional<torch::Tensor>& query_start_loc,
-+                        const c10::optional<torch::Tensor>& cache_indices,
-+                        const c10::optional<torch::Tensor>& has_initial_state,
-                         const torch::Tensor& ssm_states, int64_t pad_slot_id);
- 
- void causal_conv1d_update(const at::Tensor& x, const at::Tensor& conv_state,
-                           const at::Tensor& weight,
--                          const std::optional<at::Tensor>& bias_,
-+                          const c10::optional<at::Tensor>& bias_,
-                           bool silu_activation,
--                          const std::optional<at::Tensor>& cache_seqlens_,
--                          const std::optional<at::Tensor>& conv_state_indices_,
-+                          const c10::optional<at::Tensor>& cache_seqlens_,
-+                          const c10::optional<at::Tensor>& conv_state_indices_,
-                           int64_t pad_slot_id);
- 
- void causal_conv1d_fwd(const at::Tensor& x, const at::Tensor& weight,
--                       const std::optional<at::Tensor>& bias_,
--                       const std::optional<at::Tensor>& conv_states,
--                       const std::optional<at::Tensor>& query_start_loc,
--                       const std::optional<at::Tensor>& cache_indices,
--                       const std::optional<at::Tensor>& has_initial_state,
-+                       const c10::optional<at::Tensor>& bias_,
-+                       const c10::optional<at::Tensor>& conv_states,
-+                       const c10::optional<at::Tensor>& query_start_loc,
-+                       const c10::optional<at::Tensor>& cache_indices,
-+                       const c10::optional<at::Tensor>& has_initial_state,
-                        bool silu_activation, int64_t pad_slot_id);
- 
- #ifndef USE_ROCM
-diff --git a/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
-new file mode 100644
-index 000000000..f10aca98f
---- /dev/null
-+++ b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
-@@ -0,0 +1,474 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+#pragma once
-+#include "../gptq/Hgemm_common.cuh"
-+#include "awq_4bits.cuh"
-+#include "maca_fp16.h"
++        arrive_gvmcnt(0);
++        PERM_A;
++        MMA;
++    }
 +
-+#define input_type __half
-+#define output_type __half
-+#define scalar_type float
-+#define acc_type float
++    // final tail kloop
++    if (ktail > 0)
++    {
++        LDG_B_HEAD;
++        LDG_ZEROS;
++        LDG_SCALES;
++        LDG_A_HEAD;
++        arrive_gvmcnt(8);
++        barrier();
++        LDS_B;
++        LDS_ZEROS;
++        LDS_SCALES;
++        arrive_bsmcnt(0);
++        barrier();
++        arrive_gvmcnt(0);
++        PERM_A;
++        MMA;
++    }
 +
-+#define SEL0 0x01000504
-+#define SEL1 0x03020706
++    // store C registers into BSM & do reduction
++    int colC = m64m16 + slot * 16;
++    int rowC = m64d16 * 32;
++    scalar_type *c_sts[8];
++    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
++    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
++    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
++    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
++    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 16, (colC % 4 * 4)), colC, 128);
++    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 20, (colC % 4 * 4)), colC, 128);
++    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 24, (colC % 4 * 4)), colC, 128);
++    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 28, (colC % 4 * 4)), colC, 128);
 +
-+#define SWIZZLE_INDEX(index, phase) (index ^ phase)
-+#define ONE_DIM_INDEX(x, y, lda) (y * lda + x)
-+#define CLAMP(value, bound, align) (value < bound ? value : bound - align)
++    colC = m64d16 + slot * 4;
++    rowC = m64m16 * 4;
++    scalar_type *c_lds[2];
++    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
++    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
 +
-+#define LDG_B                                                       \
-+    {                                                               \
-+        ldg_b128_bsm_async(b_sts, dB[0], pred_n0, true);            \
-+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1, true); \
-+    }
++    PERM_C(rgC);
++    STS_C(0);
++    arrive_bsmcnt(0);
++    barrier();
++    REDUCE_C(0);
 +
-+#define LDG_B_HEAD                                                            \
-+    {                                                                         \
-+        bool pred_k = rowB_swizzle < ktail;                                   \
-+        ldg_b128_bsm_async(b_sts, dB[0], pred_n0 &&pred_k, true);             \
-+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1 && pred_k, true); \
-+    }
++    arrive_bsmcnt(0);
++    barrier();
 +
-+#define MMA_ABC1                                                             \
-+    {                                                                        \
-+        for (int index_n = 0; index_n < 2; ++index_n)                        \
-+        {                                                                    \
-+            for (int index_m = 0; index_m < 8; ++index_m)                    \
-+            {                                                                \
-+                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
-+                                cast_b64(rgB)[ONE_DIM_INDEX(0, index_n, 2)], \
-+                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
-+            }                                                                \
-+        }                                                                    \
-+    }
++    PERM_C((rgC + 8))
++    STS_C(1);
++    arrive_bsmcnt(0);
++    barrier();
++    REDUCE_C(1);
 +
-+#define MMA_ABC2                                                             \
-+    {                                                                        \
-+        for (int index_n = 0; index_n < 2; ++index_n)                        \
-+        {                                                                    \
-+            for (int index_m = 0; index_m < 8; ++index_m)                    \
-+            {                                                                \
-+                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
-+                                cast_b64(rgB)[ONE_DIM_INDEX(1, index_n, 2)], \
-+                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
-+            }                                                                \
-+        }                                                                    \
-+    }
++    arrive_bsmcnt(0);
 +
-+#define LDS_B                                          \
-+    {                                                  \
-+        cast_b128(rgB)[0] = cast_b128(b_lds)[0];       \
-+        cast_b128(rgB)[1] = cast_b128(b_lds)[16 * 16]; \
-+    }
++    // read C_input if beta!=0
++    colC = m64d16 + slot * 4 + bidy * tileN;
++    rowC = m64m16 * 4 + bidx * tileM;
++    int colC1 = colC + 16;
++    int rowC1 = rowC + 64;
 +
-+#define PERM_C(C2perm)                             \
-+    {                                              \
-+        Float4VecType C_tmp[8];                    \
-+        float *ptri = (float *)C2perm;             \
-+        float *ptro = (float *)C_tmp;              \
-+        for (int j = 0; j < 4; ++j)                \
-+        {                                          \
-+            for (int i = 0; i < 8; ++i)            \
-+            {                                      \
-+                ptro[j * 8 + i] = ptri[j + i * 4]; \
-+            }                                      \
-+        }                                          \
-+        for (int i = 0; i < 8; ++i)                \
-+        {                                          \
-+            C2perm[i] = C_tmp[i];                  \
-+        }                                          \
-+    }
 +
-+#define STS_C(phase)                                            \
-+    {                                                           \
-+        cast_b128(c_sts[0])[0] = cast_b128(rgC)[0 + phase * 8]; \
-+        cast_b128(c_sts[1])[0] = cast_b128(rgC)[1 + phase * 8]; \
-+        cast_b128(c_sts[2])[0] = cast_b128(rgC)[2 + phase * 8]; \
-+        cast_b128(c_sts[3])[0] = cast_b128(rgC)[3 + phase * 8]; \
-+        cast_b128(c_sts[4])[0] = cast_b128(rgC)[4 + phase * 8]; \
-+        cast_b128(c_sts[5])[0] = cast_b128(rgC)[5 + phase * 8]; \
-+        cast_b128(c_sts[6])[0] = cast_b128(rgC)[6 + phase * 8]; \
-+        cast_b128(c_sts[7])[0] = cast_b128(rgC)[7 + phase * 8]; \
++    if constexpr (!IsBetaZero && !splitk)
++    {
++        ldg_b64_reg_async(cast_b64(rgCi)[0],
++                          c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
++                          rowC < align_m && colC < align_n, true);
++        ldg_b64_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
++                          rowC1 < align_m && colC < align_n, true);
++        ldg_b64_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
++                          rowC < align_m && colC1 < align_n, true);
++        ldg_b64_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
++                          rowC1 < align_m && colC1 < align_n, true);
++        arrive_gvmcnt(0);
 +    }
 +
-+#define REDUCE_C(phase)                                                   \
-+    {                                                                     \
-+        cast_b128(rgC)[0 + phase * 8] = cast_b128(c_lds[0])[0];           \
-+        cast_b128(rgC)[1 + phase * 8] = cast_b128(c_lds[1])[0];           \
-+        cast_b128(rgC)[2 + phase * 8] = cast_b128(c_lds[0])[1 * 16 * 32]; \
-+        cast_b128(rgC)[3 + phase * 8] = cast_b128(c_lds[1])[1 * 16 * 32]; \
-+        cast_b128(rgC)[4 + phase * 8] = cast_b128(c_lds[0])[2 * 16 * 32]; \
-+        cast_b128(rgC)[5 + phase * 8] = cast_b128(c_lds[1])[2 * 16 * 32]; \
-+        cast_b128(rgC)[6 + phase * 8] = cast_b128(c_lds[0])[3 * 16 * 32]; \
-+        cast_b128(rgC)[7 + phase * 8] = cast_b128(c_lds[1])[3 * 16 * 32]; \
-+        float *reduc_c = reinterpret_cast<float *>(rgC);                  \
-+        for (int loop = 0; loop < 8; ++loop)                              \
-+        {                                                                 \
-+            float acc = 0;                                                \
-+            acc += reduc_c[loop + phase * 32 + 0 * 8];                    \
-+            acc += reduc_c[loop + phase * 32 + 1 * 8];                    \
-+            acc += reduc_c[loop + phase * 32 + 2 * 8];                    \
-+            acc += reduc_c[loop + phase * 32 + 3 * 8];                    \
-+            reduc_c[loop + phase * 8] = acc;                              \
-+        }                                                                 \
++    if constexpr (IsBetaZero && !splitk)
++    {
++        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
 +    }
 +
-+template <Operation_t TransA,
-+          Operation_t TransB,
-+          bool IsBetaZero,
-+          int tileM,
-+          int tileN,
-+          int tileK,
-+          bool splitk = false,
-+          bool Swap = false>
-+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_awq_4bit(int m,
-+                                                                           int n,
-+                                                                           int k,
-+                                                                           const scalar_type alpha,
-+                                                                           const scalar_type beta,
-+                                                                           const quant_packed_type *dA_input,
-+                                                                           int lda,
-+                                                                           const input_type *dB_input,
-+                                                                           int ldb,
-+                                                                           output_type *dC_input,
-+                                                                           output_type *dC_output,
-+                                                                           int ldc,
-+                                                                           quant_packed_type *d_zeros,
-+                                                                           input_type *d_scales,
-+                                                                           int splitk_iters = 1,
-+                                                                           acc_type * d_acc_tmp=nullptr)
-+{
-+    __shared__ uint8_t smem_base[0x8000];
-+    int bidx = Swap ? blockIdx.y : blockIdx.x;
-+    int bidy = Swap ? blockIdx.x : blockIdx.y;
-+    int bidz = blockIdx.z;
-+    int tid = threadIdx.x;
-+
-+    uint64_t arowstride = TransA == OP_N ? 1 : lda;
-+    uint64_t acolstride = TransA == OP_N ? lda : 1;
-+    uint64_t browstride = TransB == OP_N ? 1 : ldb;
-+    uint64_t bcolstride = TransB == OP_N ? ldb : 1;
-+
-+    const int malign = 8;
-+    const int nalign = 1;
-+    const int kalign = 8;
-+    int align_m = (m + malign - 1) / malign * malign;
-+    int align_n = (n + nalign - 1) / nalign * nalign;
-+    int align_k = (k + kalign - 1) / kalign * kalign;
 +
-+    int k_num = (align_k + splitk_iters - 1) / splitk_iters;
-+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
-+    int k_begin = bidz * k_num;
-+    int k_end = (bidz + 1) * k_num;
-+    k_end = k_end > align_k ? align_k : k_end;
-+    k_num = k_end - k_begin;
-+    int ktail = k_num % tileK;
-+    int kloop = k_begin;
-+    if (k_begin > align_k)
++    if constexpr (!splitk)
 +    {
-+        return;
-+    }    
-+
-+
-+    int slot = tid / 64;
-+    int lane = tid & 63;
-+    int m64d16 = lane / 16;
-+    int m64m16 = lane % 16;
-+    output_type *c_base_i = dC_input;
-+    output_type *c_base_o = dC_output;
-+
++        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
++        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
++        if (rowC < align_m && colC < align_n)
++        {
++            output_type result[4];
++            result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
++            result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
++            result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
++            result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
++            ptrO[0] = cast_b64(result)[0];
++        }
++        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
++        if (rowC1 < align_m && colC < align_n)
++        {
++            output_type result[4];
++            result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
++            result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
++            result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
++            result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
++            ptrO[0] = cast_b64(result)[0];
++        }
++        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
++        if (rowC < align_m && colC1 < align_n)
++        {
++            output_type result[4];
++            result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
++            result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
++            result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
++            result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
++            ptrO[0] = cast_b64(result)[0];
++        }
++        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
++        if (rowC1 < align_m && colC1 < align_n)
++        {
++            output_type result[4];
++            result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
++            result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
++            result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
++            result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
++            ptrO[0] = cast_b64(result)[0];
++        }
++    }
++    else
++    {
++        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
++        if (rowC < align_m && colC < align_n)
++        {
++            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
++            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
++            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
++            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
++        }
++        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
++        if (rowC1 < align_m && colC < align_n)
++        {
++            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
++            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
++            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
++            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
++        }
++        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
++        if (rowC < align_m && colC1 < align_n)
++        {
++            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
++            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
++            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
++            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
++        }
++        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
++        if (rowC1 < align_m && colC1 < align_n)
++        {
++            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
++            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
++            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
++            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
++        }
++    }
++}
+\ No newline at end of file
+diff --git a/csrc/quantization/awq/awq_4bits.cuh b/csrc/quantization/awq/awq_4bits.cuh
+new file mode 100644
+index 000000000..b5012b9b0
+--- /dev/null
++++ b/csrc/quantization/awq/awq_4bits.cuh
+@@ -0,0 +1,125 @@
++#pragma once
 +
-+    quant_packed_type rga[8];
-+    b128VecType rgb[2];
++#include "../gptq/Hgemm_common.cuh"
++#include "dequant.cuh"
++#define quant_packed_type uint32_t
 +
-+    b64VecType rgA[16], rgB[4], rgCi[4];
-+    Float4VecType rgC[16];
-+    uint32_t rgZeros[1];
-+    b128VecType rgScales[1];
++#define QBITS 4
++#define PACK_RATIO (32 / QBITS)
 +
-+    quant_packed_type *dA[1];
-+    input_type *dB[2];
-+
-+    // ldg A/B head
++#define LDG_A                                                                         \
++    {                                                                                 \
++        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m, true); \
++        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m, true); \
++    }
 +
-+    int rowA = m64m16 * 8;
-+    int colA = m64d16 * 8 + slot * 32;
-+    uint current_m = bidx * tileM + rowA;
-+    bool pred_m = current_m < align_m;
-+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m / PACK_RATIO) * (uint64_t)(arowstride) +
-+            (uint64_t)(colA + k_begin) * (uint64_t)(acolstride / PACK_RATIO);
++#define LDG_A_HEAD                                                                    \
++    {                                                                                 \
++        bool predk = colA < ktail;                                                    \
++        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m && predk, true); \
++        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m && predk, true); \
++    }
 +
-+    int colB = m64d16 + slot * 4;
-+    int rowB = m64m16 * 8;
-+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
-+    int current_n = bidy * tileN + colB;
-+    bool pred_n0 = current_n < align_n;
-+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
-+            (uint64_t)(current_n) * (uint64_t)bcolstride;
++// blocktileM/PACK_RATIOtileMzeroszerosint432bits8zeros
++#define LDG_ZEROS                                                                                                                              \
++    {                                                                                                                                          \
++        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO && (bidx * tileM + tid * PACK_RATIO < align_m), false); \
++    }
 +
-+    current_n += 16;
-+    bool pred_n1 = current_n < align_n;
-+    dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
-+            (uint64_t)(current_n) * (uint64_t)bcolstride;
++// blocktileM/(sizeof(uint32_t)/sizeof(__half))tileMscalesscalefp1632bits2scales
++#define LDG_SCALES                                                                                                      \
++    {                                                                                                                   \
++        ldg_b32_bsm_async(sts_scales_offset, ldg_scales_offset, tid < 64 && (bidx * tileM + tid * 2 < align_m), false); \
++    }
 +
-+    // ldgB to BSM need swizzle
-+    input_type *b_sts = (input_type *)smem_base;
-+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
++#define LDS_ZEROS                         \
++    {                                     \
++        rgZeros[0] = lds_zeros_offset[0]; \
++    }
 +
-+    // lds need swizzle
-+    input_type *b_lds = (input_type *)smem_base;
-+    int colB_lds = m64m16;
-+    int rowB_lds = m64d16 * 8 + slot * 32;
-+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
++#define LDS_SCALES                                     \
++    {                                                  \
++        rgScales[0] = cast_b128(lds_scales_offset)[0]; \
++    }
 +
-+    // ldg zeros and scales
-+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO) * (kloop / tileK) + bidx * (tileM / PACK_RATIO) + tid;
-+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
-+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
-+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + tid * 2;
-+    quant_packed_type *lds_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
-+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + m64m16 * 8;
++// zeros 0 2 4 6 1 3 5 7  index_shfl = index % 2 * 4 + index / 2
++//    int4zerosint32weightAfp16Weight_Q=Scale*(Weight_4Bit-ZeroPoint)
++#define PERM_ELEM(index)                                                                                                          \
++    {                                                                                                                             \
++        __half_raw elem;                                                                                                          \
++        if constexpr (index & 0x1)                                                                                                \
++        {                                                                                                                         \
++            elem.x = cast_b32(rgScales)[index / 2] >> 16;                                                                         \
++        }                                                                                                                         \
++        else                                                                                                                      \
++        {                                                                                                                         \
++            elem.x = cast_b32(rgScales)[index / 2] & 0xffff;                                                                      \
++        }                                                                                                                         \
++        __half scale = __half(elem);                                                                                              \
++        constexpr int index_shfl = index % 2 * 4 + index / 2;                                                                     \
++        uint32_t zero = __builtin_mxc_ubfe(rgZeros[0], QBITS * index_shfl, QBITS);                                                \
++        cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[0], QBITS * index_shfl, QBITS) -  \
++                                                                         zero) , scale);                                                                                    \
++        cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[1], QBITS * index_shfl, QBITS) -  \
++                                                                         zero) , scale);                                                                                    \
++        cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[2], QBITS * index_shfl, QBITS) -  \
++                                                                         zero) , scale);                                                                                    \
++        cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[3], QBITS * index_shfl, QBITS) -  \
++                                                                         zero) , scale);                                                                                    \
++        cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[4], QBITS * index_shfl, QBITS) - \
++                                                                          zero) , scale);                                                                                   \
++        cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[5], QBITS * index_shfl, QBITS) - \
++                                                                          zero) , scale);                                                                                   \
++        cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[6], QBITS * index_shfl, QBITS) - \
++                                                                          zero) , scale);                                                                                   \
++        cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[7], QBITS * index_shfl, QBITS) - \
++                                                                          zero) , scale);                                                                                   \
++    }
 +
-+    // clear C registers
-+    rgC[0] = 0;
-+    rgC[1] = 0;
-+    rgC[2] = 0;
-+    rgC[3] = 0;
-+    rgC[4] = 0;
-+    rgC[5] = 0;
-+    rgC[6] = 0;
-+    rgC[7] = 0;
-+    rgC[8] = 0;
-+    rgC[9] = 0;
-+    rgC[10] = 0;
-+    rgC[11] = 0;
-+    rgC[12] = 0;
-+    rgC[13] = 0;
-+    rgC[14] = 0;
-+    rgC[15] = 0;
++#define PERM_A       \
++    {                \
++        PERM_ELEM(0) \
++        PERM_ELEM(1) \
++        PERM_ELEM(2) \
++        PERM_ELEM(3) \
++        PERM_ELEM(4) \
++        PERM_ELEM(5) \
++        PERM_ELEM(6) \
++        PERM_ELEM(7) \
++    }
 +
-+    int aincr = tileK * acolstride / PACK_RATIO;
-+    int bincr = tileK * browstride;
++#define MMA_ELEM(index_m)                                        \
++    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
++                    cast_b64(rgB)[ONE_DIM_INDEX(0, 0, 2)],       \
++                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
++    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
++                    cast_b64(rgB)[ONE_DIM_INDEX(0, 1, 2)],       \
++                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);          \
++    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
++                    cast_b64(rgB)[ONE_DIM_INDEX(1, 0, 2)],       \
++                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
++    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
++                    cast_b64(rgB)[ONE_DIM_INDEX(1, 1, 2)],       \
++                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);
 +
-+    if (k_num > ktail)
-+    {
-+        LDG_B;
-+        LDG_ZEROS;
-+        LDG_SCALES;
-+        LDG_A;
++#define MMA     \
++    MMA_ELEM(0) \
++    MMA_ELEM(1) \
++    MMA_ELEM(2) \
++    MMA_ELEM(3) \
++    MMA_ELEM(4) \
++    MMA_ELEM(5) \
++    MMA_ELEM(6) \
++    MMA_ELEM(7)
+diff --git a/csrc/quantization/awq/dequant.cuh b/csrc/quantization/awq/dequant.cuh
+new file mode 100644
+index 000000000..7d6db705c
+--- /dev/null
++++ b/csrc/quantization/awq/dequant.cuh
+@@ -0,0 +1,10 @@
++#pragma once
 +
-+        dA[0] += aincr;
-+        dB[0] += bincr;
-+        dB[1] += bincr;
++#define BFE(source, bit_start, num_bits) __builtin_mxc_ubfe(source, bit_start, num_bits);
 +
-+        // lds_A, need swizzle
-+        arrive_gvmcnt(8);
-+        barrier();
-+        LDS_B;
-+        LDS_ZEROS;
-+        LDS_SCALES;
-+        arrive_bsmcnt(0);
-+        barrier();
++template <typename outputT, typename inputT, int qbits>
++__device__ __forceinline__ outputT extract(inputT *input, size_t loc)
++{
++    constexpr int widthPerElem = sizeof(inputT) * 8 / qbits;
++    return (outputT)BFE(input[loc / widthPerElem], loc % widthPerElem, qbits);
++}
+\ No newline at end of file
+diff --git a/csrc/quantization/awq/dequantize.cuh b/csrc/quantization/awq/dequantize.cuh
+index 5fa4b5f64..0f245d470 100644
+--- a/csrc/quantization/awq/dequantize.cuh
++++ b/csrc/quantization/awq/dequantize.cuh
+@@ -14,6 +14,7 @@ Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+ namespace vllm {
+ namespace awq {
+ 
++template<typename VT>
+ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+ #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 750
+   assert(false);
+@@ -39,6 +40,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+   // Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW
+   // dependency if we issue immediately before required.
+   const uint32_t top_i4s = i4s >> 8;
++#ifndef USE_MACA
+   // Extract elt_01 - (i4s & 0x000f000f) | 0x64006400
+   asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
+                : "=r"(h[0])
+@@ -59,6 +61,63 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+                : "=r"(h[3])
+                : "r"(top_i4s), "n"(TOP_MASK), "n"(I4s_TO_F16s_MAGIC_NUM),
+                  "n"(immLut));
++#else
++      // >>>> PTX2CPP Success <<<<
++{
++(h[0])=0;
++if((immLut)&0x01)(h[0])|=~(i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x02)(h[0])|=~(i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x04)(h[0])|=~(i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x08)(h[0])|=~(i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x10)(h[0])|= (i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x20)(h[0])|= (i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x40)(h[0])|= (i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x80)(h[0])|= (i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++}
 +
-+        ldg_zeros_offset += lda / PACK_RATIO;
-+        ldg_scales_offset += lda;
++    // Extract elt_23 (i4s & 0x00f000f0) | 0x64006400
 +
-+// KLOOP
-+#pragma unroll 1
-+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
-+        {
-+            // load next B
-+            LDG_B;
-+            LDG_ZEROS;
-+            LDG_SCALES;
++// >>>> PTX2CPP Success <<<<
++{
++(h[1])=0;
++if((immLut)&0x01)(h[1])|=~(i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x02)(h[1])|=~(i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x04)(h[1])|=~(i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x08)(h[1])|=~(i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x10)(h[1])|= (i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x20)(h[1])|= (i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x40)(h[1])|= (i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x80)(h[1])|= (i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++}
++// >>>> PTX2CPP Success <<<<
++{
++(h[2])=0;
++if((immLut)&0x01)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x02)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x04)(h[2])|=~(top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x08)(h[2])|=~(top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x10)(h[2])|= (top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x20)(h[2])|= (top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x40)(h[2])|= (top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x80)(h[2])|= (top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++}
++    // Extract elt_67 (top_i4s & 0x00f000f0) | 0x64006400
 +
-+            // MMA_ABC1;
-+            arrive_gvmcnt(4);
-+            PERM_A;
-+            LDG_A;
-+            MMA;
-+
-+            // sts && lds
-+            arrive_gvmcnt(8);
-+            barrier();
-+            LDS_B;
-+            LDS_ZEROS;
-+            LDS_SCALES;
-+            arrive_bsmcnt(0);
-+            barrier();
-+
-+            dA[0] += aincr;
-+            dB[0] += bincr;
-+            dB[1] += bincr;
-+            ldg_zeros_offset += lda / PACK_RATIO;
-+            ldg_scales_offset += lda;
-+        }
-+
-+        arrive_gvmcnt(0);
-+        PERM_A;
-+        MMA;
-+    }
-+
-+    // final tail kloop
-+    if (ktail > 0)
-+    {
-+        LDG_B_HEAD;
-+        LDG_ZEROS;
-+        LDG_SCALES;
-+        LDG_A_HEAD;
-+        arrive_gvmcnt(8);
-+        barrier();
-+        LDS_B;
-+        LDS_ZEROS;
-+        LDS_SCALES;
-+        arrive_bsmcnt(0);
-+        barrier();
-+        arrive_gvmcnt(0);
-+        PERM_A;
-+        MMA;
-+    }
++// >>>> PTX2CPP Success <<<<
++{
++(h[3])=0;
++if((immLut)&0x01)(h[3])|=~(top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x02)(h[3])|=~(top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x04)(h[3])|=~(top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x08)(h[3])|=~(top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x10)(h[3])|= (top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x20)(h[3])|= (top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x40)(h[3])|= (top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
++if((immLut)&0x80)(h[3])|= (top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++}
 +
-+    // store C registers into BSM & do reduction
-+    int colC = m64m16 + slot * 16;
-+    int rowC = m64d16 * 32;
-+    scalar_type *c_sts[8];
-+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
-+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
-+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
-+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
-+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 16, (colC % 4 * 4)), colC, 128);
-+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 20, (colC % 4 * 4)), colC, 128);
-+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 24, (colC % 4 * 4)), colC, 128);
-+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 28, (colC % 4 * 4)), colC, 128);
 +
-+    colC = m64d16 + slot * 4;
-+    rowC = m64m16 * 4;
-+    scalar_type *c_lds[2];
-+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
-+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
++#endif //USE_MACA
+ 
+   // I use inline PTX below because I am not sure if the compiler will emit
+   // float2half instructions if I use the half2 ctor. In this case, I chose
+@@ -77,6 +136,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+ 
+   // Finally, we construct the output numbers.
+   // Convert elt_01
++#ifndef USE_MACA
+   asm volatile("sub.f16x2 %0, %1, %2;\n"
+                : "=r"(h[0])
+                : "r"(h[0]), "r"(FP16_TOP_MAGIC_NUM));
+@@ -92,7 +152,54 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
+                : "=r"(h[3])
+                : "r"(h[3]), "r"(ONE_SIXTEENTH), "r"(NEG_64));
++#else
++    // >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(h[0]);
++unsigned int __b=(FP16_TOP_MAGIC_NUM);
++VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
++(h[0])=*(unsigned int*)&__d;
++}
++}
 +
-+    PERM_C(rgC);
-+    STS_C(0);
-+    arrive_bsmcnt(0);
-+    barrier();
-+    REDUCE_C(0);
++    // Convert elt_23
 +
-+    arrive_bsmcnt(0);
-+    barrier();
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(h[1]);
++unsigned int __b=(ONE_SIXTEENTH);
++unsigned int __c=(NEG_64);
++VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
++(h[1])=*(unsigned int*)&__d;
++}
++}
++    // Convert elt_45
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(h[2]);
++unsigned int __b=(FP16_TOP_MAGIC_NUM);
++VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
++(h[2])=*(unsigned int*)&__d;
++}
++}
 +
-+    PERM_C((rgC + 8))
-+    STS_C(1);
-+    arrive_bsmcnt(0);
-+    barrier();
-+    REDUCE_C(1);
++    // Convert elt_67
 +
-+    arrive_bsmcnt(0);
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(h[3]);
++unsigned int __b=(ONE_SIXTEENTH);
++unsigned int __c=(NEG_64);
++VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
++(h[3])=*(unsigned int*)&__d;
++}
++}
+ 
++#endif // USE_MACA
+   return result;
+ #endif
+   __builtin_unreachable();  // Suppress missing return statement warning
+diff --git a/csrc/quantization/awq/gemm_kernels.cu b/csrc/quantization/awq/gemm_kernels.cu
+index 53c47679c..81e79e938 100644
+--- a/csrc/quantization/awq/gemm_kernels.cu
++++ b/csrc/quantization/awq/gemm_kernels.cu
+@@ -1,3 +1,4 @@
++// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+ /*
+ Adapted from https://github.com/mit-han-lab/llm-awq
+ @article{lin2023awq,
+@@ -14,9 +15,56 @@ Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+ 
+ #include <cuda_fp16.h>
+ 
++#include "../gptq/hgemm_gptq.h"
++#include "../gptq/scalar_type.hpp"
 +
-+    // read C_input if beta!=0
-+    colC = m64d16 + slot * 4 + bidy * tileN;
-+    rowC = m64m16 * 4 + bidx * tileM;
-+    int colC1 = colC + 16;
-+    int rowC1 = rowC + 64;
++//#include "hgemv_nn_splitk_awq.hpp"
++//#include "hgemv_selector.hpp"
++//#include "Hgemm_nn_128x32x128_8m1n8k_awq.hpp"
 +
+ namespace vllm {
+ namespace awq {
++#define input_type __half
++#define output_type __half
++#define quant_packed_type uint32_t
++#define QUANT_GROUP 128
 +
-+    if constexpr (!IsBetaZero && !splitk)
++struct DivModFast {
++    DivModFast(int d = 1)
 +    {
-+        ldg_b64_reg_async(cast_b64(rgCi)[0],
-+                          c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
-+                          rowC < align_m && colC < align_n, true);
-+        ldg_b64_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
-+                          rowC1 < align_m && colC < align_n, true);
-+        ldg_b64_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
-+                          rowC < align_m && colC1 < align_n, true);
-+        ldg_b64_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
-+                          rowC1 < align_m && colC1 < align_n, true);
-+        arrive_gvmcnt(0);
++        d_ = (d == 0) ? 1 : d;
++        for (l_ = 0;; ++l_) {
++            if ((1U << l_) >= d_)
++                break;
++        }
++        uint64_t one = 1;
++        uint64_t m   = ((one << 32) * ((one << l_) - d_)) / d_ + 1;
++        m_           = static_cast<uint32_t>(m);
 +    }
 +
-+    if constexpr (IsBetaZero && !splitk)
++    __device__ __inline__ int div(int idx) const
 +    {
-+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
++        uint32_t tm = __umulhi(m_, idx); // get high 32-bit of the product
++        return (tm + idx) >> l_;
 +    }
 +
-+
-+    if constexpr (!splitk)
++    __device__ __inline__ int mod(int idx) const
 +    {
-+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
-+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
-+        if (rowC < align_m && colC < align_n)
-+        {
-+            output_type result[4];
-+            result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
-+            result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
-+            result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
-+            result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
-+            ptrO[0] = cast_b64(result)[0];
-+        }
-+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
-+        if (rowC1 < align_m && colC < align_n)
-+        {
-+            output_type result[4];
-+            result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
-+            result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
-+            result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
-+            result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
-+            ptrO[0] = cast_b64(result)[0];
-+        }
-+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
-+        if (rowC < align_m && colC1 < align_n)
-+        {
-+            output_type result[4];
-+            result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
-+            result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
-+            result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
-+            result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
-+            ptrO[0] = cast_b64(result)[0];
-+        }
-+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
-+        if (rowC1 < align_m && colC1 < align_n)
-+        {
-+            output_type result[4];
-+            result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
-+            result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
-+            result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
-+            result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
-+            ptrO[0] = cast_b64(result)[0];
-+        }
++        return idx - d_ * div(idx);
 +    }
-+    else
++
++    __device__ __inline__ void divmod(int idx, int &quo, int &rem)
 +    {
-+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
-+        if (rowC < align_m && colC < align_n)
-+        {
-+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
-+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
-+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
-+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
-+        }
-+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
-+        if (rowC1 < align_m && colC < align_n)
-+        {
-+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
-+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
-+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
-+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
-+        }
-+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
-+        if (rowC < align_m && colC1 < align_n)
-+        {
-+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
-+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
-+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
-+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
-+        }
-+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
-+        if (rowC1 < align_m && colC1 < align_n)
-+        {
-+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
-+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
-+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
-+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
-+        }
++        quo = div(idx);
++        rem = idx - quo * d_;
 +    }
++    
++    uint32_t d_; // divisor
++    uint32_t l_; // ceil(log2(d_))
++    uint32_t m_; // m' in the papaer
++};
+ 
++#if 0
+ template <int N>
+ __global__ void __launch_bounds__(64)
+     gemm_forward_4bit_cuda_m16nXk32(int G, int split_k_iters,
+@@ -136,6 +184,7 @@ __global__ void __launch_bounds__(64)
+       // - zero and * scale
+       // TODO (Haotian): can save 4 assembly instructions if sormulate as deq =
+       // q * scale - zero * scale.
++#ifndef USE_MACA
+       asm volatile("sub.f16x2 %0, %1, %2;\n"
+                    : "=r"(B_loaded_fp16.x)
+                    : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
+@@ -160,6 +209,7 @@ __global__ void __launch_bounds__(64)
+       asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
+                    : "=r"(B_loaded_fp16.w)
+                    : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
++#endif
+       /*
+       if (ax0_ax1_fused_0 == 0 && blockIdx_z == 0 && blockIdx_y == 0 && k_0_0 ==
+       0 && threadIdx.x == 17 && threadIdx.y == 0){ printf("[x] %X %X %X %X\n",
+@@ -176,6 +226,7 @@ __global__ void __launch_bounds__(64)
+     for (int k_0_1 = 0; k_0_1 < 2; ++k_0_1) {
+       {
+         unsigned int addr;
++#ifndef USE_MACA
+         __asm__ __volatile__(
+             "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
+             "addr; }\n"
+@@ -192,11 +243,13 @@ __global__ void __launch_bounds__(64)
+               "=r"(((unsigned*)(A_shared_warp + 0))[2]),
+               "=r"(((unsigned*)(A_shared_warp + 0))[3])
+             : "r"(addr));
++#endif
+       }
+ 
+       for (int ax1_0 = 0; ax1_0 < N / 32; ++ax1_0) {
+         {
+           unsigned int addr;
++#ifndef USE_MACA
+           __asm__ __volatile__(
+               "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
+               "addr; }\n"
+@@ -214,9 +267,11 @@ __global__ void __launch_bounds__(64)
+                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[2]),
+                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[3])
+               : "r"(addr));
++#endif
+         }
+       }
+       for (int j_0_4 = 0; j_0_4 < N / 32; ++j_0_4) {
++#ifndef USE_MACA
+   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 750
+         {
+           __asm__ __volatile__(
+@@ -329,12 +384,13 @@ __global__ void __launch_bounds__(64)
+         }
+ 
+   #endif
++#endif
+       }
+     }
+   }
+ 
+   // TODO: Shang: Hoist loop invariance.
+-  for (int ax1_0_1 = 0; ax1_0_1 < (N / 32); ++ax1_0_1) {
++  for (int ax1_0_1 = 0; ax1_0_1 < 4; ++ax1_0_1) {
+     for (int local_id = 0; local_id < 8; ++local_id) {
+       int row_offset = (((int)blockIdx_y) / j_factors1) * 16 +
+                        ((int)threadIdx.x) / 4 + (local_id % 4) / 2 * 8;
+@@ -346,20 +402,22 @@ __global__ void __launch_bounds__(64)
+   }
+ #endif
+ }
++#endif
+ 
++template<typename T, typename VT>
+ __global__ void __launch_bounds__(64)
+-    dequantize_weights(int* __restrict__ B, half* __restrict__ scaling_factors,
+-                       int* __restrict__ zeros, half* __restrict__ C, int G) {
++    dequantize_weights(int* __restrict__ B, T* __restrict__ scaling_factors,
++                       int* __restrict__ zeros, T* __restrict__ C, int G) {
+   static constexpr uint32_t ZERO = 0x0;
+-  half B_shared[32 * (128 + 8)];
+-
+-  half* B_shared_ptr2 = B_shared;
++  T B_shared[8];
++  T B_loaded_scale[8];
++  T* B_shared_ptr2 = B_shared;
+ 
+   int N = blockDim.x * gridDim.x;  // 2
+   int col = (blockIdx.x * blockDim.x + threadIdx.x);
+   int row = blockIdx.y * blockDim.y + threadIdx.y;
+   int index1 = 8 * col + 8 * row * N;
+-  half* C_ptr2 = C + index1;
++  T* C_ptr2 = C + index1;
+ 
+   int index2 = col + row * N;
+   int* B_ptr2 = B + index2;
+@@ -367,14 +425,18 @@ __global__ void __launch_bounds__(64)
+   int index3 = col + (int)(row / G) * N;
+   int* zeros_ptr2 = zeros + index3;
+   int index4 = 8 * col + (int)(row / G) * N * 8;
+-  half* scaling_factors_ptr2 = scaling_factors + index4;
++  T* scaling_factors_ptr2 = scaling_factors + index4;
+ 
+   uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr2);
+-  uint4 B_loaded_zero = dequantize_s4_to_fp16x2(zeros_loaded);
+-  uint4 B_loaded_scale = *(uint4*)(scaling_factors_ptr2);
+-
++  *(uint4*)B_loaded_scale = *(uint4*)(scaling_factors_ptr2);
+   uint32_t B_loaded = *(uint32_t*)B_ptr2;
+-  uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2(B_loaded);
++#if 1
++  //zero //4bit scale 8bit b 4bit
++  hgemm_marlin_gptq::awq_dequant_4bits<T>(B_loaded,B_shared, B_loaded_scale, zeros_loaded);
++#else
++  uint4 B_loaded_zero = dequantize_s4_to_fp16x2<VT>(zeros_loaded);
++  uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2<VT>(B_loaded);
++#ifndef USE_MACA
+   asm volatile("sub.f16x2 %0, %1, %2;\n"
+                : "=r"(B_loaded_fp16.x)
+                : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
+@@ -399,17 +461,360 @@ __global__ void __launch_bounds__(64)
+   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
+                : "=r"(B_loaded_fp16.w)
+                : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
++#else
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.x);
++unsigned int __b=(B_loaded_zero.x);
++VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
++(B_loaded_fp16.x)=*(unsigned int*)&__d;
++}
 +}
-\ No newline at end of file
-diff --git a/csrc/quantization/awq/awq_4bits.cuh b/csrc/quantization/awq/awq_4bits.cuh
-new file mode 100644
-index 000000000..7653066cc
---- /dev/null
-+++ b/csrc/quantization/awq/awq_4bits.cuh
-@@ -0,0 +1,126 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+#pragma once
 +
-+#include "../gptq/Hgemm_common.cuh"
-+#include "dequant.cuh"
-+#define quant_packed_type uint32_t
 +
-+#define QBITS 4
-+#define PACK_RATIO (32 / QBITS)
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.x);
++unsigned int __b=(B_loaded_scale.x);
++unsigned int __c=(ZERO);
++VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
++(B_loaded_fp16.x)=*(unsigned int*)&__d;
++}
++}
 +
-+#define LDG_A                                                                         \
-+    {                                                                                 \
-+        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m, true); \
-+        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m, true); \
-+    }
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.y);
++unsigned int __b=(B_loaded_zero.y);
++VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
++(B_loaded_fp16.y)=*(unsigned int*)&__d;
++}
++}
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.y);
++unsigned int __b=(B_loaded_scale.y);
++unsigned int __c=(ZERO);
++VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
++(B_loaded_fp16.y)=*(unsigned int*)&__d;
++}
++}
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.z);
++unsigned int __b=(B_loaded_zero.z);
++VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
++(B_loaded_fp16.z)=*(unsigned int*)&__d;
++}
++}
 +
-+#define LDG_A_HEAD                                                                    \
-+    {                                                                                 \
-+        bool predk = colA < ktail;                                                    \
-+        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m && predk, true); \
-+        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m && predk, true); \
-+    }
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.z);
++unsigned int __b=(B_loaded_scale.z);
++unsigned int __c=(ZERO);
++VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
++(B_loaded_fp16.z)=*(unsigned int*)&__d;
++}
++}
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.w);
++unsigned int __b=(B_loaded_zero.w);
++VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
++(B_loaded_fp16.w)=*(unsigned int*)&__d;
++}
++}
+ 
 +
-+// blocktileM/PACK_RATIOtileMzeroszerosint432bits8zeros
-+#define LDG_ZEROS                                                                                                                              \
-+    {                                                                                                                                          \
-+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO && (bidx * tileM + tid * PACK_RATIO < align_m), false); \
-+    }
++// >>>> PTX2CPP Success <<<<
++{
++{
++unsigned int __a=(B_loaded_fp16.w);
++unsigned int __b=(B_loaded_scale.w);
++unsigned int __c=(ZERO);
++VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
++(B_loaded_fp16.w)=*(unsigned int*)&__d;
++}
++}
 +
-+// blocktileM/(sizeof(uint32_t)/sizeof(__half))tileMscalesscalefp1632bits2scales
-+#define LDG_SCALES                                                                                                      \
-+    {                                                                                                                   \
-+        ldg_b32_bsm_async(sts_scales_offset, ldg_scales_offset, tid < 64 && (bidx * tileM + tid * 2 < align_m), false); \
++#endif
+   *(uint4*)B_shared_ptr2 = B_loaded_fp16;
++#endif
+ 
+   for (int i = 0; i < 8; ++i) {
+     *(C_ptr2 + i) = B_shared[i];
+   }
+ }
+ 
++template<typename T>
++__global__ void dequantize_weights_opt(int* __restrict__ B, T* __restrict__ scaling_factors,
++                       int* __restrict__ zeros, T* __restrict__ C, int G, int length, int blocksize, int num_elems, DivModFast length_fast) {
++    constexpr int N = 8;
++    T B_loaded_scale[8];
++    T B_shared[8];
++    int tid = blockIdx.x * blocksize + threadIdx.x;
++    if(tid >= num_elems) return;
++    // int row = tid / length;
++    // int col = tid % length;
++    int row, col;
++    length_fast.divmod(tid, row, col);
++    int group_row = row / G;
++    int group_offset = group_row * length + col;
++    int offset = row * length + col;
++    uint32_t* ptr_zeros = (uint32_t*)(zeros + group_offset);
++    uint32_t* ptr_B = (uint32_t*)(B + offset);
++    T* ptr_scale = scaling_factors + group_offset * N;
++    T* ptr_C = C + offset * N;
++    uint32_t zeros_loaded = *(uint32_t*)ptr_zeros;
++    uint32_t B_loaded = *(uint32_t*)ptr_B;
++    *(uint4*)(B_loaded_scale) = *(uint4*)(ptr_scale);
++    hgemm_marlin_gptq::awq_dequant_4bits<T>(B_loaded,B_shared, B_loaded_scale, zeros_loaded);
++    *(float4*)(ptr_C) = *(float4*)(B_shared);
++}
++
++template <int BLOCK_SIZE>
++__global__ void awq_to_gptq_4bit(uint32_t *output, const uint32_t *input, int k, int n) {
++    constexpr int COMPACT_FACTOR = 8;
++    constexpr int QBIT = 4;
++    int tid = threadIdx.x;
++    int tile_idx = blockIdx.x * BLOCK_SIZE + tid;
++    int N_COMPACT = (n + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
++    int K_COMPACT = (k + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
++    int tile_n_idx = tile_idx / K_COMPACT;
++    int tile_k_idx = tile_idx % K_COMPACT;
++
++    uint32_t awq_data[COMPACT_FACTOR];
++    uint32_t temp_data[COMPACT_FACTOR];
++    uint32_t gptq_data[COMPACT_FACTOR];
++
++    int gptq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
++    int awq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
++
++    // load k8xn8
++    #pragma unroll
++    for (int i = 0; i < COMPACT_FACTOR; i++) {
++        int gvm_addr_offset = (tile_k_idx * COMPACT_FACTOR + i) * N_COMPACT + tile_n_idx;
++        int pred_k = tile_k_idx * COMPACT_FACTOR + i < k;
++        int pred_n = tile_n_idx * COMPACT_FACTOR < n;
++        if (pred_k && pred_n) {
++            awq_data[i] = *(input + gvm_addr_offset);
++        }
 +    }
 +
-+#define LDS_ZEROS                         \
-+    {                                     \
-+        rgZeros[0] = lds_zeros_offset[0]; \
++    // decompress awq_data and recompress to gptq_data
++    #pragma unroll
++    for (int i = 0; i < COMPACT_FACTOR; i++) {
++        #pragma unroll
++        for(int j = 0; j < COMPACT_FACTOR; j++) {
++            temp_data[j] = ((awq_data[j] >> (awq_shift[i] * QBIT)) & 0xf);
++        }
++        #pragma unroll
++        for(int j = 0; j < COMPACT_FACTOR; j++) {
++            gptq_data[i] &= (~(0xf << (gptq_shift[j] * QBIT)));
++            gptq_data[i] |= temp_data[j] << (gptq_shift[j] * QBIT);
++
++        }
 +    }
 +
-+#define LDS_SCALES                                     \
-+    {                                                  \
-+        rgScales[0] = cast_b128(lds_scales_offset)[0]; \
++    // store k8xn8
++    #pragma unroll
++    for (int i = 0; i < COMPACT_FACTOR; i++) {
++        int gvm_addr_offset = tile_k_idx * n + tile_n_idx * COMPACT_FACTOR + i;
++        int pred_k = tile_k_idx * COMPACT_FACTOR < k;
++        int pred_n = tile_n_idx * COMPACT_FACTOR + i < n;
++        if (pred_k && pred_n) {
++            *(output + gvm_addr_offset) = gptq_data[i];
++        } else {
++            *(output + gvm_addr_offset) = 0x00000000;
++        }
 +    }
++}
 +
-+// zeros 0 2 4 6 1 3 5 7  index_shfl = index % 2 * 4 + index / 2
-+//    int4zerosint32weightAfp16Weight_Q=Scale*(Weight_4Bit-ZeroPoint)
-+#define PERM_ELEM(index)                                                                                                          \
-+    {                                                                                                                             \
-+        __half_raw elem;                                                                                                          \
-+        if constexpr (index & 0x1)                                                                                                \
-+        {                                                                                                                         \
-+            elem.x = cast_b32(rgScales)[index / 2] >> 16;                                                                         \
-+        }                                                                                                                         \
-+        else                                                                                                                      \
-+        {                                                                                                                         \
-+            elem.x = cast_b32(rgScales)[index / 2] & 0xffff;                                                                      \
-+        }                                                                                                                         \
-+        __half scale = __half(elem);                                                                                              \
-+        constexpr int index_shfl = index % 2 * 4 + index / 2;                                                                     \
-+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[0], QBITS * index_shfl, QBITS);                                                \
-+        cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[0], QBITS * index_shfl, QBITS) -  \
-+                                                                         zero) , scale);                                                                                    \
-+        cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[1], QBITS * index_shfl, QBITS) -  \
-+                                                                         zero) , scale);                                                                                    \
-+        cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[2], QBITS * index_shfl, QBITS) -  \
-+                                                                         zero) , scale);                                                                                    \
-+        cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[3], QBITS * index_shfl, QBITS) -  \
-+                                                                         zero) , scale);                                                                                    \
-+        cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[4], QBITS * index_shfl, QBITS) - \
-+                                                                          zero) , scale);                                                                                   \
-+        cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[5], QBITS * index_shfl, QBITS) - \
-+                                                                          zero) , scale);                                                                                   \
-+        cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[6], QBITS * index_shfl, QBITS) - \
-+                                                                          zero) , scale);                                                                                   \
-+        cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[7], QBITS * index_shfl, QBITS) - \
-+                                                                          zero) , scale);                                                                                   \
++
++template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
++bool launch_gemm_gptq(int m,
++                      int n,
++                      int k,
++                      int quant_group,
++                      const input_tp *dA,
++                      int lda,
++                      const quant_packed_tp *dB,
++                      int ldb,
++                      output_tp *dC,
++		      float *dC_temp,
++                      int ldc,
++                      quant_packed_tp *d_zeros,
++                      input_tp *d_scales,
++                      const cudaStream_t stream,
++                      int chunks = 1) {
++    using namespace hgemm_marlin_gptq;
++    if(n % 16 != 0) {
++        printf("n %% 16 != 0, n = %d\n", n);
++        return false;
++    }
++    if(k % 32 != 0) {
++        printf("k %% 32 != 0, k = %d\n", k);
++        return false;
++    }
++    //const vllm::ScalarTypeId w_type_id = vllm::kU4B8.id();
++    const int THREADS = 256;
++    int BLOCKS_M = div_ceil(m, SLICE_M);
++    if(BLOCKS_M >= MAX_BLOCKS_M && BLOCKS_M % MAX_BLOCKS_M != 0) {
++        printf("Error: input m is error, m = %d, blocks_m = %d\n", m, BLOCKS_M);
++        return false;
++    }
++    if (BLOCKS_M > MAX_BLOCKS_M) BLOCKS_M = MAX_BLOCKS_M;
++    int BLOCKS_N = 8;
++    //It is better let TILE_K = quant_group
++    //But if quant_group is too large, a quant_group can be divided into two parts
++    int BLOCKS_K = quant_group / SLICE_K;
++    if (quant_group > 128) BLOCKS_K = 128 / SLICE_K;
++    //if (BLOCKS_M == 1 || BLOCKS_M == 2) {
++    //    BLOCKS_N = 16;
++    //}
++    const bool HAS_ACT_ORDER = false;
++    const bool HAS_ZP = (w_type_id == vllm::kU4.id()) || (w_type_id == vllm::kU8.id());
++    int *g_idx = nullptr;
++    bool HAS_NK_PRED = true;
++    bool HAS_M_PRED = true;
++    if (n % TILE_N == 0 && k % TILE_K == 0) {
++        HAS_NK_PRED = false;
++    }
++    if (m % TILE_M == 0) {
++        HAS_M_PRED = false;
 +    }
 +
-+#define PERM_A       \
-+    {                \
-+        PERM_ELEM(0) \
-+        PERM_ELEM(1) \
-+        PERM_ELEM(2) \
-+        PERM_ELEM(3) \
-+        PERM_ELEM(4) \
-+        PERM_ELEM(5) \
-+        PERM_ELEM(6) \
-+        PERM_ELEM(7) \
++#define LAUNCH_AWQ(threads, bm, bn, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    else if (THREADS == threads && BLOCKS_M == bm && BLOCKS_N == bn \
++        && BLOCKS_K == bk  && HAS_ACT_ORDER == has_act_order \
++        && HAS_ZP == has_zp \
++        && HAS_M_PRED == has_m_pred && HAS_NK_PRED == has_nk_pred) { \
++            launch_gemm_gptq_kernel<input_tp, w_type_id, \
++                    threads, bm, bn, bk, has_act_order, has_zp, has_m_pred, has_nk_pred>( \
++                    (const PackTypeInt4*)dA, \
++                    (const PackTypeInt4*)dB, \
++                    (PackTypeInt4*)dC, \
++                    (PackTypeInt4*)dC_temp, \
++                    (const PackTypeInt4*)d_scales, \
++                    (const PackTypeInt4*)d_zeros, \
++                    nullptr, m, n, k, quant_group, chunks,\
++                    stream); \
 +    }
 +
-+#define MMA_ELEM(index_m)                                        \
-+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
-+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 0, 2)],       \
-+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
-+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
-+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 1, 2)],       \
-+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);          \
-+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
-+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 0, 2)],       \
-+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
-+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
-+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 1, 2)],       \
-+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);
++#define LAUNCH_AWQ_K(bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ(256, 1, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ(256, 2, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ(256, 3, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ(256, 4, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ(256, 1, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ(256, 2, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred)
 +
-+#define MMA     \
-+    MMA_ELEM(0) \
-+    MMA_ELEM(1) \
-+    MMA_ELEM(2) \
-+    MMA_ELEM(3) \
-+    MMA_ELEM(4) \
-+    MMA_ELEM(5) \
-+    MMA_ELEM(6) \
-+    MMA_ELEM(7)
-diff --git a/csrc/quantization/awq/dequant.cuh b/csrc/quantization/awq/dequant.cuh
-new file mode 100644
-index 000000000..a4fc08831
---- /dev/null
-+++ b/csrc/quantization/awq/dequant.cuh
-@@ -0,0 +1,11 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+#pragma once
 +
-+#define BFE(source, bit_start, num_bits) __builtin_mxc_ubfe(source, bit_start, num_bits);
++#define LAUNCH_AWQ_ZP(has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ_K(1, false, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ_K(2, false, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ_K(4, false, has_zp, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ_K(8, false, has_zp, has_nk_pred, has_m_pred)
 +
-+template <typename outputT, typename inputT, int qbits>
-+__device__ __forceinline__ outputT extract(inputT *input, size_t loc)
-+{
-+    constexpr int widthPerElem = sizeof(inputT) * 8 / qbits;
-+    return (outputT)BFE(input[loc / widthPerElem], loc % widthPerElem, qbits);
-+}
-\ No newline at end of file
-diff --git a/csrc/quantization/awq/dequantize.cuh b/csrc/quantization/awq/dequantize.cuh
-index 5fa4b5f64..6d65f3a2b 100644
---- a/csrc/quantization/awq/dequantize.cuh
-+++ b/csrc/quantization/awq/dequantize.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
- Adapted from https://github.com/mit-han-lab/llm-awq
- Modified from NVIDIA FasterTransformer:
-@@ -40,6 +41,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
-   // dependency if we issue immediately before required.
-   const uint32_t top_i4s = i4s >> 8;
-   // Extract elt_01 - (i4s & 0x000f000f) | 0x64006400
-+#ifdef MX_MACA
-   asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
-                : "=r"(h[0])
-                : "r"(i4s), "n"(BOTTOM_MASK), "n"(I4s_TO_F16s_MAGIC_NUM),
-@@ -59,7 +61,63 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
-                : "=r"(h[3])
-                : "r"(top_i4s), "n"(TOP_MASK), "n"(I4s_TO_F16s_MAGIC_NUM),
-                  "n"(immLut));
-+#else
-+      // >>>> PTX2CPP Success <<<<
-+{
-+(h[0])=0;
-+if((immLut)&0x01)(h[0])|=~(i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x02)(h[0])|=~(i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x04)(h[0])|=~(i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x08)(h[0])|=~(i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x10)(h[0])|= (i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x20)(h[0])|= (i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x40)(h[0])|= (i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x80)(h[0])|= (i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+}
++#define LAUNCH_AWQ_PRED(has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ_ZP(false, has_nk_pred, has_m_pred) \
++    LAUNCH_AWQ_ZP(true, has_nk_pred, has_m_pred)
 +
-+    // Extract elt_23 (i4s & 0x00f000f0) | 0x64006400
++    if (false) {
 +
-+// >>>> PTX2CPP Success <<<<
-+{
-+(h[1])=0;
-+if((immLut)&0x01)(h[1])|=~(i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x02)(h[1])|=~(i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x04)(h[1])|=~(i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x08)(h[1])|=~(i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x10)(h[1])|= (i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x20)(h[1])|= (i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x40)(h[1])|= (i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x80)(h[1])|= (i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+}
-+// >>>> PTX2CPP Success <<<<
-+{
-+(h[2])=0;
-+if((immLut)&0x01)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x02)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x04)(h[2])|=~(top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x08)(h[2])|=~(top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x10)(h[2])|= (top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x20)(h[2])|= (top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x40)(h[2])|= (top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x80)(h[2])|= (top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+}
-+    // Extract elt_67 (top_i4s & 0x00f000f0) | 0x64006400
- 
-+// >>>> PTX2CPP Success <<<<
-+{
-+(h[3])=0;
-+if((immLut)&0x01)(h[3])|=~(top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x02)(h[3])|=~(top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x04)(h[3])|=~(top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x08)(h[3])|=~(top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x10)(h[3])|= (top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x20)(h[3])|= (top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x40)(h[3])|= (top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
-+if((immLut)&0x80)(h[3])|= (top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
++    }
++    LAUNCH_AWQ_PRED(true, true)
++    LAUNCH_AWQ_PRED(true, false)
++    LAUNCH_AWQ_PRED(false, true)
++    LAUNCH_AWQ_PRED(false, false)
++    else {
++        printf("BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_k=%d, THREADS=%d, HAS_ACT_ORDER=%d, HAS_ZP=%d, quant_group=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d is not supported\n",
++        BLOCKS_M, BLOCKS_N, BLOCKS_K, THREADS, HAS_ACT_ORDER, HAS_ZP, quant_group, HAS_M_PRED, HAS_NK_PRED);
++        return false;
++    }
++
++    return true;
 +}
 +
 +
-+#endif
-   // I use inline PTX below because I am not sure if the compiler will emit
-   // float2half instructions if I use the half2 ctor. In this case, I chose
-   // performance reliability over code readability.
-@@ -77,6 +135,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
- 
-   // Finally, we construct the output numbers.
-   // Convert elt_01
-+#ifdef MX_MACA
-   asm volatile("sub.f16x2 %0, %1, %2;\n"
-                : "=r"(h[0])
-                : "r"(h[0]), "r"(FP16_TOP_MAGIC_NUM));
-@@ -92,7 +151,54 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
-   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
-                : "=r"(h[3])
-                : "r"(h[3]), "r"(ONE_SIXTEENTH), "r"(NEG_64));
-+#else
-+    // >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(h[0]);
-+unsigned int __b=(FP16_TOP_MAGIC_NUM);
-+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
-+(h[0])=*(unsigned int*)&__d;
-+}
++template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
++bool launch_gemm(int quant_group,
++                int m,
++                int n,
++                int k,
++                const input_tp *dA,
++                int lda,
++                const quant_packed_tp *dB,
++                int ldb,
++                output_tp *dC,
++		            float* dC_temp,
++                int ldc,
++                quant_packed_tp *d_zeros,
++                input_tp *d_scales,
++		            const cudaStream_t stream) {
++    using namespace hgemm_marlin_gptq;
++    //constexpr int max_blocks_m = 4;
++    int total_m_blocks = div_ceil(m, SLICE_M);
++    int chunks = total_m_blocks / MAX_BLOCKS_M;
++    int rest_blocks_m = total_m_blocks % MAX_BLOCKS_M;
++    // printf("m=%d,n=%d,k=%d,lda=%d,ldb=%d,ldc=%d,total_m_blocks=%d,chunks=%d,rest_blocks_m=%d\n",
++    //     m, n, k, lda, ldb, ldc, total_m_blocks, chunks, rest_blocks_m
++    // );
++    //const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
++    // const int quant_group = 128;
++    bool ret = true;
++    if (chunks > 0) {
++        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
++        ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(real_m, n, k, quant_group, dA, lda, dB, ldb, dC, dC_temp, ldc, d_zeros, d_scales, stream, chunks);
++    }
++    if (rest_blocks_m > 0) {
++        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
++        ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(m - m_offset, n, k, quant_group, dA + lda * m_offset, lda, dB, ldb, dC + ldc * m_offset, dC_temp + ldc * m_offset, ldc, d_zeros, d_scales, stream, 1);
++    }
++
++    return ret;
 +}
 +
-+    // Convert elt_23
+ }  // namespace awq
+ }  // namespace vllm
+ 
++torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight) {
 +
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(h[1]);
-+unsigned int __b=(ONE_SIXTEENTH);
-+unsigned int __c=(NEG_64);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(h[1])=*(unsigned int*)&__d;
-+}
-+}
-+    // Convert elt_45
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(h[2]);
-+unsigned int __b=(FP16_TOP_MAGIC_NUM);
-+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
-+(h[2])=*(unsigned int*)&__d;
-+}
-+}
++  const at::cuda::OptionalCUDAGuard device_guard(device_of(qweight));
++  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 +
-+    // Convert elt_67
++  const uint32_t* qweight_ptr = reinterpret_cast<const uint32_t*>(qweight.data_ptr<int>());
 +
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(h[3]);
-+unsigned int __b=(ONE_SIXTEENTH);
-+unsigned int __c=(NEG_64);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(h[3])=*(unsigned int*)&__d;
-+}
++  int num_in_channels = qweight.size(0);
++  int num_out_channels = qweight.size(1) * 8;
++
++  int compact_n = (num_out_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;
++  int compact_output_k = (num_in_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;;
++
++  int block_size = 256;
++  int tile_all_num = compact_n * compact_output_k;
++  int grid_size = (tile_all_num + 255) / 256;
++
++  auto options = torch::TensorOptions()
++                     .dtype(qweight.dtype())
++                     .device(qweight.device());
++
++  torch::Tensor out = torch::zeros({num_out_channels,  compact_output_k}, options);
++  uint32_t* out_ptr = reinterpret_cast<uint32_t*>(out.data_ptr<int>());
++
++  vllm::awq::awq_to_gptq_4bit<256><<<grid_size, block_size, 0, stream>>>((uint32_t*)out_ptr, (const uint32_t*)qweight_ptr, num_in_channels, num_out_channels);
++
++  return out;
 +}
++
+ torch::Tensor awq_dequantize(torch::Tensor _kernel,
+                              torch::Tensor _scaling_factors,
+                              torch::Tensor _zeros, int64_t split_k_iters,
+@@ -419,24 +824,6 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
+   int out_c = qout_c * 8;
+   int G = in_c / _scaling_factors.size(0);
  
-+#endif
-   return result;
- #endif
-   __builtin_unreachable();  // Suppress missing return statement warning
-diff --git a/csrc/quantization/awq/gemm_kernels.cu b/csrc/quantization/awq/gemm_kernels.cu
-index 9da724a1b..5c495577e 100644
---- a/csrc/quantization/awq/gemm_kernels.cu
-+++ b/csrc/quantization/awq/gemm_kernels.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
- Adapted from https://github.com/mit-han-lab/llm-awq
- @article{lin2023awq,
-@@ -14,9 +15,21 @@ Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+-  int x_thread = thx;
+-  int y_thread = thy;
+-
+-  int x_blocks = 1;
+-  int y_blocks = 1;
+-  if (thx == 0) {
+-    x_thread = qout_c;
+-  }
+-  if (thy == 0) {
+-    y_thread = in_c;
+-  }
+-  if (thx == 0 && thy == 0) {
+-    x_thread = 8;
+-    y_thread = 8;
+-    x_blocks = (int)(qout_c / 8);
+-    y_blocks = (int)(in_c / 8);
+-  }
+-
+   const at::cuda::OptionalCUDAGuard device_guard(device_of(_scaling_factors));
  
- #include <cuda_fp16.h>
+   auto options = torch::TensorOptions()
+@@ -444,19 +831,48 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
+                      .device(_scaling_factors.device());
+   at::Tensor _de_kernel = torch::empty({in_c, out_c}, options);
  
-+#include "../gptq/hgemm_gptq.h"
-+#include "../gptq/scalar_type.hpp"
-+
-+//#include "hgemv_nn_splitk_awq.hpp"
-+//#include "hgemv_selector.hpp"
-+//#include "Hgemm_nn_128x32x128_8m1n8k_awq.hpp"
-+
- namespace vllm {
- namespace awq {
-+#define input_type __half
-+#define output_type __half
-+#define quant_packed_type uint32_t
-+#define QUANT_GROUP 128
++  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+   auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
+-  auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());
+-  auto scaling_factors =
+-      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+   auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
+-
++#if 1
++  int blocksize = 512;
++  int num_elems = in_c * qout_c;
++  int gridsize = (num_elems + blocksize - 1) / blocksize;
++  if(_scaling_factors.dtype() == at::ScalarType::Half) {
++    auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());
++    auto scaling_factors =
++        reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
++    vllm::awq::dequantize_weights_opt<__half><<<gridsize, blocksize, 0 , stream>>>(kernel, scaling_factors, zeros, de_kernel, G, qout_c, blocksize, num_elems, vllm::awq::DivModFast(qout_c));
++  } else if(_scaling_factors.dtype() == at::ScalarType::BFloat16) {
++    auto de_kernel = reinterpret_cast<maca_bfloat16*>(_de_kernel.data_ptr<at::BFloat16>());
++    auto scaling_factors =
++        reinterpret_cast<maca_bfloat16*>(_scaling_factors.data_ptr<at::BFloat16>());
++    vllm::awq::dequantize_weights_opt<maca_bfloat16><<<gridsize, blocksize, 0, stream>>>(kernel, scaling_factors, zeros, de_kernel, G, qout_c, blocksize, num_elems, vllm::awq::DivModFast(qout_c));
++  } else {
++    printf("not support this type\n");
++    assert(0);
++  }
++#else
+   dim3 num_blocks(x_blocks, y_blocks);
+   dim3 threads_per_block(x_thread, y_thread);
  
-+#if 0
- template <int N>
- __global__ void __launch_bounds__(64)
-     gemm_forward_4bit_cuda_m16nXk32(int G, int split_k_iters,
-@@ -136,6 +149,7 @@ __global__ void __launch_bounds__(64)
-       // - zero and * scale
-       // TODO (Haotian): can save 4 assembly instructions if sormulate as deq =
-       // q * scale - zero * scale.
-+#ifdef MX_MACA
-       asm volatile("sub.f16x2 %0, %1, %2;\n"
-                    : "=r"(B_loaded_fp16.x)
-                    : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
-@@ -160,6 +174,7 @@ __global__ void __launch_bounds__(64)
-       asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
-                    : "=r"(B_loaded_fp16.w)
-                    : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
-+#endif
-       /*
-       if (ax0_ax1_fused_0 == 0 && blockIdx_z == 0 && blockIdx_y == 0 && k_0_0 ==
-       0 && threadIdx.x == 17 && threadIdx.y == 0){ printf("[x] %X %X %X %X\n",
-@@ -176,6 +191,7 @@ __global__ void __launch_bounds__(64)
-     for (int k_0_1 = 0; k_0_1 < 2; ++k_0_1) {
-       {
-         unsigned int addr;
-+#ifdef MX_MACA
-         __asm__ __volatile__(
-             "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
-             "addr; }\n"
-@@ -192,11 +208,13 @@ __global__ void __launch_bounds__(64)
-               "=r"(((unsigned*)(A_shared_warp + 0))[2]),
-               "=r"(((unsigned*)(A_shared_warp + 0))[3])
-             : "r"(addr));
+-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+-  vllm::awq::dequantize_weights<<<num_blocks, threads_per_block, 0, stream>>>(
+-      kernel, scaling_factors, zeros, de_kernel, G);
+-
++  if(_scaling_factors.dtype() == at::ScalarType::Half) {
++    auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());
++    auto scaling_factors =
++        reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
++    vllm::awq::dequantize_weights<__half,__half2><<<num_blocks, threads_per_block, 0, stream>>>(
++        kernel, scaling_factors, zeros, de_kernel, G);
++  } else if(_scaling_factors.dtype() == at::ScalarType::BFloat16) {
++    auto de_kernel = reinterpret_cast<maca_bfloat16*>(_de_kernel.data_ptr<at::BFloat16>());
++    auto scaling_factors =
++        reinterpret_cast<maca_bfloat16*>(_scaling_factors.data_ptr<at::BFloat16>());
++    vllm::awq::dequantize_weights<maca_bfloat16,maca_bfloat162><<<num_blocks, threads_per_block, 0, stream>>>(
++        kernel, scaling_factors, zeros, de_kernel, G);
++  } else {
++    printf("not support this type\n");
++    assert(0);
++  }
 +#endif
-       }
+   return _de_kernel;
+ }
  
-       for (int ax1_0 = 0; ax1_0 < N / 32; ++ax1_0) {
-         {
-           unsigned int addr;
-+#ifdef MX_MACA
-           __asm__ __volatile__(
-               "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
-               "addr; }\n"
-@@ -214,9 +232,11 @@ __global__ void __launch_bounds__(64)
-                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[2]),
-                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[3])
-               : "r"(addr));
-+#endif
-         }
-       }
-       for (int j_0_4 = 0; j_0_4 < N / 32; ++j_0_4) {
-+#ifdef MX_MACA
-   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 750
-         {
-           __asm__ __volatile__(
-@@ -329,6 +349,7 @@ __global__ void __launch_bounds__(64)
-         }
+@@ -468,59 +884,49 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
  
-   #endif
-+#endif
-       }
-     }
-   }
-@@ -346,6 +367,7 @@ __global__ void __launch_bounds__(64)
-   }
- #endif
- }
-+#endif
+ torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
+                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
+-                       int64_t split_k_iters) {
++                       int64_t split_k_iters,
++                       torch::Tensor _temp_space,
++                       bool dtype_bf16) {
+   int num_in_feats = _in_feats.size(0);
+   int num_in_channels = _in_feats.size(1);
+   const at::cuda::OptionalCUDAGuard device_guard(device_of(_in_feats));
++  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
  
- __global__ void __launch_bounds__(64)
-     dequantize_weights(int* __restrict__ B, half* __restrict__ scaling_factors,
-@@ -375,6 +397,7 @@ __global__ void __launch_bounds__(64)
+   auto options = torch::TensorOptions()
+                      .dtype(_in_feats.dtype())
+                      .device(_in_feats.device());
++                     
++  // int num_out_channels = _kernel.size(1) * 8;
++  int num_out_channels = _kernel.size(0); 
+   at::Tensor _out_feats =
+-      torch::empty({split_k_iters, num_in_feats, _kernel.size(1) * 8}, options);
+-  int num_out_feats = _out_feats.size(-2);
+-  int num_out_channels = _out_feats.size(-1);
++      torch::zeros({num_in_feats, num_out_channels}, options);
  
-   uint32_t B_loaded = *(uint32_t*)B_ptr2;
-   uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2(B_loaded);
-+#ifdef MX_MACA
-   asm volatile("sub.f16x2 %0, %1, %2;\n"
-                : "=r"(B_loaded_fp16.x)
-                : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
-@@ -399,7 +422,91 @@ __global__ void __launch_bounds__(64)
-   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
-                : "=r"(B_loaded_fp16.w)
-                : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
-+#else
-+     // >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.x);
-+unsigned int __b=(B_loaded_zero.x);
-+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
-+(B_loaded_fp16.x)=*(unsigned int*)&__d;
-+}
-+}
-+
-+
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.x);
-+unsigned int __b=(B_loaded_scale.x);
-+unsigned int __c=(ZERO);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(B_loaded_fp16.x)=*(unsigned int*)&__d;
-+}
-+}
+-  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
++  //auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
+   auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
+-  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
+-  auto scaling_factors =
+-      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
++  //auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
++  //auto scaling_factors =
++  //    reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+   auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
++  auto temp_space = reinterpret_cast<float*>(_temp_space.data_ptr<float>());
+   int group_size = num_in_channels / _scaling_factors.size(0);
  
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.y);
-+unsigned int __b=(B_loaded_zero.y);
-+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
-+(B_loaded_fp16.y)=*(unsigned int*)&__d;
-+}
-+}
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.y);
-+unsigned int __b=(B_loaded_scale.y);
-+unsigned int __c=(ZERO);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(B_loaded_fp16.y)=*(unsigned int*)&__d;
-+}
-+}
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.z);
-+unsigned int __b=(B_loaded_zero.z);
-+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
-+(B_loaded_fp16.z)=*(unsigned int*)&__d;
-+}
-+}
+-  if (num_out_channels % 64 != 0)
+-    throw std::invalid_argument("OC is not multiple of cta_N = 64");
+-  if (num_out_channels % 8 != 0)
+-    throw std::invalid_argument("OC is not multiple of pack_num = 8");
+-  if (group_size % 32 != 0)
+-    throw std::invalid_argument("Group size should be a multiple of 32");
+-  if (num_out_channels % group_size != 0)
+-    throw std::invalid_argument("OC is not multiple of Group size");
+-
+-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+-  if (num_out_channels % 128 == 0) {
+-    int j_factors1 = num_out_channels / 128 / 1;
+-    dim3 num_blocks((num_out_feats + 16 - 1) / 16 * j_factors1 * split_k_iters);
+-    // threadIdx.x: 32
+-    // threadIdx.y: i_factors[2] * j_factors[2]
+-    dim3 threads_per_block(32, 2);
+-    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<128>
+-        <<<num_blocks, threads_per_block, 0, stream>>>(
+-            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
+-            num_in_feats, num_in_channels, num_out_channels, out_feats);
+-  } else if (num_out_channels % 64 == 0) {
+-    int j_factors1 = num_out_channels / 64 / 1;
+-    dim3 num_blocks(1 * (num_out_feats + 16 - 1) / 16 * j_factors1 *
+-                    split_k_iters);
+-
+-    // threadIdx.x: 32
+-    // threadIdx.y: i_factors[2] * j_factors[2]
+-    dim3 threads_per_block(32, 2);
+-    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<64>
+-        <<<num_blocks, threads_per_block, 0, stream>>>(
+-            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
+-            num_in_feats, num_in_channels, num_out_channels, out_feats);
++  int lda = num_in_channels;
++  int ldb = num_out_channels;
++  int ldc = num_out_channels;
 +
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.z);
-+unsigned int __b=(B_loaded_scale.z);
-+unsigned int __c=(ZERO);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(B_loaded_fp16.z)=*(unsigned int*)&__d;
-+}
-+}
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.w);
-+unsigned int __b=(B_loaded_zero.w);
-+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
-+(B_loaded_fp16.w)=*(unsigned int*)&__d;
-+}
-+}
++  if (dtype_bf16) {
++	  using scalar_t = __maca_bfloat16;
++	  vllm::awq::launch_gemm<scalar_t, vllm::kU4.id(), scalar_t, quant_packed_type>(group_size, num_in_feats, num_out_channels, num_in_channels,
++                                     (const scalar_t*)_in_feats.data_ptr(), lda, (const uint32_t*)kernel, ldb, (scalar_t*)_out_feats.data_ptr(), temp_space, ldc,
++                                     (uint32_t*)zeros, (scalar_t*)_scaling_factors.data_ptr(), stream);
++  } else {
++	  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
++	  auto scaling_factors = reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
++	  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
++	  vllm::awq::launch_gemm<input_type, vllm::kU4.id(), output_type, quant_packed_type>(group_size, num_in_feats, num_out_channels, num_in_channels,
++                                     (const half*)in_feats, lda, (const uint32_t*)kernel, ldb, (half*)out_feats, nullptr, ldc,
++                                     (uint32_t*)zeros, (half*)scaling_factors, stream);
+   }
+-  return _out_feats.sum(0);
 +
++  return _out_feats;
+ }
+diff --git a/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
+new file mode 100644
+index 000000000..39570b9da
+--- /dev/null
++++ b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
+@@ -0,0 +1,520 @@
++/**
++ * @file hgemv_nn_splitk.hpp
++ * @author Jiawen Yang (jiawen.yang@metax-tech.com)
++ * @brief *
++ * @version 0.1
++ * @date 2024-03-05
++ * @copyright Copyright (c) 2024
++ *
++ *   fp16 gemv kernel template for some gemv cases
++ *   Note:
++ *    1. BlockDimX * BlockDimY = 64, and BlockDimX should be 8/16/32/64
++ *    2. LoopNum % 2 == 0, so Load_B can use ldg_b32 or ldg_b64
++ *    3. m % (BlockDimX * 8) == 0
++ *    4. k % (ThreadBlock / BlockDimX * LoopNum * SplitKNum) = 0
++ *
++ *    A load layout:
++ *
++ *       **************************** Wave_0 ******************* | Wave_1  ...
++ *       ********* Repeat LoopNum *********                      |
++ *       tid_0(ldg_b128)   tid_0 ... tid_0 | tid_(BlockDimX) ... |
++ *       tid_1                             |                     |
++ *       tid_2                             |                     |
++ *                                       |                     |
++ *       tid_(BlockDimX-1)                 |                     |
++ *
++ */
++#pragma once
 +
-+// >>>> PTX2CPP Success <<<<
-+{
-+{
-+unsigned int __a=(B_loaded_fp16.w);
-+unsigned int __b=(B_loaded_scale.w);
-+unsigned int __c=(ZERO);
-+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
-+(B_loaded_fp16.w)=*(unsigned int*)&__d;
-+}
-+}
++#include <mc_runtime.h>
++#include <maca_fp16.h>
++#include "../gptq/Hgemm_common.cuh"
++#include "dequant.cuh"
++#define quant_packed_type uint32_t
++typedef __NATIVE_VECTOR__(2, float) v2f;
 +
-+#endif
-   *(uint4*)B_shared_ptr2 = B_loaded_fp16;
- 
-   for (int i = 0; i < 8; ++i) {
-@@ -407,9 +514,392 @@ __global__ void __launch_bounds__(64)
-   }
- }
- 
-+#if 0
-+template <typename dstT, typename srcT, typename scalarT>
-+__global__ void blasMemcpy(dstT *dst, const srcT *src, size_t cnt, scalarT beta) {
-+    size_t threads = gridDim.x * blockDim.x;
-+    size_t itemsPerThread = (cnt + threads - 1) / threads;
-+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
-+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
-+        dst[loop * threads + tid] =
-+            static_cast<double>(beta) * static_cast<double>(src[loop * threads + tid]);
-+    }
-+}
++template<int N, int m_per_thread>
++__device__ __forceinline__ void dequant_fma_awq_int4(
++                const quant_packed_type& a,
++                const v2f (&scale)[m_per_thread/2],
++                const v2f (&zero)[m_per_thread/2],
++                const v2f (&b)[N],
++                v2f (&out)[N][m_per_thread/2]) {
++    uint32_t p0 = a & 0x0f0f0f0f;
++    float o1,o3;
++    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
++    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
++    v2f a0 = {o1, o3};
++    a0 = __builtin_mxc_pk_fma_f32(a0, scale[0], zero[0]);
 +
-+#define SWITCH_CASE_BATCH(BlockDimX, SplitK, BATCH) \
-+    case BATCH: {                                   \
-+        CALL_GEMM(BlockDimX, SplitK, BATCH)         \
-+        break;                                      \
++    #pragma unroll N
++    for (int y = 0; y < N; y++) {
++        out[y][0] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][0]);
 +    }
 +
-+#define APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH) \
-+    switch(BATCH) {                                 \
-+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 1)           \
-+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 2)           \
-+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 3)           \
-+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 4)           \
-+        default: {                                          \
-+            launched = false;                               \
-+            printf("ERROR: Unsupported BATCH %d\n", BATCH); \
-+            break;                                          \
-+        }                                                   \
-+    }
++    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p0));
++    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p0));
++    a0 = {o1, o3};
++    a0 = __builtin_mxc_pk_fma_f32(a0, scale[2], zero[2]);
 +
-+#define SWITCH_CASE_BlockDimX(BlockDimX, SplitK, BATCH) \
-+    case BlockDimX: {                                   \
-+        APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH)    \
-+        break;                                          \
++    #pragma unroll N
++    for (int y = 0; y < N; y++) {
++        out[y][2] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][2]);
 +    }
 +
-+#define APPLY_HGEMM(BlockDimX, SplitK, BATCH)           \
-+    switch (BlockDimX) {                                \
-+        SWITCH_CASE_BlockDimX(16, SplitK, BATCH)        \
-+        SWITCH_CASE_BlockDimX(32, SplitK, BATCH)        \
-+        SWITCH_CASE_BlockDimX(64, SplitK, BATCH)        \
-+        SWITCH_CASE_BlockDimX(128, SplitK, BATCH)       \
-+        SWITCH_CASE_BlockDimX(256, SplitK, BATCH)       \
-+        default: {                                                  \
-+            launched = false;                                       \
-+            printf("ERROR: Unsupported BlockDimX %d\n", BlockDimX); \
-+            break;                                                  \
-+        }                                                           \
++    uint32_t p1 = (a >> 4) & 0x0f0f0f0f;
++    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p1));
++    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p1));
++    a0 = {o1, o3};
++    a0 = __builtin_mxc_pk_fma_f32(a0, scale[1], zero[1]);
++
++    #pragma unroll N
++    for (int y = 0; y < N; y++) {
++        out[y][1] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][1]);
 +    }
 +
-+bool call_kernel(const half *srcB,
-+    const quant_packed_type *srcA,
-+    quant_packed_type *zeros, half *scales,
-+    half* dst_D,
-+    int m, int n, int k, int srcStride, int dstStride,
-+    int block_x, int split_k,
-+    const int* b_perm_D = nullptr) {
-+    //constexpr int PACK_RATIO = 8;
-+    constexpr int ThreadBlock = 256;
-+    const dim3 threadBlock = {static_cast<unsigned int>(ThreadBlock)};
-+    const dim3 gridBlock = {static_cast<unsigned int>((m + 8*block_x-1) / 8 / block_x), static_cast<unsigned int>(split_k)};
-+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-+    if (split_k * QUANT_GROUP > k || k % QUANT_GROUP != 0) return false;
-+    if (block_x < 16 || n > 4) return false;
-+    bool launched = true;
-+    #define CALL_GEMM(BX, SK, N) \
-+        if (SK * 128 == k) {   \
-+            hgemv_nn_splitk_awq_kb128<BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
-+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
-+        } else { \
-+            hgemv_nn_splitk_awq<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
-+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
-+        }
-+    APPLY_HGEMM(block_x, split_k, n);
-+    return launched;
-+}
++    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p1));
++    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p1));
++    a0 = {o1, o3};
++    a0 = __builtin_mxc_pk_fma_f32(a0, scale[3], zero[3]);
 +
-+void launch_gemm_awq(Operation_t trans_a,
-+                      Operation_t trans_b,
-+                      int m,
-+                      int n,
-+                      int k,
-+                      const float alpha,
-+                      const float beta,
-+                      const uint32_t* dA,
-+                      int lda,
-+                      const half* dB,
-+                      int ldb,
-+                      half* dC,
-+                      int ldc,
-+                      uint32_t* d_zeros,
-+                      half* d_scales,
-+                      float* space_mid,
-+                      cudaStream_t stream,
-+                      int splitk_iters = 1) {
-+    if (n <= 4) {
-+            constexpr int thread_block = 256;
-+            constexpr int m_per_thread = 8;
-+            auto kernel_testing = [&](int bx, int sk) -> bool {
-+                return call_kernel(dB, dA, d_zeros, d_scales, dC, m, n, k, m, m, bx, sk);
-+            };
-+            //Select parameters when warmup
-+            auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,8,m_per_thread>::selector(m, n, k, true);
-+            if (sl_warmup.valid()) {
-+                sl_warmup.run(kernel_testing);
-+            }
-+    }
-+    else {
-+            const int threads_n = 256;
-+            const int tileM = 128;
-+            const int tileN = 32;
-+            const int tileK = 128;
-+
-+            bool isSplitk = splitk_iters > 1;
-+
-+            uint32_t gridx = (m - 1) / tileM + 1;
-+            uint32_t gridy = (n - 1) / tileN + 1;
-+            uint32_t gridz = splitk_iters;
-+
-+            dim3 dimBlock(threads_n, 1, 1);
-+            dim3 dimGrid(gridx, gridy, gridz);
-+            bool isBetaZero = (beta == 0.0);
-+
-+            if (trans_a == OP_N && trans_b == OP_N && m % 8 == 0 && k % 8 == 0) {
-+                if (!isSplitk) {
-+                    if (isBetaZero)
-+                        Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, true, tileM, tileN, tileK>
-+                            <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC,
-+                                                               dC, ldc, d_zeros, d_scales);
-+                    else
-+                        Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, false, tileM, tileN, tileK>
-+                            <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC,
-+                                                               dC, ldc, d_zeros, d_scales);
-+                } else {
-+                    if (!isBetaZero)
-+                        blasMemcpy<<<104, 512, 0, stream>>>(space_mid, dC, m * n, beta);
-+                    Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true>
-+                        <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC, dC,
-+                                                           ldc, d_zeros, d_scales, splitk_iters, space_mid);
-+                    blasMemcpy<<<104, 512, 0, stream>>>(dC, space_mid, m * n, 1);
-+                }
-+            } else {
-+                printf("Parameters not supported!\n");
-+                return;
-+            }
++    #pragma unroll N
++    for (int y = 0; y < N; y++) {
++        out[y][3] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][3]);
 +    }
-+}
-+#endif
++};
 +
-+template <int BLOCK_SIZE>
-+__global__ void awq_to_gptq_4bit(uint32_t *output, const uint32_t *input, int k, int n) {
-+    constexpr int COMPACT_FACTOR = 8;
-+    constexpr int QBIT = 4;
-+    int tid = threadIdx.x;
-+    int tile_idx = blockIdx.x * BLOCK_SIZE + tid;
-+    int N_COMPACT = (n + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
-+    int K_COMPACT = (k + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
-+    int tile_n_idx = tile_idx / K_COMPACT;
-+    int tile_k_idx = tile_idx % K_COMPACT;
++template <int ThreadBlock, int BlockDimX, int BATCH>
++__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq(const half* __restrict__ srcB,
++                                                        const quant_packed_type* __restrict__ srcA,
++                                                        const quant_packed_type* __restrict__ zeros,
++                                                        const half* __restrict__ scales,
++                                                        half *dst,
++                                                        int m,
++                                                        int n,
++                                                        int k,
++                                                        int srcAStride,
++                                                        int dstStride,
++                                                        int k_div_sk,
++                                                        const int* __restrict__ b_perm = nullptr) {
++    constexpr int QBITS = 4;
++    constexpr int PACK_RATIO = (32 / QBITS);
++    constexpr int QUANT_GROUP = 128;
++    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
++    constexpr int N = BATCH;
++    const int k_stride = k;
++    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
++    const int splitKOffset = blockIdx.y * k_block;
++    if (splitKOffset + k_block > k) k = k - splitKOffset;
++    else k = k_block;
++    srcA += splitKOffset * srcAStride / PACK_RATIO;
++    //srcB += splitKOffset;
 +
-+    uint32_t awq_data[COMPACT_FACTOR];
-+    uint32_t temp_data[COMPACT_FACTOR];
-+    uint32_t gptq_data[COMPACT_FACTOR];
++    constexpr int quant_groups = 1;
++    constexpr int m_per_thread = PACK_RATIO;
++    constexpr int thread_groups = ThreadBlock / BlockDimX;
++    constexpr int group_elements = BlockDimX * m_per_thread;
++    constexpr int reduce_size = ThreadBlock * m_per_thread;
++    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
 +
-+    int gptq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
-+    int awq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
++    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
++    half *bsm_scales_ptr;
++    if constexpr(reduce_size > data_cache_size) {
++        __shared__ float bsm_ptr[reduce_size];
++        bsm_b_ptr = bsm_ptr;
++        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
++        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
++        smem = bsm_ptr;
++    } else {
++        __shared__ float bsm_ptr[data_cache_size];
++        bsm_b_ptr = bsm_ptr;
++        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
++        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
++        smem = bsm_ptr;
++    }
 +
-+    // load k8xn8
-+    #pragma unroll
-+    for (int i = 0; i < COMPACT_FACTOR; i++) {
-+        int gvm_addr_offset = (tile_k_idx * COMPACT_FACTOR + i) * N_COMPACT + tile_n_idx;
-+        int pred_k = tile_k_idx * COMPACT_FACTOR + i < k;
-+        int pred_n = tile_n_idx * COMPACT_FACTOR < n;
-+        if (pred_k && pred_n) {
-+            awq_data[i] = *(input + gvm_addr_offset);
-+        }
++    const int zeros_stride = srcAStride / PACK_RATIO;
++    const int scales_stride = srcAStride;
++    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
++    scales += splitKOffset * scales_stride / QUANT_GROUP;
++
++    dst += group_elements * blockIdx.x;
++    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
++    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
++    const int m_offset_scales = group_elements * blockIdx.x;
++
++    //store splited fma results
++    v2f c_splited[N][m_per_thread/2];
++    for (int i = 0; i < N; i++) {
++        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
 +    }
 +
-+    // decompress awq_data and recompress to gptq_data
-+    #pragma unroll
-+    for (int i = 0; i < COMPACT_FACTOR; i++) {
-+        #pragma unroll
-+        for(int j = 0; j < COMPACT_FACTOR; j++) {
-+            temp_data[j] = ((awq_data[j] >> (awq_shift[i] * QBIT)) & 0xf);
++    int tid = threadIdx.x;
++    int tidCol = tid / BlockDimX;
++    int tidRow = tid % BlockDimX;
++    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
++    int m_index = tidRow * m_per_thread;
++    for (int i = 0; i < k; i += LoopNum * thread_groups) {
++        int quant_group = i / QUANT_GROUP;
++        constexpr int loading_pack = 2;
++        int loading_count = this_group_elements / loading_pack;
++        //Load needed zeros, scales
++        const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
++        for (int x = tid; x < loading_count; x += ThreadBlock) {
++            uint8_t temp_zeros = 0;
++            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
++            half temp_scales[2];
++            int packed_group_offset = (x >> 2) << 3;
++            int packed_index = (x << 1) % 8;
++            int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
++            int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
++            //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
++            temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
++            temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
++            uint32_t z = temp_zeros;
++            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
++            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
++            float s1 = (float)(temp_scales[0]);
++            float s2 = (float)(temp_scales[1]);
++            //Store to shared memory
++            bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
++            bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
++            bsm_scales_ptr[dest_offset_0] = temp_scales[0];
++            bsm_scales_ptr[dest_offset_1] = temp_scales[1];
 +        }
-+        #pragma unroll
-+        for(int j = 0; j < COMPACT_FACTOR; j++) {
-+            gptq_data[i] &= (~(0xf << (gptq_shift[j] * QBIT)));
-+            gptq_data[i] |= temp_data[j] << (gptq_shift[j] * QBIT);
 +
-+        }
-+    }
++        int loop_index = 0;
 +
-+    // store k8xn8
-+    #pragma unroll
-+    for (int i = 0; i < COMPACT_FACTOR; i++) {
-+        int gvm_addr_offset = tile_k_idx * n + tile_n_idx * COMPACT_FACTOR + i;
-+        int pred_k = tile_k_idx * COMPACT_FACTOR < k;
-+        int pred_n = tile_n_idx * COMPACT_FACTOR + i < n;
-+        if (pred_k && pred_n) {
-+            *(output + gvm_addr_offset) = gptq_data[i];
++        //Load B and transform to float
++        if (b_perm != nullptr) {
++            for (int y = 0; y < N; y++) {
++                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
++                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
++                }
++            }
 +        } else {
-+            *(output + gvm_addr_offset) = 0x00000000;
++            for (int y = 0; y < N; y++) {
++                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
++                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
++                }
++            }
 +        }
-+    }
-+}
 +
++        __syncthreads();
 +
-+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
-+bool launch_gemm_gptq(int m,
-+                      int n,
-+                      int k,
-+                      int quant_group,
-+                      const input_tp *dA,
-+                      int lda,
-+                      const quant_packed_tp *dB,
-+                      int ldb,
-+                      output_tp *dC,
-+		      float *dC_temp,
-+                      int ldc,
-+                      quant_packed_tp *d_zeros,
-+                      input_tp *d_scales,
-+                      const cudaStream_t stream,
-+                      int chunks = 1) {
-+    using namespace hgemm_marlin_gptq;
-+    if(n % 16 != 0) {
-+        printf("n %% 16 != 0, n = %d\n", n);
-+        return false;
-+    }
-+    if(k % 32 != 0) {
-+        printf("k %% 32 != 0, k = %d\n", k);
-+        return false;
-+    }
-+    //const vllm::ScalarTypeId w_type_id = vllm::kU4B8.id();
-+    const int THREADS = 256;
-+    int BLOCKS_M = div_ceil(m, SLICE_M);
-+    if(BLOCKS_M >= MAX_BLOCKS_M && BLOCKS_M % MAX_BLOCKS_M != 0) {
-+        printf("Error: input m is error, m = %d, blocks_m = %d\n", m, BLOCKS_M);
-+        return false;
-+    }
-+    if (BLOCKS_M > MAX_BLOCKS_M) BLOCKS_M = MAX_BLOCKS_M;
-+    int BLOCKS_N = 8;
-+    //It is better let TILE_K = quant_group
-+    //But if quant_group is too large, a quant_group can be divided into two parts
-+    int BLOCKS_K = quant_group / SLICE_K;
-+    if (quant_group > 128) BLOCKS_K = 128 / SLICE_K;
-+    //if (BLOCKS_M == 1 || BLOCKS_M == 2) {
-+    //    BLOCKS_N = 16;
-+    //}
-+    const bool HAS_ACT_ORDER = false;
-+    const bool HAS_ZP = (w_type_id == vllm::kU4.id()) || (w_type_id == vllm::kU8.id());
-+    int *g_idx = nullptr;
-+    bool HAS_NK_PRED = true;
-+    bool HAS_M_PRED = true;
-+    if (n % TILE_N == 0 && k % TILE_K == 0) {
-+        HAS_NK_PRED = false;
-+    }
-+    if (m % TILE_M == 0) {
-+        HAS_M_PRED = false;
-+    }
++        //Load zero and scale from bsm
++        if (m_index < this_group_elements) {
++            v2f local_scales[m_per_thread/2];
++            for (int c = 0; c < m_per_thread / 2; c++) {
++                float s0 = (float)bsm_scales_ptr[m_index + c*2];
++                float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
++                local_scales[c] = {s0, s1};
++            }
++            v2f local_zeros[m_per_thread/2];
++            for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
 +
-+#define LAUNCH_AWQ(threads, bm, bn, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    else if (THREADS == threads && BLOCKS_M == bm && BLOCKS_N == bn \
-+        && BLOCKS_K == bk  && HAS_ACT_ORDER == has_act_order \
-+        && HAS_ZP == has_zp \
-+        && HAS_M_PRED == has_m_pred && HAS_NK_PRED == has_nk_pred) { \
-+            launch_gemm_gptq_kernel<input_tp, w_type_id, \
-+                    threads, bm, bn, bk, has_act_order, has_zp, has_m_pred, has_nk_pred>( \
-+                    (const PackTypeInt4*)dA, \
-+                    (const PackTypeInt4*)dB, \
-+                    (PackTypeInt4*)dC, \
-+                    (PackTypeInt4*)dC_temp, \
-+                    (const PackTypeInt4*)d_scales, \
-+                    (const PackTypeInt4*)d_zeros, \
-+                    nullptr, m, n, k, quant_group, chunks,\
-+                    stream); \
-+    }
++    #define DEQUANT_FMA(a, b) \
++            dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
 +
-+#define LAUNCH_AWQ_K(bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ(256, 1, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ(256, 2, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ(256, 3, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ(256, 4, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ(256, 1, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ(256, 2, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred)
++            quant_packed_type A[4];
++            const int packed_a_stride = srcAStride / PACK_RATIO;
++            int src_a_offset = (loop_index + i + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
++            A[0] = srcA[src_a_offset];
++            src_a_offset += packed_a_stride;
++            A[1] = srcA[src_a_offset];
 +
++            v2f local_b[4][N];
++            //#pragma unroll LoopNum / 4 - 1
++            for (; loop_index < LoopNum - 4; loop_index += 4) {
++                //Load A
++                src_a_offset += packed_a_stride;
++                A[2] = srcA[src_a_offset];
++                src_a_offset += packed_a_stride;
++                A[3] = srcA[src_a_offset];
 +
-+#define LAUNCH_AWQ_ZP(has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ_K(1, false, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ_K(2, false, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ_K(4, false, has_zp, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ_K(8, false, has_zp, has_nk_pred, has_m_pred)
-+
-+#define LAUNCH_AWQ_PRED(has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ_ZP(false, has_nk_pred, has_m_pred) \
-+    LAUNCH_AWQ_ZP(true, has_nk_pred, has_m_pred)
-+
-+    if (false) {
-+
++                for (int y = 0; y < N; y++) {
++                    float s[4];
++                    *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
++                    local_b[0][y] = {s[0], s[0]};
++                    local_b[1][y] = {s[1], s[1]};
++                    local_b[2][y] = {s[2], s[2]};
++                    local_b[3][y] = {s[3], s[3]};
++                }
++                DEQUANT_FMA(A[0], local_b[0])
++                DEQUANT_FMA(A[1], local_b[1])
++                src_a_offset += packed_a_stride;
++                A[0] = srcA[src_a_offset];
++                src_a_offset += packed_a_stride;
++                A[1] = srcA[src_a_offset];
++                DEQUANT_FMA(A[2], local_b[2])
++                DEQUANT_FMA(A[3], local_b[3])
++            }
++            src_a_offset += packed_a_stride;
++            A[2] = srcA[src_a_offset];
++            src_a_offset += packed_a_stride;
++            A[3] = srcA[src_a_offset];
++            for (int y = 0; y < N; y++) {
++                float s[4];
++                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
++                local_b[0][y] = {s[0], s[0]};
++                local_b[1][y] = {s[1], s[1]};
++                local_b[2][y] = {s[2], s[2]};
++                local_b[3][y] = {s[3], s[3]};
++            }
++            DEQUANT_FMA(A[0], local_b[0])
++            DEQUANT_FMA(A[1], local_b[1])
++            DEQUANT_FMA(A[2], local_b[2])
++            DEQUANT_FMA(A[3], local_b[3])
++        }
++        __syncthreads();
 +    }
-+    LAUNCH_AWQ_PRED(true, true)
-+    LAUNCH_AWQ_PRED(true, false)
-+    LAUNCH_AWQ_PRED(false, true)
-+    LAUNCH_AWQ_PRED(false, false)
-+    else {
-+        printf("BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_k=%d, THREADS=%d, HAS_ACT_ORDER=%d, HAS_ZP=%d, quant_group=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d is not supported\n",
-+        BLOCKS_M, BLOCKS_N, BLOCKS_K, THREADS, HAS_ACT_ORDER, HAS_ZP, quant_group, HAS_M_PRED, HAS_NK_PRED);
-+        return false;
++#undef DEQUANT_FMA
++    #pragma unroll N
++    for (int y = 0; y < N; y++) {
++        if (m_index < this_group_elements) {
++            for (int i = 0; i < m_per_thread/2; i++) {
++                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
++                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
++            }
++        }
++        __syncthreads();
++        constexpr int stride = ThreadBlock / BlockDimX;
++        int data_size = ThreadBlock * m_per_thread;
++        #pragma unroll
++        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
++            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
++                int reduce_group = j / i;
++                int reduce_index = j % i;
++                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
++            }
++            __syncthreads();
++            data_size /= 2;
++        }
++        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
++            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
++        }
++        if constexpr(N > 1) {
++            if (y + 1 < N) {
++                __syncthreads();
++            }
++        }
 +    }
-+
-+    return true;
 +}
 +
++template <int BX, int BATCH>
++__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq_kb128(const half* __restrict__ srcB,
++                                                        const quant_packed_type* __restrict__ srcA,
++                                                        const quant_packed_type* __restrict__ zeros,
++                                                        const half* __restrict__ scales,
++                                                        half *dst,
++                                                        int m,
++                                                        int n,
++                                                        int k,
++                                                        int srcAStride,
++                                                        int dstStride,
++                                                        int k_div_sk,
++                                                        const int* __restrict__ b_perm = nullptr) {
++    constexpr int QBITS = 4;
++    constexpr int PACK_RATIO = (32 / QBITS);
++    constexpr int QUANT_GROUP = 128;
++    constexpr int ThreadBlock = 256;
++    constexpr int BlockDimX = BX;
++    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
++    constexpr int N = BATCH;
++    const int k_stride = k;
++    const int k_block = 128;
++    const int splitKOffset = blockIdx.y * k_block;
 +
-+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
-+bool launch_gemm(int m,
-+                int n,
-+                int k,
-+                const input_tp *dA,
-+                int lda,
-+                const quant_packed_tp *dB,
-+                int ldb,
-+                output_tp *dC,
-+		float* dC_temp,
-+                int ldc,
-+                quant_packed_tp *d_zeros,
-+                input_tp *d_scales,
-+		const cudaStream_t stream) {
-+    using namespace hgemm_marlin_gptq;
-+    //constexpr int max_blocks_m = 4;
-+    int total_m_blocks = div_ceil(m, SLICE_M);
-+    int chunks = total_m_blocks / MAX_BLOCKS_M;
-+    int rest_blocks_m = total_m_blocks % MAX_BLOCKS_M;
-+    // printf("m=%d,n=%d,k=%d,lda=%d,ldb=%d,ldc=%d,total_m_blocks=%d,chunks=%d,rest_blocks_m=%d\n",
-+    //     m, n, k, lda, ldb, ldc, total_m_blocks, chunks, rest_blocks_m
-+    // );
-+    //const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-+    const int quant_group = 128;
-+    bool ret = true;
-+    if (chunks > 0) {
-+        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
-+        ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(real_m, n, k, quant_group, dA, lda, dB, ldb, dC, dC_temp, ldc, d_zeros, d_scales, stream, chunks);
-+    }
-+    if (rest_blocks_m > 0) {
-+        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
-+        ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(m - m_offset, n, k, quant_group, dA + lda * m_offset, lda, dB, ldb, dC + ldc * m_offset, dC_temp + ldc * m_offset, ldc, d_zeros, d_scales, stream, 1);
++    k = k_block;
++    srcA += splitKOffset * srcAStride / PACK_RATIO;
++    //srcB += splitKOffset;
++
++    constexpr int quant_groups = 1;
++    constexpr int m_per_thread = PACK_RATIO;
++    constexpr int thread_groups = ThreadBlock / BlockDimX;
++    constexpr int group_elements = BlockDimX * m_per_thread;
++    constexpr int reduce_size = ThreadBlock * m_per_thread;
++    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
++    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
++    half *bsm_scales_ptr;
++    if constexpr(reduce_size > data_cache_size) {
++        __shared__ float bsm_ptr[reduce_size];
++        bsm_b_ptr = bsm_ptr;
++        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
++        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
++        smem = bsm_ptr;
++    } else {
++        __shared__ float bsm_ptr[data_cache_size];
++        bsm_b_ptr = bsm_ptr;
++        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
++        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
++        smem = bsm_ptr;
 +    }
 +
-+    return ret;
-+}
++    const int zeros_stride = srcAStride / PACK_RATIO;
++    const int scales_stride = srcAStride;
++    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
++    scales += splitKOffset * scales_stride / QUANT_GROUP;
 +
++    dst += group_elements * blockIdx.x;
++    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
++    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
++    const int m_offset_scales = group_elements * blockIdx.x;
 +
++    //store splited fma results
++    v2f c_splited[N][m_per_thread/2];
++    for (int i = 0; i < N; i++) {
++        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
++    }
 +
- }  // namespace awq
- }  // namespace vllm
- 
-+torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight) {
++    int tid = threadIdx.x;
++    int tidCol = tid / BlockDimX;
++    int tidRow = tid % BlockDimX;
++    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
 +
-+  const at::cuda::OptionalCUDAGuard device_guard(device_of(qweight));
-+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
++    constexpr int quant_group = 0;
++    constexpr int loading_pack = 2;
++    int loading_count = this_group_elements / loading_pack;
++    //Load needed zeros, scales
++    const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
++    for (int x = tid; x < loading_count; x += ThreadBlock) {
++        uint8_t temp_zeros = 0;
++        temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
++        half temp_scales[2];
++        int packed_group_offset = (x >> 2) << 3;
++        int packed_index = (x << 1) % 8;
++        int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
++        int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
++        //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
++        temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
++        temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
++        uint32_t z = temp_zeros;
++        uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
++        uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
++        float s1 = (float)(temp_scales[0]);
++        float s2 = (float)(temp_scales[1]);
++        //Store to shared memory
++        bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
++        bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
++        bsm_scales_ptr[dest_offset_0] = temp_scales[0];
++        bsm_scales_ptr[dest_offset_1] = temp_scales[1];
++    }
 +
-+  const uint32_t* qweight_ptr = reinterpret_cast<const uint32_t*>(qweight.data_ptr<int>());
++    int loop_index = 0;
 +
-+  int num_in_channels = qweight.size(0);
-+  int num_out_channels = qweight.size(1) * 8;
++    //Load B and transform to float
++    if (b_perm != nullptr) {
++        for (int y = 0; y < N; y++) {
++            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
++                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + tidCol * LoopNum + x] + y * k_stride];
++            }
++        }
++    } else {
++        for (int y = 0; y < N; y++) {
++            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
++                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + tidCol * LoopNum + x + y * k_stride];
++            }
++        }
++    }
 +
-+  int compact_n = (num_out_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;
-+  int compact_output_k = (num_in_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;;
++    __syncthreads();
 +
-+  int block_size = 256;
-+  int tile_all_num = compact_n * compact_output_k;
-+  int grid_size = (tile_all_num + 255) / 256;
++    //Load zero and scale from bsm
++    int m_index = tidRow * m_per_thread;
++    if (m_index < this_group_elements) {
++        v2f local_scales[m_per_thread/2];
++        for (int c = 0; c < m_per_thread / 2; c++) {
++            float s0 = (float)bsm_scales_ptr[m_index + c*2];
++            float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
++            local_scales[c] = {s0, s1};
++        }
++        v2f local_zeros[m_per_thread/2];
++        for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
 +
-+  auto options = torch::TensorOptions()
-+                     .dtype(qweight.dtype())
-+                     .device(qweight.device());
++#define DEQUANT_FMA(a, b) \
++        dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
 +
-+  torch::Tensor out = torch::zeros({num_out_channels,  compact_output_k}, options);
-+  uint32_t* out_ptr = reinterpret_cast<uint32_t*>(out.data_ptr<int>());
-+
-+  vllm::awq::awq_to_gptq_4bit<256><<<grid_size, block_size, 0, stream>>>((uint32_t*)out_ptr, (const uint32_t*)qweight_ptr, num_in_channels, num_out_channels);
-+
-+  return out;
-+}
-+
- torch::Tensor awq_dequantize(torch::Tensor _kernel,
-                              torch::Tensor _scaling_factors,
-                              torch::Tensor _zeros, int64_t split_k_iters,
-@@ -468,59 +958,62 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
- 
- torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
-                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
--                       int64_t split_k_iters) {
-+                       int64_t split_k_iters,
-+                       torch::Tensor _temp_space,
-+                       bool dtype_bf16) {
-   int num_in_feats = _in_feats.size(0);
-   int num_in_channels = _in_feats.size(1);
-   const at::cuda::OptionalCUDAGuard device_guard(device_of(_in_feats));
-+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
- 
-   auto options = torch::TensorOptions()
-                      .dtype(_in_feats.dtype())
-                      .device(_in_feats.device());
-+                     
-+  // int num_out_channels = _kernel.size(1) * 8;
-+  int num_out_channels = _kernel.size(0); 
-   at::Tensor _out_feats =
--      torch::empty({split_k_iters, num_in_feats, _kernel.size(1) * 8}, options);
--  int num_out_feats = _out_feats.size(-2);
--  int num_out_channels = _out_feats.size(-1);
-+      torch::zeros({num_in_feats, num_out_channels}, options);
- 
--  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
-+  //auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
-   auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
--  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
--  auto scaling_factors =
--      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
-+  //auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
-+  //auto scaling_factors =
-+  //    reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
-   auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
-+  auto temp_space = reinterpret_cast<float*>(_temp_space.data_ptr<float>());
-   int group_size = num_in_channels / _scaling_factors.size(0);
- 
--  if (num_out_channels % 64 != 0)
--    throw std::invalid_argument("OC is not multiple of cta_N = 64");
--  if (num_out_channels % 8 != 0)
--    throw std::invalid_argument("OC is not multiple of pack_num = 8");
--  if (group_size % 32 != 0)
--    throw std::invalid_argument("Group size should be a multiple of 32");
--  if (num_out_channels % group_size != 0)
--    throw std::invalid_argument("OC is not multiple of Group size");
-+#if 0
-+  int lda = num_out_channels;
-+  int ldb = num_in_channels;
-+  int ldc = num_out_channels;
- 
-+  float alpha = 1.0, beta = 0.0;
-   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
--  if (num_out_channels % 128 == 0) {
--    int j_factors1 = num_out_channels / 128 / 1;
--    dim3 num_blocks((num_out_feats + 16 - 1) / 16 * j_factors1 * split_k_iters);
--    // threadIdx.x: 32
--    // threadIdx.y: i_factors[2] * j_factors[2]
--    dim3 threads_per_block(32, 2);
--    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<128>
--        <<<num_blocks, threads_per_block, 0, stream>>>(
--            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
--            num_in_feats, num_in_channels, num_out_channels, out_feats);
--  } else if (num_out_channels % 64 == 0) {
--    int j_factors1 = num_out_channels / 64 / 1;
--    dim3 num_blocks(1 * (num_out_feats + 16 - 1) / 16 * j_factors1 *
--                    split_k_iters);
--
--    // threadIdx.x: 32
--    // threadIdx.y: i_factors[2] * j_factors[2]
--    dim3 threads_per_block(32, 2);
--    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<64>
--        <<<num_blocks, threads_per_block, 0, stream>>>(
--            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
--            num_in_feats, num_in_channels, num_out_channels, out_feats);
-+  vllm::awq::launch_gemm_awq(Operation_t(0), Operation_t(0), num_out_channels, num_in_feats, num_in_channels, alpha, beta, (const uint32_t*)kernel, lda,
-+                             (const half*)in_feats, ldb,
-+                             (half*)out_feats, ldc, (uint32_t*)zeros, (half*)scaling_factors, space_mid, stream, 3);
-+#endif
-+
-+  int lda = num_in_channels;
-+  int ldb = num_out_channels;
-+  int ldc = num_out_channels;
-+
-+  if (dtype_bf16) {
-+	  using scalar_t = __maca_bfloat16;
-+	  vllm::awq::launch_gemm<scalar_t, vllm::kU4.id(), scalar_t, quant_packed_type>(num_in_feats, num_out_channels, num_in_channels,
-+                                     (const scalar_t*)_in_feats.data_ptr(), lda, (const uint32_t*)kernel, ldb, (scalar_t*)_out_feats.data_ptr(), temp_space, ldc,
-+                                     (uint32_t*)zeros, (scalar_t*)_scaling_factors.data_ptr(), stream);
-+  } else {
-+	  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
-+	  auto scaling_factors = reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
-+	  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
-+	  vllm::awq::launch_gemm<input_type, vllm::kU4.id(), output_type, quant_packed_type>(num_in_feats, num_out_channels, num_in_channels,
-+                                     (const half*)in_feats, lda, (const uint32_t*)kernel, ldb, (half*)out_feats, nullptr, ldc,
-+                                     (uint32_t*)zeros, (half*)scaling_factors, stream);
-   }
--  return _out_feats.sum(0);
-+
-+
-+  return _out_feats;
- }
-diff --git a/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
-new file mode 100644
-index 000000000..efc366c76
---- /dev/null
-+++ b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
-@@ -0,0 +1,521 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+/**
-+ * @file hgemv_nn_splitk.hpp
-+ * @author Jiawen Yang (jiawen.yang@metax-tech.com)
-+ * @brief *
-+ * @version 0.1
-+ * @date 2024-03-05
-+ * @copyright Copyright (c) 2024
-+ *
-+ *   fp16 gemv kernel template for some gemv cases
-+ *   Note:
-+ *    1. BlockDimX * BlockDimY = 64, and BlockDimX should be 8/16/32/64
-+ *    2. LoopNum % 2 == 0, so Load_B can use ldg_b32 or ldg_b64
-+ *    3. m % (BlockDimX * 8) == 0
-+ *    4. k % (ThreadBlock / BlockDimX * LoopNum * SplitKNum) = 0
-+ *
-+ *    A load layout:
-+ *
-+ *       **************************** Wave_0 ******************* | Wave_1  ...
-+ *       ********* Repeat LoopNum *********                      |
-+ *       tid_0(ldg_b128)   tid_0 ... tid_0 | tid_(BlockDimX) ... |
-+ *       tid_1                             |                     |
-+ *       tid_2                             |                     |
-+ *                                       |                     |
-+ *       tid_(BlockDimX-1)                 |                     |
-+ *
-+ */
-+#pragma once
++        quant_packed_type A[4];
++        const int packed_a_stride = srcAStride / PACK_RATIO;
++        int src_a_offset = (loop_index + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
++        A[0] = srcA[src_a_offset];
++        src_a_offset += packed_a_stride;
++        A[1] = srcA[src_a_offset];
 +
-+#include <mc_runtime.h>
-+#include <maca_fp16.h>
-+#include "../gptq/Hgemm_common.cuh"
-+#include "dequant.cuh"
-+#define quant_packed_type uint32_t
-+typedef __NATIVE_VECTOR__(2, float) v2f;
++        v2f local_b[4][N];
++        #pragma unroll LoopNum / 4 - 1
++        for (; loop_index < LoopNum - 4; loop_index += 4) {
++            //Load A
++            src_a_offset += packed_a_stride;
++            A[2] = srcA[src_a_offset];
++            src_a_offset += packed_a_stride;
++            A[3] = srcA[src_a_offset];
 +
-+template<int N, int m_per_thread>
-+__device__ __forceinline__ void dequant_fma_awq_int4(
-+                const quant_packed_type& a,
-+                const v2f (&scale)[m_per_thread/2],
-+                const v2f (&zero)[m_per_thread/2],
-+                const v2f (&b)[N],
-+                v2f (&out)[N][m_per_thread/2]) {
-+    uint32_t p0 = a & 0x0f0f0f0f;
-+    float o1,o3;
-+    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
-+    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
-+    v2f a0 = {o1, o3};
-+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[0], zero[0]);
++            for (int y = 0; y < N; y++) {
++                float s[4];
++                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
++                local_b[0][y] = {s[0], s[0]};
++                local_b[1][y] = {s[1], s[1]};
++                local_b[2][y] = {s[2], s[2]};
++                local_b[3][y] = {s[3], s[3]};
++            }
++            DEQUANT_FMA(A[0], local_b[0])
++            DEQUANT_FMA(A[1], local_b[1])
++            src_a_offset += packed_a_stride;
++            A[0] = srcA[src_a_offset];
++            src_a_offset += packed_a_stride;
++            A[1] = srcA[src_a_offset];
++            DEQUANT_FMA(A[2], local_b[2])
++            DEQUANT_FMA(A[3], local_b[3])
++        }
++        src_a_offset += packed_a_stride;
++        A[2] = srcA[src_a_offset];
++        src_a_offset += packed_a_stride;
++        A[3] = srcA[src_a_offset];
++        for (int y = 0; y < N; y++) {
++            float s[4];
++            *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
++            local_b[0][y] = {s[0], s[0]};
++            local_b[1][y] = {s[1], s[1]};
++            local_b[2][y] = {s[2], s[2]};
++            local_b[3][y] = {s[3], s[3]};
++        }
++        DEQUANT_FMA(A[0], local_b[0])
++        DEQUANT_FMA(A[1], local_b[1])
++        DEQUANT_FMA(A[2], local_b[2])
++        DEQUANT_FMA(A[3], local_b[3])
++    }
++    __syncthreads();
 +
++#undef DEQUANT_FMA
 +    #pragma unroll N
 +    for (int y = 0; y < N; y++) {
-+        out[y][0] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][0]);
++        if (m_index < this_group_elements) {
++            for (int i = 0; i < m_per_thread/2; i++) {
++                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
++                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
++            }
++        }
++        __syncthreads();
++        constexpr int stride = ThreadBlock / BlockDimX;
++        int data_size = ThreadBlock * m_per_thread;
++        #pragma unroll
++        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
++            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
++                int reduce_group = j / i;
++                int reduce_index = j % i;
++                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
++            }
++            __syncthreads();
++            data_size /= 2;
++        }
++        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
++            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
++        }
++        if constexpr(N > 1) {
++            if (y + 1 < N) {
++                __syncthreads();
++            }
++        }
 +    }
++}
+diff --git a/csrc/quantization/awq/hgemv_selector.hpp b/csrc/quantization/awq/hgemv_selector.hpp
+new file mode 100644
+index 000000000..b9d12a4e1
+--- /dev/null
++++ b/csrc/quantization/awq/hgemv_selector.hpp
+@@ -0,0 +1,287 @@
++#include <memory>
++#include <vector>
++#include <algorithm>
++#include "mc_runtime.h"
++#include "maca_fp16.h"
 +
-+    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p0));
-+    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p0));
-+    a0 = {o1, o3};
-+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[2], zero[2]);
++struct KernelEventRecorder {
++    mcEvent_t _start;
++    mcEvent_t _stop;
++    float _eventMs = -1.f;
 +
-+    #pragma unroll N
-+    for (int y = 0; y < N; y++) {
-+        out[y][2] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][2]);
++    KernelEventRecorder() {
++        mcEventCreate(&_start);
++        mcEventCreate(&_stop);
 +    }
 +
-+    uint32_t p1 = (a >> 4) & 0x0f0f0f0f;
-+    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p1));
-+    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p1));
-+    a0 = {o1, o3};
-+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[1], zero[1]);
-+
-+    #pragma unroll N
-+    for (int y = 0; y < N; y++) {
-+        out[y][1] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][1]);
++    ~KernelEventRecorder() {
++        mcEventDestroy(_start);
++        mcEventDestroy(_stop);
 +    }
 +
-+    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p1));
-+    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p1));
-+    a0 = {o1, o3};
-+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[3], zero[3]);
++    void start() {
++        mcEventRecord(_start, NULL);
++    }
 +
-+    #pragma unroll N
-+    for (int y = 0; y < N; y++) {
-+        out[y][3] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][3]);
++    float stop() {
++        mcEventRecord(_stop, NULL);
++        mcEventSynchronize(_stop);
++        mcEventElapsedTime(&_eventMs, _start, _stop);
++        return _eventMs;
 +    }
 +};
++namespace hgemv_selector {
++template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
++struct GemvParamAutoSelector {
++    int m;
++    int k;
++    int best_block_x = 0;
++    int best_split_k = 0;
 +
-+template <int ThreadBlock, int BlockDimX, int BATCH>
-+__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq(const half* __restrict__ srcB,
-+                                                        const quant_packed_type* __restrict__ srcA,
-+                                                        const quant_packed_type* __restrict__ zeros,
-+                                                        const half* __restrict__ scales,
-+                                                        half *dst,
-+                                                        int m,
-+                                                        int n,
-+                                                        int k,
-+                                                        int srcAStride,
-+                                                        int dstStride,
-+                                                        int k_div_sk,
-+                                                        const int* __restrict__ b_perm = nullptr) {
-+    constexpr int QBITS = 4;
-+    constexpr int PACK_RATIO = (32 / QBITS);
-+    constexpr int QUANT_GROUP = 128;
-+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
-+    constexpr int N = BATCH;
-+    const int k_stride = k;
-+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
-+    const int splitKOffset = blockIdx.y * k_block;
-+    if (splitKOffset + k_block > k) k = k - splitKOffset;
-+    else k = k_block;
-+    srcA += splitKOffset * srcAStride / PACK_RATIO;
-+    //srcB += splitKOffset;
-+
-+    constexpr int quant_groups = 1;
-+    constexpr int m_per_thread = PACK_RATIO;
-+    constexpr int thread_groups = ThreadBlock / BlockDimX;
-+    constexpr int group_elements = BlockDimX * m_per_thread;
-+    constexpr int reduce_size = ThreadBlock * m_per_thread;
-+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
-+
-+    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
-+    half *bsm_scales_ptr;
-+    if constexpr(reduce_size > data_cache_size) {
-+        __shared__ float bsm_ptr[reduce_size];
-+        bsm_b_ptr = bsm_ptr;
-+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
-+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
-+        smem = bsm_ptr;
-+    } else {
-+        __shared__ float bsm_ptr[data_cache_size];
-+        bsm_b_ptr = bsm_ptr;
-+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
-+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
-+        smem = bsm_ptr;
-+    }
++    std::pair<int,int> block_x_range;
++    std::pair<int,int> split_k_range;
++    bool _valid = false;
 +
-+    const int zeros_stride = srcAStride / PACK_RATIO;
-+    const int scales_stride = srcAStride;
-+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
-+    scales += splitKOffset * scales_stride / QUANT_GROUP;
++private:
++    std::vector<std::pair<int, int>> param_candidates;
++    int warmup_iters = 0;
++    int current_block_x = 0;
++    int current_split_k = 0;
++    int current_perf_iter = 0;
++    std::vector<float> perf_times;
++    float kernel_best_time_ms_ave = 99999999.0f;
++    float best_band_width;
++    float data_size_gb;
++    std::shared_ptr<KernelEventRecorder> _r;
++    bool _selected = false;
++    const static int MAX_PERF_COUNT = 20;
 +
-+    dst += group_elements * blockIdx.x;
-+    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
-+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
-+    const int m_offset_scales = group_elements * blockIdx.x;
++public:
++    GemvParamAutoSelector(int _m, int _n, int _k, bool allow_imcomplete_bx = false) : m(_m), k(_k)
++    {
++        if (!prepare_parameters(_m, _n, _k, block_x_range.second, split_k_range.second, allow_imcomplete_bx)) return;
++        block_x_range.first = block_x_range.second;
++        split_k_range.first = split_k_range.second;
++        for (int i = 0; i < 3; i++) if (block_x_range.first / 2 >= 16) block_x_range.first /= 2;
++        for (int i = 0; i < 3; i++) if (split_k_range.first % 2 == 0 && split_k_range.first / 2 > 0) split_k_range.first /= 2;
++        for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
++            for (int j = split_k_range.first; j <= split_k_range.second; j *= 2) {
++                param_candidates.emplace_back(i, j);
++            }
++        }
++        if (split_k_range.second * quant_group != k) {
++            int max_split_k = k / quant_group;
++            if (max_split_k < 256) {
++                for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
++                    param_candidates.emplace_back(i, max_split_k);
++                }
++            }
++        }
 +
-+    //store splited fma results
-+    v2f c_splited[N][m_per_thread/2];
-+    for (int i = 0; i < N; i++) {
-+        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
++        current_block_x = block_x_range.second;
++        current_split_k = split_k_range.second;
++        uint64_t data_size_in_bytes = _n * _k * sizeof(half);
++        data_size_in_bytes += (uint64_t)_m * _k / pack_ratio * sizeof(uint32_t);
++        data_size_in_bytes += (uint64_t)_m * _k / quant_group * sizeof(half);
++        data_size_in_bytes += (uint64_t)_m * _k / quant_group / pack_ratio * sizeof(uint32_t);
++        data_size_in_bytes += (uint64_t)_n * _m * sizeof(half);
++        data_size_gb = float(data_size_in_bytes) / 1024 / 1024 / 1024;
++        warmup_iters = 4;
++        _valid = true;
 +    }
 +
-+    int tid = threadIdx.x;
-+    int tidCol = tid / BlockDimX;
-+    int tidRow = tid % BlockDimX;
-+    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
-+    int m_index = tidRow * m_per_thread;
-+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
-+        int quant_group = i / QUANT_GROUP;
-+        constexpr int loading_pack = 2;
-+        int loading_count = this_group_elements / loading_pack;
-+        //Load needed zeros, scales
-+        const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
-+        for (int x = tid; x < loading_count; x += ThreadBlock) {
-+            uint8_t temp_zeros = 0;
-+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
-+            half temp_scales[2];
-+            int packed_group_offset = (x >> 2) << 3;
-+            int packed_index = (x << 1) % 8;
-+            int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
-+            int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
-+            //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
-+            temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
-+            temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
-+            uint32_t z = temp_zeros;
-+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
-+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
-+            float s1 = (float)(temp_scales[0]);
-+            float s2 = (float)(temp_scales[1]);
-+            //Store to shared memory
-+            bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
-+            bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
-+            bsm_scales_ptr[dest_offset_0] = temp_scales[0];
-+            bsm_scales_ptr[dest_offset_1] = temp_scales[1];
++    void select_in_warmup(const std::function<bool(int,int)> &f) {
++        if (!valid()) {
++            printf("Cannot run this selector!\n");
++            return;
 +        }
++        if (_selected) {
++            f(best_block_x, best_split_k);
++            return;
++        };
++        //Warmup
++        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
++            for (int i = 0; i < 5; i++) {
++                auto &p = *iter;
++                f(p.first, p.second);
++            }
++        }
++        _r.reset(new KernelEventRecorder());
++        kernel_best_time_ms_ave = 9999999.0f;
++        mcDeviceSynchronize();
 +
-+        int loop_index = 0;
-+
-+        //Load B and transform to float
-+        if (b_perm != nullptr) {
-+            for (int y = 0; y < N; y++) {
-+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
-+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
-+                }
++        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
++            auto &p = *iter;
++            auto &bx = p.first;
++            auto &sk = p.second;
++            mcDeviceSynchronize();
++            _r->start();
++            bool launched = false;
++            for (int i = 0; i < MAX_PERF_COUNT; i++) {
++                launched = f(bx, sk);
 +            }
-+        } else {
-+            for (int y = 0; y < N; y++) {
-+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
-+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
-+                }
++            auto ms = _r->stop();
++            if (launched && ms / MAX_PERF_COUNT < kernel_best_time_ms_ave) {
++                best_block_x = bx;
++                best_split_k = sk;
++                kernel_best_time_ms_ave = ms / MAX_PERF_COUNT;
 +            }
 +        }
 +
-+        __syncthreads();
++        _r.reset();
++        _selected = true;
++        warmup_iters = 0;
++    }
 +
-+        //Load zero and scale from bsm
-+        if (m_index < this_group_elements) {
-+            v2f local_scales[m_per_thread/2];
-+            for (int c = 0; c < m_per_thread / 2; c++) {
-+                float s0 = (float)bsm_scales_ptr[m_index + c*2];
-+                float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
-+                local_scales[c] = {s0, s1};
++    void run(const std::function<bool(int,int)> &f) {
++        if (!valid()) {
++            printf("Cannot run this selector!\n");
++            return;
++        }
++        if (warmup_iters > 0) {
++            f(current_block_x, current_split_k);
++            //printf("Calling warmup current_block_x = %d, current_split_k = %d\n", current_block_x, current_split_k);
++            if (current_block_x > block_x_range.first) current_block_x /= 2;
++            else if (current_split_k > split_k_range.first) current_split_k /= 2;
++            if (current_block_x == block_x_range.first && current_split_k >= split_k_range.first) {
++                current_block_x = block_x_range.second;
++                current_split_k = split_k_range.second;
 +            }
-+            v2f local_zeros[m_per_thread/2];
-+            for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
-+
-+    #define DEQUANT_FMA(a, b) \
-+            dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
++            warmup_iters--;
++            mcDeviceSynchronize();
++            return;
++        }
 +
-+            quant_packed_type A[4];
-+            const int packed_a_stride = srcAStride / PACK_RATIO;
-+            int src_a_offset = (loop_index + i + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
-+            A[0] = srcA[src_a_offset];
-+            src_a_offset += packed_a_stride;
-+            A[1] = srcA[src_a_offset];
-+
-+            v2f local_b[4][N];
-+            //#pragma unroll LoopNum / 4 - 1
-+            for (; loop_index < LoopNum - 4; loop_index += 4) {
-+                //Load A
-+                src_a_offset += packed_a_stride;
-+                A[2] = srcA[src_a_offset];
-+                src_a_offset += packed_a_stride;
-+                A[3] = srcA[src_a_offset];
-+
-+                for (int y = 0; y < N; y++) {
-+                    float s[4];
-+                    *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
-+                    local_b[0][y] = {s[0], s[0]};
-+                    local_b[1][y] = {s[1], s[1]};
-+                    local_b[2][y] = {s[2], s[2]};
-+                    local_b[3][y] = {s[3], s[3]};
-+                }
-+                DEQUANT_FMA(A[0], local_b[0])
-+                DEQUANT_FMA(A[1], local_b[1])
-+                src_a_offset += packed_a_stride;
-+                A[0] = srcA[src_a_offset];
-+                src_a_offset += packed_a_stride;
-+                A[1] = srcA[src_a_offset];
-+                DEQUANT_FMA(A[2], local_b[2])
-+                DEQUANT_FMA(A[3], local_b[3])
-+            }
-+            src_a_offset += packed_a_stride;
-+            A[2] = srcA[src_a_offset];
-+            src_a_offset += packed_a_stride;
-+            A[3] = srcA[src_a_offset];
-+            for (int y = 0; y < N; y++) {
-+                float s[4];
-+                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
-+                local_b[0][y] = {s[0], s[0]};
-+                local_b[1][y] = {s[1], s[1]};
-+                local_b[2][y] = {s[2], s[2]};
-+                local_b[3][y] = {s[3], s[3]};
++        if (_selected) {
++            f(best_block_x, best_split_k);
++        } else {
++            if (!_r) {
++                _r = std::shared_ptr<KernelEventRecorder>(new KernelEventRecorder());
++                current_block_x = block_x_range.second;
++                current_split_k = split_k_range.second;
++                current_perf_iter = MAX_PERF_COUNT;
 +            }
-+            DEQUANT_FMA(A[0], local_b[0])
-+            DEQUANT_FMA(A[1], local_b[1])
-+            DEQUANT_FMA(A[2], local_b[2])
-+            DEQUANT_FMA(A[3], local_b[3])
-+        }
-+        __syncthreads();
-+    }
-+#undef DEQUANT_FMA
-+    #pragma unroll N
-+    for (int y = 0; y < N; y++) {
-+        if (m_index < this_group_elements) {
-+            for (int i = 0; i < m_per_thread/2; i++) {
-+                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
-+                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
++            _r->start();
++            auto launched = f(current_block_x, current_split_k);
++            auto ms = _r->stop();
++            //printf("Launched with bx=%d,sk=%d,iter=%d,ms=%fms\n", current_block_x, current_split_k, current_perf_iter, ms);
++            if (!launched) {
++                printf("ERROR: Cannot run gemv with bx = %d, sk = %d\n", current_block_x, current_split_k);
++                return;
 +            }
-+        }
-+        __syncthreads();
-+        constexpr int stride = ThreadBlock / BlockDimX;
-+        int data_size = ThreadBlock * m_per_thread;
-+        #pragma unroll
-+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
-+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
-+                int reduce_group = j / i;
-+                int reduce_index = j % i;
-+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
++            if (current_perf_iter-- > 0) {
++                perf_times.emplace_back(ms);
++                return;
 +            }
-+            __syncthreads();
-+            data_size /= 2;
-+        }
-+        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
-+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
-+        }
-+        if constexpr(N > 1) {
-+            if (y + 1 < N) {
-+                __syncthreads();
++
++            std::sort(perf_times.begin(), perf_times.end());
++            float total_tm = 0;
++            for (size_t i = 1; i < perf_times.size() - 1; i++) {
++                total_tm += perf_times[i];
 +            }
-+        }
-+    }
-+}
 +
-+template <int BX, int BATCH>
-+__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq_kb128(const half* __restrict__ srcB,
-+                                                        const quant_packed_type* __restrict__ srcA,
-+                                                        const quant_packed_type* __restrict__ zeros,
-+                                                        const half* __restrict__ scales,
-+                                                        half *dst,
-+                                                        int m,
-+                                                        int n,
-+                                                        int k,
-+                                                        int srcAStride,
-+                                                        int dstStride,
-+                                                        int k_div_sk,
-+                                                        const int* __restrict__ b_perm = nullptr) {
-+    constexpr int QBITS = 4;
-+    constexpr int PACK_RATIO = (32 / QBITS);
-+    constexpr int QUANT_GROUP = 128;
-+    constexpr int ThreadBlock = 256;
-+    constexpr int BlockDimX = BX;
-+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
-+    constexpr int N = BATCH;
-+    const int k_stride = k;
-+    const int k_block = 128;
-+    const int splitKOffset = blockIdx.y * k_block;
++            ms = total_tm /= (MAX_PERF_COUNT - 2);
++            perf_times.clear();
++            current_perf_iter = MAX_PERF_COUNT;
++            //printf("get ave time %fms\n", ms);
 +
-+    k = k_block;
-+    srcA += splitKOffset * srcAStride / PACK_RATIO;
-+    //srcB += splitKOffset;
++            if (ms < kernel_best_time_ms_ave) {
++                best_block_x = current_block_x;
++                best_split_k = current_split_k;
++                kernel_best_time_ms_ave = ms;
++            }
 +
-+    constexpr int quant_groups = 1;
-+    constexpr int m_per_thread = PACK_RATIO;
-+    constexpr int thread_groups = ThreadBlock / BlockDimX;
-+    constexpr int group_elements = BlockDimX * m_per_thread;
-+    constexpr int reduce_size = ThreadBlock * m_per_thread;
-+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
-+    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
-+    half *bsm_scales_ptr;
-+    if constexpr(reduce_size > data_cache_size) {
-+        __shared__ float bsm_ptr[reduce_size];
-+        bsm_b_ptr = bsm_ptr;
-+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
-+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
-+        smem = bsm_ptr;
-+    } else {
-+        __shared__ float bsm_ptr[data_cache_size];
-+        bsm_b_ptr = bsm_ptr;
-+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
-+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
-+        smem = bsm_ptr;
++            if (current_split_k > split_k_range.first) {
++                current_split_k /= 2;
++            } else if (current_block_x > block_x_range.first){
++                current_split_k = split_k_range.second;
++                current_block_x /= 2;
++            } else if (current_block_x == block_x_range.first && current_split_k == split_k_range.first) {
++                _selected = true;
++                _r.reset();
++            } else {
++                printf("Error: all parameters are tried! current_block_x = %d, current_split_k = %d, block_x_range = %d,%d, split_k_range = %d,%d, best_block_x = %d, best_split_k = %d\n",
++                    current_block_x, current_split_k,
++                    block_x_range.first, block_x_range.second,
++                    split_k_range.first, split_k_range.second,
++                    best_block_x, best_split_k
++                );
++            }
++        }
 +    }
 +
-+    const int zeros_stride = srcAStride / PACK_RATIO;
-+    const int scales_stride = srcAStride;
-+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
-+    scales += splitKOffset * scales_stride / QUANT_GROUP;
-+
-+    dst += group_elements * blockIdx.x;
-+    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
-+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
-+    const int m_offset_scales = group_elements * blockIdx.x;
++    bool valid() const { return _valid; }
++    bool selected() const {return _selected; }
 +
-+    //store splited fma results
-+    v2f c_splited[N][m_per_thread/2];
-+    for (int i = 0; i < N; i++) {
-+        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
++    float gemv_ave_time_us_cost() {
++        return kernel_best_time_ms_ave;
 +    }
 +
-+    int tid = threadIdx.x;
-+    int tidCol = tid / BlockDimX;
-+    int tidRow = tid % BlockDimX;
-+    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
-+
-+    constexpr int quant_group = 0;
-+    constexpr int loading_pack = 2;
-+    int loading_count = this_group_elements / loading_pack;
-+    //Load needed zeros, scales
-+    const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
-+    for (int x = tid; x < loading_count; x += ThreadBlock) {
-+        uint8_t temp_zeros = 0;
-+        temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
-+        half temp_scales[2];
-+        int packed_group_offset = (x >> 2) << 3;
-+        int packed_index = (x << 1) % 8;
-+        int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
-+        int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
-+        //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
-+        temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
-+        temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
-+        uint32_t z = temp_zeros;
-+        uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
-+        uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
-+        float s1 = (float)(temp_scales[0]);
-+        float s2 = (float)(temp_scales[1]);
-+        //Store to shared memory
-+        bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
-+        bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
-+        bsm_scales_ptr[dest_offset_0] = temp_scales[0];
-+        bsm_scales_ptr[dest_offset_1] = temp_scales[1];
++    float gemv_bandwidth() {
++        return data_size_gb / (kernel_best_time_ms_ave / 1000.0f);
 +    }
 +
-+    int loop_index = 0;
++    static bool prepare_parameters(int m, int n, int k, int &bx, int &sk, bool allow_imcomplete_bx = false) {
++        if (n > 4) return false;
++        if (k % quant_group != 0) return false;
++        if (m < 16 * m_per_thread) return false;
++        int max_split_k = k / quant_group;
++        int proper_splitk = 1;
++        while (proper_splitk*2 <= 256 && proper_splitk*2 <= max_split_k) proper_splitk *= 2;
 +
-+    //Load B and transform to float
-+    if (b_perm != nullptr) {
-+        for (int y = 0; y < N; y++) {
-+            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
-+                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + tidCol * LoopNum + x] + y * k_stride];
-+            }
-+        }
-+    } else {
-+        for (int y = 0; y < N; y++) {
-+            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
-+                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + tidCol * LoopNum + x + y * k_stride];
++        int thread_block = 256;  //A thread block of 512 will wait for more warps when doing syncthreads
++
++        int proper_bx = 16;
++        if (m % (proper_bx * m_per_thread) != 0) return false;
++        //proper_bx * m_per_threads corresponds to group_elements, this value should not be larger than 2048 as limit of shared memory
++        while (proper_bx * 2 * m_per_thread && (m % (proper_bx*2*m_per_thread) == 0) && proper_bx * 2 <= 256 && proper_bx * 2 * m_per_thread <= 2048) proper_bx*=2;
++        if (allow_imcomplete_bx) {
++            int may_proper_bx = proper_bx * 2;
++            if (may_proper_bx <= 256 && may_proper_bx * m_per_thread <= 2048 && (m % (may_proper_bx*m_per_thread)) > may_proper_bx * m_per_thread / 4) {
++                proper_bx = may_proper_bx;
 +            }
 +        }
++
++        bx = proper_bx;
++        sk = proper_splitk;
++        //printf("m=%d,n=%d,k=%d,get bx=%d,sk=%d,allow_imcomplete_bx=%d\n", m, n, k, bx, sk, allow_imcomplete_bx);
++        return true;
 +    }
++};
 +
-+    __syncthreads();
-+
-+    //Load zero and scale from bsm
-+    int m_index = tidRow * m_per_thread;
-+    if (m_index < this_group_elements) {
-+        v2f local_scales[m_per_thread/2];
-+        for (int c = 0; c < m_per_thread / 2; c++) {
-+            float s0 = (float)bsm_scales_ptr[m_index + c*2];
-+            float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
-+            local_scales[c] = {s0, s1};
-+        }
-+        v2f local_zeros[m_per_thread/2];
-+        for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
-+
-+#define DEQUANT_FMA(a, b) \
-+        dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
-+
-+        quant_packed_type A[4];
-+        const int packed_a_stride = srcAStride / PACK_RATIO;
-+        int src_a_offset = (loop_index + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
-+        A[0] = srcA[src_a_offset];
-+        src_a_offset += packed_a_stride;
-+        A[1] = srcA[src_a_offset];
-+
-+        v2f local_b[4][N];
-+        #pragma unroll LoopNum / 4 - 1
-+        for (; loop_index < LoopNum - 4; loop_index += 4) {
-+            //Load A
-+            src_a_offset += packed_a_stride;
-+            A[2] = srcA[src_a_offset];
-+            src_a_offset += packed_a_stride;
-+            A[3] = srcA[src_a_offset];
-+
-+            for (int y = 0; y < N; y++) {
-+                float s[4];
-+                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
-+                local_b[0][y] = {s[0], s[0]};
-+                local_b[1][y] = {s[1], s[1]};
-+                local_b[2][y] = {s[2], s[2]};
-+                local_b[3][y] = {s[3], s[3]};
-+            }
-+            DEQUANT_FMA(A[0], local_b[0])
-+            DEQUANT_FMA(A[1], local_b[1])
-+            src_a_offset += packed_a_stride;
-+            A[0] = srcA[src_a_offset];
-+            src_a_offset += packed_a_stride;
-+            A[1] = srcA[src_a_offset];
-+            DEQUANT_FMA(A[2], local_b[2])
-+            DEQUANT_FMA(A[3], local_b[3])
-+        }
-+        src_a_offset += packed_a_stride;
-+        A[2] = srcA[src_a_offset];
-+        src_a_offset += packed_a_stride;
-+        A[3] = srcA[src_a_offset];
-+        for (int y = 0; y < N; y++) {
-+            float s[4];
-+            *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
-+            local_b[0][y] = {s[0], s[0]};
-+            local_b[1][y] = {s[1], s[1]};
-+            local_b[2][y] = {s[2], s[2]};
-+            local_b[3][y] = {s[3], s[3]};
-+        }
-+        DEQUANT_FMA(A[0], local_b[0])
-+        DEQUANT_FMA(A[1], local_b[1])
-+        DEQUANT_FMA(A[2], local_b[2])
-+        DEQUANT_FMA(A[3], local_b[3])
-+    }
-+    __syncthreads();
++template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
++class GemvSelectorHolder {
++private:
++    std::vector<GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>> _selectors;
++    static std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> _holder;
++    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> _invalid_selector;
 +
-+#undef DEQUANT_FMA
-+    #pragma unroll N
-+    for (int y = 0; y < N; y++) {
-+        if (m_index < this_group_elements) {
-+            for (int i = 0; i < m_per_thread/2; i++) {
-+                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
-+                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
-+            }
-+        }
-+        __syncthreads();
-+        constexpr int stride = ThreadBlock / BlockDimX;
-+        int data_size = ThreadBlock * m_per_thread;
-+        #pragma unroll
-+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
-+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
-+                int reduce_group = j / i;
-+                int reduce_index = j % i;
-+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
-+            }
-+            __syncthreads();
-+            data_size /= 2;
++public:
++    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& selector(int m, int n, int k, bool allow_imcomplete_bx = false) {
++        if (!GemvSelectorHolder::_holder) {
++            GemvSelectorHolder::_holder.reset(new GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>());
 +        }
-+        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
-+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
++        int bx, sk;
++        if (!GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>::prepare_parameters(m, n, k, bx, sk, allow_imcomplete_bx)) {
++            return _invalid_selector;
 +        }
-+        if constexpr(N > 1) {
-+            if (y + 1 < N) {
-+                __syncthreads();
-+            }
++        auto iter = std::find_if(_holder->_selectors.begin(), _holder->_selectors.end(),
++            [&](const GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& p) -> bool {
++                return p.m == m && p.k == k;
++            });
++        if (iter != _holder->_selectors.end()) return *iter;
++        GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> sl(m, n, k, allow_imcomplete_bx);
++        if (!sl.valid()) {
++            return _invalid_selector;
 +        }
++        _holder->_selectors.emplace_back(sl);
++        return _holder->_selectors.back();
 +    }
-+}
-diff --git a/csrc/quantization/awq/hgemv_selector.hpp b/csrc/quantization/awq/hgemv_selector.hpp
-new file mode 100644
-index 000000000..2a7fef782
---- /dev/null
-+++ b/csrc/quantization/awq/hgemv_selector.hpp
-@@ -0,0 +1,288 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+#include <memory>
-+#include <vector>
-+#include <algorithm>
-+#include "mc_runtime.h"
-+#include "maca_fp16.h"
++};
 +
-+struct KernelEventRecorder {
-+    mcEvent_t _start;
-+    mcEvent_t _stop;
-+    float _eventMs = -1.f;
++template<int quant_group, int pack_ratio, int m_per_thread>
++std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_holder;
 +
-+    KernelEventRecorder() {
-+        mcEventCreate(&_start);
-+        mcEventCreate(&_stop);
++template<int quant_group, int pack_ratio, int m_per_thread>
++GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_invalid_selector(0,8,0);
++}
+\ No newline at end of file
+diff --git a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
+index bf46cce60..5fdd5037f 100644
+--- a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
++++ b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
+@@ -12,7 +12,7 @@
+   #include <hipcub/hipcub.hpp>
+ #endif
+ 
+-static inline __device__ int8_t float_to_int8_rn(float x) {
++static __forceinline__ __device__ int8_t float_to_int8_rn(float x) {
+ #ifdef USE_ROCM
+   static constexpr auto i8_min =
+       static_cast<float>(std::numeric_limits<int8_t>::min());
+@@ -36,8 +36,13 @@ static inline __device__ int8_t float_to_int8_rn(float x) {
+   return static_cast<int8_t>(dst);
+ #else
+   // CUDA path
+-  uint32_t dst;
+-  asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
++  //uint32_t dst;
++  //asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
++  //return reinterpret_cast<const int8_t&>(dst);
++  int32_t dst;
++  dst = __float2int_rn(x);
++  dst = min(dst, 127);
++  dst = max(dst, -127);
+   return reinterpret_cast<const int8_t&>(dst);
+ #endif
+ }
+@@ -71,9 +76,13 @@ static inline __device__ int32_t float_to_int32_rn(float x) {
+   return static_cast<int32_t>(dst);
+ #else
+   // CUDA path
+-  uint32_t dst;
+-  asm volatile("cvt.rni.sat.s32.f32 %0, %1;" : "=r"(dst) : "f"(x));
+-  return reinterpret_cast<const int32_t&>(dst);
++  static constexpr auto i32_min = std::numeric_limits<int32_t>::min();
++  static constexpr auto i32_min_f = static_cast<float>(i32_min);
++  static constexpr auto i32_max = std::numeric_limits<int32_t>::max();
++  static constexpr auto i32_max_f = static_cast<float>(i32_max);
++  x = min(x, i32_max_f);
++  x = max(x, i32_min_f);
++  return __float2int_rn(x);
+ #endif
+ }
+ 
+@@ -95,9 +104,14 @@ static inline __device__ int8_t int32_to_int8(int32_t x) {
+   return static_cast<int8_t>(dst);
+ #else
+   // CUDA path
+-  uint32_t dst;
+-  asm volatile("cvt.sat.s8.s32 %0, %1;" : "=r"(dst) : "r"(x));
+-  return reinterpret_cast<const int8_t&>(dst);
++  static constexpr auto i8_min =
++      static_cast<int32_t>(std::numeric_limits<int8_t>::min());
++  static constexpr auto i8_max =
++      static_cast<int32_t>(std::numeric_limits<int8_t>::max());
++
++  // saturate
++  int32_t dst = std::clamp(x, i8_min, i8_max);
++  return static_cast<int8_t>(dst);
+ #endif
+ }
+ 
+@@ -141,21 +155,23 @@ __global__ void static_scaled_int8_azp_quant_kernel(
+   }
+ }
+ 
+-template <typename scalar_t, typename scale_type>
++template <typename scalar_t, typename scale_type, bool WITHMASK>
+ __global__ void dynamic_scaled_int8_quant_kernel(
+     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+-    scale_type* scale, const int hidden_size) {
++    scale_type* scale, const int hidden_size, const int num_tokens, int *mask_buffer = NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }
+   int const tid = threadIdx.x;
+-  int64_t const token_idx = blockIdx.x;
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
+   float absmax_val = 0.0f;
+   float const zero = 0.0f;
+ 
+-  // Must be performed using 64-bit math to avoid integer overflow.
+-  out += token_idx * hidden_size;
+-  input += token_idx * hidden_size;
+-
+   for (int i = tid; i < hidden_size; i += blockDim.x) {
+-    float val = static_cast<float>(input[i]);
++    float val = static_cast<float>(input[token_idx * hidden_size + i]);
+     val = val > zero ? val : -val;
+     absmax_val = val > absmax_val ? val : absmax_val;
+   }
+@@ -167,31 +183,330 @@ __global__ void dynamic_scaled_int8_quant_kernel(
+   __shared__ float block_absmax_val;
+   if (tid == 0) {
+     block_absmax_val = block_absmax_val_maybe;
+-    scale[token_idx] = block_absmax_val / 127.0f;
++    scale[token_idx] = block_absmax_val * 0.0078740157;
+   }
+   __syncthreads();
+ 
+-  float const tmp_scale = 127.0f / block_absmax_val;
++  float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
+   for (int i = tid; i < hidden_size; i += blockDim.x) {
+-    out[i] = float_to_int8_rn(static_cast<float>(input[i]) * tmp_scale);
++    out[token_idx * hidden_size + i] = float_to_int8_rn(
++        static_cast<float>(input[token_idx * hidden_size + i]) * tmp_scale);
+   }
+ }
+ 
+-template <typename scalar_t, typename scale_type, typename azp_type>
+-__global__ void dynamic_scaled_int8_azp_quant_kernel(
++template <typename scalar_t, typename scale_type, typename VT, typename VT1, int NUM_THREADS, bool WITHMASK>
++__global__ void dynamic_scaled_int8_quant_kernel_sreg_opt(
+     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+-    scale_type* scale, azp_type* azp, const int hidden_size) {
+-  int64_t const token_idx = blockIdx.x;
++    scale_type* scale, const int hidden_size, int num_tokens, int* mask_buffer=NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }
++  int const tid = threadIdx.x;
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
++  float absmax_val = 0.0f;
++  float const zero = 0.0f;
++  constexpr int N = sizeof(VT) / sizeof(scalar_t);
++  float reg_src0[N];
++  scalar_t const* ptr_input = input + token_idx * hidden_size;
++  int reg_length = NUM_THREADS * N;
++  int length = min(hidden_size, reg_length);
++  int index = tid * N;
++  if(index < length) {
++    VT reg_src;
++    reg_src = *(VT*)(ptr_input + index);
++    scalar_t* ptr_reg_src = (scalar_t*)&reg_src;
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      reg_src0[i] = (float)ptr_reg_src[i];
 +    }
-+
-+    ~KernelEventRecorder() {
-+        mcEventDestroy(_start);
-+        mcEventDestroy(_stop);
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      float val = abs(reg_src0[i]);
++      absmax_val = max(absmax_val, val);
 +    }
-+
-+    void start() {
-+        mcEventRecord(_start, NULL);
++  }
+ 
+-  // Must be performed using 64-bit math to avoid integer overflow.
+-  out += token_idx * hidden_size;
+-  input += token_idx * hidden_size;
++  using BlockReduce = cub::BlockReduce<float, NUM_THREADS>;
++  __shared__ typename BlockReduce::TempStorage reduceStorage;
++  float const block_absmax_val_maybe =
++      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, NUM_THREADS);
++  __shared__ float block_absmax_val;
++  if (tid == 0) {
++    block_absmax_val = block_absmax_val_maybe;
++    scale[token_idx] = static_cast<scale_type>(block_absmax_val_maybe * 0.0078740157);
++  }
++  __syncthreads();
++  float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++  int8_t* ptr_output = out + token_idx * hidden_size;
++  if(index < length) {
++    VT1 vdst;
++    int8_t* ptr_reg = (int8_t*)&vdst;
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      ptr_reg[i] = float_to_int8_rn(reg_src0[i] * tmp_scale);
 +    }
++    *(VT1*)(ptr_output + index) = vdst;
++  }
++}
 +
-+    float stop() {
-+        mcEventRecord(_stop, NULL);
-+        mcEventSynchronize(_stop);
-+        mcEventElapsedTime(&_eventMs, _start, _stop);
-+        return _eventMs;
++template <typename scalar_t, typename scale_type, typename VT, typename VT1, bool WITHMASK>
++__global__ void dynamic_scaled_int8_quant_kernel_reg_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, int blockDim_x, int num_tokens, int* mask_buffer=NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }
++  int const tid = threadIdx.x;
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
++  float absmax_val = 0.0f;
++  float const zero = 0.0f;
++  constexpr int N = sizeof(VT) / sizeof(scalar_t);
++  float reg_src0[N];
++  float reg_src1[N];
++  scalar_t const* ptr_input = input + token_idx * hidden_size;
++  int reg_length = 2 * blockDim_x * N;
++  int length = min(hidden_size, reg_length);
++  int index = 2 * tid * N;
++  if(index < length) {
++    VT reg_src = *(VT*)(ptr_input + index);
++    scalar_t* ptr_reg_src = (scalar_t*)&reg_src;
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      reg_src0[i] = (float)ptr_reg_src[i];
 +    }
-+};
-+namespace hgemv_selector {
-+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
-+struct GemvParamAutoSelector {
-+    int m;
-+    int k;
-+    int best_block_x = 0;
-+    int best_split_k = 0;
-+
-+    std::pair<int,int> block_x_range;
-+    std::pair<int,int> split_k_range;
-+    bool _valid = false;
++    reg_src = *(VT*)(ptr_input + index + N);
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      reg_src1[i] = (float)ptr_reg_src[i];
++    }
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      float val = abs(reg_src0[i]);
++      absmax_val =  max(val, absmax_val);
++      val = abs(reg_src1[i]);
++      absmax_val = max(val, absmax_val);
++    }
++  }
 +
-+private:
-+    std::vector<std::pair<int, int>> param_candidates;
-+    int warmup_iters = 0;
-+    int current_block_x = 0;
-+    int current_split_k = 0;
-+    int current_perf_iter = 0;
-+    std::vector<float> perf_times;
-+    float kernel_best_time_ms_ave = 99999999.0f;
-+    float best_band_width;
-+    float data_size_gb;
-+    std::shared_ptr<KernelEventRecorder> _r;
-+    bool _selected = false;
-+    const static int MAX_PERF_COUNT = 20;
++  using BlockReduce = cub::BlockReduce<float, 512>;
++  __shared__ typename BlockReduce::TempStorage reduceStorage;
++  float const block_absmax_val_maybe =
++      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
++  __shared__ float block_absmax_val;
++  if (tid == 0) {
++    block_absmax_val = (block_absmax_val_maybe);
++    scale[token_idx] = static_cast<scale_type>(block_absmax_val * 0.0078740157);
++  }
++  __syncthreads();
++  float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++  int8_t* ptr_output = out + token_idx * hidden_size;
++  if(index < length) {
++    VT1 vdst;
++    int8_t* ptr_reg = (int8_t*)&vdst;
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      ptr_reg[i] = float_to_int8_rn(
++             reg_src0[i] * tmp_scale);
++    }
++    ptr_reg = ptr_reg + N;
++    #pragma unroll N
++    for(int i = 0; i < N; i++) {
++      ptr_reg[i] = float_to_int8_rn(
++            reg_src1[i] * tmp_scale);
++    }
++    *(VT1*)(ptr_output + index) = vdst;
++  }
++}
 +
-+public:
-+    GemvParamAutoSelector(int _m, int _n, int _k, bool allow_imcomplete_bx = false) : m(_m), k(_k)
-+    {
-+        if (!prepare_parameters(_m, _n, _k, block_x_range.second, split_k_range.second, allow_imcomplete_bx)) return;
-+        block_x_range.first = block_x_range.second;
-+        split_k_range.first = split_k_range.second;
-+        for (int i = 0; i < 3; i++) if (block_x_range.first / 2 >= 16) block_x_range.first /= 2;
-+        for (int i = 0; i < 3; i++) if (split_k_range.first % 2 == 0 && split_k_range.first / 2 > 0) split_k_range.first /= 2;
-+        for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
-+            for (int j = split_k_range.first; j <= split_k_range.second; j *= 2) {
-+                param_candidates.emplace_back(i, j);
-+            }
-+        }
-+        if (split_k_range.second * quant_group != k) {
-+            int max_split_k = k / quant_group;
-+            if (max_split_k < 256) {
-+                for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
-+                    param_candidates.emplace_back(i, max_split_k);
-+                }
-+            }
-+        }
++template <typename scalar_t, typename scale_type, typename VT, typename VT1, int NUM_REG, int NUM_THREADS, bool WITHMASK>
++__global__ __launch_bounds__(1024) void dynamic_scaled_int8_quant_kernel_lh_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, int num_tokens, int* mask_buffer=NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }
++  int const tid = threadIdx.x;
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
++  float absmax_val = 0.0f;
++  float const zero = 0.0f;
++  constexpr int N = sizeof(VT) / sizeof(scalar_t);
++  int stride = NUM_THREADS * N;
++  float reg_src[NUM_REG][N];
++  scalar_t const* ptr_input = input + token_idx * hidden_size;
++  for(int i = tid * N, k = 0; i < hidden_size; i += stride, k++) {
++    VT vsrc = *(VT*)(ptr_input + i);
++    scalar_t *ptr_src = (scalar_t*)&vsrc;
++    #pragma unroll N
++    for(int j = 0; j < N; j++) {
++        float val = static_cast<float>(ptr_src[j]);
++        reg_src[k][j] = val;
++        val = abs(val);
++        absmax_val = max(val, absmax_val);
++    }
++  }
++  using BlockReduce = cub::BlockReduce<float, NUM_THREADS>;
++  __shared__ typename BlockReduce::TempStorage reduceStorage;
++  float const block_absmax_val_maybe =
++      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, NUM_THREADS);
++  __shared__ float block_absmax_val;
++  if (tid == 0) {
++    block_absmax_val = block_absmax_val_maybe;
++    scale[token_idx] = block_absmax_val * 0.0078740157;
++  }
++  
++  __syncthreads();
 +
-+        current_block_x = block_x_range.second;
-+        current_split_k = split_k_range.second;
-+        uint64_t data_size_in_bytes = _n * _k * sizeof(half);
-+        data_size_in_bytes += (uint64_t)_m * _k / pack_ratio * sizeof(uint32_t);
-+        data_size_in_bytes += (uint64_t)_m * _k / quant_group * sizeof(half);
-+        data_size_in_bytes += (uint64_t)_m * _k / quant_group / pack_ratio * sizeof(uint32_t);
-+        data_size_in_bytes += (uint64_t)_n * _m * sizeof(half);
-+        data_size_gb = float(data_size_in_bytes) / 1024 / 1024 / 1024;
-+        warmup_iters = 4;
-+        _valid = true;
++  float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++  int8_t* ptr_output = out + token_idx * hidden_size;
++  for(int i = tid * N, k = 0; i < hidden_size; i += stride, k++) {
++    VT1 vdst;
++    int8_t* ptr_reg = (int8_t*)&vdst;
++    #pragma unroll N
++    for(int j = 0; j < N; j++) {
++        ptr_reg[j] = float_to_int8_rn(
++            reg_src[k][j] * tmp_scale);
 +    }
++    *(VT1*)(ptr_output + i) = vdst;
++  }
++}
 +
-+    void select_in_warmup(const std::function<bool(int,int)> &f) {
-+        if (!valid()) {
-+            printf("Cannot run this selector!\n");
-+            return;
-+        }
-+        if (_selected) {
-+            f(best_block_x, best_split_k);
-+            return;
-+        };
-+        //Warmup
-+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
-+            for (int i = 0; i < 5; i++) {
-+                auto &p = *iter;
-+                f(p.first, p.second);
-+            }
-+        }
-+        _r.reset(new KernelEventRecorder());
-+        kernel_best_time_ms_ave = 9999999.0f;
-+        mcDeviceSynchronize();
++template <typename scalar_t, typename scale_type, typename VT, typename VT1, bool WITHMASK>
++__global__ void dynamic_scaled_int8_quant_kernel_sm_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, int blockDim_x, int num_tokens, int* mask_buffer=NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }
++  int const tid = threadIdx.x;
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
++  float absmax_val = 0.0f;
++  float const zero = 0.0f;
++  constexpr int N = sizeof(VT) / sizeof(scalar_t);
++  int stride = blockDim_x * N;
++  __shared__ float sm_buffer[8064];
++  scalar_t const* ptr_input = input + token_idx * hidden_size;
++  for(int i = tid * N; i < hidden_size; i += stride) {
++    VT vsrc = *(VT*)(ptr_input + i);
++    scalar_t *ptr_src = (scalar_t*)&vsrc;
++    float* ptr_sm_buffer = sm_buffer + i;
++    #pragma unroll N
++    for(int j = 0; j < N; j++) {
++        float val = static_cast<float>(ptr_src[j]);
++        ptr_sm_buffer[j] = val;
++        val = abs(val);
++        absmax_val = max(val, absmax_val);
++    }
++  }
++  using BlockReduce = cub::BlockReduce<float, 512>;
++  __shared__ typename BlockReduce::TempStorage reduceStorage;
++  float const block_absmax_val_maybe =
++      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
++  __shared__ float block_absmax_val;
++  if (tid == 0) {
++    block_absmax_val = block_absmax_val_maybe;
++    scale[token_idx] = block_absmax_val * 0.0078740157;
++  }
++  
++  __syncthreads();
+ 
++  float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++  int8_t* ptr_output = out + token_idx * hidden_size;
++  for(int i = tid * N; i < hidden_size; i += stride) {
++    VT1 vdst;
++    int8_t* ptr_reg = (int8_t*)&vdst;
++    float* ptr_sm_buffer = sm_buffer + i;
++    #pragma unroll N
++    for(int j = 0; j < N; j++) {
++        ptr_reg[j] = float_to_int8_rn(
++            ptr_sm_buffer[j] * tmp_scale);
++    }
++    *(VT1*)(ptr_output + i) = vdst;
++  }
++}
 +
-+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
-+            auto &p = *iter;
-+            auto &bx = p.first;
-+            auto &sk = p.second;
-+            mcDeviceSynchronize();
-+            _r->start();
-+            bool launched = false;
-+            for (int i = 0; i < MAX_PERF_COUNT; i++) {
-+                launched = f(bx, sk);
-+            }
-+            auto ms = _r->stop();
-+            if (launched && ms / MAX_PERF_COUNT < kernel_best_time_ms_ave) {
-+                best_block_x = bx;
-+                best_split_k = sk;
-+                kernel_best_time_ms_ave = ms / MAX_PERF_COUNT;
-+            }
-+        }
++template <typename scalar_t, typename scale_type, typename VT, typename VT1, bool WITHMASK>
++__launch_bounds__(1024) __global__ void dynamic_scaled_int8_quant_kernel_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, const int blockDim_x, const int num_tokens, int* mask_buffer=NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }
++  constexpr int N = sizeof(VT) / sizeof(scalar_t);
++  int const tid = threadIdx.x * N;
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
++  float absmax_val = 0.0f;
++  int stride = blockDim_x * N;
++  const scalar_t * ptr_input = input + token_idx * hidden_size;
 +
-+        _r.reset();
-+        _selected = true;
-+        warmup_iters = 0;
++  for (int i = tid ; i < hidden_size; i += stride) {
++    VT vsrc = *(VT*)(ptr_input + i);
++    scalar_t *ptr_src = (scalar_t*)&vsrc;
++    #pragma unroll N
++    for(int j = 0; j < N; j++) {
++        float val = static_cast<float>(ptr_src[j]);
++        val = abs(val);
++        absmax_val = max(val, absmax_val);
 +    }
++  }
 +
-+    void run(const std::function<bool(int,int)> &f) {
-+        if (!valid()) {
-+            printf("Cannot run this selector!\n");
-+            return;
-+        }
-+        if (warmup_iters > 0) {
-+            f(current_block_x, current_split_k);
-+            //printf("Calling warmup current_block_x = %d, current_split_k = %d\n", current_block_x, current_split_k);
-+            if (current_block_x > block_x_range.first) current_block_x /= 2;
-+            else if (current_split_k > split_k_range.first) current_split_k /= 2;
-+            if (current_block_x == block_x_range.first && current_split_k >= split_k_range.first) {
-+                current_block_x = block_x_range.second;
-+                current_split_k = split_k_range.second;
-+            }
-+            warmup_iters--;
-+            mcDeviceSynchronize();
-+            return;
-+        }
++    using BlockReduce = cub::BlockReduce<float, 1024>;
++  __shared__ typename BlockReduce::TempStorage reduceStorage;
++  float const block_absmax_val_maybe =
++      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
++  __shared__ float block_absmax_val;
++  if (tid == 0) {
++    block_absmax_val = block_absmax_val_maybe;
++    scale[token_idx] = block_absmax_val * 0.0078740157;
++  }
++  __syncthreads();
 +
-+        if (_selected) {
-+            f(best_block_x, best_split_k);
-+        } else {
-+            if (!_r) {
-+                _r = std::shared_ptr<KernelEventRecorder>(new KernelEventRecorder());
-+                current_block_x = block_x_range.second;
-+                current_split_k = split_k_range.second;
-+                current_perf_iter = MAX_PERF_COUNT;
-+            }
-+            _r->start();
-+            auto launched = f(current_block_x, current_split_k);
-+            auto ms = _r->stop();
-+            //printf("Launched with bx=%d,sk=%d,iter=%d,ms=%fms\n", current_block_x, current_split_k, current_perf_iter, ms);
-+            if (!launched) {
-+                printf("ERROR: Cannot run gemv with bx = %d, sk = %d\n", current_block_x, current_split_k);
-+                return;
-+            }
-+            if (current_perf_iter-- > 0) {
-+                perf_times.emplace_back(ms);
-+                return;
-+            }
++  float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++  int8_t* ptr_output = out + token_idx * hidden_size;
++  for (int i = tid; i < hidden_size; i += stride) {
++    VT vsrc = *(VT*)(ptr_input + i);
++    VT1 vdst;
++    scalar_t *ptr_src = (scalar_t*)&vsrc;
++    int8_t* ptr_dst = (int8_t*)&vdst;
++    #pragma unroll N
++    for(int j = 0; j < N; ++j) {
++        ptr_dst[j] = float_to_int8_rn(
++        static_cast<float>(ptr_src[j]) * tmp_scale);
++    }
++    *(VT1*)(ptr_output + i) = vdst;
++  }
++}
 +
-+            std::sort(perf_times.begin(), perf_times.end());
-+            float total_tm = 0;
-+            for (size_t i = 1; i < perf_times.size() - 1; i++) {
-+                total_tm += perf_times[i];
-+            }
++template <typename scalar_t, typename scale_type, typename azp_type, bool WITHMASK>
++__global__ void dynamic_scaled_int8_azp_quant_kernel(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, azp_type* azp, const int hidden_size, const int num_tokens, int* mask_buffer=NULL) {
++  if constexpr(WITHMASK) {
++    __shared__ int sm_max_token;
++    if(threadIdx.x == 0) sm_max_token = mask_buffer[blockIdx.y]; 
++    __syncthreads();
++    if(blockIdx.x >= sm_max_token) return;
++  }  
++  int64_t const token_idx = blockIdx.y * num_tokens + blockIdx.x;
+   // Scan for the min and max value for this token
+   float max_val = std::numeric_limits<float>::min();
+   float min_val = std::numeric_limits<float>::max();
+   for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
+-    auto val = static_cast<float>(input[i]);
++    auto val = static_cast<float>(input[token_idx * hidden_size + i]);
+     max_val = std::max(max_val, val);
+     min_val = std::min(min_val, val);
+   }
+@@ -226,11 +541,830 @@ __global__ void dynamic_scaled_int8_azp_quant_kernel(
+ 
+   // Quantize the values
+   for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
+-    auto const val = static_cast<float>(input[i]);
++    auto const val = static_cast<float>(input[token_idx * hidden_size + i]);
+     auto const quant_val =
+         int32_to_int8(float_to_int32_rn(val / scale_val) + azp_val);
+-    out[i] = quant_val;
++    out[token_idx * hidden_size + i] = quant_val;
++  }
++}
 +
-+            ms = total_tm /= (MAX_PERF_COUNT - 2);
-+            perf_times.clear();
-+            current_perf_iter = MAX_PERF_COUNT;
-+            //printf("get ave time %fms\n", ms);
++template <typename scalar_t, typename scale_type>
++__global__ void dynamic_scaled_int8_quant_mask_kernel(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, const int num_tokens, const int mask_size, const int grid_size, int * mask) {
++  int const tid = threadIdx.x;
++  __shared__ int sm_mask[16];
++  __shared__ int sm_stride[16];
++  if(tid < mask_size) {
++    sm_mask[tid] = mask[tid];
++  }
++  __syncthreads();
++  if(tid < mask_size) {
++    int tmp = 0;
++    for(int i = 0; i < tid; i++) {
++      tmp += sm_mask[i];
++    }
++    sm_stride[tid] = tmp;
++  }
++  __syncthreads();
++  int total_tokens = sm_stride[mask_size - 1] + sm_mask[mask_size - 1];
++  for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++    int token_id = mask_size - 1;
++    while(idx < sm_stride[token_id]) {
++      token_id--;
++    }
++    int64_t const token_idx = token_id * num_tokens + idx - sm_stride[token_id];
++    float absmax_val = 0.0f;
++    float const zero = 0.0f;
 +
-+            if (ms < kernel_best_time_ms_ave) {
-+                best_block_x = current_block_x;
-+                best_split_k = current_split_k;
-+                kernel_best_time_ms_ave = ms;
-+            }
++    for (int i = tid; i < hidden_size; i += blockDim.x) {
++      float val = static_cast<float>(input[token_idx * hidden_size + i]);
++      val = val > zero ? val : -val;
++      absmax_val = val > absmax_val ? val : absmax_val;
++    }
 +
-+            if (current_split_k > split_k_range.first) {
-+                current_split_k /= 2;
-+            } else if (current_block_x > block_x_range.first){
-+                current_split_k = split_k_range.second;
-+                current_block_x /= 2;
-+            } else if (current_block_x == block_x_range.first && current_split_k == split_k_range.first) {
-+                _selected = true;
-+                _r.reset();
-+            } else {
-+                printf("Error: all parameters are tried! current_block_x = %d, current_split_k = %d, block_x_range = %d,%d, split_k_range = %d,%d, best_block_x = %d, best_split_k = %d\n",
-+                    current_block_x, current_split_k,
-+                    block_x_range.first, block_x_range.second,
-+                    split_k_range.first, split_k_range.second,
-+                    best_block_x, best_split_k
-+                );
-+            }
-+        }
++    using BlockReduce = cub::BlockReduce<float, 1024>;
++    __shared__ typename BlockReduce::TempStorage reduceStorage;
++    float const block_absmax_val_maybe =
++        BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
++    __shared__ float block_absmax_val;
++    if (tid == 0) {
++      block_absmax_val = block_absmax_val_maybe;
++      scale[token_idx] = block_absmax_val * 0.0078740157;
 +    }
++    __syncthreads();
 +
-+    bool valid() const { return _valid; }
-+    bool selected() const {return _selected; }
++    float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++    for (int i = tid; i < hidden_size; i += blockDim.x) {
++      out[token_idx * hidden_size + i] = float_to_int8_rn(
++          static_cast<float>(input[token_idx * hidden_size + i]) * tmp_scale);
++    }
++  }
++}
 +
-+    float gemv_ave_time_us_cost() {
-+        return kernel_best_time_ms_ave;
++template <typename scalar_t, typename scale_type, typename VT, typename VT1>
++__global__ void dynamic_scaled_int8_quant_mask_kernel_sreg_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, int blockDim_x, int num_tokens, int mask_size, int grid_size, int* mask=NULL) {
++  int const tid = threadIdx.x;
++  __shared__ int sm_mask[16];
++  __shared__ int sm_stride[16];
++  if(tid < mask_size) {
++    sm_mask[tid] = mask[tid];
++  }
++  __syncthreads();
++  if(tid < mask_size) {
++    int tmp = 0;
++    for(int i = 0; i < tid; i++) {
++      tmp += sm_mask[i];
 +    }
++    sm_stride[tid] = tmp;
++  }
++  __syncthreads();
++  int total_tokens = sm_stride[mask_size - 1] + sm_mask[mask_size - 1];
++  for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++      int token_id = mask_size - 1;
++      while(idx < sm_stride[token_id]) {
++        token_id--;
++      }
++      int64_t const token_idx = token_id * num_tokens + idx - sm_stride[token_id];
++      scalar_t absmax_val = static_cast<scalar_t>(0.0f);
++      float const zero = 0.0f;
++      constexpr int N = sizeof(VT) / sizeof(scalar_t);
++      scalar_t reg_src0[N];
++      scalar_t const* ptr_input = input + token_idx * hidden_size;
++      int reg_length = blockDim_x * N;
++      int length = min(hidden_size, reg_length);
++      int index = tid * N;
++      if(index < length) {
++        *(VT*)reg_src0 = *(VT*)(ptr_input + index);
++        #pragma unroll N
++        for(int i = 0; i < N; i++) {
++          scalar_t val = abs(reg_src0[i]);
++          absmax_val = max(absmax_val, val);
++        }
++      }
 +
-+    float gemv_bandwidth() {
-+        return data_size_gb / (kernel_best_time_ms_ave / 1000.0f);
++      using BlockReduce = cub::BlockReduce<scalar_t, 512>;
++      __shared__ typename BlockReduce::TempStorage reduceStorage;
++      float const block_absmax_val_maybe =
++          BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
++      __shared__ scale_type block_absmax_val;
++      if (tid == 0) {
++        block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
++        scale[token_idx] = static_cast<scale_type>(block_absmax_val * 0.0078740157);
++      }
++      __syncthreads();
++      float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++      int8_t* ptr_output = out + token_idx * hidden_size;
++      if(index < length) {
++        VT1 vdst;
++        int8_t* ptr_reg = (int8_t*)&vdst;
++        #pragma unroll N
++        for(int i = 0; i < N; i++) {
++          ptr_reg[i] = float_to_int8_rn(
++                static_cast<float>(reg_src0[i]) * tmp_scale);
++        }
++        *(VT1*)(ptr_output + index) = vdst;
++      }
++  }
++}
++
++template <typename scalar_t, typename scale_type, typename VT, typename VT1>
++__global__ void dynamic_scaled_int8_quant_mask_kernel_reg_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, int blockDim_x, int num_tokens, int mask_size, int grid_size, int* mask=NULL) {
++  int const tid = threadIdx.x;
++  __shared__ int sm_mask[16];
++  __shared__ int sm_stride[16];
++  if(tid < mask_size) {
++    sm_mask[tid] = mask[tid];
+   }
++  __syncthreads();
++  if(tid < mask_size) {
++    int tmp = 0;
++    for(int i = 0; i < tid; i++) {
++      tmp += sm_mask[i];
++    }
++    sm_stride[tid] = tmp;
++  }
++  __syncthreads();
++  int total_tokens = sm_stride[mask_size - 1] + sm_mask[mask_size - 1];
++  for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) 
++  {
++    int token_id = mask_size - 1;
++    while(idx < sm_stride[token_id]) {
++      token_id--;
++    }
++    int64_t const token_idx = token_id * num_tokens + idx - sm_stride[token_id];
++    scalar_t absmax_val = static_cast<scalar_t>(0.0f);
++    float const zero = 0.0f;
++    constexpr int N = sizeof(VT) / sizeof(scalar_t);
++    scalar_t reg_src0[N];
++    scalar_t reg_src1[N];
++    scalar_t const* ptr_input = input + token_idx * hidden_size;
++    int reg_length = 2 * blockDim_x * N;
++    int length = min(hidden_size, reg_length);
++    int index = 2 * tid * N;
++    if(index < length) {
++      *(VT*)reg_src0 = *(VT*)(ptr_input + index);
++      *(VT*)reg_src1 = *(VT*)(ptr_input + index + N);
++      #pragma unroll N
++      for(int i = 0; i < N; i++) {
++        scalar_t val = abs(reg_src0[i]);
++        absmax_val =  max(val, absmax_val);
++        val = abs(reg_src1[i]);
++        absmax_val = max(val, absmax_val);
++      }
 +    }
 +
-+    static bool prepare_parameters(int m, int n, int k, int &bx, int &sk, bool allow_imcomplete_bx = false) {
-+        if (n > 4) return false;
-+        if (k % quant_group != 0) return false;
-+        if (m < 16 * m_per_thread) return false;
-+        int max_split_k = k / quant_group;
-+        int proper_splitk = 1;
-+        while (proper_splitk*2 <= 256 && proper_splitk*2 <= max_split_k) proper_splitk *= 2;
++    using BlockReduce = cub::BlockReduce<scalar_t, 512>;
++    __shared__ typename BlockReduce::TempStorage reduceStorage;
++    scalar_t const block_absmax_val_maybe =
++        BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
++    __shared__ scale_type block_absmax_val;
++    if (tid == 0) {
++      block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
++      scale[token_idx] = block_absmax_val * 0.0078740157;
++    }
++    __syncthreads();
++    float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++    int8_t* ptr_output = out + token_idx * hidden_size;
++    if(index < length) {
++      VT1 vdst;
++      int8_t* ptr_reg = (int8_t*)&vdst;
++      constexpr int ON = 2 * N;
++      #pragma unroll N
++      for(int i = 0; i < N; i++) {
++        ptr_reg[i] = float_to_int8_rn(
++              static_cast<float>(reg_src0[i]) * tmp_scale);
++      }
++      ptr_reg = ptr_reg + N;
++      #pragma unroll N
++      for(int i = 0; i < N; i++) {
++        ptr_reg[i] = float_to_int8_rn(
++              static_cast<float>(reg_src1[i]) * tmp_scale);
++      }
++      *(VT1*)(ptr_output + index) = vdst;
++    }
++  }
++}
 +
-+        int thread_block = 256;  //A thread block of 512 will wait for more warps when doing syncthreads
++template <typename scalar_t, typename scale_type, typename VT, typename VT1>
++__global__ void dynamic_scaled_int8_quant_mask_kernel_sm_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, int blockDim_x, int num_tokens, int mask_size, int grid_size, int* mask=NULL) {
++  int const tid = threadIdx.x;
++  __shared__ int sm_mask[16];
++  __shared__ int sm_stride[16];
++  if(tid < mask_size) {
++    sm_mask[tid] = mask[tid];
++  }
++  __syncthreads();
++  if(tid < mask_size) {
++    int tmp = 0;
++    for(int i = 0; i < tid; i++) {
++      tmp += sm_mask[i];
++    }
++    sm_stride[tid] = tmp;
++  }
++  __syncthreads();
++  int total_tokens = sm_stride[mask_size - 1] + sm_mask[mask_size - 1];
++  for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++    int token_id = mask_size - 1;
++    while(idx < sm_stride[token_id]) {
++      token_id--;
++    }
++    int64_t const token_idx = token_id * num_tokens + idx - sm_stride[token_id];
++    float absmax_val = 0.0f;
++    float const zero = 0.0f;
++    constexpr int N = sizeof(VT) / sizeof(scalar_t);
++    int stride = blockDim_x * N;
++    __shared__ float sm_buffer[8064];
++    scalar_t const* ptr_input = input + token_idx * hidden_size;
++    for(int i = tid * N; i < hidden_size; i += stride) {
++      VT vsrc = *(VT*)(ptr_input + i);
++      scalar_t *ptr_src = (scalar_t*)&vsrc;
++      float* ptr_sm_buffer = sm_buffer + i;
++      #pragma unroll N
++      for(int j = 0; j < N; j++) {
++          float val = static_cast<float>(ptr_src[j]);
++          ptr_sm_buffer[j] = val;
++          val = val > zero ? val : -val;
++          absmax_val = val > absmax_val ? val : absmax_val;
++      }
++    }
++    using BlockReduce = cub::BlockReduce<float, 512>;
++    __shared__ typename BlockReduce::TempStorage reduceStorage;
++    float const block_absmax_val_maybe =
++        BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
++    __shared__ float block_absmax_val;
++    if (tid == 0) {
++      block_absmax_val = block_absmax_val_maybe;
++      scale[token_idx] = block_absmax_val * 0.0078740157;
++    }
++    
++    __syncthreads();
 +
-+        int proper_bx = 16;
-+        if (m % (proper_bx * m_per_thread) != 0) return false;
-+        //proper_bx * m_per_threads corresponds to group_elements, this value should not be larger than 2048 as limit of shared memory
-+        while (proper_bx * 2 * m_per_thread && (m % (proper_bx*2*m_per_thread) == 0) && proper_bx * 2 <= 256 && proper_bx * 2 * m_per_thread <= 2048) proper_bx*=2;
-+        if (allow_imcomplete_bx) {
-+            int may_proper_bx = proper_bx * 2;
-+            if (may_proper_bx <= 256 && may_proper_bx * m_per_thread <= 2048 && (m % (may_proper_bx*m_per_thread)) > may_proper_bx * m_per_thread / 4) {
-+                proper_bx = may_proper_bx;
-+            }
-+        }
++    float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++    int8_t* ptr_output = out + token_idx * hidden_size;
++    for(int i = tid * N; i < hidden_size; i += stride) {
++      VT1 vdst;
++      int8_t* ptr_reg = (int8_t*)&vdst;
++      float* ptr_sm_buffer = sm_buffer + i;
++      #pragma unroll N
++      for(int j = 0; j < N; j++) {
++          ptr_reg[j] = float_to_int8_rn(
++              ptr_sm_buffer[j] * tmp_scale);
++      }
++      *(VT1*)(ptr_output + i) = vdst;
++    }
++  }
++}
 +
-+        bx = proper_bx;
-+        sk = proper_splitk;
-+        //printf("m=%d,n=%d,k=%d,get bx=%d,sk=%d,allow_imcomplete_bx=%d\n", m, n, k, bx, sk, allow_imcomplete_bx);
-+        return true;
++template <typename scalar_t, typename scale_type, typename VT, typename VT1>
++__launch_bounds__(1024) __global__ void dynamic_scaled_int8_quant_mask_kernel_opt(
++    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
++    scale_type* scale, const int hidden_size, const int blockDim_x, const int num_tokens, int mask_size, int grid_size, int* mask=NULL) {
++  constexpr int N = sizeof(VT) / sizeof(scalar_t);
++  int const tid = threadIdx.x * N;
++  __shared__ int sm_mask[16];
++  __shared__ int sm_stride[16];
++  if(threadIdx.x < mask_size) {
++    sm_mask[threadIdx.x] = mask[threadIdx.x];
++  }
++  __syncthreads();
++  if(threadIdx.x < mask_size) {
++    int tmp = 0;
++    for(int i = 0; i < threadIdx.x; i++) {
++      tmp += sm_mask[i];
 +    }
-+};
++    sm_stride[threadIdx.x] = tmp;
++  }
++  __syncthreads();
++  int total_tokens = sm_stride[mask_size - 1] + sm_mask[mask_size - 1];
++  
++  for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++    int token_id = mask_size - 1;
++    while(idx < sm_stride[token_id]) {
++      token_id--;
++    }
++    int64_t const token_idx = token_id * num_tokens + idx - sm_stride[token_id];
++    float absmax_val = 0.0f;
++    int stride = blockDim_x * N;
++    const scalar_t * ptr_input = input + token_idx * hidden_size;
 +
-+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
-+class GemvSelectorHolder {
-+private:
-+    std::vector<GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>> _selectors;
-+    static std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> _holder;
-+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> _invalid_selector;
++    for (int i = tid ; i < hidden_size; i += stride) {
++      VT vsrc = *(VT*)(ptr_input + i);
++      scalar_t *ptr_src = (scalar_t*)&vsrc;
++      #pragma unroll N
++      for(int j = 0; j < N; j++) {
++          float val = static_cast<float>(ptr_src[j]);
++          val = val > 0 ? val : -val;
++          absmax_val = val > absmax_val ? val : absmax_val;
++      }
++    }
 +
-+public:
-+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& selector(int m, int n, int k, bool allow_imcomplete_bx = false) {
-+        if (!GemvSelectorHolder::_holder) {
-+            GemvSelectorHolder::_holder.reset(new GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>());
-+        }
-+        int bx, sk;
-+        if (!GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>::prepare_parameters(m, n, k, bx, sk, allow_imcomplete_bx)) {
-+            return _invalid_selector;
-+        }
-+        auto iter = std::find_if(_holder->_selectors.begin(), _holder->_selectors.end(),
-+            [&](const GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& p) -> bool {
-+                return p.m == m && p.k == k;
-+            });
-+        if (iter != _holder->_selectors.end()) return *iter;
-+        GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> sl(m, n, k, allow_imcomplete_bx);
-+        if (!sl.valid()) {
-+            return _invalid_selector;
-+        }
-+        _holder->_selectors.emplace_back(sl);
-+        return _holder->_selectors.back();
++      using BlockReduce = cub::BlockReduce<float, 1024>;
++    __shared__ typename BlockReduce::TempStorage reduceStorage;
++    float const block_absmax_val_maybe =
++        BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
++    __shared__ float block_absmax_val;
++    if (tid == 0) {
++      block_absmax_val = block_absmax_val_maybe;
++      scale[token_idx] = block_absmax_val * 0.0078740157;
 +    }
-+};
++    __syncthreads();
 +
-+template<int quant_group, int pack_ratio, int m_per_thread>
-+std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_holder;
++    float const tmp_scale = 127.0f *__builtin_mxc_rcpf(block_absmax_val);
++    int8_t* ptr_output = out + token_idx * hidden_size;
++    for (int i = tid; i < hidden_size; i += stride) {
++      VT vsrc = *(VT*)(ptr_input + i);
++      VT1 vdst;
++      scalar_t *ptr_src = (scalar_t*)&vsrc;
++      int8_t* ptr_dst = (int8_t*)&vdst;
++      #pragma unroll N
++      for(int j = 0; j < N; ++j) {
++          ptr_dst[j] = float_to_int8_rn(
++          static_cast<float>(ptr_src[j]) * tmp_scale);
++      }
++      *(VT1*)(ptr_output + i) = vdst;
++    }
++  }
++}
 +
-+template<int quant_group, int pack_ratio, int m_per_thread>
-+GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_invalid_selector(0,8,0);
-+}
-\ No newline at end of file
-diff --git a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
-index e79785827..9bf453e3b 100644
---- a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
-+++ b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include <ATen/cuda/CUDAContext.h>
- #include <torch/all.h>
- #include <cmath>
-@@ -30,8 +31,13 @@ static inline __device__ int8_t float_to_int8_rn(float x) {
-   return static_cast<int8_t>(dst);
- #else
-   // CUDA path
--  uint32_t dst;
--  asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
-+  //uint32_t dst;
-+  //asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
-+  //return reinterpret_cast<const int8_t&>(dst);
-+  constexpr float c = 0.5;
-+  int32_t dst;
-+  dst = (int32_t)(x > 0 ? x + c: x - c);
-+  dst = (char)(dst > 127 ? 127 : (dst < -128 ? -128 : dst));
-   return reinterpret_cast<const int8_t&>(dst);
- #endif
- }
-@@ -66,7 +72,9 @@ static inline __device__ int32_t float_to_int32_rn(float x) {
- #else
-   // CUDA path
-   uint32_t dst;
-+#ifdef MX_MACA
-   asm volatile("cvt.rni.sat.s32.f32 %0, %1;" : "=r"(dst) : "f"(x));
-+#endif
-   return reinterpret_cast<const int32_t&>(dst);
- #endif
- }
-@@ -84,7 +92,9 @@ static inline __device__ int8_t int32_to_int8(int32_t x) {
- #else
-   // CUDA path
-   uint32_t dst;
-+#ifdef MX_MACA
-   asm volatile("cvt.sat.s8.s32 %0, %1;" : "=r"(dst) : "r"(x));
-+#endif
-   return reinterpret_cast<const int8_t&>(dst);
- #endif
- }
-@@ -96,15 +106,12 @@ __global__ void static_scaled_int8_quant_kernel(
-     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-     scale_type const* scale_ptr, const int hidden_size) {
-   int const tid = threadIdx.x;
--  int64_t const token_idx = blockIdx.x;
-+  int const token_idx = blockIdx.x;
-   scale_type const scale = *scale_ptr;
- 
--  // Must be performed using 64-bit math to avoid integer overflow.
--  out += token_idx * hidden_size;
--  input += token_idx * hidden_size;
--
-   for (int i = tid; i < hidden_size; i += blockDim.x) {
--    out[i] = float_to_int8_rn(static_cast<float>(input[i]) / scale);
-+    out[token_idx * hidden_size + i] = float_to_int8_rn(
-+        static_cast<float>(input[token_idx * hidden_size + i]) / scale);
-   }
- }
- 
-@@ -114,18 +121,14 @@ __global__ void static_scaled_int8_azp_quant_kernel(
-     scale_type const* scale_ptr, azp_type const* azp_ptr,
-     const int hidden_size) {
-   int const tid = threadIdx.x;
--  int64_t const token_idx = blockIdx.x;
-+  int const token_idx = blockIdx.x;
-   scale_type const scale = *scale_ptr;
-   azp_type const azp = *azp_ptr;
- 
--  // Must be performed using 64-bit math to avoid integer overflow.
--  out += token_idx * hidden_size;
--  input += token_idx * hidden_size;
--
-   for (int i = tid; i < hidden_size; i += blockDim.x) {
--    auto const val = static_cast<float>(input[i]);
-+    auto const val = static_cast<float>(input[token_idx * hidden_size + i]);
-     auto const quant_val = int32_to_int8(float_to_int32_rn(val / scale) + azp);
--    out[i] = quant_val;
-+    out[token_idx * hidden_size + i] = quant_val;
-   }
- }
- 
-@@ -134,16 +137,12 @@ __global__ void dynamic_scaled_int8_quant_kernel(
-     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-     scale_type* scale, const int hidden_size) {
-   int const tid = threadIdx.x;
--  int64_t const token_idx = blockIdx.x;
-+  int const token_idx = blockIdx.x;
-   float absmax_val = 0.0f;
-   float const zero = 0.0f;
- 
--  // Must be performed using 64-bit math to avoid integer overflow.
--  out += token_idx * hidden_size;
--  input += token_idx * hidden_size;
--
-   for (int i = tid; i < hidden_size; i += blockDim.x) {
--    float val = static_cast<float>(input[i]);
-+    float val = static_cast<float>(input[token_idx * hidden_size + i]);
-     val = val > zero ? val : -val;
-     absmax_val = val > absmax_val ? val : absmax_val;
-   }
-@@ -161,7 +160,213 @@ __global__ void dynamic_scaled_int8_quant_kernel(
- 
-   float const tmp_scale = 127.0f / block_absmax_val;
-   for (int i = tid; i < hidden_size; i += blockDim.x) {
--    out[i] = float_to_int8_rn(static_cast<float>(input[i]) * tmp_scale);
-+    out[token_idx * hidden_size + i] = float_to_int8_rn(
-+        static_cast<float>(input[token_idx * hidden_size + i]) * tmp_scale);
-+  }
-+}
++template<typename T, typename T1, typename VT, typename VT1, int NUM_VT> 
++__global__ void silu_and_mul_mask_quant_pack(T* input, T* output,T1* mask, int mask_size, int64_t grid_size, int64_t num_tokens, int64_t hidden_size, int64_t out_stirde, int blockDim_x)
++{
++    constexpr int N = sizeof(VT) / sizeof(T);
++    int const tid = threadIdx.x;
++    __shared__ T1 sm_mask[16];
++    __shared__ T1 sm_stride[16];
++    if(tid < mask_size) {
++        sm_mask[tid] = mask[tid];
++    }
 +
-+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
-+__global__ void dynamic_scaled_int8_quant_kernel_sreg_opt(
-+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-+    scale_type* scale, const int hidden_size, int blockDim_x) {
-+  int const tid = threadIdx.x;
-+  int const token_idx = blockIdx.x;
-+  scalar_t absmax_val = static_cast<scalar_t>(0.0f);
-+  float const zero = 0.0f;
-+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
-+  scalar_t reg_src0[N];
-+  scalar_t const* ptr_input = input + token_idx * hidden_size;
-+  int reg_length = blockDim_x * N;
-+  int length = min(hidden_size, reg_length);
-+  int index = tid * N;
-+  if(index < length) {
-+    *(VT*)reg_src0 = *(VT*)(ptr_input + index);
-+    #pragma unroll N
-+    for(int i = 0; i < N; i++) {
-+      scalar_t val = abs(reg_src0[i]);
-+      absmax_val = max(absmax_val, val);
++    int64_t hidden_size2 = hidden_size << 1;
++    __syncthreads();
++    if(tid < mask_size) {
++        T1 tmp = 0;
++        for(int i = 0; i < tid; i++) {
++        tmp += sm_mask[i];
++        }
++        sm_stride[tid] = tmp;
 +    }
-+  }
++    int stride = blockDim_x * N;
++    __syncthreads();
++    int64_t total_tokens = sm_stride[mask_size - 1] + sm_mask[mask_size - 1];
 +
-+  using BlockReduce = cub::BlockReduce<scalar_t, 512>;
-+  __shared__ typename BlockReduce::TempStorage reduceStorage;
-+  float const block_absmax_val_maybe =
-+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
-+  __shared__ scale_type block_absmax_val;
-+  if (tid == 0) {
-+    block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
-+    scale[token_idx] = static_cast<scale_type>(block_absmax_val / 127.0f);
-+  }
-+  __syncthreads();
-+  float const tmp_scale = 127.0f / block_absmax_val;
-+  int8_t* ptr_output = out + token_idx * hidden_size;
-+  if(index < length) {
-+    VT1 vdst;
-+    int8_t* ptr_reg = (int8_t*)&vdst;
-+    #pragma unroll N
-+    for(int i = 0; i < N; i++) {
-+      ptr_reg[i] = float_to_int8_rn(
-+            static_cast<float>(reg_src0[i]) * tmp_scale);
++    for(int64_t idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++        float reg_i[NUM_VT][N];
++        int64_t token_id = mask_size - 1;
++        while(idx < sm_stride[token_id]) {
++            token_id--;
++        }
++        int64_t const token_idx = token_id * num_tokens + idx - sm_stride[token_id];
++        const T* ptr_input0 = input + token_idx * hidden_size2;
++        const T* ptr_input1 = ptr_input0 + hidden_size;
++        float absmax_val = 0.0f;
++        for(int i = tid*N, j = 0; i < hidden_size; i += stride, j++) {
++            VT vsrc0, vsrc1;
++            vsrc0 = *(VT*)(ptr_input0 + i);
++            vsrc1 = *(VT*)(ptr_input1 + i);
++            T* ptr_local0 = (T*)&vsrc0;
++            T* ptr_local1 = (T*)&vsrc1;
++            #pragma unroll N
++            for(int k = 0; k < N; k++) {
++                float val0 = static_cast<float>(ptr_local0[k]);
++                float val1 = static_cast<float>(ptr_local1[k]);
++                float sigmoid = val0 * __builtin_mxc_rcpf(1.0f + __builtin_expf(-val0));
++                float gate_up = val1 * sigmoid;
++                reg_i[j][k] = gate_up;
++                absmax_val = max(absmax_val, abs(gate_up));
++            }
++        }
++
++        using BlockReduce = cub::BlockReduce<float, 512>;
++        __shared__ typename BlockReduce::TempStorage reduceStorage;
++        float const block_absmax_val_maybe =
++            BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
++        __shared__ float block_absmax_val;
++        int8_t* ptr_output = (int8_t*)(output + token_idx * out_stirde);
++        float* ptr_scale = (float*)(ptr_output + hidden_size);
++        if (tid == 0) {
++            block_absmax_val = block_absmax_val_maybe;
++            ptr_scale[0] = block_absmax_val * 0.0078740157;
++        }
++        __syncthreads();
++        float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++        for (int i = tid*N, k = 0; i < hidden_size; i += stride, k++) {
++            VT1 vdst;
++            int8_t* ptr_dst = (int8_t*)&vdst;
++            #pragma unroll N
++            for(int j = 0; j < N; ++j) {
++                ptr_dst[j] = float_to_int8_rn(reg_i[k][j] * tmp_scale);
++            }
++            *(VT1*)(ptr_output + i) = vdst;
++        }
 +    }
-+    *(VT1*)(ptr_output + index) = vdst;
-+  }
 +}
 +
-+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
-+__global__ void dynamic_scaled_int8_quant_kernel_reg_opt(
-+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-+    scale_type* scale, const int hidden_size, int blockDim_x) {
-+  int const tid = threadIdx.x;
-+  int const token_idx = blockIdx.x;
-+  scalar_t absmax_val = static_cast<scalar_t>(0.0f);
-+  float const zero = 0.0f;
-+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
-+  scalar_t reg_src0[N];
-+  scalar_t reg_src1[N];
-+  scalar_t const* ptr_input = input + token_idx * hidden_size;
-+  int reg_length = 2 * blockDim_x * N;
-+  int length = min(hidden_size, reg_length);
-+  int index = 2 * tid * N;
-+  if(index < length) {
-+    *(VT*)reg_src0 = *(VT*)(ptr_input + index);
-+    *(VT*)reg_src1 = *(VT*)(ptr_input + index + N);
-+    #pragma unroll N
-+    for(int i = 0; i < N; i++) {
-+      scalar_t val = abs(reg_src0[i]);
-+      absmax_val =  max(val, absmax_val);
-+      val = abs(reg_src1[i]);
-+      absmax_val = max(val, absmax_val);
-+    }
-+  }
++template<typename T, typename T1, typename VT, typename VT1, int NUM_VT, int NUM_THREADS> 
++__global__ void silu_and_mul_mask_quant_pack_1mask(T* input, T* output,T1* mask, int grid_size, int64_t num_tokens, int64_t hidden_size, int64_t out_stirde)
++{
++    constexpr int N = sizeof(VT) / sizeof(T);
++    int const tid = threadIdx.x;
++    T1 mask_stride;
++    mask_stride = mask[0];
++    int64_t hidden_size2 = hidden_size << 1;
++    int stride = NUM_THREADS * N;
++    int total_tokens = mask_stride;
++
++    for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++        float reg_i[NUM_VT][N];
++        int64_t const token_idx = idx;
++        const T* ptr_input0 = input + token_idx * hidden_size2;
++        const T* ptr_input1 = ptr_input0 + hidden_size;
++        float absmax_val = 0.0f;
++        for(int i = tid*N, j = 0; i < hidden_size; i += stride, j++) {
++            VT vsrc0, vsrc1;
++            vsrc0 = *(VT*)(ptr_input0 + i);
++            vsrc1 = *(VT*)(ptr_input1 + i);
++            T* ptr_local0 = (T*)&vsrc0;
++            T* ptr_local1 = (T*)&vsrc1;
++            #pragma unroll N
++            for(int k = 0; k < N; k++) {
++                float val0 = static_cast<float>(ptr_local0[k]);
++                float val1 = static_cast<float>(ptr_local1[k]);
++                float sigmoid = val0 * __builtin_mxc_rcpf(1.0f + __builtin_expf(-val0));
++                float gate_up = val1 * sigmoid;
++                reg_i[j][k] = gate_up;
++                absmax_val = max(absmax_val, abs(gate_up));
++            }
++        }
 +
-+  using BlockReduce = cub::BlockReduce<scalar_t, 512>;
-+  __shared__ typename BlockReduce::TempStorage reduceStorage;
-+  scalar_t const block_absmax_val_maybe =
-+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
-+  __shared__ scale_type block_absmax_val;
-+  if (tid == 0) {
-+    block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
-+    scale[token_idx] = block_absmax_val / 127.0f;
-+  }
-+  __syncthreads();
-+  float const tmp_scale = 127.0f / block_absmax_val;
-+  int8_t* ptr_output = out + token_idx * hidden_size;
-+  if(index < length) {
-+    VT1 vdst;
-+    int8_t* ptr_reg = (int8_t*)&vdst;
-+    constexpr int ON = 2 * N;
-+    #pragma unroll N
-+    for(int i = 0; i < N; i++) {
-+      ptr_reg[i] = float_to_int8_rn(
-+             static_cast<float>(reg_src0[i]) * tmp_scale);
++        using BlockReduce = cub::BlockReduce<float, NUM_THREADS>;
++        __shared__ typename BlockReduce::TempStorage reduceStorage;
++        float const block_absmax_val_maybe =
++            BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, NUM_THREADS);
++        __shared__ float block_absmax_val;
++        int8_t* ptr_output = (int8_t*)(output + token_idx * out_stirde);
++        float* ptr_scale = (float*)(ptr_output + hidden_size);
++        if (tid == 0) {
++            block_absmax_val = block_absmax_val_maybe;
++            ptr_scale[0] = block_absmax_val * 0.0078740157;
++        }
++        __syncthreads();
++        float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++        for (int i = tid*N, k = 0; i < hidden_size; i += stride, k++) {
++            VT1 vdst;
++            int8_t* ptr_dst = (int8_t*)&vdst;
++            #pragma unroll N
++            for(int j = 0; j < N; ++j) {
++                ptr_dst[j] = float_to_int8_rn(reg_i[k][j] * tmp_scale);
++            }
++            *(VT1*)(ptr_output + i) = vdst;
++        }
 +    }
-+    ptr_reg = ptr_reg + N;
-+    #pragma unroll N
-+    for(int i = 0; i < N; i++) {
-+      ptr_reg[i] = float_to_int8_rn(
-+            static_cast<float>(reg_src1[i]) * tmp_scale);
++}
++
++template<typename T, typename T1, typename VT, typename VT1, typename VMASK_TYPE, int NUM_VT, int NUM_THREADS> 
++__global__ void silu_and_mul_mask_quant_pack_2mask(T* input, T* output,T1* mask, int grid_size, int64_t num_tokens, int64_t hidden_size, int64_t out_stirde)
++{
++    constexpr int N = sizeof(VT) / sizeof(T);
++    int const tid = threadIdx.x;
++    VMASK_TYPE vmask_reg = *(VMASK_TYPE*)mask;
++    T1 mask_stride[2];
++    T1* ptr_mask = (T1*)&vmask_reg;
++    mask_stride[0] = 0;
++    mask_stride[1] = ptr_mask[0];
++    int64_t hidden_size2 = hidden_size << 1;
++    int stride = NUM_THREADS * N;
++    int total_tokens = ptr_mask[0] + ptr_mask[1];
++
++    for(int idx = blockIdx.x; idx < total_tokens; idx += grid_size) {
++        float reg_i[NUM_VT][N];
++        int64_t token_id = idx < mask_stride[1] ? 0 : 1;;
++        int64_t const token_idx = token_id * num_tokens + idx - mask_stride[token_id];
++        const T* ptr_input0 = input + token_idx * hidden_size2;
++        const T* ptr_input1 = ptr_input0 + hidden_size;
++        float absmax_val = 0.0f;
++        for(int i = tid*N, j = 0; i < hidden_size; i += stride, j++) {
++            VT vsrc0, vsrc1;
++            vsrc0 = *(VT*)(ptr_input0 + i);
++            vsrc1 = *(VT*)(ptr_input1 + i);
++            T* ptr_local0 = (T*)&vsrc0;
++            T* ptr_local1 = (T*)&vsrc1;
++            #pragma unroll N
++            for(int k = 0; k < N; k++) {
++                float val0 = static_cast<float>(ptr_local0[k]);
++                float val1 = static_cast<float>(ptr_local1[k]);
++                float sigmoid = val0 * __builtin_mxc_rcpf(1.0f + __builtin_expf(-val0));
++                float gate_up = val1 * sigmoid;
++                reg_i[j][k] = gate_up;
++                absmax_val = max(absmax_val, abs(gate_up));
++            }
++        }
++
++        using BlockReduce = cub::BlockReduce<float, NUM_THREADS>;
++        __shared__ typename BlockReduce::TempStorage reduceStorage;
++        float const block_absmax_val_maybe =
++            BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, NUM_THREADS);
++        __shared__ float block_absmax_val;
++        int8_t* ptr_output = (int8_t*)(output + token_idx * out_stirde);
++        float* ptr_scale = (float*)(ptr_output + hidden_size);
++        if (tid == 0) {
++            block_absmax_val = block_absmax_val_maybe;
++            ptr_scale[0] = block_absmax_val * 0.0078740157;
++        }
++        __syncthreads();
++        float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++        for (int i = tid*N, k = 0; i < hidden_size; i += stride, k++) {
++            VT1 vdst;
++            int8_t* ptr_dst = (int8_t*)&vdst;
++            #pragma unroll N
++            for(int j = 0; j < N; ++j) {
++                ptr_dst[j] = float_to_int8_rn(reg_i[k][j] * tmp_scale);
++            }
++            *(VT1*)(ptr_output + i) = vdst;
++        }
 +    }
-+    *(VT1*)(ptr_output + index) = vdst;
-+  }
 +}
 +
-+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
-+__global__ void dynamic_scaled_int8_quant_kernel_sm_opt(
-+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-+    scale_type* scale, const int hidden_size, int blockDim_x) {
-+  int const tid = threadIdx.x;
-+  int const token_idx = blockIdx.x;
-+  float absmax_val = 0.0f;
-+  float const zero = 0.0f;
-+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
-+  int stride = blockDim_x * N;
-+  __shared__ float sm_buffer[8064];
-+  scalar_t const* ptr_input = input + token_idx * hidden_size;
-+  for(int i = tid * N; i < hidden_size; i += stride) {
-+    VT vsrc = *(VT*)(ptr_input + i);
-+    scalar_t *ptr_src = (scalar_t*)&vsrc;
-+    float* ptr_sm_buffer = sm_buffer + i;
-+    #pragma unroll N
-+    for(int j = 0; j < N; j++) {
-+        float val = static_cast<float>(ptr_src[j]);
-+        ptr_sm_buffer[j] = val;
-+        val = val > zero ? val : -val;
-+        absmax_val = val > absmax_val ? val : absmax_val;
++template<typename T, typename VT, typename VT1, int NUM_VT> 
++__global__ void silu_and_mul_quant(T* input, int8_t* output, float* scale, int64_t hidden_size, int blockDim_x)
++{
++    constexpr int N = sizeof(VT) / sizeof(T);
++    int const tid = threadIdx.x;
++    int stride = blockDim_x * N;
++    int64_t const token_idx = blockIdx.x;
++    int64_t hidden_size2 = hidden_size << 1;
++    const T* ptr_input0 = input + token_idx * hidden_size2;
++    const T* ptr_input1 = ptr_input0 + hidden_size;
++    float absmax_val = 0.0f;
++    float reg_i[NUM_VT][N];
++    for(int i = tid*N, j = 0; i < hidden_size; i += stride, j++) {
++        VT vsrc0, vsrc1;
++        vsrc0 = *(VT*)(ptr_input0 + i);
++        vsrc1 = *(VT*)(ptr_input1 + i);
++        T* ptr_local0 = (T*)&vsrc0;
++        T* ptr_local1 = (T*)&vsrc1;
++        #pragma unroll N
++        for(int k = 0; k < N; k++) {
++            float val0 = static_cast<float>(ptr_local0[k]);
++            float val1 = static_cast<float>(ptr_local1[k]);
++            float sigmoid = val0 * __builtin_mxc_rcpf(1.0f + __builtin_expf(-val0));
++            float gate_up = val1 * sigmoid;
++            reg_i[j][k] = gate_up;
++            absmax_val = max(absmax_val, abs(gate_up));
++        }
 +    }
-+  }
-+  using BlockReduce = cub::BlockReduce<float, 512>;
-+  __shared__ typename BlockReduce::TempStorage reduceStorage;
-+  float const block_absmax_val_maybe =
-+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
-+  __shared__ float block_absmax_val;
-+  if (tid == 0) {
-+    block_absmax_val = block_absmax_val_maybe;
-+    scale[token_idx] = block_absmax_val / 127.0f;
-+  }
-+  
-+  __syncthreads();
 +
-+  float const tmp_scale = 127.0f / block_absmax_val;
-+  int8_t* ptr_output = out + token_idx * hidden_size;
-+  for(int i = tid * N; i < hidden_size; i += stride) {
-+    VT1 vdst;
-+    int8_t* ptr_reg = (int8_t*)&vdst;
-+    float* ptr_sm_buffer = sm_buffer + i;
-+    #pragma unroll N
-+    for(int j = 0; j < N; j++) {
-+        ptr_reg[j] = float_to_int8_rn(
-+            ptr_sm_buffer[j] * tmp_scale);
++    using BlockReduce = cub::BlockReduce<float, 512>;
++    __shared__ typename BlockReduce::TempStorage reduceStorage;
++    float const block_absmax_val_maybe =
++        BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
++    __shared__ float block_absmax_val;
++    int8_t* ptr_output = (int8_t*)(output + token_idx * hidden_size);
++    if (tid == 0) {
++        block_absmax_val = block_absmax_val_maybe;
++        scale[token_idx] = block_absmax_val * 0.0078740157;
++    }
++    __syncthreads();
++    float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++    for (int i = tid*N, k = 0; i < hidden_size; i += stride, k++) {
++        VT1 vdst;
++        int8_t* ptr_dst = (int8_t*)&vdst;
++        #pragma unroll N
++        for(int j = 0; j < N; ++j) {
++            ptr_dst[j] = float_to_int8_rn(reg_i[k][j] * tmp_scale);
++        }
++        *(VT1*)(ptr_output + i) = vdst;
 +    }
-+    *(VT1*)(ptr_output + i) = vdst;
-+  }
 +}
 +
-+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
-+__launch_bounds__(1024) __global__ void dynamic_scaled_int8_quant_kernel_opt(
-+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-+    scale_type* scale, const int hidden_size, const int blockDim_x) {
-+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
-+  int const tid = threadIdx.x * N;
-+  int const token_idx = blockIdx.x;
-+  float absmax_val = 0.0f;
-+  int stride = blockDim_x * N;
-+  const scalar_t * ptr_input = input + token_idx * hidden_size;
++template<typename T, typename VT, typename VT1, int NUM_VT> 
++__global__ void silu_and_mul_sm_quant(T* input, int8_t* output, float* scale, int64_t hidden_size, int blockDim_x)
++{
++    constexpr int N = sizeof(VT) / sizeof(T);
++    int const tid = threadIdx.x;
++    int stride = blockDim_x * N;
++    int64_t const token_idx = blockIdx.x;
++    int64_t hidden_size2 = hidden_size << 1;
++    const T* ptr_input0 = input + token_idx * hidden_size2;
++    const T* ptr_input1 = ptr_input0 + hidden_size;
++    float absmax_val = 0.0f;
++    float reg_i[4][N];
++    __shared__ float sm_gate[4096];
++    int hidden_size1 = stride * 4;
++    for(int i = tid*N, j = 0; i < hidden_size1; i += stride, j++) {
++        VT vsrc0, vsrc1;
++        vsrc0 = *(VT*)(ptr_input0 + i);
++        vsrc1 = *(VT*)(ptr_input1 + i);
++        T* ptr_local0 = (T*)&vsrc0;
++        T* ptr_local1 = (T*)&vsrc1;
++        #pragma unroll N
++        for(int k = 0; k < N; k++) {
++            float val0 = static_cast<float>(ptr_local0[k]);
++            float val1 = static_cast<float>(ptr_local1[k]);
++            float sigmoid = val0 * __builtin_mxc_rcpf(1.0f + __builtin_expf(-val0));
++            float gate_up = val1 * sigmoid;
++            reg_i[j][k] = gate_up;
++            absmax_val = max(absmax_val, abs(gate_up));
++        }
++    }
++    const T* ptr_input2 = ptr_input0 + hidden_size1;
++    const T* ptr_input3 = ptr_input1 + hidden_size1;
++    int remain_hidden_size = hidden_size - hidden_size1;
++    for(int i = tid*N; i < remain_hidden_size; i += stride) {
++        VT vsrc0, vsrc1;
++        vsrc0 = *(VT*)(ptr_input2 + i);
++        vsrc1 = *(VT*)(ptr_input3 + i);
++        T* ptr_local0 = (T*)&vsrc0;
++        T* ptr_local1 = (T*)&vsrc1;
++        float* ptr_sm = sm_gate + i;
++        #pragma unroll N
++        for(int k = 0; k < N; k++) {
++            float val0 = static_cast<float>(ptr_local0[k]);
++            float val1 = static_cast<float>(ptr_local1[k]);
++            float sigmoid = val0 * __builtin_mxc_rcpf(1.0f + __builtin_expf(-val0));
++            float gate_up = val1 * sigmoid;
++            ptr_sm[k] = gate_up;
++            absmax_val = max(absmax_val, abs(gate_up));
++        }
++    }
 +
-+  for (int i = tid ; i < hidden_size; i += stride) {
-+    VT vsrc = *(VT*)(ptr_input + i);
-+    scalar_t *ptr_src = (scalar_t*)&vsrc;
-+    #pragma unroll N
-+    for(int j = 0; j < N; j++) {
-+        float val = static_cast<float>(ptr_src[j]);
-+        val = val > 0 ? val : -val;
-+        absmax_val = val > absmax_val ? val : absmax_val;
++    using BlockReduce = cub::BlockReduce<float, 512>;
++    __shared__ typename BlockReduce::TempStorage reduceStorage;
++    float const block_absmax_val_maybe =
++        BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
++    __shared__ float block_absmax_val;
++    int8_t* ptr_output = (int8_t*)(output + token_idx * hidden_size);
++    if (tid == 0) {
++        block_absmax_val = block_absmax_val_maybe;
++        scale[token_idx] = block_absmax_val * 0.0078740157;
++    }
++    __syncthreads();
++    float const tmp_scale = 127.0f * __builtin_mxc_rcpf(block_absmax_val);
++    for (int i = tid*N, k = 0; i < hidden_size1; i += stride, k++) {
++        VT1 vdst;
++        int8_t* ptr_dst = (int8_t*)&vdst;
++        #pragma unroll N
++        for(int j = 0; j < N; ++j) {
++            ptr_dst[j] = float_to_int8_rn(reg_i[k][j] * tmp_scale);
++        }
++        *(VT1*)(ptr_output + i) = vdst;
 +    }
-+  }
 +
-+    using BlockReduce = cub::BlockReduce<float, 1024>;
-+  __shared__ typename BlockReduce::TempStorage reduceStorage;
-+  float const block_absmax_val_maybe =
-+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
-+  __shared__ float block_absmax_val;
-+  if (tid == 0) {
-+    block_absmax_val = block_absmax_val_maybe;
-+    scale[token_idx] = block_absmax_val / 127.0f;
-+  }
-+  __syncthreads();
++    ptr_output = ptr_output + hidden_size1;
++    for(int i = tid*N; i < remain_hidden_size; i += stride) {
++        VT1 vdst;
++        int8_t* ptr_dst = (int8_t*)&vdst;
++        float* ptr_sm = sm_gate + i;
++        #pragma unroll N
++        for(int j = 0; j < N; ++j) {
++            ptr_dst[j] = float_to_int8_rn(ptr_sm[j] * tmp_scale);
++        }
++        *(VT1*)(ptr_output + i) = vdst;
++    }
++}
 +
-+  float const tmp_scale = 127.0f / block_absmax_val;
-+  int8_t* ptr_output = out + token_idx * hidden_size;
-+  for (int i = tid; i < hidden_size; i += stride) {
-+    VT vsrc = *(VT*)(ptr_input + i);
-+    VT1 vdst;
-+    scalar_t *ptr_src = (scalar_t*)&vsrc;
-+    int8_t* ptr_dst = (int8_t*)&vdst;
-+    #pragma unroll N
-+    for(int j = 0; j < N; ++j) {
-+        ptr_dst[j] = float_to_int8_rn(
-+        static_cast<float>(ptr_src[j]) * tmp_scale);
++template<typename T, typename T1>
++void launch_silu_mul_quant_pack(T* input, T* output, T1* mask, int64_t num_tokens, int64_t hidden_size, int64_t out_stride, int64_t mask_size,cudaStream_t stream) {
++    int dev = 0;
++    cudaGetDevice(&dev);
++    int sm_count = 0;
++    cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, dev);
++    int gridsize = sm_count;
++    int64_t inner_hidden_size = hidden_size / 2;
++    int blocksize = 512;
++    int N = sizeof(float4) / sizeof(T);
++    if(mask_size == 1 && N == 8&&(inner_hidden_size & (N - 1)) == 0 && (out_stride & (N -1)) == 0) {
++        int base = blocksize * N;
++        if(inner_hidden_size <= 64*N) {
++          gridsize = gridsize * 8;
++          silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 1, 64><<<gridsize, 64,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else if(inner_hidden_size <= 128*N) {
++          gridsize = gridsize * 4;
++          silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 1, 128><<<gridsize, 128,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else if(inner_hidden_size <= 256*N) {
++          gridsize = gridsize * 2;
++          silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 1, 256><<<gridsize, 256,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else if(inner_hidden_size <= base) {
++            silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 1, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else if(inner_hidden_size <= base*2) {
++            silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 2, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else if(inner_hidden_size <= base * 3) {
++            silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 3, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else if(inner_hidden_size <= base * 4) {
++            silu_and_mul_mask_quant_pack_1mask<T, T1, float4, float2, 4, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++        } else {
++            assert(0);
++        }
++    } else if(mask_size == 2 && (N == 8&&(inner_hidden_size & (N - 1)) == 0 && (out_stride & (N -1)) == 0)) {
++        int base = blocksize * N;
++        if(sizeof(T1) == 4) {
++          if(inner_hidden_size <= 64 * N) {
++            gridsize = gridsize * 8;
++            silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 1, 64><<<gridsize, 64,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= 128 * N){
++            gridsize = gridsize * 4;
++            silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 1, 128><<<gridsize, 128,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= 256 * N) {
++            gridsize = gridsize * 2;
++            silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 1, 256><<<gridsize, 256,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 1, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base*2) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 2, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base * 3) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 3, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base * 4) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float2, 4, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else {
++              assert(0);
++          }
++        } else if(sizeof(T1) == 8) {
++          if(inner_hidden_size <= 64 * N) {
++            gridsize = gridsize * 8;
++            silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 1, 64><<<gridsize, 64,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= 128 * N) {
++            gridsize = gridsize * 4;
++            silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 1, 128><<<gridsize, 128,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= 256 * N) {
++            gridsize = gridsize * 2;
++            silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 1, 256><<<gridsize, 256,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 1, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base*2) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 2, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base * 3) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 3,512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else if(inner_hidden_size <= base * 4) {
++              silu_and_mul_mask_quant_pack_2mask<T, T1, float4, float2, float4, 4, 512><<<gridsize, 512,0,stream>>>(input, output, mask, gridsize, num_tokens, inner_hidden_size, out_stride);
++          } else {
++              assert(0);
++          }
++        }
++        
++    } else if(N == 8&&(inner_hidden_size & (N - 1)) == 0 && (out_stride & (N -1)) == 0) {
++        int base = blocksize * N;
++        if(inner_hidden_size <= base) {
++            silu_and_mul_mask_quant_pack<T, T1, float4, float2, 1><<<gridsize, blocksize,0,stream>>>(input, output, mask, mask_size, gridsize, num_tokens, inner_hidden_size, out_stride, blocksize);
++        } else if(inner_hidden_size <= base*2) {
++            silu_and_mul_mask_quant_pack<T, T1, float4, float2, 2><<<gridsize, blocksize,0,stream>>>(input, output, mask, mask_size, gridsize, num_tokens, inner_hidden_size, out_stride, blocksize);
++        } else if(inner_hidden_size <= base * 3) {
++            silu_and_mul_mask_quant_pack<T, T1, float4, float2, 3><<<gridsize, blocksize,0,stream>>>(input, output, mask, mask_size, gridsize, num_tokens, inner_hidden_size, out_stride, blocksize);
++        } else if(inner_hidden_size <= base * 4) {
++            silu_and_mul_mask_quant_pack<T, T1, float4, float2, 4><<<gridsize, blocksize,0,stream>>>(input, output, mask, mask_size, gridsize, num_tokens, inner_hidden_size, out_stride, blocksize);
++        } else {
++            assert(0);
++        }
++    } else {
++        assert(0);
++    }
++}
++
++template<typename T>
++void launch_silu_mul_quan_no_mask(T* input, int8_t* output, float* scale, int64_t num_tokens, int64_t hidden_size,cudaStream_t stream) {
++    int64_t inner_hidden_size = hidden_size / 2;
++    int blocksize = 512;
++    int N = sizeof(float4) / sizeof(T);
++    if(N == 8&&(inner_hidden_size & (N - 1)) == 0) {
++        int base = blocksize * N;
++        if(inner_hidden_size <= base) {
++            silu_and_mul_quant<T, float4, float2, 1><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base*2) {
++            silu_and_mul_quant<T, float4, float2, 2><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base * 3) {
++            silu_and_mul_quant<T, float4, float2, 3><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base * 4) {
++            silu_and_mul_quant<T, float4, float2, 4><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base*4 + 4096) {
++            silu_and_mul_sm_quant<T, float4, float2, 4><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else {
++            printf("silu_and_mul_quant not support\n");
++            assert(0);
++        }
++    } else if(N == 4 && (inner_hidden_size & (N - 1)) == 0) {
++        int base = blocksize * N;
++        if(inner_hidden_size <= base) {
++            silu_and_mul_quant<T, float4, float, 1><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base*2) {
++            silu_and_mul_quant<T, float4, float, 2><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base * 3) {
++            silu_and_mul_quant<T, float4, float, 3><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base * 4) {
++            silu_and_mul_quant<T, float4, float, 4><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else if(inner_hidden_size <= base * 8) {
++            silu_and_mul_quant<T, float4, float, 8><<<num_tokens, blocksize,0,stream>>>(input, output, scale, inner_hidden_size, blocksize);
++        } else {
++            printf("silu_and_mul_quant not support\n");
++            assert(0);
++        }
++    } else {
++        assert(0);
 +    }
-+    *(VT1*)(ptr_output + i) = vdst;
-   }
- }
- 
-@@ -169,17 +374,13 @@ template <typename scalar_t, typename scale_type, typename azp_type>
- __global__ void dynamic_scaled_int8_azp_quant_kernel(
-     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
-     scale_type* scale, azp_type* azp, const int hidden_size) {
--  int64_t const token_idx = blockIdx.x;
--
--  // Must be performed using 64-bit math to avoid integer overflow.
--  out += token_idx * hidden_size;
--  input += token_idx * hidden_size;
-+  int const token_idx = blockIdx.x;
- 
-   // Scan for the min and max value for this token
-   float max_val = std::numeric_limits<float>::min();
-   float min_val = std::numeric_limits<float>::max();
-   for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
--    auto val = static_cast<float>(input[i]);
-+    auto val = static_cast<float>(input[token_idx * hidden_size + i]);
-     max_val = std::max(max_val, val);
-     min_val = std::min(min_val, val);
-   }
-@@ -214,10 +415,10 @@ __global__ void dynamic_scaled_int8_azp_quant_kernel(
- 
-   // Quantize the values
-   for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
--    auto const val = static_cast<float>(input[i]);
-+    auto const val = static_cast<float>(input[token_idx * hidden_size + i]);
-     auto const quant_val =
-         int32_to_int8(float_to_int32_rn(val / scale_val) + azp_val);
--    out[i] = quant_val;
-+    out[token_idx * hidden_size + i] = quant_val;
-   }
  }
  
-@@ -226,7 +427,7 @@ __global__ void dynamic_scaled_int8_azp_quant_kernel(
+ }  // namespace vllm
+@@ -238,7 +1372,7 @@ __global__ void dynamic_scaled_int8_azp_quant_kernel(
  void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
                                torch::Tensor const& input,  // [..., hidden_size]
                                torch::Tensor const& scale,
@@ -8859,7 +8546,7 @@ index e79785827..9bf453e3b 100644
    TORCH_CHECK(input.is_contiguous());
    TORCH_CHECK(out.is_contiguous());
    TORCH_CHECK(scale.numel() == 1);
-@@ -257,7 +458,7 @@ void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
+@@ -269,7 +1403,7 @@ void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
  void dynamic_scaled_int8_quant(
      torch::Tensor& out,          // [..., hidden_size]
      torch::Tensor const& input,  // [..., hidden_size]
@@ -8868,110 +8555,276 @@ index e79785827..9bf453e3b 100644
    TORCH_CHECK(input.is_contiguous());
    TORCH_CHECK(out.is_contiguous());
    TORCH_CHECK(scales.is_contiguous());
-@@ -271,10 +472,30 @@ void dynamic_scaled_int8_quant(
+@@ -277,22 +1411,184 @@ void dynamic_scaled_int8_quant(
+ 
+   int const hidden_size = input.size(-1);
+   int const num_tokens = input.numel() / hidden_size;
+-  dim3 const grid(num_tokens);
++  dim3 const grid(num_tokens,1,1);
+   dim3 const block(std::min(hidden_size, 1024));
+   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    VLLM_DISPATCH_FLOATING_TYPES(
        input.scalar_type(), "dynamic_scaled_int8_quant_kernel", [&] {
          if (!azp) {
 -          vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>
--              <<<grid, block, 0, stream>>>(
--                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
--                  scales.data_ptr<float>(), hidden_size);
 +          int n = 16 / sizeof(scalar_t);
 +          if(hidden_size <= 4096 && ((hidden_size & (n - 1)) == 0) && n == 8) {
-+            int gridsize = num_tokens;
-+            int blocksize = 512;
-+            vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
++            if(hidden_size > 256*n) {
++              vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2, 512, false><<<grid, 512, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens);
++            } else if(hidden_size > 128*n) {
++              vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2, 256, false><<<grid, 256, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens);
++            } else if(hidden_size > 64 * n) {
++              vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2, 128, false><<<grid, 128, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens);
++            } else {
++              vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2, 64, false><<<grid, 64, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens);
++            }
 +          } else if(hidden_size > 4096 &&hidden_size <= 8192 && ((hidden_size & (2*n - 1)) == 0) && n == 8) {
-+            int gridsize = num_tokens; int blocksize = 512;
-+            vllm::dynamic_scaled_int8_quant_kernel_reg_opt<scalar_t, float, float4, float4><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
++            int blocksize = 512;
++            vllm::dynamic_scaled_int8_quant_kernel_reg_opt<scalar_t, float, float4, float4, false><<<grid, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize, num_tokens);
 +          } else if(hidden_size <= 8064 && (hidden_size & (n - 1)) == 0 && n == 8) {
-+            int gridsize = num_tokens;
 +            int blocksize = 512;
-+            vllm::dynamic_scaled_int8_quant_kernel_sm_opt<scalar_t, float, float4, float2><<<gridsize, blocksize, 0, stream>>>(
-+              input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
++            vllm::dynamic_scaled_int8_quant_kernel_sm_opt<scalar_t, float, float4, float2,false><<<grid, blocksize, 0, stream>>>(
++              input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize,num_tokens);
++          } else if(hidden_size >= 16384 && hidden_size <= 18432 && (hidden_size & (n - 1)) == 0 && n == 8) {
++            vllm::dynamic_scaled_int8_quant_kernel_lh_opt<scalar_t, float, float4, float2, 3,1024, false><<<grid, 1024, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens);
 +          } else if (hidden_size > 8064 && ((hidden_size & (n - 1)) == 0 && n == 8)) {
 +            int blocksize = 1024;
-+            vllm::dynamic_scaled_int8_quant_kernel_opt<scalar_t, float,float4,float2>
++            vllm::dynamic_scaled_int8_quant_kernel_opt<scalar_t, float,float4,float2, false>
 +                    <<<grid, blocksize, 0, stream>>>(
-+                        input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
++                        input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize, num_tokens);
 +          } else {
-+              vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>
++              vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float, false>
 +                  <<<grid, block, 0, stream>>>(
 +                      input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
-+                      scales.data_ptr<float>(), hidden_size);
++                      scales.data_ptr<float>(), hidden_size, num_tokens);
++          }
++        } else {
++          vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t, false>
+               <<<grid, block, 0, stream>>>(
+                   input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+-                  scales.data_ptr<float>(), hidden_size);
++                  scales.data_ptr<float>(), azp->data_ptr<int32_t>(),
++                  hidden_size, num_tokens);
++        }
++      });
++}
++
++void dynamic_scaled_int8_mask_quant(
++    torch::Tensor& out,          // [..., hidden_size]
++    torch::Tensor const& input,  // [..., hidden_size]
++    torch::Tensor const &mask,
++    torch::Tensor& scales, c10::optional<torch::Tensor> const& azp) {
++  TORCH_CHECK(input.is_contiguous());
++  TORCH_CHECK(out.is_contiguous());
++  TORCH_CHECK(scales.is_contiguous());
++  TORCH_CHECK(mask.is_contiguous());
++  TORCH_CHECK(!azp || azp->is_contiguous());
++
++  int const hidden_size = input.size(-1);
++  int const num_tokens = input.numel() / hidden_size;
++  int const mask_size = mask.numel();
++  int const num_tokens_batch = num_tokens / mask_size;
++  dim3 const grid(num_tokens_batch, mask_size, 1);
++  dim3 const block(std::min(hidden_size, 1024));
++  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
++  VLLM_DISPATCH_FLOATING_TYPES(
++      input.scalar_type(), "dynamic_scaled_int8_quant_kernel_mask", [&] {
++        if (!azp) {
++          int n = 16 / sizeof(scalar_t);
++          if(mask_size < 16) {
++            int dev = 0;
++            cudaGetDevice(&dev);
++            int sm_count = 0;
++            cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, dev);
++            int gridsize = sm_count;
++            if(hidden_size <= 4096 && ((hidden_size & (n - 1)) == 0) && n == 8) {
++              int blocksize = 512;
++              vllm::dynamic_scaled_int8_quant_mask_kernel_sreg_opt<scalar_t, float, float4, float2><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize,num_tokens_batch,mask_size,gridsize,mask.data_ptr<int>());
++            } else if(hidden_size > 4096 &&hidden_size <= 8192 && ((hidden_size & (2*n - 1)) == 0) && n == 8) {
++                int blocksize = 512;
++              vllm::dynamic_scaled_int8_quant_mask_kernel_reg_opt<scalar_t, float, float4, float4><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize,num_tokens_batch,mask_size, gridsize,mask.data_ptr<int>());
++            } else if(hidden_size <= 8064 && (hidden_size & (n - 1)) == 0 && n == 8) {
++                int blocksize = 512;
++                vllm::dynamic_scaled_int8_quant_mask_kernel_sm_opt<scalar_t, float, float4, float2><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize,num_tokens_batch,mask_size, gridsize, mask.data_ptr<int>());
++            } else if(hidden_size > 8064 && ((hidden_size & (n - 1)) == 0 && n == 8)){
++                int blocksize = 1024;
++                vllm::dynamic_scaled_int8_quant_mask_kernel_opt<scalar_t, float,float4,float2>
++                        <<<gridsize, blocksize, 0, stream>>>(
++                            input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize,num_tokens_batch,mask_size, gridsize, mask.data_ptr<int>());
++            } else {
++              vllm::dynamic_scaled_int8_quant_mask_kernel<scalar_t, float>
++                    <<<gridsize, block, 0, stream>>>(
++                        input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
++                        scales.data_ptr<float>(), hidden_size, num_tokens_batch, mask_size, gridsize, mask.data_ptr<int>());
++            }
++          } else {
++            if(hidden_size <= 4096 && ((hidden_size & (n - 1)) == 0) && n == 8) {
++              if(hidden_size > 256*n) {
++                vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2,512,true><<<grid, 512, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens_batch, mask.data_ptr<int>());
++              } else if(hidden_size > 128*n) {
++                vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2,256,true><<<grid, 256, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens_batch, mask.data_ptr<int>());
++              } else if(hidden_size > 64*n) {
++                vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2,128,true><<<grid, 128, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens_batch, mask.data_ptr<int>());
++              } else {
++                vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2,64,true><<<grid, 64, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size, num_tokens_batch, mask.data_ptr<int>());
++              }
++            } else if(hidden_size > 4096 &&hidden_size <= 8192 && ((hidden_size & (2*n - 1)) == 0) && n == 8) {
++              int blocksize = 512;
++              vllm::dynamic_scaled_int8_quant_kernel_reg_opt<scalar_t, float, float4, float4, true><<<grid, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize,num_tokens_batch,mask.data_ptr<int>());
++            } else if(hidden_size <= 8064 && (hidden_size & (n - 1)) == 0 && n == 8) {
++              int blocksize = 512;
++              vllm::dynamic_scaled_int8_quant_kernel_sm_opt<scalar_t, float, float4, float2, true><<<grid, blocksize, 0, stream>>>(
++                input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize, num_tokens_batch,mask.data_ptr<int>());
++            } else if (hidden_size > 8064 && ((hidden_size & (n - 1)) == 0 && n == 8)) {
++              int blocksize = 1024;
++              vllm::dynamic_scaled_int8_quant_kernel_opt<scalar_t, float,float4,float2, true>
++                      <<<grid, blocksize, 0, stream>>>(
++                          input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize, num_tokens_batch, mask.data_ptr<int>());
++            } else {
++                vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float,true>
++                    <<<grid, block, 0, stream>>>(
++                        input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
++                        scales.data_ptr<float>(), hidden_size, num_tokens_batch, mask.data_ptr<int>());
++            }
 +          }
          } else {
-           vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t>
+-          vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t>
++          vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t,true>
                <<<grid, block, 0, stream>>>(
+                   input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+                   scales.data_ptr<float>(), azp->data_ptr<int32_t>(),
+-                  hidden_size);
++                  hidden_size, num_tokens_batch, mask.data_ptr<int>());
+         }
+       });
+ }
++
++
++void fused_silu_mul_dq_mask_quant_pack(
++    torch::Tensor& out,          
++    torch::Tensor const& input, 
++    torch::Tensor const &mask)
++{
++  TORCH_CHECK(input.is_contiguous());
++  TORCH_CHECK(out.is_contiguous());
++  TORCH_CHECK(mask.is_contiguous());
++  int64_t const hidden_size = input.size(-1);
++  int64_t const num_tokens = input.numel() / hidden_size;
++  int64_t const mask_size = mask.numel();
++  int64_t const num_tokens_batch = num_tokens / mask_size;
++  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
++  int64_t out_stride = ((hidden_size/4 + 2) + 255)/ 256 * 256;
++  switch(mask.element_size()) {
++    case 8:
++      VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), "launch_silu_mul_quant_pack", [&] {
++        vllm::launch_silu_mul_quant_pack<scalar_t, int64_t>(input.data_ptr<scalar_t>(), out.data_ptr<scalar_t>(), mask.data_ptr<int64_t>(), num_tokens_batch, hidden_size, out_stride, mask_size, stream);
++      });
++    break;
++    case 4:
++       VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), "launch_silu_mul_quant_pack", [&] {
++        vllm::launch_silu_mul_quant_pack<scalar_t, int32_t>(input.data_ptr<scalar_t>(), out.data_ptr<scalar_t>(), mask.data_ptr<int32_t>(), num_tokens_batch, hidden_size, out_stride, mask_size, stream);
++      });
++      break;
++    default:
++    return;
++  }
++}
++
++void fused_silu_mul_dq_quant_interface(
++    torch::Tensor& out,
++    torch::Tensor& scale,   
++    torch::Tensor const& input)
++{
++  TORCH_CHECK(input.is_contiguous());
++  TORCH_CHECK(scale.is_contiguous());
++  TORCH_CHECK(out.is_contiguous());
++  int64_t const hidden_size = input.size(-1);
++  int64_t const num_tokens = input.numel() / hidden_size;
++  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
++  VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), "launch_silu_mul_quan_no_mask", [&] {
++    vllm::launch_silu_mul_quan_no_mask<scalar_t>(input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(), scale.data_ptr<float>(), num_tokens, hidden_size, stream);
++  });
++}
+\ No newline at end of file
 diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
-index 865fef5ae..835f319b3 100644
+index 865fef5ae..95065bcef 100644
 --- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
 +++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
-@@ -1,14 +1,10 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -1,14 +1,21 @@
  #include <stddef.h>
  #include <torch/all.h>
--#include "cutlass/cutlass.h"
++#ifdef USE_MACA
 +#include "mctlass/mctlass.h"
++#include "mctlass/epilogue/thread/scale_type.h"
++#else
+ #include "cutlass/cutlass.h"
++#endif
  
  #include "scaled_mm_c2x.cuh"
  #include "scaled_mm_c2x_sm75_dispatch.cuh"
--#include "scaled_mm_c2x_sm80_dispatch.cuh"
--#include "scaled_mm_c2x_sm89_fp8_dispatch.cuh"
--#include "scaled_mm_c2x_sm89_int8_dispatch.cuh"
--
--#include "cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp"
++#ifndef USE_MACA
+ #include "scaled_mm_c2x_sm80_dispatch.cuh"
+ #include "scaled_mm_c2x_sm89_fp8_dispatch.cuh"
+ #include "scaled_mm_c2x_sm89_int8_dispatch.cuh"
+ 
+ #include "cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp"
++#endif
  
  using namespace vllm;
  
-@@ -22,6 +18,7 @@ template <template <typename, typename> typename Epilogue,
+@@ -22,6 +29,7 @@ template <template <typename, typename> typename Epilogue,
  void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
                                       torch::Tensor const& b,
                                       EpilogueArgs&&... epilogue_args) {
-+#if 0
++#ifndef USE_MACA
    TORCH_CHECK(a.dtype() == torch::kInt8);
    TORCH_CHECK(b.dtype() == torch::kInt8);
  
-@@ -33,26 +30,196 @@ void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
+@@ -33,6 +41,7 @@ void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
      return cutlass_gemm_sm75_dispatch<int8_t, cutlass::half_t, Epilogue>(
          out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);
    }
-+#endif
++#endif // USE_MACA
  }
  
  void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
-                             torch::Tensor const& b,
+@@ -40,6 +49,7 @@ void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
                              torch::Tensor const& a_scales,
                              torch::Tensor const& b_scales,
--                            std::optional<torch::Tensor> const& bias) {
--  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
--  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
--  if (bias) {
--    TORCH_CHECK(bias->dtype() == out.dtype(),
--                "currently bias dtype must match output dtype ", out.dtype());
--    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBias>(
--        out, a, b, a_scales, b_scales, *bias);
--  } else {
--    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogue>(
--        out, a, b, a_scales, b_scales);
--  }
-+                            c10::optional<torch::Tensor> const& bias) {
-+    int32_t m = a.size(0);
-+    int32_t n = b.size(1);
-+    int32_t k = a.size(1);
-+
-+    using ArchTag = mctlass::arch::Sm80;
-+    using ElementA = int8_t;
-+    using ElementB = int8_t;
-+    using ElementC = mctlass::half_t;
-+    using ElementCompute = float;
-+    using LayoutA = mctlass::layout::RowMajor;
-+    //using LayoutB = mctlass::layout::RowMajor;
-+    using LayoutB = mctlass::layout::ColumnMajor;
-+    using LayoutC = mctlass::layout::RowMajor;
+                             std::optional<torch::Tensor> const& bias) {
++#ifndef USE_MACA
+   TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
+   TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
+   if (bias) {
+@@ -51,6 +61,195 @@ void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
+     return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogue>(
+         out, a, b, a_scales, b_scales);
+   }
++#else
++  int32_t m = a.size(0);
++  int32_t n = b.size(1);
++  int32_t k = a.size(1);
++  int32_t batch_count = 1;
++  if (a.dim() == 3 && b.dim() == 3) {
++      // a.size = [batch_size, M, K], b.size = [batch_size, K, N]
++      m = a.size(1);
++      n = b.size(2);
++      k = a.size(2);
++      batch_count = a.size(0);
++  }
 +
-+    if (out.dtype() == torch::kBFloat16)
-+    {
++  using ArchTag = mctlass::arch::Sm80;
++  using ElementA = int8_t;
++  using ElementB = int8_t;
++  using ElementC = mctlass::half_t;
++  using ElementCompute = float;
++  using LayoutA = mctlass::layout::RowMajor;
++  //using LayoutB = mctlass::layout::RowMajor;
++  using LayoutB = mctlass::layout::ColumnMajor;
++  using LayoutC = mctlass::layout::RowMajor;
++
++  if (out.dtype() == torch::kBFloat16)
++  {
 +    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
 +    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
 +    auto c_ptr = static_cast<maca_bfloat16*>(out.data_ptr());
@@ -8979,81 +8832,81 @@ index 865fef5ae..835f319b3 100644
 +    auto scale_b = b_scales.data_ptr<float>();
 +    auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
 +    if (bias) {
-+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
-+        mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
-+        using mctlassGemmScaleOp = mctlassGemmScale<
-+          ElementA,
-+          LayoutA,
-+          ElementB,
-+          LayoutB,
-+          maca_bfloat16,
-+          LayoutC,
-+          ElementCompute,
-+          ArchTag,
-+          scale_type
-+        >;
-+        maca_bfloat16 *bias_t;
-+        bias_t = static_cast<maca_bfloat16 *>(bias.value().data_ptr());
-+        mctlassGemmScaleOp mctlass_op;
-+        mctlass::gemm::GemmCoord problem_size(m, n, k);
-+        typename mctlassGemmScaleOp::Arguments arguments{
-+            mctlass::gemm::GemmUniversalMode::kGemm,
-+            problem_size,
-+            1,//batch_count
-+            {scale_a, scale_b, bias_t},
-+            a_ptr,
-+            b_ptr,
-+            c_ptr,
-+            c_ptr,
-+            m * k,
-+            n * k,
-+            m * n,
-+            m * n,
-+            k,
-+            n,
-+            n,
-+            n
-+        };
-+        mctlass_op(arguments, NULL, stream);
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++      mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
++      using mctlassGemmScaleOp = mctlassGemmScale<
++        ElementA,
++        LayoutA,
++        ElementB,
++        LayoutB,
++        maca_bfloat16,
++        LayoutC,
++        ElementCompute,
++        ArchTag,
++        scale_type
++      >;
++      maca_bfloat16 *bias_t;
++      bias_t = static_cast<maca_bfloat16 *>(bias.value().data_ptr());
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          1,//batch_count
++          {scale_a, scale_b, bias_t},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
 +    }
 +    else{
-+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
-+        mctlass::epilogue::thread::ScaleType::ScaleAvBv;
-+        using mctlassGemmScaleOp = mctlassGemmScale<
-+          ElementA,
-+          LayoutA,
-+          ElementB,
-+          LayoutB,
-+          maca_bfloat16,
-+          LayoutC,
-+          ElementCompute,
-+          ArchTag,
-+          scale_type
-+        >;
-+        mctlassGemmScaleOp mctlass_op;
-+        mctlass::gemm::GemmCoord problem_size(m, n, k);
-+        typename mctlassGemmScaleOp::Arguments arguments{
-+            mctlass::gemm::GemmUniversalMode::kGemm,
-+            problem_size,
-+            1,//batch_count
-+            {scale_a, scale_b, nullptr},
-+            a_ptr,
-+            b_ptr,
-+            c_ptr,
-+            c_ptr,
-+            m * k,
-+            n * k,
-+            m * n,
-+            m * n,
-+            k,
-+            n,
-+            n,
-+            n
-+        };
-+        mctlass_op(arguments, NULL, stream);
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++      mctlass::epilogue::thread::ScaleType::ScaleAvBv;
++      using mctlassGemmScaleOp = mctlassGemmScale<
++        ElementA,
++        LayoutA,
++        ElementB,
++        LayoutB,
++        maca_bfloat16,
++        LayoutC,
++        ElementCompute,
++        ArchTag,
++        scale_type
++      >;
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          1,//batch_count
++          {scale_a, scale_b, nullptr},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
 +    }
-+    }
-+    else{
++  }
++  else{
 +    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
 +    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
 +    auto c_ptr = static_cast<ElementC*>(out.data_ptr());
@@ -9061,249 +8914,388 @@ index 865fef5ae..835f319b3 100644
 +    auto scale_b = b_scales.data_ptr<float>();
 +    auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
 +    if (bias) {
-+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
-+        mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
-+        using mctlassGemmScaleOp = mctlassGemmScale<
-+          ElementA,
-+          LayoutA,
-+          ElementB,
-+          LayoutB,
-+          ElementC,
-+          LayoutC,
-+          ElementCompute,
-+          ArchTag,
-+          scale_type
-+        >;
-+        ElementC *bias_t;
-+        bias_t = static_cast<ElementC *>(bias.value().data_ptr());
-+        mctlassGemmScaleOp mctlass_op;
-+        mctlass::gemm::GemmCoord problem_size(m, n, k);
-+        typename mctlassGemmScaleOp::Arguments arguments{
-+            mctlass::gemm::GemmUniversalMode::kGemm,
-+            problem_size,
-+            1,//batch_count
-+            {scale_a, scale_b, bias_t},
-+            a_ptr,
-+            b_ptr,
-+            c_ptr,
-+            c_ptr,
-+            m * k,
-+            n * k,
-+            m * n,
-+            m * n,
-+            k,
-+            n,
-+            n,
-+            n
-+        };
-+        mctlass_op(arguments, NULL, stream);
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++      mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
++      using mctlassGemmScaleOp = mctlassGemmScale<
++        ElementA,
++        LayoutA,
++        ElementB,
++        LayoutB,
++        ElementC,
++        LayoutC,
++        ElementCompute,
++        ArchTag,
++        scale_type
++      >;
++      ElementC *bias_t;
++      bias_t = static_cast<ElementC *>(bias.value().data_ptr());
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          1,//batch_count
++          {scale_a, scale_b, bias_t},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
 +    }
 +    else{
-+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
-+        mctlass::epilogue::thread::ScaleType::ScaleAvBv;
-+        using mctlassGemmScaleOp = mctlassGemmScale<
-+          ElementA,
-+          LayoutA,
-+          ElementB,
-+          LayoutB,
-+          ElementC,
-+          LayoutC,
-+          ElementCompute,
-+          ArchTag,
-+          scale_type
-+        >;
-+        mctlassGemmScaleOp mctlass_op;
-+        mctlass::gemm::GemmCoord problem_size(m, n, k);
-+        typename mctlassGemmScaleOp::Arguments arguments{
-+            mctlass::gemm::GemmUniversalMode::kGemm,
-+            problem_size,
-+            1,//batch_count
-+            {scale_a, scale_b, nullptr},
-+            a_ptr,
-+            b_ptr,
-+            c_ptr,
-+            c_ptr,
-+            m * k,
-+            n * k,
-+            m * n,
-+            m * n,
-+            k,
-+            n,
-+            n,
-+            n
-+        };
-+        mctlass_op(arguments, NULL, stream);
-+    }
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++      mctlass::epilogue::thread::ScaleType::ScaleAvBv;
++      using mctlassGemmScaleOp = mctlassGemmScale<
++        ElementA,
++        LayoutA,
++        ElementB,
++        LayoutB,
++        ElementC,
++        LayoutC,
++        ElementCompute,
++        ArchTag,
++        scale_type
++      >;
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          1,//batch_count
++          {scale_a, scale_b, nullptr},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
 +    }
++  }
++#endif // USE_MACA
  }
  
-+#if 0
  void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
-                                 torch::Tensor const& b,
-                                 torch::Tensor const& a_scales,
-@@ -165,7 +332,7 @@ void cutlass_scaled_mm_sm89(torch::Tensor& out, torch::Tensor const& a,
-                             torch::Tensor const& b,
-                             torch::Tensor const& a_scales,
-                             torch::Tensor const& b_scales,
--                            std::optional<torch::Tensor> const& bias) {
-+                            c10::optional<torch::Tensor> const& bias) {
+@@ -60,6 +259,7 @@ void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
+                                 torch::Tensor const& azp_adj,
+                                 std::optional<torch::Tensor> const& azp,
+                                 std::optional<torch::Tensor> const& bias) {
++#ifndef USE_MACA
    TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
    TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
-   if (bias) {
-@@ -178,14 +345,15 @@ void cutlass_scaled_mm_sm89(torch::Tensor& out, torch::Tensor const& a,
-         out, a, b, a_scales, b_scales);
+ 
+@@ -70,8 +270,178 @@ void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
+     return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBiasAzp>(
+         out, a, b, a_scales, b_scales, azp_adj, bias);
    }
++#elif (MACA_VERSION_MAJOR * 100 + MACA_VERSION_MINOR) >= 231 // MACA version >= 2.31.0.x
++  int32_t m = a.size(0);
++  int32_t n = b.size(1);
++  int32_t k = a.size(1);
++  int32_t batchsize = 1;
++  if (a.dim() == 3 && b.dim() == 3) {
++    // a.size = [batch_size, M, K], b.size = [batch_size, K, N]
++    m = a.size(1);
++    n = b.size(2);
++    k = a.size(2);
++    batchsize = a.size(0);
++  }
++
++  using ArchTag = mctlass::arch::Sm80;
++  using ElementA = int8_t;
++  using ElementB = int8_t;
++
++  using ElementCompute = float;
++  using ElementAccumulator = int32_t;
++
++  using LayoutA = mctlass::layout::RowMajor;
++  using LayoutB = mctlass::layout::ColumnMajor;
++  using LayoutC = mctlass::layout::RowMajor;
++
++  auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
++
++  if (out.dtype() == torch::kBFloat16) {
++    using ElementC = maca_bfloat16;
++    using ElementOutput = ElementC;
++
++    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
++    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
++    auto c_ptr = static_cast<ElementC*>(out.data_ptr());
++    auto scale_a = a_scales.data_ptr<float>();
++    auto scale_b = b_scales.data_ptr<float>();
++
++    ElementAccumulator* azp_ptr = NULL;
++    auto azp_adj_ptr = azp_adj.data_ptr<ElementAccumulator>();
++    ElementOutput* bias_t = static_cast<ElementOutput*>(bias.value().data_ptr());
++
++    if (azp) {
++      azp_ptr = static_cast<ElementAccumulator*>(azp.value().data_ptr());
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++          mctlass::epilogue::thread::ScaleType::ScaleAvBvBiasAzpPerTorken;
++      using mctlassGemmScaleOp =
++          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
++                           LayoutC, ElementCompute, ArchTag, scale_type>;
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          batchsize,
++          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
++    } else {
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++          mctlass::epilogue::thread::ScaleType::ScaleAsBvBiasAzp;
++      using mctlassGemmScaleOp =
++          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
++                           LayoutC, ElementCompute, ArchTag, scale_type>;
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          batchsize,
++          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
++    }
++  } else {
++    using ElementC = mctlass::half_t;
++    using ElementOutput = ElementC;
++
++    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
++    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
++    auto c_ptr = static_cast<ElementC*>(out.data_ptr());
++    auto scale_a = a_scales.data_ptr<float>();
++    auto scale_b = b_scales.data_ptr<float>();
++
++    ElementAccumulator* azp_ptr = nullptr;
++    auto azp_adj_ptr = azp_adj.data_ptr<ElementAccumulator>();
++    ElementOutput* bias_t = static_cast<ElementOutput*>(bias.value().data_ptr());
++
++    if (azp) {
++      azp_ptr = static_cast<ElementAccumulator*>(azp.value().data_ptr());
++
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++          mctlass::epilogue::thread::ScaleType::ScaleAvBvBiasAzpPerTorken;
++      using mctlassGemmScaleOp =
++          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
++                           LayoutC, ElementCompute, ArchTag, scale_type>;
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          batchsize,
++          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
++    } else {
++      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
++          mctlass::epilogue::thread::ScaleType::ScaleAsBvBiasAzp;
++      using mctlassGemmScaleOp =
++          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
++                           LayoutC, ElementCompute, ArchTag, scale_type>;
++      mctlassGemmScaleOp mctlass_op;
++      mctlass::gemm::GemmCoord problem_size(m, n, k);
++      typename mctlassGemmScaleOp::Arguments arguments{
++          mctlass::gemm::GemmUniversalMode::kGemm,
++          problem_size,
++          batchsize,
++          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
++          a_ptr,
++          b_ptr,
++          c_ptr,
++          c_ptr,
++          m * k,
++          n * k,
++          m * n,
++          m * n,
++          k,
++          n,
++          n,
++          n
++      };
++      mctlass_op(arguments, NULL, stream);
++    }
++  }
++#endif //USE_MACA
  }
--
-+#endif
- void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,
-                                 torch::Tensor const& b,
-                                 torch::Tensor const& a_scales,
-                                 torch::Tensor const& b_scales,
-                                 torch::Tensor const& azp_adj,
--                                std::optional<torch::Tensor> const& azp,
--                                std::optional<torch::Tensor> const& bias) {
-+                                c10::optional<torch::Tensor> const& azp,
-+                                c10::optional<torch::Tensor> const& bias) {
-+#if 0
-   TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
-   TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
  
-@@ -196,4 +364,5 @@ void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,
-     return cutlass_scaled_mm_sm89_epilogue<c2x::ScaledEpilogueBiasAzp>(
++#ifndef USE_MACA
+ template <template <typename, typename> typename Epilogue,
+           typename... EpilogueArgs>
+ void cutlass_scaled_mm_sm80_epilogue(torch::Tensor& out, torch::Tensor const& a,
+@@ -197,3 +567,4 @@ void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,
          out, a, b, a_scales, b_scales, azp_adj, bias);
    }
-+#endif
  }
++#endif // USE_MACA
 diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
-index f2fae4b66..3af219185 100644
+index ce7cf2f35..096c90749 100644
 --- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
 +++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- #include <stddef.h>
- #include <torch/all.h>
-@@ -6,6 +7,7 @@
+@@ -6,6 +6,7 @@
  
  // clang-format will break include orders
  // clang-format off
-+#if 0
++#ifndef USE_MACA
  #include "cute/tensor.hpp"
  #include "cute/atom/mma_atom.hpp"
  #include "cutlass/numeric_types.h"
-@@ -20,6 +22,13 @@
- 
+@@ -21,6 +22,14 @@
  #include "cutlass/epilogue/threadblock/fusion/visitors.hpp"
  #include "cutlass/gemm/kernel/default_gemm_universal_with_visitor.h"
-+#endif
-+
+ 
++#else
 +#include "mctlass/mctlass_ex.h"
 +#include "mctlass/half.h"
 +#include "mctlass/layout/matrix.h"
 +#include "mctlass/epilogue/thread/scale_type.h"
 +#include "mctlass/util/command_line.h"
- 
++#endif // USE_MACA
++
  #include "core/math.hpp"
  #include "cutlass_extensions/common.hpp"
+ // clang-format on
 @@ -42,6 +51,7 @@ namespace vllm {
  // reduce the size of the compiled binary.
  // __CUDA_ARCH__ is not defined in host code, so this lets us smuggle the ifdef
  // into code that will be executed on the device where it is defined.
-+#if 0
++#ifndef USE_MACA
  template <typename Kernel>
  struct enable_sm75_to_sm80 : Kernel {
    template <typename... Args>
-@@ -71,12 +81,15 @@ struct enable_sm89_to_sm90 : Kernel {
+@@ -51,7 +61,9 @@ struct enable_sm75_to_sm80 : Kernel {
  #endif
    }
  };
-+#endif
-+
- template <typename Arch, template <typename> typename ArchGuard,
-           typename ElementAB_, typename ElementD_,
-           template <typename, typename> typename Epilogue_, typename TileShape,
-           typename WarpShape, typename InstructionShape, int32_t MainLoopStages,
--          typename FP8MathOperator = cutlass::arch::OpMultiplyAdd>
-+          typename FP8MathOperator = mctlass::arch::OpMultiplyAdd>
- struct cutlass_2x_gemm {
-+#if 0
-   using ElementAB = ElementAB_;
-   using ElementD = ElementD_;
++#endif // USE_MACA
  
-@@ -122,12 +135,14 @@ struct cutlass_2x_gemm {
-   // clang-format on
++#ifndef USE_MACA
+ template <typename Kernel>
+ struct enable_sm80_to_sm89 : Kernel {
+   template <typename... Args>
+@@ -128,11 +140,13 @@ struct cutlass_2x_gemm {
  
    using Op = cutlass::gemm::device::GemmUniversalAdapter<KernelType>;
-+#endif
  };
++#endif // USE_MACA
  
  template <typename Gemm, typename... EpilogueArgs>
  inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
                                  torch::Tensor const& b,
                                  EpilogueArgs&&... epilogue_params) {
-+#if 0
++#ifndef USE_MACA
    using ElementAB = typename Gemm::ElementAB;
    using ElementD = typename Gemm::ElementD;
  
-@@ -188,6 +203,7 @@ inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
+@@ -193,6 +207,7 @@ inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
    CUTLASS_CHECK(gemm_op.can_implement(args));
    cutlass::Status status = gemm_op(args, workspace.data_ptr(), stream);
    CUTLASS_CHECK(status);
-+#endif
++#endif // USE_MACA
  }
  
  template <typename Gemm, typename FallbackGemm, typename... EpilogueArgs>
-@@ -195,6 +211,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
+@@ -200,6 +215,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
                                           torch::Tensor const& a,
                                           torch::Tensor const& b,
                                           EpilogueArgs&&... args) {
-+#if 0
++#ifndef USE_MACA
    // In some cases, the GPU isn't able to accommodate the
    // shared memory requirements of the Gemm. In such cases, use
    // the FallbackGemm instead.
-@@ -215,6 +232,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
+@@ -220,6 +236,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
      return cutlass_gemm_caller<FallbackGemm>(
          out, a, b, std::forward<EpilogueArgs>(args)...);
    }
-+#endif
++#endif // USE_MACA
  }
  
  }  // namespace vllm
 diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
-index a562fd896..dcafcf0f7 100644
+index a562fd896..c7dec5784 100644
 --- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
 +++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #include "scaled_mm_c2x.cuh"
-@@ -9,6 +10,7 @@
+@@ -9,6 +9,7 @@
  
  namespace vllm {
  
-+#if 0
++#ifndef USE_MACA
  template <typename InType, typename OutType,
            template <typename, typename> typename Epilogue>
  struct sm75_config_default {
-@@ -66,6 +68,7 @@ struct sm75_config_M32 {
+@@ -66,6 +67,7 @@ struct sm75_config_M32 {
        cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,
                        Epilogue, TileShape, WarpShape, InstructionShape, 2>;
  };
-+#endif
++#endif // USE_MACA
  
  template <typename InType, typename OutType,
            template <typename, typename> typename Epilogue,
-@@ -74,6 +77,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
+@@ -74,6 +76,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
                                         torch::Tensor const& a,
                                         torch::Tensor const& b,
                                         EpilogueArgs&&... args) {
-+#if 0
++#ifndef USE_MACA
    static_assert(std::is_same<InType, int8_t>());
    TORCH_CHECK(a.dtype() == torch::kInt8);
    TORCH_CHECK(b.dtype() == torch::kInt8);
-@@ -118,6 +122,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
+@@ -118,6 +121,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
      return fallback_cutlass_gemm_caller<Cutlass2xGemmDefault, FallbackGemm>(
          out, a, b, std::forward<EpilogueArgs>(args)...);
    }
@@ -9312,405 +9304,104 @@ index a562fd896..dcafcf0f7 100644
  
  }  // namespace vllm
 diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
-index 6bef55088..87bd5506d 100644
+index 348525810..cc3b9f1bb 100644
 --- a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
 +++ b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include <cudaTypedefs.h>
- 
- #include <c10/cuda/CUDAGuard.h>
-@@ -9,26 +10,26 @@ void cutlass_scaled_mm_sm75(torch::Tensor& c, torch::Tensor const& a,
-                             torch::Tensor const& b,
-                             torch::Tensor const& a_scales,
-                             torch::Tensor const& b_scales,
--                            std::optional<torch::Tensor> const& bias);
-+                            c10::optional<torch::Tensor> const& bias);
- 
- void cutlass_scaled_mm_sm80(torch::Tensor& c, torch::Tensor const& a,
-                             torch::Tensor const& b,
-                             torch::Tensor const& a_scales,
-                             torch::Tensor const& b_scales,
--                            std::optional<torch::Tensor> const& bias);
-+                            c10::optional<torch::Tensor> const& bias);
- 
- void cutlass_scaled_mm_sm89(torch::Tensor& c, torch::Tensor const& a,
-                             torch::Tensor const& b,
-                             torch::Tensor const& a_scales,
-                             torch::Tensor const& b_scales,
--                            std::optional<torch::Tensor> const& bias);
-+                            c10::optional<torch::Tensor> const& bias);
- 
- #if defined ENABLE_SCALED_MM_C3X && ENABLE_SCALED_MM_C3X
- void cutlass_scaled_mm_sm90(torch::Tensor& c, torch::Tensor const& a,
-                             torch::Tensor const& b,
-                             torch::Tensor const& a_scales,
-                             torch::Tensor const& b_scales,
--                            std::optional<torch::Tensor> const& bias);
-+                            c10::optional<torch::Tensor> const& bias);
- #endif
- 
- void cutlass_scaled_mm_azp_sm75(torch::Tensor& c, torch::Tensor const& a,
-@@ -36,24 +37,24 @@ void cutlass_scaled_mm_azp_sm75(torch::Tensor& c, torch::Tensor const& a,
-                                 torch::Tensor const& a_scales,
-                                 torch::Tensor const& b_scales,
-                                 torch::Tensor const& azp_adj,
--                                std::optional<torch::Tensor> const& azp,
--                                std::optional<torch::Tensor> const& bias);
-+                                c10::optional<torch::Tensor> const& azp,
-+                                c10::optional<torch::Tensor> const& bias);
- 
- void cutlass_scaled_mm_azp_sm80(torch::Tensor& c, torch::Tensor const& a,
-                                 torch::Tensor const& b,
-                                 torch::Tensor const& a_scales,
-                                 torch::Tensor const& b_scales,
-                                 torch::Tensor const& azp_adj,
--                                std::optional<torch::Tensor> const& azp,
--                                std::optional<torch::Tensor> const& bias);
-+                                c10::optional<torch::Tensor> const& azp,
-+                                c10::optional<torch::Tensor> const& bias);
- 
- void cutlass_scaled_mm_azp_sm89(torch::Tensor& c, torch::Tensor const& a,
-                                 torch::Tensor const& b,
-                                 torch::Tensor const& a_scales,
-                                 torch::Tensor const& b_scales,
-                                 torch::Tensor const& azp_adj,
--                                std::optional<torch::Tensor> const& azp,
--                                std::optional<torch::Tensor> const& bias);
-+                                c10::optional<torch::Tensor> const& azp,
-+                                c10::optional<torch::Tensor> const& bias);
- 
- #if defined CUDA_VERSION && CUDA_VERSION >= 12000
- void cutlass_scaled_mm_azp_sm90(torch::Tensor& c, torch::Tensor const& a,
-@@ -61,8 +62,8 @@ void cutlass_scaled_mm_azp_sm90(torch::Tensor& c, torch::Tensor const& a,
-                                 torch::Tensor const& a_scales,
-                                 torch::Tensor const& b_scales,
-                                 torch::Tensor const& azp_adj,
--                                std::optional<torch::Tensor> const& azp,
--                                std::optional<torch::Tensor> const& bias);
-+                                c10::optional<torch::Tensor> const& azp,
-+                                c10::optional<torch::Tensor> const& bias);
- #endif
- 
- bool cutlass_scaled_mm_supports_fp8(int64_t cuda_device_capability) {
-@@ -97,60 +98,10 @@ bool cutlass_scaled_mm_supports_block_fp8(int64_t cuda_device_capability) {
- void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
+@@ -149,6 +149,9 @@ void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
                         torch::Tensor const& b, torch::Tensor const& a_scales,
                         torch::Tensor const& b_scales,
--                       std::optional<torch::Tensor> const& bias) {
--  // Checks for conformality
--  TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);
--  TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&
--              b.size(1) == c.size(1));
-+                       c10::optional<torch::Tensor> const& bias) {
- 
--  // Check for strides and alignment
--  TORCH_CHECK(a.stride(1) == 1 && c.stride(1) == 1);  // Row-major
--  TORCH_CHECK(b.stride(0) == 1);                      // Column-major
--  TORCH_CHECK(c.stride(0) % 16 == 0 &&
--              b.stride(1) % 16 == 0);  // 16 Byte Alignment
--
--  if (bias) {
--    TORCH_CHECK(bias->numel() == b.size(1) && bias->is_contiguous() &&
--                bias->dim() == 1);
--  }
--
--  at::cuda::OptionalCUDAGuard const device_guard(device_of(a));
--  int32_t version_num = get_sm_version_num();
--  // Hopper
--
--  // Guard against compilation issues for sm90 kernels
--#if defined ENABLE_SCALED_MM_C3X && ENABLE_SCALED_MM_C3X
--  if (version_num >= 90) {
--    cutlass_scaled_mm_sm90(c, a, b, a_scales, b_scales, bias);
--    return;
--  }
--#endif
+                        std::optional<torch::Tensor> const& bias) {
++#ifdef USE_MACA
 +  cutlass_scaled_mm_sm75(c, a, b, a_scales, b_scales, bias);
++#else
+   // Checks for conformality
+   TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);
+   TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&
+@@ -209,6 +212,7 @@ void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
+       "No compiled cutlass_scaled_mm for a compute capability less than "
+       "CUDA device capability: ",
+       version_num);
++#endif // USE_MACA
+ }
  
--#if defined ENABLE_SCALED_MM_C2X && ENABLE_SCALED_MM_C2X
--  if (version_num == 89) {
--    // Ada Lovelace
--    cutlass_scaled_mm_sm89(c, a, b, a_scales, b_scales, bias);
--    return;
--  }
--
--  if (version_num >= 80) {
--    // Ampere
--    cutlass_scaled_mm_sm80(c, a, b, a_scales, b_scales, bias);
--    return;
--  }
--
--  if (version_num >= 75) {
--    // Turing
--    cutlass_scaled_mm_sm75(c, a, b, a_scales, b_scales, bias);
--    return;
--  }
--#endif
--
--  TORCH_CHECK_NOT_IMPLEMENTED(
--      false,
--      "No compiled cutlass_scaled_mm for a compute capability less than "
--      "CUDA device capability: ",
--      version_num);
- }
+ void cutlass_moe_mm(
+@@ -317,6 +321,29 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
  
- void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
-@@ -158,8 +109,9 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
-                            torch::Tensor const& a_scales,
-                            torch::Tensor const& b_scales,
-                            torch::Tensor const& azp_adj,
--                           std::optional<torch::Tensor> const& azp,
--                           std::optional<torch::Tensor> const& bias) {
-+                           c10::optional<torch::Tensor> const& azp,
-+                           c10::optional<torch::Tensor> const& bias) {
-+#if 0
-   // Checks for conformality
-   TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);
-   TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&
-@@ -225,4 +177,5 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
+   at::cuda::OptionalCUDAGuard const device_guard(device_of(a));
+ 
++#ifdef USE_MACA
++
++  if (!bias) {
++    // mctlass not support None bias
++
++    int32_t n = b.size(1);
++    int32_t batchsize = 1;
++    if (a.dim() == 3 && b.dim() == 3) {
++      // a.size = [batch_size, M, K], b.size = [batch_size, K, N]
++      n = b.size(2);
++      batchsize = a.size(0);
++    }
++    auto options = torch::TensorOptions()
++                     .dtype(c.dtype())
++                     .device(a.device());
++    torch::Tensor zero_bias = torch::zeros({batchsize,  n}, options);
++    cutlass_scaled_mm_azp_sm75(c, a, b, a_scales, b_scales, azp_adj, azp, zero_bias);
++  } else {
++    cutlass_scaled_mm_azp_sm75(c, a, b, a_scales, b_scales, azp_adj, azp, bias);
++  }
++
++#else
++
+   int32_t version_num = get_sm_version_num();
+ 
+ #if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90
+@@ -350,4 +377,6 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
        "No compiled cutlass_scaled_mm_azp for a compute capability less than "
        "CUDA device capability: ",
        version_num);
-+#endif
++
++#endif // USE_MACA
  }
-diff --git a/csrc/quantization/fp8/common.cu b/csrc/quantization/fp8/common.cu
-index e4f6615ed..8feafca25 100644
---- a/csrc/quantization/fp8/common.cu
-+++ b/csrc/quantization/fp8/common.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include "common.cuh"
- #include "dispatch_utils.h"
- 
-@@ -126,7 +127,7 @@ void dynamic_scaled_fp8_quant(torch::Tensor& out,          // [..., d]
- void dynamic_per_token_scaled_fp8_quant(
-     torch::Tensor& out,          // [..., d]
-     torch::Tensor const& input,  // [..., d]
--    torch::Tensor& scales, std::optional<at::Tensor> const& scale_ub) {
-+    torch::Tensor& scales, c10::optional<at::Tensor> const& scale_ub) {
-   TORCH_CHECK(input.is_contiguous());
-   TORCH_CHECK(out.is_contiguous());
- 
-diff --git a/csrc/quantization/fp8/nvidia/quant_utils.cuh b/csrc/quantization/fp8/nvidia/quant_utils.cuh
-index f8cd1dcba..f04db47d4 100644
---- a/csrc/quantization/fp8/nvidia/quant_utils.cuh
-+++ b/csrc/quantization/fp8/nvidia/quant_utils.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- #include "../../../attention/attention_dtypes.h"
-@@ -563,6 +564,54 @@ __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
-           TORCH_CHECK(false,                                                   \
-                       "Unsupported input type of kv cache: ", SRC_DTYPE);      \
-         }                                                                      \
-+      } else if (KV_DTYPE == "int8") {                                         \
-+        if (SRC_DTYPE == at::ScalarType::Half) {                               \
-+          FN(uint16_t, int8_t, vllm::Fp8KVCacheDataType::kInt8);               \
-+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
-+          FN(__nv_bfloat16, int8_t, vllm::Fp8KVCacheDataType::kInt8);          \
-+        } else {                                                               \
-+          TORCH_CHECK(false,                                                   \
-+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
-+        }                                                                      \    
-+      } else {                                                                 \
-+        TORCH_CHECK(false, "Unsupported data type of kv cache: ", KV_DTYPE);   \
-+      }                                                                        \
-+    }
-+
-+  #define DISPATCH_BY_KV_CACHE_V2_DTYPE(SRC_DTYPE, KV_DTYPE, COUNT_INIT_ONCE, FN)                  \
-+    if (KV_DTYPE == "auto") {                                                  \
-+      if (SRC_DTYPE == at::ScalarType::Float) {                                \
-+        FN(float, float, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);                     \
-+      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \
-+        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);               \
-+      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \
-+        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);     \
-+      } else {                                                                 \
-+        TORCH_CHECK(false, "Unsupported input type of kv cache: ", SRC_DTYPE); \
-+      }                                                                        \
-+    } else {                                                                   \
-+      if (KV_DTYPE == "fp8" || KV_DTYPE == "fp8_e4m3") {                       \
-+        if (SRC_DTYPE == at::ScalarType::Float) {                              \
-+          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);              \
-+        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \
-+          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);           \
-+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
-+          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);      \
-+        } else {                                                               \
-+          TORCH_CHECK(false,                                                   \
-+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
-+        }                                                                      \
-+      } else if (KV_DTYPE == "fp8_e5m2") {                                     \
-+        if (SRC_DTYPE == at::ScalarType::Float) {                              \
-+          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);              \
-+        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \
-+          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);           \
-+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
-+          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);      \
-+        } else {                                                               \
-+          TORCH_CHECK(false,                                                   \
-+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
-+        }                                                                      \
-       } else {                                                                 \
-         TORCH_CHECK(false, "Unsupported data type of kv cache: ", KV_DTYPE);   \
-       }                                                                        \
-diff --git a/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu b/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
-index 3c4f183bf..b1b6ab8eb 100644
---- a/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
-+++ b/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- 
- #include <ATen/cuda/CUDAContext.h>
- #include <c10/cuda/CUDAGuard.h>
-@@ -95,8 +96,8 @@ void rms_norm_dynamic_per_token_quant_dispatch(
-     torch::Tensor const& weight,  // [hidden_size]
-     torch::Tensor& scales,        // [num_tokens]
-     double const var_epsilon,     // Variance epsilon used in norm calculation
--    std::optional<at::Tensor> const& scale_ub,
--    std::optional<at::Tensor>& residual) {
-+    c10::optional<at::Tensor> const& scale_ub,
-+    c10::optional<at::Tensor>& residual) {
-   int32_t hidden_size = input.size(-1);
-   int32_t num_tokens = input.numel() / hidden_size;
- 
-@@ -143,7 +144,7 @@ void rms_norm_dynamic_per_token_quant(
-     torch::Tensor const& weight,  // [hidden_size]
-     torch::Tensor& scales,        // [num_tokens]
-     double const var_epsilon,     // Variance epsilon used in norm calculation
--    std::optional<at::Tensor> scale_ub, std::optional<at::Tensor> residual) {
-+    c10::optional<at::Tensor> scale_ub, c10::optional<at::Tensor> residual) {
-   TORCH_CHECK(out.dtype() == kFp8Type || out.dtype() == torch::kInt8);
-   TORCH_CHECK(out.is_contiguous() && input.is_contiguous());
- 
 diff --git a/csrc/quantization/fused_kernels/quant_conversions.cuh b/csrc/quantization/fused_kernels/quant_conversions.cuh
-index f8a987222..0a15bee98 100644
+index 4e6118e52..435582408 100644
 --- a/csrc/quantization/fused_kernels/quant_conversions.cuh
 +++ b/csrc/quantization/fused_kernels/quant_conversions.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- 
- /**
-@@ -7,6 +8,7 @@
- #include "quantization/vectorization.cuh"
- // TODO(luka/varun):refactor common.cuh to use this file instead
- #include "quantization/fp8/common.cuh"
-+#include "attention/dtype_float16.cuh"
- 
- namespace vllm {
- 
-@@ -25,12 +27,43 @@ static __device__ __forceinline__ int8_t float_to_int8_rn(float const x) {
-   return static_cast<int8_t>(dst);
+@@ -32,7 +32,12 @@ static __device__ __forceinline__ int8_t float_to_int8_rn(float const x) {
  #else
    // CUDA path
--  uint32_t dst;
--  asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
-+  // uint32_t dst;
-+  // asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "r"(x));
-+  // return reinterpret_cast<const int8_t&>(dst);
-+
-+  // int32_t dst;
-+  // dst = (int32_t)(x > 0? x + 0.5: x - 0.5);
-+  // dst = (char)(dst > 127 ? 127 : (dst < -128 ? -128 : dst));
-+  // return reinterpret_cast<const int8_t&>(dst);
-+  
-+  int32_t dst;
-+  dst = __float2int_rn(x);
-+  dst = min(dst, 127);
-+  dst = max(dst, -127);
+   uint32_t dst;
++#ifndef USE_MACA
+   asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
++#else
++  dst = (int32_t)(x > 0? x + 0.5: x - 0.5);
++  dst = (char)(dst > 127 ? 127 : (dst < -128 ? -128 : dst));
++#endif // USE_MACA
    return reinterpret_cast<const int8_t&>(dst);
-+
-+  // return static_cast<int8_t>(fmaxf(-127.0f, fminf(roundf(x), 127.0f)));
  #endif
  }
- 
-+template <typename Tin>
-+__inline__ __device__ int8_t scaled_quant_to_int8(
-+    const Tin& x, const float *scale);
-+
-+// half -> int8
-+template <>
-+__inline__ __device__ int8_t scaled_quant_to_int8<uint16_t>(
-+    const uint16_t& x, const float *scale) {
-+  return float_to_int8_rn(half_to_float(x) / *scale);
-+}
-+
-+// bf16 -> int8
-+template <>
-+__inline__ __device__ int8_t scaled_quant_to_int8<__nv_bfloat16>(
-+    const __nv_bfloat16& x, const float *scale) {
-+  return float_to_int8_rn(__bfloat162float(x) / *scale);
-+}
-+
- static __device__ __forceinline__ FP8_TYPE float_to_fp8(float const x) {
-   float const r = fmax(-FP8_E4M3_MAX, fmin(x, FP8_E4M3_MAX));
-   return static_cast<FP8_TYPE>(r);
 diff --git a/csrc/quantization/gguf/ggml-common.h b/csrc/quantization/gguf/ggml-common.h
-index d42205a65..4be54baa7 100644
+index 6bef5db3c..8dbab74a3 100644
 --- a/csrc/quantization/gguf/ggml-common.h
 +++ b/csrc/quantization/gguf/ggml-common.h
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- // copied from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-common.h
- #define QK_K 256
- #define K_QUANTS_PER_ITERATION 2
-@@ -1075,13 +1076,14 @@ typedef float (*vec_dot_q_mul_mat_cuda_t)(
- 
- // Utility function
- 
--#if defined(USE_ROCM)
--
--#ifndef __has_builtin
--    #define __has_builtin(x) 0
--#endif
-+//#if defined(USE_ROCM)
-+#if 1
-+//#ifndef __has_builtin
-+//    #define __has_builtin(x) 0
-+//#endif
- 
- typedef int8_t int8x4_t __attribute__((ext_vector_type(4)));
-+#if 0
- static __device__ __forceinline__ int __vsubss4(const int a, const int b) {
-     const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);
-     const int8x4_t vb = reinterpret_cast<const int8x4_t&>(b);
-@@ -1101,9 +1103,11 @@ static __device__ __forceinline__ int __vsubss4(const int a, const int b) {
-     return reinterpret_cast<int &>(c);
- #endif // __has_builtin(__builtin_elementwise_sub_sat)
- }
-+#endif
- 
- static __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {
--#if __has_builtin(__builtin_amdgcn_sdot4)
-+//#if __has_builtin(__builtin_amdgcn_sdot4)
-+#if 0
-     c = __builtin_amdgcn_sdot4(a, b, c, false);
- #else
-     const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);
-@@ -1113,6 +1117,7 @@ static __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {
-     return c;
- }
- 
-+#if 0
- static __device__ __forceinline__ uint32_t __vcmpeq4(const uint32_t a, const uint32_t b) {
-     uint32_t neq = a^b;
-     return !(neq & 0xff000000) * 0xff000000 |
-@@ -1127,4 +1132,5 @@ static __device__ __forceinline__ uint32_t __vsub4(const uint32_t a, const uint3
+@@ -1147,4 +1147,12 @@ static __device__ __forceinline__ uint32_t __vsub4(const uint32_t a, const uint3
             (static_cast<uint8_t>(((a & 0x0000ff00) >>  8) - ((b & 0x0000ff00) >>  8)) <<  8) +
             (static_cast<uint8_t>(((a & 0x000000ff) >>  0) - ((b & 0x000000ff) >>  0)) <<  0);
  }
-+#endif
++#elif defined(USE_MACA)
++typedef int8_t int8x4_t __attribute__((ext_vector_type(4)));
++static __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {
++    const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);
++    const int8x4_t vb = reinterpret_cast<const int8x4_t&>(b);
++    c += va[0] * vb[0] + va[1] * vb[1] + va[2] * vb[2] + va[3] * vb[3];
++    return c;
++}
  #endif // defined(USE_ROCM)
 diff --git a/csrc/quantization/gptq/Hgemm_common.cuh b/csrc/quantization/gptq/Hgemm_common.cuh
 new file mode 100644
-index 000000000..d49b4be54
+index 000000000..22723dd75
 --- /dev/null
 +++ b/csrc/quantization/gptq/Hgemm_common.cuh
-@@ -0,0 +1,91 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,92 @@
 +#pragma once
 +#include "maca_fp16.h"
 +
@@ -9743,6 +9434,7 @@ index 000000000..d49b4be54
 +    dst = __builtin_mxc_ldg_b128_predicator(cast_b128(base), 0, ret0_en, true, false, false, pred, 1, \
 +                                           MACA_ICMP_EQ);
 +
++
 +#define ldg_b32_reg_async(dst, base, pred, ret0_en)                                                \
 +    dst = __builtin_mxc_ldg_b32_predicator(cast_b32(base), 0, ret0_en, true, false, true, pred, 1, \
 +                                           MACA_ICMP_EQ)[0];
@@ -9766,6 +9458,7 @@ index 000000000..d49b4be54
 +    __builtin_mxc_ldg_b128_bsm_predicator(cast_b128(saddr), cast_b128(base), 0, ret0_en, true, false, \
 +                                          false, pred, 1, MACA_ICMP_EQ);
 +
++
 +#define ldg_b32_bsm_async(saddr, base, pred, ret0_en)                                                    \
 +    __builtin_mxc_ldg_b32_bsm_predicator(cast_b32(saddr), cast_b32(base), 0, ret0_en, true, false, true, \
 +                                         pred, 1, MACA_ICMP_EQ);
@@ -9803,11 +9496,10 @@ index 000000000..d49b4be54
 +#define barrier_inst __builtin_mxc_barrier_ex(2)
 diff --git a/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
 new file mode 100644
-index 000000000..9a866c81a
+index 000000000..5a97af2ad
 --- /dev/null
 +++ b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
-@@ -0,0 +1,427 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,426 @@
 +#pragma once
 +#include "Hgemm_common.cuh"
 +#include "gptq.cuh"
@@ -10236,11 +9928,10 @@ index 000000000..9a866c81a
 +}
 diff --git a/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
 new file mode 100644
-index 000000000..ac25b323f
+index 000000000..2cd807f57
 --- /dev/null
 +++ b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
-@@ -0,0 +1,441 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,440 @@
 +#pragma once
 +#include "Hgemm_common.cuh"
 +#include "gptq.cuh"
@@ -10683,11 +10374,10 @@ index 000000000..ac25b323f
 +}
 diff --git a/csrc/quantization/gptq/dequant.cuh b/csrc/quantization/gptq/dequant.cuh
 new file mode 100644
-index 000000000..2cfe33bab
+index 000000000..b192a7804
 --- /dev/null
 +++ b/csrc/quantization/gptq/dequant.cuh
-@@ -0,0 +1,12 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,11 @@
 +// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 +#pragma once
 +
@@ -10702,11 +10392,10 @@ index 000000000..2cfe33bab
 \ No newline at end of file
 diff --git a/csrc/quantization/gptq/gptq.cuh b/csrc/quantization/gptq/gptq.cuh
 new file mode 100644
-index 000000000..b676c5776
+index 000000000..c91bf4168
 --- /dev/null
 +++ b/csrc/quantization/gptq/gptq.cuh
-@@ -0,0 +1,366 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,365 @@
 +#pragma once
 +
 +#include "Hgemm_common.cuh"
@@ -11074,10 +10763,10 @@ index 000000000..b676c5776
 +    }
 diff --git a/csrc/quantization/gptq/hgemm_gptq.h b/csrc/quantization/gptq/hgemm_gptq.h
 new file mode 100644
-index 000000000..c646e6198
+index 000000000..383b42ab8
 --- /dev/null
 +++ b/csrc/quantization/gptq/hgemm_gptq.h
-@@ -0,0 +1,1936 @@
+@@ -0,0 +1,2031 @@
 +// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 +/*
 +hgemm gptq 4bits
@@ -11443,7 +11132,103 @@ index 000000000..c646e6198
 +...
 +k03n48 k03n49 k03n50 k03n51 ... k03n63
 +*/
++template<class scalar_t>
++__device__ __forceinline__ void awq_dequant_4bits(const uint32_t& p, scalar_t (&out)[8], scalar_t (&scale)[8], const uint32_t& scale_zero) {
++    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
++        v2f v2z,v2scale, v2neg;
++        v2neg[0] = -1.0f; v2neg[1] = -1.0f;
++        v2z[0] = 0.0f; v2z[1] = 0.0f;
++        v2f v2zero;
++        float tmp[2];
++        int p0 = p & 0x0f0f0f0f;
++        int z0 = scale_zero & 0x0f0f0f0f;
++        CVT_B0TOF32(p0, tmp[0]);
++        CVT_B2TOF32(p0, tmp[1]);
++        CVT_B0TOF32(z0, v2zero[0]);
++        CVT_B2TOF32(z0, v2zero[1]);
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
++        v2scale[0] = (float)scale[0]; v2scale[1] = (float)scale[1];
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
++        f32x2_cvt_bf16x2(*((uint32_t*)out), tmp);
++
++        CVT_B1TOF32(p0, tmp[0]);
++        CVT_B3TOF32(p0, tmp[1]);
++        CVT_B1TOF32(z0, v2zero[0]);
++        CVT_B3TOF32(z0, v2zero[1]);
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
++        v2scale[0] = (float)scale[4]; v2scale[1] = (float)scale[5];
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
++        f32x2_cvt_bf16x2(*((uint32_t*)(out + 4)), tmp);
++
++        p0 = (p >> 4) & 0x0f0f0f0f;
++        z0 = (scale_zero >> 4) & 0x0f0f0f0f;
++        CVT_B0TOF32(p0, tmp[0]);
++        CVT_B2TOF32(p0, tmp[1]);
++        CVT_B0TOF32(z0, v2zero[0]);
++        CVT_B2TOF32(z0, v2zero[1]);
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
++        v2scale[0] = (float)scale[2]; v2scale[1] = (float)scale[3];
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
++        f32x2_cvt_bf16x2(*((uint32_t*)(out + 2)), tmp);
++
++        CVT_B1TOF32(p0, tmp[0]);
++        CVT_B3TOF32(p0, tmp[1]);
++        CVT_B1TOF32(z0, v2zero[0]);
++        CVT_B3TOF32(z0, v2zero[1]);
++
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
++        v2scale[0] = (float)scale[6]; v2scale[1] = (float)scale[7];
++        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
++        f32x2_cvt_bf16x2(*((uint32_t*)(out + 6)), tmp);
++    } else {
++        v2f a0, v2z, v2scale, v2zero, v2neg;
++        v2z[0] = 0; v2z[1] = 0;
++        v2neg[0] = -1.0f; v2neg[1] = -1.0f;
++        int p0 = p & 0x0f0f0f0f;
++        int z0 = scale_zero & 0x0f0f0f0f;
++        CVT_B0TOF32(p0, a0.x);
++        CVT_B2TOF32(p0, a0.y);
++        CVT_B0TOF32(z0, v2zero[0]);
++        CVT_B2TOF32(z0, v2zero[1]);
++        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
++        v2scale[0] = (float)scale[0]; v2scale[1] = (float)scale[1];
++        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
++        out[0] = (scalar_t)a0.x;
++        out[1] = (scalar_t)a0.y;
++
++        CVT_B1TOF32(p0, a0.x);
++        CVT_B3TOF32(p0, a0.y);
++        CVT_B1TOF32(z0, v2zero[0]);
++        CVT_B3TOF32(z0, v2zero[1]);
++        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
++        v2scale[0] = (float)scale[4]; v2scale[1] = (float)scale[5];
++        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
++        out[4] = (scalar_t)a0.x;
++        out[5] = (scalar_t)a0.y;
 +
++        p0 = (p >> 4) & 0x0f0f0f0f;
++        z0 = (scale_zero >> 4) & 0x0f0f0f0f;
++        CVT_B0TOF32(p0, a0.x);
++        CVT_B2TOF32(p0, a0.y);
++        CVT_B0TOF32(z0, v2zero[0]);
++        CVT_B2TOF32(z0, v2zero[1]);
++        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
++        v2scale[0] = (float)scale[2]; v2scale[1] = (float)scale[3];
++        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
++        out[2] = (scalar_t)a0.x;
++        out[3] = (scalar_t)a0.y;
++
++        CVT_B1TOF32(p0, a0.x);
++        CVT_B3TOF32(p0, a0.y);
++        CVT_B1TOF32(z0, v2zero[0]);
++        CVT_B3TOF32(z0, v2zero[1]);
++        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
++        v2scale[0] = (float)scale[6]; v2scale[1] = (float)scale[7];
++        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
++        out[6] = (scalar_t)a0.x;
++        out[7] = (scalar_t)a0.y;
++    }
++}
 +
 +//deqaunt a uint32_t
 +template<class scalar_t>
@@ -12981,11 +12766,10 @@ index 000000000..c646e6198
 +
 +    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
 +        size_t num_elem = size_t(m) * size_t(n);
-+        size_t clean_blocks = std::max(size_t(1), num_elem / (clean_kernel_thread_num * clean_kernel_pack_num));
-+        clean_zero<clean_kernel_thread_num, clean_kernel_pack_num><<<clean_blocks, clean_kernel_thread_num>>>((float*)C_temp, num_elem);
++        size_t clean_blocks = std::max(size_t(1), (num_elem + clean_kernel_thread_num * clean_kernel_pack_num - 1)/ (clean_kernel_thread_num * clean_kernel_pack_num));
++        clean_zero<clean_kernel_thread_num, clean_kernel_pack_num><<<clean_blocks, clean_kernel_thread_num, 0, stream>>>((float*)C_temp, num_elem);
 +    }
 +
-+
 +    //It is better to do perm before launch kernel
 +    if constexpr(BLOCKS_K % 2 == 1) {
 +        __hgemm_singular_blocks_k::hgemm_gptq<scalar_t,
@@ -13005,8 +12789,8 @@ index 000000000..c646e6198
 +
 +    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
 +        size_t num_elem = size_t(m) * size_t(n);
-+        size_t reduce_blocks = std::max(size_t(1), num_elem / (reduce_kernel_thread_num * reduce_kernel_pack_num));
-+        all_reduce<reduce_kernel_thread_num, reduce_kernel_pack_num, false><<<reduce_blocks, reduce_kernel_thread_num>>>((float*)C_temp, (maca_bfloat16*)C, num_elem);
++        size_t reduce_blocks = std::max(size_t(1), (num_elem  + reduce_kernel_thread_num * reduce_kernel_pack_num - 1) / (reduce_kernel_thread_num * reduce_kernel_pack_num));
++        all_reduce<reduce_kernel_thread_num, reduce_kernel_pack_num, false><<<reduce_blocks, reduce_kernel_thread_num, 0, stream>>>((float*)C_temp, (maca_bfloat16*)C, num_elem);
 +    }
 +
 +    return true;
@@ -13016,11 +12800,10 @@ index 000000000..c646e6198
 +
 diff --git a/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp b/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
 new file mode 100644
-index 000000000..398071852
+index 000000000..6e0fdb0e3
 --- /dev/null
 +++ b/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
-@@ -0,0 +1,634 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,633 @@
 +// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 +#pragma once
 +
@@ -13656,11 +13439,10 @@ index 000000000..398071852
 +}
 diff --git a/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp b/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
 new file mode 100644
-index 000000000..8a04b9035
+index 000000000..2a178d362
 --- /dev/null
 +++ b/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
-@@ -0,0 +1,216 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,215 @@
 +#pragma once
 +
 +#include <mc_runtime.h>
@@ -13878,11 +13660,10 @@ index 000000000..8a04b9035
 +}
 diff --git a/csrc/quantization/gptq/hgemv_selector.hpp b/csrc/quantization/gptq/hgemv_selector.hpp
 new file mode 100644
-index 000000000..2a7fef782
+index 000000000..b9d12a4e1
 --- /dev/null
 +++ b/csrc/quantization/gptq/hgemv_selector.hpp
-@@ -0,0 +1,288 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,287 @@
 +#include <memory>
 +#include <vector>
 +#include <algorithm>
@@ -14172,18 +13953,14 @@ index 000000000..2a7fef782
 +}
 \ No newline at end of file
 diff --git a/csrc/quantization/gptq/q_gemm.cu b/csrc/quantization/gptq/q_gemm.cu
-index 785f1a09c..5f9a2c6d2 100644
+index 6fad16e19..7a2c5eb6a 100644
 --- a/csrc/quantization/gptq/q_gemm.cu
 +++ b/csrc/quantization/gptq/q_gemm.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
- Adapted from https://github.com/turboderp/exllamav2 and
- https://github.com/qwopqwop200/GPTQ-for-LLaMa
-@@ -19,6 +20,15 @@ https://github.com/qwopqwop200/GPTQ-for-LLaMa
+@@ -19,6 +19,17 @@ https://github.com/qwopqwop200/GPTQ-for-LLaMa
  #include "qdq_4.cuh"
  #include "qdq_8.cuh"
  
++#ifdef USE_MACA
 +#include "hgemm_gptq.h"
 +#include "scalar_type.hpp"
 +
@@ -14192,16 +13969,16 @@ index 785f1a09c..5f9a2c6d2 100644
 +#include "hgemv_selector.hpp"
 +#include "Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp"
 +#include "Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp"
++#endif // USE_MACA
 +
  namespace vllm {
  namespace gptq {
  
-@@ -31,6 +41,8 @@ namespace gptq {
+@@ -31,6 +42,7 @@ namespace gptq {
  #define THREADS_X 32
  #define THREADS_Y 32
  #define DIVIDE(x, size) (((x) + (size) - 1) / (size))
 +#define QUANT_GROUP 128
-+#define BF16_HIGH_PRECISION
  
  #if defined(USE_ROCM)
    #include <hipblas/hipblas.h>
@@ -14364,14 +14141,6 @@ index 785f1a09c..5f9a2c6d2 100644
 -  gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
 -  gridDim.y = DIVIDE(size_m, m_count);
 -  gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
--
--  fp_gemm_half_q_half_gptq_kernel kernel =
--      pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
--
--  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
--  kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
--                                           b_gptq_scales, c, size_m, size_n,
--                                           size_k, groups, b_q_perm);
 +                                int m_count, int groups, int bit, bool m_sign, bool v_sign) {
 +  if ((bit == 4 || bit == 8) && m_sign && !v_sign){
 +        const int threads_n = 256;
@@ -14512,29 +14281,36 @@ index 785f1a09c..5f9a2c6d2 100644
 +         }
 +  }
 +  else {
-+	 dim3 blockDim, gridDim;
-+	 blockDim.x = BLOCK_KN_SIZE;
-+	 blockDim.y = 1;
-+	 blockDim.z = 1;
-+	 gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
-+	 gridDim.y = DIVIDE(size_m, m_count);
-+	 gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
-+
-+	 fp_gemm_half_q_half_gptq_kernel kernel =
-+	     pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
-+
-+	 const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-+	 kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
-+						  b_gptq_scales, c, size_m, size_n,
-+						  size_k, groups, b_q_perm);
++    dim3 blockDim, gridDim;
++    blockDim.x = BLOCK_KN_SIZE;
++    blockDim.y = 1;
++    blockDim.z = 1;
++    gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
++    gridDim.y = DIVIDE(size_m, m_count);
++    gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
+ 
+-  fp_gemm_half_q_half_gptq_kernel kernel =
+-      pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
++    fp_gemm_half_q_half_gptq_kernel kernel =
++        pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
+ 
+-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+-  kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
+-                                           b_gptq_scales, c, size_m, size_n,
+-                                           size_k, groups, b_q_perm);
++    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
++    kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
++                                            b_gptq_scales, c, size_m, size_n,
++                                            size_k, groups, b_q_perm);
 +  }
  }
  
  __global__ void reconstruct_exllama_8bit_kernel(
-@@ -1487,56 +1782,329 @@ void reconstruct_gptq(const uint32_t* b_q_weight, const uint32_t* b_gptq_qzeros,
+@@ -1487,56 +1782,339 @@ void reconstruct_gptq(const uint32_t* b_q_weight, const uint32_t* b_gptq_qzeros,
                                             width, groups, out);
  }
  
++
 +template <int tileK, int tileM, typename dtype>
 +__global__ void perm_a(dtype *output, const dtype *input, const int *idx, int k, int m, int lda) {
 +    int tid = threadIdx.x;
@@ -14562,7 +14338,7 @@ index 785f1a09c..5f9a2c6d2 100644
 +                      const quant_packed_tp *dB,
 +                      int ldb,
 +                      output_tp *dC,
-+		      float *dC_temp,
++		                  float *dC_temp,
 +                      int ldc,
 +                      quant_packed_tp *d_zeros,
 +                      input_tp *d_scales,
@@ -14677,6 +14453,7 @@ index 785f1a09c..5f9a2c6d2 100644
 +}
 +#endif
 +
++
 +template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
 +bool launch_gemm(int m,
 +                int n,
@@ -14687,7 +14464,7 @@ index 785f1a09c..5f9a2c6d2 100644
 +                const quant_packed_tp *dB,
 +                int ldb,
 +                output_tp *dC,
-+		float *dC_temp,
++                float *dC_temp,
 +                int ldc,
 +                quant_packed_tp *d_zeros,
 +                input_tp *d_scales,
@@ -14721,13 +14498,23 @@ index 785f1a09c..5f9a2c6d2 100644
 +    if (chunks > 0) {
 +        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
 +        if (is_gptq) {
-+            ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(real_m, n, k, quant_group, dA_actual, lda, dB, ldb, dC, dC_temp, ldc, d_zeros, d_scales, stream, chunks);
++            ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(
++                    real_m, n, k, quant_group, 
++                    dA_actual, lda, 
++                    dB, ldb, 
++                    dC, dC_temp, ldc, 
++                    d_zeros, d_scales, stream, chunks);
 +        }
 +    }
 +    if (rest_blocks_m > 0) {
 +        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
 +        if (is_gptq) {
-+            ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(m - m_offset, n, k, quant_group, dA_actual + lda * m_offset, lda, dB, ldb, dC + ldc * m_offset, dC_temp + ldc * m_offset, ldc, d_zeros, d_scales, stream, 1);
++            ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(
++                            m - m_offset, n, k, quant_group, 
++                            dA_actual + lda * m_offset, lda, 
++                            dB, ldb, 
++                            dC + ldc * m_offset, dC_temp, ldc, 
++                            d_zeros, d_scales, stream, 1);
 +        }
 +    }
 +
@@ -14746,7 +14533,6 @@ index 785f1a09c..5f9a2c6d2 100644
 +    #endif
 +#endif
 +
-+
 +    return ret;
 +}
 +
@@ -14775,7 +14561,6 @@ index 785f1a09c..5f9a2c6d2 100644
 +  }
 +
 +}
-+
  void gemm_half_q_half_cuda(cublasHandle_t cublas_handle, const half* a,
                             const uint32_t* b_q_weight,
                             const uint32_t* b_gptq_qzeros,
@@ -14783,26 +14568,59 @@ index 785f1a09c..5f9a2c6d2 100644
                             half* c, half* temp_dq, int size_m, int size_n,
 -                           int size_k, int groups, bool use_exllama, int bit) {
 +                           int size_k, int groups, bool use_exllama, int bit,
-+			   int group_size, half* perm_space) {
++                           int group_size, half* perm_space) {
    bool use_reconstruct;
 -  if (use_exllama) {
 -    use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
 -                       (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
--  } else {
++  bool opt = ((group_size == 128) || (group_size == 64));
++  if ((bit == 4) && opt) {
++          if ((size_m <= 2) && (group_size == 128)) {
++                  gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
++                                             b_g_idx, c, size_m, size_n, size_k,
++                                             BLOCK_M_SIZE_MAX, groups, bit, true, true);
++          } else {
++                  uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
++                  half* scales = const_cast<half*>(b_gptq_scales);
++                  launch_gemm<input_type, vllm::kU4B8.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
++                                  group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
++                                  b_g_idx, perm_space, true);
++          }
++  } else if ((bit == 8) && opt) {
++          uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
++          half* scales = const_cast<half*>(b_gptq_scales);
++          launch_gemm<input_type, vllm::kU8B128.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
++                          group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
++                          b_g_idx, perm_space, true);
+   } else {
 -    // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
 -    // we disabled them for now.
 -    use_reconstruct = (bit < 4 || size_m > MAX_ALT_GEMM_ROWS);
 -  }
 -  if (use_reconstruct) {
 -    // Reconstruct FP16 matrix, then cuBLAS
--    if (use_exllama) {
+     if (use_exllama) {
 -      reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
 -                          temp_dq, size_k, size_n, groups, bit);
--    } else {
++      use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
++                        (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
+     } else {
 -      reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
 -                       temp_dq, size_k, size_n, groups, bit);
--    }
--
++      // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
++      // we disabled them for now.
++      use_reconstruct = (bit < 4 || size_m > 0);
+     }
++    if (use_reconstruct) {
++      // Reconstruct FP16 matrix, then cuBLAS
++      if (use_exllama) {
++        reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
++                            temp_dq, size_k, size_n, groups, bit);
++      } else {
++        reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
++                        temp_dq, size_k, size_n, groups, bit);
++      }
+ 
 -    const half alpha = __float2half(1.0f);
 -    const half beta = __float2half(0.0f);
 -    cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
@@ -14818,64 +14636,17 @@ index 785f1a09c..5f9a2c6d2 100644
 -                                 b_g_idx, c, last_chunk, size_n, size_k,
 -                                 BLOCK_M_SIZE_MAX, groups, bit);
 -    }
--
--    if (last_chunk_size) {
--      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
--                                 b_gptq_qzeros, b_gptq_scales, b_g_idx,
--                                 c + last_chunk * size_n, last_chunk_size,
--                                 size_n, size_k, last_chunk_size, groups, bit);
--    }
-+  bool opt = ((group_size == 128) || (group_size == 64));
-+  if ((bit == 4) && opt) {
-+          if ((size_m <= 2) && (group_size == 128)) {
-+                  gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
-+                                             b_g_idx, c, size_m, size_n, size_k,
-+                                             BLOCK_M_SIZE_MAX, groups, bit, true, true);
-+          } else {
-+                  uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
-+                  half* scales = const_cast<half*>(b_gptq_scales);
-+                  launch_gemm<input_type, vllm::kU4B8.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
-+                                  group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
-+                                  b_g_idx, perm_space, true);
-+          }
-+  } else if ((bit == 8) && opt) {
-+          uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
-+          half* scales = const_cast<half*>(b_gptq_scales);
-+          launch_gemm<input_type, vllm::kU8B128.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
-+                          group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
-+                          b_g_idx, perm_space, true);
-   } else {
--    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
--                         c, size_m, size_n, size_k, bit);
-+	  if (use_exllama) {
-+	    use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
-+			       (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
-+	  } else {
-+	    // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
-+	    // we disabled them for now.
-+	    use_reconstruct = (bit < 4 || size_m > 0);
-+	  }
-+	  if (use_reconstruct) {
-+	    // Reconstruct FP16 matrix, then cuBLAS
-+	    if (use_exllama) {
-+	      reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-+				  temp_dq, size_k, size_n, groups, bit);
-+	    } else {
-+	      reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-+			       temp_dq, size_k, size_n, groups, bit);
-+	    }
-+
-+	    const half alpha = __float2half(1.0f);
-+	    const half beta = __float2half(0.0f);
-+	    cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
-+			&alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);
-+	  } else if (use_exllama) {
-+	    // Quantized matmul
-+	    int max_chunks = size_m / BLOCK_M_SIZE_MAX;
-+	    int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
-+	    int last_chunk_size = size_m - last_chunk;
-+
-+	    bool m_sign;
++      const half alpha = __float2half(1.0f);
++      const half beta = __float2half(0.0f);
++      cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
++                  &alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);
++    } else if (use_exllama) {
++      // Quantized matmul
++      int max_chunks = size_m / BLOCK_M_SIZE_MAX;
++      int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
++      int last_chunk_size = size_m - last_chunk;
++
++      bool m_sign;
 +            bool v_sign;
 +            if (group_size == 128) {
 +                    m_sign = size_m <= 50;
@@ -14885,33 +14656,41 @@ index 785f1a09c..5f9a2c6d2 100644
 +                    v_sign = false;
 +            }
 +
-+	    if (max_chunks) {
-+	      gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
-+					 b_g_idx, c, last_chunk, size_n, size_k,
-+					 BLOCK_M_SIZE_MAX, groups, bit, m_sign, v_sign);
-+	    }
-+
-+	    if (last_chunk_size) {
-+	      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
-+					 b_gptq_qzeros, b_gptq_scales, b_g_idx,
-+					 c + last_chunk * size_n, last_chunk_size,
-+					 size_n, size_k, last_chunk_size, groups, bit, m_sign, v_sign);
-+	    }
-+	  } else {
-+	    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-+				 c, size_m, size_n, size_k, bit);
-+	  }
++      if (max_chunks) {
++        gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
++                                  b_g_idx, c, last_chunk, size_n, size_k,
++                                  BLOCK_M_SIZE_MAX, groups, bit, m_sign, v_sign);
++      }
+ 
+-    if (last_chunk_size) {
+-      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
+-                                 b_gptq_qzeros, b_gptq_scales, b_g_idx,
+-                                 c + last_chunk * size_n, last_chunk_size,
+-                                 size_n, size_k, last_chunk_size, groups, bit);
++      if (last_chunk_size) {
++        gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
++                                  b_gptq_qzeros, b_gptq_scales, b_g_idx,
++                                  c + last_chunk * size_n, last_chunk_size,
++                                  size_n, size_k, last_chunk_size, groups, bit,  m_sign, v_sign);
++      }
++    } else {
++      gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
++                          c, size_m, size_n, size_k, bit);
+     }
+-  } else {
+-    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+-                         c, size_m, size_n, size_k, bit);
    }
  }
  
-@@ -1823,25 +2391,45 @@ void shuffle_exllama_weight(uint32_t* q_weight, int* q_perm, int height,
+@@ -1823,25 +2401,45 @@ void shuffle_exllama_weight(uint32_t* q_weight, int* q_perm, int height,
  torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
                          torch::Tensor b_gptq_qzeros,
                          torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
 -                        bool use_exllama, int64_t bit) {
-+                        bool use_exllama, int64_t bit, int64_t group_size, 
-+			torch::Tensor perm_space, torch::Tensor temp_space,
-+			bool dtype_bf16) {
++                        bool use_exllama, int64_t bit, int64_t group_size,
++                        torch::Tensor perm_space, torch::Tensor temp_space,
++                        bool dtype_bf16) {
    const at::cuda::OptionalCUDAGuard device_guard(device_of(a));
    auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());
 -  at::Tensor c = torch::empty({a.size(0), b_q_weight.size(1)}, options);
@@ -14946,7 +14725,7 @@ index 785f1a09c..5f9a2c6d2 100644
 +        bit, group_size,
 +        (__maca_bfloat16*)perm_space.data_ptr());
 +  } else {
-+      vllm::gptq::gemm_half_q_half_cuda(
++    vllm::gptq::gemm_half_q_half_cuda(
 +        at::cuda::getCurrentCUDABlasHandle(), (const half*)a.data_ptr(),
 +        (const uint32_t*)b_q_weight.data_ptr(),
 +        (const uint32_t*)b_gptq_qzeros.data_ptr(),
@@ -14965,34 +14744,30 @@ index 785f1a09c..5f9a2c6d2 100644
  }
  
 diff --git a/csrc/quantization/gptq/qdq_4.cuh b/csrc/quantization/gptq/qdq_4.cuh
-index 7f65d2d28..34714f9a5 100644
+index 7f65d2d28..6462584bc 100644
 --- a/csrc/quantization/gptq/qdq_4.cuh
 +++ b/csrc/quantization/gptq/qdq_4.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- /*
- Copied from https://github.com/turboderp/exllamav2
- */
-@@ -86,6 +87,7 @@ __forceinline__ __device__ void dequant_4bit_8_prep_zero(const uint32_t zero,
+@@ -85,7 +85,7 @@ __forceinline__ __device__ void dequant_4bit_8_prep_zero(const uint32_t zero,
+   y1y16[0] = __half2half2(y1);
    y1y16[1] = __half2half2(y16);
  }
- 
+-
 +typedef __NATIVE_VECTOR__(2, float) v2f;
  __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
                                                      half2 (&dq)[4],
                                                      half2 (&z1z16)[2],
-@@ -112,12 +114,50 @@ __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
+@@ -112,12 +112,50 @@ __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
      dq[2] = __hfma2(q2.as_half2, y1y16[0], z1z16[0]);
      dq[3] = __hfma2(q3.as_half2, y1y16[1], z1z16[1]);
    } else {
-+#if 0
++#ifndef USE_MACA
      dq[0] = __hadd2(q0.as_half2, z1z16[0]);  // half2( q[0] - z, q[1] - z )
      dq[1] = __hfma2(q1.as_half2, y1y16[1],
                      z1z16[1]);               // half2( q[2] - z, q[3] - z )
      dq[2] = __hadd2(q2.as_half2, z1z16[0]);  // half2( q[4] - z, q[5] - z )
      dq[3] = __hfma2(q3.as_half2, y1y16[1],
                      z1z16[1]);  // half2( q[6] - z, q[7] - z )
-+#endif
++#endif // USE_MACA
 +#if 1
 +    dq[0] = __hadd2(q0.as_half2, z1z16[0]);
 +    dq[2] = __hadd2(q2.as_half2, z1z16[0]);
@@ -15034,11 +14809,10 @@ index 7f65d2d28..34714f9a5 100644
  }  // namespace gptq
 diff --git a/csrc/quantization/gptq/scalar_type.hpp b/csrc/quantization/gptq/scalar_type.hpp
 new file mode 100644
-index 000000000..d17dea475
+index 000000000..46596ff49
 --- /dev/null
 +++ b/csrc/quantization/gptq/scalar_type.hpp
-@@ -0,0 +1,552 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+@@ -0,0 +1,551 @@
 +#pragma once
 +
 +#include <variant>
@@ -15590,2330 +15364,159 @@ index 000000000..d17dea475
 +static inline constexpr auto kFloat16Id = kFloat16.id();
 +#endif
 +};  // namespace vllm
-diff --git a/csrc/quantization/vectorization.cuh b/csrc/quantization/vectorization.cuh
-index 44c999130..2bf2c884a 100644
---- a/csrc/quantization/vectorization.cuh
-+++ b/csrc/quantization/vectorization.cuh
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #pragma once
- /**
-  * __device__ datatypes vectorized by 4
-@@ -5,8 +6,8 @@
- 
- // Include both AMD and NVIDIA fp8 types to avoid circular import
- // TODO(luka/varun) use FP8_TYPE instead after refactoring
--#include <c10/util/Float8_e4m3fnuz.h>
--#include <c10/util/Float8_e4m3fn.h>
-+//#include <c10/util/Float8_e4m3fnuz.h>
-+//#include <c10/util/Float8_e4m3fn.h>
- 
- namespace vllm {
- 
-@@ -21,9 +22,9 @@ struct __align__(8) vec4_t {
- 
- template <typename quant_type_t>
- struct __align__(4) q8x4_t {
--  static_assert(std::is_same_v<quant_type_t, int8_t> ||
--                std::is_same_v<quant_type_t, c10::Float8_e4m3fn> ||
--                std::is_same_v<quant_type_t, c10::Float8_e4m3fnuz>);
-+  //static_assert(std::is_same_v<quant_type_t, int8_t> ||
-+  //              std::is_same_v<quant_type_t, c10::Float8_e4m3fn> ||
-+  //              std::is_same_v<quant_type_t, c10::Float8_e4m3fnuz>);
-   quant_type_t x;
-   quant_type_t y;
-   quant_type_t z;
-diff --git a/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu b/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
-index 371de0950..a871383fa 100644
---- a/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
-+++ b/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
-@@ -1,3 +1,4 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- #include <cudaTypedefs.h>
- 
- #include <c10/cuda/CUDAGuard.h>
-@@ -30,7 +31,7 @@ void cutlass_scaled_sparse_mm(torch::Tensor& c, torch::Tensor const& a,
-                               torch::Tensor const& bt_meta,
-                               torch::Tensor const& a_scales,
-                               torch::Tensor const& b_scales,
--                              std::optional<torch::Tensor> const& bias) {
-+                              c10::optional<torch::Tensor> const& bias) {
-   // Checks for conformality
-   TORCH_CHECK(a.dim() == 2 && bt_nzs.dim() == 2 && c.dim() == 2);
-   TORCH_CHECK(c.size(1) == bt_nzs.size(0) && bt_nzs.size(1) * 2 == a.size(1) &&
-diff --git a/csrc/torch_bindings.cpp b/csrc/torch_bindings.cpp
-index c03806f43..175426438 100644
---- a/csrc/torch_bindings.cpp
-+++ b/csrc/torch_bindings.cpp
-@@ -1,3 +1,5 @@
-+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+
- #include "cache.h"
- #include "cuda_utils.h"
- #include "ops.h"
-@@ -40,16 +42,25 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-   ops.def(
-       "paged_attention_v2("
-       "    Tensor! out, Tensor! exp_sums, Tensor! max_logits,"
--      "    Tensor! tmp_out, Tensor query, Tensor key_cache,"
-+      "    Tensor tmp_out, Tensor block_count, Tensor query, Tensor key_cache,"
-       "    Tensor value_cache, int num_kv_heads, float scale,"
-       "    Tensor block_tables, Tensor seq_lens, int block_size,"
-       "    int max_seq_len, Tensor? alibi_slopes,"
-       "    str kv_cache_dtype, Tensor k_scale, Tensor v_scale,"
-       "    int tp_rank, int blocksparse_local_blocks,"
-       "    int blocksparse_vert_stride, int blocksparse_block_size,"
--      "    int blocksparse_head_sliding_step) -> ()");
-+      "    int blocksparse_head_sliding_step, bool count_init_once) -> ()");
-   ops.impl("paged_attention_v2", torch::kCUDA, &paged_attention_v2);
- 
-+  ops.def(
-+      "page_reshape_kv_cache("
-+      "    Tensor! key_cache, Tensor! value_cache,"
-+      "    Tensor! key_cache_new_layer, Tensor! value_cache_new_layer,"
-+      "    int num_seqs, int num_heads, int head_size, int num_kv_heads, int block_size,"
-+      "           str! kv_cache_dtype) -> ()");
-+  //ops.def("page_reshape_kv_cache", &page_reshape_kv_cache);
-+  ops.impl("page_reshape_kv_cache", torch::kCUDA, &page_reshape_kv_cache);
-+
-   // Activation ops
-   // Activation function used in SwiGLU.
-   ops.def("silu_and_mul(Tensor! out, Tensor input) -> ()");
-@@ -114,6 +125,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "float epsilon) -> ()");
-   ops.impl("fused_add_rms_norm", torch::kCUDA, &fused_add_rms_norm);
- 
-+// #ifdef MX_MACA
-   // Layernorm-quant
-   // Apply Root Mean Square (RMS) Normalization to the input tensor.
-   ops.def(
-@@ -138,6 +150,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "Tensor? scale_ub, Tensor!? residual) -> ()");
-   ops.impl("rms_norm_dynamic_per_token_quant", torch::kCUDA,
-            &rms_norm_dynamic_per_token_quant);
-+// #endif
- 
-   // Rotary embedding
-   // Apply GPT-NeoX or GPT-J style rotary embedding to query and key.
-@@ -158,6 +171,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-   ops.impl("batched_rotary_embedding", torch::kCUDA, &batched_rotary_embedding);
+diff --git a/csrc/torch_bindings.cpp b/csrc/torch_bindings.cpp
+index 1a1896b4c..cf326b922 100644
+--- a/csrc/torch_bindings.cpp
++++ b/csrc/torch_bindings.cpp
+@@ -222,6 +222,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
  
    // Quantization ops
-+#ifdef MX_MACA
  #ifndef USE_ROCM
++#ifndef USE_MACA
    // Quantized GEMM for AQLM.
    ops.def(
-@@ -172,18 +186,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "int[] codebook_partition_sizes) -> Tensor");
+       "aqlm_gemm(Tensor input, Tensor codes, Tensor codebooks, "
+@@ -236,21 +237,26 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+       "int[] codebook_partition_sizes) -> Tensor",
+       {stride_tag});
    ops.impl("aqlm_dequant", torch::kCUDA, &aqlm_dequant);
++#endif // USE_MACA
  
--  // Quantized GEMM for AWQ.
--  ops.def(
--      "awq_gemm(Tensor _in_feats, Tensor _kernel, Tensor _scaling_factors, "
--      "Tensor _zeros, SymInt split_k_iters) -> Tensor");
--  ops.impl("awq_gemm", torch::kCUDA, &awq_gemm);
--
--  // Dequantization for AWQ.
--  ops.def(
--      "awq_dequantize(Tensor _kernel, Tensor _scaling_factors, "
--      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor");
--  ops.impl("awq_dequantize", torch::kCUDA, &awq_dequantize);
--
+   // Quantized GEMM for AWQ.
+   ops.def(
+       "awq_gemm(Tensor _in_feats, Tensor _kernel, Tensor _scaling_factors, "
+-      "Tensor _zeros, SymInt split_k_iters) -> Tensor",
+-      {stride_tag});
++      "Tensor _zeros, SymInt split_k_iters, Tensor _temp_space, bool dtype_bf16) -> Tensor");
+   ops.impl("awq_gemm", torch::kCUDA, &awq_gemm);
+ 
+   // Dequantization for AWQ.
+   ops.def(
+       "awq_dequantize(Tensor _kernel, Tensor _scaling_factors, "
+-      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor",
+-      {stride_tag});
++      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor");
+   ops.impl("awq_dequantize", torch::kCUDA, &awq_dequantize);
+ 
++  // Convert AWQ to GPTQ
++  ops.def(
++      "awq_to_gptq_4bit(Tensor qweight) -> Tensor");
++  ops.impl("awq_to_gptq_4bit", torch::kCUDA, &awq_to_gptq_4bit);
++
++#ifndef USE_MACA
    // Note about marlin kernel 'workspace' arguments:
    // Technically these should be mutable since they are modified by the kernel.
    // But since they are set back to zero once the kernel is finished we can
-@@ -213,41 +215,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "SymInt size_m, SymInt size_n, SymInt size_k) -> Tensor");
+@@ -281,7 +287,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+       "SymInt size_m, SymInt size_n, SymInt size_k) -> Tensor",
+       {stride_tag});
    //  conditionally compiled so impl in source file
+-
++#endif // USE_MACA
+   // Machete (Dense) Optimized Mixed Precision GEMM for Hopper.
+   ops.def(
+       "machete_supported_schedules("
+@@ -319,6 +325,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+   ops.def("permute_cols(Tensor A, Tensor perm) -> Tensor");
+   ops.impl("permute_cols", torch::kCUDA, &permute_cols);
  
--  // Machete (Dense) Optimized Mixed Precision GEMM for Hopper.
--  ops.def(
--      "machete_supported_schedules("
--      "   ScalarType a_type,"
--      "   int b_type,"
--      "   ScalarType? maybe_group_scales_type,"
--      "   ScalarType? maybe_group_zeros_type,"
--      "   ScalarType? maybe_channel_scales_type,"
--      "   ScalarType? maybe_token_scales_type,"
--      "   ScalarType? maybe_out_type"
--      ") -> str[]");
--  ops.def(
--      "machete_mm("
--      "   Tensor A,"
--      "   Tensor B,"
--      "   int b_type,"
--      "   ScalarType? out_type,"
--      "   Tensor? group_scales,"
--      "   Tensor? group_zeros,"
--      "   int?    group_size,"
--      "   Tensor? channel_scales,"
--      "   Tensor? token_scales,"
--      "   str?    schedule"
--      ") -> Tensor");
--  ops.def(
--      "machete_prepack_B("
--      "   Tensor B,"
--      "   ScalarType a_type,"
--      "   int b_type,"
--      "   ScalarType? group_scales_type"
--      ") -> Tensor");
--  // conditionally compiled so impl registration is in source file
- 
--  ops.def("permute_cols(Tensor A, Tensor perm) -> Tensor");
--  ops.impl("permute_cols", torch::kCUDA, &permute_cols);
- 
++#ifndef USE_MACA
    // gptq_marlin Optimized Quantized GEMM for GPTQ.
    ops.def(
-@@ -271,21 +239,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-   // conditionally compiled so impl registrations are in source file
- #endif
+       "gptq_marlin_gemm(Tensor a, Tensor? c_or_none, Tensor b_q_weight, "
+@@ -374,8 +381,10 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+   ops.impl("ggml_moe_a8_vec", torch::kCUDA, &ggml_moe_a8_vec);
+ 
+   ops.def("ggml_moe_get_block_size", &ggml_moe_get_block_size);
++#endif // USE_MACA
  
--  // Dequantization for GGML.
--  ops.def("ggml_dequantize(Tensor W, int type, SymInt m, SymInt n) -> Tensor");
--  ops.impl("ggml_dequantize", torch::kCUDA, &ggml_dequantize);
--
--  // mmvq kernel for GGML.
--  ops.def(
--      "ggml_mul_mat_vec_a8(Tensor W, Tensor X, int type, SymInt row) "
--      "-> Tensor");
--  ops.impl("ggml_mul_mat_vec_a8", torch::kCUDA, &ggml_mul_mat_vec_a8);
--
--  // mmq kernel for GGML.
--  ops.def(
--      "ggml_mul_mat_a8(Tensor W, Tensor X, int type, SymInt row) -> Tensor");
--  ops.impl("ggml_mul_mat_a8", torch::kCUDA, &ggml_mul_mat_a8);
--
  #ifndef USE_ROCM
-   // fp8_marlin Optimized Quantized GEMM for FP8 weight-only.
++#ifndef USE_MACA
+   // marlin_qqq_gemm for QQQ.
    ops.def(
-@@ -302,23 +255,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "SymInt size_k) -> Tensor");
-   // conditionally compiled so impl registration is in source file
- 
--  // CUTLASS w8a8 GEMM, supporting symmetric per-tensor or per-row/column
--  // quantization, as well as bias
--  ops.def(
--      "cutlass_scaled_mm(Tensor! out, Tensor a,"
--      "                  Tensor b, Tensor a_scales,"
--      "                  Tensor b_scales, Tensor? bias) -> ()");
--  ops.impl("cutlass_scaled_mm", torch::kCUDA, &cutlass_scaled_mm);
--
--  // CUTLASS w8a8 GEMM, supporting asymmetric per-tensor or per-row/column
--  // quantization.
--  ops.def(
--      "cutlass_scaled_mm_azp(Tensor! out, Tensor a,"
--      "                  Tensor b, Tensor a_scales,"
--      "                  Tensor b_scales, Tensor azp_adj,"
--      "                  Tensor? azp, Tensor? bias) -> ()");
--  ops.impl("cutlass_scaled_mm_azp", torch::kCUDA, &cutlass_scaled_mm_azp);
--
+       "marlin_qqq_gemm(Tensor a, Tensor b_q_weight, "
+@@ -400,6 +409,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+       " Tensor problem_sizes, Tensor expert_offsets, Tensor sf_offsets) -> ()",
+       {stride_tag});
+   ops.impl("cutlass_fp4_group_mm", torch::kCUDA, &cutlass_fp4_group_mm);
++#endif // USE_MACA
+ 
+   // CUTLASS w8a8 GEMM, supporting symmetric per-tensor or per-row/column
+   // quantization, as well as bias
+@@ -420,6 +430,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+       {stride_tag});
+   ops.impl("cutlass_scaled_mm_azp", torch::kCUDA, &cutlass_scaled_mm_azp);
+ 
++#ifndef USE_MACA
    // Check if cutlass scaled_mm is supported for CUDA devices of the given
    // capability
    ops.def("cutlass_scaled_mm_supports_fp8(int cuda_device_capability) -> bool");
-@@ -353,6 +289,46 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "                              Tensor a) -> bool");
-   ops.impl("cutlass_sparse_compress_entry", &cutlass_sparse_compress_entry);
+@@ -498,6 +509,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+   // CUTLASS sparse matrix compressor
+   ops.def("cutlass_sparse_compress(Tensor a) -> Tensor[]");
+   ops.impl("cutlass_sparse_compress", &cutlass_sparse_compress);
++#endif // USE_MACA
  
-+
-+#endif
-+#endif
-+
-+ops.def("permute_cols(Tensor A, Tensor perm) -> Tensor");
-+ops.impl("permute_cols", torch::kCUDA, &permute_cols);
-+
-+  // Machete (Dense) Optimized Mixed Precision GEMM for Hopper.
-+  ops.def(
-+      "machete_supported_schedules("
-+      "   ScalarType a_type,"
-+      "   int b_type,"
-+      "   ScalarType? maybe_group_scales_type,"
-+      "   ScalarType? maybe_group_zeros_type,"
-+      "   ScalarType? maybe_channel_scales_type,"
-+      "   ScalarType? maybe_token_scales_type,"
-+      "   ScalarType? maybe_out_type"
-+      ") -> str[]");
-+  ops.def(
-+      "machete_mm("
-+      "   Tensor A,"
-+      "   Tensor B,"
-+      "   int b_type,"
-+      "   ScalarType? out_type,"
-+      "   Tensor? group_scales,"
-+      "   Tensor? group_zeros,"
-+      "   int?    group_size,"
-+      "   Tensor? channel_scales,"
-+      "   Tensor? token_scales,"
-+      "   str?    schedule"
-+      ") -> Tensor");
-+  ops.def(
-+      "machete_prepack_B("
-+      "   Tensor B,"
-+      "   ScalarType a_type,"
-+      "   int b_type,"
-+      "   ScalarType? group_scales_type"
-+      ") -> Tensor");
-+  // conditionally compiled so impl registration is in source file
-+
-   // Mamba selective scan kernel
+   // CUTLASS MLA decode
    ops.def(
-       "selective_scan_fwd(Tensor! u, Tensor! delta,"
-@@ -387,15 +363,64 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "bool silu_activation,"
-       "int pad_slot_id) -> ()");
-   ops.impl("causal_conv1d_fwd", torch::kCUDA, &causal_conv1d_fwd);
--#endif
-+
-+  // CUTLASS w8a8 GEMM, supporting symmetric per-tensor or per-row/column
-+  // quantization, as well as bias
-+  ops.def(
-+      "cutlass_scaled_mm(Tensor! out, Tensor a,"
-+      "                  Tensor b, Tensor a_scales,"
-+      "                  Tensor b_scales, Tensor? bias) -> ()");
-+  ops.impl("cutlass_scaled_mm", torch::kCUDA, &cutlass_scaled_mm);
-+
-+  // CUTLASS w8a8 GEMM, supporting asymmetric per-tensor or per-row/column
-+  // quantization.
-+  ops.def(
-+      "cutlass_scaled_mm_azp(Tensor! out, Tensor a,"
-+      "                  Tensor b, Tensor a_scales,"
-+      "                  Tensor b_scales, Tensor azp_adj,"
-+      "                  Tensor? azp, Tensor? bias) -> ()");
-+  ops.impl("cutlass_scaled_mm_azp", torch::kCUDA, &cutlass_scaled_mm_azp);
-+
-+  // Dequantization for GGML.
-+  ops.def("ggml_dequantize(Tensor W, int type, SymInt m, SymInt n) -> Tensor");
-+  ops.impl("ggml_dequantize", torch::kCUDA, &ggml_dequantize);
-+
-+  // mmvq kernel for GGML.
-+  ops.def(
-+      "ggml_mul_mat_vec_a8(Tensor W, Tensor X, int type, SymInt row) "
-+      "-> Tensor");
-+  ops.impl("ggml_mul_mat_vec_a8", torch::kCUDA, &ggml_mul_mat_vec_a8);
-+
-+  // mmq kernel for GGML.
-+  ops.def(
-+      "ggml_mul_mat_a8(Tensor W, Tensor X, int type, SymInt row) -> Tensor");
-+  ops.impl("ggml_mul_mat_a8", torch::kCUDA, &ggml_mul_mat_a8);
-+
-+
-+  // Quantized GEMM for AWQ.
-+  ops.def(
-+      "awq_gemm(Tensor _in_feats, Tensor _kernel, Tensor _scaling_factors, "
-+      "Tensor _zeros, SymInt split_k_iters, Tensor _temp_space, bool dtype_bf16) -> Tensor");
-+  ops.impl("awq_gemm", torch::kCUDA, &awq_gemm);
-+
-+  // Dequantization for AWQ.
-+  ops.def(
-+      "awq_dequantize(Tensor _kernel, Tensor _scaling_factors, "
-+      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor");
-+  ops.impl("awq_dequantize", torch::kCUDA, &awq_dequantize);
-+  
-+  // Convert AWQ to GPTQ
-+  ops.def(
-+      "awq_to_gptq_4bit(Tensor qweight) -> Tensor");
-+  ops.impl("awq_to_gptq_4bit", torch::kCUDA, &awq_to_gptq_4bit);
+@@ -506,6 +518,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+       "                   Tensor page_table, float scale) -> ()");
+   ops.impl("cutlass_mla_decode", torch::kCUDA, &cutlass_mla_decode);
+ 
++#ifndef USE_MACA
+   // Compute NVFP4 block quantized tensor.
+   ops.def(
+       "scaled_fp4_quant(Tensor! output, Tensor input,"
+@@ -523,6 +536,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+   // of the given capability
+   ops.def("cutlass_scaled_mm_supports_fp4(int cuda_device_capability) -> bool");
+   ops.impl("cutlass_scaled_mm_supports_fp4", &cutlass_scaled_mm_supports_fp4);
++#endif // USE_MACA
+ #endif
  
    // Quantized GEMM for GPTQ.
-   // Note: even though the C++ inferred schema is correct for this op, it seems
+@@ -530,9 +544,8 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
    // to prevent the meta function registry.
    ops.def(
        "gptq_gemm(Tensor a, Tensor b_q_weight, Tensor b_gptq_qzeros, "
 -      "Tensor b_gptq_scales, Tensor b_g_idx, bool use_exllama, int bit) "
--      "-> Tensor");
+-      "-> Tensor",
+-      {stride_tag});
 +      "Tensor b_gptq_scales, Tensor b_g_idx, bool use_exllama, int bit, int group_size, Tensor perm_space, "
 +      "Tensor temp_space, bool dtype_bf16)-> Tensor");
    ops.impl("gptq_gemm", torch::kCUDA, &gptq_gemm);
  
    // Post processing for GPTQ.
-@@ -415,6 +440,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-       "()");
-   ops.impl("dynamic_scaled_fp8_quant", torch::kCUDA, &dynamic_scaled_fp8_quant);
- 
-+// #ifdef MX_MACA
-   // Compute dynamic-per-token FP8 quantized tensor and scaling factor.
-   ops.def(
-       "dynamic_per_token_scaled_fp8_quant(Tensor! result, Tensor input, "
-@@ -423,6 +449,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
-   ops.impl("dynamic_per_token_scaled_fp8_quant", torch::kCUDA,
-            &dynamic_per_token_scaled_fp8_quant);
+@@ -572,6 +585,16 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
+       "Tensor!? azp) -> ()");
+   ops.impl("dynamic_scaled_int8_quant", torch::kCUDA,
+            &dynamic_scaled_int8_quant);
++  ops.def(
++      "dynamic_scaled_int8_mask_quant(Tensor! result, Tensor input, Tensor mask, Tensor! scale, "
++      "Tensor!? azp) -> ()");
++  ops.impl("dynamic_scaled_int8_mask_quant", torch::kCUDA,
++           &dynamic_scaled_int8_mask_quant);
++  ops.def(
++    "fused_silu_mul_dq_mask_quant_pack(Tensor! out, "
++    "Tensor input,"
++    "Tensor mask) -> ()");
++  ops.impl("fused_silu_mul_dq_mask_quant_pack", torch::kCUDA, &fused_silu_mul_dq_mask_quant_pack);
  
-+// #endif
-   // Compute int8 quantized tensor for given scaling factor.
+   // Mamba selective scan kernel
    ops.def(
-       "static_scaled_int8_quant(Tensor! result, Tensor input, Tensor scale,"
-@@ -463,6 +490,15 @@ TORCH_LIBRARY_EXPAND(CONCAT(TORCH_EXTENSION_NAME, _cache_ops), cache_ops) {
-       "                  Tensor k_scale, Tensor v_scale) -> ()");
-   cache_ops.impl("reshape_and_cache", torch::kCUDA, &reshape_and_cache);
- 
-+  cache_ops.def(
-+      "reshape_and_cache_new(Tensor key, Tensor value,"
-+      "                  Tensor! key_cache, Tensor! value_cache,"
-+      "                  Tensor slot_mapping,"
-+      "                  str kv_cache_dtype,"
-+      "                  float kv_scale,"
-+      "                  float v_scale) -> ()");
-+  cache_ops.impl("reshape_and_cache_new", torch::kCUDA, &reshape_and_cache_new);
-+
-   // Reshape the key and value tensors and cache them.
-   cache_ops.def(
-       "reshape_and_cache_flash(Tensor key, Tensor value,"
-diff --git a/docs/source/models/supported_models.md b/docs/source/models/supported_models.md
-index 32f3e9def..acb387594 100644
---- a/docs/source/models/supported_models.md
-+++ b/docs/source/models/supported_models.md
-@@ -1,971 +1,981 @@
--(supported-models)=
--
--# List of Supported Models
--
--vLLM supports generative and pooling models across various tasks.
--If a model supports more than one task, you can set the task via the `--task` argument.
--
--For each task, we list the model architectures that have been implemented in vLLM.
--Alongside each architecture, we include some popular models that use it.
--
--## Loading a Model
--
--### HuggingFace Hub
--
--By default, vLLM loads models from [HuggingFace (HF) Hub](https://huggingface.co/models).
--
--To determine whether a given model is supported, you can check the `config.json` file inside the HF repository.
--If the `"architectures"` field contains a model architecture listed below, then it should be supported in theory.
--
--:::{tip}
--The easiest way to check if your model is really supported at runtime is to run the program below:
--
--```python
--from vllm import LLM
--
--# For generative models (task=generate) only
--llm = LLM(model=..., task="generate")  # Name or path of your model
--output = llm.generate("Hello, my name is")
--print(output)
--
--# For pooling models (task={embed,classify,reward,score}) only
--llm = LLM(model=..., task="embed")  # Name or path of your model
--output = llm.encode("Hello, my name is")
--print(output)
--```
--
--If vLLM successfully returns text (for generative models) or hidden states (for pooling models), it indicates that your model is supported.
--:::
--
--Otherwise, please refer to [Adding a New Model](#new-model) for instructions on how to implement your model in vLLM.
--Alternatively, you can [open an issue on GitHub](https://github.com/vllm-project/vllm/issues/new/choose) to request vLLM support.
--
--### Transformers fallback
--
--After the merge of <gh-pr:11330>, `vllm` can fallback to models that are available in `transformers`. This does not work for all models for now, but most decoder language models are supported, and vision language model support is planned!
--
--To check if the backend is `transformers`, you can simply do this:
--
--```python 
--from vllm import LLM
--llm = LLM(model=..., task="generate")  # Name or path of your model
--llm.apply_model(lambda model: print(model.__class__))
--```
--
--If it is `TransformersModel` then it means it's based on `transformers`!
--
--#### Supported features
--
--##### LORA and quantization
--
--Both are not supported yet! Make sure to open an issue and we'll work on this together with the `transformers` team!
--
--Usually `transformers` model load weights via the `load_adapters` API, that depends on PEFT. We need to work a bit to either use this api (for now this would result in some weights not being marked as loaded) or replace modules accordingly.
--
--Hints as to how this would look like:
--
--```python
--class TransformersModel(nn.Module, SupportsLoRA):
--  def __init__(*):
--    ...
--    self.model.load_adapter(vllm_config.load_config.model_loader_extra_config["qlora_adapter_name_or_path"])
--```
--
--Blocker is that you need to specify supported lora layers, when we would ideally want to load whatever is inside the checkpoint!
--
--##### Remote code
--
--This fallback also means that any model on the hub that can be used in `transformers` with `trust_remote_code=True` that correctly implements attention can be used in production!
--
--```python 
--from vllm import LLM
--llm = LLM(model=..., task="generate", trust_remote_code=True)  # Name or path of your model
--llm.apply_model(lambda model: print(model.__class__))
--```
--
--A model just needs the following two things:
--
--```python
--from transformers import PreTrainedModel
--from torch import nn
--
--class MyAttention(nn.Module):
--
--  def forward(self, hidden_states, **kwargs): # <- kwargs are required
--
--    ...
--    attention_interface = attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
--    attn_output, attn_weights = attention_interface(
--      self,
--      query_states,
--      key_states,
--      value_states,
--      **kwargs,
--    )
--    ...
--
--class MyModel(PreTrainedModel):
--  _supports_attention_backend = True
--```
--
--Here is what happens in the background:
--
--1. The config is loaded
--2. `MyModel` python class is loaded from the `auto_map`, and we check that the model `_supports_attention_backend`.
--3. The `TransformersModel` backend is used. See `/model_executors/models/transformers`, which leverage `self.config._attn_implementation = "vllm"`, thus the need to use `ALL_ATTENTION_FUNCTION`.
--
--That's it!
--
--### ModelScope
--
--To use models from [ModelScope](https://www.modelscope.cn) instead of HuggingFace Hub, set an environment variable:
--
--```shell
--export VLLM_USE_MODELSCOPE=True
--```
--
--And use with `trust_remote_code=True`.
--
--```python
--from vllm import LLM
--
--llm = LLM(model=..., revision=..., task=..., trust_remote_code=True)
--
--# For generative models (task=generate) only
--output = llm.generate("Hello, my name is")
--print(output)
--
--# For pooling models (task={embed,classify,reward,score}) only
--output = llm.encode("Hello, my name is")
--print(output)
--```
--
--## List of Text-only Language Models
--
--### Generative Models
--
--See [this page](#generative-models) for more information on how to use generative models.
--
--#### Text Generation (`--task generate`)
--
--:::{list-table}
--:widths: 25 25 50 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--- * `AquilaForCausalLM`
--  * Aquila, Aquila2
--  * `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.
--  * 
--  * 
--- * `ArcticForCausalLM`
--  * Arctic
--  * `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc.
--  *
--  * 
--- * `BaiChuanForCausalLM`
--  * Baichuan2, Baichuan
--  * `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.
--  * 
--  * 
--- * `BloomForCausalLM`
--  * BLOOM, BLOOMZ, BLOOMChat
--  * `bigscience/bloom`, `bigscience/bloomz`, etc.
--  *
--  * 
--- * `BartForConditionalGeneration`
--  * BART
--  * `facebook/bart-base`, `facebook/bart-large-cnn`, etc.
--  *
--  *
--- * `ChatGLMModel`
--  * ChatGLM
--  * `THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.
--  * 
--  * 
--- * `CohereForCausalLM`, `Cohere2ForCausalLM`
--  * Command-R
--  * `CohereForAI/c4ai-command-r-v01`, `CohereForAI/c4ai-command-r7b-12-2024`, etc.
--  * 
--  * 
--- * `DbrxForCausalLM`
--  * DBRX
--  * `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc.
--  *
--  * 
--- * `DeciLMForCausalLM`
--  * DeciLM
--  * `Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc.
--  *
--  * 
--- * `DeepseekForCausalLM`
--  * DeepSeek
--  * `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat` etc.
--  *
--  * 
--- * `DeepseekV2ForCausalLM`
--  * DeepSeek-V2
--  * `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat` etc.
--  *
--  * 
--- * `DeepseekV3ForCausalLM`
--  * DeepSeek-V3
--  * `deepseek-ai/DeepSeek-V3-Base`, `deepseek-ai/DeepSeek-V3` etc.
--  *
--  * 
--- * `ExaoneForCausalLM`
--  * EXAONE-3
--  * `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc.
--  * 
--  * 
--- * `FalconForCausalLM`
--  * Falcon
--  * `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.
--  *
--  * 
--- * `FalconMambaForCausalLM`
--  * FalconMamba
--  * `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc.
--  * 
--  * 
--- * `GemmaForCausalLM`
--  * Gemma
--  * `google/gemma-2b`, `google/gemma-7b`, etc.
--  * 
--  * 
--- * `Gemma2ForCausalLM`
--  * Gemma2
--  * `google/gemma-2-9b`, `google/gemma-2-27b`, etc.
--  * 
--  * 
--- * `GlmForCausalLM`
--  * GLM-4
--  * `THUDM/glm-4-9b-chat-hf`, etc.
--  * 
--  * 
--- * `GPT2LMHeadModel`
--  * GPT-2
--  * `gpt2`, `gpt2-xl`, etc.
--  *
--  * 
--- * `GPTBigCodeForCausalLM`
--  * StarCoder, SantaCoder, WizardCoder
--  * `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc.
--  * 
--  * 
--- * `GPTJForCausalLM`
--  * GPT-J
--  * `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.
--  *
--  * 
--- * `GPTNeoXForCausalLM`
--  * GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM
--  * `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.
--  *
--  * 
--- * `GraniteForCausalLM`
--  * Granite 3.0, Granite 3.1, PowerLM
--  * `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc.
--  * 
--  * 
--- * `GraniteMoeForCausalLM`
--  * Granite 3.0 MoE, PowerMoE
--  * `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc.
--  * 
--  * 
--- * `GritLM`
--  * GritLM
--  * `parasail-ai/GritLM-7B-vllm`.
--  * 
--  * 
--- * `InternLMForCausalLM`
--  * InternLM
--  * `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.
--  * 
--  * 
--- * `InternLM2ForCausalLM`
--  * InternLM2
--  * `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc.
--  * 
--  * 
--- * `InternLM3ForCausalLM`
--  * InternLM3
--  * `internlm/internlm3-8b-instruct`, etc.
--  * 
--  * 
--- * `JAISLMHeadModel`
--  * Jais
--  * `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc.
--  *
--  * 
--- * `JambaForCausalLM`
--  * Jamba
--  * `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc.
--  * 
--  * 
--- * `LlamaForCausalLM`
--  * Llama 3.1, Llama 3, Llama 2, LLaMA, Yi
--  * `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc.
--  * 
--  * 
--- * `MambaForCausalLM`
--  * Mamba
--  * `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc.
--  *
--  * 
--- * `MiniCPMForCausalLM`
--  * MiniCPM
--  * `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc.
--  * 
--  * 
--- * `MiniCPM3ForCausalLM`
--  * MiniCPM3
--  * `openbmb/MiniCPM3-4B`, etc.
--  * 
--  * 
--- * `MistralForCausalLM`
--  * Mistral, Mistral-Instruct
--  * `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.
--  * 
--  * 
--- * `MixtralForCausalLM`
--  * Mixtral-8x7B, Mixtral-8x7B-Instruct
--  * `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc.
--  * 
--  * 
--- * `MPTForCausalLM`
--  * MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter
--  * `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc.
--  *
--  * 
--- * `NemotronForCausalLM`
--  * Nemotron-3, Nemotron-4, Minitron
--  * `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc.
--  * 
--  * 
--- * `OLMoForCausalLM`
--  * OLMo
--  * `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc.
--  *
--  * 
--- * `OLMo2ForCausalLM`
--  * OLMo2
--  * `allenai/OLMo2-7B-1124`, etc.
--  *
--  * 
--- * `OLMoEForCausalLM`
--  * OLMoE
--  * `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc.
--  * 
--  * 
--- * `OPTForCausalLM`
--  * OPT, OPT-IML
--  * `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.
--  *
--  * 
--- * `OrionForCausalLM`
--  * Orion
--  * `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc.
--  *
--  * 
--- * `PhiForCausalLM`
--  * Phi
--  * `microsoft/phi-1_5`, `microsoft/phi-2`, etc.
--  * 
--  * 
--- * `Phi3ForCausalLM`
--  * Phi-4, Phi-3
--  * `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc.
--  * 
--  * 
--- * `Phi3SmallForCausalLM`
--  * Phi-3-Small
--  * `microsoft/Phi-3-small-8k-instruct`, `microsoft/Phi-3-small-128k-instruct`, etc.
--  *
--  * 
--- * `PhiMoEForCausalLM`
--  * Phi-3.5-MoE
--  * `microsoft/Phi-3.5-MoE-instruct`, etc.
--  * 
--  * 
--- * `PersimmonForCausalLM`
--  * Persimmon
--  * `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc.
--  *
--  * 
--- * `QWenLMHeadModel`
--  * Qwen
--  * `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.
--  * 
--  * 
--- * `Qwen2ForCausalLM`
--  * QwQ, Qwen2
--  * `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc.
--  * 
--  * 
--- * `Qwen2MoeForCausalLM`
--  * Qwen2MoE
--  * `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.
--  *
--  * 
--- * `StableLmForCausalLM`
--  * StableLM
--  * `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.
--  *
--  * 
--- * `Starcoder2ForCausalLM`
--  * Starcoder2
--  * `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc.
--  *
--  * 
--- * `SolarForCausalLM`
--  * Solar Pro
--  * `upstage/solar-pro-preview-instruct`, etc.
--  * 
--  * 
--- * `TeleChat2ForCausalLM`
--  * TeleChat2
--  * `TeleAI/TeleChat2-3B`, `TeleAI/TeleChat2-7B`, `TeleAI/TeleChat2-35B`, etc.
--  * 
--  * 
--- * `XverseForCausalLM`
--  * XVERSE
--  * `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc.
--  * 
--  * 
--:::
--
--:::{note}
--Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.
--:::
--
--### Pooling Models
--
--See [this page](pooling-models) for more information on how to use pooling models.
--
--:::{important}
--Since some model architectures support both generative and pooling tasks,
--you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
--:::
--
--#### Text Embedding (`--task embed`)
--
--:::{list-table}
--:widths: 25 25 50 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--- * `BertModel`
--  * BERT-based
--  * `BAAI/bge-base-en-v1.5`, etc.
--  *
--  *
--- * `Gemma2Model`
--  * Gemma2-based
--  * `BAAI/bge-multilingual-gemma2`, etc.
--  *
--  * 
--- * `GritLM`
--  * GritLM
--  * `parasail-ai/GritLM-7B-vllm`.
--  * 
--  * 
--- * `LlamaModel`, `LlamaForCausalLM`, `MistralModel`, etc.
--  * Llama-based
--  * `intfloat/e5-mistral-7b-instruct`, etc.
--  * 
--  * 
--- * `Qwen2Model`, `Qwen2ForCausalLM`
--  * Qwen2-based
--  * `ssmits/Qwen2-7B-Instruct-embed-base` (see note), `Alibaba-NLP/gte-Qwen2-7B-instruct` (see note), etc.
--  * 
--  * 
--- * `RobertaModel`, `RobertaForMaskedLM`
--  * RoBERTa-based
--  * `sentence-transformers/all-roberta-large-v1`, `sentence-transformers/all-roberta-large-v1`, etc.
--  *
--  *
--- * `XLMRobertaModel`
--  * XLM-RoBERTa-based
--  * `intfloat/multilingual-e5-large`, etc.
--  *
--  *
--:::
--
--:::{note}
--`ssmits/Qwen2-7B-Instruct-embed-base` has an improperly defined Sentence Transformers config.
--You should manually set mean pooling by passing `--override-pooler-config '{"pooling_type": "MEAN"}'`.
--:::
--
--:::{note}
--Unlike base Qwen2, `Alibaba-NLP/gte-Qwen2-7B-instruct` uses bi-directional attention.
--You can set `--hf-overrides '{"is_causal": false}'` to change the attention mask accordingly.
--
--On the other hand, its 1.5B variant (`Alibaba-NLP/gte-Qwen2-1.5B-instruct`) uses causal attention
--despite being described otherwise on its model card.
--
--Regardless of the variant, you need to enable `--trust-remote-code` for the correct tokenizer to be
--loaded. See [relevant issue on HF Transformers](https://github.com/huggingface/transformers/issues/34882).
--:::
--
--If your model is not in the above list, we will try to automatically convert the model using
--{func}`~vllm.model_executor.models.adapters.as_embedding_model`. By default, the embeddings
--of the whole prompt are extracted from the normalized hidden state corresponding to the last token.
--
--#### Reward Modeling (`--task reward`)
--
--:::{list-table}
--:widths: 25 25 50 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--- * `InternLM2ForRewardModel`
--  * InternLM2-based
--  * `internlm/internlm2-1_8b-reward`, `internlm/internlm2-7b-reward`, etc.
--  * 
--  * 
--- * `LlamaForCausalLM`
--  * Llama-based
--  * `peiyi9979/math-shepherd-mistral-7b-prm`, etc.
--  * 
--  * 
--- * `Qwen2ForRewardModel`
--  * Qwen2-based
--  * `Qwen/Qwen2.5-Math-RM-72B`, etc.
--  * 
--  * 
--- * `Qwen2ForProcessRewardModel`
--  * Qwen2-based
--  * `Qwen/Qwen2.5-Math-PRM-7B`, `Qwen/Qwen2.5-Math-PRM-72B`, etc.
--  * 
--  * 
--:::
--
--If your model is not in the above list, we will try to automatically convert the model using
--{func}`~vllm.model_executor.models.adapters.as_reward_model`. By default, we return the hidden states of each token directly.
--
--:::{important}
--For process-supervised reward models such as `peiyi9979/math-shepherd-mistral-7b-prm`, the pooling config should be set explicitly,
--e.g.: `--override-pooler-config '{"pooling_type": "STEP", "step_tag_id": 123, "returned_token_ids": [456, 789]}'`.
--:::
--
--#### Classification (`--task classify`)
--
--:::{list-table}
--:widths: 25 25 50 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--- * `JambaForSequenceClassification`
--  * Jamba
--  * `ai21labs/Jamba-tiny-reward-dev`, etc.
--  * 
--  * 
--- * `Qwen2ForSequenceClassification`
--  * Qwen2-based
--  * `jason9693/Qwen2.5-1.5B-apeach`, etc.
--  * 
--  * 
--:::
--
--If your model is not in the above list, we will try to automatically convert the model using
--{func}`~vllm.model_executor.models.adapters.as_classification_model`. By default, the class probabilities are extracted from the softmaxed hidden state corresponding to the last token.
--
--#### Sentence Pair Scoring (`--task score`)
--
--:::{list-table}
--:widths: 25 25 50 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--- * `BertForSequenceClassification`
--  * BERT-based
--  * `cross-encoder/ms-marco-MiniLM-L-6-v2`, etc.
--  *
--  *
--- * `RobertaForSequenceClassification`
--  * RoBERTa-based
--  * `cross-encoder/quora-roberta-base`, etc.
--  *
--  *
--- * `XLMRobertaForSequenceClassification`
--  * XLM-RoBERTa-based
--  * `BAAI/bge-reranker-v2-m3`, etc.
--  *
--  *
--:::
--
--(supported-mm-models)=
--
--## List of Multimodal Language Models
--
--The following modalities are supported depending on the model:
--
--- **T**ext
--- **I**mage
--- **V**ideo
--- **A**udio
--
--Any combination of modalities joined by `+` are supported.
--
--- e.g.: `T + I` means that the model supports text-only, image-only, and text-with-image inputs.
--
--On the other hand, modalities separated by `/` are mutually exclusive.
--
--- e.g.: `T / I` means that the model supports text-only and image-only inputs, but not text-with-image inputs.
--
--See [this page](#multimodal-inputs) on how to pass multi-modal inputs to the model.
--
--:::{important}
--To enable multiple multi-modal items per text prompt, you have to set `limit_mm_per_prompt` (offline inference)
--or `--limit-mm-per-prompt` (online serving). For example, to enable passing up to 4 images per text prompt:
--
--Offline inference:
--
--```python
--llm = LLM(
--    model="Qwen/Qwen2-VL-7B-Instruct",
--    limit_mm_per_prompt={"image": 4},
--)
--```
--
--Online serving:
--
--```bash
--vllm serve Qwen/Qwen2-VL-7B-Instruct --limit-mm-per-prompt image=4
--```
--
--:::
--
--:::{note}
--vLLM currently only supports adding LoRA to the language backbone of multimodal models.
--:::
--
--### Generative Models
--
--See [this page](#generative-models) for more information on how to use generative models.
--
--#### Text Generation (`--task generate`)
--
--:::{list-table}
--:widths: 25 25 15 20 5 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Inputs
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--  * [V1](gh-issue:8779)
--- * `AriaForConditionalGeneration`
--  * Aria
--  * T + I<sup>+</sup>
--  * `rhymes-ai/Aria`
--  *
--  * 
--  * 
--- * `Blip2ForConditionalGeneration`
--  * BLIP-2
--  * T + I<sup>E</sup>
--  * `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc.
--  *
--  * 
--  * 
--- * `ChameleonForConditionalGeneration`
--  * Chameleon
--  * T + I
--  * `facebook/chameleon-7b` etc.
--  *
--  * 
--  * 
--- * `DeepseekVLV2ForCausalLM`
--  * DeepSeek-VL2
--  * T + I<sup>+</sup>
--  * `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2` etc. (see note)
--  *
--  * 
--  * 
--- * `FuyuForCausalLM`
--  * Fuyu
--  * T + I
--  * `adept/fuyu-8b` etc.
--  *
--  * 
--  * 
--- * `ChatGLMModel`
--  * GLM-4V
--  * T + I
--  * `THUDM/glm-4v-9b` etc.
--  * 
--  * 
--  *
--- * `H2OVLChatModel`
--  * H2OVL
--  * T + I<sup>E+</sup>
--  * `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc.
--  *
--  * 
--  * \*
--- * `Idefics3ForConditionalGeneration`
--  * Idefics3
--  * T + I
--  * `HuggingFaceM4/Idefics3-8B-Llama3` etc.
--  * 
--  *
--  * 
--- * `InternVLChatModel`
--  * InternVL 2.5, Mono-InternVL, InternVL 2.0
--  * T + I<sup>E+</sup>
--  * `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc.
--  *
--  * 
--  * 
--- * `LlavaForConditionalGeneration`
--  * LLaVA-1.5
--  * T + I<sup>E+</sup>
--  * `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), etc.
--  *
--  * 
--  * 
--- * `LlavaNextForConditionalGeneration`
--  * LLaVA-NeXT
--  * T + I<sup>E+</sup>
--  * `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc.
--  *
--  * 
--  * 
--- * `LlavaNextVideoForConditionalGeneration`
--  * LLaVA-NeXT-Video
--  * T + V
--  * `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc.
--  *
--  * 
--  * 
--- * `LlavaOnevisionForConditionalGeneration`
--  * LLaVA-Onevision
--  * T + I<sup>+</sup> + V<sup>+</sup>
--  * `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc.
--  *
--  * 
--  * 
--- * `MiniCPMO`
--  * MiniCPM-O
--  * T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup>
--  * `openbmb/MiniCPM-o-2_6`, etc.
--  * 
--  * 
--  *
--- * `MiniCPMV`
--  * MiniCPM-V
--  * T + I<sup>E+</sup> + V<sup>E+</sup>
--  * `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc.
--  * 
--  * 
--  *
--- * `MllamaForConditionalGeneration`
--  * Llama 3.2
--  * T + I<sup>+</sup>
--  * `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc.
--  *
--  *
--  *
--- * `MolmoForCausalLM`
--  * Molmo
--  * T + I
--  * `allenai/Molmo-7B-D-0924`, `allenai/Molmo-72B-0924`, etc.
--  * 
--  * 
--  * 
--- * `NVLM_D_Model`
--  * NVLM-D 1.0
--  * T + I<sup>+</sup>
--  * `nvidia/NVLM-D-72B`, etc.
--  *
--  * 
--  * 
--- * `PaliGemmaForConditionalGeneration`
--  * PaliGemma, PaliGemma 2
--  * T + I<sup>E</sup>
--  * `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc.
--  *
--  * 
--  *
--- * `Phi3VForCausalLM`
--  * Phi-3-Vision, Phi-3.5-Vision
--  * T + I<sup>E+</sup>
--  * `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc.
--  *
--  * 
--  * 
--- * `PixtralForConditionalGeneration`
--  * Pixtral
--  * T + I<sup>+</sup>
--  * `mistralai/Pixtral-12B-2409`, `mistral-community/pixtral-12b` (see note), etc.
--  *
--  * 
--  * 
--- * `QWenLMHeadModel`
--  * Qwen-VL
--  * T + I<sup>E+</sup>
--  * `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc.
--  * 
--  * 
--  * 
--- * `Qwen2AudioForConditionalGeneration`
--  * Qwen2-Audio
--  * T + A<sup>+</sup>
--  * `Qwen/Qwen2-Audio-7B-Instruct`
--  *
--  * 
--  * 
--- * `Qwen2VLForConditionalGeneration`
--  * QVQ, Qwen2-VL
--  * T + I<sup>E+</sup> + V<sup>E+</sup>
--  * `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc.
--  * 
--  * 
--  * 
--- * `Qwen2_5_VLForConditionalGeneration`
--  * Qwen2.5-VL
--  * T + I<sup>E+</sup> + V<sup>E+</sup>
--  * `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc.
--  *
--  * 
--  * 
--- * `UltravoxModel`
--  * Ultravox
--  * T + A<sup>E+</sup>
--  * `fixie-ai/ultravox-v0_3`
--  * 
--  * 
--  * 
--:::
--
--<sup>E</sup> Pre-computed embeddings can be inputted for this modality.  
--<sup>+</sup> Multiple items can be inputted per text prompt for this modality.
--
--:::{note}
--To use DeepSeek-VL2 series models, you have to pass `--hf_overrides '{"architectures": ["DeepseekVLV2ForCausalLM"]}'` when running vLLM.
--:::
--
--:::{note}
--H2O-VL series models will be available in V1 once we support backends other than FlashAttention.
--:::
--
--:::{note}
--To use `TIGER-Lab/Mantis-8B-siglip-llama3`, you have to pass `--hf_overrides '{"architectures": ["MantisForConditionalGeneration"]}'` when running vLLM.
--:::
--
--:::{note}
--The official `openbmb/MiniCPM-V-2` doesn't work yet, so we need to use a fork (`HwwwH/MiniCPM-V-2`) for now.
--For more details, please see: <gh-pr:4087#issuecomment-2250397630>
--:::
--
--:::{note}
--`mistral-community/pixtral-12b` does not support V1 yet.
--:::
--
--:::{note}
--To use Qwen2.5-VL series models, you have to install Huggingface `transformers` library from source via `pip install git+https://github.com/huggingface/transformers`.
--:::
--
--### Pooling Models
--
--See [this page](pooling-models) for more information on how to use pooling models.
--
--:::{important}
--Since some model architectures support both generative and pooling tasks,
--you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
--:::
--
--#### Text Embedding (`--task embed`)
--
--Any text generation model can be converted into an embedding model by passing `--task embed`.
--
--:::{note}
--To get the best results, you should use pooling models that are specifically trained as such.
--:::
--
--The following table lists those that are tested in vLLM.
--
--:::{list-table}
--:widths: 25 25 15 25 5 5
--:header-rows: 1
--
--- * Architecture
--  * Models
--  * Inputs
--  * Example HF Models
--  * [LoRA](#lora-adapter)
--  * [PP](#distributed-serving)
--- * `LlavaNextForConditionalGeneration`
--  * LLaVA-NeXT-based
--  * T / I
--  * `royokong/e5-v`
--  *
--  * 
--- * `Phi3VForCausalLM`
--  * Phi-3-Vision-based
--  * T + I
--  * `TIGER-Lab/VLM2Vec-Full`
--  * 
--  * 
--- * `Qwen2VLForConditionalGeneration`
--  * Qwen2-VL-based
--  * T + I
--  * `MrLight/dse-qwen2-2b-mrl-v1`
--  *
--  * 
--:::
--
--_________________
--
--## Model Support Policy
--
--At vLLM, we are committed to facilitating the integration and support of third-party models within our ecosystem. Our approach is designed to balance the need for robustness and the practical limitations of supporting a wide range of models. Heres how we manage third-party model support:
--
--1. **Community-Driven Support**: We encourage community contributions for adding new models. When a user requests support for a new model, we welcome pull requests (PRs) from the community. These contributions are evaluated primarily on the sensibility of the output they generate, rather than strict consistency with existing implementations such as those in transformers. **Call for contribution:** PRs coming directly from model vendors are greatly appreciated!
--
--2. **Best-Effort Consistency**: While we aim to maintain a level of consistency between the models implemented in vLLM and other frameworks like transformers, complete alignment is not always feasible. Factors like acceleration techniques and the use of low-precision computations can introduce discrepancies. Our commitment is to ensure that the implemented models are functional and produce sensible results.
--
--    :::{tip}
--    When comparing the output of `model.generate` from HuggingFace Transformers with the output of `llm.generate` from vLLM, note that the former reads the model's generation config file (i.e., [generation_config.json](https://github.com/huggingface/transformers/blob/19dabe96362803fb0a9ae7073d03533966598b17/src/transformers/generation/utils.py#L1945)) and applies the default parameters for generation, while the latter only uses the parameters passed to the function. Ensure all sampling parameters are identical when comparing outputs.
--    :::
--
--3. **Issue Resolution and Model Updates**: Users are encouraged to report any bugs or issues they encounter with third-party models. Proposed fixes should be submitted via PRs, with a clear explanation of the problem and the rationale behind the proposed solution. If a fix for one model impacts another, we rely on the community to highlight and address these cross-model dependencies. Note: for bugfix PRs, it is good etiquette to inform the original author to seek their feedback.
--
--4. **Monitoring and Updates**: Users interested in specific models should monitor the commit history for those models (e.g., by tracking changes in the main/vllm/model_executor/models directory). This proactive approach helps users stay informed about updates and changes that may affect the models they use.
--
--5. **Selective Focus**: Our resources are primarily directed towards models with significant user interest and impact. Models that are less frequently used may receive less attention, and we rely on the community to play a more active role in their upkeep and improvement.
--
--Through this approach, vLLM fosters a collaborative environment where both the core development team and the broader community contribute to the robustness and diversity of the third-party models supported in our ecosystem.
--
--Note that, as an inference engine, vLLM does not introduce new models. Therefore, all models supported by vLLM are third-party models in this regard.
--
--We have the following levels of testing for models:
--
--1. **Strict Consistency**: We compare the output of the model with the output of the model in the HuggingFace Transformers library under greedy decoding. This is the most stringent test. Please refer to [models tests](https://github.com/vllm-project/vllm/blob/main/tests/models) for the models that have passed this test.
--2. **Output Sensibility**: We check if the output of the model is sensible and coherent, by measuring the perplexity of the output and checking for any obvious errors. This is a less stringent test.
--3. **Runtime Functionality**: We check if the model can be loaded and run without errors. This is the least stringent test. Please refer to [functionality tests](gh-dir:tests) and [examples](gh-dir:main/examples) for the models that have passed this test.
--4. **Community Feedback**: We rely on the community to provide feedback on the models. If a model is broken or not working as expected, we encourage users to raise issues to report it or open pull requests to fix it. The rest of the models fall under this category.
-+(supported-models)=
-+
-+# List of Supported Models
-+
-+vLLM supports generative and pooling models across various tasks.
-+If a model supports more than one task, you can set the task via the `--task` argument.
-+
-+For each task, we list the model architectures that have been implemented in vLLM.
-+Alongside each architecture, we include some popular models that use it.
-+
-+## Loading a Model
-+
-+### HuggingFace Hub
-+
-+By default, vLLM loads models from [HuggingFace (HF) Hub](https://huggingface.co/models).
-+
-+To determine whether a given model is supported, you can check the `config.json` file inside the HF repository.
-+If the `"architectures"` field contains a model architecture listed below, then it should be supported in theory.
-+
-+:::{tip}
-+The easiest way to check if your model is really supported at runtime is to run the program below:
-+
-+```python
-+from vllm import LLM
-+
-+# For generative models (task=generate) only
-+llm = LLM(model=..., task="generate")  # Name or path of your model
-+output = llm.generate("Hello, my name is")
-+print(output)
-+
-+# For pooling models (task={embed,classify,reward,score}) only
-+llm = LLM(model=..., task="embed")  # Name or path of your model
-+output = llm.encode("Hello, my name is")
-+print(output)
-+```
-+
-+If vLLM successfully returns text (for generative models) or hidden states (for pooling models), it indicates that your model is supported.
-+:::
-+
-+Otherwise, please refer to [Adding a New Model](#new-model) for instructions on how to implement your model in vLLM.
-+Alternatively, you can [open an issue on GitHub](https://github.com/vllm-project/vllm/issues/new/choose) to request vLLM support.
-+
-+### Transformers fallback
-+
-+After the merge of <gh-pr:11330>, `vllm` can fallback to models that are available in `transformers`. This does not work for all models for now, but most decoder language models are supported, and vision language model support is planned!
-+
-+To check if the backend is `transformers`, you can simply do this:
-+
-+```python 
-+from vllm import LLM
-+llm = LLM(model=..., task="generate")  # Name or path of your model
-+llm.apply_model(lambda model: print(model.__class__))
-+```
-+
-+If it is `TransformersModel` then it means it's based on `transformers`!
-+
-+#### Supported features
-+
-+##### LORA and quantization
-+
-+Both are not supported yet! Make sure to open an issue and we'll work on this together with the `transformers` team!
-+
-+Usually `transformers` model load weights via the `load_adapters` API, that depends on PEFT. We need to work a bit to either use this api (for now this would result in some weights not being marked as loaded) or replace modules accordingly.
-+
-+Hints as to how this would look like:
-+
-+```python
-+class TransformersModel(nn.Module, SupportsLoRA):
-+  def __init__(*):
-+    ...
-+    self.model.load_adapter(vllm_config.load_config.model_loader_extra_config["qlora_adapter_name_or_path"])
-+```
-+
-+Blocker is that you need to specify supported lora layers, when we would ideally want to load whatever is inside the checkpoint!
-+
-+##### Remote code
-+
-+This fallback also means that any model on the hub that can be used in `transformers` with `trust_remote_code=True` that correctly implements attention can be used in production!
-+
-+```python 
-+from vllm import LLM
-+llm = LLM(model=..., task="generate", trust_remote_code=True)  # Name or path of your model
-+llm.apply_model(lambda model: print(model.__class__))
-+```
-+
-+A model just needs the following two things:
-+
-+```python
-+from transformers import PreTrainedModel
-+from torch import nn
-+
-+class MyAttention(nn.Module):
-+
-+  def forward(self, hidden_states, **kwargs): # <- kwargs are required
-+
-+    ...
-+    attention_interface = attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
-+    attn_output, attn_weights = attention_interface(
-+      self,
-+      query_states,
-+      key_states,
-+      value_states,
-+      **kwargs,
-+    )
-+    ...
-+
-+class MyModel(PreTrainedModel):
-+  _supports_attention_backend = True
-+```
-+
-+Here is what happens in the background:
-+
-+1. The config is loaded
-+2. `MyModel` python class is loaded from the `auto_map`, and we check that the model `_supports_attention_backend`.
-+3. The `TransformersModel` backend is used. See `/model_executors/models/transformers`, which leverage `self.config._attn_implementation = "vllm"`, thus the need to use `ALL_ATTENTION_FUNCTION`.
-+
-+That's it!
-+
-+### ModelScope
-+
-+To use models from [ModelScope](https://www.modelscope.cn) instead of HuggingFace Hub, set an environment variable:
-+
-+```shell
-+export VLLM_USE_MODELSCOPE=True
-+```
-+
-+And use with `trust_remote_code=True`.
-+
-+```python
-+from vllm import LLM
-+
-+llm = LLM(model=..., revision=..., task=..., trust_remote_code=True)
-+
-+# For generative models (task=generate) only
-+output = llm.generate("Hello, my name is")
-+print(output)
-+
-+# For pooling models (task={embed,classify,reward,score}) only
-+output = llm.encode("Hello, my name is")
-+print(output)
-+```
-+
-+## List of Text-only Language Models
-+
-+### Generative Models
-+
-+See [this page](#generative-models) for more information on how to use generative models.
-+
-+#### Text Generation (`--task generate`)
-+
-+:::{list-table}
-+:widths: 25 25 50 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+- * `AquilaForCausalLM`
-+  * Aquila, Aquila2
-+  * `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.
-+  * 
-+  * 
-+- * `ArcticForCausalLM`
-+  * Arctic
-+  * `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc.
-+  *
-+  * 
-+- * `BaiChuanForCausalLM`
-+  * Baichuan2, Baichuan
-+  * `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.
-+  * 
-+  * 
-+- * `BloomForCausalLM`
-+  * BLOOM, BLOOMZ, BLOOMChat
-+  * `bigscience/bloom`, `bigscience/bloomz`, etc.
-+  *
-+  * 
-+- * `BartForConditionalGeneration`
-+  * BART
-+  * `facebook/bart-base`, `facebook/bart-large-cnn`, etc.
-+  *
-+  *
-+- * `ChatGLMModel`
-+  * ChatGLM
-+  * `THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.
-+  * 
-+  * 
-+- * `CohereForCausalLM`, `Cohere2ForCausalLM`
-+  * Command-R
-+  * `CohereForAI/c4ai-command-r-v01`, `CohereForAI/c4ai-command-r7b-12-2024`, etc.
-+  * 
-+  * 
-+- * `DbrxForCausalLM`
-+  * DBRX
-+  * `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc.
-+  *
-+  * 
-+- * `DeciLMForCausalLM`
-+  * DeciLM
-+  * `Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc.
-+  *
-+  * 
-+- * `DeepseekForCausalLM`
-+  * DeepSeek
-+  * `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat` etc.
-+  *
-+  * 
-+- * `DeepseekV2ForCausalLM`
-+  * DeepSeek-V2
-+  * `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat` etc.
-+  *
-+  * 
-+- * `DeepseekV3ForCausalLM`
-+  * DeepSeek-V3
-+  * `deepseek-ai/DeepSeek-V3-Base`, `deepseek-ai/DeepSeek-V3` etc.
-+  *
-+  * 
-+- * `ExaoneForCausalLM`
-+  * EXAONE-3
-+  * `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc.
-+  * 
-+  * 
-+- * `FalconForCausalLM`
-+  * Falcon
-+  * `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.
-+  *
-+  * 
-+- * `FalconMambaForCausalLM`
-+  * FalconMamba
-+  * `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc.
-+  * 
-+  * 
-+- * `GemmaForCausalLM`
-+  * Gemma
-+  * `google/gemma-2b`, `google/gemma-7b`, etc.
-+  * 
-+  * 
-+- * `Gemma2ForCausalLM`
-+  * Gemma2
-+  * `google/gemma-2-9b`, `google/gemma-2-27b`, etc.
-+  * 
-+  * 
-+- * `GlmForCausalLM`
-+  * GLM-4
-+  * `THUDM/glm-4-9b-chat-hf`, etc.
-+  * 
-+  * 
-+- * `GPT2LMHeadModel`
-+  * GPT-2
-+  * `gpt2`, `gpt2-xl`, etc.
-+  *
-+  * 
-+- * `GPTBigCodeForCausalLM`
-+  * StarCoder, SantaCoder, WizardCoder
-+  * `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc.
-+  * 
-+  * 
-+- * `GPTJForCausalLM`
-+  * GPT-J
-+  * `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.
-+  *
-+  * 
-+- * `GPTNeoXForCausalLM`
-+  * GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM
-+  * `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.
-+  *
-+  * 
-+- * `GraniteForCausalLM`
-+  * Granite 3.0, Granite 3.1, PowerLM
-+  * `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc.
-+  * 
-+  * 
-+- * `GraniteMoeForCausalLM`
-+  * Granite 3.0 MoE, PowerMoE
-+  * `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc.
-+  * 
-+  * 
-+- * `GritLM`
-+  * GritLM
-+  * `parasail-ai/GritLM-7B-vllm`.
-+  * 
-+  * 
-+- * `InternLMForCausalLM`
-+  * InternLM
-+  * `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.
-+  * 
-+  * 
-+- * `InternLM2ForCausalLM`
-+  * InternLM2
-+  * `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc.
-+  * 
-+  * 
-+- * `InternLM3ForCausalLM`
-+  * InternLM3
-+  * `internlm/internlm3-8b-instruct`, etc.
-+  * 
-+  * 
-+- * `JAISLMHeadModel`
-+  * Jais
-+  * `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc.
-+  *
-+  * 
-+- * `JambaForCausalLM`
-+  * Jamba
-+  * `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc.
-+  * 
-+  * 
-+- * `LlamaForCausalLM`
-+  * Llama 3.1, Llama 3, Llama 2, LLaMA, Yi
-+  * `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc.
-+  * 
-+  * 
-+- * `MambaForCausalLM`
-+  * Mamba
-+  * `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc.
-+  *
-+  * 
-+- * `MiniCPMForCausalLM`
-+  * MiniCPM
-+  * `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc.
-+  * 
-+  * 
-+- * `MiniCPM3ForCausalLM`
-+  * MiniCPM3
-+  * `openbmb/MiniCPM3-4B`, etc.
-+  * 
-+  * 
-+- * `MistralForCausalLM`
-+  * Mistral, Mistral-Instruct
-+  * `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.
-+  * 
-+  * 
-+- * `MixtralForCausalLM`
-+  * Mixtral-8x7B, Mixtral-8x7B-Instruct
-+  * `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc.
-+  * 
-+  * 
-+- * `MPTForCausalLM`
-+  * MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter
-+  * `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc.
-+  *
-+  * 
-+- * `NemotronForCausalLM`
-+  * Nemotron-3, Nemotron-4, Minitron
-+  * `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc.
-+  * 
-+  * 
-+- * `OLMoForCausalLM`
-+  * OLMo
-+  * `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc.
-+  *
-+  * 
-+- * `OLMo2ForCausalLM`
-+  * OLMo2
-+  * `allenai/OLMo2-7B-1124`, etc.
-+  *
-+  * 
-+- * `OLMoEForCausalLM`
-+  * OLMoE
-+  * `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc.
-+  * 
-+  * 
-+- * `OPTForCausalLM`
-+  * OPT, OPT-IML
-+  * `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.
-+  *
-+  * 
-+- * `OrionForCausalLM`
-+  * Orion
-+  * `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc.
-+  *
-+  * 
-+- * `PhiForCausalLM`
-+  * Phi
-+  * `microsoft/phi-1_5`, `microsoft/phi-2`, etc.
-+  * 
-+  * 
-+- * `Phi3ForCausalLM`
-+  * Phi-4, Phi-3
-+  * `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc.
-+  * 
-+  * 
-+- * `Phi3SmallForCausalLM`
-+  * Phi-3-Small
-+  * `microsoft/Phi-3-small-8k-instruct`, `microsoft/Phi-3-small-128k-instruct`, etc.
-+  *
-+  * 
-+- * `PhiMoEForCausalLM`
-+  * Phi-3.5-MoE
-+  * `microsoft/Phi-3.5-MoE-instruct`, etc.
-+  * 
-+  * 
-+- * `PersimmonForCausalLM`
-+  * Persimmon
-+  * `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc.
-+  *
-+  * 
-+- * `QWenLMHeadModel`
-+  * Qwen
-+  * `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.
-+  * 
-+  * 
-+- * `Qwen2ForCausalLM`
-+  * QwQ, Qwen2
-+  * `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc.
-+  * 
-+  * 
-+- * `Qwen2MoeForCausalLM`
-+  * Qwen2MoE
-+  * `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.
-+  *
-+  * 
-+  - * `Qwen3ForCausalLM`
-+  * Qwen3
-+  * `Qwen/Qwen3-8B`, etc.
-+  * 
-+  * 
-+- * `Qwen3MoeForCausalLM`
-+  * Qwen3MoE
-+  * `Qwen/Qwen3-30B-A3B`, etc.
-+  * 
-+  * 
-+- * `StableLmForCausalLM`
-+  * StableLM
-+  * `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.
-+  *
-+  * 
-+- * `Starcoder2ForCausalLM`
-+  * Starcoder2
-+  * `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc.
-+  *
-+  * 
-+- * `SolarForCausalLM`
-+  * Solar Pro
-+  * `upstage/solar-pro-preview-instruct`, etc.
-+  * 
-+  * 
-+- * `TeleChat2ForCausalLM`
-+  * TeleChat2
-+  * `TeleAI/TeleChat2-3B`, `TeleAI/TeleChat2-7B`, `TeleAI/TeleChat2-35B`, etc.
-+  * 
-+  * 
-+- * `XverseForCausalLM`
-+  * XVERSE
-+  * `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc.
-+  * 
-+  * 
-+:::
-+
-+:::{note}
-+Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.
-+:::
-+
-+### Pooling Models
-+
-+See [this page](pooling-models) for more information on how to use pooling models.
-+
-+:::{important}
-+Since some model architectures support both generative and pooling tasks,
-+you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
-+:::
-+
-+#### Text Embedding (`--task embed`)
-+
-+:::{list-table}
-+:widths: 25 25 50 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+- * `BertModel`
-+  * BERT-based
-+  * `BAAI/bge-base-en-v1.5`, etc.
-+  *
-+  *
-+- * `Gemma2Model`
-+  * Gemma2-based
-+  * `BAAI/bge-multilingual-gemma2`, etc.
-+  *
-+  * 
-+- * `GritLM`
-+  * GritLM
-+  * `parasail-ai/GritLM-7B-vllm`.
-+  * 
-+  * 
-+- * `LlamaModel`, `LlamaForCausalLM`, `MistralModel`, etc.
-+  * Llama-based
-+  * `intfloat/e5-mistral-7b-instruct`, etc.
-+  * 
-+  * 
-+- * `Qwen2Model`, `Qwen2ForCausalLM`
-+  * Qwen2-based
-+  * `ssmits/Qwen2-7B-Instruct-embed-base` (see note), `Alibaba-NLP/gte-Qwen2-7B-instruct` (see note), etc.
-+  * 
-+  * 
-+- * `RobertaModel`, `RobertaForMaskedLM`
-+  * RoBERTa-based
-+  * `sentence-transformers/all-roberta-large-v1`, `sentence-transformers/all-roberta-large-v1`, etc.
-+  *
-+  *
-+- * `XLMRobertaModel`
-+  * XLM-RoBERTa-based
-+  * `intfloat/multilingual-e5-large`, etc.
-+  *
-+  *
-+:::
-+
-+:::{note}
-+`ssmits/Qwen2-7B-Instruct-embed-base` has an improperly defined Sentence Transformers config.
-+You should manually set mean pooling by passing `--override-pooler-config '{"pooling_type": "MEAN"}'`.
-+:::
-+
-+:::{note}
-+Unlike base Qwen2, `Alibaba-NLP/gte-Qwen2-7B-instruct` uses bi-directional attention.
-+You can set `--hf-overrides '{"is_causal": false}'` to change the attention mask accordingly.
-+
-+On the other hand, its 1.5B variant (`Alibaba-NLP/gte-Qwen2-1.5B-instruct`) uses causal attention
-+despite being described otherwise on its model card.
-+
-+Regardless of the variant, you need to enable `--trust-remote-code` for the correct tokenizer to be
-+loaded. See [relevant issue on HF Transformers](https://github.com/huggingface/transformers/issues/34882).
-+:::
-+
-+If your model is not in the above list, we will try to automatically convert the model using
-+{func}`~vllm.model_executor.models.adapters.as_embedding_model`. By default, the embeddings
-+of the whole prompt are extracted from the normalized hidden state corresponding to the last token.
-+
-+#### Reward Modeling (`--task reward`)
-+
-+:::{list-table}
-+:widths: 25 25 50 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+- * `InternLM2ForRewardModel`
-+  * InternLM2-based
-+  * `internlm/internlm2-1_8b-reward`, `internlm/internlm2-7b-reward`, etc.
-+  * 
-+  * 
-+- * `LlamaForCausalLM`
-+  * Llama-based
-+  * `peiyi9979/math-shepherd-mistral-7b-prm`, etc.
-+  * 
-+  * 
-+- * `Qwen2ForRewardModel`
-+  * Qwen2-based
-+  * `Qwen/Qwen2.5-Math-RM-72B`, etc.
-+  * 
-+  * 
-+- * `Qwen2ForProcessRewardModel`
-+  * Qwen2-based
-+  * `Qwen/Qwen2.5-Math-PRM-7B`, `Qwen/Qwen2.5-Math-PRM-72B`, etc.
-+  * 
-+  * 
-+:::
-+
-+If your model is not in the above list, we will try to automatically convert the model using
-+{func}`~vllm.model_executor.models.adapters.as_reward_model`. By default, we return the hidden states of each token directly.
-+
-+:::{important}
-+For process-supervised reward models such as `peiyi9979/math-shepherd-mistral-7b-prm`, the pooling config should be set explicitly,
-+e.g.: `--override-pooler-config '{"pooling_type": "STEP", "step_tag_id": 123, "returned_token_ids": [456, 789]}'`.
-+:::
-+
-+#### Classification (`--task classify`)
-+
-+:::{list-table}
-+:widths: 25 25 50 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+- * `JambaForSequenceClassification`
-+  * Jamba
-+  * `ai21labs/Jamba-tiny-reward-dev`, etc.
-+  * 
-+  * 
-+- * `Qwen2ForSequenceClassification`
-+  * Qwen2-based
-+  * `jason9693/Qwen2.5-1.5B-apeach`, etc.
-+  * 
-+  * 
-+:::
-+
-+If your model is not in the above list, we will try to automatically convert the model using
-+{func}`~vllm.model_executor.models.adapters.as_classification_model`. By default, the class probabilities are extracted from the softmaxed hidden state corresponding to the last token.
-+
-+#### Sentence Pair Scoring (`--task score`)
-+
-+:::{list-table}
-+:widths: 25 25 50 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+- * `BertForSequenceClassification`
-+  * BERT-based
-+  * `cross-encoder/ms-marco-MiniLM-L-6-v2`, etc.
-+  *
-+  *
-+- * `RobertaForSequenceClassification`
-+  * RoBERTa-based
-+  * `cross-encoder/quora-roberta-base`, etc.
-+  *
-+  *
-+- * `XLMRobertaForSequenceClassification`
-+  * XLM-RoBERTa-based
-+  * `BAAI/bge-reranker-v2-m3`, etc.
-+  *
-+  *
-+:::
-+
-+(supported-mm-models)=
-+
-+## List of Multimodal Language Models
-+
-+The following modalities are supported depending on the model:
-+
-+- **T**ext
-+- **I**mage
-+- **V**ideo
-+- **A**udio
-+
-+Any combination of modalities joined by `+` are supported.
-+
-+- e.g.: `T + I` means that the model supports text-only, image-only, and text-with-image inputs.
-+
-+On the other hand, modalities separated by `/` are mutually exclusive.
-+
-+- e.g.: `T / I` means that the model supports text-only and image-only inputs, but not text-with-image inputs.
-+
-+See [this page](#multimodal-inputs) on how to pass multi-modal inputs to the model.
-+
-+:::{important}
-+To enable multiple multi-modal items per text prompt, you have to set `limit_mm_per_prompt` (offline inference)
-+or `--limit-mm-per-prompt` (online serving). For example, to enable passing up to 4 images per text prompt:
-+
-+Offline inference:
-+
-+```python
-+llm = LLM(
-+    model="Qwen/Qwen2-VL-7B-Instruct",
-+    limit_mm_per_prompt={"image": 4},
-+)
-+```
-+
-+Online serving:
-+
-+```bash
-+vllm serve Qwen/Qwen2-VL-7B-Instruct --limit-mm-per-prompt image=4
-+```
-+
-+:::
-+
-+:::{note}
-+vLLM currently only supports adding LoRA to the language backbone of multimodal models.
-+:::
-+
-+### Generative Models
-+
-+See [this page](#generative-models) for more information on how to use generative models.
-+
-+#### Text Generation (`--task generate`)
-+
-+:::{list-table}
-+:widths: 25 25 15 20 5 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Inputs
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+  * [V1](gh-issue:8779)
-+- * `AriaForConditionalGeneration`
-+  * Aria
-+  * T + I<sup>+</sup>
-+  * `rhymes-ai/Aria`
-+  *
-+  * 
-+  * 
-+- * `Blip2ForConditionalGeneration`
-+  * BLIP-2
-+  * T + I<sup>E</sup>
-+  * `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc.
-+  *
-+  * 
-+  * 
-+- * `ChameleonForConditionalGeneration`
-+  * Chameleon
-+  * T + I
-+  * `facebook/chameleon-7b` etc.
-+  *
-+  * 
-+  * 
-+- * `DeepseekVLV2ForCausalLM`
-+  * DeepSeek-VL2
-+  * T + I<sup>+</sup>
-+  * `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2` etc. (see note)
-+  *
-+  * 
-+  * 
-+- * `FuyuForCausalLM`
-+  * Fuyu
-+  * T + I
-+  * `adept/fuyu-8b` etc.
-+  *
-+  * 
-+  * 
-+- * `ChatGLMModel`
-+  * GLM-4V
-+  * T + I
-+  * `THUDM/glm-4v-9b` etc.
-+  * 
-+  * 
-+  *
-+- * `H2OVLChatModel`
-+  * H2OVL
-+  * T + I<sup>E+</sup>
-+  * `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc.
-+  *
-+  * 
-+  * \*
-+- * `Idefics3ForConditionalGeneration`
-+  * Idefics3
-+  * T + I
-+  * `HuggingFaceM4/Idefics3-8B-Llama3` etc.
-+  * 
-+  *
-+  * 
-+- * `InternVLChatModel`
-+  * InternVL 2.5, Mono-InternVL, InternVL 2.0
-+  * T + I<sup>E+</sup>
-+  * `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc.
-+  *
-+  * 
-+  * 
-+- * `LlavaForConditionalGeneration`
-+  * LLaVA-1.5
-+  * T + I<sup>E+</sup>
-+  * `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), etc.
-+  *
-+  * 
-+  * 
-+- * `LlavaNextForConditionalGeneration`
-+  * LLaVA-NeXT
-+  * T + I<sup>E+</sup>
-+  * `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc.
-+  *
-+  * 
-+  * 
-+- * `LlavaNextVideoForConditionalGeneration`
-+  * LLaVA-NeXT-Video
-+  * T + V
-+  * `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc.
-+  *
-+  * 
-+  * 
-+- * `LlavaOnevisionForConditionalGeneration`
-+  * LLaVA-Onevision
-+  * T + I<sup>+</sup> + V<sup>+</sup>
-+  * `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc.
-+  *
-+  * 
-+  * 
-+- * `MiniCPMO`
-+  * MiniCPM-O
-+  * T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup>
-+  * `openbmb/MiniCPM-o-2_6`, etc.
-+  * 
-+  * 
-+  *
-+- * `MiniCPMV`
-+  * MiniCPM-V
-+  * T + I<sup>E+</sup> + V<sup>E+</sup>
-+  * `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc.
-+  * 
-+  * 
-+  *
-+- * `MllamaForConditionalGeneration`
-+  * Llama 3.2
-+  * T + I<sup>+</sup>
-+  * `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc.
-+  *
-+  *
-+  *
-+- * `MolmoForCausalLM`
-+  * Molmo
-+  * T + I
-+  * `allenai/Molmo-7B-D-0924`, `allenai/Molmo-72B-0924`, etc.
-+  * 
-+  * 
-+  * 
-+- * `NVLM_D_Model`
-+  * NVLM-D 1.0
-+  * T + I<sup>+</sup>
-+  * `nvidia/NVLM-D-72B`, etc.
-+  *
-+  * 
-+  * 
-+- * `PaliGemmaForConditionalGeneration`
-+  * PaliGemma, PaliGemma 2
-+  * T + I<sup>E</sup>
-+  * `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc.
-+  *
-+  * 
-+  *
-+- * `Phi3VForCausalLM`
-+  * Phi-3-Vision, Phi-3.5-Vision
-+  * T + I<sup>E+</sup>
-+  * `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc.
-+  *
-+  * 
-+  * 
-+- * `PixtralForConditionalGeneration`
-+  * Pixtral
-+  * T + I<sup>+</sup>
-+  * `mistralai/Pixtral-12B-2409`, `mistral-community/pixtral-12b` (see note), etc.
-+  *
-+  * 
-+  * 
-+- * `QWenLMHeadModel`
-+  * Qwen-VL
-+  * T + I<sup>E+</sup>
-+  * `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc.
-+  * 
-+  * 
-+  * 
-+- * `Qwen2AudioForConditionalGeneration`
-+  * Qwen2-Audio
-+  * T + A<sup>+</sup>
-+  * `Qwen/Qwen2-Audio-7B-Instruct`
-+  *
-+  * 
-+  * 
-+- * `Qwen2VLForConditionalGeneration`
-+  * QVQ, Qwen2-VL
-+  * T + I<sup>E+</sup> + V<sup>E+</sup>
-+  * `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc.
-+  * 
-+  * 
-+  * 
-+- * `Qwen2_5_VLForConditionalGeneration`
-+  * Qwen2.5-VL
-+  * T + I<sup>E+</sup> + V<sup>E+</sup>
-+  * `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc.
-+  *
-+  * 
-+  * 
-+- * `UltravoxModel`
-+  * Ultravox
-+  * T + A<sup>E+</sup>
-+  * `fixie-ai/ultravox-v0_3`
-+  * 
-+  * 
-+  * 
-+:::
-+
-+<sup>E</sup> Pre-computed embeddings can be inputted for this modality.  
-+<sup>+</sup> Multiple items can be inputted per text prompt for this modality.
-+
-+:::{note}
-+To use DeepSeek-VL2 series models, you have to pass `--hf_overrides '{"architectures": ["DeepseekVLV2ForCausalLM"]}'` when running vLLM.
-+:::
-+
-+:::{note}
-+H2O-VL series models will be available in V1 once we support backends other than FlashAttention.
-+:::
-+
-+:::{note}
-+To use `TIGER-Lab/Mantis-8B-siglip-llama3`, you have to pass `--hf_overrides '{"architectures": ["MantisForConditionalGeneration"]}'` when running vLLM.
-+:::
-+
-+:::{note}
-+The official `openbmb/MiniCPM-V-2` doesn't work yet, so we need to use a fork (`HwwwH/MiniCPM-V-2`) for now.
-+For more details, please see: <gh-pr:4087#issuecomment-2250397630>
-+:::
-+
-+:::{note}
-+`mistral-community/pixtral-12b` does not support V1 yet.
-+:::
-+
-+:::{note}
-+To use Qwen2.5-VL series models, you have to install Huggingface `transformers` library from source via `pip install git+https://github.com/huggingface/transformers`.
-+:::
-+
-+### Pooling Models
-+
-+See [this page](pooling-models) for more information on how to use pooling models.
-+
-+:::{important}
-+Since some model architectures support both generative and pooling tasks,
-+you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
-+:::
-+
-+#### Text Embedding (`--task embed`)
-+
-+Any text generation model can be converted into an embedding model by passing `--task embed`.
-+
-+:::{note}
-+To get the best results, you should use pooling models that are specifically trained as such.
-+:::
-+
-+The following table lists those that are tested in vLLM.
-+
-+:::{list-table}
-+:widths: 25 25 15 25 5 5
-+:header-rows: 1
-+
-+- * Architecture
-+  * Models
-+  * Inputs
-+  * Example HF Models
-+  * [LoRA](#lora-adapter)
-+  * [PP](#distributed-serving)
-+- * `LlavaNextForConditionalGeneration`
-+  * LLaVA-NeXT-based
-+  * T / I
-+  * `royokong/e5-v`
-+  *
-+  * 
-+- * `Phi3VForCausalLM`
-+  * Phi-3-Vision-based
-+  * T + I
-+  * `TIGER-Lab/VLM2Vec-Full`
-+  * 
-+  * 
-+- * `Qwen2VLForConditionalGeneration`
-+  * Qwen2-VL-based
-+  * T + I
-+  * `MrLight/dse-qwen2-2b-mrl-v1`
-+  *
-+  * 
-+:::
-+
-+_________________
-+
-+## Model Support Policy
-+
-+At vLLM, we are committed to facilitating the integration and support of third-party models within our ecosystem. Our approach is designed to balance the need for robustness and the practical limitations of supporting a wide range of models. Heres how we manage third-party model support:
-+
-+1. **Community-Driven Support**: We encourage community contributions for adding new models. When a user requests support for a new model, we welcome pull requests (PRs) from the community. These contributions are evaluated primarily on the sensibility of the output they generate, rather than strict consistency with existing implementations such as those in transformers. **Call for contribution:** PRs coming directly from model vendors are greatly appreciated!
-+
-+2. **Best-Effort Consistency**: While we aim to maintain a level of consistency between the models implemented in vLLM and other frameworks like transformers, complete alignment is not always feasible. Factors like acceleration techniques and the use of low-precision computations can introduce discrepancies. Our commitment is to ensure that the implemented models are functional and produce sensible results.
-+
-+    :::{tip}
-+    When comparing the output of `model.generate` from HuggingFace Transformers with the output of `llm.generate` from vLLM, note that the former reads the model's generation config file (i.e., [generation_config.json](https://github.com/huggingface/transformers/blob/19dabe96362803fb0a9ae7073d03533966598b17/src/transformers/generation/utils.py#L1945)) and applies the default parameters for generation, while the latter only uses the parameters passed to the function. Ensure all sampling parameters are identical when comparing outputs.
-+    :::
-+
-+3. **Issue Resolution and Model Updates**: Users are encouraged to report any bugs or issues they encounter with third-party models. Proposed fixes should be submitted via PRs, with a clear explanation of the problem and the rationale behind the proposed solution. If a fix for one model impacts another, we rely on the community to highlight and address these cross-model dependencies. Note: for bugfix PRs, it is good etiquette to inform the original author to seek their feedback.
-+
-+4. **Monitoring and Updates**: Users interested in specific models should monitor the commit history for those models (e.g., by tracking changes in the main/vllm/model_executor/models directory). This proactive approach helps users stay informed about updates and changes that may affect the models they use.
-+
-+5. **Selective Focus**: Our resources are primarily directed towards models with significant user interest and impact. Models that are less frequently used may receive less attention, and we rely on the community to play a more active role in their upkeep and improvement.
-+
-+Through this approach, vLLM fosters a collaborative environment where both the core development team and the broader community contribute to the robustness and diversity of the third-party models supported in our ecosystem.
-+
-+Note that, as an inference engine, vLLM does not introduce new models. Therefore, all models supported by vLLM are third-party models in this regard.
-+
-+We have the following levels of testing for models:
-+
-+1. **Strict Consistency**: We compare the output of the model with the output of the model in the HuggingFace Transformers library under greedy decoding. This is the most stringent test. Please refer to [models tests](https://github.com/vllm-project/vllm/blob/main/tests/models) for the models that have passed this test.
-+2. **Output Sensibility**: We check if the output of the model is sensible and coherent, by measuring the perplexity of the output and checking for any obvious errors. This is a less stringent test.
-+3. **Runtime Functionality**: We check if the model can be loaded and run without errors. This is the least stringent test. Please refer to [functionality tests](gh-dir:tests) and [examples](gh-dir:main/examples) for the models that have passed this test.
-+4. **Community Feedback**: We rely on the community to provide feedback on the models. If a model is broken or not working as expected, we encourage users to raise issues to report it or open pull requests to fix it. The rest of the models fall under this category.
+diff --git a/docs/models/supported_models.md b/docs/models/supported_models.md
+index a8a6f3417..0560f7aa5 100644
+--- a/docs/models/supported_models.md
++++ b/docs/models/supported_models.md
+@@ -533,7 +533,7 @@ Specified using `--task generate`.
+ | `LlavaNextVideoForConditionalGeneration`     | LLaVA-NeXT-Video                                                         | T + V                                                                 | `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc.                                                                                                                 |                       |                           |                       |
+ | `LlavaOnevisionForConditionalGeneration`     | LLaVA-Onevision                                                          | T + I<sup>+</sup> + V<sup>+</sup>                                     | `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc.                                                            |                       |                           |                       |
+ | `MiniCPMO`                                   | MiniCPM-O                                                                | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup>                  | `openbmb/MiniCPM-o-2_6`, etc.                                                                                                                           |                      |                           |                     |
+-| `MiniCPMV`                                   | MiniCPM-V                                                                | T + I<sup>E+</sup> + V<sup>E+</sup>                                   | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc.                                                         |                      |                            |                     |
++| `MiniCPMV`                                   | MiniCPM-V                                                                | T + I<sup>E+</sup> + V<sup>E+</sup>                                   | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, `openbmb/MiniCPM-V-4`, etc.                                                         |                      |                            |                     |
+ | `MiniMaxVL01ForConditionalGeneration`        | MiniMax-VL                                                               | T + I<sup>E+</sup>                                                    | `MiniMaxAI/MiniMax-VL-01`, etc.                                                                                                                         |                       |                           |                       |
+ | `Mistral3ForConditionalGeneration`           | Mistral3                                                                 | T + I<sup>+</sup>                                                     | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, etc.                                                                                                   |                      |                           |                     |
+ | `MllamaForConditionalGeneration`             | Llama 3.2                                                                | T + I<sup>+</sup>                                                     | `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc.                                                                     |                        |                             |                        |
 diff --git a/env.sh b/env.sh
 new file mode 100644
 index 000000000..e95190521
@@ -17930,182 +15533,174 @@ index 000000000..e95190521
 +export VLLM_INSTALL_PUNICA_KERNELS=1
 +
 +echo "MACA PATH: ${MACA_PATH} Compile Code"
-diff --git a/examples/llm_engine_example_bytemlperf.py b/examples/llm_engine_example_bytemlperf.py
-new file mode 100644
-index 000000000..c4766ca9e
---- /dev/null
-+++ b/examples/llm_engine_example_bytemlperf.py
-@@ -0,0 +1,61 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+import argparse
-+from typing import List, Tuple
-+
-+from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams
-+from vllm.utils import FlexibleArgumentParser
-+
-+
-+def create_test_prompts() -> List[Tuple[str, SamplingParams]]:
-+    """Create a list of test prompts with their sampling parameters."""
-+    return [
-+        ("A robot may not injure a human being",
-+         SamplingParams(temperature=0.8, top_k=1, presence_penalty=0.2)),
-+        ("To be or not to be,",
-+         SamplingParams(temperature=0.8, top_k=1, presence_penalty=0.2)),
-+        # ("What is the meaning of life?",
-+        #  SamplingParams(n=2,
-+        #                 best_of=5,
-+        #                 temperature=0.8,
-+        #                 top_p=0.95,
-+        #                 frequency_penalty=0.1)),
-+    ]
-+
-+def process_requests(engine: LLMEngine,
-+                     test_prompts: List[Tuple[str, SamplingParams]]):
-+    """Continuously process a list of prompts and handle the outputs."""
-+    request_id = 0
-+
-+    while test_prompts or engine.has_unfinished_requests():
-+        if test_prompts:
-+            prompt, sampling_params = test_prompts.pop(0)
-+            engine.add_request(str(request_id), prompt, sampling_params)
-+            request_id += 1
-+
-+        request_outputs, hidden_states = engine.step_new()
-+        print(f"hidden_states: {hidden_states.shape}")
-+
-+        for request_output in request_outputs:
-+            if request_output.finished:
-+                print(request_output)
-+
-+
-+def initialize_engine(args: argparse.Namespace) -> LLMEngine:
-+    """Initialize the LLMEngine from the command line arguments."""
-+    engine_args = EngineArgs.from_cli_args(args)
-+    return LLMEngine.from_engine_args(engine_args)
-+
-+
-+def main(args: argparse.Namespace):
-+    """Main function that sets up and runs the prompt processing."""
-+    engine = initialize_engine(args)
-+    test_prompts = create_test_prompts()
-+    process_requests(engine, test_prompts)
-+
-+
-+if __name__ == '__main__':
-+    parser = FlexibleArgumentParser(
-+        description='Demo on using the LLMEngine class directly')
-+    parser = EngineArgs.add_cli_args(parser)
-+    args = parser.parse_args()
-+    main(args)
-diff --git a/requirements-cuda.txt b/requirements-cuda.txt
-index 78fa360f2..4b2a6945d 100644
---- a/requirements-cuda.txt
-+++ b/requirements-cuda.txt
-@@ -3,9 +3,9 @@
+diff --git a/pyproject.toml b/pyproject.toml
+index 307878f7e..685922ac3 100644
+--- a/pyproject.toml
++++ b/pyproject.toml
+@@ -15,8 +15,7 @@ build-backend = "setuptools.build_meta"
+ [project]
+ name = "vllm"
+ authors = [{name = "vLLM Team"}]
+-license = "Apache-2.0"
+-license-files = ["LICENSE"]
++license = { "file"= "LICENSE" }
+ readme = "README.md"
+ description = "A high-throughput and memory-efficient inference and serving engine for LLMs"
+ classifiers = [
+diff --git a/requirements/build.txt b/requirements/build.txt
+index 528cd3b53..cb528a965 100644
+--- a/requirements/build.txt
++++ b/requirements/build.txt
+@@ -4,7 +4,7 @@ ninja
+ packaging>=24.2
+ setuptools>=77.0.3,<80.0.0
+ setuptools-scm>=8
+-torch==2.7.0
++torch==2.6.0
+ wheel
+ jinja2>=3.1.6
+ regex
+diff --git a/requirements/common.txt b/requirements/common.txt
+index 871df3d21..ed3941e43 100644
+--- a/requirements/common.txt
++++ b/requirements/common.txt
+@@ -2,12 +2,14 @@ regex # Replace re for higher-performance regex matching
+ cachetools
+ psutil
+ sentencepiece  # Required for LLaMA tokenizer.
+-numpy
++numpy < 2.0
+ requests >= 2.26.0
+ tqdm
+ blake3
+ py-cpuinfo
+-transformers >= 4.51.1
++# Fix transformers version to avoid mixtral models loading error (config["head_dim"] == null)
++# transformers >= 4.51.1
++transformers == 4.51.3
+ huggingface-hub[hf_xet] >= 0.32.0  # Required for Xet downloads.
+ tokenizers >= 0.21.1  # Required for fast incremental detokenization.
+ protobuf # Required by LlamaTokenizer.
+diff --git a/requirements/cuda.txt b/requirements/cuda.txt
+index a71d9728f..bf32479b2 100644
+--- a/requirements/cuda.txt
++++ b/requirements/cuda.txt
+@@ -5,10 +5,10 @@ numba == 0.60.0; python_version == '3.9' # v0.61 doesn't support Python 3.9. Req
+ numba == 0.61.2; python_version > '3.9'
  
  # Dependencies for NVIDIA GPUs
- ray[default] >= 2.9
--nvidia-ml-py >= 12.560.30 # for pynvml package
--torch == 2.5.1
--torchaudio==2.5.1
-+#nvidia-ml-py >= 12.560.30 # for pynvml package
-+#torch == 2.1.2
-+#torchaudio==2.5.1
- # These must be updated alongside torch
--torchvision == 0.20.1 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
--xformers == 0.0.28.post3; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.5.1
-+#torchvision == 0.20.1 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
-+#xformers == 0.0.28.post3; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.5.1
+-ray[cgraph]>=2.43.0, !=2.44.* # Ray Compiled Graph, required for pipeline parallelism in V1.
+-torch==2.7.0
+-torchaudio==2.7.0
+-# These must be updated alongside torch
+-torchvision==0.22.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
+-# https://github.com/facebookresearch/xformers/releases/tag/v0.0.30
+-xformers==0.0.30; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch >= 2.7
++# ray[cgraph]>=2.43.0, !=2.44.* # Ray Compiled Graph, required for pipeline parallelism in V1.
++torch==2.6.0
++# torchaudio==2.7.0
++# # These must be updated alongside torch
++# torchvision==0.22.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
++# # https://github.com/facebookresearch/xformers/releases/tag/v0.0.30
++# xformers==0.0.30; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch >= 2.7
 diff --git a/setup.py b/setup.py
-index a4043c43a..e63951689 100755
+index ea7cd0169..786206d86 100644
 --- a/setup.py
 +++ b/setup.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import ctypes
-@@ -15,7 +16,7 @@ import torch
+@@ -16,9 +16,11 @@ import torch
  from packaging.version import Version, parse
- from setuptools import Extension, find_packages, setup
+ from setuptools import Extension, setup
  from setuptools.command.build_ext import build_ext
 -from setuptools_scm import get_version
-+#from setuptools_scm import get_version
++# from setuptools_scm import get_version
  from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME
  
++USE_MACA = True
++CMAKE_EXECUTABLE = 'cmake' if not USE_MACA else 'cmake_maca'
  
-@@ -125,8 +126,10 @@ class cmake_build_ext(build_ext):
-         # Note: optimization level + debug info are set by the build type
+ def load_module_from_path(module_name, path):
+     spec = importlib.util.spec_from_file_location(module_name, path)
+@@ -145,10 +147,19 @@ class cmake_build_ext(build_ext):
          default_cfg = "Debug" if self.debug else "RelWithDebInfo"
          cfg = envs.CMAKE_BUILD_TYPE or default_cfg
--
-+ 
-+        use_maca = "ON"
+ 
++        maca_version = get_maca_version_list()
++
          cmake_args = [
-+            '-DUSE_MACA={}'.format(use_maca),
              '-DCMAKE_BUILD_TYPE={}'.format(cfg),
              '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
          ]
-@@ -184,13 +187,13 @@ class cmake_build_ext(build_ext):
++        if USE_MACA:
++            maca_args_ext = ['-DUSE_MACA=ON',
++                '-DMACA_VERSION_MAJOR={}'.format(maca_version[0]),
++                '-DMACA_VERSION_MINOR={}'.format(maca_version[1]),
++                '-DMACA_VERSION_PATCH={}'.format(maca_version[2]),
++                '-DMACA_VERSION_BUILD={}'.format(maca_version[3]),]
++            cmake_args.extend(maca_args_ext)
+ 
+         verbose = envs.VERBOSE
+         if verbose:
+@@ -203,16 +214,16 @@ class cmake_build_ext(build_ext):
              # Default build tool to whatever cmake picks.
              build_tool = []
+         # Make sure we use the nvcc from CUDA_HOME
+-        if _is_cuda():
+-            cmake_args += [f'-DCMAKE_CUDA_COMPILER={CUDA_HOME}/bin/nvcc']
++        # if _is_cuda():
++        #     cmake_args += [f'-DCMAKE_CUDA_COMPILER={CUDA_HOME}/bin/nvcc']
          subprocess.check_call(
 -            ['cmake', ext.cmake_lists_dir, *build_tool, *cmake_args],
-+            ['cmake_maca', ext.cmake_lists_dir, *build_tool, *cmake_args],
++            [CMAKE_EXECUTABLE, ext.cmake_lists_dir, *build_tool, *cmake_args],
              cwd=self.build_temp)
  
      def build_extensions(self) -> None:
          # Ensure that CMake is present and working
          try:
 -            subprocess.check_output(['cmake', '--version'])
-+            subprocess.check_output(['cmake_maca', '--version'])
++            subprocess.check_output([CMAKE_EXECUTABLE, '--version'])
          except OSError as e:
              raise RuntimeError('Cannot find CMake executable') from e
  
-@@ -217,7 +220,7 @@ class cmake_build_ext(build_ext):
+@@ -239,7 +250,7 @@ class cmake_build_ext(build_ext):
              *[f"--target={name}" for name in targets],
          ]
  
 -        subprocess.check_call(["cmake", *build_args], cwd=self.build_temp)
-+        subprocess.check_call(["cmake_maca", *build_args], cwd=self.build_temp)
++        subprocess.check_call([CMAKE_EXECUTABLE, *build_args], cwd=self.build_temp)
  
          # Install the libraries
          for ext in self.extensions:
-@@ -239,7 +242,7 @@ class cmake_build_ext(build_ext):
+@@ -258,7 +269,7 @@ class cmake_build_ext(build_ext):
  
              # prefix here should actually be the same for all components
              install_args = [
 -                "cmake", "--install", ".", "--prefix", prefix, "--component",
-+                "cmake_maca", "--install", ".", "--prefix", prefix, "--component",
++                CMAKE_EXECUTABLE, "--install", ".", "--prefix", prefix, "--component",
                  target_name(ext.name)
              ]
              subprocess.check_call(install_args, cwd=self.build_temp)
-@@ -252,7 +255,7 @@ class cmake_build_ext(build_ext):
-         # directory so that they can be included in the editable build
-         import glob
-         files = glob.glob(
--            os.path.join(self.build_lib, "vllm", "vllm_flash_attn", "*.py"))
-+            os.path.join(self.build_lib, "vllm", "*.py"))
-         for file in files:
-             dst_file = os.path.join("vllm/vllm_flash_attn",
-                                     os.path.basename(file))
-@@ -303,8 +306,8 @@ class repackage_wheel(build_ext):
+@@ -374,9 +385,9 @@ class repackage_wheel(build_ext):
              files_to_copy = [
                  "vllm/_C.abi3.so",
                  "vllm/_moe_C.abi3.so",
+-                "vllm/_flashmla_C.abi3.so",
 -                "vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
 -                "vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
-+                #"vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
-+                #"vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
-                 "vllm/vllm_flash_attn/flash_attn_interface.py",
-                 "vllm/vllm_flash_attn/__init__.py",
++                # "vllm/_flashmla_C.abi3.so",
++                # "vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
++                # "vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
                  "vllm/cumem_allocator.abi3.so",
-@@ -477,11 +480,45 @@ def get_gaudi_sw_version():
+                 # "vllm/_version.py", # not available in nightly wheels yet
+             ]
+@@ -544,9 +555,52 @@ def get_gaudi_sw_version():
              " ", "").split(":")[1][:-1].split("-")[0]
      return "0.0.0"  # when hl-smi is not available
  
 +def get_maca_version():
-+    maca_path = os.getenv('MACA_PATH')
++    """
++    Returns the MACA SDK Version
++    """
++    maca_path = str(os.getenv('MACA_PATH'))
 +    if not os.path.exists(maca_path):
 +        return None
 +    file_full_path = os.path.join(maca_path, 'Version.txt')
@@ -18116,6 +15711,12 @@ index a4043c43a..e63951689 100755
 +        first_line = file.readline().strip()
 +    return first_line.split(":")[-1]
 +
++def get_maca_version_list():
++    version_str = get_maca_version()
++    version_list = list(map(int, (version_str or "0.0.0.0").split('.')))
++    version_list.extend([0] * (4 - len(version_list)))
++    return version_list
++    
 +def get_git_commit():
 +    curdir = os.path.dirname(__file__)
 +    default_gitdir = os.path.normpath(os.path.join(curdir, ".git"))
@@ -18137,19 +15738,15 @@ index a4043c43a..e63951689 100755
 +        print(f"Error writing to file: {e}")
  
  def get_vllm_version() -> str:
--    version = get_version(
--        write_to="vllm/_version.py",  # TODO: move this to pyproject.toml
--    )
-+    #version = get_version(
-+    #    write_to="vllm/_version.py",  # TODO: move this to pyproject.toml
-+    #)
+-    version = get_version(write_to="vllm/_version.py")
++    # version = get_version(write_to="vllm/_version.py")
 +    commit_id = get_git_commit()
 +    write_to_file("vllm/_release_info.txt", commit_id)
-+    version = "0.7.2"
- 
++    version = "0.9.1"
      sep = "+" if "+" not in version else "."  # dev versions might contain +
  
-@@ -492,12 +529,18 @@ def get_vllm_version() -> str:
+     if _no_device():
+@@ -556,12 +610,18 @@ def get_vllm_version() -> str:
          if envs.VLLM_USE_PRECOMPILED:
              version += f"{sep}precompiled"
          else:
@@ -18159,8 +15756,7 @@ index a4043c43a..e63951689 100755
 -                # skip this for source tarball, required for pypi
 -                if "sdist" not in sys.argv:
 -                    version += f"{sep}cu{cuda_version_str}"
-+            maca_version_str = get_maca_version()
-+            if maca_version_str is None:
++            if not USE_MACA:
 +                cuda_version = str(get_nvcc_cuda_version())
 +                if cuda_version != MAIN_CUDA_VERSION:
 +                    cuda_version_str = cuda_version.replace(".", "")[:3]
@@ -18168,22 +15764,36 @@ index a4043c43a..e63951689 100755
 +                    if "sdist" not in sys.argv:
 +                        version += f"{sep}cu{cuda_version_str}"
 +            else:
++                maca_version_str = get_maca_version()
 +                torch_version = torch.__version__
 +                major_minor_version = ".".join(torch_version.split(".")[:2])
 +                version += f"{sep}maca{maca_version_str}torch{major_minor_version}"
      elif _is_hip():
          # Get the Rocm Version
          rocm_version = get_rocm_version() or torch.version.hip
-@@ -598,7 +641,7 @@ if _is_cuda() or _is_hip():
+@@ -648,7 +708,7 @@ if _is_cuda() or _is_hip():
  if _is_hip():
      ext_modules.append(CMakeExtension(name="vllm._rocm_C"))
  
 -if _is_cuda():
 +if _is_cuda() and False:
      ext_modules.append(CMakeExtension(name="vllm.vllm_flash_attn._vllm_fa2_C"))
-     if envs.VLLM_USE_PRECOMPILED or get_nvcc_cuda_version() >= Version("12.0"):
-         # FA3 requires CUDA 12.0 or later
-@@ -614,6 +657,8 @@ package_data = {
+     if envs.VLLM_USE_PRECOMPILED or get_nvcc_cuda_version() >= Version("12.3"):
+         # FA3 requires CUDA 12.3 or later
+@@ -658,9 +718,11 @@ if _is_cuda():
+         # not targeting a hopper system
+         ext_modules.append(
+             CMakeExtension(name="vllm._flashmla_C", optional=True))
++
++if _is_cuda():
+     ext_modules.append(CMakeExtension(name="vllm.cumem_allocator"))
+ 
+-if _build_custom_ops():
++if _build_custom_ops() or True:
+     ext_modules.append(CMakeExtension(name="vllm._C"))
+ 
+ package_data = {
+@@ -668,6 +730,8 @@ package_data = {
          "py.typed",
          "model_executor/layers/fused_moe/configs/*.json",
          "model_executor/layers/quantization/utils/configs/*.json",
@@ -18192,1209 +15802,49 @@ index a4043c43a..e63951689 100755
      ]
  }
  
-diff --git a/tests/distributed/test_pynccl.py b/tests/distributed/test_pynccl.py
-index 4c42a0ed8..fb57f8b96 100644
---- a/tests/distributed/test_pynccl.py
-+++ b/tests/distributed/test_pynccl.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+diff --git a/tests/models/registry.py b/tests/models/registry.py
+index e6543c197..eaa08a819 100644
+--- a/tests/models/registry.py
++++ b/tests/models/registry.py
+@@ -156,6 +156,10 @@ _TEXT_GENERATION_EXAMPLE_MODELS = {
+                                          trust_remote_code=True),
+     "DeepseekV3ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V3",  # noqa: E501
+                                          trust_remote_code=True),
++    "Ernie4_5_ForCausalLM": _HfExamplesInfo("baidu/ERNIE-4.5-0.3B-PT",
++                                        trust_remote_code=True),
++    "Ernie4_5_MoeForCausalLM": _HfExamplesInfo("baidu/ERNIE-4.5-21B-A3B-PT",
++                                        trust_remote_code=True),
+     "ExaoneForCausalLM": _HfExamplesInfo("LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"),  # noqa: E501
+     "Fairseq2LlamaForCausalLM": _HfExamplesInfo("mgleize/fairseq2-dummy-Llama-3.2-1B"),  # noqa: E501
+     "FalconForCausalLM": _HfExamplesInfo("tiiuae/falcon-7b"),
+@@ -351,7 +355,7 @@ _MULTIMODAL_EXAMPLE_MODELS = {
+     "MiniCPMO": _HfExamplesInfo("openbmb/MiniCPM-o-2_6",
+                                 trust_remote_code=True),
+     "MiniCPMV": _HfExamplesInfo("openbmb/MiniCPM-Llama3-V-2_5",
+-                                extras={"2.6": "openbmb/MiniCPM-V-2_6"},  # noqa: E501
++                                extras={"2.6": "openbmb/MiniCPM-V-2_6", "4.0": "openbmb/MiniCPM-V-4"},  # noqa: E501
+                                 trust_remote_code=True),
+     "MiniMaxVL01ForConditionalGeneration": _HfExamplesInfo("MiniMaxAI/MiniMax-VL-01", # noqa: E501
+                                               trust_remote_code=True,
+diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
+index 92de1f5ef..6ae546dc2 100644
+--- a/vllm/_custom_ops.py
++++ b/vllm/_custom_ops.py
+@@ -268,7 +268,6 @@ def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
+                                           cos_sin_cache, is_neox, rot_dim,
+                                           cos_sin_cache_offsets)
  
- import multiprocessing
-diff --git a/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py b/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
-index f7b81be48..da9a898e9 100644
---- a/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
-+++ b/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from typing import List
-@@ -15,32 +16,62 @@ start_token = "<think>"
- end_token = "</think>"
- 
- SIMPLE_REASONING = {
--    "output": "<think>This is a reasoning section</think>This is the rest",
-+    "output": "This is a reasoning section</think>This is the rest",
-     "reasoning_content": "This is a reasoning section",
-     "content": "This is the rest",
- }
- COMPLETE_REASONING = {
--    "output": "<think>This is a reasoning section</think>",
-+    "output": "This is a reasoning section</think>",
-     "reasoning_content": "This is a reasoning section",
-     "content": None,
- }
--NO_REASONING = {
-+NO_CONTENT = {
-+    "output": "This is content",
-+    "reasoning_content": "This is content",
-+    "content": None,
-+}
-+NO_REASONING_STREAMING = {
-     "output": "This is a reasoning section",
--    "reasoning_content": None,
--    "content": "This is a reasoning section",
-+    "reasoning_content": "This is a reasoning section",
-+    "content": None,
- }
- MULTIPLE_LINES = {
--    "output": "<think>This\nThat</think>This is the rest\nThat",
-+    "output": "This\nThat</think>This is the rest\nThat",
-     "reasoning_content": "This\nThat",
-     "content": "This is the rest\nThat",
- }
- SHORTEST_REASONING_NO_STREAMING = {
--    "output": "<think></think>This is the rest",
-+    "output": "</think>This is the rest",
-     "reasoning_content": "",
-     "content": "This is the rest",
- }
- SHORTEST_REASONING = {
--    "output": "<think></think>This is the rest",
-+    "output": "</think>This is the rest",
-+    "reasoning_content": None,
-+    "content": "This is the rest",
-+}
-+REASONING_WITH_THINK = {
-+    "output": "<think>This is a reasoning section</think>This is the rest",
-+    "reasoning_content": "This is a reasoning section",
-+    "content": "This is the rest",
-+}
-+COMPLETE_REASONING_WITH_THINK = {
-+    "output": "<think>This is a reasoning section</think>",
-+    "reasoning_content": "This is a reasoning section",
-+    "content": None,
-+}
-+MULTIPLE_LINES_WITH_THINK = {
-+    "output": "<think>This\nThat</think>This is the rest\nThat",
-+    "reasoning_content": "This\nThat",
-+    "content": "This is the rest\nThat",
-+}
-+SHORTEST_REASONING_NO_STREAMING_WITH_THINK = {
-+    "output": "</think>This is the rest",
-+    "reasoning_content": "",
-+    "content": "This is the rest",
-+}
-+SHORTEST_REASONING_WITH_THINK = {
-+    "output": "</think>This is the rest",
-     "reasoning_content": None,
-     "content": "This is the rest",
- }
-@@ -49,37 +80,37 @@ TEST_CASES = [
-     pytest.param(
-         False,
-         SIMPLE_REASONING,
--        id="simple_streaming",
-+        id="simple_reasoning",
-     ),
-     pytest.param(
-         True,
-         SIMPLE_REASONING,
--        id="simple_streaming",
-+        id="simple_reasoning_streaming",
-     ),
-     pytest.param(
-         False,
-         COMPLETE_REASONING,
--        id="complete_streaming",
-+        id="complete_reasoning",
-     ),
-     pytest.param(
-         True,
-         COMPLETE_REASONING,
--        id="complete_streaming",
-+        id="complete_reasoning_streaming",
-     ),
-     pytest.param(
-         False,
--        NO_REASONING,
--        id="no_streaming",
-+        NO_CONTENT,
-+        id="no_content_token",
-     ),
-     pytest.param(
-         True,
--        NO_REASONING,
--        id="no_streaming",
-+        NO_REASONING_STREAMING,
-+        id="no_reasoning_token_streaming",
-     ),
-     pytest.param(
-         False,
-         MULTIPLE_LINES,
--        id="multiple_lines_streaming",
-+        id="multiple_lines",
-     ),
-     pytest.param(
-         True,
-@@ -89,23 +120,65 @@ TEST_CASES = [
-     pytest.param(
-         True,
-         SHORTEST_REASONING,
--        id="shortest_streaming",
-+        id="shortest",
-     ),
-     pytest.param(
-         False,
-         SHORTEST_REASONING_NO_STREAMING,
-         id="shortest_streaming",
-     ),
-+    pytest.param(
-+        False,
-+        REASONING_WITH_THINK,
-+        id="reasoning_with_think",
-+    ),
-+    pytest.param(
-+        True,
-+        REASONING_WITH_THINK,
-+        id="reasoning_with_think_streaming",
-+    ),
-+    pytest.param(
-+        False,
-+        COMPLETE_REASONING_WITH_THINK,
-+        id="complete_reasoning_with_think",
-+    ),
-+    pytest.param(
-+        True,
-+        COMPLETE_REASONING_WITH_THINK,
-+        id="complete_reasoning_with_think_streaming",
-+    ),
-+    pytest.param(
-+        False,
-+        MULTIPLE_LINES_WITH_THINK,
-+        id="multiple_lines_with_think",
-+    ),
-+    pytest.param(
-+        True,
-+        MULTIPLE_LINES_WITH_THINK,
-+        id="multiple_lines_with_think_streaming",
-+    ),
-+    pytest.param(
-+        False,
-+        SHORTEST_REASONING_NO_STREAMING_WITH_THINK,
-+        id="shortest_with_think",
-+    ),
-+    pytest.param(
-+        True,
-+        SHORTEST_REASONING_WITH_THINK,
-+        id="shortest_with_think_streaming",
-+    ),
- ]
- 
-+# Global tokenizer initialization to avoid repeated loading
-+tokenizer = AutoTokenizer.from_pretrained("/pde_ai/models/llm/OPT/opt-125/")
-+tokenizer.add_tokens([start_token, end_token])
-+
- 
- @pytest.mark.parametrize("streaming, param_dict", TEST_CASES)
- def test_reasoning(
-     streaming: bool,
-     param_dict: dict,
- ):
--    tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
--    tokenizer.add_tokens([start_token, end_token])
-     output = tokenizer.tokenize(param_dict["output"])
-     # decode everything to tokens
-     output_tokens: List[str] = [
-@@ -119,4 +192,4 @@ def test_reasoning(
-                                                   streaming=streaming)
- 
-     assert reasoning == param_dict["reasoning_content"]
--    assert content == param_dict["content"]
-+    assert content == param_dict["content"]
-\ No newline at end of file
-diff --git a/tests/kernels/test_awq.py b/tests/kernels/test_awq.py
-index ace75a336..cb9c3b023 100644
---- a/tests/kernels/test_awq.py
-+++ b/tests/kernels/test_awq.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import os
-@@ -5,7 +6,7 @@ import os
- import pytest
- import torch
- 
--from tests.kernels.utils import opcheck
-+# from tests.kernels.utils import opcheck
- from vllm import _custom_ops as ops  # noqa: F401
- 
- 
-@@ -22,17 +23,17 @@ def test_awq_dequantize_opcheck():
-     split_k_iters = 0
-     thx = 0
-     thy = 0
--    opcheck(torch.ops._C.awq_dequantize,
--            (qweight, scales, zeros, split_k_iters, thx, thy))
-+    torch.ops._C.awq_dequantize(qweight, scales, zeros, split_k_iters, thx, thy)
- 
- 
- @pytest.mark.skipif(not hasattr(torch.ops._C, "awq_gemm"),
-                     reason="AWQ is not supported on this GPU type.")
--def test_awq_gemm_opcheck():
-+@pytest.mark.parametrize("dtype_bf16", [True, False])
-+def test_awq_gemm_opcheck(dtype_bf16: bool):
-     os.environ["VLLM_USE_TRITON_AWQ"] = "0"
--    input = torch.rand((2, 8192), device='cuda', dtype=torch.float16)
-+    input = torch.rand((2, 8192), device='cuda', dtype=torch.float16 if not dtype_bf16 else torch.bfloat16)
-     qweight = torch.randint(-2000000000,
--                            2000000000, (8192, 256),
-+                            2000000000, (256* 8, (input.shape[1] + 8 - 1) // 8),
-                             device='cuda',
-                             dtype=torch.int32)
-     scales = torch.randint(-2000000000,
-@@ -41,5 +42,9 @@ def test_awq_gemm_opcheck():
-                            dtype=torch.int32)
-     qzeros = torch.empty((64, 2048), device='cuda', dtype=torch.float16)
-     split_k_iters = 8
--    opcheck(torch.ops._C.awq_gemm,
--            (input, qweight, qzeros, scales, split_k_iters))
-+
-+    temp_space = torch.empty(0)
-+    if input.dtype == torch.bfloat16:
-+            temp_space = torch.zeros(input.shape[0], qweight.shape[0],
-+                                        dtype=torch.float32, device="cuda")
-+    torch.ops._C.awq_gemm(input, qweight, qzeros, scales, split_k_iters, temp_space, dtype_bf16)
-diff --git a/tests/kernels/test_causal_conv1d.py b/tests/kernels/test_causal_conv1d.py
-index 93064e23d..69cd0208c 100644
---- a/tests/kernels/test_causal_conv1d.py
-+++ b/tests/kernels/test_causal_conv1d.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from typing import Optional
-diff --git a/tests/kernels/test_cutlass.py b/tests/kernels/test_cutlass.py
-index 49fd8ed63..e225b10e5 100644
---- a/tests/kernels/test_cutlass.py
-+++ b/tests/kernels/test_cutlass.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Tests for cutlass kernels
- 
-@@ -8,7 +9,7 @@ from typing import Type
- import pytest
- import torch
- 
--from tests.kernels.utils import opcheck
-+#from tests.kernels.utils import opcheck
- from vllm import _custom_ops as ops
- from vllm.platforms import current_platform
- from vllm.utils import cdiv
-@@ -99,8 +100,8 @@ def cutlass_fp8_gemm_helper(m: int,
- 
-     torch.testing.assert_close(out, baseline, rtol=1e-2, atol=5e-2)
- 
--    opcheck(torch.ops._C.cutlass_scaled_mm,
--            (out, a, b, scale_a, scale_b, bias))
-+    #opcheck(torch.ops._C.cutlass_scaled_mm,
-+    #        (out, a, b, scale_a, scale_b, bias))
- 
- 
- def cutlass_int8_gemm_helper(m: int,
-@@ -132,8 +133,8 @@ def cutlass_int8_gemm_helper(m: int,
- 
-     torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)
- 
--    opcheck(torch.ops._C.cutlass_scaled_mm,
--            (out, a, b, scale_a, scale_b, bias))
-+    #opcheck(torch.ops._C.cutlass_scaled_mm,
-+    #        (out, a, b, scale_a, scale_b, bias))
- 
- 
- @pytest.mark.parametrize("m,n,k", MNK_FACTORS)
-@@ -420,7 +421,8 @@ def test_cutlass_int8_azp(m: int, n: int, k: int, out_dtype: torch.dtype,
-     atol = 1e-3
-     torch.testing.assert_close(out, baseline_dq, rtol=rtol, atol=atol)
-     torch.testing.assert_close(out, baseline_q, rtol=rtol, atol=atol)
--
-+    
-+    """
-     if azp_per_token:
-         opcheck(torch.ops._C.cutlass_scaled_mm_azp,
-                 (out, aq_i8, bq_i8, scale_a, scale_b, azp_adj_i32, azp_i32,
-@@ -429,7 +431,7 @@ def test_cutlass_int8_azp(m: int, n: int, k: int, out_dtype: torch.dtype,
-         opcheck(torch.ops._C.cutlass_scaled_mm_azp,
-                 (out, aq_i8, bq_i8, scale_a, scale_b, azp_with_adj_i32, None,
-                  func_bias))
--
-+    """
- 
- # Test working with a subset of A and B
- def test_cutlass_subset():
-@@ -505,6 +507,7 @@ def test_cutlass_cuda_graph(per_act_token: bool, per_out_ch: bool):
-                         scale_b * b.to(dtype=torch.float32)).to(torch.bfloat16)
-     torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)
- 
--
-+"""
- def test_cutlass_support_opcheck():
-     opcheck(torch.ops._C.cutlass_scaled_mm_supports_fp8, (capability, ))
-+"""
-diff --git a/tests/kernels/test_fused_quant_layernorm.py b/tests/kernels/test_fused_quant_layernorm.py
-index d4b674b23..797184682 100644
---- a/tests/kernels/test_fused_quant_layernorm.py
-+++ b/tests/kernels/test_fused_quant_layernorm.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from typing import Optional, Tuple, Union
-diff --git a/tests/kernels/test_int8_quant.py b/tests/kernels/test_int8_quant.py
-index 25dcb587e..ca9892055 100644
---- a/tests/kernels/test_int8_quant.py
-+++ b/tests/kernels/test_int8_quant.py
-@@ -1,10 +1,11 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import pytest
- import torch
- 
- from tests.kernels.quant_utils import ref_dynamic_per_token_quant
--from tests.kernels.utils import opcheck
-+#from tests.kernels.utils import opcheck
- from vllm._custom_ops import scaled_int8_quant
- from vllm.platforms import current_platform
- 
-@@ -14,7 +15,7 @@ NUM_TOKENS = [1, 7, 83, 4096]  # Arbitrary values for testing
- SEEDS = [0]
- SCALE = [0.1, 2.1]
- 
--
-+"""
- def opcheck_int8_quant_static(output, input, scale, azp=None):
-     if azp is None:
-         opcheck(torch.ops._C.static_scaled_int8_quant,
-@@ -37,7 +38,7 @@ def opcheck_int8_quant_dynamic(output, input, symmetric=True):
-                           dtype=torch.int32)
-         opcheck(torch.ops._C.dynamic_scaled_int8_quant,
-                 (output, input, scale, azp))
--
-+"""
- 
- @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
- @pytest.mark.parametrize("hidden_size", HIDDEN_SIZES)
-@@ -59,7 +60,7 @@ def test_dynamic_scaled_int8_quant(num_tokens: int, hidden_size: int,
-     # big atol to account for rounding errors
-     torch.testing.assert_close(ops_out, ref_out, atol=1, rtol=0.0)
- 
--    opcheck_int8_quant_dynamic(ops_out, x)
-+    #opcheck_int8_quant_dynamic(ops_out, x)
- 
- 
- @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
-@@ -98,7 +99,7 @@ def test_dynamic_scaled_int8_azp_quant(num_tokens: int, hidden_size: int,
-     # if AZP is off by 1, after rounding-to-even, the output may be off by 2
-     torch.testing.assert_close(ops_out, torch_out, atol=2, rtol=0.0)
- 
--    opcheck_int8_quant_dynamic(ops_out, x, False)
-+    #opcheck_int8_quant_dynamic(ops_out, x, False)
- 
- 
- @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
-@@ -124,7 +125,7 @@ def test_static_scaled_int8_quant(num_tokens: int, hidden_size: int,
-     # big atol to account for rounding errors
-     torch.testing.assert_close(out1, out2, atol=1, rtol=0.0)
- 
--    opcheck_int8_quant_static(out2, x, scale_arg)
-+    #opcheck_int8_quant_static(out2, x, scale_arg)
- 
- 
- @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
-@@ -158,7 +159,7 @@ def test_static_scaled_int8_azp_quant(num_tokens: int, hidden_size: int,
-     # big atol to account for rounding errors
-     torch.testing.assert_close(out1, out2, atol=1, rtol=0.0)
- 
--    opcheck_int8_quant_static(out2, x, scale_arg, azp_arg)
-+    #opcheck_int8_quant_static(out2, x, scale_arg, azp_arg)
- 
- 
- @pytest.mark.parametrize("is_max", [True, False])
-diff --git a/tests/kernels/test_mamba_ssm.py b/tests/kernels/test_mamba_ssm.py
-index 84d4c347e..048961262 100644
---- a/tests/kernels/test_mamba_ssm.py
-+++ b/tests/kernels/test_mamba_ssm.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import pytest
-diff --git a/tests/kernels/test_marlin_gemm.py b/tests/kernels/test_marlin_gemm.py
-index b96aca06c..75d032a4e 100644
---- a/tests/kernels/test_marlin_gemm.py
-+++ b/tests/kernels/test_marlin_gemm.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Tests for the marlin kernel.
- 
-diff --git a/tests/kernels/utils.py b/tests/kernels/utils.py
-index 5be111d71..b7c88f22d 100644
---- a/tests/kernels/utils.py
-+++ b/tests/kernels/utils.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Kernel test utils"""
- 
-diff --git a/tests/models/registry.py b/tests/models/registry.py
-index 20787fe00..f635cbf06 100644
---- a/tests/models/registry.py
-+++ b/tests/models/registry.py
-@@ -1,324 +1,326 @@
--# SPDX-License-Identifier: Apache-2.0
--
--from dataclasses import dataclass, field
--from typing import AbstractSet, Any, Literal, Mapping, Optional
--
--import pytest
--from packaging.version import Version
--from transformers import __version__ as TRANSFORMERS_VERSION
--
--
--@dataclass(frozen=True)
--class _HfExamplesInfo:
--    default: str
--    """The default model to use for testing this architecture."""
--
--    extras: Mapping[str, str] = field(default_factory=dict)
--    """Extra models to use for testing this architecture."""
--
--    tokenizer: Optional[str] = None
--    """Set the tokenizer to load for this architecture."""
--
--    tokenizer_mode: str = "auto"
--    """Set the tokenizer type for this architecture."""
--
--    speculative_model: Optional[str] = None
--    """
--    The default model to use for testing this architecture, which is only used
--    for speculative decoding.
--    """
--
--    min_transformers_version: Optional[str] = None
--    """
--    The minimum version of HF Transformers that is required to run this model.
--    """
--
--    is_available_online: bool = True
--    """
--    Set this to ``False`` if the name of this architecture no longer exists on
--    the HF repo. To maintain backwards compatibility, we have not removed them
--    from the main model registry, so without this flag the registry tests will
--    fail.
--    """
--
--    trust_remote_code: bool = False
--    """The ``trust_remote_code`` level required to load the model."""
--
--    hf_overrides: dict[str, Any] = field(default_factory=dict)
--    """The ``hf_overrides`` required to load the model."""
--
--    def check_transformers_version(
--        self,
--        *,
--        on_fail: Literal["error", "skip"],
--    ) -> None:
--        """
--        If the installed transformers version does not meet the requirements,
--        perform the given action.
--        """
--        if self.min_transformers_version is None:
--            return
--
--        current_version = TRANSFORMERS_VERSION
--        required_version = self.min_transformers_version
--        if Version(current_version) < Version(required_version):
--            msg = (
--                f"You have `transformers=={current_version}` installed, but "
--                f"`transformers>={required_version}` is required to run this "
--                "model")
--
--            if on_fail == "error":
--                raise RuntimeError(msg)
--            else:
--                pytest.skip(msg)
--
--    def check_available_online(
--        self,
--        *,
--        on_fail: Literal["error", "skip"],
--    ) -> None:
--        """
--        If the model is not available online, perform the given action.
--        """
--        if not self.is_available_online:
--            msg = "Model is not available online"
--
--            if on_fail == "error":
--                raise RuntimeError(msg)
--            else:
--                pytest.skip(msg)
--
--
--# yapf: disable
--_TEXT_GENERATION_EXAMPLE_MODELS = {
--    # [Decoder-only]
--    "AquilaModel": _HfExamplesInfo("BAAI/AquilaChat-7B",
--                                   trust_remote_code=True),
--    "AquilaForCausalLM": _HfExamplesInfo("BAAI/AquilaChat2-7B",
--                                         trust_remote_code=True),
--    "ArcticForCausalLM": _HfExamplesInfo("Snowflake/snowflake-arctic-instruct",
--                                         trust_remote_code=True),
--    "BaiChuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan-7B",
--                                         trust_remote_code=True),
--    "BaichuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan2-7B-chat",
--                                         trust_remote_code=True),
--    "BloomForCausalLM": _HfExamplesInfo("bigscience/bloomz-1b1"),
--    # ChatGLMModel supports multimodal
--    "CohereForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r-v01",
--                                         trust_remote_code=True),
--    "Cohere2ForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r7b-12-2024", # noqa: E501
--                                         trust_remote_code=True),
--    "DbrxForCausalLM": _HfExamplesInfo("databricks/dbrx-instruct"),
--    "DeciLMForCausalLM": _HfExamplesInfo("Deci/DeciLM-7B-instruct",
--                                         trust_remote_code=True),
--    "DeepseekForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-llm-7b-chat"),
--    "DeepseekV2ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V2-Lite-Chat",  # noqa: E501
--                                         trust_remote_code=True),
--    "DeepseekV3ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V3",  # noqa: E501
--                                         trust_remote_code=True),
--    "ExaoneForCausalLM": _HfExamplesInfo("LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"),  # noqa: E501
--    "Fairseq2LlamaForCausalLM": _HfExamplesInfo("mgleize/fairseq2-dummy-Llama-3.2-1B"),  # noqa: E501
--    "FalconForCausalLM": _HfExamplesInfo("tiiuae/falcon-7b"),
--    "GemmaForCausalLM": _HfExamplesInfo("google/gemma-2b"),
--    "Gemma2ForCausalLM": _HfExamplesInfo("google/gemma-2-9b"),
--    "GlmForCausalLM": _HfExamplesInfo("THUDM/glm-4-9b-chat-hf"),
--    "GPT2LMHeadModel": _HfExamplesInfo("gpt2"),
--    "GPTBigCodeForCausalLM": _HfExamplesInfo("bigcode/starcoder"),
--    "GPTJForCausalLM": _HfExamplesInfo("EleutherAI/gpt-j-6b"),
--    "GPTNeoXForCausalLM": _HfExamplesInfo("EleutherAI/pythia-160m"),
--    "GraniteForCausalLM": _HfExamplesInfo("ibm/PowerLM-3b"),
--    "GraniteMoeForCausalLM": _HfExamplesInfo("ibm/PowerMoE-3b"),
--    "InternLMForCausalLM": _HfExamplesInfo("internlm/internlm-chat-7b",
--                                           trust_remote_code=True),
--    "InternLM2ForCausalLM": _HfExamplesInfo("internlm/internlm2-chat-7b",
--                                            trust_remote_code=True),
--    "InternLM2VEForCausalLM": _HfExamplesInfo("OpenGVLab/Mono-InternVL-2B",
--                                              trust_remote_code=True),
--    "InternLM3ForCausalLM": _HfExamplesInfo("internlm/internlm3-8b-instruct",
--                                            trust_remote_code=True),
--    "JAISLMHeadModel": _HfExamplesInfo("inceptionai/jais-13b-chat"),
--    "JambaForCausalLM": _HfExamplesInfo("ai21labs/AI21-Jamba-1.5-Mini"),
--    "LlamaForCausalLM": _HfExamplesInfo("meta-llama/Meta-Llama-3-8B"),
--    "LLaMAForCausalLM": _HfExamplesInfo("decapoda-research/llama-7b-hf",
--                                        is_available_online=False),
--    "MambaForCausalLM": _HfExamplesInfo("state-spaces/mamba-130m-hf"),
--    "FalconMambaForCausalLM": _HfExamplesInfo("tiiuae/falcon-mamba-7b-instruct"),  # noqa: E501
--    "MiniCPMForCausalLM": _HfExamplesInfo("openbmb/MiniCPM-2B-sft-bf16",
--                                         trust_remote_code=True),
--    "MiniCPM3ForCausalLM": _HfExamplesInfo("openbmb/MiniCPM3-4B",
--                                         trust_remote_code=True),
--    "MistralForCausalLM": _HfExamplesInfo("mistralai/Mistral-7B-Instruct-v0.1"),
--    "MixtralForCausalLM": _HfExamplesInfo("mistralai/Mixtral-8x7B-Instruct-v0.1"),  # noqa: E501
--    "QuantMixtralForCausalLM": _HfExamplesInfo("mistral-community/Mixtral-8x22B-v0.1-AWQ"),  # noqa: E501
--    "MptForCausalLM": _HfExamplesInfo("mpt", is_available_online=False),
--    "MPTForCausalLM": _HfExamplesInfo("mosaicml/mpt-7b"),
--    "NemotronForCausalLM": _HfExamplesInfo("nvidia/Minitron-8B-Base"),
--    "OlmoForCausalLM": _HfExamplesInfo("allenai/OLMo-1B-hf"),
--    "Olmo2ForCausalLM": _HfExamplesInfo("shanearora/OLMo-7B-1124-hf"),
--    "OlmoeForCausalLM": _HfExamplesInfo("allenai/OLMoE-1B-7B-0924-Instruct"),
--    "OPTForCausalLM": _HfExamplesInfo("facebook/opt-iml-max-1.3b"),
--    "OrionForCausalLM": _HfExamplesInfo("OrionStarAI/Orion-14B-Chat",
--                                        trust_remote_code=True),
--    "PersimmonForCausalLM": _HfExamplesInfo("adept/persimmon-8b-chat"),
--    "PhiForCausalLM": _HfExamplesInfo("microsoft/phi-2"),
--    "Phi3ForCausalLM": _HfExamplesInfo("microsoft/Phi-3-mini-4k-instruct"),
--    "Phi3SmallForCausalLM": _HfExamplesInfo("microsoft/Phi-3-small-8k-instruct",
--                                            trust_remote_code=True),
--    "PhiMoEForCausalLM": _HfExamplesInfo("microsoft/Phi-3.5-MoE-instruct",
--                                         trust_remote_code=True),
--    # QWenLMHeadModel supports multimodal
--    "Qwen2ForCausalLM": _HfExamplesInfo("Qwen/Qwen2-7B-Instruct"),
--    "Qwen2MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen1.5-MoE-A2.7B-Chat"),
--    "RWForCausalLM": _HfExamplesInfo("tiiuae/falcon-40b",
--                                     is_available_online=False),
--    "StableLMEpochForCausalLM": _HfExamplesInfo("stabilityai/stablelm-zephyr-3b",  # noqa: E501
--                                                is_available_online=False),
--    "StableLmForCausalLM": _HfExamplesInfo("stabilityai/stablelm-3b-4e1t"),
--    "Starcoder2ForCausalLM": _HfExamplesInfo("bigcode/starcoder2-3b"),
--    "SolarForCausalLM": _HfExamplesInfo("upstage/solar-pro-preview-instruct"),
--    "TeleChat2ForCausalLM": _HfExamplesInfo("Tele-AI/TeleChat2-3B",
--                                            trust_remote_code=True),
--    "XverseForCausalLM": _HfExamplesInfo("xverse/XVERSE-7B-Chat",
--                                         is_available_online=False,
--                                         trust_remote_code=True),
--    # [Encoder-decoder]
--    "BartModel": _HfExamplesInfo("facebook/bart-base"),
--    "BartForConditionalGeneration": _HfExamplesInfo("facebook/bart-large-cnn"),
--    # Florence-2 uses BartFastTokenizer which can't be loaded from AutoTokenizer
--    # Therefore, we borrow the BartTokenizer from the original Bart model
--    "Florence2ForConditionalGeneration": _HfExamplesInfo("microsoft/Florence-2-base",  # noqa: E501
--                                                         tokenizer="facebook/bart-base",
--                                                         trust_remote_code=True),  # noqa: E501
--}
--
--_EMBEDDING_EXAMPLE_MODELS = {
--    # [Text-only]
--    "BertModel": _HfExamplesInfo("BAAI/bge-base-en-v1.5"),
--    "Gemma2Model": _HfExamplesInfo("BAAI/bge-multilingual-gemma2"),
--    "GritLM": _HfExamplesInfo("parasail-ai/GritLM-7B-vllm"),
--    "InternLM2ForRewardModel": _HfExamplesInfo("internlm/internlm2-1_8b-reward",
--                                               trust_remote_code=True),
--    "JambaForSequenceClassification": _HfExamplesInfo("ai21labs/Jamba-tiny-reward-dev"),  # noqa: E501
--    "LlamaModel": _HfExamplesInfo("llama", is_available_online=False),
--    "MistralModel": _HfExamplesInfo("intfloat/e5-mistral-7b-instruct"),
--    "Qwen2Model": _HfExamplesInfo("ssmits/Qwen2-7B-Instruct-embed-base"),
--    "Qwen2ForRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-RM-72B"),
--    "Qwen2ForProcessRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-PRM-7B"),
--    "Qwen2ForSequenceClassification": _HfExamplesInfo("jason9693/Qwen2.5-1.5B-apeach"),  # noqa: E501
--    "RobertaModel": _HfExamplesInfo("sentence-transformers/stsb-roberta-base-v2"),  # noqa: E501
--    "RobertaForMaskedLM": _HfExamplesInfo("sentence-transformers/all-roberta-large-v1"),  # noqa: E501
--    "XLMRobertaModel": _HfExamplesInfo("intfloat/multilingual-e5-large"),
--    # [Multimodal]
--    "LlavaNextForConditionalGeneration": _HfExamplesInfo("royokong/e5-v"),
--    "Phi3VForCausalLM": _HfExamplesInfo("TIGER-Lab/VLM2Vec-Full",
--                                         trust_remote_code=True),
--    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("MrLight/dse-qwen2-2b-mrl-v1"), # noqa: E501
--}
--
--_CROSS_ENCODER_EXAMPLE_MODELS = {
--    # [Text-only]
--    "BertForSequenceClassification": _HfExamplesInfo("cross-encoder/ms-marco-MiniLM-L-6-v2"),  # noqa: E501
--    "RobertaForSequenceClassification": _HfExamplesInfo("cross-encoder/quora-roberta-base"),  # noqa: E501
--    "XLMRobertaForSequenceClassification": _HfExamplesInfo("BAAI/bge-reranker-v2-m3"),  # noqa: E501
--}
--
--_MULTIMODAL_EXAMPLE_MODELS = {
--    # [Decoder-only]
--    "AriaForConditionalGeneration": _HfExamplesInfo("rhymes-ai/Aria"),
--    "Blip2ForConditionalGeneration": _HfExamplesInfo("Salesforce/blip2-opt-2.7b"),  # noqa: E501
--    "ChameleonForConditionalGeneration": _HfExamplesInfo("facebook/chameleon-7b"),  # noqa: E501
--    "ChatGLMModel": _HfExamplesInfo("THUDM/glm-4v-9b",
--                                    extras={"text_only": "THUDM/chatglm3-6b"},
--                                    trust_remote_code=True),
--    "ChatGLMForConditionalGeneration": _HfExamplesInfo("chatglm2-6b",
--                                                       is_available_online=False),
--    "DeepseekVLV2ForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-vl2-tiny",  # noqa: E501
--                                               hf_overrides={"architectures": ["DeepseekVLV2ForCausalLM"]}),  # noqa: E501
--    "FuyuForCausalLM": _HfExamplesInfo("adept/fuyu-8b"),
--    "H2OVLChatModel": _HfExamplesInfo("h2oai/h2ovl-mississippi-800m"),
--    "InternVLChatModel": _HfExamplesInfo("OpenGVLab/InternVL2-1B",
--                                         trust_remote_code=True),
--    "Idefics3ForConditionalGeneration": _HfExamplesInfo("HuggingFaceM4/Idefics3-8B-Llama3"),  # noqa: E501
--    "LlavaForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-1.5-7b-hf",
--                                                     extras={"mistral": "mistral-community/pixtral-12b"}),  # noqa: E501
--    "LlavaNextForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-v1.6-mistral-7b-hf"),  # noqa: E501
--    "LlavaNextVideoForConditionalGeneration": _HfExamplesInfo("llava-hf/LLaVA-NeXT-Video-7B-hf"),  # noqa: E501
--    "LlavaOnevisionForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-onevision-qwen2-0.5b-ov-hf"),  # noqa: E501
--    "MantisForConditionalGeneration": _HfExamplesInfo("TIGER-Lab/Mantis-8B-siglip-llama3",  # noqa: E501
--                                                      hf_overrides={"architectures": ["MantisForConditionalGeneration"]}),  # noqa: E501
--    "MiniCPMO": _HfExamplesInfo("openbmb/MiniCPM-o-2_6",
--                                trust_remote_code=True),
--    "MiniCPMV": _HfExamplesInfo("openbmb/MiniCPM-V-2_6",
--                                trust_remote_code=True),
--    "MolmoForCausalLM": _HfExamplesInfo("allenai/Molmo-7B-D-0924",
--                                        trust_remote_code=True),
--    "NVLM_D": _HfExamplesInfo("nvidia/NVLM-D-72B",
--                              trust_remote_code=True),
--    "PaliGemmaForConditionalGeneration": _HfExamplesInfo("google/paligemma-3b-pt-224"),  # noqa: E501
--    "Phi3VForCausalLM": _HfExamplesInfo("microsoft/Phi-3-vision-128k-instruct",
--                                        trust_remote_code=True),
--    "PixtralForConditionalGeneration": _HfExamplesInfo("mistralai/Pixtral-12B-2409",  # noqa: E501
--                                                       tokenizer_mode="mistral"),
--    "QWenLMHeadModel": _HfExamplesInfo("Qwen/Qwen-VL-Chat",
--                                       extras={"text_only": "Qwen/Qwen-7B-Chat"},  # noqa: E501
--                                       trust_remote_code=True),
--    "Qwen2AudioForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-Audio-7B-Instruct"),  # noqa: E501
--    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-VL-2B-Instruct"),  # noqa: E501
--    "Qwen2_5_VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2.5-VL-3B-Instruct",  # noqa: E501
--                                                          min_transformers_version="4.49"),  # noqa: E501
--    "UltravoxModel": _HfExamplesInfo("fixie-ai/ultravox-v0_3",
--                                     trust_remote_code=True),
--    # [Encoder-decoder]
--    "MllamaForConditionalGeneration": _HfExamplesInfo("meta-llama/Llama-3.2-11B-Vision-Instruct"),  # noqa: E501
--    "WhisperForConditionalGeneration": _HfExamplesInfo("openai/whisper-large-v3"),  # noqa: E501
--}
--
--_SPECULATIVE_DECODING_EXAMPLE_MODELS = {
--    "EAGLEModel": _HfExamplesInfo("JackFram/llama-68m",
--                                  speculative_model="abhigoyal/vllm-eagle-llama-68m-random"),  # noqa: E501
--    "MedusaModel": _HfExamplesInfo("JackFram/llama-68m",
--                                   speculative_model="abhigoyal/vllm-medusa-llama-68m-random"),  # noqa: E501
--    "MLPSpeculatorPreTrainedModel": _HfExamplesInfo("JackFram/llama-160m",
--                                                    speculative_model="ibm-ai-platform/llama-160m-accelerator"),  # noqa: E501
--}
--
--_FALLBACK_MODEL = {
--    "TransformersModel": _HfExamplesInfo("ArthurZ/Ilama-3.2-1B", trust_remote_code=True),  # noqa: E501
--}
--
--_EXAMPLE_MODELS = {
--    **_TEXT_GENERATION_EXAMPLE_MODELS,
--    **_EMBEDDING_EXAMPLE_MODELS,
--    **_CROSS_ENCODER_EXAMPLE_MODELS,
--    **_MULTIMODAL_EXAMPLE_MODELS,
--    **_SPECULATIVE_DECODING_EXAMPLE_MODELS,
--    **_FALLBACK_MODEL,
--}
--
--
--class HfExampleModels:
--    def __init__(self, hf_models: Mapping[str, _HfExamplesInfo]) -> None:
--        super().__init__()
--
--        self.hf_models = hf_models
--
--    def get_supported_archs(self) -> AbstractSet[str]:
--        return self.hf_models.keys()
--
--    def get_hf_info(self, model_arch: str) -> _HfExamplesInfo:
--        return self.hf_models[model_arch]
--
--    def find_hf_info(self, model_id: str) -> _HfExamplesInfo:
--        for info in self.hf_models.values():
--            if info.default == model_id:
--                return info
--
--        # Fallback to extras
--        for info in self.hf_models.values():
--            if any(extra == model_id for extra in info.extras.values()):
--                return info
--
--        raise ValueError(f"No example model defined for {model_id}")
--
--
--HF_EXAMPLE_MODELS = HfExampleModels(_EXAMPLE_MODELS)
-+# SPDX-License-Identifier: Apache-2.0
-+
-+from dataclasses import dataclass, field
-+from typing import AbstractSet, Any, Literal, Mapping, Optional
-+
-+import pytest
-+from packaging.version import Version
-+from transformers import __version__ as TRANSFORMERS_VERSION
-+
-+
-+@dataclass(frozen=True)
-+class _HfExamplesInfo:
-+    default: str
-+    """The default model to use for testing this architecture."""
-+
-+    extras: Mapping[str, str] = field(default_factory=dict)
-+    """Extra models to use for testing this architecture."""
-+
-+    tokenizer: Optional[str] = None
-+    """Set the tokenizer to load for this architecture."""
-+
-+    tokenizer_mode: str = "auto"
-+    """Set the tokenizer type for this architecture."""
-+
-+    speculative_model: Optional[str] = None
-+    """
-+    The default model to use for testing this architecture, which is only used
-+    for speculative decoding.
-+    """
-+
-+    min_transformers_version: Optional[str] = None
-+    """
-+    The minimum version of HF Transformers that is required to run this model.
-+    """
-+
-+    is_available_online: bool = True
-+    """
-+    Set this to ``False`` if the name of this architecture no longer exists on
-+    the HF repo. To maintain backwards compatibility, we have not removed them
-+    from the main model registry, so without this flag the registry tests will
-+    fail.
-+    """
-+
-+    trust_remote_code: bool = False
-+    """The ``trust_remote_code`` level required to load the model."""
-+
-+    hf_overrides: dict[str, Any] = field(default_factory=dict)
-+    """The ``hf_overrides`` required to load the model."""
-+
-+    def check_transformers_version(
-+        self,
-+        *,
-+        on_fail: Literal["error", "skip"],
-+    ) -> None:
-+        """
-+        If the installed transformers version does not meet the requirements,
-+        perform the given action.
-+        """
-+        if self.min_transformers_version is None:
-+            return
-+
-+        current_version = TRANSFORMERS_VERSION
-+        required_version = self.min_transformers_version
-+        if Version(current_version) < Version(required_version):
-+            msg = (
-+                f"You have `transformers=={current_version}` installed, but "
-+                f"`transformers>={required_version}` is required to run this "
-+                "model")
-+
-+            if on_fail == "error":
-+                raise RuntimeError(msg)
-+            else:
-+                pytest.skip(msg)
-+
-+    def check_available_online(
-+        self,
-+        *,
-+        on_fail: Literal["error", "skip"],
-+    ) -> None:
-+        """
-+        If the model is not available online, perform the given action.
-+        """
-+        if not self.is_available_online:
-+            msg = "Model is not available online"
-+
-+            if on_fail == "error":
-+                raise RuntimeError(msg)
-+            else:
-+                pytest.skip(msg)
-+
-+
-+# yapf: disable
-+_TEXT_GENERATION_EXAMPLE_MODELS = {
-+    # [Decoder-only]
-+    "AquilaModel": _HfExamplesInfo("BAAI/AquilaChat-7B",
-+                                   trust_remote_code=True),
-+    "AquilaForCausalLM": _HfExamplesInfo("BAAI/AquilaChat2-7B",
-+                                         trust_remote_code=True),
-+    "ArcticForCausalLM": _HfExamplesInfo("Snowflake/snowflake-arctic-instruct",
-+                                         trust_remote_code=True),
-+    "BaiChuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan-7B",
-+                                         trust_remote_code=True),
-+    "BaichuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan2-7B-chat",
-+                                         trust_remote_code=True),
-+    "BloomForCausalLM": _HfExamplesInfo("bigscience/bloomz-1b1"),
-+    # ChatGLMModel supports multimodal
-+    "CohereForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r-v01",
-+                                         trust_remote_code=True),
-+    "Cohere2ForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r7b-12-2024", # noqa: E501
-+                                         trust_remote_code=True),
-+    "DbrxForCausalLM": _HfExamplesInfo("databricks/dbrx-instruct"),
-+    "DeciLMForCausalLM": _HfExamplesInfo("Deci/DeciLM-7B-instruct",
-+                                         trust_remote_code=True),
-+    "DeepseekForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-llm-7b-chat"),
-+    "DeepseekV2ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V2-Lite-Chat",  # noqa: E501
-+                                         trust_remote_code=True),
-+    "DeepseekV3ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V3",  # noqa: E501
-+                                         trust_remote_code=True),
-+    "ExaoneForCausalLM": _HfExamplesInfo("LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"),  # noqa: E501
-+    "Fairseq2LlamaForCausalLM": _HfExamplesInfo("mgleize/fairseq2-dummy-Llama-3.2-1B"),  # noqa: E501
-+    "FalconForCausalLM": _HfExamplesInfo("tiiuae/falcon-7b"),
-+    "GemmaForCausalLM": _HfExamplesInfo("google/gemma-2b"),
-+    "Gemma2ForCausalLM": _HfExamplesInfo("google/gemma-2-9b"),
-+    "GlmForCausalLM": _HfExamplesInfo("THUDM/glm-4-9b-chat-hf"),
-+    "GPT2LMHeadModel": _HfExamplesInfo("gpt2"),
-+    "GPTBigCodeForCausalLM": _HfExamplesInfo("bigcode/starcoder"),
-+    "GPTJForCausalLM": _HfExamplesInfo("EleutherAI/gpt-j-6b"),
-+    "GPTNeoXForCausalLM": _HfExamplesInfo("EleutherAI/pythia-160m"),
-+    "GraniteForCausalLM": _HfExamplesInfo("ibm/PowerLM-3b"),
-+    "GraniteMoeForCausalLM": _HfExamplesInfo("ibm/PowerMoE-3b"),
-+    "InternLMForCausalLM": _HfExamplesInfo("internlm/internlm-chat-7b",
-+                                           trust_remote_code=True),
-+    "InternLM2ForCausalLM": _HfExamplesInfo("internlm/internlm2-chat-7b",
-+                                            trust_remote_code=True),
-+    "InternLM2VEForCausalLM": _HfExamplesInfo("OpenGVLab/Mono-InternVL-2B",
-+                                              trust_remote_code=True),
-+    "InternLM3ForCausalLM": _HfExamplesInfo("internlm/internlm3-8b-instruct",
-+                                            trust_remote_code=True),
-+    "JAISLMHeadModel": _HfExamplesInfo("inceptionai/jais-13b-chat"),
-+    "JambaForCausalLM": _HfExamplesInfo("ai21labs/AI21-Jamba-1.5-Mini"),
-+    "LlamaForCausalLM": _HfExamplesInfo("meta-llama/Meta-Llama-3-8B"),
-+    "LLaMAForCausalLM": _HfExamplesInfo("decapoda-research/llama-7b-hf",
-+                                        is_available_online=False),
-+    "MambaForCausalLM": _HfExamplesInfo("state-spaces/mamba-130m-hf"),
-+    "FalconMambaForCausalLM": _HfExamplesInfo("tiiuae/falcon-mamba-7b-instruct"),  # noqa: E501
-+    "MiniCPMForCausalLM": _HfExamplesInfo("openbmb/MiniCPM-2B-sft-bf16",
-+                                         trust_remote_code=True),
-+    "MiniCPM3ForCausalLM": _HfExamplesInfo("openbmb/MiniCPM3-4B",
-+                                         trust_remote_code=True),
-+    "MistralForCausalLM": _HfExamplesInfo("mistralai/Mistral-7B-Instruct-v0.1"),
-+    "MixtralForCausalLM": _HfExamplesInfo("mistralai/Mixtral-8x7B-Instruct-v0.1"),  # noqa: E501
-+    "QuantMixtralForCausalLM": _HfExamplesInfo("mistral-community/Mixtral-8x22B-v0.1-AWQ"),  # noqa: E501
-+    "MptForCausalLM": _HfExamplesInfo("mpt", is_available_online=False),
-+    "MPTForCausalLM": _HfExamplesInfo("mosaicml/mpt-7b"),
-+    "NemotronForCausalLM": _HfExamplesInfo("nvidia/Minitron-8B-Base"),
-+    "OlmoForCausalLM": _HfExamplesInfo("allenai/OLMo-1B-hf"),
-+    "Olmo2ForCausalLM": _HfExamplesInfo("shanearora/OLMo-7B-1124-hf"),
-+    "OlmoeForCausalLM": _HfExamplesInfo("allenai/OLMoE-1B-7B-0924-Instruct"),
-+    "OPTForCausalLM": _HfExamplesInfo("facebook/opt-iml-max-1.3b"),
-+    "OrionForCausalLM": _HfExamplesInfo("OrionStarAI/Orion-14B-Chat",
-+                                        trust_remote_code=True),
-+    "PersimmonForCausalLM": _HfExamplesInfo("adept/persimmon-8b-chat"),
-+    "PhiForCausalLM": _HfExamplesInfo("microsoft/phi-2"),
-+    "Phi3ForCausalLM": _HfExamplesInfo("microsoft/Phi-3-mini-4k-instruct"),
-+    "Phi3SmallForCausalLM": _HfExamplesInfo("microsoft/Phi-3-small-8k-instruct",
-+                                            trust_remote_code=True),
-+    "PhiMoEForCausalLM": _HfExamplesInfo("microsoft/Phi-3.5-MoE-instruct",
-+                                         trust_remote_code=True),
-+    # QWenLMHeadModel supports multimodal
-+    "Qwen2ForCausalLM": _HfExamplesInfo("Qwen/Qwen2-7B-Instruct"),
-+    "Qwen2MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen1.5-MoE-A2.7B-Chat"),
-+    "Qwen3ForCausalLM": _HfExamplesInfo("Qwen/Qwen3-8B"),
-+    "Qwen3MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen3-30B-A3B"),
-+    "RWForCausalLM": _HfExamplesInfo("tiiuae/falcon-40b",
-+                                     is_available_online=False),
-+    "StableLMEpochForCausalLM": _HfExamplesInfo("stabilityai/stablelm-zephyr-3b",  # noqa: E501
-+                                                is_available_online=False),
-+    "StableLmForCausalLM": _HfExamplesInfo("stabilityai/stablelm-3b-4e1t"),
-+    "Starcoder2ForCausalLM": _HfExamplesInfo("bigcode/starcoder2-3b"),
-+    "SolarForCausalLM": _HfExamplesInfo("upstage/solar-pro-preview-instruct"),
-+    "TeleChat2ForCausalLM": _HfExamplesInfo("Tele-AI/TeleChat2-3B",
-+                                            trust_remote_code=True),
-+    "XverseForCausalLM": _HfExamplesInfo("xverse/XVERSE-7B-Chat",
-+                                         is_available_online=False,
-+                                         trust_remote_code=True),
-+    # [Encoder-decoder]
-+    "BartModel": _HfExamplesInfo("facebook/bart-base"),
-+    "BartForConditionalGeneration": _HfExamplesInfo("facebook/bart-large-cnn"),
-+    # Florence-2 uses BartFastTokenizer which can't be loaded from AutoTokenizer
-+    # Therefore, we borrow the BartTokenizer from the original Bart model
-+    "Florence2ForConditionalGeneration": _HfExamplesInfo("microsoft/Florence-2-base",  # noqa: E501
-+                                                         tokenizer="facebook/bart-base",
-+                                                         trust_remote_code=True),  # noqa: E501
-+}
-+
-+_EMBEDDING_EXAMPLE_MODELS = {
-+    # [Text-only]
-+    "BertModel": _HfExamplesInfo("BAAI/bge-base-en-v1.5"),
-+    "Gemma2Model": _HfExamplesInfo("BAAI/bge-multilingual-gemma2"),
-+    "GritLM": _HfExamplesInfo("parasail-ai/GritLM-7B-vllm"),
-+    "InternLM2ForRewardModel": _HfExamplesInfo("internlm/internlm2-1_8b-reward",
-+                                               trust_remote_code=True),
-+    "JambaForSequenceClassification": _HfExamplesInfo("ai21labs/Jamba-tiny-reward-dev"),  # noqa: E501
-+    "LlamaModel": _HfExamplesInfo("llama", is_available_online=False),
-+    "MistralModel": _HfExamplesInfo("intfloat/e5-mistral-7b-instruct"),
-+    "Qwen2Model": _HfExamplesInfo("ssmits/Qwen2-7B-Instruct-embed-base"),
-+    "Qwen2ForRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-RM-72B"),
-+    "Qwen2ForProcessRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-PRM-7B"),
-+    "Qwen2ForSequenceClassification": _HfExamplesInfo("jason9693/Qwen2.5-1.5B-apeach"),  # noqa: E501
-+    "RobertaModel": _HfExamplesInfo("sentence-transformers/stsb-roberta-base-v2"),  # noqa: E501
-+    "RobertaForMaskedLM": _HfExamplesInfo("sentence-transformers/all-roberta-large-v1"),  # noqa: E501
-+    "XLMRobertaModel": _HfExamplesInfo("intfloat/multilingual-e5-large"),
-+    # [Multimodal]
-+    "LlavaNextForConditionalGeneration": _HfExamplesInfo("royokong/e5-v"),
-+    "Phi3VForCausalLM": _HfExamplesInfo("TIGER-Lab/VLM2Vec-Full",
-+                                         trust_remote_code=True),
-+    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("MrLight/dse-qwen2-2b-mrl-v1"), # noqa: E501
-+}
-+
-+_CROSS_ENCODER_EXAMPLE_MODELS = {
-+    # [Text-only]
-+    "BertForSequenceClassification": _HfExamplesInfo("cross-encoder/ms-marco-MiniLM-L-6-v2"),  # noqa: E501
-+    "RobertaForSequenceClassification": _HfExamplesInfo("cross-encoder/quora-roberta-base"),  # noqa: E501
-+    "XLMRobertaForSequenceClassification": _HfExamplesInfo("BAAI/bge-reranker-v2-m3"),  # noqa: E501
-+}
-+
-+_MULTIMODAL_EXAMPLE_MODELS = {
-+    # [Decoder-only]
-+    "AriaForConditionalGeneration": _HfExamplesInfo("rhymes-ai/Aria"),
-+    "Blip2ForConditionalGeneration": _HfExamplesInfo("Salesforce/blip2-opt-2.7b"),  # noqa: E501
-+    "ChameleonForConditionalGeneration": _HfExamplesInfo("facebook/chameleon-7b"),  # noqa: E501
-+    "ChatGLMModel": _HfExamplesInfo("THUDM/glm-4v-9b",
-+                                    extras={"text_only": "THUDM/chatglm3-6b"},
-+                                    trust_remote_code=True),
-+    "ChatGLMForConditionalGeneration": _HfExamplesInfo("chatglm2-6b",
-+                                                       is_available_online=False),
-+    "DeepseekVLV2ForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-vl2-tiny",  # noqa: E501
-+                                               hf_overrides={"architectures": ["DeepseekVLV2ForCausalLM"]}),  # noqa: E501
-+    "FuyuForCausalLM": _HfExamplesInfo("adept/fuyu-8b"),
-+    "H2OVLChatModel": _HfExamplesInfo("h2oai/h2ovl-mississippi-800m"),
-+    "InternVLChatModel": _HfExamplesInfo("OpenGVLab/InternVL2-1B",
-+                                         trust_remote_code=True),
-+    "Idefics3ForConditionalGeneration": _HfExamplesInfo("HuggingFaceM4/Idefics3-8B-Llama3"),  # noqa: E501
-+    "LlavaForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-1.5-7b-hf",
-+                                                     extras={"mistral": "mistral-community/pixtral-12b"}),  # noqa: E501
-+    "LlavaNextForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-v1.6-mistral-7b-hf"),  # noqa: E501
-+    "LlavaNextVideoForConditionalGeneration": _HfExamplesInfo("llava-hf/LLaVA-NeXT-Video-7B-hf"),  # noqa: E501
-+    "LlavaOnevisionForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-onevision-qwen2-0.5b-ov-hf"),  # noqa: E501
-+    "MantisForConditionalGeneration": _HfExamplesInfo("TIGER-Lab/Mantis-8B-siglip-llama3",  # noqa: E501
-+                                                      hf_overrides={"architectures": ["MantisForConditionalGeneration"]}),  # noqa: E501
-+    "MiniCPMO": _HfExamplesInfo("openbmb/MiniCPM-o-2_6",
-+                                trust_remote_code=True),
-+    "MiniCPMV": _HfExamplesInfo("openbmb/MiniCPM-V-2_6",
-+                                trust_remote_code=True),
-+    "MolmoForCausalLM": _HfExamplesInfo("allenai/Molmo-7B-D-0924",
-+                                        trust_remote_code=True),
-+    "NVLM_D": _HfExamplesInfo("nvidia/NVLM-D-72B",
-+                              trust_remote_code=True),
-+    "PaliGemmaForConditionalGeneration": _HfExamplesInfo("google/paligemma-3b-pt-224"),  # noqa: E501
-+    "Phi3VForCausalLM": _HfExamplesInfo("microsoft/Phi-3-vision-128k-instruct",
-+                                        trust_remote_code=True),
-+    "PixtralForConditionalGeneration": _HfExamplesInfo("mistralai/Pixtral-12B-2409",  # noqa: E501
-+                                                       tokenizer_mode="mistral"),
-+    "QWenLMHeadModel": _HfExamplesInfo("Qwen/Qwen-VL-Chat",
-+                                       extras={"text_only": "Qwen/Qwen-7B-Chat"},  # noqa: E501
-+                                       trust_remote_code=True),
-+    "Qwen2AudioForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-Audio-7B-Instruct"),  # noqa: E501
-+    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-VL-2B-Instruct"),  # noqa: E501
-+    "Qwen2_5_VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2.5-VL-3B-Instruct",  # noqa: E501
-+                                                          min_transformers_version="4.49"),  # noqa: E501
-+    "UltravoxModel": _HfExamplesInfo("fixie-ai/ultravox-v0_3",
-+                                     trust_remote_code=True),
-+    # [Encoder-decoder]
-+    "MllamaForConditionalGeneration": _HfExamplesInfo("meta-llama/Llama-3.2-11B-Vision-Instruct"),  # noqa: E501
-+    "WhisperForConditionalGeneration": _HfExamplesInfo("openai/whisper-large-v3"),  # noqa: E501
-+}
-+
-+_SPECULATIVE_DECODING_EXAMPLE_MODELS = {
-+    "EAGLEModel": _HfExamplesInfo("JackFram/llama-68m",
-+                                  speculative_model="abhigoyal/vllm-eagle-llama-68m-random"),  # noqa: E501
-+    "MedusaModel": _HfExamplesInfo("JackFram/llama-68m",
-+                                   speculative_model="abhigoyal/vllm-medusa-llama-68m-random"),  # noqa: E501
-+    "MLPSpeculatorPreTrainedModel": _HfExamplesInfo("JackFram/llama-160m",
-+                                                    speculative_model="ibm-ai-platform/llama-160m-accelerator"),  # noqa: E501
-+}
-+
-+_FALLBACK_MODEL = {
-+    "TransformersModel": _HfExamplesInfo("ArthurZ/Ilama-3.2-1B", trust_remote_code=True),  # noqa: E501
-+}
-+
-+_EXAMPLE_MODELS = {
-+    **_TEXT_GENERATION_EXAMPLE_MODELS,
-+    **_EMBEDDING_EXAMPLE_MODELS,
-+    **_CROSS_ENCODER_EXAMPLE_MODELS,
-+    **_MULTIMODAL_EXAMPLE_MODELS,
-+    **_SPECULATIVE_DECODING_EXAMPLE_MODELS,
-+    **_FALLBACK_MODEL,
-+}
-+
-+
-+class HfExampleModels:
-+    def __init__(self, hf_models: Mapping[str, _HfExamplesInfo]) -> None:
-+        super().__init__()
-+
-+        self.hf_models = hf_models
-+
-+    def get_supported_archs(self) -> AbstractSet[str]:
-+        return self.hf_models.keys()
-+
-+    def get_hf_info(self, model_arch: str) -> _HfExamplesInfo:
-+        return self.hf_models[model_arch]
-+
-+    def find_hf_info(self, model_id: str) -> _HfExamplesInfo:
-+        for info in self.hf_models.values():
-+            if info.default == model_id:
-+                return info
-+
-+        # Fallback to extras
-+        for info in self.hf_models.values():
-+            if any(extra == model_id for extra in info.extras.values()):
-+                return info
-+
-+        raise ValueError(f"No example model defined for {model_id}")
-+
-+
-+HF_EXAMPLE_MODELS = HfExampleModels(_EXAMPLE_MODELS)
-diff --git a/vllm/__init__.py b/vllm/__init__.py
-index 566c5116d..0bebaad83 100644
---- a/vllm/__init__.py
-+++ b/vllm/__init__.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """vLLM: a high-throughput and memory-efficient inference engine for LLMs"""
- import os
-@@ -32,7 +33,7 @@ os.environ['NCCL_CUMEM_ENABLE'] = '0'
- # see https://github.com/vllm-project/vllm/issues/10480
- os.environ['TORCHINDUCTOR_COMPILE_THREADS'] = '1'
- # see https://github.com/vllm-project/vllm/issues/10619
--torch._inductor.config.compile_threads = 1
-+#torch._inductor.config.compile_threads = 1
- 
- __all__ = [
-     "__version__",
-diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
-index a68235016..b6008ec40 100644
---- a/vllm/_custom_ops.py
-+++ b/vllm/_custom_ops.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import contextlib
-@@ -24,7 +25,7 @@ supports_moe_ops = False
- with contextlib.suppress(ImportError):
-     import vllm._moe_C  # noqa: F401
-     supports_moe_ops = True
--
-+"""
- if TYPE_CHECKING:
- 
-     def register_fake(fn):
-@@ -34,7 +35,7 @@ else:
-         from torch.library import register_fake
-     except ImportError:
-         from torch.library import impl_abstract as register_fake
--
-+"""
- 
- # page attention ops
- def paged_attention_v1(
-@@ -71,6 +72,7 @@ def paged_attention_v2(
-     exp_sum: torch.Tensor,
-     max_logits: torch.Tensor,
-     tmp_out: torch.Tensor,
-+    block_count: torch.Tensor,
-     query: torch.Tensor,
-     key_cache: torch.Tensor,
-     value_cache: torch.Tensor,
-@@ -89,14 +91,14 @@ def paged_attention_v2(
-     blocksparse_vert_stride: int = 0,
-     blocksparse_block_size: int = 64,
-     blocksparse_head_sliding_step: int = 0,
-+    count_init_once: bool = False,
- ) -> None:
-     torch.ops._C.paged_attention_v2(
--        out, exp_sum, max_logits, tmp_out, query, key_cache, value_cache,
-+        out, exp_sum, max_logits, tmp_out, block_count, query, key_cache, value_cache,
-         num_kv_heads, scale, block_tables, seq_lens, block_size, max_seq_len,
-         alibi_slopes, kv_cache_dtype, k_scale, v_scale, tp_rank,
-         blocksparse_local_blocks, blocksparse_vert_stride,
--        blocksparse_block_size, blocksparse_head_sliding_step)
--
-+        blocksparse_block_size, blocksparse_head_sliding_step, count_init_once)
- 
- def paged_attention_rocm(
-     out: torch.Tensor,
-@@ -146,6 +148,20 @@ def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
-                                           cos_sin_cache, is_neox, rot_dim,
-                                           cos_sin_cache_offsets)
- 
-+def page_reshape_kv_cache(
-+    key_cache: torch.Tensor,
-+    value_cache: torch.Tensor,
-+    key_cache_new_layer: torch.Tensor,
-+    value_cache_new_layer: torch.Tensor,
-+    num_seqs: int,
-+    num_heads: int,
-+    head_size: int,
-+    num_kv_heads: int,
-+    block_size: int,
-+    kv_cache_dtype: str,
-+  )->None:
-+    torch.ops._C.page_reshape_kv_cache(key_cache, value_cache, key_cache_new_layer,
-+            value_cache_new_layer, num_seqs, num_heads, head_size, num_kv_heads, block_size, kv_cache_dtype)
- 
- # layer norm ops
- def rms_norm(out: torch.Tensor, input: torch.Tensor, weight: torch.Tensor,
-@@ -224,23 +240,32 @@ def awq_dequantize(qweight: torch.Tensor, scales: torch.Tensor,
+-
+ # layer norm ops
+ def rms_norm(out: torch.Tensor, input: torch.Tensor, weight: torch.Tensor,
+              epsilon: float) -> None:
+@@ -387,21 +386,30 @@ def awq_dequantize(qweight: torch.Tensor, scales: torch.Tensor,
  
  
  def awq_gemm(input: torch.Tensor, qweight: torch.Tensor, qzeros: torch.Tensor,
 -             scales: torch.Tensor, split_k_iters: int) -> torch.Tensor:
-+        scales: torch.Tensor, split_k_iters: int, temp_space: torch.Tensor, 
-+        dtype_bf16: bool) -> torch.Tensor:
++             scales: torch.Tensor, split_k_iters: int, temp_space: torch.Tensor, 
++             dtype_bf16: bool) -> torch.Tensor:
      if envs.VLLM_USE_TRITON_AWQ:
          from vllm.model_executor.layers.quantization.awq_triton import (
              awq_gemm_triton)
@@ -19402,13 +15852,13 @@ index a68235016..b6008ec40 100644
 -    return torch.ops._C.awq_gemm(input, qweight, qzeros, scales, split_k_iters)
 +        return awq_gemm_triton(input, qweight, scales, qzeros, split_k_iters)
 +    return torch.ops._C.awq_gemm(input, qweight, scales, qzeros, split_k_iters,
-+                                 temp_space, dtype_bf16)
++                                temp_space, dtype_bf16)
  
++# awq to gptq 4bit conversion
 +def awq_to_gptq_4bit(qweight: torch.Tensor) -> torch.Tensor:
 +    if envs.VLLM_USE_TRITON_AWQ:
 +        return qweight
 +    return torch.ops._C.awq_to_gptq_4bit(qweight)
-+    
  
  # gptq
  def gptq_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
@@ -19419,73 +15869,137 @@ index a68235016..b6008ec40 100644
 +              temp_space: torch.Tensor, dtype_bf16: bool) -> torch.Tensor:
      return torch.ops._C.gptq_gemm(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
 -                                  b_g_idx, use_exllama, bit)
--
 +                                  b_g_idx, use_exllama, bit, group_size, 
 +                                  perm_space, temp_space, dtype_bf16)
  
-+"""
- if hasattr(torch.ops._C, "gptq_gemm"):
  
-     @register_fake("_C::gptq_gemm")
-@@ -251,7 +276,7 @@ if hasattr(torch.ops._C, "gptq_gemm"):
+ if hasattr(torch.ops._C, "gptq_gemm"):
+@@ -410,7 +418,9 @@ if hasattr(torch.ops._C, "gptq_gemm"):
+     def _gptq_gemm_fake(a: torch.Tensor, b_q_weight: torch.Tensor,
+                         b_gptq_qzeros: torch.Tensor,
+                         b_gptq_scales: torch.Tensor, b_g_idx: torch.Tensor,
+-                        use_exllama: bool, bit: int) -> torch.Tensor:
++                        use_exllama: bool, bit: int, 
++                        group_size: int, perm_space: torch.Tensor,
++                        temp_space: torch.Tensor, dtype_bf16: bool) -> torch.Tensor:
          return torch.empty((a.size(0), b_q_weight.size(1)),
                             dtype=a.dtype,
                             device=a.device)
--
-+"""
- 
- def gptq_shuffle(q_weight: torch.Tensor, q_perm: torch.Tensor,
-                  bit: int) -> None:
-@@ -275,7 +300,7 @@ def gptq_marlin_24_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
-                                             workspace, b_q_type.id, size_m,
-                                             size_n, size_k)
+@@ -503,7 +513,8 @@ if hasattr(torch.ops._C, "gptq_marlin_24_gemm"):
+     @register_fake("_C::awq_gemm")
+     def _awq_gemm_fake(input: torch.Tensor, qweight: torch.Tensor,
+                        qzeros: torch.Tensor, scales: torch.Tensor,
+-                       split_k_iters: torch.SymInt) -> torch.Tensor:
++                       split_k_iters: torch.SymInt, temp_space: torch.Tensor, 
++                       dtype_bf16: bool) -> torch.Tensor:
+         num_in_feats = input.size(0)
+         return torch.empty((split_k_iters, num_in_feats, qweight.size(1) * 8),
+                            dtype=input.dtype,
+@@ -645,8 +656,8 @@ if hasattr(torch.ops._C, "ggml_moe_a8_vec"):
  
+ # cutlass
+ def cutlass_scaled_mm_supports_fp4(cuda_device_capability: int) -> bool:
+-    return torch.ops._C.cutlass_scaled_mm_supports_fp4(cuda_device_capability)
 -
-+"""
- if hasattr(torch.ops._C, "gptq_marlin_24_gemm"):
++    # return torch.ops._C.cutlass_scaled_mm_supports_fp4(cuda_device_capability)
++    return False
  
-     @register_fake("_C::gptq_marlin_24_gemm")
-@@ -430,16 +455,18 @@ if hasattr(torch.ops._C, "ggml_dequantize"):
-     ) -> torch.Tensor:
-         batch = X.size(0)
-         return torch.empty((batch, row), dtype=torch.float16, device=W.device)
--
-+"""
+ def cutlass_scaled_fp4_mm(a: torch.Tensor, b: torch.Tensor,
+                           block_scale_a: torch.Tensor,
+@@ -659,15 +670,24 @@ def cutlass_scaled_fp4_mm(a: torch.Tensor, b: torch.Tensor,
+                                        alpha)
+     return out
  
- # cutlass
+-
  def cutlass_scaled_mm_supports_fp8(cuda_device_capability: int) -> bool:
 -    return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
-+    #return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
-+    return 
++    # return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
++    return True
  
  
  def cutlass_scaled_mm_supports_block_fp8(cuda_device_capability: int) -> bool:
 -    return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
 -        cuda_device_capability)
-+    #return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
-+    #    cuda_device_capability)
-+    return 
- 
- 
- def cutlass_scaled_mm(a: torch.Tensor,
-@@ -752,14 +779,14 @@ def machete_prepack_B(
-     return torch.ops._C.machete_prepack_B(b_q_weight, a_type, b_type.id,
-                                           group_scales_type)
- 
 -
-+"""
- if hasattr(torch.ops._C, "permute_cols"):
- 
-     @register_fake("_C::permute_cols")
-     def _permute_cols_fake(a: torch.Tensor,
-                            perm: torch.Tensor) -> torch.Tensor:
-         return torch.empty_like(a)
--
-+"""
++    # return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
++    #     cuda_device_capability)
++    return True
++
++# Batch gemm in vllm, support w8a8 int8 quantization
++def cutlass_scaled_batch_mm(a: torch.Tensor, b: torch.Tensor,
++                            scale_a: torch.Tensor, scale_b: torch.Tensor,
++                            out_dtype: torch.dtype, bias: Optional[torch.Tensor] = None) -> torch.Tensor:
++    assert (a.shape[0] == b.shape[0] and a.shape[2] == b.shape[1])
++    out = torch.empty((a.shape[0], a.shape[1], b.shape[2]), device = a.device, dtype = out_dtype)
++    torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
++    return out
  
- def permute_cols(a: torch.Tensor, perm: torch.Tensor) -> torch.Tensor:
-     return torch.ops._C.permute_cols(a, perm)
-@@ -967,7 +994,16 @@ def topk_softmax(topk_weights: torch.Tensor, topk_ids: torch.Tensor,
+ def cutlass_scaled_mm(a: torch.Tensor,
+                       b: torch.Tensor,
+@@ -1380,6 +1400,62 @@ def scaled_int8_quant(
+                                            input_azp)
+     return output, input_scales, input_azp
+ 
++def scaled_int8_quant_mask(
++    input: torch.Tensor,
++    mask:  torch.Tensor,
++    scale: Optional[torch.Tensor] = None,
++    azp: Optional[torch.Tensor] = None,
++    symmetric: bool = True
++) -> tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
++    """
++    Quantize the input tensor to int8 and return the quantized tensor and scale, and maybe azp.
++
++    Args:
++        input: The input tensor to be quantized to int8.
++        scale: Optional scaling factor for the int8 quantization.
++            When not provided, we invoke dynamic-per-token quantization.
++        azp: Optional zero-point for the int8 quantization.
++            Must be provided for asymmetric quantization if `scale` is provided.
++        mask: mask
++        symmetric: Whether to use symmetric quantization (scale only, azp ignored).
++
++    Returns:
++      Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]] : Output int8 tensor, scales, and optionally azp.
++    """
++    output = torch.empty_like(input, dtype=torch.int8)
++    if scale is not None:
++        # static-per-tensor quantization.
++        assert symmetric == (
++            azp
++            is None), "azp must only be provided for asymmetric quantization."
++        torch.ops._C.static_scaled_int8_quant(output, input, scale, azp)
++        return output, scale, azp
++
++    # dynamic-per-token quantization.
++    input_scales = torch.empty((input.numel() // input.shape[-1], 1),
++                               device=input.device,
++                               dtype=torch.float32)
++    input_azp = None if symmetric else torch.empty_like(input_scales,
++                                                        dtype=torch.int32)
++    torch.ops._C.dynamic_scaled_int8_mask_quant(output, input, mask, input_scales,
++                                           input_azp)
++    return output, input_scales, input_azp
++
++def fused_silu_mul_dq_mask_quant(
++    input: torch.Tensor,
++    mask:  torch.Tensor
++) -> torch.Tensor:
++    """
++    input shape [expert_num, token_num_padded, hidden_dim]
++    output shape [expert_num, token_num_padded, hidden_dim // 2], dtype bf16
++    masked_m shape [expert_num], indicates valid tokens per expert
++
++    implement silu_and_mul + quant + package
++    """
++    out_stride = (input.shape[-1] // 4 + 257) // 256 * 256
++    output = torch.empty((input.shape[0], input.shape[1], out_stride), device=input.device, dtype=input.dtype)
++    torch.ops._C.fused_silu_mul_dq_mask_quant_pack(output, input, mask)
++    return output
+ 
+ # qqq ops
+ def marlin_qqq_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
+@@ -1555,6 +1631,14 @@ def topk_softmax(topk_weights: torch.Tensor, topk_ids: torch.Tensor,
      torch.ops._moe_C.topk_softmax(topk_weights, topk_ids,
                                    token_expert_indicies, gating_output)
  
@@ -19498,47 +16012,8 @@ index a68235016..b6008ec40 100644
 +                    sorted_token_ids, expert_ids,
 +                    num_tokens_post_padded, mul_routed_weight, top_k, tileConfig)
  
-+"""
- if supports_moe_ops and hasattr(torch.ops._moe_C, "marlin_gemm_moe"):
- 
-     @register_fake("_moe_C::marlin_gemm_moe")
-@@ -985,7 +1021,7 @@ if supports_moe_ops and hasattr(torch.ops._moe_C, "marlin_gemm_moe"):
-         return torch.empty((size_m, topk, size_n),
-                            dtype=a.dtype,
-                            device=a.device)
--
-+"""
- 
- def reshape_and_cache(
-     key: torch.Tensor,
-@@ -1001,6 +1037,19 @@ def reshape_and_cache(
-                                              value_cache, slot_mapping,
-                                              kv_cache_dtype, k_scale, v_scale)
- 
-+def reshape_and_cache_new(
-+    key: torch.Tensor,
-+    value: torch.Tensor,
-+    key_cache: torch.Tensor,
-+    value_cache: torch.Tensor,
-+    slot_mapping: torch.Tensor,
-+    kv_cache_dtype: str,
-+    kv_scale: float,
-+    v_scale: float,
-+) -> None:
-+    torch.ops._C_cache_ops.reshape_and_cache_new(key, value, key_cache,
-+                                             value_cache, slot_mapping,
-+                                             kv_cache_dtype, kv_scale, v_scale)
- 
- def reshape_and_cache_flash(
-     key: torch.Tensor,
-diff --git a/vllm/_release_info.txt b/vllm/_release_info.txt
-new file mode 100644
-index 000000000..32a80d8ac
---- /dev/null
-+++ b/vllm/_release_info.txt
-@@ -0,0 +1 @@
-+git error
-\ No newline at end of file
+ def moe_wna16_marlin_gemm(input: torch.Tensor, output: Optional[torch.Tensor],
+                           b_qweight: torch.Tensor, b_scales: torch.Tensor,
 diff --git a/vllm/attention/backends/configs/tp8_merge.json b/vllm/attention/backends/configs/tp8_merge.json
 new file mode 100644
 index 000000000..773051b85
@@ -20533,3557 +17008,1016 @@ index 000000000..773051b85
 +]
 \ No newline at end of file
 diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py
-old mode 100755
-new mode 100644
-index 6a82127ac..2aac995e1
+index 73e377268..a46c0a8f4 100755
 --- a/vllm/attention/backends/flash_attn.py
 +++ b/vllm/attention/backends/flash_attn.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Attention layer with FlashAttention."""
- from collections import defaultdict
-@@ -23,10 +24,12 @@ from vllm.logger import init_logger
+@@ -23,13 +23,15 @@ from vllm.attention.backends.utils import (
+     compute_slot_mapping_start_idx, get_num_prefill_decode_query_kv_tokens,
+     get_seq_len_block_table_args, is_all_cross_attn_metadata_set,
+     is_all_encoder_attn_metadata_set, is_block_tables_empty)
+-from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
+-                                           get_flash_attn_version)
++# from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
++#                                            get_flash_attn_version)
+ from vllm.logger import init_logger
  from vllm.multimodal import MultiModalPlaceholderMap
- from vllm.platforms import current_platform
  from vllm.utils import async_tensor_h2d, make_tensor_with_pad
--from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
--                                  flash_attn_varlen_func,
--                                  flash_attn_with_kvcache,
--                                  is_fa_version_supported)
-+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
-+#                                  flash_attn_varlen_func,
-+#                                  flash_attn_with_kvcache,
-+#                                  is_fa_version_supported)
+-from vllm.vllm_flash_attn import (flash_attn_varlen_func,
+-                                  flash_attn_with_kvcache)
 +from flash_attn import (flash_attn_varlen_func,
 +                        flash_attn_with_kvcache)
++def flash_attn_supports_fp8() -> bool:
++    return False
  
  if TYPE_CHECKING:
      from vllm.worker.model_runner import (ModelInputForGPUBuilder,
-@@ -41,7 +44,7 @@ class FlashAttentionBackend(AttentionBackend):
- 
-     @staticmethod
-     def get_supported_head_sizes() -> List[int]:
--        return [32, 64, 96, 128, 160, 192, 224, 256]
-+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
- 
-     @staticmethod
-     def get_name() -> str:
-@@ -647,21 +650,21 @@ class FlashAttentionImpl(AttentionImpl):
-         # if hopper default to FA3, otherwise stick to FA2 for now
-         # TODO(lucas): profile FA3 on ampere to see if it makes sense to
-         #  use FA3 as default for both
--        if current_platform.get_device_capability()[0] >= 9:
--            self.fa_version = 3 if is_fa_version_supported(3) else 2
--        else:
--            self.fa_version = 2
-+        #if current_platform.get_device_capability()[0] >= 9:
-+        #    self.fa_version = 3 if is_fa_version_supported(3) else 2
-+        #else:
-+        #    self.fa_version = 2
- 
--        if VLLM_FLASH_ATTN_VERSION is not None:
--            assert VLLM_FLASH_ATTN_VERSION in [2, 3]
--            self.fa_version = VLLM_FLASH_ATTN_VERSION
-+        #if VLLM_FLASH_ATTN_VERSION is not None:
-+        #    assert VLLM_FLASH_ATTN_VERSION in [2, 3]
-+        #    self.fa_version = VLLM_FLASH_ATTN_VERSION
- 
--        if not is_fa_version_supported(self.fa_version):
--            logger.error("Cannot use FA version %d is not supported due to %s",
--                         self.fa_version,
--                         fa_version_unsupported_reason(self.fa_version))
-+        #if not is_fa_version_supported(self.fa_version):
-+        #    logger.error("Cannot use FA version %d is not supported due to %s",
-+        #                 self.fa_version,
-+        #                 fa_version_unsupported_reason(self.fa_version))
- 
--        assert is_fa_version_supported(self.fa_version)
-+        #assert is_fa_version_supported(self.fa_version)
- 
-     def forward(
-         self,
-@@ -767,7 +770,8 @@ class FlashAttentionImpl(AttentionImpl):
-                 key = key[:num_prefill_kv_tokens]
-                 value = value[:num_prefill_kv_tokens]
- 
+@@ -640,8 +642,8 @@ class FlashAttentionImpl(AttentionImpl):
+         self.sliding_window = ((sliding_window - 1,
+                                 0) if sliding_window is not None else (-1, -1))
+         self.kv_cache_dtype = kv_cache_dtype
+-        self.vllm_flash_attn_version = get_flash_attn_version(
+-            requires_alibi=self.alibi_slopes is not None)
++        # self.vllm_flash_attn_version = get_flash_attn_version(
++        #     requires_alibi=self.alibi_slopes is not None)
+         if is_quantized_kv_cache(self.kv_cache_dtype) and (
+                 not self.kv_cache_dtype.startswith("fp8")
+                 or not flash_attn_supports_fp8()):
+@@ -809,7 +811,7 @@ class FlashAttentionImpl(AttentionImpl):
+                         (num_kv_tokens, num_kv_heads, head_size))
+ 
+                 descale_shape = (q_seq_start_loc.shape[0] - 1, key.shape[1])
 -                flash_attn_varlen_func(
-+                #flash_attn_varlen_func(
 +                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
                      q=query,
                      k=key,
                      v=value,
-@@ -780,8 +784,8 @@ class FlashAttentionImpl(AttentionImpl):
+@@ -822,11 +824,11 @@ class FlashAttentionImpl(AttentionImpl):
                      window_size=window_size,
                      alibi_slopes=alibi_slopes,
                      softcap=logits_soft_cap,
 -                    out=prefill_output,
--                    fa_version=self.fa_version,
-+                    #out=prefill_output,
-+                    #fa_version=self.fa_version,
+-                    fa_version=self.vllm_flash_attn_version,
+-                    q_descale=layer._q_scale.expand(descale_shape),
+-                    k_descale=layer._k_scale.expand(descale_shape),
+-                    v_descale=layer._v_scale.expand(descale_shape),
++                    # out=prefill_output,
++                    # fa_version=self.vllm_flash_attn_version,
++                    # q_descale=layer._q_scale.expand(descale_shape),
++                    # k_descale=layer._k_scale.expand(descale_shape),
++                    # v_descale=layer._v_scale.expand(descale_shape),
                  )
              else:
                  # prefix-enabled attention
-@@ -789,13 +793,15 @@ class FlashAttentionImpl(AttentionImpl):
-                     "Only decoder-only models support prefix caching")
-                 assert prefill_meta.seq_lens is not None
+@@ -837,13 +839,13 @@ class FlashAttentionImpl(AttentionImpl):
                  max_seq_len = max(prefill_meta.seq_lens)
+                 descale_shape = (prefill_meta.query_start_loc.shape[0] - 1,
+                                  key.shape[1])
 -                flash_attn_varlen_func(  # noqa
-+                #flash_attn_varlen_func(  # noqa
-+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
++                output[:num_prefill_query_tokens] = flash_attn_varlen_func(  # noqa
                      q=query,
                      k=key_cache,
                      v=value_cache,
                      cu_seqlens_q=prefill_meta.query_start_loc,
                      max_seqlen_q=prefill_meta.max_query_len,
 -                    seqused_k=prefill_meta.seq_lens_tensor,
-+                    #seqused_k=prefill_meta.seq_lens_tensor,
 +                    cu_seqlens_k=prefill_meta.seq_start_loc,
                      max_seqlen_k=max_seq_len,
                      softmax_scale=softmax_scale,
                      causal=True,
-@@ -803,8 +809,8 @@ class FlashAttentionImpl(AttentionImpl):
+@@ -851,11 +853,11 @@ class FlashAttentionImpl(AttentionImpl):
                      alibi_slopes=alibi_slopes,
                      block_table=prefill_meta.block_tables,
                      softcap=logits_soft_cap,
--                    out=prefill_output,
--                    fa_version=self.fa_version,
-+                    #out=prefill_output,
-+                    #fa_version=self.fa_version,
-                 )
- 
-         if decode_meta := attn_metadata.decode_metadata:
-@@ -818,13 +824,14 @@ class FlashAttentionImpl(AttentionImpl):
-                 assert attn_type == AttentionType.DECODER, (
-                     "Only decoder-only models support max_decode_query_len > 1"
-                 )
--                flash_attn_varlen_func(
-+                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
-                     q=decode_query,
-                     k=key_cache,
-                     v=value_cache,
-                     cu_seqlens_q=decode_meta.query_start_loc,
-                     max_seqlen_q=decode_meta.max_decode_query_len,
--                    seqused_k=decode_meta.seq_lens_tensor,
-+                    # seqused_k=decode_meta.seq_lens_tensor,
-+                    cu_seqlens_k=decode_meta.seq_start_loc,
-                     max_seqlen_k=decode_meta.max_decode_seq_len,
-                     softmax_scale=softmax_scale,
-                     causal=True,
-@@ -832,8 +839,8 @@ class FlashAttentionImpl(AttentionImpl):
-                     alibi_slopes=alibi_slopes,
-                     softcap=logits_soft_cap,
-                     block_table=decode_meta.block_tables,
--                    out=decode_output,
--                    fa_version=self.fa_version,
-+                    #out=decode_output,
-+                    #fa_version=self.fa_version,
-                 )
-             else:
-                 # Use flash_attn_with_kvcache for normal decoding.
-@@ -842,7 +849,7 @@ class FlashAttentionImpl(AttentionImpl):
-                     _,
-                     block_tables_arg,
-                 ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
--                flash_attn_with_kvcache(
-+                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
-                     q=decode_query.unsqueeze(1),
-                     k_cache=key_cache,
-                     v_cache=value_cache,
-@@ -853,9 +860,9 @@ class FlashAttentionImpl(AttentionImpl):
-                     window_size=window_size,
-                     alibi_slopes=alibi_slopes,
-                     softcap=logits_soft_cap,
--                    out=decode_output.unsqueeze(1),
--                    fa_version=self.fa_version,
--                )
-+                    #out=decode_output.unsqueeze(1),
-+                    #fa_version=self.fa_version,
-+                ).squeeze(1)
-         return output
- 
- 
-diff --git a/vllm/attention/backends/flash_attn_pg.py b/vllm/attention/backends/flash_attn_pg.py
-new file mode 100755
-index 000000000..8435f9baa
---- /dev/null
-+++ b/vllm/attention/backends/flash_attn_pg.py
-@@ -0,0 +1,1028 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# SPDX-License-Identifier: Apache-2.0
-+"""Attention layer with FlashAttention."""
-+from collections import defaultdict
-+from dataclasses import dataclass
-+from itertools import accumulate
-+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type
-+
-+import torch
-+
-+from vllm import _custom_ops as ops
-+from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
-+                                              AttentionLayer,
-+                                              AttentionMetadata,
-+                                              AttentionMetadataBuilder,
-+                                              AttentionType)
-+from vllm.attention.backends.utils import (
-+    PAD_SLOT_ID, CommonAttentionState, compute_slot_mapping,
-+    compute_slot_mapping_start_idx, get_num_prefill_decode_query_kv_tokens,
-+    get_seq_len_block_table_args, is_all_cross_attn_metadata_set,
-+    is_all_encoder_attn_metadata_set, is_block_tables_empty)
-+from vllm.envs import VLLM_FLASH_ATTN_VERSION
-+from vllm.logger import init_logger
-+from vllm.multimodal import MultiModalPlaceholderMap
-+from vllm.platforms import current_platform
-+from vllm.utils import async_tensor_h2d, make_tensor_with_pad
-+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
-+#                                  flash_attn_varlen_func,
-+#                                  flash_attn_with_kvcache,
-+#                                  is_fa_version_supported)
-+from vllm.attention.ops.paged_attn import (PagedAttention,
-+                                           PagedAttentionMetadata)
-+
-+from flash_attn import (flash_attn_varlen_func,
-+                        flash_attn_with_kvcache)
-+
-+if TYPE_CHECKING:
-+    from vllm.worker.model_runner import (ModelInputForGPUBuilder,
-+                                          ModelInputForGPUWithSamplingMetadata)
-+
-+logger = init_logger(__name__)
-+
-+
-+class FlashAttentionBackend(AttentionBackend):
-+
-+    accept_output_buffer: bool = True
-+
-+    @staticmethod
-+    def get_supported_head_sizes() -> List[int]:
-+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
-+
-+    @staticmethod
-+    def get_name() -> str:
-+        return "FLASH_ATTN"
-+
-+    @staticmethod
-+    def get_impl_cls() -> Type["FlashAttentionImpl"]:
-+        return FlashAttentionImpl
-+
-+    @staticmethod
-+    def get_metadata_cls() -> Type["AttentionMetadata"]:
-+        return FlashAttentionMetadata
-+
-+    @staticmethod
-+    def get_builder_cls() -> Type["FlashAttentionMetadataBuilder"]:
-+        return FlashAttentionMetadataBuilder
-+
-+    @staticmethod
-+    def get_state_cls() -> Type["CommonAttentionState"]:
-+        return CommonAttentionState
-+
-+    @staticmethod
-+    def get_kv_cache_shape(
-+        num_blocks: int,
-+        block_size: int,
-+        num_kv_heads: int,
-+        head_size: int,
-+    ) -> Tuple[int, ...]:
-+        """
-+        if block_size % 16 != 0:
-+            raise ValueError("Block size must be a multiple of 16.")
-+        return (2, num_blocks, block_size, num_kv_heads, head_size)
-+        """
-+        return PagedAttention.get_kv_cache_shape(num_blocks, block_size,
-+                                                 num_kv_heads, head_size)
-+
-+    @staticmethod
-+    def swap_blocks(
-+        src_kv_cache: torch.Tensor,
-+        dst_kv_cache: torch.Tensor,
-+        src_to_dst: torch.Tensor,
-+    ) -> None:
-+        """
-+        src_key_cache = src_kv_cache[0]
-+        dst_key_cache = dst_kv_cache[0]
-+        ops.swap_blocks(src_key_cache, dst_key_cache, src_to_dst)
-+        src_value_cache = src_kv_cache[1]
-+        dst_value_cache = dst_kv_cache[1]
-+        ops.swap_blocks(src_value_cache, dst_value_cache, src_to_dst)
-+        """
-+        PagedAttention.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)
-+
-+    @staticmethod
-+    def copy_blocks(
-+        kv_caches: List[torch.Tensor],
-+        src_to_dists: torch.Tensor,
-+    ) -> None:
-+        """
-+        key_caches = [kv_cache[0] for kv_cache in kv_caches]
-+        value_caches = [kv_cache[1] for kv_cache in kv_caches]
-+
-+        ops.copy_blocks(key_caches, value_caches, src_to_dists)
-+        """
-+        PagedAttention.copy_blocks(kv_caches, src_to_dists)
-+
-+
-+@dataclass
-+#class FlashAttentionMetadata(AttentionMetadata):
-+class FlashAttentionMetadata(AttentionMetadata, PagedAttentionMetadata):
-+    """Metadata for FlashAttentionBackend.
-+
-+    NOTE: Any python object stored here is not updated when it is
-+    cuda-graph replayed. If you have values that need to be changed
-+    dynamically, it should be stored in tensor. The tensor has to be
-+    updated from `CUDAGraphRunner.forward` API.
-+    """
-+    # (batch_size,). The sequence length per sequence. Sequence length means
-+    # the computed tokens + new tokens None if it is a decoding.
-+    seq_lens: Optional[List[int]]
-+    # seq_lens stored as a tensor.
-+    seq_lens_tensor: Optional[torch.Tensor]
-+
-+    # NOTE(sang): Definition of context_len, query_len, and seq_len.
-+    # |---------- N-1 iteration --------|
-+    # |---------------- N iteration ---------------------|
-+    # |- tokenA -|......................|-- newTokens ---|
-+    # |---------- context_len ----------|
-+    # |-------------------- seq_len ---------------------|
-+    #                                   |-- query_len ---|
-+
-+    # Maximum sequence length among prefill batch. 0 if there are decoding
-+    # requests only.
-+    max_prefill_seq_len: int
-+    # Maximum sequence length among decode batch. 0 if there are prefill
-+    # requests only.
-+    max_decode_seq_len: int
-+    # (batch_size,) A tensor of context lengths (tokens that are computed
-+    # so far).
-+    context_lens_tensor: Optional[torch.Tensor]
-+
-+    # (batch_size, max_blocks_per_seq).
-+    # Block addresses per sequence. (Seq id -> list of physical block)
-+    # E.g., [0, 1, 2] means tokens are stored in 0th, 1st, and 2nd blocks
-+    # in the kv cache. Each block can contain up to block_size tokens.
-+    # 2nd dimensions are padded up to max_blocks_per_seq if it is cuda-graph
-+    # captured.
-+    block_tables: Optional[torch.Tensor]
-+
-+    # Whether or not if cuda graph is enabled.
-+    # Cuda-graph is currently enabled for decoding only.
-+    # TODO(woosuk): Move `use_cuda_graph` out since it's unrelated to attention.
-+
-+    use_cuda_graph: bool
-+
-+    # Maximum query length in the batch.
-+    max_query_len: Optional[int] = None
-+
-+    # Max number of query tokens among request in the batch.
-+    max_decode_query_len: Optional[int] = None
-+
-+    # (batch_size + 1,). The cumulative subquery lengths of the sequences in
-+    # the batch, used to index into subquery. E.g., if the subquery length
-+    # is [4, 6], it is [0, 4, 10].
-+    query_start_loc: Optional[torch.Tensor] = None
-+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
-+    # the batch, used to index into sequence. E.g., if the sequence length is
-+    # [4, 6], it is [0, 4, 10].
-+    seq_start_loc: Optional[torch.Tensor] = None
-+
-+    _cached_prefill_metadata: Optional["FlashAttentionMetadata"] = None
-+    _cached_decode_metadata: Optional["FlashAttentionMetadata"] = None
-+
-+    # Begin encoder attn & enc/dec cross-attn fields...
-+
-+    # Encoder sequence lengths representation
-+    encoder_seq_lens: Optional[List[int]] = None
-+    encoder_seq_lens_tensor: Optional[torch.Tensor] = None
-+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
-+    # the batch, used to index into sequence. E.g., if the sequence length is
-+    # [4, 6], it is [0, 4, 10].
-+    encoder_seq_start_loc: Optional[torch.Tensor] = None
-+    # Maximum sequence length among encoder sequences
-+    max_encoder_seq_len: Optional[int] = None
-+    # Number of tokens input to encoder
-+    num_encoder_tokens: Optional[int] = None
-+
-+    # Cross-attention memory-mapping data structures: slot mapping
-+    # and block tables
-+    cross_slot_mapping: Optional[torch.Tensor] = None
-+    cross_block_tables: Optional[torch.Tensor] = None
-+
-+    @property
-+    def is_all_encoder_attn_metadata_set(self):
-+        '''
-+        All attention metadata required for encoder attention is set.
-+        '''
-+        return is_all_encoder_attn_metadata_set(self)
-+
-+    @property
-+    def is_all_cross_attn_metadata_set(self):
-+        '''
-+        All attention metadata required for enc/dec cross-attention is set.
-+
-+        Superset of encoder attention required metadata.
-+        '''
-+        return is_all_cross_attn_metadata_set(self)
-+
-+    @property
-+    def prefill_metadata(self) -> Optional["FlashAttentionMetadata"]:
-+        if self.num_prefills == 0:
-+            return None
-+
-+        if self._cached_prefill_metadata is not None:
-+            return self._cached_prefill_metadata
-+
-+        assert ((self.seq_lens is not None)
-+                or (self.encoder_seq_lens is not None))
-+        assert ((self.seq_lens_tensor is not None)
-+                or (self.encoder_seq_lens_tensor is not None))
-+
-+        # Compute some attn_metadata fields which default to None
-+        query_start_loc = (None if self.query_start_loc is None else
-+                           self.query_start_loc[:self.num_prefills + 1])
-+        slot_mapping = (None if self.slot_mapping is None else
-+                        self.slot_mapping[:self.num_prefill_tokens])
-+        seq_lens = (None if self.seq_lens is None else
-+                    self.seq_lens[:self.num_prefills])
-+        seq_lens_tensor = (None if self.seq_lens_tensor is None else
-+                           self.seq_lens_tensor[:self.num_prefills])
-+        seq_start_loc = (None if self.seq_start_loc is None else
-+                         self.seq_start_loc[:self.num_prefills + 1])
-+        context_lens_tensor = (None if self.context_lens_tensor is None else
-+                               self.context_lens_tensor[:self.num_prefills])
-+        block_tables = (None if self.block_tables is None else
-+                        self.block_tables[:self.num_prefills])
-+
-+        self._cached_prefill_metadata = FlashAttentionMetadata(
-+            num_prefills=self.num_prefills,
-+            num_prefill_tokens=self.num_prefill_tokens,
-+            num_decode_tokens=0,
-+            slot_mapping=slot_mapping,
-+            multi_modal_placeholder_index_maps=self.
-+            multi_modal_placeholder_index_maps,
-+            enable_kv_scales_calculation=self.enable_kv_scales_calculation,
-+            seq_lens=seq_lens,
-+            seq_lens_tensor=seq_lens_tensor,
-+            max_query_len=self.max_query_len,
-+            max_prefill_seq_len=self.max_prefill_seq_len,
-+            max_decode_query_len=0,
-+            max_decode_seq_len=0,
-+            query_start_loc=query_start_loc,
-+            seq_start_loc=seq_start_loc,
-+            context_lens_tensor=context_lens_tensor,
-+            block_tables=block_tables,
-+            use_cuda_graph=False,
-+            # Begin encoder & cross attn fields below...
-+            encoder_seq_lens=self.encoder_seq_lens,
-+            encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
-+            encoder_seq_start_loc=self.encoder_seq_start_loc,
-+            max_encoder_seq_len=self.max_encoder_seq_len,
-+            cross_slot_mapping=self.cross_slot_mapping,
-+            cross_block_tables=self.cross_block_tables)
-+        return self._cached_prefill_metadata
-+
-+    @property
-+    def decode_metadata(self) -> Optional["FlashAttentionMetadata"]:
-+        if self.num_decode_tokens == 0:
-+            return None
-+
-+        if self._cached_decode_metadata is not None:
-+            return self._cached_decode_metadata
-+        assert ((self.seq_lens_tensor is not None)
-+                or (self.encoder_seq_lens_tensor is not None))
-+
-+        # Compute some attn_metadata fields which default to None
-+        slot_mapping = (None if self.slot_mapping is None else
-+                        self.slot_mapping[self.num_prefill_tokens:])
-+        seq_lens_tensor = (None if self.seq_lens_tensor is None else
-+                           self.seq_lens_tensor[self.num_prefills:])
-+        block_tables = (None if self.block_tables is None else
-+                        self.block_tables[self.num_prefills:])
-+
-+        self._cached_decode_metadata = FlashAttentionMetadata(
-+            num_prefills=0,
-+            num_prefill_tokens=0,
-+            num_decode_tokens=self.num_decode_tokens,
-+            slot_mapping=slot_mapping,
-+            multi_modal_placeholder_index_maps=None,
-+            enable_kv_scales_calculation=True,
-+            seq_lens=None,
-+            seq_lens_tensor=seq_lens_tensor,
-+            max_decode_query_len=self.max_decode_query_len,
-+            max_query_len=self.max_query_len,
-+            max_prefill_seq_len=0,
-+            max_decode_seq_len=self.max_decode_seq_len,
-+            # Batch may be composed of prefill|decodes, adjust query start
-+            # indices to refer to the start of decodes. E.g.
-+            # in tokens:[3 prefills|6 decodes], query_start_loc=[3,9] => [0,6].
-+            query_start_loc=(self.query_start_loc[self.num_prefills:] -
-+                             self.query_start_loc[self.num_prefills])
-+            if self.query_start_loc is not None else None,
-+            seq_start_loc=self.seq_start_loc[self.num_prefills:]
-+            if self.seq_start_loc is not None else None,
-+            context_lens_tensor=None,
-+            block_tables=block_tables,
-+            use_cuda_graph=self.use_cuda_graph,
-+            # Begin encoder & cross attn fields below...
-+            encoder_seq_lens=self.encoder_seq_lens,
-+            encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
-+            encoder_seq_start_loc=self.encoder_seq_start_loc,
-+            max_encoder_seq_len=self.max_encoder_seq_len,
-+            cross_slot_mapping=self.cross_slot_mapping,
-+            cross_block_tables=self.cross_block_tables)
-+        return self._cached_decode_metadata
-+
-+    def advance_step(self,
-+                     model_input: "ModelInputForGPUWithSamplingMetadata",
-+                     sampled_token_ids: Optional[torch.Tensor],
-+                     block_size: int,
-+                     num_seqs: int,
-+                     num_queries: int,
-+                     turn_prefills_into_decodes: bool = False):
-+        """
-+        Update metadata in-place to advance one decode step.
-+        """
-+        # When using cudagraph, the num_seqs is padded to the next captured
-+        # batch sized, but num_queries tracks the actual number of requests in
-+        # the batch. For --enforce-eager mode, num_seqs == num_queries
-+        if num_seqs != num_queries:
-+            assert num_seqs > num_queries
-+            assert self.use_cuda_graph
-+
-+        if turn_prefills_into_decodes:
-+            # When Mutli-Step is enabled with Chunked-Prefill, prefills and
-+            # decodes are scheduled together. In the first step, all the
-+            # prefills turn into decodes. This update reflects that
-+            # conversion.
-+            assert self.num_decode_tokens + self.num_prefills == num_seqs
-+            self.num_decode_tokens += self.num_prefills
-+            self.num_prefills = 0
-+            self.num_prefill_tokens = 0
-+            self.max_prefill_seq_len = 0
-+            self.max_query_len = 1
-+
-+            self.slot_mapping = self.slot_mapping[:num_seqs]
-+        else:
-+            assert self.seq_lens is not None
-+            assert self.max_decode_seq_len == max(self.seq_lens)
-+
-+        assert self.num_prefills == 0
-+        assert self.num_prefill_tokens == 0
-+        assert self.num_decode_tokens == num_seqs
-+        assert self.slot_mapping.shape == (num_seqs, )
-+
-+        assert self.seq_lens is not None
-+        assert len(self.seq_lens) == num_seqs
-+        assert self.seq_lens_tensor is not None
-+        assert self.seq_lens_tensor.shape == (num_seqs, )
-+        assert self.max_query_len == 1
-+        assert self.max_prefill_seq_len == 0
-+
-+        assert self.query_start_loc is not None
-+        assert self.query_start_loc.shape == (num_queries + 1, )
-+        assert self.seq_start_loc is not None
-+        assert self.seq_start_loc.shape == (num_seqs + 1, )
-+
-+        assert self.context_lens_tensor is not None
-+        assert self.context_lens_tensor.shape == (num_queries, )
-+
-+        assert self.block_tables is not None
-+        assert self.block_tables.shape[0] == num_seqs
-+
-+        # Update query lengths. Note that we update only queries and not seqs,
-+        # since tensors may be padded due to captured cuda graph batch size
-+        for i in range(num_queries):
-+            self.seq_lens[i] += 1
-+        self.max_decode_seq_len = max(self.seq_lens)
-+
-+        ops.advance_step_flashattn(num_seqs=num_seqs,
-+                                   num_queries=num_queries,
-+                                   block_size=block_size,
-+                                   input_tokens=model_input.input_tokens,
-+                                   sampled_token_ids=sampled_token_ids,
-+                                   input_positions=model_input.input_positions,
-+                                   seq_lens=self.seq_lens_tensor,
-+                                   slot_mapping=self.slot_mapping,
-+                                   block_tables=self.block_tables)
-+
-+
-+class FlashAttentionMetadataBuilder(
-+        AttentionMetadataBuilder[FlashAttentionMetadata]):
-+
-+    def __init__(self, input_builder: "ModelInputForGPUBuilder"):
-+        self.input_builder = input_builder
-+        self.runner = input_builder.runner
-+        self.sliding_window = input_builder.sliding_window
-+        self.block_size = input_builder.block_size
-+
-+    def prepare(self):
-+        self.slot_mapping: List[int] = []
-+        self.prefill_seq_lens: List[int] = []
-+        self.context_lens: List[int] = []
-+        self.block_tables: List[List[int]] = []
-+        self.curr_seq_lens: List[int] = []
-+        self.multimodal_placeholder_maps: Dict[
-+            str,
-+            MultiModalPlaceholderMap] = defaultdict(MultiModalPlaceholderMap)
-+        self.num_prefills = 0
-+        self.num_prefill_tokens = 0
-+        self.num_decode_tokens = 0
-+        self.has_prefix_cache_hit = False
-+
-+    def _add_seq_group(
-+            self, inter_data: "ModelInputForGPUBuilder.InterDataForSeqGroup",
-+            chunked_prefill_enabled: bool, prefix_cache_hit: bool):
-+        """Add a sequence group to the metadata. Specifically update/append
-+        1. context length.
-+        2. block table.
-+        3. slot mapping.
-+        """
-+        is_prompt = inter_data.is_prompt
-+        block_tables = inter_data.block_tables
-+
-+        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,
-+             curr_sliding_window_block) in zip(
-+                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],
-+                 inter_data.orig_seq_lens, inter_data.seq_lens,
-+                 inter_data.query_lens, inter_data.context_lens,
-+                 inter_data.curr_sliding_window_blocks):
-+            self.context_lens.append(context_len)
-+
-+            if is_prompt:
-+                mm_maps = inter_data.multi_modal_placeholder_maps
-+                if mm_maps:
-+                    for modality, placeholders in mm_maps.items():
-+                        self.multimodal_placeholder_maps[modality].extend(
-+                            placeholders)
-+
-+                self.num_prefills += 1
-+                self.num_prefill_tokens += token_len
-+                self.prefill_seq_lens.append(seq_len)
-+            else:
-+                self.num_decode_tokens += query_len
-+                self.curr_seq_lens.append(curr_seq_len)
-+
-+            # Compute block table.
-+            # TODO(sang): Combine chunked prefill and prefix caching by
-+            # only allowing multiple of block_size chunk size.
-+            # NOTE: This only works for oooooooxxx style attention.
-+            block_table = []
-+            if prefix_cache_hit:
-+                # NOTE(woosuk): For flash-attn, the block table should
-+                # include the entries for the incoming prefill tokens.
-+                block_table = block_tables[seq_id]
-+            elif ((chunked_prefill_enabled or not is_prompt)
-+                  and block_tables is not None):
-+                if curr_sliding_window_block == 0:
-+                    block_table = block_tables[seq_id]
-+                else:
-+                    block_table = block_tables[seq_id][
-+                        -curr_sliding_window_block:]
-+            self.block_tables.append(block_table)
-+
-+            # Compute slot mapping.
-+            is_profile_run = is_block_tables_empty(block_tables)
-+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,
-+                                                       context_len,
-+                                                       self.sliding_window)
-+            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,
-+                                 seq_len, context_len, start_idx,
-+                                 self.block_size, inter_data.block_tables)
-+
-+    def _get_graph_runner_block_tables(
-+            self, num_seqs: int,
-+            block_tables: List[List[int]]) -> torch.Tensor:
-+        # The shape of graph_block_tables is
-+        # [max batch size, max context len // block size].
-+        max_batch_size, max_blocks = self.runner.graph_block_tables.shape
-+        assert max_batch_size >= num_seqs
-+
-+        graph_block_tables = self.runner.graph_block_tables[:num_seqs]
-+        for i, block_table in enumerate(block_tables):
-+            if block_table:
-+                num_blocks = len(block_table)
-+                if num_blocks <= max_blocks:
-+                    graph_block_tables[i, :num_blocks] = block_table
-+                else:
-+                    # It may be possible to have more blocks allocated due
-+                    # to lookahead slots of multi-step, however, they are
-+                    # not used anyway, so can be safely ignored.
-+                    graph_block_tables[
-+                        i, :max_blocks] = block_table[:max_blocks]
-+
-+        return torch.from_numpy(graph_block_tables).to(
-+            device=self.runner.device, non_blocking=True)
-+
-+    def build(self, seq_lens: List[int], query_lens: List[int],
-+              cuda_graph_pad_size: int, batch_size: int):
-+        """Build attention metadata with on-device tensors.
-+
-+        Args:
-+            seq_lens: The maybe padded sequence lengths of the input sequences.
-+            query_lens: The query lengths of the input sequences.
-+            cuda_graph_pad_size: The padding size for cuda graph.
-+                                 -1 if cuda graph is not used.
-+            batch_size: The maybe padded batch size.
-+        """
-+        prefix_cache_hit = any([
-+            inter_data.prefix_cache_hit
-+            for inter_data in self.input_builder.inter_data_list
-+        ])
-+        for inter_data in self.input_builder.inter_data_list:
-+            self._add_seq_group(inter_data,
-+                                self.input_builder.chunked_prefill_enabled,
-+                                prefix_cache_hit)
-+
-+        device = self.runner.device
-+        use_captured_graph = cuda_graph_pad_size != -1
-+
-+        max_query_len = max(query_lens)
-+        decode_query_lens = query_lens[self.num_prefills:]
-+        if len(decode_query_lens) > 0:
-+            max_decode_query_len = max(decode_query_lens)
-+        else:
-+            max_decode_query_len = 1
-+        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)
-+        max_decode_seq_len = max(self.curr_seq_lens, default=0)
-+        num_decode_tokens = self.num_decode_tokens
-+        query_start_loc = list(accumulate(query_lens, initial=0))
-+        seq_start_loc = list(accumulate(seq_lens, initial=0))
-+
-+        num_seqs = len(seq_lens)
-+        if use_captured_graph:
-+            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)
-+            self.block_tables.extend([] * cuda_graph_pad_size)
-+            num_decode_tokens = batch_size - self.num_prefill_tokens
-+            block_tables = self._get_graph_runner_block_tables(
-+                num_seqs, self.block_tables)
-+        else:
-+            block_tables = make_tensor_with_pad(
-+                self.block_tables,
-+                pad=0,
-+                dtype=torch.int,
-+                device=device,
-+            )
-+        assert max_query_len > 0, ("query_lens: {}".format(query_lens))
-+
-+        assert device is not None
-+        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,
-+                                               device, self.runner.pin_memory)
-+        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,
-+                                           self.runner.pin_memory)
-+        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
-+                                               device, self.runner.pin_memory)
-+        query_start_loc_tensor = async_tensor_h2d(query_start_loc, torch.int32,
-+                                                  device,
-+                                                  self.runner.pin_memory)
-+        seq_start_loc_tensor = async_tensor_h2d(seq_start_loc, torch.int32,
-+                                                device, self.runner.pin_memory)
-+        placeholder_index_maps = {
-+            modality: placeholder_map.index_map()
-+            for modality, placeholder_map in
-+            self.multimodal_placeholder_maps.items()
-+        }
-+
-+        return FlashAttentionMetadata(
-+            num_prefills=self.num_prefills,
-+            slot_mapping=slot_mapping_tensor,
-+            num_prefill_tokens=self.num_prefill_tokens,
-+            num_decode_tokens=num_decode_tokens,
-+            seq_lens=seq_lens,
-+            multi_modal_placeholder_index_maps=placeholder_index_maps,
-+            enable_kv_scales_calculation=True,
-+            seq_lens_tensor=seq_lens_tensor,
-+            max_query_len=max_query_len,
-+            max_decode_query_len=max_decode_query_len,
-+            max_prefill_seq_len=max_prefill_seq_len,
-+            max_decode_seq_len=max_decode_seq_len,
-+            query_start_loc=query_start_loc_tensor,
-+            seq_start_loc=seq_start_loc_tensor,
-+            context_lens_tensor=context_lens_tensor,
-+            block_tables=block_tables,
-+            use_cuda_graph=use_captured_graph,
-+        )
-+
-+
-+class FlashAttentionImpl(AttentionImpl):
-+    """
-+    If the input tensors contain prompt tokens, the layout is as follows:
-+    |<--------------- num_prefill_tokens ----------------->|	
-+    |<--prefill_0-->|<--prefill_1-->|...|<--prefill_N-1--->|
-+
-+    Otherwise, the layout is as follows:	
-+    |<----------------- num_decode_tokens ------------------>|	
-+    |<--decode_0-->|..........|<--decode_M-1-->|<--padding-->|
-+
-+    Generation tokens can contain padding when cuda-graph is used.
-+    Currently, prompt tokens don't contain any padding.
-+
-+    The prompts might have different lengths, while the generation tokens
-+    always have length 1.
-+
-+    If chunked prefill is enabled, prefill tokens and decode tokens can be
-+    batched together in a flattened 1D query.
-+
-+    |<----- num_prefill_tokens ---->|<------- num_decode_tokens --------->|
-+    |<-prefill_0->|...|<-prefill_N-1->|<--decode_0-->|...|<--decode_M-1-->|
-+
-+    Currently, cuda graph is disabled for chunked prefill, meaning there's no
-+    padding between prefill and decode tokens.
-+    """
-+
-+    def __init__(
-+        self,
-+        num_heads: int,
-+        head_size: int,
-+        scale: float,
-+        num_kv_heads: int,
-+        alibi_slopes: Optional[List[float]],
-+        sliding_window: Optional[int],
-+        kv_cache_dtype: str,
-+        blocksparse_params: Optional[Dict[str, Any]] = None,
-+        logits_soft_cap: Optional[float] = None,
-+        attn_type: str = AttentionType.DECODER,
-+    ) -> None:
-+        if blocksparse_params is not None:
-+            raise ValueError(
-+                "FlashAttention does not support block-sparse attention.")
-+        self.num_heads = num_heads
-+        self.head_size = head_size
-+        self.scale = float(scale)
-+        self.num_kv_heads = num_kv_heads
-+        if alibi_slopes is not None:
-+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)
-+        self.alibi_slopes = alibi_slopes
-+        self.sliding_window = ((sliding_window - 1,
-+                                0) if sliding_window is not None else (-1, -1))
-+        self.kv_cache_dtype = kv_cache_dtype
-+        if logits_soft_cap is None:
-+            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.
-+            logits_soft_cap = 0
-+        self.logits_soft_cap = logits_soft_cap
-+
-+        assert self.num_heads % self.num_kv_heads == 0
-+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
-+
-+        support_head_sizes = FlashAttentionBackend.get_supported_head_sizes()
-+        if head_size not in support_head_sizes:
-+            raise ValueError(
-+                f"Head size {head_size} is not supported by FlashAttention. "
-+                f"Supported head sizes are: {support_head_sizes}.")
-+        self.attn_type = attn_type
-+
-+        # if hopper default to FA3, otherwise stick to FA2 for now
-+        # TODO(lucas): profile FA3 on ampere to see if it makes sense to
-+        #  use FA3 as default for both
-+        #if current_platform.get_device_capability()[0] >= 9:
-+        #    self.fa_version = 3 if is_fa_version_supported(3) else 2
-+        #else:
-+        #    self.fa_version = 2
-+
-+        #if VLLM_FLASH_ATTN_VERSION is not None:
-+        #    assert VLLM_FLASH_ATTN_VERSION in [2, 3]
-+        #    self.fa_version = VLLM_FLASH_ATTN_VERSION
-+
-+        #if not is_fa_version_supported(self.fa_version):
-+        #    logger.error("Cannot use FA version %d is not supported due to %s",
-+        #                 self.fa_version,
-+        #                 fa_version_unsupported_reason(self.fa_version))
-+
-+        #assert is_fa_version_supported(self.fa_version)
-+
-+    def forward(
-+        self,
-+        layer: AttentionLayer,
-+        query: torch.Tensor,
-+        key: torch.Tensor,
-+        value: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: FlashAttentionMetadata,
-+        output: Optional[torch.Tensor] = None,
-+    ) -> torch.Tensor:
-+        """Forward pass with FlashAttention.
-+
-+        Args:
-+            query: shape = [num_tokens, num_heads, head_size]
-+            key: shape = [num_tokens, num_kv_heads, head_size]
-+            value: shape = [num_tokens, num_kv_heads, head_size]
-+            output: shape = [num_tokens, num_heads, head_size]
-+            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
-+                NOTE: kv_cache will be an empty tensor with shape [0]
-+                for profiling run.
-+            attn_metadata: Metadata for attention.
-+        NOTE: It in-place updates the output tensor.
-+        """
-+        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.
-+        assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0, (
-+            "key/v_scale is not supported in FlashAttention.")
-+
-+        assert output is not None, "Output tensor must be provided."
-+        k_scale = layer._k_scale
-+        v_scale = layer._v_scale
-+
-+        attn_type = self.attn_type
-+        if (attn_type == AttentionType.ENCODER
-+                and (not attn_metadata.is_all_encoder_attn_metadata_set)):
-+            raise AttributeError("Encoder attention requires setting "
-+                                 "encoder metadata attributes.")
-+        elif (attn_type == AttentionType.ENCODER_DECODER
-+              and (not attn_metadata.is_all_cross_attn_metadata_set)):
-+            raise AttributeError("Encoder/decoder cross-attention "
-+                                 "requires setting cross-attention "
-+                                 "metadata attributes.")
-+
-+        kv_cache_dtype: str = self.kv_cache_dtype
-+        softmax_scale: float = self.scale
-+        window_size = self.sliding_window
-+        alibi_slopes: Optional[torch.Tensor] = self.alibi_slopes
-+        logits_soft_cap: Optional[float] = self.logits_soft_cap
-+
-+        if kv_cache.numel() > 0:
-+            #key_cache = kv_cache[0]
-+            #value_cache = kv_cache[1]
-+            key_cache, value_cache = PagedAttention.split_kv_cache(
-+                kv_cache, self.num_kv_heads, self.head_size)
-+            # We skip updating the KV cache under two conditions:
-+            #  a. When the Attention Type is ENCODER. In this phase, we compute
-+            #     only the encoder attention without updating the cache.
-+            #  b. When both Key and Value are None. This occurs during
-+            #     cross-attention computation in the decoding phase, where the
-+            #     KV cache is already populated with the cross-attention
-+            #     tensor. Thus, we skip cache updates during this time.
-+            if (attn_type != AttentionType.ENCODER) and (key is not None) and (
-+                    value is not None):
-+                if attn_type == AttentionType.ENCODER_DECODER:
-+                    # Update cross-attention KV cache (prefill-only)
-+                    updated_slot_mapping = attn_metadata.cross_slot_mapping
-+                else:
-+                    # Update self-attention KV cache (prefill/decode)
-+                    updated_slot_mapping = attn_metadata.slot_mapping
-+
-+                # Reshape the input keys and values and store them in the cache.
-+                # If kv_cache is not provided, the new key and value tensors are
-+                # not cached. This happens during the initial memory
-+                # profiling run.
-+                #torch.ops._C_cache_ops.reshape_and_cache_flash(
-+                #    key,
-+                #    value,
-+                #    kv_cache[0],
-+                #    kv_cache[1],
-+                #    updated_slot_mapping.flatten(),  # type: ignore[union-attr]
-+                #    kv_cache_dtype,
-+                #    layer._k_scale,
-+                #    layer._v_scale,
-+                #)
-+                PagedAttention.write_to_paged_cache(key, value, key_cache,
-+                                                value_cache,
-+                                                attn_metadata.slot_mapping,
-+                                                self.kv_cache_dtype, k_scale, v_scale)
-+
-+        (num_prefill_query_tokens, num_prefill_kv_tokens,
-+        num_decode_query_tokens) = \
-+            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)
-+        decode_query = query[num_prefill_query_tokens:]
-+        decode_output = output[num_prefill_query_tokens:]
-+        # QKV for prefill.
-+        query = query[:num_prefill_query_tokens]
-+        prefill_output = output[:num_prefill_query_tokens]
-+        assert query.shape[0] == num_prefill_query_tokens
-+        assert decode_query.shape[0] == num_decode_query_tokens
-+
-+        if prefill_meta := attn_metadata.prefill_metadata:
-+            # Prompt run.
-+            if (kv_cache.numel() == 0 or prefill_meta.block_tables is None
-+                    or prefill_meta.block_tables.numel() == 0):
-+                # normal attention
-+                # When block_tables are not filled, it means q and k are the
-+                # prompt, and they have the same length.
-+                q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len = \
-+                    _get_query_key_seq_metadata(prefill_meta, True, attn_type)
-+
-+                key = key[:num_prefill_kv_tokens]
-+                value = value[:num_prefill_kv_tokens]
-+
-+                #flash_attn_varlen_func(
-+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
-+                    q=query,
-+                    k=key,
-+                    v=value,
-+                    cu_seqlens_q=q_seq_start_loc,
-+                    cu_seqlens_k=k_seq_start_loc,
-+                    max_seqlen_q=q_seq_len,
-+                    max_seqlen_k=k_seq_len,
-+                    softmax_scale=softmax_scale,
-+                    causal=_get_causal_option(attn_type),
-+                    window_size=window_size,
-+                    alibi_slopes=alibi_slopes,
-+                    softcap=logits_soft_cap,
-+                    #out=prefill_output,
-+                    #fa_version=self.fa_version,
-+                )
-+            else:
-+                # prefix-enabled attention
-+                assert attn_type == AttentionType.DECODER, (
-+                    "Only decoder-only models support prefix caching")
-+                assert prefill_meta.seq_lens is not None
-+                max_seq_len = max(prefill_meta.seq_lens)
-+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads, self.head_size//(32 // kv_cache.element_size()), -1, 2, 16 // kv_cache.element_size())
-+                key_cache = key_cache.permute(0,1,2,4,3,5)
-+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads,self.head_size//(16 // kv_cache.element_size()),-1,16 // kv_cache.element_size())
-+                value_cache =  value_cache.permute(0,1,3,2)
-+                #flash_attn_varlen_func(  # noqa
-+                """
-+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
-+                    q=query,
-+                    k=key_cache,
-+                    v=value_cache,
-+                    cu_seqlens_q=prefill_meta.query_start_loc,
-+                    max_seqlen_q=prefill_meta.max_query_len,
-+                    seqused_k=prefill_meta.seq_lens_tensor,
-+                    max_seqlen_k=max_seq_len,
-+                    softmax_scale=softmax_scale,
-+                    causal=True,
-+                    window_size=window_size,
-+                    alibi_slopes=alibi_slopes,
-+                    block_table=prefill_meta.block_tables,
-+                    softcap=logits_soft_cap,
-+                    #out=prefill_output,
-+                    #fa_version=self.fa_version,
-+                )
-+                """
-+                output[:num_prefill_query_tokens] = PagedAttention.forward_prefix(
-+                    query,
-+                    key,
-+                    value,
-+                    self.kv_cache_dtype,
-+                    key_cache,
-+                    value_cache,
-+                    attn_metadata.block_tables,
-+                    attn_metadata.query_start_loc,
-+                    attn_metadata.seq_lens_tensor,
-+                    attn_metadata.context_lens_tensor,
-+                    attn_metadata.max_query_len,
-+                    self.alibi_slopes,
-+                    self.logits_soft_cap,
-+                    k_scale,
-+                    v_scale,
-+                )
-+                value_cache =  value_cache.permute(0,1,3,2)
-+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads,self.head_size//(32 // kv_cache.element_size()),2,-1,16 // kv_cache.element_size())
-+                key_cache = key_cache.permute(0,1,2,4,3,5)
-+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads, self.head_size//(32 // kv_cache.element_size()), -1, 32 // kv_cache.element_size())
-+
-+        if decode_meta := attn_metadata.decode_metadata:
-+            # Decoding run.
-+            # Use flash_attn_varlen_func kernel for speculative decoding
-+            # because different queries might have different lengths.
-+
-+            assert decode_meta.max_decode_query_len is not None
-+            # use only for actual varlen decoding
-+            if decode_meta.max_decode_query_len > 1:
-+                assert attn_type == AttentionType.DECODER, (
-+                    "Only decoder-only models support max_decode_query_len > 1"
-+                )
-+                """
-+                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
-+                    q=decode_query,
-+                    k=key_cache,
-+                    v=value_cache,
-+                    cu_seqlens_q=decode_meta.query_start_loc,
-+                    max_seqlen_q=decode_meta.max_decode_query_len,
-+                    seqused_k=decode_meta.seq_lens_tensor,
-+                    max_seqlen_k=decode_meta.max_decode_seq_len,
-+                    softmax_scale=softmax_scale,
-+                    causal=True,
-+                    window_size=window_size,
-+                    alibi_slopes=alibi_slopes,
-+                    softcap=logits_soft_cap,
-+                    block_table=decode_meta.block_tables,
-+                    #out=decode_output,
-+                    #fa_version=self.fa_version,
-+                )
-+                """
-+                output[num_prefill_query_tokens:] = PagedAttention.forward_decode(
-+                    decode_query,
-+                    key_cache,
-+                    value_cache,
-+                    block_tables=attn_metadata.block_tables,
-+                    seq_lens=attn_metadata.seq_lens_tensor,    #attn_metadata.context_lens
-+                    max_seq_len=attn_metadata.max_decode_seq_len,  #attn_metadata.max_context_len
-+                    kv_cache_dtype='auto',
-+                    num_kv_heads=self.num_kv_heads,
-+                    scale=self.scale,
-+                    alibi_slopes=self.alibi_slopes,
-+                    k_scale=k_scale,
-+                    v_scale=v_scale,
-+                )
-+            else:
-+                # Use flash_attn_with_kvcache for normal decoding.
-+                (
-+                    seq_lens_arg,
-+                    _,
-+                    block_tables_arg,
-+                ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
-+                """
-+                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
-+                    q=decode_query.unsqueeze(1),
-+                    k_cache=key_cache,
-+                    v_cache=value_cache,
-+                    block_table=block_tables_arg,
-+                    cache_seqlens=seq_lens_arg,
-+                    softmax_scale=softmax_scale,
-+                    causal=True,
-+                    window_size=window_size,
-+                    alibi_slopes=alibi_slopes,
-+                    softcap=logits_soft_cap,
-+                    #out=decode_output.unsqueeze(1),
-+                    #fa_version=self.fa_version,
-+                ).squeeze(1)
-+                """
-+                output[num_prefill_query_tokens:] = PagedAttention.forward_decode(
-+                    decode_query,
-+                    key_cache,
-+                    value_cache,
-+                    block_tables=attn_metadata.block_tables,
-+                    seq_lens=attn_metadata.seq_lens_tensor,    #attn_metadata.context_lens
-+                    max_seq_len=attn_metadata.max_decode_seq_len,  #attn_metadata.max_context_len
-+                    kv_cache_dtype='auto',
-+                    num_kv_heads=self.num_kv_heads,
-+                    scale=self.scale,
-+                    alibi_slopes=self.alibi_slopes,
-+                    k_scale=k_scale,
-+                    v_scale=v_scale,
-+                )
-+        return output
-+
-+
-+def _get_query_key_seq_metadata(
-+    attn_metadata,
-+    is_prompt: bool,
-+    attn_type: str,
-+) -> tuple:
-+    """
-+    Returns sequence metadata for key and query based on the specified 
-+    attention type and whether input is a prompt.
-+
-+    This function computes the starting locations and maximum sequence lengths 
-+    for key and query sequences for different attention types.
-+
-+    Args:
-+        attn_metadata: The attention metadata object
-+        is_prompt (bool): A flag indicating if the input is a prompt
-+        attn_type (AttentionType): The type of attention being used.
-+
-+    Returns:
-+        tuple: A tuple containing four integers:
-+            - Starting location for the query sequence.
-+            - Maximum sequence length for the query sequence.
-+            - Starting location for the key sequence.
-+            - Maximum sequence length for the key sequence.
-+
-+    Raises:
-+        AttributeError: If an invalid attention type is provided.
-+    """
-+    if attn_type == AttentionType.DECODER:
-+        # Decoder self-attention
-+        # Choose max_seq_len based on whether we are in prompt_run
-+        if is_prompt:
-+            max_seq_len = attn_metadata.max_prefill_seq_len
-+        else:
-+            max_seq_len = attn_metadata.max_decode_seq_len
-+        return (attn_metadata.seq_start_loc, max_seq_len,
-+                attn_metadata.seq_start_loc, max_seq_len)
-+
-+    elif attn_type == AttentionType.ENCODER_DECODER:
-+        # This is cross attention between the where the key
-+        # is the precomputed encoder attention and query
-+        # is the input sequence.
-+        # Choose query max length based on whether it is prompt
-+        # or not.
-+        if is_prompt:
-+            max_seq_len = attn_metadata.max_prefill_seq_len
-+        else:
-+            max_seq_len = attn_metadata.max_decode_seq_len
-+        return (attn_metadata.seq_start_loc, max_seq_len,
-+                attn_metadata.encoder_seq_start_loc,
-+                attn_metadata.max_encoder_seq_len)
-+    elif attn_type == AttentionType.ENCODER:
-+        # For encoder attention both the query and the key are same i.e the
-+        # encoder sequence.
-+        return (attn_metadata.encoder_seq_start_loc,
-+                attn_metadata.max_encoder_seq_len,
-+                attn_metadata.encoder_seq_start_loc,
-+                attn_metadata.max_encoder_seq_len)
-+    elif attn_type == AttentionType.ENCODER_ONLY:
-+        assert is_prompt, "Should not have decode for encoder only model."
-+        return (attn_metadata.seq_start_loc, attn_metadata.max_prefill_seq_len,
-+                attn_metadata.seq_start_loc, attn_metadata.max_prefill_seq_len)
-+    else:
-+        raise AttributeError(f"Invalid attention type {str(attn_type)}")
-+
-+
-+def _get_causal_option(attn_type: str) -> bool:
-+    """
-+    Determine whether the given attention type is suitable for causal 
-+    attention mechanisms.
-+
-+    Args:
-+        attn_type (AttentionType): The type of attention being evaluated
-+
-+    Returns:
-+        bool: Returns `True` if the attention type is suitable for causal 
-+        attention (i.e., not encoder, encoder-only, or encoder-decoder), 
-+        otherwise returns `False`.
-+    """
-+    return not (attn_type == AttentionType.ENCODER
-+                or attn_type == AttentionType.ENCODER_ONLY
-+                or attn_type == AttentionType.ENCODER_DECODER)
-diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py
-index cd8c08e5a..5d0fe19d3 100644
---- a/vllm/attention/backends/mla/utils.py
-+++ b/vllm/attention/backends/mla/utils.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from abc import abstractmethod
-@@ -218,16 +219,6 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
-                 .view(-1, self.num_heads, self.kv_lora_rank)
- 
-     def process_weights_after_loading(self, act_dtype: torch.dtype):
--
--        def is_layer_fp8(layer: LinearBase) -> bool:
--            return isinstance(layer.quant_method, Fp8LinearMethod) or\
--                (isinstance(layer.quant_method, CompressedTensorsLinearMethod)\
--                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8))
--
--        def quantization_scheme_supported(layer: LinearBase) -> bool:
--            return isinstance(layer.quant_method, UnquantizedLinearMethod) or \
--                is_layer_fp8(layer)
--
-         # TODO(lucas) This is very gross, we need a more wide scale refactor of
-         # all the FP8 code with a more standard way of
-         # defining schemes/group-shapes, we should also potentially force
-@@ -237,7 +228,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
-         def get_scale_group_shapes_for_fp8(layer: LinearBase) -> \
-             Tuple[Tuple[int, int], Tuple[int, int]]:
-             if isinstance(layer.quant_method, Fp8LinearMethod):
--                if layer.quant_method.block_quant is not None:
-+                if layer.quant_method.block_quant:
-                     weight_block_size = \
-                         layer.quant_method.quant_config.weight_block_size
-                     # per-token-group (1, X), block-quantized (X, Y)
-@@ -265,41 +256,34 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
-                     f"{layer.quant_method}, please run with VLLM_MLA_DISABLE=1"
-                 )
- 
--        def get_scales(layer: LinearBase) -> torch.Tensor:
--            if hasattr(layer, "weight_scale_inv"):
--                return layer.weight_scale_inv
--            return layer.weight_scale
-+        def get_layer_weight(layer):
-+            WEIGHT_NAMES = ("weight", "qweight", "weight_packed")
-+            for attr in WEIGHT_NAMES:
-+                if hasattr(layer, attr):
-+                    return getattr(layer, attr)
-+            raise AttributeError(
-+                f"Layer '{layer}' has no recognized weight attribute:"
-+                f" {WEIGHT_NAMES}.")
- 
-         def get_and_maybe_dequant_weights(layer: LinearBase):
--            if is_layer_fp8(layer):
--                if isinstance(layer.quant_method, \
--                    CompressedTensorsLinearMethod) and \
--                    isinstance(layer.scheme, CompressedTensorsW8A8Fp8):
--                    # NOTE(lucas): note sure why but `CompressedTensorsW8A8Fp8`
--                    # seems to store weights as (input, output) instead of
--                    # (output, input) so we need to transpose
--                    weight = layer.weight.T  # standardize to (output, input)
--                else:
--                    weight = layer.weight
--                _, weight_scale_group_shape = \
--                    get_scale_group_shapes_for_fp8(layer)
--                scales = get_scales(layer)
--
--                return scaled_dequantize(weight, scales,
--                                         weight_scale_group_shape)
--            else:
--                return layer.weight
--
--        if not (quantization_scheme_supported(self.kv_b_proj) and\
--            quantization_scheme_supported(self.q_proj) and\
--                quantization_scheme_supported(self.o_proj)):
--            raise NotImplementedError(
--                "Only FP8 and UnquantizedLinearMethod are supported for MLA"
--                ", please run with VLLM_MLA_DISABLE=1")
--
--        weight_dtype = self.kv_b_proj.weight.dtype
--        assert self.o_proj.weight.dtype == weight_dtype
--        assert self.q_proj.weight.dtype == weight_dtype
-+            if not isinstance(layer.quant_method, UnquantizedLinearMethod):
-+                # NOTE: This should only be used offline, since it's O(N^3)
-+                eye = torch.eye(layer.input_size_per_partition,
-+                                dtype=act_dtype,
-+                                device=get_layer_weight(layer).device)
-+                dequant_weights = layer.quant_method.apply(layer,
-+                                                           eye,
-+                                                           bias=None)
-+                del eye
-+                # standardize to (output, input)
-+                return dequant_weights.T
-+
-+            return layer.weight.T
-+            #return layer.weight
-+
-+        weight_dtype = get_layer_weight(self.kv_b_proj).dtype
-+        assert get_layer_weight(self.o_proj).dtype == weight_dtype
-+        assert get_layer_weight(self.q_proj).dtype == weight_dtype
- 
-         kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T
-         assert kv_b_proj_weight.shape == (
-@@ -419,28 +403,11 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
-         q_nope: torch.Tensor,
-         q_pe: torch.Tensor,
-         kv_cache: torch.Tensor,
-+        _k_scale_float: torch.Tensor,
-         attn_metadata: T,
-     ) -> torch.Tensor:
-         raise NotImplementedError
- 
--    def apply_pure_rope(
--        self,
--        input_positions: torch.Tensor,
--        q_pe: torch.Tensor,
--        k_pe: torch.Tensor,
--    ) -> tuple[torch.Tensor, torch.Tensor]:
--        seq_len = input_positions.size(0)
--        ori_q_pe_shape, ori_k_pe_shape = q_pe.shape, k_pe.shape
--
--        q_pe, k_pe = self.rotary_emb(
--            input_positions,
--            q_pe.reshape(seq_len, -1),
--            k_pe.reshape(seq_len, -1),
--        )
--        q_pe, k_pe = q_pe.view(ori_q_pe_shape), k_pe.view(ori_k_pe_shape)
--
--        return q_pe, k_pe
--
-     def forward(
-         self,
-         layer: AttentionLayer,
-@@ -465,14 +432,13 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
-         # Restore head dim (for rotary embedding)
-         k_pe = k_pe.unsqueeze(1)
-         assert hasattr(attn_metadata, "input_positions")
--        rope_fn = (self.rotary_emb
--                   if self.use_yarn_rope else self.apply_pure_rope)
- 
-         if is_decode:
-             q_nope = self._q_proj_and_k_up_proj(hidden_states_or_q_c)
-             q_pe = torch.matmul(hidden_states_or_q_c, self.W_QR)\
-                 .view(-1, self.num_heads, self.qk_rope_head_dim)
--            q_pe, k_pe = rope_fn(attn_metadata.input_positions, q_pe, k_pe)
-+            q_pe, k_pe = self.rotary_emb(attn_metadata.input_positions, q_pe,
-+                                         k_pe)
-         else:
-             assert is_prefill
-             q = self.q_proj(hidden_states_or_q_c)[0]\
-@@ -480,7 +446,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
- 
-             # TODO(lucas): there must be a nicer way to write this line
-             q[..., self.qk_nope_head_dim:], k_pe = \
--                rope_fn(
-+                self.rotary_emb(
-                     attn_metadata.input_positions,
-                     q[..., self.qk_nope_head_dim:], k_pe)
- 
-@@ -499,7 +465,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
-             return self._forward_prefill(q, k_c_normed, k_pe, attn_metadata)
- 
-         if attn_metadata.decode_metadata is not None:
--            return self._forward_decode(q_nope, q_pe, kv_cache, attn_metadata)
-+            return self._forward_decode(q_nope, q_pe, kv_cache, layer._k_scale, attn_metadata)
- 
-     # Optional common flash-attn based prefill
-     def _forward_prefill_flash(
-diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
-index 9a1984a93..1ade587f5 100644
---- a/vllm/attention/backends/triton_mla.py
-+++ b/vllm/attention/backends/triton_mla.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from collections import defaultdict
-@@ -33,6 +34,42 @@ if TYPE_CHECKING:
-     from vllm.worker.model_runner import (ModelInputForGPUBuilder,
-                                           ModelInputForGPUWithSamplingMetadata)
- 
-+import json
-+import os
-+
-+# TODO: Configure environment variables temporarily. New versions do not need to be configured
-+os.environ['TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP'] = '1'
-+os.environ['TRITON_ENABLE_MACA_CHAIN_DOT_OPT'] = '1'
-+
-+def load_config():
-+    # Load JSON data from the file
-+    json_path = config_file_path = os.path.join(
-+        os.path.dirname(os.path.realpath(__file__)), "configs", "tp8_merge.json")
-+    with open(json_path, 'r') as file:
-+        data = json.load(file)
-+    return data
-+
-+JSON_DATA = load_config()
-+
-+def find_best_mla_para(json_data, batch_size, input_len, tp_size):
-+    best_match = None
-+    best_batch_size_diff = float('inf')
-+    best_input_len_diff = float('inf')
-+    
-+    for entry in json_data:
-+        if entry["BS"] == batch_size and entry["L"] == input_len:
-+            return entry["num_kv_splits"], entry['num_stages']
-+        batch_size_diff = abs(entry["BS"] - batch_size)
-+        input_len_diff = abs(entry["L"] - input_len)
-+        
-+        # Check if this is a better match than the current best match
-+        if batch_size_diff < best_batch_size_diff or (batch_size_diff == best_batch_size_diff and input_len_diff < best_input_len_diff):
-+            best_match = entry
-+            best_batch_size_diff = batch_size_diff
-+            best_input_len_diff = input_len_diff
-+    
-+    # If a match was found, return the best_kv_splits, otherwise return None
-+    return best_match["num_kv_splits"],best_match["num_stages"]
- 
- class TritonMLABackend(AttentionBackend):
- 
-@@ -256,15 +293,17 @@ class TritonMLAMetadata(MLACommonMetadata):
- 
-     num_prefill_tokens: int
- 
--    num_kv_splits: int = 4  # TODO(lucas) add heuristic
-+    num_kv_splits: int = 16  # TODO(lucas) add heuristic
-     attn_logits: Optional[torch.Tensor] = None
-     req_idx: Optional[torch.Tensor] = None
- 
-     # The dimension of the attention heads
-     head_dim: Optional[int] = None
-+    num_stages: int = 1
- 
-     def __post_init__(self):
-         supported_head_sizes = TritonMLABackend.get_supported_head_sizes()
-+
-         if self.head_dim is not None and self.head_dim \
-                 not in supported_head_sizes:
-             raise ValueError(
-@@ -342,6 +381,14 @@ class TritonMLAMetadata(MLACommonMetadata):
-         input_positions = (None if self.input_positions is None else
-                            self.input_positions[self.num_prefill_tokens:])
- 
-+        if (seq_lens_tensor is not None):
-+            batch = seq_lens_tensor.shape[0]
-+            max_seq_len = int(seq_lens_tensor.max())
-+            num_kv_splits, num_stages = find_best_mla_para(JSON_DATA, batch, max_seq_len, 8)
-+        else:
-+            num_kv_splits = 4
-+            num_stages = 1
-+
-         self._cached_decode_metadata = TritonMLAMetadata(
-             num_prefills=0,
-             num_prefill_tokens=0,
-@@ -367,7 +414,9 @@ class TritonMLAMetadata(MLACommonMetadata):
-             block_tables=block_tables,
-             use_cuda_graph=self.use_cuda_graph,
-             input_positions=input_positions,
--            head_dim=self.head_dim)
-+            head_dim=self.head_dim,
-+            num_kv_splits=num_kv_splits,
-+            num_stages=num_stages)
-         return self._cached_decode_metadata
- 
-     def advance_step(self,
-@@ -438,7 +487,7 @@ class TritonMLAMetadata(MLACommonMetadata):
-                                    block_size=block_size,
-                                    input_tokens=model_input.input_tokens,
-                                    sampled_token_ids=sampled_token_ids,
--                                   input_positions=model_input.input_positions,
-+                                   input_positions=self.input_positions,
-                                    seq_lens=self.seq_lens_tensor,
-                                    slot_mapping=self.slot_mapping,
-                                    block_tables=self.block_tables)
-@@ -608,7 +657,13 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):
-                                                device, self.runner.pin_memory)
-         seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,
-                                            self.runner.pin_memory)
--        input_positions = async_tensor_h2d(self.input_positions, torch.long,
-+
-+        # aligned according to batch_size for advance_step_flashattn used
-+        input_positions_list = self.input_positions
-+        for _ in range(len(self.input_positions), batch_size):
-+            input_positions_list.append(0)
-+        
-+        input_positions = async_tensor_h2d(input_positions_list, torch.long,
-                                            device, self.runner.pin_memory)
-         slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
-                                                device, self.runner.pin_memory)
-@@ -623,6 +678,9 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):
-             self.multimodal_placeholder_maps.items()
-         }
- 
-+        # TODO(m01124) simple set by jira:C500-30310, need tune
-+        num_kv_splits = 16 if max(seq_lens) > 256 else 2
-+        
-         return TritonMLAMetadata(
-             num_prefills=self.num_prefills,
-             slot_mapping=slot_mapping_tensor,
-@@ -642,7 +700,7 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):
-             context_lens_tensor=context_lens_tensor,
-             block_tables=block_tables,
-             use_cuda_graph=use_captured_graph,
--            num_kv_splits=4,  # TODO(lucas) add heuristic
-+            num_kv_splits=num_kv_splits, 
-             head_dim=self.runner.model_config.get_head_size(),
-         )
- 
-@@ -700,6 +758,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
-         q_nope: torch.Tensor,
-         q_pe: torch.Tensor,
-         kv_c_and_k_pe_cache: torch.Tensor,
-+        kv_scale: torch.Tensor,
-         attn_metadata: TritonMLAMetadata,
-     ) -> torch.Tensor:
-         assert kv_c_and_k_pe_cache.numel() > 0
-@@ -740,7 +799,6 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
-         decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,
-                              decode_meta.block_tables,
-                              decode_meta.seq_lens_tensor, attn_logits,
--                             attn_metadata.num_kv_splits, self.scale,
--                             PAGE_SIZE)
--
-+                             attn_metadata.num_kv_splits, attn_metadata.num_stages, self.scale,
-+                             kv_scale, PAGE_SIZE)
-         return self._v_up_proj_and_o_proj(o)
-diff --git a/vllm/attention/layer.py b/vllm/attention/layer.py
-index e4df7ffc5..1abb1d1ed 100644
---- a/vllm/attention/layer.py
-+++ b/vllm/attention/layer.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Attention layer."""
- from typing import Any, Dict, List, Optional
-@@ -175,7 +176,7 @@ class Attention(nn.Module):
-                 key = key.view(-1, self.num_kv_heads, self.head_size)
-             if value is not None:
-                 value = value.view(-1, self.num_kv_heads, self.head_size)
--            if self.use_direct_call:
-+            if self.use_direct_call or True:
-                 forward_context: ForwardContext = get_forward_context()
-                 ctx_attn_metadata = forward_context.attn_metadata
-                 self_kv_cache = self.kv_cache[forward_context.virtual_engine]
-@@ -198,7 +199,7 @@ class Attention(nn.Module):
-                 return self.impl.forward(self, query, key, value,
-                                          self_kv_cache, ctx_attn_metadata)
-             else:
--                return torch.ops.vllm.unified_attention(
-+                return unified_attention(
-                     query, key, value, self.layer_name)
- 
-     def calc_kv_scales(self, key, value):
-diff --git a/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py b/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
-index 71caf3cba..57da5ee6d 100644
---- a/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
-+++ b/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import torch
-@@ -117,7 +118,7 @@ def blocksparse_flash_attn_varlen_fwd(
-                          q_block_size),  # smaller for decoding
-         EVEN_D=block_d == head_size,
-         num_warps=1 if decoding_only else 4,
--        num_stages=3)
-+        num_stages=1)
- 
-     return out
- 
-diff --git a/vllm/attention/ops/blocksparse_attention/interface.py b/vllm/attention/ops/blocksparse_attention/interface.py
-index 6ab69ea5b..83f16e63a 100644
---- a/vllm/attention/ops/blocksparse_attention/interface.py
-+++ b/vllm/attention/ops/blocksparse_attention/interface.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import math
-@@ -11,8 +12,9 @@ from .utils import (dense_to_crow_col, get_head_sliding_step,
- 
- IS_COMPUTE_8_OR_ABOVE = current_platform.has_device_capability(80)
- 
--if IS_COMPUTE_8_OR_ABOVE:
--    from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
-+#if IS_COMPUTE_8_OR_ABOVE:
-+#    from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
-+from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
- 
- 
- class LocalStridedBlockSparseAttn(torch.nn.Module):
-@@ -34,8 +36,7 @@ class LocalStridedBlockSparseAttn(torch.nn.Module):
-         super().__init__()
-         if use_spda is None:
-             use_spda = current_platform.is_rocm() or \
--                        current_platform.is_cpu() or not \
--                        IS_COMPUTE_8_OR_ABOVE
-+                        current_platform.is_cpu()
-         device = device or (torch.cuda.current_device()
-                             if current_platform.is_cuda_alike() else "cpu")
-         device = torch.device(device)
-@@ -123,10 +124,10 @@ class LocalStridedBlockSparseAttn(torch.nn.Module):
- 
-         return: tensor of shape as q.
-         """
--        assert (
--            IS_COMPUTE_8_OR_ABOVE
--        ), "Requires compute capability of 8 or above (Ampere or newer) to use \
--            Triton kernel."
-+        #assert (
-+        #    IS_COMPUTE_8_OR_ABOVE
-+        #), "Requires compute capability of 8 or above (Ampere or newer) to use \
-+        #    Triton kernel."
- 
-         sm_scale = sm_scale or 1.0 / math.sqrt(q.size(-1))
- 
-diff --git a/vllm/attention/ops/paged_attn.py b/vllm/attention/ops/paged_attn.py
-index 2c60bd0c3..5ffc5198a 100644
---- a/vllm/attention/ops/paged_attn.py
-+++ b/vllm/attention/ops/paged_attn.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from dataclasses import dataclass
-@@ -53,14 +54,14 @@ class PagedAttention:
-         num_kv_heads: int,
-         head_size: int,
-     ) -> Tuple[torch.Tensor, torch.Tensor]:
--        x = 16 // kv_cache.element_size()
-+        x = 32 // kv_cache.element_size()
-         num_blocks = kv_cache.shape[1]
- 
-         key_cache = kv_cache[0]
-         key_cache = key_cache.view(num_blocks, num_kv_heads, head_size // x,
-                                    -1, x)
-         value_cache = kv_cache[1]
--        value_cache = value_cache.view(num_blocks, num_kv_heads, head_size, -1)
-+        value_cache = value_cache.view(num_blocks, num_kv_heads, -1, head_size)
-         return key_cache, value_cache
- 
-     @staticmethod
-@@ -74,7 +75,7 @@ class PagedAttention:
-         k_scale: torch.Tensor,
-         v_scale: torch.Tensor,
-     ) -> None:
--        ops.reshape_and_cache(
-+        ops.reshape_and_cache_new(
-             key,
-             value,
-             key_cache,
-@@ -107,14 +108,14 @@ class PagedAttention:
-     ) -> torch.Tensor:
-         if blocksparse_vert_stride is not None and blocksparse_vert_stride > 1:
-             # use blocksparse paged attention
--            block_size = value_cache.size(-1)
-+            block_size = value_cache.shape[2]
-             assert (blocksparse_block_size > 0 and
-                     blocksparse_block_size % block_size == 0), \
-                 (f"{blocksparse_block_size=} needs to be a multiple of"
-                  f"{block_size=} used in block_tables.")
- 
-         output = torch.empty_like(query)
--        block_size = value_cache.shape[3]
-+        block_size = value_cache.shape[2]
-         num_seqs, num_heads, head_size = query.shape
-         max_num_partitions = ((max_seq_len + _PARTITION_SIZE - 1) //
-                               _PARTITION_SIZE)
-@@ -159,17 +160,24 @@ class PagedAttention:
-                 dtype=output.dtype,
-                 device=output.device,
-             )
-+            block_count = torch.zeros(
-+                size=(num_seqs, num_heads),
-+                dtype=torch.int,
-+                device=output.device,
-+            )
-             exp_sums = torch.empty(
-                 size=(num_seqs, num_heads, max_num_partitions),
-                 dtype=torch.float32,
-                 device=output.device,
-             )
-             max_logits = torch.empty_like(exp_sums)
-+            block_count_init_once = False
-             ops.paged_attention_v2(
-                 output,
-                 exp_sums,
-                 max_logits,
-                 tmp_output,
-+                block_count,
-                 query,
-                 key_cache,
-                 value_cache,
-@@ -188,6 +196,7 @@ class PagedAttention:
-                 blocksparse_vert_stride,
-                 blocksparse_block_size,
-                 blocksparse_head_sliding_step,
-+                block_count_init_once
-             )
-         return output
- 
-diff --git a/vllm/attention/ops/triton_decode_attention.py b/vllm/attention/ops/triton_decode_attention.py
-index 057fccb5e..4b31a6247 100644
---- a/vllm/attention/ops/triton_decode_attention.py
-+++ b/vllm/attention/ops/triton_decode_attention.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Adapted from
-@@ -30,12 +31,13 @@ It supports page size >= 1.
- 
- import logging
- 
-+import torch
- import triton
- import triton.language as tl
- 
--from vllm.platforms import current_platform
-+# from vllm.platforms import current_platform
- 
--is_hip_ = current_platform.is_rocm()
-+# is_hip_ = current_platform.is_rocm()
- 
- logger = logging.getLogger(__name__)
- 
-@@ -45,7 +47,6 @@ logger.warning(
-     "The following error message 'operation scheduled before its operands' "
-     "can be ignored.")
- 
--
- @triton.jit
- def tanh(x):
-     # Tanh is just a scaled sigmoid
-@@ -225,6 +226,161 @@ def _decode_att_m_fwd(
-     )
- 
- 
-+@triton.jit
-+def _fwd_grouped_kernel_stage1_kvint8(
-+    Q,
-+    K_Buffer,
-+    V_Buffer,
-+    KV_Scale,
-+    sm_scale,
-+    Req_to_tokens,
-+    B_Seqlen,
-+    Att_Out,
-+    stride_req_to_tokens_b,
-+    stride_qbs,
-+    stride_qh,
-+    stride_buf_kbs,
-+    stride_buf_kh,
-+    stride_buf_vbs,
-+    stride_buf_vh,
-+    stride_mid_ob,
-+    stride_mid_oh,
-+    stride_mid_os,
-+    kv_group_num: tl.constexpr,
-+    q_head_num: tl.constexpr,
-+    BLOCK_DMODEL: tl.constexpr,
-+    BLOCK_DPE: tl.constexpr,
-+    BLOCK_DV: tl.constexpr,
-+    BLOCK_N: tl.constexpr,
-+    BLOCK_H: tl.constexpr,
-+    NUM_KV_SPLITS: tl.constexpr,
-+    PAGE_SIZE: tl.constexpr,
-+    logit_cap: tl.constexpr,
-+    Lk: tl.constexpr,
-+    Lv: tl.constexpr,
-+):
-+    kv_scale = tl.load(KV_Scale + 0)
-+
-+    cur_batch = tl.program_id(0)
-+    cur_head_id = tl.program_id(1)
-+    cur_kv_head = cur_head_id // tl.cdiv(kv_group_num, BLOCK_H)
-+    split_kv_id = tl.program_id(2)
-+
-+    if kv_group_num > BLOCK_H:
-+        VALID_BLOCK_H: tl.constexpr = BLOCK_H
-+    else:
-+        VALID_BLOCK_H: tl.constexpr = kv_group_num
-+    cur_head = cur_head_id * VALID_BLOCK_H + tl.arange(0, BLOCK_H)
-+    mask_h = cur_head < (cur_head_id + 1) * VALID_BLOCK_H
-+    mask_h = mask_h & (cur_head < q_head_num)
-+
-+    offs_d = tl.arange(0, BLOCK_DMODEL)
-+    offs_dv = tl.arange(0, BLOCK_DV)
-+    mask_d = offs_d < Lk
-+    mask_dv = offs_dv < Lv
-+    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)
-+    cur_batch_req_idx = cur_batch
-+
-+    offs_q = cur_batch * stride_qbs + cur_head[:, None] * stride_qh + offs_d[
-+        None, :]
-+    q = tl.load(Q + offs_q,
-+                mask=(mask_h[:, None]) & (mask_d[None, :]),
-+                other=0.0)
-+
-+    if BLOCK_DPE > 0:
-+        offs_dpe = BLOCK_DMODEL + tl.arange(0, BLOCK_DPE)
-+        mask_dpe = offs_dpe < Lk
-+        off_qpe = (cur_batch * stride_qbs + cur_head[:, None] * stride_qh +
-+                   offs_dpe[None, :])
-+        qpe = tl.load(Q + off_qpe,
-+                      mask=(mask_h[:, None]) & (mask_dpe[None, :]),
-+                      other=0.0)
-+
-+    kv_len_per_split = tl.cdiv(cur_batch_seq_len, NUM_KV_SPLITS)
-+    split_kv_start = kv_len_per_split * split_kv_id
-+    split_kv_end = tl.minimum(split_kv_start + kv_len_per_split,
-+                              cur_batch_seq_len)
-+
-+    e_max = tl.zeros([BLOCK_H], dtype=tl.float32) - float("inf")
-+    e_sum = tl.zeros([BLOCK_H], dtype=tl.float32)
-+    acc = tl.zeros([BLOCK_H, BLOCK_DV], dtype=tl.float32)
-+
-+    if split_kv_end > split_kv_start:
-+        for start_n in range(split_kv_start, split_kv_end, BLOCK_N):
-+            offs_n = start_n + tl.arange(0, BLOCK_N)
-+            kv_page_number = tl.load(
-+                Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx +
-+                offs_n // PAGE_SIZE,
-+                mask=offs_n < split_kv_end,
-+                other=0,
-+            )
-+            kv_loc = kv_page_number * PAGE_SIZE + offs_n % PAGE_SIZE
-+            offs_buf_k = (kv_loc[None, :] * stride_buf_kbs +
-+                          cur_kv_head * stride_buf_kh + offs_d[:, None])
-+            k = tl.load(
-+                K_Buffer + offs_buf_k,
-+                mask=(offs_n[None, :] < split_kv_end) & (mask_d[:, None]),
-+                other=0,
-+            )          
-+            k = k * kv_scale
-+            qk = tl.dot(q, k.to(q.dtype))
-+            if BLOCK_DPE > 0:
-+                offs_buf_kpe = (kv_loc[None, :] * stride_buf_kbs +
-+                                cur_kv_head * stride_buf_kh +
-+                                offs_dpe[:, None])
-+                kpe = tl.load(
-+                    K_Buffer + offs_buf_kpe,
-+                    mask=(offs_n[None, :] < split_kv_end) &
-+                    (mask_dpe[:, None]),
-+                    other=0,
-+                )
-+                kpe = kpe * kv_scale
-+                qk += tl.dot(qpe, kpe.to(qpe.dtype))
-+            qk *= sm_scale
-+
-+            if logit_cap > 0:
-+                qk = logit_cap * tanh(qk / logit_cap)
-+
-+            qk = tl.where(mask_h[:, None] & (offs_n[None, :] < split_kv_end),
-+                          qk, float("-inf"))
-+
-+            offs_buf_v = (kv_loc[:, None] * stride_buf_vbs +
-+                          cur_kv_head * stride_buf_vh + offs_dv[None, :])
-+            v = tl.load(
-+                V_Buffer + offs_buf_v,
-+                mask=(offs_n[:, None] < split_kv_end) & (mask_dv[None, :]),
-+                other=0,
-+            )
-+            v = (v * kv_scale).to(q.dtype)
-+
-+            n_e_max = tl.maximum(tl.max(qk, 1), e_max)
-+            re_scale = tl.exp(e_max - n_e_max)
-+            p = tl.exp(qk - n_e_max[:, None])
-+            acc *= re_scale[:, None]
-+            acc += tl.dot(p.to(v.dtype), v)
-+
-+            e_sum = e_sum * re_scale + tl.sum(p, 1)
-+            e_max = n_e_max
-+
-+        offs_mid_o = (cur_batch * stride_mid_ob +
-+                      cur_head[:, None] * stride_mid_oh +
-+                      split_kv_id * stride_mid_os + offs_dv[None, :])
-+
-+        tl.store(
-+            Att_Out + offs_mid_o,
-+            acc / e_sum[:, None],
-+            mask=(mask_h[:, None]) & (mask_dv[None, :]),
-+        )
-+
-+        offs_mid_o_1 = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +
-+                        split_kv_id * stride_mid_os + Lv)
-+
-+        tl.store(
-+            Att_Out + offs_mid_o_1,
-+            e_max + tl.log(e_sum),
-+            mask=mask_h,
-+        )
-+
- @triton.jit
- def _fwd_grouped_kernel_stage1(
-     Q,
-@@ -383,17 +539,19 @@ def _decode_grouped_att_m_fwd(
-     Req_to_tokens,
-     B_Seqlen,
-     num_kv_splits,
-+    num_stages,
-     sm_scale,
-+    kv_scale,
-     page_size,
-     logit_cap,
- ):
--    BLOCK = 32
-+    BLOCK = 16
-     Lk = k_buffer.shape[-1]
-     Lv = v_buffer.shape[-1]
- 
-     # [TODO] work around shmem limit on MI3xx
--    if is_hip_ and Lk >= 576:
--        BLOCK = 16
-+    # if is_hip_ and Lk >= 576:
-+    #     BLOCK = 16
- 
-     if Lk == 576:
-         BLOCK_DMODEL = 512
-@@ -410,57 +568,100 @@ def _decode_grouped_att_m_fwd(
-     kv_group_num = q.shape[1] // k_buffer.shape[-2]
- 
-     BLOCK_H = 16
-+
-     NUM_KV_SPLITS = num_kv_splits
-+
-     grid = (
-         batch,
-         triton.cdiv(head_num, min(BLOCK_H, kv_group_num)),
-         NUM_KV_SPLITS,
-     )
--
--    extra_kargs = {}
--    if is_hip_:
--        # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
--        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
--        extra_kargs = {
--            "waves_per_eu": 4,
--            "matrix_instr_nonkdim": 16,
--            "kpack": 2
--        }
--
--    _fwd_grouped_kernel_stage1[grid](
--        q,
--        k_buffer,
--        v_buffer,
--        sm_scale,
--        Req_to_tokens,
--        B_Seqlen,
--        att_out,
--        Req_to_tokens.stride(0),
--        q.stride(0),
--        q.stride(1),
--        k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
--        k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
--        v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
--        v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
--        att_out.stride(0),
--        att_out.stride(1),
--        att_out.stride(2),
--        kv_group_num=kv_group_num,
--        q_head_num=head_num,
--        BLOCK_DMODEL=BLOCK_DMODEL,
--        BLOCK_DPE=BLOCK_DPE,
--        BLOCK_DV=BLOCK_DV,
--        BLOCK_N=BLOCK,
--        BLOCK_H=BLOCK_H,
--        NUM_KV_SPLITS=NUM_KV_SPLITS,
--        PAGE_SIZE=page_size,
--        logit_cap=logit_cap,
--        num_warps=4,
--        num_stages=2,
--        Lk=Lk,
--        Lv=Lv,
--        **extra_kargs,
--    )
-+    if num_stages == 1:
-+        extra_kargs = {"scenario":"mla"}
-+    elif num_stages == 2:
-+        extra_kargs = {"scenario" : "mla", "pipeline" : "cpasync"}
-+    else:
-+        KeyError("num_stages should be 1 or 2")
-+    # num_stages=1
-+    # extra_kargs = {"scenario":"mla"}
-+    # if is_hip_:
-+    #     # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
-+    #     # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
-+    #     extra_kargs = {
-+    #         "waves_per_eu": 4,
-+    #         "matrix_instr_nonkdim": 16,
-+    #         "kpack": 2
-+    #     }
-+    if k_buffer.dtype == torch.int8:
-+        _fwd_grouped_kernel_stage1_kvint8[grid](
-+            q,
-+            k_buffer,
-+            v_buffer,
-+            kv_scale,
-+            sm_scale,
-+            Req_to_tokens,
-+            B_Seqlen,
-+            att_out,
-+            Req_to_tokens.stride(0),
-+            q.stride(0),
-+            q.stride(1),
-+            k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            att_out.stride(0),
-+            att_out.stride(1),
-+            att_out.stride(2),
-+            kv_group_num=kv_group_num,
-+            q_head_num=head_num,
-+            BLOCK_DMODEL=BLOCK_DMODEL,
-+            BLOCK_DPE=BLOCK_DPE,
-+            BLOCK_DV=BLOCK_DV,
-+            BLOCK_N=BLOCK,
-+            BLOCK_H=BLOCK_H,
-+            NUM_KV_SPLITS=NUM_KV_SPLITS,
-+            PAGE_SIZE=page_size,
-+            logit_cap=logit_cap,
-+            num_stages=num_stages,
-+            Lk=Lk,
-+            Lv=Lv,
-+            **extra_kargs,
-+        )
-+    else:
-+        _fwd_grouped_kernel_stage1[grid](
-+            q,
-+            k_buffer,
-+            v_buffer,
-+            sm_scale,
-+            Req_to_tokens,
-+            B_Seqlen,
-+            att_out,
-+            Req_to_tokens.stride(0),
-+            q.stride(0),
-+            q.stride(1),
-+            k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-+            att_out.stride(0),
-+            att_out.stride(1),
-+            att_out.stride(2),
-+            kv_group_num=kv_group_num,
-+            q_head_num=head_num,
-+            BLOCK_DMODEL=BLOCK_DMODEL,
-+            BLOCK_DPE=BLOCK_DPE,
-+            BLOCK_DV=BLOCK_DV,
-+            BLOCK_N=BLOCK,
-+            BLOCK_H=BLOCK_H,
-+            NUM_KV_SPLITS=NUM_KV_SPLITS,
-+            PAGE_SIZE=page_size,
-+            logit_cap=logit_cap,
-+            num_warps=4,
-+            num_stages=num_stages,
-+            Lk=Lk,
-+            Lv=Lv,
-+            **extra_kargs,
-+        )
- 
- 
- @triton.jit
-@@ -535,14 +736,14 @@ def _decode_softmax_reducev_fwd(
-     NUM_KV_SPLITS = num_kv_splits
- 
-     extra_kargs = {}
--    if is_hip_:
--        # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
--        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
--        extra_kargs = {
--            "waves_per_eu": 4,
--            "matrix_instr_nonkdim": 16,
--            "kpack": 2
--        }
-+    # if is_hip_:
-+    #     # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
-+    #     # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
-+    #     extra_kargs = {
-+    #         "waves_per_eu": 4,
-+    #         "matrix_instr_nonkdim": 16,
-+    #         "kpack": 2
-+    #     }
- 
-     grid = (batch, head_num)
-     _fwd_kernel_stage2[grid](
-@@ -601,7 +802,9 @@ def decode_attention_fwd_grouped(
-     b_seq_len,
-     attn_logits,
-     num_kv_splits,
-+    num_stages,
-     sm_scale,
-+    kv_scale,
-     page_size,
-     logit_cap=0.0,
- ):
-@@ -613,7 +816,9 @@ def decode_attention_fwd_grouped(
-         req_to_token,
-         b_seq_len,
-         num_kv_splits,
-+        num_stages,
-         sm_scale,
-+        kv_scale,
-         page_size,
-         logit_cap,
-     )
-@@ -630,7 +835,9 @@ def decode_attention_fwd(
-     b_seq_len,
-     attn_logits,
-     num_kv_splits,
-+    num_stages,
-     sm_scale,
-+    kv_scale,
-     page_size=1,
-     logit_cap=0.0,
- ):
-@@ -663,7 +870,9 @@ def decode_attention_fwd(
-             b_seq_len,
-             attn_logits,
-             num_kv_splits,
-+            num_stages,
-             sm_scale,
-+            kv_scale,
-             page_size,
-             logit_cap,
-         )
-diff --git a/vllm/config.py b/vllm/config.py
-index 9ba497576..dd6276160 100644
---- a/vllm/config.py
-+++ b/vllm/config.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+-                    out=prefill_output,
+-                    fa_version=self.vllm_flash_attn_version,
+-                    q_descale=layer._q_scale.expand(descale_shape),
+-                    k_descale=layer._k_scale.expand(descale_shape),
+-                    v_descale=layer._v_scale.expand(descale_shape),
++                    # out=prefill_output,
++                    # fa_version=self.vllm_flash_attn_version,
++                    # q_descale=layer._q_scale.expand(descale_shape),
++                    # k_descale=layer._k_scale.expand(descale_shape),
++                    # v_descale=layer._v_scale.expand(descale_shape),
+                 )
  
- import ast
-@@ -595,6 +596,8 @@ class ModelConfig:
-                 method = get_quantization_config(name)
-                 quantization_override = method.override_quantization_method(
-                     quant_cfg, self.quantization)
-+                if quantization_override !=name:
-+                    quantization_override =None
-                 if quantization_override:
-                     quant_method = quantization_override
-                     self.quantization = quantization_override
-@@ -985,37 +988,7 @@ class ModelConfig:
+         if decode_meta := attn_metadata.decode_metadata:
+@@ -872,13 +874,13 @@ class FlashAttentionImpl(AttentionImpl):
+                 assert decode_meta.query_start_loc is not None
+                 descale_shape = (decode_meta.query_start_loc.shape[0] - 1,
+                                  key.shape[1])
+-                flash_attn_varlen_func(
++                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
+                     q=decode_query,
+                     k=key_cache,
+                     v=value_cache,
+                     cu_seqlens_q=decode_meta.query_start_loc,
+                     max_seqlen_q=decode_meta.max_decode_query_len,
+-                    seqused_k=decode_meta.seq_lens_tensor,
++                    cu_seqlens_k=decode_meta.seq_start_loc,
+                     max_seqlen_k=decode_meta.max_decode_seq_len,
+                     softmax_scale=softmax_scale,
+                     causal=True,
+@@ -886,11 +888,11 @@ class FlashAttentionImpl(AttentionImpl):
+                     alibi_slopes=alibi_slopes,
+                     softcap=logits_soft_cap,
+                     block_table=decode_meta.block_tables,
+-                    out=decode_output,
+-                    fa_version=self.vllm_flash_attn_version,
+-                    q_descale=layer._q_scale.expand(descale_shape),
+-                    k_descale=layer._k_scale.expand(descale_shape),
+-                    v_descale=layer._v_scale.expand(descale_shape),
++                    # out=decode_output,
++                    # fa_version=self.vllm_flash_attn_version,
++                    # q_descale=layer._q_scale.expand(descale_shape),
++                    # k_descale=layer._k_scale.expand(descale_shape),
++                    # v_descale=layer._v_scale.expand(descale_shape),
+                 )
+             else:
+                 # Use flash_attn_with_kvcache for normal decoding.
+@@ -900,7 +902,7 @@ class FlashAttentionImpl(AttentionImpl):
+                     block_tables_arg,
+                 ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
+                 descale_shape = (seq_lens_arg.shape[0], key_cache.shape[-2])
+-                flash_attn_with_kvcache(
++                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
+                     q=decode_query.unsqueeze(1),
+                     k_cache=key_cache,
+                     v_cache=value_cache,
+@@ -911,12 +913,12 @@ class FlashAttentionImpl(AttentionImpl):
+                     window_size=window_size,
+                     alibi_slopes=alibi_slopes,
+                     softcap=logits_soft_cap,
+-                    out=decode_output.unsqueeze(1),
+-                    fa_version=self.vllm_flash_attn_version,
+-                    q_descale=layer._q_scale.expand(descale_shape),
+-                    k_descale=layer._k_scale.expand(descale_shape),
+-                    v_descale=layer._v_scale.expand(descale_shape),
+-                )
++                    # out=decode_output.unsqueeze(1),
++                    # fa_version=self.vllm_flash_attn_version,
++                    # q_descale=layer._q_scale.expand(descale_shape),
++                    # k_descale=layer._k_scale.expand(descale_shape),
++                    # v_descale=layer._v_scale.expand(descale_shape),
++                ).squeeze(1)
+         return output
  
-     @property
-     def use_mla(self) -> bool:
--        if not self.is_deepseek_mla or envs.VLLM_MLA_DISABLE:
--            return False
--
--        if self.quantization is not None and self.quantization not in [\
--            "fp8", "compressed-tensors"]:
--            logger.warning(
--                "MLA is not supported with %s quantization. "
--                "Disabling MLA.", self.quantization)
--            return False
--
--        # If using a "compressed-tensors" checkpoint, check that all groups
--        # have fp8 for both weights and activations.
--        if self.quantization == "compressed-tensors":
--            quant_config = self._parse_quant_hf_config()
--            for group_name, cfg in quant_config.get("config_groups", {
--                    "": {}
--            }).items():
--                act_cfg = cfg.get("input_activations", {})
--                act_type = None if act_cfg is None else act_cfg.get("type", "")
--                w_cfg = cfg.get("weights", {})
--                w_type = None if w_cfg is None else w_cfg.get("type", "")
--                if act_type != "fp8" or w_type != "fp8":
--                    logger.warning(
--                        "compressed-tensors MLA support requires fp8 "
--                        "activations and weights in group '%s', but got "
--                        "activations type '%s' and weights type '%s'.\n "
--                        "Full config: %s", group_name, act_type, w_type,
--                        quant_config)
--                    return False
--
--        return True
-+        return self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE
  
-     @property
-     def supported_runner_types(self) -> Set[RunnerType]:
-@@ -1111,7 +1084,7 @@ class CacheConfig:
-     def _verify_cache_dtype(self) -> None:
-         if self.cache_dtype == "auto":
-             pass
--        elif self.cache_dtype in ("fp8", "fp8_e4m3", "fp8_e5m2"):
-+        elif self.cache_dtype in ("fp8", "fp8_e4m3", "fp8_e5m2", "int8"):
-             logger.info(
-                 "Using fp8 data type to store kv cache. It reduces the GPU "
-                 "memory footprint and boosts the performance. "
-@@ -2706,6 +2679,12 @@ class KVTransferConfig(BaseModel):
-         return self.kv_connector is not None and \
-             self.kv_role in ["kv_consumer", "kv_both"]
+diff --git a/vllm/attention/backends/flashinfer.py b/vllm/attention/backends/flashinfer.py
+index a3937760f..5e8036b86 100644
+--- a/vllm/attention/backends/flashinfer.py
++++ b/vllm/attention/backends/flashinfer.py
+@@ -15,7 +15,8 @@ try:
+     from flashinfer.decode import CUDAGraphBatchDecodeWithPagedKVCacheWrapper
+     from flashinfer.prefill import BatchPrefillWithPagedKVCacheWrapper
+ 
+-    from vllm.vllm_flash_attn import flash_attn_varlen_func
++    # from vllm.vllm_flash_attn import flash_attn_varlen_func
++    from flash_attn import flash_attn_varlen_func
+     FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024
+ except ImportError:
+     # Avoid turning these types into variables during type checking
+diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py
+index 78cf95288..103fdce57 100644
+--- a/vllm/attention/backends/mla/common.py
++++ b/vllm/attention/backends/mla/common.py
+@@ -208,7 +208,7 @@ from vllm.attention.backends.utils import (PAD_SLOT_ID, compute_slot_mapping,
+                                            compute_slot_mapping_start_idx,
+                                            is_block_tables_empty)
+ from vllm.attention.ops.merge_attn_states import merge_attn_states
+-from vllm.attention.utils.fa_utils import get_flash_attn_version
++# from vllm.attention.utils.fa_utils import get_flash_attn_version
+ from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                                LinearBase,
+                                                UnquantizedLinearMethod)
+@@ -239,6 +239,8 @@ if TYPE_CHECKING:
+ 
+ is_hip = current_platform.is_rocm()
+ 
++def get_flash_attn_version():
++    return None
  
-+    @property
-+    def is_layerwise_kv_transfer(self) -> bool:
-+        # so far, only LayerwisePyNcclConnector supports layerwise kv transfer
-+        return self.kv_connector is not None and self.kv_connector in [
-+            "LayerwisePyNcclConnector"
-+        ]
+ class MLACommonBackend(AttentionBackend):
  
- class CompilationLevel:
-     # constants for the levels of the compilation process
-@@ -3170,12 +3149,12 @@ class VllmConfig:
+@@ -1135,7 +1137,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
+                 del eye
+                 # standardize to (output, input)
+                 return dequant_weights.T
+-            return layer.weight
++            return layer.weight if not envs.MACA_VLLM_USE_TN_2_NN else layer.weight.T
  
-             if capability_tuple is not None:
-                 capability = capability_tuple.to_int()
--                if capability < quant_config.get_min_capability():
--                    raise ValueError(
--                        f"The quantization method {model_config.quantization} "
--                        "is not supported for the current GPU. Minimum "
--                        f"capability: {quant_config.get_min_capability()}. "
--                        f"Current capability: {capability}.")
-+                #if capability < quant_config.get_min_capability():
-+                #    raise ValueError(
-+                #        f"The quantization method {model_config.quantization} "
-+                #        "is not supported for the current GPU. Minimum "
-+                #        f"capability: {quant_config.get_min_capability()}. "
-+                #        f"Current capability: {capability}.")
-             supported_dtypes = quant_config.get_supported_act_dtypes()
-             if model_config.dtype not in supported_dtypes:
-                 raise ValueError(
-diff --git a/vllm/distributed/device_communicators/pynccl_wrapper.py b/vllm/distributed/device_communicators/pynccl_wrapper.py
-index 03c3b0be7..13681390b 100644
---- a/vllm/distributed/device_communicators/pynccl_wrapper.py
-+++ b/vllm/distributed/device_communicators/pynccl_wrapper.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+         # we currently do not have quantized bmm's which are needed for
+         # `W_UV` and `W_UK_T`, we we just store fp16/bf16 copies and perform
+diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
+index e06f7d54e..5cf761817 100644
+--- a/vllm/attention/backends/triton_mla.py
++++ b/vllm/attention/backends/triton_mla.py
+@@ -1,6 +1,7 @@
  # SPDX-License-Identifier: Apache-2.0
+ # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
  
- # This file is a pure Python wrapper for the NCCL library.
-@@ -127,18 +128,18 @@ class Function:
- class NCCLLibrary:
-     exported_functions = [
-         # const char* ncclGetErrorString(ncclResult_t result)
--        Function("ncclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
-+        Function("mcclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
-         # ncclResult_t  ncclGetVersion(int *version);
--        Function("ncclGetVersion", ncclResult_t,
-+        Function("mcclGetVersion", ncclResult_t,
-                  [ctypes.POINTER(ctypes.c_int)]),
-         # ncclResult_t ncclGetUniqueId(ncclUniqueId* uniqueId);
--        Function("ncclGetUniqueId", ncclResult_t,
-+        Function("mcclGetUniqueId", ncclResult_t,
-                  [ctypes.POINTER(ncclUniqueId)]),
-         # ncclResult_t  ncclCommInitRank(
-         #   ncclComm_t* comm, int nranks, ncclUniqueId commId, int rank);
-         # note that ncclComm_t is a pointer type, so the first argument
-         # is a pointer to a pointer
--        Function("ncclCommInitRank", ncclResult_t, [
-+        Function("mcclCommInitRank", ncclResult_t, [
-             ctypes.POINTER(ncclComm_t), ctypes.c_int, ncclUniqueId,
-             ctypes.c_int
-         ]),
-@@ -148,7 +149,7 @@ class NCCLLibrary:
-         #   cudaStream_t stream);
-         # note that cudaStream_t is a pointer type, so the last argument
-         # is a pointer
--        Function("ncclAllReduce", ncclResult_t, [
-+        Function("mcclAllReduce", ncclResult_t, [
-             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
-             ncclRedOp_t, ncclComm_t, cudaStream_t
-         ]),
-@@ -159,7 +160,7 @@ class NCCLLibrary:
-         #   cudaStream_t stream);
-         # note that cudaStream_t is a pointer type, so the last argument
-         # is a pointer
--        Function("ncclAllGather", ncclResult_t, [
-+        Function("mcclAllGather", ncclResult_t, [
-             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
-             ncclComm_t, cudaStream_t
-         ]),
-@@ -170,7 +171,7 @@ class NCCLLibrary:
-         #   cudaStream_t stream);
-         # note that cudaStream_t is a pointer type, so the last argument
-         # is a pointer
--        Function("ncclReduceScatter", ncclResult_t, [
-+        Function("mcclReduceScatter", ncclResult_t, [
-             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
-             ncclRedOp_t, ncclComm_t, cudaStream_t
-         ]),
-@@ -178,7 +179,7 @@ class NCCLLibrary:
-         # ncclResult_t  ncclSend(
-         #   const void* sendbuff, size_t count, ncclDataType_t datatype,
-         #   int dest, ncclComm_t comm, cudaStream_t stream);
--        Function("ncclSend", ncclResult_t, [
-+        Function("mcclSend", ncclResult_t, [
-             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
-             ncclComm_t, cudaStream_t
-         ]),
-@@ -186,7 +187,7 @@ class NCCLLibrary:
-         # ncclResult_t  ncclRecv(
-         #   void* recvbuff, size_t count, ncclDataType_t datatype,
-         #   int src, ncclComm_t comm, cudaStream_t stream);
--        Function("ncclRecv", ncclResult_t, [
-+        Function("mcclRecv", ncclResult_t, [
-             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
-             ncclComm_t, cudaStream_t
-         ]),
-@@ -195,7 +196,7 @@ class NCCLLibrary:
-         #   const void* sendbuff, void* recvbuff, size_t count,
-         #   ncclDataType_t datatype, int root, ncclComm_t comm,
-         #   cudaStream_t stream);
--        Function("ncclBroadcast", ncclResult_t, [
-+        Function("mcclBroadcast", ncclResult_t, [
-             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
-             ctypes.c_int, ncclComm_t, cudaStream_t
-         ]),
-@@ -205,7 +206,7 @@ class NCCLLibrary:
-         # because Python object destruction can happen in random order,
-         # it is better not to call it at all.
-         # ncclResult_t  ncclCommDestroy(ncclComm_t comm);
--        Function("ncclCommDestroy", ncclResult_t, [ncclComm_t]),
-+        Function("mcclCommDestroy", ncclResult_t, [ncclComm_t]),
-     ]
++from dataclasses import dataclass
+ from typing import Any, Dict, List, Optional, Type
  
-     # class attribute to store the mapping from the path to the library
-@@ -248,7 +249,7 @@ class NCCLLibrary:
-         self._funcs = NCCLLibrary.path_to_dict_mapping[so_file]
+ import torch
+@@ -13,6 +14,44 @@ from vllm.attention.backends.mla.common import (MLACommonBackend,
+ from vllm.attention.ops.triton_decode_attention import decode_attention_fwd
  
-     def ncclGetErrorString(self, result: ncclResult_t) -> str:
--        return self._funcs["ncclGetErrorString"](result).decode("utf-8")
-+        return self._funcs["mcclGetErrorString"](result).decode("utf-8")
  
-     def NCCL_CHECK(self, result: ncclResult_t) -> None:
-         if result != 0:
-@@ -257,7 +258,7 @@ class NCCLLibrary:
++import json
++import os
++
++# TODO: Configure environment variables temporarily. New versions do not need to be configured
++os.environ['TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP'] = '1'
++os.environ['TRITON_ENABLE_MACA_CHAIN_DOT_OPT'] = '1'
++
++def load_config():
++    # Load JSON data from the file
++    json_path = config_file_path = os.path.join(
++        os.path.dirname(os.path.realpath(__file__)), "configs", "tp8_merge.json")
++    with open(json_path, 'r') as file:
++        data = json.load(file)
++    return data
++
++JSON_DATA = load_config()
++
++def find_best_mla_para(json_data, batch_size, input_len, tp_size):
++    best_match = None
++    best_batch_size_diff = float('inf')
++    best_input_len_diff = float('inf')
++    
++    for entry in json_data:
++        if entry["BS"] == batch_size and entry["L"] == input_len:
++            return entry["num_kv_splits"], entry['num_stages']
++        batch_size_diff = abs(entry["BS"] - batch_size)
++        input_len_diff = abs(entry["L"] - input_len)
++        
++        # Check if this is a better match than the current best match
++        if batch_size_diff < best_batch_size_diff or (batch_size_diff == best_batch_size_diff and input_len_diff < best_input_len_diff):
++            best_match = entry
++            best_batch_size_diff = batch_size_diff
++            best_input_len_diff = input_len_diff
++    
++    # If a match was found, return the best_kv_splits, otherwise return None
++    return best_match["num_kv_splits"],best_match["num_stages"]
++
++
+ class TritonMLABackend(MLACommonBackend):
  
-     def ncclGetVersion(self) -> str:
-         version = ctypes.c_int()
--        self.NCCL_CHECK(self._funcs["ncclGetVersion"](ctypes.byref(version)))
-+        self.NCCL_CHECK(self._funcs["mcclGetVersion"](ctypes.byref(version)))
-         version_str = str(version.value)
-         # something like 21903 --> "2.19.3"
-         major = version_str[0].lstrip("0")
-@@ -267,14 +268,14 @@ class NCCLLibrary:
+     @staticmethod
+@@ -23,8 +62,37 @@ class TritonMLABackend(MLACommonBackend):
+     def get_impl_cls() -> Type["TritonMLAImpl"]:
+         return TritonMLAImpl
  
-     def ncclGetUniqueId(self) -> ncclUniqueId:
-         unique_id = ncclUniqueId()
--        self.NCCL_CHECK(self._funcs["ncclGetUniqueId"](
-+        self.NCCL_CHECK(self._funcs["mcclGetUniqueId"](
-             ctypes.byref(unique_id)))
-         return unique_id
+-
+-class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
++    @staticmethod
++    def get_metadata_cls() -> Type["TritonMLAMetadata"]:
++        return TritonMLAMetadata
++
++@dataclass
++class TritonMLAMetadata(MLACommonMetadata):
++    num_kv_splits: int = 4  # TODO: heuristic
++    num_stages: int = 1
++
++    @property
++    def decode_metadata(self):
++        if self.num_decode_tokens == 0:
++            return None
++        if self._cached_decode_metadata is not None:
++            return self._cached_decode_metadata
++        
++        decode_metadata = super().decode_metadata
++        
++        if decode_metadata is not None:
++            if decode_metadata.seq_lens_tensor is not None:
++                batch = decode_metadata.seq_lens_tensor.shape[0]
++                max_seq_len = int(decode_metadata.seq_lens_tensor.max())
++                num_kv_splits, num_stages = find_best_mla_para(JSON_DATA, batch, max_seq_len, 8)
++            else:
++                num_kv_splits = self.num_kv_splits
++                num_stages = self.num_stages
++            decode_metadata.num_kv_splits = num_kv_splits
++            decode_metadata.num_stages = num_stages
++        return decode_metadata
++
++class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
  
-     def ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId,
-                          rank: int) -> ncclComm_t:
-         comm = ncclComm_t()
--        self.NCCL_CHECK(self._funcs["ncclCommInitRank"](ctypes.byref(comm),
-+        self.NCCL_CHECK(self._funcs["mcclCommInitRank"](ctypes.byref(comm),
-                                                         world_size, unique_id,
-                                                         rank))
-         return comm
-@@ -287,7 +288,7 @@ class NCCLLibrary:
-         # both are aliases of `ctypes.c_int`
-         # when we pass int to a function, it will be converted to `ctypes.c_int`
-         # by ctypes automatically
--        self.NCCL_CHECK(self._funcs["ncclAllReduce"](sendbuff, recvbuff, count,
-+        self.NCCL_CHECK(self._funcs["mcclAllReduce"](sendbuff, recvbuff, count,
-                                                      datatype, op, comm,
-                                                      stream))
+     def __init__(
+             self,
+@@ -70,7 +138,7 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+         q_nope: torch.Tensor,
+         q_pe: torch.Tensor,
+         kv_c_and_k_pe_cache: torch.Tensor,
+-        attn_metadata: MLACommonMetadata,
++        attn_metadata: TritonMLAMetadata,
+     ) -> torch.Tensor:
+         assert kv_c_and_k_pe_cache.numel() > 0
  
-@@ -299,7 +300,7 @@ class NCCLLibrary:
-         # both are aliases of `ctypes.c_int`
-         # when we pass int to a function, it will be converted to `ctypes.c_int`
-         # by ctypes automatically
--        self.NCCL_CHECK(self._funcs["ncclReduceScatter"](sendbuff, recvbuff,
-+        self.NCCL_CHECK(self._funcs["mcclReduceScatter"](sendbuff, recvbuff,
-                                                          count, datatype, op,
-                                                          comm, stream))
+@@ -85,14 +153,12 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+                         dtype=q.dtype,
+                         device=q.device)
+ 
+-        num_kv_splits = 4  # TODO: heuristic
+-
+         # TODO(lucas) Allocate ahead of time
+         attn_logits = torch.empty(
+             (
+                 B,
+                 self.num_heads,
+-                num_kv_splits,
++                decode_meta.num_kv_splits,
+                 # NOTE(lucas) idk why the +1 is here but sglang has it so we
+                 # just mirror that
+                 self.kv_lora_rank + 1,
+@@ -110,6 +176,6 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+         decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,
+                              decode_meta.block_tables,
+                              decode_meta.seq_lens_tensor, attn_logits,
+-                             num_kv_splits, self.scale, PAGE_SIZE)
++                             decode_meta.num_kv_splits, decode_meta.num_stages, self.scale, PAGE_SIZE)
+ 
+         return self._v_up_proj(o)
+diff --git a/vllm/attention/ops/flashmla.py b/vllm/attention/ops/flashmla.py
+index b85f27ac4..0eaf50b29 100644
+--- a/vllm/attention/ops/flashmla.py
++++ b/vllm/attention/ops/flashmla.py
+@@ -10,29 +10,37 @@ from vllm.platforms import current_platform
+ 
+ logger = init_logger(__name__)
+ 
+-if current_platform.is_cuda():
+-    try:
+-        import vllm._flashmla_C  # noqa: F401
+-        _flashmla_C_AVAILABLE = True
+-    except ImportError:
+-        _flashmla_C_AVAILABLE = False
+-else:
+-    _flashmla_C_AVAILABLE = False
++# if current_platform.is_cuda():
++#     try:
++#         import vllm._flashmla_C  # noqa: F401
++#         _flashmla_C_AVAILABLE = True
++#     except ImportError:
++#         _flashmla_C_AVAILABLE = False
++# else:
++#     _flashmla_C_AVAILABLE = False
++try :
++    import flash_mla
++    _flashmla_AVAILABLE = True
++except ImportError as e:
++    logger.warning("Failed to import from flash_mla with %r on MACA Platform", e)
++    _flashmla_AVAILABLE = False
  
-@@ -310,28 +311,28 @@ class NCCLLibrary:
-         # which is an aliases of `ctypes.c_int`
-         # when we pass int to a function, it will be converted to `ctypes.c_int`
-         # by ctypes automatically
--        self.NCCL_CHECK(self._funcs["ncclAllGather"](sendbuff, recvbuff, count,
-+        self.NCCL_CHECK(self._funcs["mcclAllGather"](sendbuff, recvbuff, count,
-                                                      datatype, comm, stream))
  
-     def ncclSend(self, sendbuff: buffer_type, count: int, datatype: int,
-                  dest: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
--        self.NCCL_CHECK(self._funcs["ncclSend"](sendbuff, count, datatype,
-+        self.NCCL_CHECK(self._funcs["mcclSend"](sendbuff, count, datatype,
-                                                 dest, comm, stream))
+ def is_flashmla_supported() -> Tuple[bool, Optional[str]]:
+     """
+     Return: is_supported_flag, unsupported_reason (optional).
+     """
+-    if not current_platform.is_cuda():
+-        return False, "FlashMLA is only supported on CUDA devices."
+-    if current_platform.get_device_capability()[0] != 9:
+-        return False, "FlashMLA is only supported on Hopper devices."
+-    if not _flashmla_C_AVAILABLE:
+-        return False, "vllm._flashmla_C is not available, likely was not "\
+-            "compiled due to insufficient nvcc version or a supported arch "\
+-            "(only sm90a currently) was not in the list of target arches to "\
+-            "compile for."
++    # if not current_platform.is_cuda():
++    #     return False, "FlashMLA is only supported on CUDA devices."
++    # if current_platform.get_device_capability()[0] != 9:
++    #     return False, "FlashMLA is only supported on Hopper devices."
++    # if not _flashmla_C_AVAILABLE:
++    #     return False, "vllm._flashmla_C is not available, likely was not "\
++    #         "compiled due to insufficient nvcc version or a supported arch "\
++    #         "(only sm90a currently) was not in the list of target arches to "\
++    #         "compile for."
++    if not _flashmla_AVAILABLE:
++        return False, "flash_mla is not available"
+     return True, None
+ 
+ 
+@@ -52,7 +60,10 @@ def get_mla_metadata(
+                                  dtype torch.int32.
+         num_splits: (batch_size + 1), dtype torch.int32.
+     """
+-    return torch.ops._flashmla_C.get_mla_metadata(cache_seqlens,
++    # return torch.ops._flashmla_C.get_mla_metadata(cache_seqlens,
++    #                                               num_heads_per_head_k,
++    #                                               num_heads_k)
++    return flash_mla.flash_mla_interface.get_mla_metadata(cache_seqlens,
+                                                   num_heads_per_head_k,
+                                                   num_heads_k)
+ 
+@@ -86,19 +97,30 @@ def flash_mla_with_kvcache(
+         out: (batch_size, seq_len_q, num_heads_q, head_dim_v).
+         softmax_lse: (batch_size, num_heads_q, seq_len_q), torch.float32.
+     """
+-    if softmax_scale is None:
+-        softmax_scale = q.shape[-1]**(-0.5)
+-    out, softmax_lse = torch.ops._flashmla_C.fwd_kvcache_mla(
++    # if softmax_scale is None:
++    #     softmax_scale = q.shape[-1]**(-0.5)
++    # out, softmax_lse = torch.ops._flashmla_C.fwd_kvcache_mla(
++    #     q,
++    #     k_cache,
++    #     None,
++    #     head_dim_v,
++    #     cache_seqlens,
++    #     block_table,
++    #     softmax_scale,
++    #     causal,
++    #     tile_scheduler_metadata,
++    #     num_splits,
++    # )
++    out, softmax_lse = flash_mla.flash_mla_interface.flash_mla_with_kvcache(
+         q,
+         k_cache,
+-        None,
+-        head_dim_v,
+-        cache_seqlens,
+         block_table,
+-        softmax_scale,
+-        causal,
++        cache_seqlens,
++        head_dim_v,
+         tile_scheduler_metadata,
+         num_splits,
++        softmax_scale,
++        causal,
+     )
+     return out, softmax_lse
  
-     def ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int,
-                  src: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
--        self.NCCL_CHECK(self._funcs["ncclRecv"](recvbuff, count, datatype, src,
-+        self.NCCL_CHECK(self._funcs["mcclRecv"](recvbuff, count, datatype, src,
-                                                 comm, stream))
+diff --git a/vllm/attention/ops/triton_decode_attention.py b/vllm/attention/ops/triton_decode_attention.py
+index c27b377ae..38bc59bbf 100644
+--- a/vllm/attention/ops/triton_decode_attention.py
++++ b/vllm/attention/ops/triton_decode_attention.py
+@@ -34,7 +34,7 @@ import logging
+ from vllm.platforms import current_platform
+ from vllm.triton_utils import tl, triton
  
-     def ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type,
-                       count: int, datatype: int, root: int, comm: ncclComm_t,
-                       stream: cudaStream_t) -> None:
--        self.NCCL_CHECK(self._funcs["ncclBroadcast"](sendbuff, recvbuff, count,
-+        self.NCCL_CHECK(self._funcs["mcclBroadcast"](sendbuff, recvbuff, count,
-                                                      datatype, root, comm,
-                                                      stream))
+-is_hip_ = current_platform.is_rocm()
++# is_hip_ = current_platform.is_rocm()
  
-     def ncclCommDestroy(self, comm: ncclComm_t) -> None:
--        self.NCCL_CHECK(self._funcs["ncclCommDestroy"](comm))
-+        self.NCCL_CHECK(self._funcs["mcclCommDestroy"](comm))
+ logger = logging.getLogger(__name__)
  
+@@ -178,7 +178,8 @@ def _decode_att_m_fwd(
+     page_size,
+     logit_cap,
+ ):
+-    BLOCK = 64 if not is_hip_ else 8
++    # BLOCK = 64 if not is_hip_ else 8
++    BLOCK = 8
  
- __all__ = [
-diff --git a/vllm/distributed/kv_transfer/kv_connector/base.py b/vllm/distributed/kv_transfer/kv_connector/base.py
-index 57c764b48..b10d934d1 100644
---- a/vllm/distributed/kv_transfer/kv_connector/base.py
-+++ b/vllm/distributed/kv_transfer/kv_connector/base.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """
- KVConnectorBase Class for Distributed KV Cache & Hidden State communication
-@@ -7,6 +8,7 @@ The class provides two primary abstract methods:
- 2. recv_kv_caches_and_hidden_states(): Recv KV caches and hidden states
- """
+     NUM_KV_SPLITS = num_kv_splits
+     Lk = k_buffer.shape[-1]
+@@ -191,7 +192,8 @@ def _decode_att_m_fwd(
  
-+import inspect
- from abc import ABC, abstractmethod
- from typing import TYPE_CHECKING, List, Tuple, Union
+     num_warps = 4
+     if kv_group_num != 1:
+-        num_warps = 1 if is_hip_ else 2
++        # num_warps = 1 if is_hip_ else 2
++        num_warps = 1
  
-@@ -15,6 +17,7 @@ import torch
- from vllm.sequence import IntermediateTensors
+     BLOCK_DMODEL = triton.next_power_of_2(Lk)
+     BLOCK_DV = triton.next_power_of_2(Lv)
+@@ -386,17 +388,18 @@ def _decode_grouped_att_m_fwd(
+     Req_to_tokens,
+     B_Seqlen,
+     num_kv_splits,
++    num_stages,
+     sm_scale,
+     page_size,
+     logit_cap,
+ ):
+-    BLOCK = 32
++    BLOCK = 16
+     Lk = k_buffer.shape[-1]
+     Lv = v_buffer.shape[-1]
  
- if TYPE_CHECKING:
-+    from vllm.attention import AttentionMetadata
-     from vllm.config import VllmConfig
-     from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
+     # [TODO] work around shmem limit on MI3xx
+-    if is_hip_ and Lk >= 576:
+-        BLOCK = 16
++    # if is_hip_ and Lk >= 576:
++    #     BLOCK = 16
  
-@@ -28,6 +31,55 @@ class KVConnectorBase(ABC):
-     2. recv_kv_caches_and_hidden_states(): Recv KV caches and hidden states
-     """
+     if Lk == 576:
+         BLOCK_DMODEL = 512
+@@ -420,17 +423,21 @@ def _decode_grouped_att_m_fwd(
+         NUM_KV_SPLITS,
+     )
  
-+    def __init_subclass__(cls, **kwargs):
-+        super().__init_subclass__(**kwargs)
-+
-+        mode_methods_dict = {
-+            "bulk_transfer": [
-+                "send_kv_caches_and_hidden_states",
-+                "recv_kv_caches_and_hidden_states",
-+            ],
-+            "layerwise_transfer": [
-+                "send_one_layer_kv_cache",
-+                "send_hidden_states",
-+                "recv_kv_caches_and_hidden_states",
-+            ],
-+        }
-+
-+        supported_groups = [
-+            group_name for group_name, methods in mode_methods_dict.items()
-+            if all(method in cls.__dict__ for method in methods)
-+        ]
-+
-+        if not supported_groups:
-+            error_msg = (
-+                f"{cls.__name__} must implement at least one of the groups:\n"
-+                + "\n".join(
-+                    f"- {group_name}: {', '.join(methods)}"
-+                    for group_name, methods in mode_methods_dict.items()))
-+            raise TypeError(error_msg)
-+
-+        # Validate method signatures for all methods in supported groups
-+        signature_errors = []
-+        for group_name in supported_groups:
-+            for method in mode_methods_dict[group_name]:
-+                # Get base class method and subclass method
-+                base_method = getattr(__class__, method)
-+                subclass_method = getattr(cls, method)
-+
-+                # Compare signatures
-+                base_sig = inspect.signature(base_method)
-+                subclass_sig = inspect.signature(subclass_method)
-+                if base_sig != subclass_sig:
-+                    signature_errors.append(
-+                        f"Signature mismatch in group '{group_name}': "
-+                        f"Method '{method}' expects {base_sig}, "
-+                        f"got {subclass_sig}")
-+
-+        # Raise all signature errors at once
-+        if signature_errors:
-+            raise TypeError("\n".join(signature_errors))
-+
-     @abstractmethod
-     def __init__(
-         self,
-@@ -49,7 +101,6 @@ class KVConnectorBase(ABC):
-         """
-         raise NotImplementedError
+-    extra_kargs = {}
+-    num_stages = 2
+-    if is_hip_:
+-        # https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/workload.html#mi300x-triton-kernel-performance-optimization
+-        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
+-        extra_kargs = {
+-            "waves_per_eu": 1,
+-            "matrix_instr_nonkdim": 16,
+-            "kpack": 2
+-        }
+-        num_stages = 1
++    if num_stages == 1:
++        extra_kargs = {"scenario":"mla"}
++    elif num_stages == 2:
++        extra_kargs = {"scenario" : "mla", "pipeline" : "cpasync"}
++    else:
++        KeyError("num_stages should be 1 or 2") 
++    # if is_hip_:
++    #     # https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/workload.html#mi300x-triton-kernel-performance-optimization
++    #     # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
++    #     extra_kargs = {
++    #         "waves_per_eu": 1,
++    #         "matrix_instr_nonkdim": 16,
++    #         "kpack": 2
++    #     }
++    #     num_stages = 1
  
--    @abstractmethod
-     def send_kv_caches_and_hidden_states(
-         self,
-         model_executable: torch.nn.Module,
-@@ -83,11 +134,12 @@ class KVConnectorBase(ABC):
+     _fwd_grouped_kernel_stage1[grid](
+         q,
+@@ -540,14 +547,14 @@ def _decode_softmax_reducev_fwd(
+     NUM_KV_SPLITS = num_kv_splits
  
-         raise NotImplementedError
+     extra_kargs = {}
+-    if is_hip_:
+-        # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
+-        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
+-        extra_kargs = {
+-            "waves_per_eu": 4,
+-            "matrix_instr_nonkdim": 16,
+-            "kpack": 2
+-        }
++    # if is_hip_:
++    #     # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
++    #     # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
++    #     extra_kargs = {
++    #         "waves_per_eu": 4,
++    #         "matrix_instr_nonkdim": 16,
++    #         "kpack": 2
++    #     }
  
--    @abstractmethod
-     def recv_kv_caches_and_hidden_states(
--        self, model_executable: torch.nn.Module,
-+        self,
-+        model_executable: torch.nn.Module,
-         model_input: "ModelInputForGPUWithSamplingMetadata",
--        kv_caches: List[torch.Tensor]
-+        kv_caches: List[torch.Tensor],
-+        **kwargs,
-     ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
-                "ModelInputForGPUWithSamplingMetadata"]:
-         """
-@@ -111,7 +163,7 @@ class KVConnectorBase(ABC):
-             IntermediateTensors): 
-                 Concatenated hidden states if all required data is retrieved, 
-                 otherwise `None`.
--            - bypass_model_exec (bool): 
-+            - bypass_model_exec (List[bool]): 
-                 Indicates whether the model execution can be skipped (True) or 
-                 needs to be redone (False).
-             - model_input (ModelInputForGPUWithSamplingMetadata): 
-@@ -121,3 +173,49 @@ class KVConnectorBase(ABC):
-         """
+     grid = (batch, head_num)
+     _fwd_kernel_stage2[grid](
+@@ -606,6 +613,7 @@ def decode_attention_fwd_grouped(
+     b_seq_len,
+     attn_logits,
+     num_kv_splits,
++    num_stages,
+     sm_scale,
+     page_size,
+     logit_cap=0.0,
+@@ -618,6 +626,7 @@ def decode_attention_fwd_grouped(
+         req_to_token,
+         b_seq_len,
+         num_kv_splits,
++        num_stages,
+         sm_scale,
+         page_size,
+         logit_cap,
+@@ -635,6 +644,7 @@ def decode_attention_fwd(
+     b_seq_len,
+     attn_logits,
+     num_kv_splits,
++    num_stages,
+     sm_scale,
+     page_size=1,
+     logit_cap=0.0,
+@@ -668,6 +678,7 @@ def decode_attention_fwd(
+             b_seq_len,
+             attn_logits,
+             num_kv_splits,
++            num_stages,
+             sm_scale,
+             page_size,
+             logit_cap,
+diff --git a/vllm/compilation/cuda_piecewise_backend.py b/vllm/compilation/cuda_piecewise_backend.py
+index 993def49a..8c49ea6cc 100644
+--- a/vllm/compilation/cuda_piecewise_backend.py
++++ b/vllm/compilation/cuda_piecewise_backend.py
+@@ -14,6 +14,7 @@ from vllm.compilation.backends import VllmBackend
+ from vllm.compilation.counter import compilation_counter
+ from vllm.compilation.monitor import end_monitoring_torch_compile
+ from vllm.config import VllmConfig
++from vllm.forward_context import get_forward_context
+ from vllm.logger import init_logger
+ from vllm.utils import weak_ref_tensors
  
-         raise NotImplementedError
-+
-+    def send_one_layer_kv_cache(self, layer_id: int,
-+                                input_token_hash: List[str],
-+                                kv_cache: torch.Tensor,
-+                                attn_metadata: "AttentionMetadata",
-+                                block_size: int) -> None:
-+        """
-+        Sends the KV cache of a single layer to the connector.
-+        This method transmits the KV cache for a specific transformer layer,
-+        along with metadata, allowing the connector to store or utilize it
-+        for future requests. The transmission is layer-specific.
-+        Args:
-+            layer_id (int): 
-+                The id of the transformer layer being sent.
-+            input_token_hash (List[str]): 
-+                Hashes of the input tokens associated with this KV cache.
-+            kv_cache (torch.Tensor): 
-+                The KV cache tensor for the specified layer.
-+            attn_metadata (AttentionMetadata): 
-+                Attention metadata for the current batch.
-+            block_size (int): 
-+                The number of tokens in one page of KV cache.
-+        Returns:
-+            None: This method does not return a value.
-+        """
-+        raise NotImplementedError
-+
-+    def send_hidden_states(self, input_token_hash: List[str],
-+                           hidden_states: torch.Tensor,
-+                           attn_metadata: "AttentionMetadata") -> None:
-+        """
-+        Sends hidden states to the connector.
-+        Transmits computed hidden states along with attention metadata,
-+        enabling the connector to bypass the full model execution 
-+        using these cached states.
-+        Args:
-+            input_token_hash (List[str]): 
-+                Hash values of the input tokens.
-+            hidden_states (torch.Tensor): 
-+                The hidden states tensor computed by the model.
-+            attn_metadata (AttentionMetadata): 
-+                Attention metadata associated with these hidden states.
-+        Returns:
-+            None: This method does not return a value.
-+        """
-+        raise NotImplementedError
-\ No newline at end of file
-diff --git a/vllm/distributed/kv_transfer/kv_connector/factory.py b/vllm/distributed/kv_transfer/kv_connector/factory.py
-index fe4805334..5aeac29d8 100644
---- a/vllm/distributed/kv_transfer/kv_connector/factory.py
-+++ b/vllm/distributed/kv_transfer/kv_connector/factory.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -137,7 +138,10 @@ class CUDAPiecewiseBackend:
+             if self.is_last_graph and not self.to_be_compiled_sizes:
+                 self.check_for_ending_compilation()
  
- import importlib
-@@ -48,3 +49,8 @@ KVConnectorFactory.register_connector(
-     "MooncakeConnector",
-     "vllm.distributed.kv_transfer.kv_connector.simple_connector",
-     "SimpleConnector")
-+
-+KVConnectorFactory.register_connector(
-+    "LayerwisePyNcclConnector",
-+    "vllm.distributed.kv_transfer.kv_connector.layerwise_connector",
-+    "LayerwiseConnector")
-\ No newline at end of file
-diff --git a/vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py b/vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
-new file mode 100755
-index 000000000..4250db4b7
---- /dev/null
-+++ b/vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
-@@ -0,0 +1,269 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# SPDX-License-Identifier: Apache-2.0
-+"""
-+Simple KV Cache Connector for Distributed Machine Learning Inference
-+
-+LayerwiseConnector transfers KV caches between prefill vLLM worker (KV cache 
-+producer) and decode vLLM worker (KV cache consumer) using PyNcclPipe, 
-+layer bylayer.
-+
-+The logic can be extended to support other pipe and lookup buffer.
-+"""
-+from typing import TYPE_CHECKING, List, Optional, Tuple, Union
-+
-+import torch
-+
-+from vllm.config import VllmConfig
-+from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
-+from vllm.distributed.kv_transfer.kv_lookup_buffer.simple_dict_buffer import (
-+    SimpleDictBuffer)
-+from vllm.distributed.kv_transfer.kv_transfer_utils import (
-+    get_tensor_stable_hash)
-+from vllm.logger import init_logger
-+from vllm.sequence import IntermediateTensors
-+
-+if TYPE_CHECKING:
-+    from vllm.attention import AttentionMetadata
-+    from vllm.config import VllmConfig
-+    from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
-+
-+logger = init_logger(__name__)
-+
-+
-+class LayerwiseConnector(KVConnectorBase):
-+
-+    def __init__(
-+        self,
-+        rank: int,
-+        local_rank: int,
-+        config: VllmConfig,
-+    ):
-+
-+        self.config = config.kv_transfer_config
-+
-+        if self.config.kv_connector != "LayerwisePyNcclConnector":
-+            raise ValueError(
-+                "LayerwiseConnector only supports LayerwisePyNcclConnector")
-+
-+        from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import PyNcclPipe
-+        logger.info("Initializing PyNcclConfig under kv_transfer_config %s",
-+                    self.config)
-+
-+        self.lookup_buffer_size = self.config.kv_buffer_size
-+
-+        self.producer_buffer: Optional[SimpleDictBuffer] = None
-+        self.consumer_buffer: Optional[SimpleDictBuffer] = None
-+
-+        self.producer_data_pipe: PyNcclPipe
-+        self.consumer_data_pipe: PyNcclPipe
-+        self.producer_signal_pipe: PyNcclPipe
-+        self.consumer_signal_pipe: PyNcclPipe
-+
-+        # 2 pipes for every rank in the world
-+        port_offset_base = 2 * rank
-+
-+        # In disaggregated prefill, the prefill vLLM only uses send pipe
-+        # and the decode vLLM only uses recv pipe
-+        if self.config.is_kv_producer:
-+
-+            self.producer_data_pipe = PyNcclPipe(
-+                local_rank=local_rank,
-+                config=self.config,
-+                port_offset=port_offset_base,
-+            )
-+            self.producer_signal_pipe = PyNcclPipe(
-+                local_rank=local_rank,
-+                config=self.config,
-+                port_offset=port_offset_base + 1,
-+                device="cpu",
-+            )
-+
-+            self.producer_buffer = SimpleDictBuffer(self.producer_signal_pipe,
-+                                                    self.producer_data_pipe,
-+                                                    self.config.kv_buffer_size)
-+
-+        else:
-+
-+            # the current vLLM instance is KV consumer, so it needs to connect
-+            # its recv pipe to the send pipe of KV producder
-+
-+            self.consumer_data_pipe = PyNcclPipe(
-+                local_rank=local_rank,
-+                config=self.config,
-+                port_offset=port_offset_base,
-+            )
-+            self.consumer_signal_pipe = PyNcclPipe(
-+                local_rank=local_rank,
-+                config=self.config,
-+                port_offset=port_offset_base + 1,
-+                device="cpu",
-+            )
-+
-+            self.consumer_buffer = SimpleDictBuffer(
-+                self.consumer_signal_pipe,
-+                self.consumer_data_pipe,
-+                self.config.kv_buffer_size,
-+            )
-+
-+    def select(self, key: List[str]) -> Optional[List[torch.Tensor]]:
-+
-+        assert self.consumer_buffer is not None, "Please initialize the "\
-+            "consumer buffer before calling select."
-+        return self.consumer_buffer.drop_select(key)
-+
-+    def insert(self, key: str, value: torch.Tensor) -> None:
-+
-+        assert self.producer_buffer is not None, "Please initialize the "\
-+            "producer buffer before calling insert."
-+
-+        self.producer_buffer.insert(key, value)
-+
-+    def _get_kv_cache_key(self, input_tokens_hash: str, layer: int) -> str:
-+        return f"{input_tokens_hash}_layer_{layer}"
-+
-+    def _get_hs_cache_key(self, input_tokens_hash: str) -> str:
-+        return f"{input_tokens_hash}_hs"
-+
-+    def send_one_layer_kv_cache(self, layer_id: int,
-+                                input_token_hash: List[str],
-+                                kv_cache: torch.Tensor,
-+                                attn_metadata: "AttentionMetadata",
-+                                block_size: int) -> None:
-+        seq_lens = attn_metadata.seq_lens
-+        slot_mapping_flat = attn_metadata.slot_mapping.flatten()
-+
-+        assert len(input_token_hash) == len(seq_lens)
-+
-+        for idx, slen in enumerate(seq_lens):
-+            kv_cache_key = self._get_kv_cache_key(input_token_hash[idx],
-+                                                  layer_id)
-+
-+            start_pos = sum(seq_lens[:idx])
-+            end_pos = start_pos + slen
-+            current_slot_mapping = slot_mapping_flat[start_pos:end_pos]
-+
-+            page_index = current_slot_mapping[::block_size].div(
-+                block_size, rounding_mode='floor').long()
-+
-+            paged_kv_cache = kv_cache[:, page_index, ...]
-+
-+            self.insert(kv_cache_key, paged_kv_cache)
-+
-+    def send_hidden_states(self, input_token_hash: List[str],
-+                           hidden_states: torch.Tensor,
-+                           attn_metadata: "AttentionMetadata") -> None:
-+        seq_lens = attn_metadata.seq_lens
-+        assert len(input_token_hash) == len(seq_lens)
-+
-+        for idx, slen in enumerate(seq_lens):
-+            hs_cache_key = self._get_hs_cache_key(input_token_hash[idx])
-+
-+            start_pos = sum(seq_lens[:idx])
-+            end_pos = start_pos + slen
-+
-+            hs_cache = hidden_states[start_pos:end_pos]
-+
-+            self.insert(hs_cache_key, hs_cache)
-+
-+    def recv_kv_caches_and_hidden_states(
-+        self,
-+        model_executable: torch.nn.Module,
-+        model_input: "ModelInputForGPUWithSamplingMetadata",
-+        kv_caches: List[torch.Tensor],
-+        **kwargs,
-+    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
-+               "ModelInputForGPUWithSamplingMetadata"]:
-+
-+        assert 'block_size' in kwargs, "block_size is required in kwargs"
-+        block_size = kwargs['block_size']
-+
-+        # When bypass_model_exec is set to False, it means that at least for one
-+        # request its corresponding KV cache or hidden state is missing.
-+        # In this case we need to do prefilling to recompute missing KV cache
-+        # and hidden states.
-+        bypass_model_exec = True
-+
-+        input_tokens_tensor = model_input.input_tokens
-+        seq_lens = model_input.attn_metadata.seq_lens
-+        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
-+
-+        hidden_or_intermediate_states_for_one_req = []
-+
-+        try:
-+
-+            for idx, slen in enumerate(seq_lens):
-+
-+                start_pos = sum(seq_lens[:idx])
-+                end_pos = start_pos + slen
-+                current_tokens = input_tokens_tensor[start_pos:end_pos]
-+                current_tokens_hash = get_tensor_stable_hash(current_tokens)
-+                current_slot_mapping = slot_mapping[start_pos:end_pos]
-+                num_tokens = slen
-+
-+                page_index = current_slot_mapping[::block_size].div(
-+                    block_size, rounding_mode='floor').long()
-+
-+                keys = [
-+                    self._get_kv_cache_key(current_tokens_hash, i)
-+                    for i in range(model_executable.model.start_layer,
-+                                   model_executable.model.end_layer)
-+                ]
-+
-+                recv_kv_cache = self.select(keys)
-+
-+                assert model_executable.model.end_layer - \
-+                    model_executable.model.start_layer == len(recv_kv_cache)
-+
-+                for i in range(model_executable.model.start_layer,
-+                               model_executable.model.end_layer):
-+
-+                    kv_cache = kv_caches[i -
-+                                         model_executable.model.start_layer]
-+
-+                    kv_cache[:, page_index, ...] = recv_kv_cache[i]
-+
-+                hs_cache_key = self._get_hs_cache_key(current_tokens_hash)
-+                hidden_states = self.select([hs_cache_key])
-+
-+                hidden_or_intermediate_states_for_one_req.append(
-+                    hidden_states[0])
-+
-+                assert num_tokens == hidden_states[0].shape[0]
-+
-+        except Exception as e:
-+            import traceback
-+            traceback.print_stack()
-+            logger.error(
-+                "[rank%d]: Failed to receive all KVs and hidden states. "
-+                "Error: %s", torch.distributed.get_rank(), e)
-+            bypass_model_exec = False
-+
-+        if not bypass_model_exec:
-+            # Some of the KV cache is not retrieved
-+            # Here we will fall back to normal model forwarding
-+            # But optionally you can adjust model_input so that you only do
-+            # prefilling on those tokens that are missing KV caches.
-+            logger.debug(
-+                "[rank%d]: Failed to receive all KVs and hidden "
-+                "states, redo model forwarding.", torch.distributed.get_rank())
-+            hidden_or_intermediate_states = None
-+
-+        else:
-+            logger.debug(
-+                "[rank%d]: Successfully received all KVs and hidden "
-+                "states, skip model forwarding.", torch.distributed.get_rank())
-+            hidden_or_intermediate_states = torch.cat(
-+                hidden_or_intermediate_states_for_one_req, dim=0)
-+
-+        return hidden_or_intermediate_states, bypass_model_exec, model_input
-+
-+    def close(self):
-+        self.producer_data_pipe.close()
-+        self.consumer_data_pipe.close()
-+        if self.config.kv_connector == "PyNcclConnector":
-+            self.producer_signal_pipe.close()
-+            self.consumer_signal_pipe.close()
-+        elif self.config.kv_connector == "MooncakeConnector":
-+            # MooncakePipe reuses data_pipe for signal_pipe, so we only have to
-+            # close the data_pipe.
-+            pass
-\ No newline at end of file
-diff --git a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-index 2033e9762..8b5c83c17 100644
---- a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-+++ b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """
- Simple KV Cache Connector for Distributed Machine Learning Inference
-@@ -8,11 +9,14 @@ MooncakePipe.
+-        if not entry.use_cudagraph:
++        # Skip CUDA graphs if this entry doesn't use them OR
++        # if we're supposed to skip them globally
++        skip_cuda_graphs = get_forward_context().skip_cuda_graphs
++        if not entry.use_cudagraph or skip_cuda_graphs:
+             return entry.runnable(*args)
  
- But the logic can be extended to support other pipe and lookup buffer.
- """
-+from copy import deepcopy
- from typing import TYPE_CHECKING, List, Optional, Tuple, Union
-+from torch.nn.utils.rnn import pad_sequence
+         if entry.cudagraph is None:
+diff --git a/vllm/config.py b/vllm/config.py
+index 3fbb6015f..aa13f229e 100644
+--- a/vllm/config.py
++++ b/vllm/config.py
+@@ -331,7 +331,7 @@ class ModelConfig:
+     """Whether to disable sliding window. If True, we will disable the sliding
+     window functionality of the model, capping to sliding window size. If the
+     model does not support sliding window, this argument is ignored."""
+-    disable_cascade_attn: bool = False
++    disable_cascade_attn: bool = True
+     """Disable cascade attention for V1. While cascade attention does not
+     change the mathematical correctness, disabling it could be useful for
+     preventing potential numerical issues. Note that even if this is set to
+@@ -899,13 +899,13 @@ class ModelConfig:
+                     # Raise error if the override is not custom (custom would
+                     # be in QUANTIZATION_METHODS but not QuantizationMethods)
+                     # and hasn't been added to the overrides list.
+-                    if (name in get_args(QuantizationMethods)
+-                            and name not in overrides):
+-                        raise ValueError(
+-                            f"Quantization method {name} is an override but "
+-                            "is has not been added to the `overrides` list "
+-                            "above. This is necessary to ensure that the "
+-                            "overrides are checked in order of preference.")
++                    # if (name in get_args(QuantizationMethods)
++                    #         and name not in overrides):
++                    #     raise ValueError(
++                    #         f"Quantization method {name} is an override but "
++                    #         "is has not been added to the `overrides` list "
++                    #         "above. This is necessary to ensure that the "
++                    #         "overrides are checked in order of preference.")
+                     quant_method = quantization_override
+                     self.quantization = quantization_override
+                     break
+@@ -1429,10 +1429,17 @@ class ModelConfig:
+             spec_target_max_model_len=self.spec_target_max_model_len,
+             encoder_config=self.encoder_config)
+ 
+-        tokenizer_config = try_get_tokenizer_config(
+-            self.tokenizer,
+-            trust_remote_code=self.trust_remote_code,
+-            revision=self.tokenizer_revision)
++        # For pooling models, the tokenizer's `model_max_length` is often a
++        # reliable source for the maximum sequence length. However, for
++        # generative models, this can be incorrect and unduly limit the
++        # context window (e.g., DeepSeek-R1). Therefore, we only consider
++        # tokenizer_config for pooling models.
++        tokenizer_config = None
++        if self.runner_type == "pooling":
++            tokenizer_config = try_get_tokenizer_config(
++                self.tokenizer,
++                trust_remote_code=self.trust_remote_code,
++                revision=self.tokenizer_revision)
+ 
+         if tokenizer_config is None:
+             return max_model_len
+@@ -1765,7 +1772,7 @@ class ParallelConfig:
+     sequentially in multiple batches. To avoid RAM OOM when using tensor
+     parallel and large models."""
+ 
+-    disable_custom_all_reduce: bool = False
++    disable_custom_all_reduce: bool = True
+     """Disable the custom all-reduce kernel and fall back to NCCL."""
+ 
+     tokenizer_pool_config: Optional[TokenizerPoolConfig] = None
+@@ -2040,8 +2047,8 @@ class SchedulerConfig:
+     NOTE: This will be replaced by speculative config in the future; it is
+     present to enable correctness tests until then."""
+ 
+-    cuda_graph_sizes: list[int] = field(default_factory=lambda: [512])
+-    """Cuda graph capture sizes, default is 512.
++    cuda_graph_sizes: list[int] = field(default_factory=lambda: [256])
++    """Cuda graph capture sizes, default is 256.
+     1. if one value is provided, then the capture list would follow the
+     pattern: [1, 2, 4] + [i for i in range(8, cuda_graph_sizes + 1, 8)]
+     2. more than one value (e.g. 1 2 128) is provided, then the capture list
+@@ -4256,6 +4263,7 @@ class VllmConfig:
+         from vllm import __version__
+         vllm_factors.append(__version__)
+         vllm_factors.append(envs.VLLM_USE_V1)
++        vllm_factors.append(envs.MACA_VLLM_USE_TN_2_NN)
+         if self.model_config:
+             vllm_factors.append(self.model_config.compute_hash())
+         else:
+@@ -4353,12 +4361,14 @@ class VllmConfig:
  
- import torch
+             if capability_tuple is not None:
+                 capability = capability_tuple.to_int()
++                """
+                 if capability < quant_config.get_min_capability():
+                     raise ValueError(
+                         f"The quantization method {model_config.quantization} "
+                         "is not supported for the current GPU. Minimum "
+                         f"capability: {quant_config.get_min_capability()}. "
+                         f"Current capability: {capability}.")
++                """
+             supported_dtypes = quant_config.get_supported_act_dtypes()
+             if model_config.dtype not in supported_dtypes:
+                 raise ValueError(
+diff --git a/vllm/distributed/device_communicators/cuda_wrapper.py b/vllm/distributed/device_communicators/cuda_wrapper.py
+index 2c38e8ed2..0052ba082 100644
+--- a/vllm/distributed/device_communicators/cuda_wrapper.py
++++ b/vllm/distributed/device_communicators/cuda_wrapper.py
+@@ -65,33 +65,33 @@ def find_loaded_library(lib_name) -> Optional[str]:
+ class CudaRTLibrary:
+     exported_functions = [
+         # cudaError_t cudaSetDevice ( int  device )
+-        Function("cudaSetDevice", cudaError_t, [ctypes.c_int]),
++        Function("mcSetDevice", cudaError_t, [ctypes.c_int]),
+         # cudaError_t 	cudaDeviceSynchronize ( void )
+-        Function("cudaDeviceSynchronize", cudaError_t, []),
++        Function("mcDeviceSynchronize", cudaError_t, []),
+         # cudaError_t cudaDeviceReset ( void )
+-        Function("cudaDeviceReset", cudaError_t, []),
++        Function("mcDeviceReset", cudaError_t, []),
+ 
+         # const char* 	cudaGetErrorString ( cudaError_t error )
+-        Function("cudaGetErrorString", ctypes.c_char_p, [cudaError_t]),
++        Function("mcGetErrorString", ctypes.c_char_p, [cudaError_t]),
+ 
+         # cudaError_t 	cudaMalloc ( void** devPtr, size_t size )
+-        Function("cudaMalloc", cudaError_t,
++        Function("mcMalloc", cudaError_t,
+                  [ctypes.POINTER(ctypes.c_void_p), ctypes.c_size_t]),
+         # cudaError_t 	cudaFree ( void* devPtr )
+-        Function("cudaFree", cudaError_t, [ctypes.c_void_p]),
++        Function("mcFree", cudaError_t, [ctypes.c_void_p]),
+         # cudaError_t cudaMemset ( void* devPtr, int  value, size_t count )
+-        Function("cudaMemset", cudaError_t,
++        Function("mcMemset", cudaError_t,
+                  [ctypes.c_void_p, ctypes.c_int, ctypes.c_size_t]),
+         # cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind ) # noqa
+-        Function("cudaMemcpy", cudaError_t, [
++        Function("mcMemcpy", cudaError_t, [
+             ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t, cudaMemcpyKind
+         ]),
  
- from vllm import _custom_ops as ops
-+from vllm.attention.backends.flash_attn import FlashAttentionMetadata
- from vllm.config import VllmConfig
- from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
- from vllm.distributed.kv_transfer.kv_lookup_buffer.simple_buffer import (
-@@ -37,6 +41,8 @@ class SimpleConnector(KVConnectorBase):
- 
-         self.config = config.kv_transfer_config
-         self.tp_size = config.parallel_config.tensor_parallel_size
-+        # The following config is needed to rebuild the model input
-+        self.cache_config = config.cache_config
- 
-         if self.config.kv_connector == "PyNcclConnector":
-             from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import (
-@@ -148,12 +154,10 @@ class SimpleConnector(KVConnectorBase):
-         self.producer_buffer.insert(input_tokens, roi, key, value, hidden)
- 
-     def send_kv_caches_and_hidden_states(
--        self,
--        model_executable: torch.nn.Module,
-+        self, model_executable: torch.nn.Module,
-         model_input: "ModelInputForGPUWithSamplingMetadata",
-         kv_caches: List[torch.Tensor],
--        hidden_or_intermediate_states: Union[torch.Tensor,
--                                             IntermediateTensors],
-+        hidden_or_intermediate_states: Union[torch.Tensor, IntermediateTensors]
-     ) -> None:
+         # cudaError_t cudaIpcGetMemHandle ( cudaIpcMemHandle_t* handle, void* devPtr ) # noqa
+-        Function("cudaIpcGetMemHandle", cudaError_t,
++        Function("mcIpcGetMemHandle", cudaError_t,
+                  [ctypes.POINTER(cudaIpcMemHandle_t), ctypes.c_void_p]),
+         # cudaError_t cudaIpcOpenMemHandle ( void** devPtr, cudaIpcMemHandle_t handle, unsigned int  flags ) # noqa
+-        Function("cudaIpcOpenMemHandle", cudaError_t, [
++        Function("mcIpcOpenMemHandle", cudaError_t, [
+             ctypes.POINTER(ctypes.c_void_p), cudaIpcMemHandle_t, ctypes.c_uint
+         ]),
+     ]
+@@ -106,7 +106,7 @@ class CudaRTLibrary:
+ 
+     def __init__(self, so_file: Optional[str] = None):
+         if so_file is None:
+-            so_file = find_loaded_library("libcudart")
++            so_file = find_loaded_library("libmcruntime")
+             if so_file is None:
+                 so_file = envs.VLLM_CUDART_SO_PATH  # fallback to env var
+             assert so_file is not None, \
+@@ -135,39 +135,39 @@ class CudaRTLibrary:
+             raise RuntimeError(f"CUDART error: {error_str}")
+ 
+     def cudaGetErrorString(self, error: cudaError_t) -> str:
+-        return self.funcs["cudaGetErrorString"](error).decode("utf-8")
++        return self.funcs["mcGetErrorString"](error).decode("utf-8")
+ 
+     def cudaSetDevice(self, device: int) -> None:
+-        self.CUDART_CHECK(self.funcs["cudaSetDevice"](device))
++        self.CUDART_CHECK(self.funcs["mcSetDevice"](device))
+ 
+     def cudaDeviceSynchronize(self) -> None:
+-        self.CUDART_CHECK(self.funcs["cudaDeviceSynchronize"]())
++        self.CUDART_CHECK(self.funcs["mcDeviceSynchronize"]())
+ 
+     def cudaDeviceReset(self) -> None:
+-        self.CUDART_CHECK(self.funcs["cudaDeviceReset"]())
++        self.CUDART_CHECK(self.funcs["mcDeviceReset"]())
+ 
+     def cudaMalloc(self, size: int) -> ctypes.c_void_p:
+         devPtr = ctypes.c_void_p()
+-        self.CUDART_CHECK(self.funcs["cudaMalloc"](ctypes.byref(devPtr), size))
++        self.CUDART_CHECK(self.funcs["mcMalloc"](ctypes.byref(devPtr), size))
+         return devPtr
+ 
+     def cudaFree(self, devPtr: ctypes.c_void_p) -> None:
+-        self.CUDART_CHECK(self.funcs["cudaFree"](devPtr))
++        self.CUDART_CHECK(self.funcs["mcFree"](devPtr))
+ 
+     def cudaMemset(self, devPtr: ctypes.c_void_p, value: int,
+                    count: int) -> None:
+-        self.CUDART_CHECK(self.funcs["cudaMemset"](devPtr, value, count))
++        self.CUDART_CHECK(self.funcs["mcMemset"](devPtr, value, count))
+ 
+     def cudaMemcpy(self, dst: ctypes.c_void_p, src: ctypes.c_void_p,
+                    count: int) -> None:
+         cudaMemcpyDefault = 4
+         kind = cudaMemcpyDefault
+-        self.CUDART_CHECK(self.funcs["cudaMemcpy"](dst, src, count, kind))
++        self.CUDART_CHECK(self.funcs["mcMemcpy"](dst, src, count, kind))
+ 
+     def cudaIpcGetMemHandle(self,
+                             devPtr: ctypes.c_void_p) -> cudaIpcMemHandle_t:
+         handle = cudaIpcMemHandle_t()
+-        self.CUDART_CHECK(self.funcs["cudaIpcGetMemHandle"](
++        self.CUDART_CHECK(self.funcs["mcIpcGetMemHandle"](
+             ctypes.byref(handle), devPtr))
+         return handle
+ 
+@@ -175,6 +175,6 @@ class CudaRTLibrary:
+                              handle: cudaIpcMemHandle_t) -> ctypes.c_void_p:
+         cudaIpcMemLazyEnablePeerAccess = 1
+         devPtr = ctypes.c_void_p()
+-        self.CUDART_CHECK(self.funcs["cudaIpcOpenMemHandle"](
++        self.CUDART_CHECK(self.funcs["mcIpcOpenMemHandle"](
+             ctypes.byref(devPtr), handle, cudaIpcMemLazyEnablePeerAccess))
+         return devPtr
+diff --git a/vllm/distributed/device_communicators/pynccl_wrapper.py b/vllm/distributed/device_communicators/pynccl_wrapper.py
+index 04a4d0147..718d7ea82 100644
+--- a/vllm/distributed/device_communicators/pynccl_wrapper.py
++++ b/vllm/distributed/device_communicators/pynccl_wrapper.py
+@@ -128,18 +128,18 @@ class Function:
+ class NCCLLibrary:
+     exported_functions = [
+         # const char* ncclGetErrorString(ncclResult_t result)
+-        Function("ncclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
++        Function("mcclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
+         # ncclResult_t  ncclGetVersion(int *version);
+-        Function("ncclGetVersion", ncclResult_t,
++        Function("mcclGetVersion", ncclResult_t,
+                  [ctypes.POINTER(ctypes.c_int)]),
+         # ncclResult_t ncclGetUniqueId(ncclUniqueId* uniqueId);
+-        Function("ncclGetUniqueId", ncclResult_t,
++        Function("mcclGetUniqueId", ncclResult_t,
+                  [ctypes.POINTER(ncclUniqueId)]),
+         # ncclResult_t  ncclCommInitRank(
+         #   ncclComm_t* comm, int nranks, ncclUniqueId commId, int rank);
+         # note that ncclComm_t is a pointer type, so the first argument
+         # is a pointer to a pointer
+-        Function("ncclCommInitRank", ncclResult_t, [
++        Function("mcclCommInitRank", ncclResult_t, [
+             ctypes.POINTER(ncclComm_t), ctypes.c_int, ncclUniqueId,
+             ctypes.c_int
+         ]),
+@@ -149,7 +149,7 @@ class NCCLLibrary:
+         #   cudaStream_t stream);
+         # note that cudaStream_t is a pointer type, so the last argument
+         # is a pointer
+-        Function("ncclAllReduce", ncclResult_t, [
++        Function("mcclAllReduce", ncclResult_t, [
+             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
+             ncclRedOp_t, ncclComm_t, cudaStream_t
+         ]),
+@@ -160,7 +160,7 @@ class NCCLLibrary:
+         #   cudaStream_t stream);
+         # note that cudaStream_t is a pointer type, so the last argument
+         # is a pointer
+-        Function("ncclAllGather", ncclResult_t, [
++        Function("mcclAllGather", ncclResult_t, [
+             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
+             ncclComm_t, cudaStream_t
+         ]),
+@@ -171,7 +171,7 @@ class NCCLLibrary:
+         #   cudaStream_t stream);
+         # note that cudaStream_t is a pointer type, so the last argument
+         # is a pointer
+-        Function("ncclReduceScatter", ncclResult_t, [
++        Function("mcclReduceScatter", ncclResult_t, [
+             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
+             ncclRedOp_t, ncclComm_t, cudaStream_t
+         ]),
+@@ -179,7 +179,7 @@ class NCCLLibrary:
+         # ncclResult_t  ncclSend(
+         #   const void* sendbuff, size_t count, ncclDataType_t datatype,
+         #   int dest, ncclComm_t comm, cudaStream_t stream);
+-        Function("ncclSend", ncclResult_t, [
++        Function("mcclSend", ncclResult_t, [
+             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
+             ncclComm_t, cudaStream_t
+         ]),
+@@ -187,7 +187,7 @@ class NCCLLibrary:
+         # ncclResult_t  ncclRecv(
+         #   void* recvbuff, size_t count, ncclDataType_t datatype,
+         #   int src, ncclComm_t comm, cudaStream_t stream);
+-        Function("ncclRecv", ncclResult_t, [
++        Function("mcclRecv", ncclResult_t, [
+             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
+             ncclComm_t, cudaStream_t
+         ]),
+@@ -196,7 +196,7 @@ class NCCLLibrary:
+         #   const void* sendbuff, void* recvbuff, size_t count,
+         #   ncclDataType_t datatype, int root, ncclComm_t comm,
+         #   cudaStream_t stream);
+-        Function("ncclBroadcast", ncclResult_t, [
++        Function("mcclBroadcast", ncclResult_t, [
+             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
+             ctypes.c_int, ncclComm_t, cudaStream_t
+         ]),
+@@ -206,7 +206,7 @@ class NCCLLibrary:
+         # because Python object destruction can happen in random order,
+         # it is better not to call it at all.
+         # ncclResult_t  ncclCommDestroy(ncclComm_t comm);
+-        Function("ncclCommDestroy", ncclResult_t, [ncclComm_t]),
++        Function("mcclCommDestroy", ncclResult_t, [ncclComm_t]),
+     ]
  
-         input_tokens_tensor = model_input.input_tokens
-@@ -200,18 +204,14 @@ class SimpleConnector(KVConnectorBase):
-         logger.debug("[rank%d]: KV send DONE.", torch.distributed.get_rank())
+     # class attribute to store the mapping from the path to the library
+@@ -249,16 +249,16 @@ class NCCLLibrary:
+         self._funcs = NCCLLibrary.path_to_dict_mapping[so_file]
  
-     def recv_kv_caches_and_hidden_states(
--        self, model_executable: torch.nn.Module,
-+        self,
-+        model_executable: torch.nn.Module,
-         model_input: "ModelInputForGPUWithSamplingMetadata",
--        kv_caches: List[torch.Tensor]
-+        kv_caches: List[torch.Tensor],
-+        **kwargs,
-     ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
-                "ModelInputForGPUWithSamplingMetadata"]:
- 
--        # When bypass_model_exec is set to False, it means that at least for one
--        # request its corresponding KV cache or hidden state is missing.
--        # In this case we need to do prefilling to recompute missing KV cache
--        # and hidden states.
--        bypass_model_exec = True
--
-         input_tokens_tensor = model_input.input_tokens
-         seq_lens = model_input.attn_metadata.seq_lens
-         slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
-@@ -222,6 +222,12 @@ class SimpleConnector(KVConnectorBase):
-         num_computed_tokens_list = []
-         start_pos_list = []
- 
-+        # When bypass_model_exec[i] is set to False, it means that for
-+        # request[i] its corresponding KV cache or hidden state is missing.
-+        # In this case we need to do prefilling to recompute missing KV cache
-+        # and hidden states of request[i].
-+        bypass_model_exec = [True] * len(seq_lens)
-+
-         # enumerate different requests
-         # FIXME(Kuntai): This impl assumes that all requests are prefill.
-         for idx, slen in enumerate(seq_lens):
-@@ -239,7 +245,7 @@ class SimpleConnector(KVConnectorBase):
-                               torch.ones_like(current_tokens, dtype=bool))
-             if ret[0] is None:
-                 # didn't find any match.
--                bypass_model_exec = False
-+                bypass_model_exec[idx] = False
-                 num_computed_tokens_list.append(0)
-                 continue
- 
-@@ -255,11 +261,17 @@ class SimpleConnector(KVConnectorBase):
-             # If not, need to redo the forwarding to compute missing states
-             if not all([(num_computed_tokens == num_tokens), hidden is not None
-                         ]):
--                bypass_model_exec = False
-+                bypass_model_exec[idx] = False
-+                continue
+     def ncclGetErrorString(self, result: ncclResult_t) -> str:
+-        return self._funcs["ncclGetErrorString"](result).decode("utf-8")
++        return self._funcs["mcclGetErrorString"](result).decode("utf-8")
  
-             # update the end position based on how many tokens are cached.
-             end_pos = start_pos + num_computed_tokens
- 
-+            # Avoid error when prefix is exactly the same as the retrieved
-+            if num_computed_tokens == num_tokens:
-+                num_computed_tokens -= 1
-+            num_computed_tokens_list.append(num_computed_tokens)
-+
-             # put received KV caches into paged memory
-             for i in range(model_executable.model.start_layer,
-                            model_executable.model.end_layer):
-@@ -283,23 +295,35 @@ class SimpleConnector(KVConnectorBase):
- 
-             hidden_or_intermediate_states_for_one_req.append(hidden)
- 
--        if not bypass_model_exec:
--            # Some of the KV cache is not retrieved
--            # Here we will fall back to normal model forwarding
--            # But optionally you can adjust model_input so that you only do
--            # prefilling on those tokens that are missing KV caches.
--            logger.debug(
--                "[rank%d]: Failed to receive all KVs and hidden "
--                "states, redo model forwarding.", torch.distributed.get_rank())
--            hidden_or_intermediate_states = None
--
--        else:
-+        all_bypass_flag = True
-+        for idx, bypass_flag in enumerate(bypass_model_exec):
-+            if not bypass_flag:
-+                # Some of the KV cache of this request is not retrieved
-+                # Here we will fall back to normal model forwarding
-+                logger.error(
-+                    "[rank%d]: Failed to receive request %d's"
-+                    " KVs and hidden states, "
-+                    "redo model forwarding.", torch.distributed.get_rank(),
-+                    idx)
-+
-+                hidden_or_intermediate_states = torch.cat(
-+                    hidden_or_intermediate_states_for_one_req, dim=0)
-+                all_bypass_flag = False
-+        if all_bypass_flag:
-             logger.debug(
-                 "[rank%d]: Successfully received all KVs and hidden "
-                 "states, skip model forwarding.", torch.distributed.get_rank())
-             hidden_or_intermediate_states = torch.cat(
-                 hidden_or_intermediate_states_for_one_req, dim=0)
- 
-+        if not all(bypass_model_exec):
-+            rebuilt_model_input = self.build_partial_prefill_input(
-+                model_input, input_tokens_list, num_computed_tokens_list,
-+                start_pos_list, slot_mapping, kv_caches[0][0].device)
-+            logger.error("Rebuilt the input!")
-+            return (hidden_or_intermediate_states, bypass_model_exec,
-+                    rebuilt_model_input)
-+
-         return hidden_or_intermediate_states, bypass_model_exec, model_input
- 
-     def close(self):
-@@ -312,3 +336,115 @@ class SimpleConnector(KVConnectorBase):
-             # MooncakePipe reuses data_pipe for signal_pipe, so we only have to
-             # close the data_pipe.
-             pass
-+
-+    def build_partial_prefill_input(
-+            self, model_input: "ModelInputForGPUWithSamplingMetadata",
-+            full_tokens_list: List[torch.Tensor],
-+            num_computed_tokens_list: List[int], start_pos_list: List[int],
-+            slot_mapping_flat: torch.Tensor,
-+            device: torch.device) -> "ModelInputForGPUWithSamplingMetadata":
-+        """Helper function to rebuild the model input for the current request.
-+        """
-+        assert model_input.attn_metadata is not None
-+        assert isinstance(model_input.attn_metadata, FlashAttentionMetadata), \
-+            "Only FlashAttention backend is supported for now."
-+        assert model_input.attn_metadata.context_lens_tensor is not None
-+        assert model_input.attn_metadata.block_tables is not None
-+        assert model_input.attn_metadata.query_start_loc is not None
-+        assert model_input.input_positions is not None
-+
-+        rebuilt_input_tokens = []
-+        rebuilt_input_positions = []
-+        rebuilt_num_prefills = 0
-+        rebuilt_num_prefill_tokens = 0
-+        rebuilt_slot_mapping = []
-+        rebuilt_max_query_len = 0
-+
-+        rebuilt_block_tables = []
-+
-+        rebuilt_query_start_loc = [0]
-+        rebuilt_context_lens_tensor = []
-+
-+        last_query_start_loc = 0
-+
-+        # recounting query and context lengths
-+        for idx in range(len(full_tokens_list)):
-+            token_tensor = full_tokens_list[idx]
-+            num_token = len(token_tensor)
-+            num_computed_token = num_computed_tokens_list[idx]
-+            start_pos = start_pos_list[idx]
-+            q_len = num_token - num_computed_token
-+
-+            rebuilt_input_tokens.append(token_tensor[num_computed_token:])
-+
-+            assert q_len > 0
-+            start_input_pos_idx = start_pos + num_computed_token
-+            end_input_pos_idx = start_input_pos_idx + q_len
-+            rebuilt_input_positions.append(
-+                model_input.
-+                input_positions[start_input_pos_idx:end_input_pos_idx])
-+
-+            # Attn metadata-related
-+            rebuilt_num_prefills += 1
-+            rebuilt_num_prefill_tokens += q_len
-+            start_slot_idx = start_pos + num_computed_token
-+            end_slot_idx = start_slot_idx + q_len
-+            new_slot_mapping = slot_mapping_flat[start_slot_idx:end_slot_idx]
-+            rebuilt_slot_mapping.append(new_slot_mapping)
-+            rebuilt_max_query_len = max(q_len, rebuilt_max_query_len)
-+            last_query_start_loc += q_len
-+            rebuilt_query_start_loc.append(last_query_start_loc)
-+            rebuilt_context_lens_tensor.append(num_computed_token)
-+
-+            # recover `block_table`
-+            if len(model_input.attn_metadata.block_tables[idx]) > 0:
-+                rebuilt_block_tables.append(
-+                    model_input.attn_metadata.block_tables[idx])
-+            else:
-+                slot_mapping_req = slot_mapping_flat[start_pos:end_slot_idx]
-+                vllm_block_size = self.cache_config.block_size
-+                rebuilt_block_table = slot_mapping_req[::16].to(torch.int32) \
-+                    // vllm_block_size
-+                rebuilt_block_tables.append(rebuilt_block_table)
-+
-+        # rebuilt attn_metadata
-+        rebuilt_attn_metadata = deepcopy(model_input.attn_metadata)
-+        rebuilt_attn_metadata.num_prefills = rebuilt_num_prefills
-+        rebuilt_attn_metadata.num_prefill_tokens = rebuilt_num_prefill_tokens
-+        rebuilt_attn_metadata.slot_mapping = torch.cat(
-+            rebuilt_slot_mapping).to(device)
-+        rebuilt_attn_metadata.max_query_len = rebuilt_max_query_len
-+        rebuilt_attn_metadata.block_tables = pad_sequence(
-+            rebuilt_block_tables, batch_first=True).to(device)
-+        rebuilt_attn_metadata.query_start_loc = torch.tensor(
-+            rebuilt_query_start_loc,
-+            dtype=model_input.attn_metadata.query_start_loc.dtype).to(device)
-+        rebuilt_attn_metadata.context_lens_tensor = torch.tensor(
-+            rebuilt_context_lens_tensor,
-+            dtype=model_input.attn_metadata.context_lens_tensor.dtype,
-+        ).to(device)
-+        rebuilt_attn_metadata._cached_prefill_metadata = None
-+
-+        # import here to avoid circular import.
-+        from vllm.worker.model_runner import (
-+            ModelInputForGPUWithSamplingMetadata)
-+        rebuilt_model_input = ModelInputForGPUWithSamplingMetadata(
-+            input_tokens=torch.cat(rebuilt_input_tokens).to(device),
-+            input_positions=torch.cat(rebuilt_input_positions).to(device),
-+            seq_lens=model_input.seq_lens,
-+            query_lens=model_input.query_lens,
-+            lora_mapping=model_input.lora_mapping,
-+            lora_requests=model_input.lora_requests,
-+            attn_metadata=rebuilt_attn_metadata,
-+            prompt_adapter_mapping=model_input.prompt_adapter_mapping,
-+            prompt_adapter_requests=model_input.prompt_adapter_requests,
-+            multi_modal_kwargs=model_input.multi_modal_kwargs,
-+            request_ids_to_seq_ids=model_input.request_ids_to_seq_ids,
-+            finished_requests_ids=model_input.finished_requests_ids,
-+            virtual_engine=model_input.virtual_engine,
-+            sampling_metadata=model_input.sampling_metadata,
-+            is_prompt=model_input.is_prompt,
-+            async_callback=model_input.async_callback,
-+        )
-+
-+        return rebuilt_model_input
-\ No newline at end of file
-diff --git a/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py b/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
-new file mode 100755
-index 000000000..8d732678d
---- /dev/null
-+++ b/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
-@@ -0,0 +1,151 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# SPDX-License-Identifier: Apache-2.0
-+"""
-+    Implements a distributed key-value (KV) cache transfer mechanism.
-+    Key Features:
-+    - Distributed KV cache transmission using PyNccl pipes.
-+    - Non-blocking `insert`, blocking `drop_select`.
-+    - Use CPU signal pipe to avoid racing condition
-+    - Handles buffer size constraints and provide backpressure mechanism to 
-+      stop the prefill instance when the decode instance is slow.
-+"""
-+import threading
-+from typing import Dict, List, Optional
-+
-+import torch
-+
-+from vllm.distributed.kv_transfer.kv_lookup_buffer.base import (
-+    KVLookupBufferBase)
-+from vllm.distributed.kv_transfer.kv_pipe.base import KVPipeBase
-+from vllm.logger import init_logger
-+
-+logger = init_logger(__name__)
-+
-+
-+def _string_to_byte_tensor(s: str) -> torch.Tensor:
-+    byte_data = s.encode('utf-8')
-+    byte_list = list(byte_data)
-+    byte_tensor = torch.tensor(byte_list, dtype=torch.uint8, device='cpu')
-+    return byte_tensor
-+
-+
-+def _byte_tensor_to_string(t: torch.Tensor) -> str:
-+    byte_list = t.tolist()
-+    byte_data = bytes(byte_list)
-+    return byte_data.decode('utf-8')
-+
-+
-+class SimpleDictBuffer(KVLookupBufferBase):
-+
-+    def __init__(self, signal_pipe: KVPipeBase, data_pipe: KVPipeBase,
-+                 buffer_size_thresh: float):
-+        """
-+        signal_pipe: on CPU 
-+        
-+        NOTE: on-device recv will block all threads in the process, making the 
-+        KV cache producer unable to listen to new request while transmitting 
-+        KV cache. Luckily CPU recv only blocks the current thread so we use 
-+        CPU recv to listen to new request.
-+        
-+        data_pipe: on device (e.g. GPU)
-+        """
-+
-+        self.buffer: Dict[str, List[torch.Tensor]] = {}
-+
-+        self.buffer_size = 0
-+        self.buffer_size_threshold = buffer_size_thresh
-+        self.buffer_cv = threading.Condition()
-+        self.signal_pipe = signal_pipe
-+        self.data_pipe = data_pipe
-+        self.request_handling_thread: Optional[threading.Thread] = None
-+
-+        self.normal_signal = torch.tensor([0], device="cpu")
-+        self.end_signal = None
-+
-+    def _is_end_signal(self, signal):
-+        return signal is None
-+
-+    def drop_select_handler(self):
-+
-+        try:
-+
-+            while True:
-+                signal = self.signal_pipe.recv_tensor()
-+                if self._is_end_signal(signal):
-+                    logger.info("Received end signal!")
-+                    break
-+
-+                key_bytes = self.data_pipe.recv_tensor()
-+                key_hashes = _byte_tensor_to_string(key_bytes).split(";")
-+
-+                with self.buffer_cv:
-+                    for key_hash in key_hashes:
-+                        if key_hash not in self.buffer:
-+                            self.data_pipe.send_tensor(None)
-+                            raise RuntimeError(
-+                                f"Key {key_hash} not found in buffer")
-+
-+                    for key_hash in key_hashes:
-+                        self.data_pipe.send_tensor(self.buffer[key_hash])
-+                        self.buffer_cv.notify()
-+                        self.buffer_size -= self.buffer[key_hash].element_size() * self.buffer[key_hash].numel()
-+                        del self.buffer[key_hash]
-+
-+        except RuntimeError as e:
-+            if 'Connection closed by peer' not in str(e):
-+                raise e
-+
-+        logger.debug("Closing drop_select_handler")
-+
-+    def drop_select(self, keys: List[str]) -> List[Optional[torch.Tensor]]:
-+
-+        assert self.request_handling_thread is None, \
-+            "drop_select should be called by the KV cache consumer "\
-+            "(e.g. the decode vLLM instance)"
-+
-+        self.signal_pipe.send_tensor(self.normal_signal)
-+
-+        self.data_pipe.send_tensor(_string_to_byte_tensor(";".join(keys)))
-+
-+        value = []
-+        for _ in range(len(keys)):
-+            value.append(self.data_pipe.recv_tensor())
-+
-+        return value
-+
-+    def insert(self, key: str, value: List[torch.Tensor]) -> None:
-+
-+        if key in self.buffer:
-+            return
-+
-+        tensor_size = sum(t.element_size() * t.numel() for t in value)
-+
-+        with self.buffer_cv:
-+            if self.buffer_size + tensor_size > self.buffer_size_threshold:
-+                # log outside the while loop to avoid this message being logged
-+                # repeatedly.
-+                logger.error("KV transfer buffer is full. Handling...")
-+                while self.buffer_size + tensor_size > self.buffer_size_threshold:
-+                    self.buffer_cv.wait()
-+
-+            self.buffer_size += tensor_size
-+            self.buffer[key] = value
-+            self.buffer_cv.notify()
-+
-+        # when calling the insert, the current process is a sender
-+        # need to launch the request handler and start listening to request.
-+        if self.request_handling_thread is None:
-+            self.request_handling_thread = threading.Thread(
-+                target=self.drop_select_handler)
-+            self.request_handling_thread.start()
-+
-+    def close(self):
-+
-+        if hasattr(self, "request_handling_thread"
-+                   ) and self.request_handling_thread is not None:
-+            self.request_handling_thread.join()
-+
-+        else:
-+            # TODO: have a explicit close signal and have a explicit way to
-+            # check if it's requester
-+            self.signal_pipe.send_tensor(self.end_signal)
-\ No newline at end of file
-diff --git a/vllm/distributed/kv_transfer/kv_transfer_agent.py b/vllm/distributed/kv_transfer/kv_transfer_agent.py
-index 1e80e0bd7..75a7d697a 100644
---- a/vllm/distributed/kv_transfer/kv_transfer_agent.py
-+++ b/vllm/distributed/kv_transfer/kv_transfer_agent.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """A centralized entrypoint to perform distributed KV cache transfer.
+     def NCCL_CHECK(self, result: ncclResult_t) -> None:
+         if result != 0:
+             error_str = self.ncclGetErrorString(result)
+-            raise RuntimeError(f"NCCL error: {error_str}")
++            raise RuntimeError(f"MCCL error: {error_str}")
  
-@@ -18,6 +19,9 @@ from vllm.distributed.kv_transfer.kv_connector.factory import (
- from vllm.logger import init_logger
- from vllm.sequence import IntermediateTensors
+     def ncclGetVersion(self) -> str:
+         version = ctypes.c_int()
+-        self.NCCL_CHECK(self._funcs["ncclGetVersion"](ctypes.byref(version)))
++        self.NCCL_CHECK(self._funcs["mcclGetVersion"](ctypes.byref(version)))
+         version_str = str(version.value)
+         # something like 21903 --> "2.19.3"
+         major = version_str[0].lstrip("0")
+@@ -268,14 +268,14 @@ class NCCLLibrary:
  
-+if TYPE_CHECKING:
-+    from vllm.attention import AttentionMetadata
-+
- logger = init_logger(__name__)
+     def ncclGetUniqueId(self) -> ncclUniqueId:
+         unique_id = ncclUniqueId()
+-        self.NCCL_CHECK(self._funcs["ncclGetUniqueId"](
++        self.NCCL_CHECK(self._funcs["mcclGetUniqueId"](
+             ctypes.byref(unique_id)))
+         return unique_id
  
+     def ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId,
+                          rank: int) -> ncclComm_t:
+         comm = ncclComm_t()
+-        self.NCCL_CHECK(self._funcs["ncclCommInitRank"](ctypes.byref(comm),
++        self.NCCL_CHECK(self._funcs["mcclCommInitRank"](ctypes.byref(comm),
+                                                         world_size, unique_id,
+                                                         rank))
+         return comm
+@@ -288,7 +288,7 @@ class NCCLLibrary:
+         # both are aliases of `ctypes.c_int`
+         # when we pass int to a function, it will be converted to `ctypes.c_int`
+         # by ctypes automatically
+-        self.NCCL_CHECK(self._funcs["ncclAllReduce"](sendbuff, recvbuff, count,
++        self.NCCL_CHECK(self._funcs["mcclAllReduce"](sendbuff, recvbuff, count,
+                                                      datatype, op, comm,
+                                                      stream))
  
-@@ -57,20 +61,36 @@ class KVTransferAgent:
-         hidden_or_intermediate_states: Union[torch.Tensor,
-                                              IntermediateTensors],
-     ) -> None:
--
-+        print("model_executable",model_executable)
-         self.connector.send_kv_caches_and_hidden_states(
-             model_executable, model_input, kv_caches,
-             hidden_or_intermediate_states)
- 
-+    def send_one_layer_kv_cache(self, layer_id: int,
-+                                input_token_hash: List[str],
-+                                kv_cache: torch.Tensor,
-+                                attn_metadata: "AttentionMetadata",
-+                                block_size: int) -> None:
-+        self.connector.send_one_layer_kv_cache(layer_id, input_token_hash,
-+                                               kv_cache, attn_metadata,
-+                                               block_size)
-+
-+    def send_hidden_states(self, input_token_hash: List[str],
-+                           hidden_states: torch.Tensor, attn_metadata) -> None:
-+        self.connector.send_hidden_states(input_token_hash, hidden_states,
-+                                          attn_metadata)
-+
-     def close(self) -> None:
-         self.connector.close()
- 
-     def recv_kv_caches_and_hidden_states(
--        self, model_executable: torch.nn.Module,
-+        self,
-+        model_executable: torch.nn.Module,
-         model_input: "ModelInputForGPUWithSamplingMetadata",
--        kv_caches: List[torch.Tensor]
--    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
-+        kv_caches: List[torch.Tensor],
-+        **kwargs,
-+    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], List[bool],
-                "ModelInputForGPUWithSamplingMetadata"]:
- 
-         return self.connector.recv_kv_caches_and_hidden_states(
--            model_executable, model_input, kv_caches)
-+            model_executable, model_input, kv_caches, **kwargs)
-diff --git a/vllm/distributed/kv_transfer/kv_transfer_utils.py b/vllm/distributed/kv_transfer/kv_transfer_utils.py
-new file mode 100644
-index 000000000..2661431a1
---- /dev/null
-+++ b/vllm/distributed/kv_transfer/kv_transfer_utils.py
-@@ -0,0 +1,150 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+import hashlib
-+import threading
-+
-+import torch
-+
-+from vllm.config import VllmConfig
-+from vllm.distributed import get_kv_transfer_group
-+
-+# Thread-local storage for context management
-+_hook_context = threading.local()
-+
-+
-+# the built-in hash function is not deterministic
-+# due to the design of hash randomization.
-+# so we need a way to hash the tensor in a deterministic way
-+# between the Prefill & Decode processes.
-+def get_tensor_stable_hash(tensor):
-+    data = tensor.cpu().numpy().tobytes()
-+    return hashlib.md5(data).hexdigest()
-+
-+
-+def set_context_value(value):
-+    """Set a value in the thread-local context."""
-+    _hook_context.value = value
-+
-+
-+def get_context_value():
-+    """Get the value from the thread-local context."""
-+    return getattr(_hook_context, "value", None)
-+
-+
-+def maybe_register_PD_disagg_hooks(model: torch.nn.Module,
-+                                   vllm_config: VllmConfig):
-+    if vllm_config.kv_transfer_config is None:
-+        return
-+
-+    if (vllm_config.kv_transfer_config.is_kv_producer
-+            and vllm_config.kv_transfer_config.is_layerwise_kv_transfer):
-+        register_model_PD_disagg_hooks(model, vllm_config)
-+        register_decoder_layer_PD_disagg_hooks(model)
-+
-+
-+def register_model_PD_disagg_hooks(model: torch.nn.Module,
-+                                   vllm_config: VllmConfig):
-+    """
-+    Registers hooks:
-+    - A pre-forward hook to save context data.
-+    - A post-forward hook to send the hidden states.
-+    """
-+
-+    # Pre-forward hook for the top-level model
-+    def pre_forward_hook(module, args, kwargs):
-+        if 'input_ids' not in kwargs:
-+            raise ValueError("No input_ids tensor found in kwargs.")
-+        if 'attn_metadata' not in kwargs:
-+            raise ValueError("No attn_metadata tensor found in kwargs.")
-+
-+        input_ids = kwargs['input_ids']
-+        attn_metadata = kwargs['attn_metadata']
-+
-+        # skip if it is a profile run
-+        if input_ids.view(-1)[0].item() == 0:
-+            return
-+
-+        input_id_hashes = []
-+        start_pos = 0
-+        for seq_length in attn_metadata.seq_lens:
-+            end_pos = start_pos + seq_length
-+            input_id_hashes.append(
-+                get_tensor_stable_hash(input_ids[start_pos:end_pos]))
-+            start_pos = end_pos
-+
-+        context_dict = {
-+            'input_id_hashes': input_id_hashes,
-+            'block_size': vllm_config.cache_config.block_size
-+        }
-+        set_context_value(context_dict)
-+
-+    def post_forward_hook(module, args, kwargs, output):
-+
-+        # in case of PP, the output might be of IntermediateTensors type
-+        if not isinstance(output, torch.Tensor):
-+            return output
-+
-+        context = get_context_value()
-+        if context is None or 'input_id_hashes' not in context:
-+            return output
-+
-+        input_id_hashes = get_context_value()['input_id_hashes']
-+
-+        if 'attn_metadata' not in kwargs:
-+            raise ValueError("No attn_metadata tensor found in kwargs.")
-+
-+        attn_metadata = kwargs['attn_metadata']
-+
-+        hidden_states = output
-+
-+        get_kv_transfer_group().send_hidden_states(input_id_hashes,
-+                                                   hidden_states,
-+                                                   attn_metadata)
-+
-+        return output
-+
-+    # Register pre-forward and post-forward hooks to the top-level model
-+    model.register_forward_pre_hook(pre_forward_hook, with_kwargs=True)
-+    model.register_forward_hook(post_forward_hook, with_kwargs=True)
-+
-+
-+def register_decoder_layer_PD_disagg_hooks(module: torch.nn.Module,
-+                                           suffix="DecoderLayer"):
-+    """
-+    Find the modules of decoder layers and register forward hooks to send
-+    kv cache of one layer.
-+    """
-+
-+    def create_decoderlayer_hook(idx):
-+
-+        def decoderlayer_hook(module, args, kwargs, output):
-+            # do nothing if is it is profile run
-+            context = get_context_value()
-+            if context is None:
-+                return output
-+            if any(key not in context
-+                   for key in ('input_id_hashes', 'block_size')):
-+                return output
-+
-+            input_id_hashes = context['input_id_hashes']
-+            block_size = context['block_size']
-+
-+            kv_cache, attn_metadata = args[2], args[3]
-+            get_kv_transfer_group().send_one_layer_kv_cache(
-+                idx, input_id_hashes, kv_cache, attn_metadata, block_size)
-+
-+            return output
-+
-+        return decoderlayer_hook
-+
-+    if hasattr(module, "layers") and isinstance(
-+            module.layers,
-+        (list, torch.nn.ModuleList
-+         )) and module.layers[0].__class__.__name__.endswith(suffix):
-+        for idx, child_module in enumerate(module.layers):
-+            child_module.register_forward_hook(create_decoderlayer_hook(idx),
-+                                               with_kwargs=True)
-+
-+    # Recurse over standard named_children as well, in case nested modules
-+    # also contain relevant children or their own 'layers' attribute.
-+    for child_name, child_module in module.named_children():
-+        register_decoder_layer_PD_disagg_hooks(child_module, suffix)
-\ No newline at end of file
+@@ -300,7 +300,7 @@ class NCCLLibrary:
+         # both are aliases of `ctypes.c_int`
+         # when we pass int to a function, it will be converted to `ctypes.c_int`
+         # by ctypes automatically
+-        self.NCCL_CHECK(self._funcs["ncclReduceScatter"](sendbuff, recvbuff,
++        self.NCCL_CHECK(self._funcs["mcclReduceScatter"](sendbuff, recvbuff,
+                                                          count, datatype, op,
+                                                          comm, stream))
+ 
+@@ -311,28 +311,28 @@ class NCCLLibrary:
+         # which is an aliases of `ctypes.c_int`
+         # when we pass int to a function, it will be converted to `ctypes.c_int`
+         # by ctypes automatically
+-        self.NCCL_CHECK(self._funcs["ncclAllGather"](sendbuff, recvbuff, count,
++        self.NCCL_CHECK(self._funcs["mcclAllGather"](sendbuff, recvbuff, count,
+                                                      datatype, comm, stream))
+ 
+     def ncclSend(self, sendbuff: buffer_type, count: int, datatype: int,
+                  dest: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
+-        self.NCCL_CHECK(self._funcs["ncclSend"](sendbuff, count, datatype,
++        self.NCCL_CHECK(self._funcs["mcclSend"](sendbuff, count, datatype,
+                                                 dest, comm, stream))
+ 
+     def ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int,
+                  src: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
+-        self.NCCL_CHECK(self._funcs["ncclRecv"](recvbuff, count, datatype, src,
++        self.NCCL_CHECK(self._funcs["mcclRecv"](recvbuff, count, datatype, src,
+                                                 comm, stream))
+ 
+     def ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type,
+                       count: int, datatype: int, root: int, comm: ncclComm_t,
+                       stream: cudaStream_t) -> None:
+-        self.NCCL_CHECK(self._funcs["ncclBroadcast"](sendbuff, recvbuff, count,
++        self.NCCL_CHECK(self._funcs["mcclBroadcast"](sendbuff, recvbuff, count,
+                                                      datatype, root, comm,
+                                                      stream))
+ 
+     def ncclCommDestroy(self, comm: ncclComm_t) -> None:
+-        self.NCCL_CHECK(self._funcs["ncclCommDestroy"](comm))
++        self.NCCL_CHECK(self._funcs["mcclCommDestroy"](comm))
+ 
+ 
+ __all__ = [
 diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
-index 321902d11..71916bd59 100644
+index 10f87c49b..c596f8875 100644
 --- a/vllm/distributed/parallel_state.py
 +++ b/vllm/distributed/parallel_state.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Copyright 2023 The vLLM team.
-@@ -354,7 +355,8 @@ class GroupCoordinator:
-                 not self.xpu_communicator.disabled:
-             return self.xpu_communicator.all_reduce(input_)
- 
--        return torch.ops.vllm.all_reduce(input_, group_name=self.unique_name)
-+        return all_reduce(input_, group_name=self.unique_name)
-+        #return torch.ops.vllm.all_reduce(input_, group_name=self.unique_name)
- 
-     def _all_reduce_out_place(self, input_: torch.Tensor) -> torch.Tensor:
-         # always try custom allreduce first,
-@@ -1193,12 +1195,13 @@ def cleanup_dist_env_and_memory(shutdown_ray: bool = False):
-     from vllm.platforms import current_platform
-     if not current_platform.is_cpu():
-         torch.cuda.empty_cache()
+@@ -1203,13 +1203,14 @@ def cleanup_dist_env_and_memory(shutdown_ray: bool = False):
+     empty_cache = current_platform.empty_cache
+     if empty_cache is not None:
+         empty_cache()
 +    """
      try:
-         torch._C._host_emptyCache()
+         if not current_platform.is_cpu():
+             torch._C._host_emptyCache()
      except AttributeError:
          logger.warning(
              "torch._C._host_emptyCache() only available in Pytorch >=2.5")
@@ -24091,649 +18025,158 @@ index 321902d11..71916bd59 100644
 +    """
  
  def in_the_same_node_as(pg: Union[ProcessGroup, StatelessProcessGroup],
-                         source_rank: int = 0) -> List[bool]:
-diff --git a/vllm/distributed/utils.py b/vllm/distributed/utils.py
-index 84f8c0a8e..a615bd3ab 100644
---- a/vllm/distributed/utils.py
-+++ b/vllm/distributed/utils.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Copyright 2023 The vLLM team.
-@@ -64,7 +65,14 @@ def get_pp_indices(num_hidden_layers: int, pp_rank: int,
-                    pp_size: int) -> Tuple[int, int]:
-     """Try to evenly distribute layers across partitions.
-     If the number of layers is not divisible by the number of partitions,
--    the last partition will have the remaining layers.
-+    the remaining layers are evenly distributed across all but the last
-+    partition. The last partition is excluded because it often contains an
-+    additional norm layer and we are attempting to balance compute.
-+    If `pp_size > 2` and the number of remaining layers is
-+    `0 < x <= pp_size - 2` then the remaining layers are evenly distributed
-+    across the middle partitions. The first and last partitions are excluded
-+    because they contain the input and output embeddings respectively and we
-+    are attempting to reduce maximum memory consumption across partitions.
-     """
-     partition_list_str = envs.VLLM_PP_LAYER_PARTITION
-     if partition_list_str is not None:
-@@ -80,15 +88,20 @@ def get_pp_indices(num_hidden_layers: int, pp_rank: int,
-         if sum(partitions) != num_hidden_layers:
-             raise ValueError(
-                 f"{sum(partitions)=} does not match {num_hidden_layers=}.")
--        start_layer = sum(partitions[:pp_rank])
--        end_layer = start_layer + partitions[pp_rank]
-     else:
-         layers_per_partition = num_hidden_layers // pp_size
--        start_layer = pp_rank * layers_per_partition
--        end_layer = start_layer + layers_per_partition
--
--        if pp_rank == pp_size - 1:
--            end_layer = num_hidden_layers
-+        partitions = [layers_per_partition for _ in range(pp_size)]
-+
-+        if remaining_layers := num_hidden_layers % pp_size:
-+            for i in range(2, remaining_layers + 2):
-+                partitions[-i] += 1
-+            logger.info("Hidden layers were unevenly partitioned: %s",
-+                        ",".join(str(p) for p in partitions))
-+            logger.info("This can be manually overridden using the "
-+                        "VLLM_PP_LAYER_PARTITION environment variable")
-+
-+    start_layer = sum(partitions[:pp_rank])
-+    end_layer = start_layer + partitions[pp_rank]
- 
-     return (start_layer, end_layer)
- 
-diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
-index 40c6fb456..ecc80d284 100644
---- a/vllm/engine/arg_utils.py
-+++ b/vllm/engine/arg_utils.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import argparse
-@@ -349,7 +350,7 @@ class EngineArgs:
-         parser.add_argument(
-             '--kv-cache-dtype',
-             type=str,
--            choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],
-+            choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3', 'int8'],
-             default=EngineArgs.kv_cache_dtype,
-             help='Data type for kv cache storage. If "auto", will use model '
-             'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '
-@@ -1273,6 +1274,12 @@ class EngineArgs:
-             or "all" in detailed_trace_modules,
-         )
- 
-+        if (self.kv_transfer_config is not None and \
-+                self.kv_transfer_config.is_layerwise_kv_transfer and \
-+                self.kv_transfer_config.is_kv_producer and \
-+                not model_config.enforce_eager):
-+            raise ValueError("layerwise KV producer only supports eager mode")
-+
-         config = VllmConfig(
-             model_config=model_config,
-             cache_config=cache_config,
-diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
-index d82d9ad9d..88da08990 100644
---- a/vllm/engine/llm_engine.py
-+++ b/vllm/engine/llm_engine.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import copy
-@@ -230,6 +231,7 @@ class LLMEngine:
-         self.prompt_adapter_config = vllm_config.prompt_adapter_config  # noqa
-         self.observability_config = vllm_config.observability_config or ObservabilityConfig(  # noqa
-         )
-+        self.parallel_config.disable_custom_all_reduce = True
- 
-         logger.info(
-             "Initializing a V0 LLM engine (v%s) with config: %s, "
-@@ -1459,6 +1461,211 @@ class LLMEngine:
- 
-         return ctx.request_outputs
- 
-+    def step_new(self):
-+        """Performs one decoding iteration and returns newly generated results.
-+
-+        .. figure:: https://i.imgur.com/sv2HssD.png
-+            :alt: Overview of the step function
-+            :align: center
-+
-+            Overview of the step function.
-+
-+        Details:
-+            - Step 1: Schedules the sequences to be executed in the next
-+              iteration and the token blocks to be swapped in/out/copy.
-+
-+                - Depending on the scheduling policy,
-+                  sequences may be `preempted/reordered`.
-+                - A Sequence Group (SG) refer to a group of sequences
-+                  that are generated from the same prompt.
-+
-+            - Step 2: Calls the distributed executor to execute the model.
-+            - Step 3: Processes the model output. This mainly includes:
-+
-+                - Decodes the relevant outputs.
-+                - Updates the scheduled sequence groups with model outputs
-+                  based on its `sampling parameters` (`use_beam_search` or not).
-+                - Frees the finished sequence groups.
-+
-+            - Finally, it creates and returns the newly generated results.
-+
-+        Example:
-+            >>> # Please see the example/ folder for more detailed examples.
-+            >>>
-+            >>> # initialize engine and request arguments
-+            >>> engine = LLMEngine.from_engine_args(engine_args)
-+            >>> example_inputs = [(0, "What is LLM?",
-+            >>>    SamplingParams(temperature=0.0))]
-+            >>>
-+            >>> # Start the engine with an event loop
-+            >>> while True:
-+            >>>     if example_inputs:
-+            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)
-+            >>>         engine.add_request(str(req_id),prompt,sampling_params)
-+            >>>
-+            >>>     # continue the request processing
-+            >>>     request_outputs = engine.step()
-+            >>>     for request_output in request_outputs:
-+            >>>         if request_output.finished:
-+            >>>             # return or show the request output
-+            >>>
-+            >>>     if not (engine.has_unfinished_requests() or example_inputs):
-+            >>>         break
-+        """
-+        if self.parallel_config.pipeline_parallel_size > 1:
-+            raise NotImplementedError(
-+                "Pipeline parallelism is only supported through AsyncLLMEngine "
-+                "as performance will be severely degraded otherwise.")
-+
-+        # For llm_engine, there is no pipeline parallel support, so the engine
-+        # used is always 0.
-+        virtual_engine = 0
-+
-+        # These are cached outputs from previous iterations. None if on first
-+        # iteration
-+        cached_outputs = self.cached_scheduler_outputs[virtual_engine]
-+        seq_group_metadata_list = cached_outputs.seq_group_metadata_list
-+        scheduler_outputs = cached_outputs.scheduler_outputs
-+        allow_async_output_proc = cached_outputs.allow_async_output_proc
-+
-+        ctx = self.scheduler_contexts[virtual_engine]
-+
-+        # Clear outputs for each new scheduler iteration
-+        ctx.request_outputs.clear()
-+
-+        # Skip the scheduler if there are any remaining steps in the seq groups.
-+        # This ensures that the scheduler is only called again when the current
-+        # batch has completed.
-+        if not self._has_remaining_steps(seq_group_metadata_list):
-+            # Schedule iteration
-+            (seq_group_metadata_list, scheduler_outputs,
-+             allow_async_output_proc
-+             ) = self.scheduler[virtual_engine].schedule()
-+
-+            ctx.seq_group_metadata_list = seq_group_metadata_list
-+            ctx.scheduler_outputs = scheduler_outputs
-+
-+            finished_requests_ids = self.scheduler[
-+                virtual_engine].get_and_reset_finished_requests_ids()
-+
-+            # Maybe switch from async mode to sync mode
-+            if not allow_async_output_proc and len(ctx.output_queue) > 0:
-+                self._process_model_outputs(ctx=ctx)
-+
-+            if (self.scheduler_config.is_multi_step
-+                    and scheduler_outputs.num_lookahead_slots > 0):
-+                # cache the scheduler outputs for the next iteration if we have
-+                # lookahead slots
-+                self._cache_scheduler_outputs_for_multi_step(
-+                    virtual_engine, seq_group_metadata_list, scheduler_outputs,
-+                    allow_async_output_proc)
-+        else:
-+            finished_requests_ids = list()
-+
-+        assert seq_group_metadata_list is not None
-+        assert scheduler_outputs is not None
-+
-+        if not scheduler_outputs.is_empty():
-+
-+            # Check if we have a cached last_output from the previous iteration.
-+            # For supporting PP this is probably the best way to pass the
-+            # sampled_token_ids, as a separate broadcast over all the PP stages
-+            # will cause one virtual engine's microbatch to block the pipeline.
-+            last_sampled_token_ids = \
-+                self._get_last_sampled_token_ids(virtual_engine)
-+
-+            execute_model_req = ExecuteModelRequest(
-+                seq_group_metadata_list=seq_group_metadata_list,
-+                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,
-+                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,
-+                blocks_to_copy=scheduler_outputs.blocks_to_copy,
-+                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,
-+                running_queue_size=scheduler_outputs.running_queue_size,
-+                finished_requests_ids=finished_requests_ids,
-+                # We use ExecuteModelRequest to pass the last sampled_token_ids
-+                # to each of the non-last PP stages for in-place prepare_input.
-+                last_sampled_token_ids=last_sampled_token_ids)
-+
-+            if allow_async_output_proc:
-+                execute_model_req.async_callback = self.async_callbacks[
-+                    virtual_engine]
-+
-+            outputs = self.model_executor.execute_model_return_hidden(
-+                execute_model_req=execute_model_req)
-+
-+            # We need to do this here so that last step's sampled_token_ids can
-+            # be passed to the next iteration for PP.
-+            if self.scheduler_config.is_multi_step:
-+                self._update_cached_scheduler_output(virtual_engine, outputs)
-+        else:
-+            # Nothing scheduled => If there is pending async postprocessor,
-+            # then finish it here.
-+            if len(ctx.output_queue) > 0:
-+                self._process_model_outputs(ctx=ctx)
-+            # No outputs in this case
-+            outputs = []
-+
-+        # Finish the current step for all the sequence groups.
-+        if self.scheduler_config.is_multi_step:
-+            for seq_group in seq_group_metadata_list:
-+                seq_group.finish_step()
-+
-+        if not self._has_remaining_steps(seq_group_metadata_list):
-+            # clear the cache if we have finished all the steps.
-+            if self.scheduler_config.is_multi_step:
-+                self.cached_scheduler_outputs[0] = SchedulerOutputState()
-+
-+            # is_first_step_output is True only when the num_steps of all
-+            # the sequences are 1. When the num_steps > 1,
-+            # multi_step_model_runner does the first-step output append.
-+            is_first_step_output: bool = False if not seq_group_metadata_list \
-+                else seq_group_metadata_list[0].state.num_steps == 1
-+
-+            # Add results to the output_queue
-+            ctx.append_output(outputs=outputs,
-+                              seq_group_metadata_list=seq_group_metadata_list,
-+                              scheduler_outputs=scheduler_outputs,
-+                              is_async=allow_async_output_proc,
-+                              is_last_step=True,
-+                              is_first_step_output=is_first_step_output)
-+
-+            if outputs and allow_async_output_proc:
-+                assert len(outputs) == 1, (
-+                    "Async postprocessor expects only a single output set")
-+
-+                self._advance_to_next_step(
-+                    outputs[0], seq_group_metadata_list,
-+                    scheduler_outputs.scheduled_seq_groups)
-+
-+            # Check if need to run the usual non-async path
-+            if not allow_async_output_proc:
-+                self._process_model_outputs(ctx=ctx)
-+
-+                # Log stats.
-+                self.do_log_stats(scheduler_outputs, outputs)
-+
-+                # Tracing
-+                self.do_tracing(scheduler_outputs)
-+        else:
-+            # Multi-step case # do not support step new return logits
-+            return ctx.request_outputs, None
-+
-+        if not self.has_unfinished_requests():
-+            # Drain async postprocessor (if exists)
-+            if len(ctx.output_queue) > 0:
-+                self._process_model_outputs(ctx=ctx)
-+            assert len(ctx.output_queue) == 0
-+
-+            # Stop the execute model loop in parallel workers until there are
-+            # more requests to process. This avoids waiting indefinitely in
-+            # torch.distributed ops which may otherwise timeout, and unblocks
-+            # the RPC thread in the workers so that they can process any other
-+            # queued control plane messages, such as add/remove lora adapters.
-+            logger.debug("Stopping remote worker execution loop.")
-+            self.model_executor.stop_remote_worker_execution_loop()
-+
-+        return ctx.request_outputs, outputs[0].hidden_states
-+
-     def _has_remaining_steps(
-         self, seq_group_metadata_list: Optional[List[SequenceGroupMetadata]]
-     ) -> bool:
-diff --git a/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py b/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
-index 5c19888d4..354d4cc84 100644
---- a/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
-+++ b/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import re
-@@ -67,6 +68,8 @@ class DeepSeekR1ReasoningParser(ReasoningParser):
-         ]):
-             return None
- 
-+        # Check if <think> is present in previous or delta.
-+        # Keep compatibility with models that don't generate <think> tokens.
-         if self.think_start_token_id in previous_token_ids:
-             if self.think_end_token_id in delta_token_ids:
-                 # <think> in previous, </think> in delta,
-@@ -85,7 +88,6 @@ class DeepSeekR1ReasoningParser(ReasoningParser):
-                 # reasoning content continues
-                 return DeltaMessage(reasoning_content=delta_text)
-         elif self.think_start_token_id in delta_token_ids:
--            logger.info(delta_text)
-             if self.think_end_token_id in delta_token_ids:
-                 # <think> in delta, </think> in delta, extract reasoning content
-                 start_index = delta_text.find(self.think_start_token)
-@@ -101,35 +103,46 @@ class DeepSeekR1ReasoningParser(ReasoningParser):
-                 # reasoning content continues
-                 return DeltaMessage(reasoning_content=delta_text)
-         else:
--            # No <think> in previous or delta, reasoning content continues.
--            return DeltaMessage(content=delta_text)
-+            # No <think> in previous or delta, also need to check for </think>.
-+            # Because the model may have generated </think> without <think>
-+            # Ref https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/8a58a132790c9935686eb97f042afa8013451c9f
-+            if self.think_end_token_id in delta_token_ids:
-+                # </think> in delta with more tokens,
-+                # extract reasoning content and content
-+                end_index = delta_text.find(self.think_end_token)
-+                reasoning_content = delta_text[:end_index]
-+                content = delta_text[end_index + len(self.think_end_token):]
-+                return DeltaMessage(reasoning_content=reasoning_content,
-+                                    content=content if content else None)
-+            elif self.think_end_token_id in previous_token_ids:
-+                # </think> in previous, thinking content ends
-+                return DeltaMessage(content=delta_text)
-+            else:
-+                # no </think> in previous or delta, reasoning content continues
-+                return DeltaMessage(reasoning_content=delta_text)
- 
-     def extract_reasoning_content(
-             self, model_output: str, request: ChatCompletionRequest
-     ) -> Tuple[Optional[str], Optional[str]]:
- 
--        # Check if the model output contains the <think> tokens.
--        if (self.think_start_token not in model_output
--                or self.think_end_token not in model_output):
--            return None, model_output
-+        # DeepSeek R1 doesn't generate <think> now.
-+        # Thus we assume the reasoning content is always at the start.
-+        # Ref https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/8a58a132790c9935686eb97f042afa8013451c9f
-+        if self.think_end_token not in model_output:
-+            return model_output, None
-         else:
-+            # Add a start token if it's missing to keep compatibility.
-+            if self.think_start_token not in model_output:
-+                model_output = f"{self.think_start_token}{model_output}"
-             # Use a regex to find the reasoning content
-             reasoning_content = self.reasoning_regex.findall(model_output)[0]
- 
--            # Remove the reasoning content from the model output
--            # Although deepseek's <think> token is always at the
--            # beginning of the line, we cannot guarantee that the
--            # other models will follow this convention.
--            # Therefore, we need to add :start_index.
--            start_index = model_output.find(self.think_start_token)
--            if start_index != -1:
--                end_index = start_index + len(
--                    f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
--                )
--                model_output = model_output[:start_index] + \
--                                model_output[end_index:]
--
--                if len(model_output) == 0:
--                    return reasoning_content, None
--
--            return reasoning_content, model_output
-+            end_index = len(
-+                f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
-+            )
-+            final_output = model_output[end_index:]
-+
-+            if len(final_output) == 0:
-+                return reasoning_content, None
-+
-+            return reasoning_content, final_output
-\ No newline at end of file
+                         source_rank: int = 0) -> list[bool]:
+diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
+index 6e3cb18fc..a5a8a812c 100644
+--- a/vllm/entrypoints/llm.py
++++ b/vllm/entrypoints/llm.py
+@@ -173,13 +173,14 @@ class LLM:
+         cpu_offload_gb: float = 0,
+         enforce_eager: bool = False,
+         max_seq_len_to_capture: int = 8192,
+-        disable_custom_all_reduce: bool = False,
++        disable_custom_all_reduce: bool = True,
+         disable_async_output_proc: bool = False,
+         hf_token: Optional[Union[bool, str]] = None,
+         hf_overrides: Optional[HfOverrides] = None,
+         mm_processor_kwargs: Optional[dict[str, Any]] = None,
+         override_pooler_config: Optional[PoolerConfig] = None,
+-        compilation_config: Optional[Union[int, dict[str, Any]]] = None,
++        compilation_config: Optional[Union[int, dict[str, Any],
++                                            CompilationConfig]] = None,
+         **kwargs,
+     ) -> None:
+         """LLM constructor."""
+diff --git a/vllm/env_override.py b/vllm/env_override.py
+index 2bede4963..a127a5587 100644
+--- a/vllm/env_override.py
++++ b/vllm/env_override.py
+@@ -38,4 +38,4 @@ os.environ['PYTORCH_NVML_BASED_CUDA_CHECK'] = '1'
+ # see https://github.com/vllm-project/vllm/issues/10480
+ os.environ['TORCHINDUCTOR_COMPILE_THREADS'] = '1'
+ # see https://github.com/vllm-project/vllm/issues/10619
+-torch._inductor.config.compile_threads = 1
++# torch._inductor.config.compile_threads = 1
 diff --git a/vllm/envs.py b/vllm/envs.py
-index 745b068b7..6c1e3ad90 100644
+index 80c5f289b..1d351238b 100644
 --- a/vllm/envs.py
 +++ b/vllm/envs.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import os
-@@ -76,8 +77,8 @@ if TYPE_CHECKING:
-     VLLM_ENABLE_V1_MULTIPROCESSING: bool = True
-     VLLM_LOG_BATCHSIZE_INTERVAL: float = -1
-     VLLM_DISABLE_COMPILE_CACHE: bool = False
--    K_SCALE_CONSTANT: int = 200
--    V_SCALE_CONSTANT: int = 100
-+    K_SCALE_CONSTANT: int = 127
-+    V_SCALE_CONSTANT: int = 127
-     VLLM_SERVER_DEV_MODE: bool = False
-     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128
-     VLLM_MLA_DISABLE: bool = False
-@@ -87,6 +88,8 @@ if TYPE_CHECKING:
-     VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False
-     VLLM_RAY_PER_WORKER_GPUS: float = 1.0
-     VLLM_RAY_BUNDLE_INDICES: str = ""
-+    TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP: bool = True
-+    TRITON_ENABLE_MACA_CHAIN_DOT_OPT: bool = True
- 
+@@ -127,7 +127,7 @@ if TYPE_CHECKING:
+     VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS: int = 1
+     VLLM_SLEEP_WHEN_IDLE: bool = False
+     VLLM_MQ_MAX_CHUNK_BYTES_MB: int = 16
+-
++    MACA_VLLM_USE_TN_2_NN: bool = True
  
  def get_default_cache_root():
-@@ -496,11 +499,11 @@ environment_variables: Dict[str, Callable[[], Any]] = {
- 
-     # Divisor for dynamic key scale factor calculation for FP8 KV Cache
-     "K_SCALE_CONSTANT":
--    lambda: int(os.getenv("K_SCALE_CONSTANT", "200")),
-+    lambda: int(os.getenv("K_SCALE_CONSTANT", "127")),
- 
-     # Divisor for dynamic value scale factor calculation for FP8 KV Cache
-     "V_SCALE_CONSTANT":
--    lambda: int(os.getenv("V_SCALE_CONSTANT", "100")),
-+    lambda: int(os.getenv("V_SCALE_CONSTANT", "127")),
-     # If set, enable multiprocessing in LLM for the V1 code path.
-     "VLLM_ENABLE_V1_MULTIPROCESSING":
-     lambda: bool(int(os.getenv("VLLM_ENABLE_V1_MULTIPROCESSING", "1"))),
-@@ -546,6 +549,10 @@ environment_variables: Dict[str, Callable[[], Any]] = {
-     "VLLM_MLA_DISABLE_REQUANTIZATION":
-     lambda: bool(int(os.getenv("VLLM_MLA_DISABLE_REQUANTIZATION", "0"))),
- 
-+    # Trion kernel `decode_attention_fwd` maca opt
-+    "TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP": True,
-+    "TRITON_ENABLE_MACA_CHAIN_DOT_OPT": True,
-+
-     # If set, vLLM will use the Triton implementation of moe_align_block_size,
-     # i.e. moe_align_block_size_triton in fused_moe.py.
-     "VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON":
-@@ -571,7 +578,7 @@ environment_variables: Dict[str, Callable[[], Any]] = {
-     # for all allocations. Currently this primarily affects MLA, for most other
-     # models the alignment is already naturally aligned to 256 bytes.
-     "VLLM_CUDA_MEM_ALIGN_KV_CACHE":
--    lambda: bool(int(os.getenv("VLLM_CUDA_MEM_ALIGN_KV_CACHE", "1"))),
-+    lambda: bool(int(os.getenv("VLLM_CUDA_MEM_ALIGN_KV_CACHE", "0"))),
- }
- 
- # end-env-vars-definition
-diff --git a/vllm/executor/executor_base.py b/vllm/executor/executor_base.py
-index fb76276bb..117f8978e 100644
---- a/vllm/executor/executor_base.py
-+++ b/vllm/executor/executor_base.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import asyncio
-@@ -139,6 +140,13 @@ class ExecutorBase(ABC):
-                                      args=(execute_model_req, ))
-         return output[0]
- 
-+    def execute_model_return_hidden(
-+        self, execute_model_req: ExecuteModelRequest
-+    ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:
-+        output = self.collective_rpc("execute_model_return_hidden",
-+                                    args=(execute_model_req, ))
-+        return output[0]
-+
-     def stop_remote_worker_execution_loop(self) -> None:
-         """Releases parallel workers from model loop."""
-         return
-@@ -275,6 +283,21 @@ class DistributedExecutorBase(ExecutorBase):
-         driver_outputs = self._driver_execute_model(execute_model_req)
-         assert driver_outputs is not None
-         return driver_outputs
-+    
-+    def execute_model_return_hidden(
-+        self,
-+        execute_model_req: ExecuteModelRequest,
-+    ) -> List[SamplerOutput]:
-+        if self.parallel_worker_tasks is None:
-+            self.parallel_worker_tasks = self._run_workers(
-+                "start_worker_execution_loop_return_hidden",
-+                async_run_tensor_parallel_workers_only=True,
-+                **self.extra_execute_model_run_workers_kwargs)
-+
-+        # Only the driver worker returns the sampling results.
-+        driver_outputs = self._driver_execute_model_return_hidden(execute_model_req)
-+        assert driver_outputs is not None
-+        return driver_outputs
- 
-     def stop_remote_worker_execution_loop(self) -> None:
-         if self.parallel_worker_tasks is None:
-@@ -286,6 +309,17 @@ class DistributedExecutorBase(ExecutorBase):
-         # Ensure that workers exit model loop cleanly
-         # (this will raise otherwise)
-         self._wait_for_tasks_completion(parallel_worker_tasks)
-+    
-+    def stop_remote_worker_execution_loop_for_hidden(self) -> None:
-+        if self.parallel_worker_tasks is None:
-+            return
-+
-+        self._driver_execute_model_return_hidden(execute_model_req=None)
-+        parallel_worker_tasks = self.parallel_worker_tasks
-+        self.parallel_worker_tasks = None
-+        # Ensure that workers exit model loop cleanly
-+        # (this will raise otherwise)
-+        self._wait_for_tasks_completion(parallel_worker_tasks)
- 
-     @abstractmethod
-     def _driver_execute_model(
-@@ -299,6 +333,18 @@ class DistributedExecutorBase(ExecutorBase):
-         """
-         raise NotImplementedError
- 
-+    @abstractmethod
-+    def _driver_execute_model_return_hidden(
-+        self, execute_model_req: Optional[ExecuteModelRequest]
-+    ) -> Optional[List[SamplerOutput]]:
-+        """Run execute_model in the driver worker.
-+
-+        Passing None will cause the driver to stop the model execution loop
-+        running in each of the remote workers. In this case, this method
-+        returns None. Otherwise, this method returns the model output.
-+        """
-+        raise NotImplementedError
-+
-     def collective_rpc(self,
-                        method: Union[str, Callable],
-                        timeout: Optional[float] = None,
-diff --git a/vllm/executor/mp_distributed_executor.py b/vllm/executor/mp_distributed_executor.py
-index d1f8c36fb..a97b5a451 100644
---- a/vllm/executor/mp_distributed_executor.py
-+++ b/vllm/executor/mp_distributed_executor.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import asyncio
-@@ -47,6 +48,12 @@ class MultiprocessingDistributedExecutor(DistributedExecutorBase):
-                 f"please ensure that world_size ({world_size}) "
-                 f"is less than than max local gpu count ({cuda_device_count})")
- 
-+        if "MACA_VISIBLE_DEVICES" in os.environ:
-+            logger.info("update CUDA_VISIBLE_DEVICES with the value of MACA_VISIBLE_DEVICES")
-+            update_environment_variables: ({
-+                "CUDA_VISIBLE_DEVICES": ("".join(os.environ["MACA_VISIBLE_DEVICES"]))
-+            })
-+
-         # Set CUDA_VISIBLE_DEVICES for the driver, inherited by workers
-         if "CUDA_VISIBLE_DEVICES" not in os.environ:
-             update_environment_variables({
-@@ -142,6 +149,16 @@ class MultiprocessingDistributedExecutor(DistributedExecutorBase):
-         loop running in each of the remote workers.
-         """
-         return self.driver_worker.execute_model(execute_model_req)
+     return os.getenv(
+@@ -870,6 +870,10 @@ environment_variables: dict[str, Callable[[], Any]] = {
+     # processes via zmq.
+     "VLLM_MQ_MAX_CHUNK_BYTES_MB":
+     lambda: int(os.getenv("VLLM_MQ_MAX_CHUNK_BYTES_MB", "16")),
 +    
-+    def _driver_execute_model_return_hidden(
-+        self, execute_model_req: Optional[ExecuteModelRequest]
-+    ) -> Optional[List[SamplerOutput]]:
-+        """Run execute_model in the driver worker.
-+
-+        Passing None will cause the driver to stop the model execution
-+        loop running in each of the remote workers.
-+        """
-+        return self.driver_worker.execute_model_return_hidden(execute_model_req)
++    # if set, enable loading weight by transpose
++    "MACA_VLLM_USE_TN_2_NN":
++    lambda: os.environ.get("MACA_VLLM_USE_TN_2_NN", "1") == "1",
+ }
  
-     def _run_workers(
-         self,
+ # --8<-- [end:env-vars-definition]
 diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
-index 6a25a4d50..528b9c5a6 100644
+index bdc2b1f4c..174fbc26d 100644
 --- a/vllm/executor/ray_distributed_executor.py
 +++ b/vllm/executor/ray_distributed_executor.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import asyncio
-@@ -400,6 +401,20 @@ class RayDistributedExecutor(DistributedExecutorBase):
-             "driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1")
-         return self.driver_worker.execute_method("execute_model",
-                                                  execute_model_req)
-+    
-+    def _driver_execute_model_return_hidden(
-+        self, execute_model_req: Optional[ExecuteModelRequest]
-+    ) -> Optional[List[SamplerOutput]]:
-+        """Run execute_model in the driver worker.
-+
-+        Passing None will cause the driver to stop the model execution
-+        loop running in each of the remote workers.
-+        """
-+        assert not self.use_ray_spmd_worker, (
-+            "driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1")
-+        return self.driver_worker.execute_method("execute_model_return_hidden",
-+                                                 execute_model_req)
-+
+@@ -557,8 +557,16 @@ class RayDistributedExecutor(DistributedExecutorBase):
+     def _compiled_ray_dag(self, enable_asyncio: bool):
+         assert self.parallel_config.use_ray
+         self._check_ray_cgraph_installation()
++        # Enlarge the default value of "RAY_CGRAPH_get_timeout" to 300 seconds
++        # (it is 10 seconds by default). This is a Ray environment variable to
++        # control the timeout of getting result from a compiled graph execution,
++        # i.e., the distributed execution that includes model forward runs and
++        # intermediate tensor communications, in the case of vllm.
++        os.environ.setdefault("RAY_CGRAPH_get_timeout", "300")  # noqa: SIM112
+         from ray.dag import InputNode, MultiOutputNode
+ 
++        logger.info("RAY_CGRAPH_get_timeout is set to %s",
++                    os.environ["RAY_CGRAPH_get_timeout"])  # noqa: SIM112
+         logger.info("VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = %s",
+                     envs.VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE)
+         logger.info("VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = %s",
+@@ -570,14 +578,6 @@ class RayDistributedExecutor(DistributedExecutorBase):
+                 "Invalid value for VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE: "
+                 f"{channel_type}. Valid values are: 'auto', 'nccl', or 'shm'.")
+ 
+-        # Enlarge the default value of "RAY_CGRAPH_get_timeout" to 300 seconds
+-        # (it is 10 seconds by default). This is a Ray environment variable to
+-        # control the timeout of getting result from a compiled graph execution,
+-        # i.e., the distributed execution that includes model forward runs and
+-        # intermediate tensor communications, in the case of vllm.
+-        os.environ.setdefault("RAY_CGRAPH_get_timeout", "300")  # noqa: SIM112
+-        logger.info("RAY_CGRAPH_get_timeout is set to %s",
+-                    os.environ["RAY_CGRAPH_get_timeout"])  # noqa: SIM112
+ 
+         with InputNode() as input_data:
+             # Example DAG: PP=2, TP=4
+diff --git a/vllm/forward_context.py b/vllm/forward_context.py
+index f3b0518a4..dd55b19fe 100644
+--- a/vllm/forward_context.py
++++ b/vllm/forward_context.py
+@@ -94,6 +94,7 @@ class ForwardContext:
+     virtual_engine: int  # set dynamically for each forward pass
+     # set dynamically for each forward pass
+     dp_metadata: Optional[DPMetadata] = None
++    skip_cuda_graphs: bool = False
+ 
+ 
+ _forward_context: Optional[ForwardContext] = None
+@@ -108,11 +109,14 @@ def get_forward_context() -> ForwardContext:
+ 
+ 
+ @contextmanager
+-def set_forward_context(attn_metadata: Any,
+-                        vllm_config: VllmConfig,
+-                        virtual_engine: int = 0,
+-                        num_tokens: Optional[int] = None,
+-                        num_tokens_across_dp: Optional[torch.Tensor] = None):
++def set_forward_context(
++    attn_metadata: Any,
++    vllm_config: VllmConfig,
++    virtual_engine: int = 0,
++    num_tokens: Optional[int] = None,
++    num_tokens_across_dp: Optional[torch.Tensor] = None,
++    skip_cuda_graphs: bool = False,
++):
+     """A context manager that stores the current forward context,
+     can be attention metadata, etc.
+     Here we can inject common logic for every model forward pass.
+@@ -135,7 +139,9 @@ def set_forward_context(attn_metadata: Any,
+         static_forward_context,
+         virtual_engine=virtual_engine,
+         attn_metadata=attn_metadata,
+-        dp_metadata=dp_metadata)
++        dp_metadata=dp_metadata,
++        skip_cuda_graphs=skip_cuda_graphs,
++    )
  
-     def execute_model(
-             self,
+     try:
+         yield
 diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
-index f14200e02..1fd914b19 100644
+index ba1498e65..9d74c6857 100644
 --- a/vllm/model_executor/layers/fused_moe/fused_moe.py
 +++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Fused MoE kernel."""
+@@ -4,9 +4,10 @@
  import functools
-@@ -16,6 +17,7 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (
-     per_token_group_quant_fp8)
- from vllm.platforms import current_platform
- from vllm.utils import direct_register_custom_op
-+import math
+ import json
+ import os
+-from typing import Any, Callable, Optional
++from typing import Any, List, Callable, Optional
  
- logger = init_logger(__name__)
+ import torch
++import math
  
-@@ -61,6 +63,8 @@ def fused_moe_kernel_gptq_awq(
+ import vllm.envs as envs
+ import vllm.model_executor.layers.fused_moe.modular_kernel as mk
+@@ -82,6 +83,8 @@ def fused_moe_kernel_gptq_awq(
          BLOCK_SIZE_N: tl.constexpr,
          BLOCK_SIZE_K: tl.constexpr,
          GROUP_SIZE_M: tl.constexpr,
@@ -24742,54 +18185,62 @@ index f14200e02..1fd914b19 100644
          MUL_ROUTED_WEIGHT: tl.constexpr,
          top_k: tl.constexpr,
          compute_type: tl.constexpr,
-@@ -220,6 +224,21 @@ def fused_moe_kernel_gptq_awq(
+@@ -251,6 +254,21 @@ def fused_moe_kernel_gptq_awq(
      tl.store(c_ptrs, accumulator, mask=c_mask)
  
  
 +@triton.heuristics(
 +    {
-+        "UPGRADE": lambda args: math.ceil((args["EM"] * args["N"]) / (args["BLOCK_SIZE_M"] * args["BLOCK_SIZE_N"])).bit_length() > 32,
++        "UPGRADE": lambda args: math.ceil((args["EM"] * args["N"]) / (args["BLOCK_SIZE_M"] * args["BLOCK_SIZE_N"])).bit_length() > 31,
 +    }
 +)
 +@triton.heuristics(
 +    {
-+        "UPGRADE_A_OFFS": lambda args: (args["num_valid_tokens"] // args["top_k"] * args["stride_am"] + args["BLOCK_SIZE_K"] * args["stride_ak"]).bit_length() > 32,
++        "UPGRADE_A_OFFS": lambda args: (args["num_valid_tokens"] // args["top_k"] * args["stride_am"] + args["BLOCK_SIZE_K"] * args["stride_ak"]).bit_length() > 31,
 +    }
 +)
 +@triton.heuristics(
 +    {
-+        "UPGRADE_B_OFFS": lambda args: (args["experts_num"] * args["stride_be"] + args["BLOCK_SIZE_K"] * args["stride_bk"] + args["N"] * args["stride_bn"]).bit_length() > 32,
++        "UPGRADE_B_OFFS": lambda args: ((args["E"]-1) * args["stride_be"] + (args["N"]-1) * args["stride_bn"]+(args["K"]-1) * args["stride_bk"]).bit_length() > 31,
 +    }
 +)
  @triton.jit
  def fused_moe_kernel(
-         # Pointers to matrices
-@@ -261,11 +280,18 @@ def fused_moe_kernel(
-         BLOCK_SIZE_N: tl.constexpr,
-         BLOCK_SIZE_K: tl.constexpr,
-         GROUP_SIZE_M: tl.constexpr,
-+        SPLIT_K: tl.constexpr,
-+        ACCF32: tl.constexpr,
-         MUL_ROUTED_WEIGHT: tl.constexpr,
-         top_k: tl.constexpr,
-+        experts_num: tl.constexpr,
-         compute_type: tl.constexpr,
-         use_fp8_w8a8: tl.constexpr,
--        use_int8_w8a16: tl.constexpr):
-+        use_int8_w8a8: tl.constexpr,
-+        use_int8_w8a16: tl.constexpr,
-+        UPGRADE: tl.constexpr,
-+        UPGRADE_A_OFFS: tl.constexpr,
-+        UPGRADE_B_OFFS: tl.constexpr):
+     # Pointers to matrices
+@@ -264,6 +282,7 @@ def fused_moe_kernel(
+     expert_ids_ptr,
+     num_tokens_post_padded_ptr,
+     # Matrix dimensions
++    E,          # B.shape[0]
+     N,
+     K,
+     EM,
+@@ -292,13 +311,20 @@ def fused_moe_kernel(
+     BLOCK_SIZE_N: tl.constexpr,
+     BLOCK_SIZE_K: tl.constexpr,
+     GROUP_SIZE_M: tl.constexpr,
++    SPLIT_K: tl.constexpr,
++    ACCF32: tl.constexpr,
+     MUL_ROUTED_WEIGHT: tl.constexpr,
+     top_k: tl.constexpr,
++    # experts_num: tl.constexpr,
+     compute_type: tl.constexpr,
+     use_fp8_w8a8: tl.constexpr,
+     use_int8_w8a8: tl.constexpr,
+     use_int8_w8a16: tl.constexpr,
+     per_channel_quant: tl.constexpr,
++    UPGRADE: tl.constexpr,
++    UPGRADE_A_OFFS: tl.constexpr,
++    UPGRADE_B_OFFS: tl.constexpr,
++    FAST_F32_TO_BF16: tl.constexpr
+ ):
      """
      Implements the fused computation for a Mixture of Experts (MOE) using
-     token and expert matrices.
-@@ -295,7 +321,13 @@ def fused_moe_kernel(
+@@ -329,7 +355,12 @@ def fused_moe_kernel(
      # -----------------------------------------------------------
      # Map program ids `pid` to the block of C it should compute.
      # This is done in a grouped ordering to promote L2 data reuse.
 -    pid = tl.program_id(axis=0)
-+    # pid = tl.program_id(axis=0)
 +    if UPGRADE:
 +        pid = tl.program_id(axis=0).to(tl.int64)
 +        pid_z = tl.program_id(axis=1).to(tl.int64)
@@ -24799,12 +18250,25 @@ index f14200e02..1fd914b19 100644
      num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)
      num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
      num_pid_in_group = GROUP_SIZE_M * num_pid_n
-@@ -318,14 +350,23 @@ def fused_moe_kernel(
-         tl.int64)
+@@ -353,7 +384,14 @@ def fused_moe_kernel(
      offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)
      token_mask = offs_token < num_valid_tokens
+ 
+-    off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
++    if UPGRADE_B_OFFS:
++        off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
++    else:
++        off_experts = tl.load(expert_ids_ptr + pid_m)
++        
 +    if UPGRADE_A_OFFS:
 +        offs_token = offs_token.to(tl.int64)
++
+     if off_experts == -1:
+         # -----------------------------------------------------------
+         # Write back zeros to the output when the expert is not
+@@ -363,9 +401,16 @@ def fused_moe_kernel(
+                               BLOCK_SIZE_N, compute_type)
+         return
  
 -    offs_bn = (pid_n * BLOCK_SIZE_N +
 -               tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N
@@ -24815,47 +18279,44 @@ index f14200e02..1fd914b19 100644
 +    else:
 +        offs_bn = (pid_n * BLOCK_SIZE_N +
 +                   tl.arange(0, BLOCK_SIZE_N)) % N
++        
++    # offs_k = tl.arange(0, BLOCK_SIZE_K)
 +    offs_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)
++
      a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +
                        offs_k[None, :] * stride_ak)
  
--    off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
-+    if UPGRADE_B_OFFS:
-+        off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
-+    else:
-+        off_experts = tl.load(expert_ids_ptr + pid_m)
-     b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +
-                                                 offs_bn[None, :] * stride_bn)
-     if use_int8_w8a16:
-@@ -333,6 +374,12 @@ def fused_moe_kernel(
+@@ -376,7 +421,13 @@ def fused_moe_kernel(
              None, :] * stride_bsn
          b_scale = tl.load(b_scale_ptrs)
  
+-    if use_fp8_w8a8 or use_int8_w8a8:
 +    if use_int8_w8a8:  
 +        a_scale = tl.load(a_scale_ptr+(offs_token[:, None] // top_k * stride_asm),mask=token_mask[:, None],other=0.0)
 +        b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + offs_bn[
 +            None, :] * stride_bsn
 +        b_scale = tl.load(b_scale_ptrs)
 +
-     if use_fp8_w8a8:
++    if use_fp8_w8a8:
+         # block-wise
          if group_k > 0 and group_n > 0:
              a_scale_ptrs = a_scale_ptr + (offs_token // top_k) * stride_asm
-@@ -348,24 +395,28 @@ def fused_moe_kernel(
+@@ -402,23 +453,28 @@ def fused_moe_kernel(
      # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
      # of fp32 values for higher accuracy.
      # `accumulator` will be converted back to fp16 after the loop.
 -    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+-    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
 +    # accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
 +    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32 if use_int8_w8a8 else tl.float32)
- 
--    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
++
 +    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):
          # Load the next block of A and B, generate a mask by checking the
          # K dimension.
          a = tl.load(a_ptrs,
                      mask=token_mask[:, None] &
 -                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),
-+                   (offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K),
++                    (offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K),
                      other=0.0)
          b = tl.load(b_ptrs,
 -                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,
@@ -24864,82 +18325,118 @@ index f14200e02..1fd914b19 100644
          # We accumulate along the K dimension.
          if use_int8_w8a16:
              accumulator = tl.dot(a, b.to(compute_type), acc=accumulator)
+-        elif use_fp8_w8a8 or use_int8_w8a8:
 +        elif use_int8_w8a8:
 +            a = a.to(tl.int8)
 +            accumulator += tl.dot(a, b,out_dtype=accumulator.dtype)
-         elif use_fp8_w8a8:
++        elif use_fp8_w8a8:
              if group_k > 0 and group_n > 0:
 -                k_start = k * BLOCK_SIZE_K
 +                k_start = k * BLOCK_SIZE_K * SPLIT_K
                  offs_ks = k_start // group_k
                  a_scale = tl.load(a_scale_ptrs + offs_ks * stride_ask,
                                    mask=token_mask,
-@@ -379,8 +430,8 @@ def fused_moe_kernel(
+@@ -436,8 +492,8 @@ def fused_moe_kernel(
          else:
              accumulator += tl.dot(a, b)
          # Advance the ptrs to the next K block.
 -        a_ptrs += BLOCK_SIZE_K * stride_ak
 -        b_ptrs += BLOCK_SIZE_K * stride_bk
 +        a_ptrs += BLOCK_SIZE_K * stride_ak * SPLIT_K
-+        b_ptrs += BLOCK_SIZE_K * stride_bk* SPLIT_K
++        b_ptrs += BLOCK_SIZE_K * stride_bk * SPLIT_K
  
      if MUL_ROUTED_WEIGHT:
          moe_weight = tl.load(topk_weights_ptr + offs_token,
-@@ -389,6 +440,11 @@ def fused_moe_kernel(
+@@ -446,7 +502,15 @@ def fused_moe_kernel(
          accumulator = accumulator * moe_weight[:, None]
      if use_int8_w8a16:
          accumulator = (accumulator * b_scale).to(compute_type)
+-    elif use_fp8_w8a8 or use_int8_w8a8:
 +    elif use_int8_w8a8:
 +        accumulator = accumulator.to(tl.float32)
 +        accumulator = (accumulator * a_scale * b_scale)
 +        if not ACCF32:
-+            accumulator = accumulator.to(compute_type)
-     elif use_fp8_w8a8:
++            if FAST_F32_TO_BF16:
++                accumulator = accumulator.to(compute_type, "rtne_no_nan")
++            else:
++                accumulator = accumulator.to(compute_type)
++    elif use_fp8_w8a8:
          if group_k > 0 and group_n > 0:
              accumulator = accumulator.to(compute_type)
-@@ -402,7 +458,10 @@ def fused_moe_kernel(
+         else:
+@@ -459,8 +523,10 @@ def fused_moe_kernel(
      c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[
          None, :]
      c_mask = token_mask[:, None] & (offs_cn[None, :] < N)
 -    tl.store(c_ptrs, accumulator, mask=c_mask)
+-
 +    if SPLIT_K == 1:
 +        tl.store(c_ptrs, accumulator, mask=c_mask)
 +    else:
 +        tl.atomic_add(c_ptrs, accumulator, mask=c_mask)
  
- 
- def ceil_div(a, b):
-@@ -636,8 +695,10 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
-                             config: Dict[str, Any],
-                             compute_type: tl.dtype,
-                             use_fp8_w8a8: bool,
-+                            use_int8_w8a8: bool, 
+ def invoke_fused_moe_kernel(A: torch.Tensor,
+                             B: torch.Tensor,
+@@ -480,6 +546,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
+                             use_int8_w8a8: bool,
                              use_int8_w8a16: bool,
                              use_int4_w4a16: bool,
 +                            orig_acc_dtype: torch.dtype,
-                             block_shape: Optional[List[int]] = None) -> None:
-     assert topk_weights.stride(1) == 1
-     assert sorted_token_ids.stride(0) == 1
-@@ -653,6 +714,9 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
-             assert triton.cdiv(A.shape[-1], block_k) == A_scale.shape[-1]
-             assert triton.cdiv(B.shape[-2], block_n) == B_scale.shape[-2]
-             assert triton.cdiv(B.shape[-1], block_k) == B_scale.shape[-1]
-+    elif use_int8_w8a8:
-+        A, A_scale,_ = ops.scaled_int8_quant(A, A_scale)
-+        assert B_scale is not None
-     elif use_int8_w8a16 or use_int4_w4a16:
-         assert B_scale is not None
-         assert block_shape is None or block_shape[0] == 0
-@@ -669,7 +733,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
+                             per_channel_quant: bool,
+                             block_shape: Optional[list[int]] = None) -> None:
+     assert topk_weights is not None or not mul_routed_weight
+@@ -512,7 +579,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
          EM = min(sorted_token_ids.shape[0],
                   A.shape[0] * top_k * config['BLOCK_SIZE_M'])
      grid = lambda META: (triton.cdiv(EM, META['BLOCK_SIZE_M']) * triton.cdiv(
 -        B.shape[1], META['BLOCK_SIZE_N']), )
-+        B.shape[1], META['BLOCK_SIZE_N']), META['SPLIT_K'] )
++        B.shape[1], META['BLOCK_SIZE_N']), META['SPLIT_K'])
  
      if (use_int8_w8a16 or use_int4_w4a16) and \
              block_shape is not None and block_shape[1] > 0:
-@@ -703,7 +767,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
+@@ -524,26 +591,27 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
+             group_size=block_shape[1],
+             num_experts=B.shape[0],
+             bit=4 if use_int4_w4a16 else 8)
+-        config = config.copy()
+-        config.update(
+-            get_moe_wna16_block_config(config=config,
+-                                       use_moe_wna16_cuda=use_moe_wna16_cuda,
+-                                       num_valid_tokens=num_tokens,
+-                                       size_k=A.shape[1],
+-                                       size_n=B.shape[1],
+-                                       num_experts=B.shape[1],
+-                                       group_size=block_shape[1],
+-                                       real_top_k=top_k,
+-                                       block_size_m=config["BLOCK_SIZE_M"]))
+-
+-        if use_moe_wna16_cuda:
++        # TODO: missing config for BLOCK_SIZE_K
++        # config = config.copy()
++        # config.update(
++        #     get_moe_wna16_block_config(config=config,
++        #                                use_moe_wna16_cuda=use_moe_wna16_cuda,
++        #                                num_valid_tokens=num_tokens,
++        #                                size_k=A.shape[1],
++        #                                size_n=B.shape[1],
++        #                                num_experts=B.shape[1],
++        #                                group_size=block_shape[1],
++        #                                real_top_k=top_k,
++        #                                block_size_m=config["BLOCK_SIZE_M"]))
++
++        if False and use_moe_wna16_cuda:
+             bit = 4 if use_int4_w4a16 else 8
+             ops.moe_wna16_gemm(A, C, B, B_scale, B_zp,
+                                topk_weights if mul_routed_weight else None,
+                                sorted_token_ids, expert_ids,
+                                num_tokens_post_padded, top_k,
+                                config["BLOCK_SIZE_M"], config["BLOCK_SIZE_N"],
+-                               config["BLOCK_SIZE_K"], bit)
++                               config["BLOCK_SIZE_K"] * config["SPLIT_K"], bit)
+             return
+ 
+         fused_moe_kernel_gptq_awq[grid](
+@@ -573,7 +641,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
              B_zp.stride(0) if B_zp is not None else 0,
              B_zp.stride(2) if B_zp is not None else 0,
              B_zp.stride(1) if B_zp is not None else 0,
@@ -24948,140 +18445,216 @@ index f14200e02..1fd914b19 100644
              group_size=block_shape[1],
              MUL_ROUTED_WEIGHT=mul_routed_weight,
              top_k=top_k,
-@@ -750,11 +814,16 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
+@@ -599,6 +667,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
+             sorted_token_ids,
+             expert_ids,
+             num_tokens_post_padded,
++            B.shape[0],
+             B.shape[1],
+             B.shape[2],
+             EM,
+@@ -624,14 +693,18 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
              0 if block_shape is None else block_shape[1],
              MUL_ROUTED_WEIGHT=mul_routed_weight,
              top_k=top_k,
-+            experts_num=expert_ids.shape[0],
++            # experts_num=expert_ids.shape[0],
              compute_type=compute_type,
              use_fp8_w8a8=use_fp8_w8a8,
-+            use_int8_w8a8=use_int8_w8a8,
+             use_int8_w8a8=use_int8_w8a8,
              use_int8_w8a16=use_int8_w8a16,
+             per_channel_quant=per_channel_quant,
+             BLOCK_SIZE_K=BLOCK_SIZE_K,
++            FAST_F32_TO_BF16 = True,
              **config,
          )
 +    if config["ACCF32"]:
 +       C = C.to(orig_acc_dtype)
-+    return C
  
  
  # Adapted from: https://github.com/sgl-project/sglang/pull/2628
-@@ -763,6 +832,7 @@ def get_config_file_name(E: int,
+@@ -639,7 +712,8 @@ def get_config_file_name(E: int,
+                          N: int,
                           dtype: Optional[str],
-                          block_shape: Optional[List[int]] = None) -> str:
-     device_name = current_platform.get_device_name().replace(" ", "_")
+                          block_shape: Optional[list[int]] = None) -> str:
+-    device_name = current_platform.get_device_name().replace(" ", "_")
++    # device_name = current_platform.get_device_name().replace(" ", "_")
 +    device_name = "Device_4000"
      dtype_selector = "" if not dtype else f",dtype={dtype}"
      block_shape_selector = ("" if not block_shape or not all(block_shape) else
                              f",block_shape={block_shape}").replace(" ", "")
-@@ -974,14 +1044,17 @@ def grouped_topk(hidden_states: torch.Tensor,
- 
- def get_config_dtype_str(dtype: torch.dtype,
-                          use_int4_w4a16: Optional[bool] = False,
-+                         use_int8_w8a8: Optional[bool] = False,
-                          use_int8_w8a16: Optional[bool] = False,
-                          use_fp8_w8a8: Optional[bool] = False):
+@@ -654,6 +728,7 @@ def get_moe_configs(
+     dtype: Optional[str],
+     block_n: Optional[int] = None,
+     block_k: Optional[int] = None,
++    H: int = 0,
+ ) -> Optional[dict[int, Any]]:
+     """
+     Return optimized configurations for the fused MoE kernel.
+@@ -668,9 +743,17 @@ def get_moe_configs(
+     # directory
+     block_shape = [block_n, block_k] if block_n and block_k else None
+     json_file_name = get_config_file_name(E, N, dtype, block_shape)
++    json_file_name_new = f"H={H},{json_file_name}"
+ 
+     config_file_path = os.path.join(
+         os.path.dirname(os.path.realpath(__file__)), "configs", json_file_name)
++    config_file_path_new = os.path.join(
++        os.path.dirname(os.path.realpath(__file__)), "configs", json_file_name_new)
++
++    # First find H, E, N config file
++    if os.path.exists(config_file_path_new):
++        config_file_path = config_file_path_new
++
+     if os.path.exists(config_file_path):
+         with open(config_file_path) as f:
+             logger.info("Using configuration from %s for MoE layer.",
+@@ -773,21 +856,22 @@ def get_default_config(
+             "num_warps": 4,
+             "num_stages": 3 if not current_platform.is_rocm() else 2,
+         }
+-    elif dtype in ["int4_w4a16", "int8_w8a16"] and block_shape is not None:
+-        # moe wna16 kernels
+-        # only set BLOCK_SIZE_M
+-        # BLOCK_SIZE_N and BLOCK_SIZE_K would be set later
+-        bit = 4 if dtype == "int4_w4a16" else 8
+-        use_moe_wna16_cuda = should_moe_wna16_use_cuda(M * topk,
+-                                                       block_shape[1], E, bit)
+-        if use_moe_wna16_cuda:
+-            config = {"BLOCK_SIZE_M": min(16, M)}
+-        elif M <= 20:
+-            config = {"BLOCK_SIZE_M": 16, "GROUP_SIZE_M": 1}
+-        elif M <= 40:
+-            config = {"BLOCK_SIZE_M": 32, "GROUP_SIZE_M": 1}
+-        else:
+-            config = {"BLOCK_SIZE_M": 64, "GROUP_SIZE_M": 1}
++    # TODO: missing config for BLOCK_SIZE_K
++    # elif dtype in ["int4_w4a16", "int8_w8a16"] and block_shape is not None:
++    #     # moe wna16 kernels
++    #     # only set BLOCK_SIZE_M
++    #     # BLOCK_SIZE_N and BLOCK_SIZE_K would be set later
++    #     bit = 4 if dtype == "int4_w4a16" else 8
++    #     use_moe_wna16_cuda = should_moe_wna16_use_cuda(M * topk,
++    #                                                    block_shape[1], E, bit)
++    #     if use_moe_wna16_cuda:
++    #         config = {"BLOCK_SIZE_M": min(16, M)}
++    #     elif M <= 20:
++    #         config = {"BLOCK_SIZE_M": 16, "GROUP_SIZE_M": 1}
++    #     elif M <= 40:
++    #         config = {"BLOCK_SIZE_M": 32, "GROUP_SIZE_M": 1}
++    #     else:
++    #         config = {"BLOCK_SIZE_M": 64, "GROUP_SIZE_M": 1}
+     elif is_marlin:
+         for block_size_m in [8, 16, 32, 48, 64]:
+             if M * topk / E / block_size_m < 0.9:
+@@ -818,6 +902,7 @@ def try_get_optimal_moe_config(
+     M: int,
+     is_marlin: bool = False,
+     block_shape: Optional[list[int]] = None,
++    H: int = 0,
+ ):
+     from vllm.model_executor.layers.fused_moe import get_config
+     override_config = get_config()
+@@ -826,11 +911,12 @@ def try_get_optimal_moe_config(
+     else:
+         # First try to load optimal config from the file
+         E, _, N = w2_shape
+-        if dtype == "int4_w4a16":
+-            N = N * 2
++        # TODO: why we need N * 2
++        # if dtype == "int4_w4a16":
++        #     N = N * 2
+         block_n = block_shape[0] if block_shape else 0
+         block_k = block_shape[1] if block_shape else 0
+-        configs = get_moe_configs(E, N, dtype, block_n, block_k)
++        configs = get_moe_configs(E, N, dtype, block_n, block_k, H)
+ 
+         if configs:
+             # If an optimal configuration map has been found, look up the
+@@ -965,10 +1051,13 @@ def grouped_topk(
+ def get_config_dtype_str(
+         dtype: torch.dtype,
+         use_int4_w4a16: Optional[bool] = False,
++        use_int8_w8a8: Optional[bool] = False,
+         use_int8_w8a16: Optional[bool] = False,
+         use_fp8_w8a8: Optional[bool] = False) -> Optional[str]:
      if use_fp8_w8a8:
          return "fp8_w8a8"
-     elif use_int8_w8a16:
-         return "int8_w8a16"
 +    elif use_int8_w8a8:
 +        return "int8_w8a8"
+     elif use_int8_w8a16:
+         return "int8_w8a16"
      elif use_int4_w4a16:
--        return "int4_w8a16"
-+        return "int4_w4a16"
-     elif dtype == torch.float:
-         # avoiding cases where kernel fails when float32 MoE
-         # use fp16/bfloat16 configs
-@@ -995,6 +1068,7 @@ def inplace_fused_experts(hidden_states: torch.Tensor,
-                           topk_weights: torch.Tensor,
-                           topk_ids: torch.Tensor,
-                           use_fp8_w8a8: bool = False,
-+                          use_int8_w8a8: bool = False, 
-                           use_int8_w8a16: bool = False,
-                           use_int4_w4a16: bool = False,
-                           w1_scale: Optional[torch.Tensor] = None,
-@@ -1005,7 +1079,7 @@ def inplace_fused_experts(hidden_states: torch.Tensor,
+@@ -1014,7 +1103,7 @@ def inplace_fused_experts(hidden_states: torch.Tensor,
+                           w2_zp: Optional[torch.Tensor] = None,
+                           a1_scale: Optional[torch.Tensor] = None,
                            a2_scale: Optional[torch.Tensor] = None,
-                           block_shape: Optional[List[int]] = None) -> None:
+-                          block_shape: Optional[list[int]] = None) -> None:
++                          block_shape: Optional[List[int]] = None) -> None:
      fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,
--                       use_fp8_w8a8, use_int8_w8a16, use_int4_w4a16, w1_scale,
-+                       use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, w1_scale,
-                        w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape)
- 
- 
-@@ -1043,6 +1117,7 @@ def outplace_fused_experts(
-         topk_weights: torch.Tensor,
-         topk_ids: torch.Tensor,
-         use_fp8_w8a8: bool = False,
-+        use_int8_w8a8: bool = False,
-         use_int8_w8a16: bool = False,
-         use_int4_w4a16: bool = False,
-         w1_scale: Optional[torch.Tensor] = None,
-@@ -1053,7 +1128,7 @@ def outplace_fused_experts(
+                        activation, apply_router_weight_on_input, use_fp8_w8a8,
+                        use_int8_w8a8, use_int8_w8a16, use_int4_w4a16,
+@@ -1044,7 +1133,7 @@ def inplace_fused_experts_fake(
+         w2_zp: Optional[torch.Tensor] = None,
+         a1_scale: Optional[torch.Tensor] = None,
          a2_scale: Optional[torch.Tensor] = None,
-         block_shape: Optional[List[int]] = None) -> torch.Tensor:
+-        block_shape: Optional[list[int]] = None) -> None:
++        block_shape: Optional[List[int]] = None) -> None:
+     pass
+ 
+ 
+@@ -1078,7 +1167,7 @@ def outplace_fused_experts(
+         w2_zp: Optional[torch.Tensor] = None,
+         a1_scale: Optional[torch.Tensor] = None,
+         a2_scale: Optional[torch.Tensor] = None,
+-        block_shape: Optional[list[int]] = None) -> torch.Tensor:
++        block_shape: Optional[List[int]] = None) -> torch.Tensor:
      return fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids,
--                              False, use_fp8_w8a8, use_int8_w8a16,
-+                              False, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16,
-                               use_int4_w4a16, w1_scale, w2_scale, w1_zp, w2_zp,
-                               a1_scale, a2_scale, block_shape)
- 
-@@ -1092,6 +1167,7 @@ def fused_experts(hidden_states: torch.Tensor,
-                   topk_ids: torch.Tensor,
-                   inplace: bool = False,
-                   use_fp8_w8a8: bool = False,
-+                  use_int8_w8a8: bool = False, 
-                   use_int8_w8a16: bool = False,
-                   use_int4_w4a16: bool = False,
-                   w1_scale: Optional[torch.Tensor] = None,
-@@ -1102,16 +1178,18 @@ def fused_experts(hidden_states: torch.Tensor,
-                   a2_scale: Optional[torch.Tensor] = None,
-                   block_shape: Optional[List[int]] = None):
-     if inplace:
--        torch.ops.vllm.inplace_fused_experts(hidden_states, w1, w2,
-+        #torch.ops.vllm.inplace_fused_experts(hidden_states, w1, w2,
-+        inplace_fused_experts(hidden_states, w1, w2,
-                                              topk_weights, topk_ids,
--                                             use_fp8_w8a8, use_int8_w8a16,
-+                                             use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16,
-                                              use_int4_w4a16, w1_scale,
-                                              w2_scale, w1_zp, w2_zp, a1_scale,
-                                              a2_scale, block_shape)
-         return hidden_states
-     else:
--        return torch.ops.vllm.outplace_fused_experts(
--            hidden_states, w1, w2, topk_weights, topk_ids, use_fp8_w8a8,
-+        #return torch.ops.vllm.outplace_fused_experts(
-+        return outplace_fused_experts(
-+            hidden_states, w1, w2, topk_weights, topk_ids, use_fp8_w8a8, use_int8_w8a8,
-             use_int8_w8a16, use_int4_w4a16, w1_scale, w2_scale, w1_zp, w2_zp,
-             a1_scale, a2_scale, block_shape)
- 
-@@ -1123,6 +1201,7 @@ def fused_experts_impl(hidden_states: torch.Tensor,
-                        topk_ids: torch.Tensor,
-                        inplace: bool = False,
-                        use_fp8_w8a8: bool = False,
-+                       use_int8_w8a8: bool = False,
-                        use_int8_w8a16: bool = False,
-                        use_int4_w4a16: bool = False,
-                        w1_scale: Optional[torch.Tensor] = None,
-@@ -1154,6 +1233,7 @@ def fused_experts_impl(hidden_states: torch.Tensor,
+                               False, activation, apply_router_weight_on_input,
+                               use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16,
+@@ -1108,7 +1197,7 @@ def outplace_fused_experts_fake(
+         w2_zp: Optional[torch.Tensor] = None,
+         a1_scale: Optional[torch.Tensor] = None,
+         a2_scale: Optional[torch.Tensor] = None,
+-        block_shape: Optional[list[int]] = None) -> torch.Tensor:
++        block_shape: Optional[List[int]] = None) -> torch.Tensor:
+     return torch.empty_like(hidden_states)
+ 
+ 
+@@ -1247,6 +1336,7 @@ def fused_experts_impl(
+         torch.float32, torch.float16, torch.bfloat16
+     ]
+ 
++    H = hidden_states.shape[-1]
+     num_tokens = hidden_states.shape[0]
+     E, N, _ = w1.shape
+     K = w2.shape[1]
+@@ -1258,6 +1348,7 @@ def fused_experts_impl(
      CHUNK_SIZE = envs.VLLM_FUSED_MOE_CHUNK_SIZE
      M = min(num_tokens, CHUNK_SIZE)
      config_dtype = get_config_dtype_str(use_fp8_w8a8=use_fp8_w8a8,
-+                                        use_int8_w8a8=use_int8_w8a8, 
++                                        use_int8_w8a8=use_int8_w8a8,
                                          use_int8_w8a16=use_int8_w8a16,
                                          use_int4_w4a16=use_int4_w4a16,
                                          dtype=hidden_states.dtype)
-@@ -1169,15 +1249,44 @@ def fused_experts_impl(hidden_states: torch.Tensor,
+@@ -1274,17 +1365,52 @@ def fused_experts_impl(
+         top_k_num,
+         config_dtype,
+         block_shape=block_shape,
++        H=H,
+     )
  
      config = get_config_func(M)
  
--    intermediate_cache1 = torch.empty((M, topk_ids.shape[1], N),
--                                      device=hidden_states.device,
--                                      dtype=hidden_states.dtype)
+     # We can reuse the memory between these because by the time we need
+     # cache3, we're done with cache1
+-    cache13 = torch.empty(M * top_k_num * max(N, K),
+-                          device=hidden_states.device,
+-                          dtype=hidden_states.dtype)
+-    intermediate_cache1 = cache13[:M * top_k_num * N].view(M, top_k_num, N)
+-    intermediate_cache3 = cache13[:M * top_k_num * K].view(M, top_k_num, K)
 +    stage1_config = config["stage1"] if "stage1" in config else config
 +    stage2_config = config["stage2"] if "stage2" in config else config
++    
 +    if 'ACCF32' not in stage1_config:
 +        stage1_config['ACCF32'] = False
 +    if 'ACCF32' not in stage2_config:
@@ -25089,7 +18662,7 @@ index f14200e02..1fd914b19 100644
 +    if 'SPLIT_K' not in stage1_config:
 +        stage1_config['SPLIT_K'] = 1
 +    if 'SPLIT_K' not in stage2_config:
-+        stage2_config['SPLIT_K'] = 1
++        stage2_config['SPLIT_K'] = 1    
 +
 +    if stage1_config['ACCF32']:
 +       acc_type1 = torch.float32
@@ -25099,6 +18672,8 @@ index f14200e02..1fd914b19 100644
 +       acc_type2 = torch.float32
 +    else:
 +       acc_type2 = hidden_states.dtype
++       
++
 +    if stage1_config['SPLIT_K'] > 1:
 +        intermediate_cache1 = torch.zeros((M, topk_ids.shape[1], N),
 +                                          device=hidden_states.device,
@@ -25107,12 +18682,7 @@ index f14200e02..1fd914b19 100644
 +        intermediate_cache1 = torch.empty((M, topk_ids.shape[1], N),
 +                                          device=hidden_states.device,
 +                                          dtype=hidden_states.dtype)
-     intermediate_cache2 = torch.empty((M * topk_ids.shape[1], N // 2),
-                                       device=hidden_states.device,
-                                       dtype=hidden_states.dtype)
--    intermediate_cache3 = torch.empty((M, topk_ids.shape[1], w2.shape[1]),
--                                      device=hidden_states.device,
--                                      dtype=hidden_states.dtype)
++        
 +    if stage2_config['SPLIT_K'] > 1:
 +        intermediate_cache3 = torch.zeros((M, topk_ids.shape[1], w2.shape[1]),
 +                                          device=hidden_states.device,
@@ -25122,37 +18692,25 @@ index f14200e02..1fd914b19 100644
 +                                          device=hidden_states.device,
 +                                          dtype=hidden_states.dtype)
  
-     if hidden_states.dtype == torch.bfloat16:
-         compute_type = tl.bfloat16
-@@ -1216,51 +1325,71 @@ def fused_experts_impl(hidden_states: torch.Tensor,
+     # This needs separate memory since it's used concurrently with cache1
+     intermediate_cache2 = torch.empty((M * top_k_num, N // 2),
+@@ -1329,18 +1455,30 @@ def fused_experts_impl(
          curr_topk_ids = topk_ids[begin_chunk_idx:end_chunk_idx]
          curr_topk_weights = topk_weights[begin_chunk_idx:end_chunk_idx]
  
-+       
-         sorted_token_ids, expert_ids, num_tokens_post_padded = (
--            moe_align_block_size(curr_topk_ids, config['BLOCK_SIZE_M'], E))
+-        qcurr_hidden_states, a1q_scale = moe_kernel_quantize_input(
+-            A=curr_hidden_states,
+-            A_scale=a1_scale,
+-            qtype=qtype,
+-            per_channel_quant=per_channel_quant,
+-            block_shape=block_shape)
 -
--        invoke_fused_moe_kernel(curr_hidden_states,
--                                w1,
--                                intermediate_cache1,
--                                a1_scale,
--                                w1_scale,
--                                w1_zp,
--                                curr_topk_weights,
--                                curr_topk_ids,
--                                sorted_token_ids,
--                                expert_ids,
--                                num_tokens_post_padded,
--                                False,
--                                topk_ids.shape[1],
--                                config,
--                                compute_type=compute_type,
--                                use_fp8_w8a8=use_fp8_w8a8,
--                                use_int8_w8a16=use_int8_w8a16,
--                                use_int4_w4a16=use_int4_w4a16,
--                                block_shape=block_shape)
-+            moe_align_block_size(curr_topk_ids, stage1_config['BLOCK_SIZE_M'], E))
-+
+         sorted_token_ids, expert_ids, num_tokens_post_padded = (
+-            moe_align_block_size(curr_topk_ids, config['BLOCK_SIZE_M'],
++            moe_align_block_size(curr_topk_ids, stage1_config['BLOCK_SIZE_M'],
+                                  global_num_experts, expert_map))
+ 
+-        invoke_fused_moe_kernel(qcurr_hidden_states,
 +        if (stage1_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and (topk_ids.shape[1] == 1 or topk_ids.shape[1] == 2) and
 +            (curr_hidden_states.dtype == torch.bfloat16 or curr_hidden_states.dtype == torch.float16) and
 +            w1.shape[1] % 4 == 0 and w1.shape[2] % 8 == 0):
@@ -25160,2559 +18718,443 @@ index f14200e02..1fd914b19 100644
 +                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
 +                                expert_ids, num_tokens_post_padded, False,
 +                                topk_ids.shape[1], 0)
-+        
 +        else:
-+            intermediate_cache1 = invoke_fused_moe_kernel(curr_hidden_states,
-+                                                          w1,
-+                                                          intermediate_cache1,
-+                                                          a1_scale,
-+                                                          w1_scale,
-+                                                          w1_zp,
-+                                                          curr_topk_weights,
-+                                                          curr_topk_ids,
-+                                                          sorted_token_ids,
-+                                                          expert_ids,
-+                                                          num_tokens_post_padded,
-+                                                          False,
-+                                                          topk_ids.shape[1],
-+                                                          stage1_config,
-+                                                          compute_type=compute_type,
-+                                                          use_fp8_w8a8=use_fp8_w8a8,
-+                                                          use_int8_w8a8=use_int8_w8a8,
-+                                                          use_int8_w8a16=use_int8_w8a16,
-+                                                          use_int4_w4a16=use_int4_w4a16,
-+                                                          orig_acc_dtype=hidden_states.dtype,
-+                                                          block_shape=block_shape)
- 
-         torch.ops._C.silu_and_mul(intermediate_cache2,
-                                   intermediate_cache1.view(-1, N))
- 
--        invoke_fused_moe_kernel(intermediate_cache2,
--                                w2,
--                                intermediate_cache3,
--                                a2_scale,
--                                w2_scale,
--                                w2_zp,
--                                curr_topk_weights,
--                                curr_topk_ids,
--                                sorted_token_ids,
--                                expert_ids,
--                                num_tokens_post_padded,
--                                True,
--                                1,
++            if stage2_config['BLOCK_SIZE_M'] != stage1_config['BLOCK_SIZE_M']:
++                sorted_token_ids, expert_ids, num_tokens_post_padded = (
++                moe_align_block_size(curr_topk_ids, stage2_config['BLOCK_SIZE_M'], global_num_experts, expert_map))
++
++            qcurr_hidden_states, a1q_scale = moe_kernel_quantize_input(
++                A=curr_hidden_states,
++                A_scale=a1_scale,
++                qtype=qtype,
++                per_channel_quant=per_channel_quant,
++                block_shape=block_shape)
++            
++            invoke_fused_moe_kernel(qcurr_hidden_states,
+                                 w1,
+                                 intermediate_cache1,
+                                 a1q_scale,
+@@ -1352,12 +1490,13 @@ def fused_experts_impl(
+                                 num_tokens_post_padded,
+                                 apply_router_weight_on_input,
+                                 top_k_num,
 -                                config,
--                                compute_type=compute_type,
--                                use_fp8_w8a8=use_fp8_w8a8,
--                                use_int8_w8a16=use_int8_w8a16,
--                                use_int4_w4a16=use_int4_w4a16,
--                                block_shape=block_shape)
-+        if (stage2_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and w2.shape[1] % 4 == 0 and w2.shape[2] % 8 == 0 and
-+            (hidden_states.dtype == torch.bfloat16 or hidden_states.dtype == torch.float16)):
-+            ops.fused_moe_kernel(intermediate_cache2, w2, intermediate_cache3,
-+                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
-+                                expert_ids, num_tokens_post_padded, True, 1, 0)
-+        else:
-+            intermediate_cache3 = invoke_fused_moe_kernel(intermediate_cache2,
-+                                                          w2,
-+                                                          intermediate_cache3,
-+                                                          a2_scale,
-+                                                          w2_scale,
-+                                                          w2_zp,
-+                                                          curr_topk_weights,
-+                                                          curr_topk_ids,
-+                                                          sorted_token_ids,
-+                                                          expert_ids,
-+                                                          num_tokens_post_padded,
-+                                                          True,
-+                                                          1,
-+                                                          stage2_config,
-+                                                          compute_type=compute_type,
-+                                                          use_fp8_w8a8=use_fp8_w8a8,
-+                                                          use_int8_w8a8=use_int8_w8a8,
-+                                                          use_int8_w8a16=use_int8_w8a16,
-+                                                          use_int4_w4a16=use_int4_w4a16,
-+                                                          orig_acc_dtype=hidden_states.dtype,
-+                                                          block_shape=block_shape)
- 
-         ops.moe_sum(intermediate_cache3.view(*intermediate_cache3.shape),
-                     out_hidden_states[begin_chunk_idx:end_chunk_idx])
-@@ -1280,6 +1409,7 @@ def fused_moe(
-     topk_group: Optional[int] = None,
-     custom_routing_function: Optional[Callable] = None,
-     use_fp8_w8a8: bool = False,
-+    use_int8_w8a8: bool = False,
-     use_int8_w8a16: bool = False,
-     use_int4_w4a16: bool = False,
-     w1_scale: Optional[torch.Tensor] = None,
-@@ -1310,7 +1440,9 @@ def fused_moe(
-         note: Deepseekv2 model uses grouped_topk
-     - use_fp8_w8a8 (bool): If True, use fp8 arithmetic to compute the inner
-         products for w1 and w2. Defaults to False.
--    - use_int8_w8a16 (bool): If True, use matmul of int8 weight and bf16/fp16
-+    - use_int8_w8a8 (bool): If True, use weight int8, activation int8 arithmetic to compute the inner
-+        products for w1 and w2. Defaults to False.
-+    - use_int8_w8a16 (bool): If True, use weight int8, activation int16 arithmetic to compute the inner
-         activation to compute the inner products for w1 and w2.
-         Defaults to False.
-     - use_int4_w4a16 (bool): If True, use matmul of int4 weight and bf16/fp16
-@@ -1352,6 +1484,7 @@ def fused_moe(
-                          topk_ids,
-                          inplace=inplace,
-                          use_fp8_w8a8=use_fp8_w8a8,
-+                         use_int8_w8a8=use_int8_w8a8,
-                          use_int8_w8a16=use_int8_w8a16,
-                          use_int4_w4a16=use_int4_w4a16,
-                          w1_scale=w1_scale,
-diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
-index da8db08fe..94499f673 100644
---- a/vllm/model_executor/layers/linear.py
-+++ b/vllm/model_executor/layers/linear.py
-@@ -1,1159 +1,1202 @@
--# SPDX-License-Identifier: Apache-2.0
--
--import itertools
--from abc import abstractmethod
--from typing import Optional
--
--import torch
--import torch.nn.functional as F
--from torch.nn.parameter import Parameter, UninitializedParameter
--
--from vllm.distributed import (divide, get_tensor_model_parallel_rank,
--                              get_tensor_model_parallel_world_size,
--                              split_tensor_along_last_dim,
--                              tensor_model_parallel_all_gather,
--                              tensor_model_parallel_all_reduce)
--from vllm.logger import init_logger
--from vllm.model_executor.layers.quantization.base_config import (
--    QuantizationConfig, QuantizeMethodBase)
--# yapf: disable
--from vllm.model_executor.parameter import (BasevLLMParameter,
--                                           BlockQuantScaleParameter,
--                                           PackedColumnParameter,
--                                           PackedvLLMParameter,
--                                           PerTensorScaleParameter,
--                                           RowvLLMParameter)
--# yapf: enable
--from vllm.model_executor.utils import set_weight_attrs
--
--logger = init_logger(__name__)
--
--WEIGHT_LOADER_V2_SUPPORTED = [
--    "CompressedTensorsLinearMethod", "AWQMarlinLinearMethod",
--    "AWQLinearMethod", "GPTQMarlinLinearMethod", "Fp8LinearMethod",
--    "MarlinLinearMethod", "QQQLinearMethod", "GPTQMarlin24LinearMethod",
--    "TPUInt8LinearMethod", "GPTQLinearMethod", "FBGEMMFp8LinearMethod",
--    "ModelOptFp8LinearMethod", "IPEXAWQLinearMethod", "IPEXGPTQLinearMethod",
--    "HQQMarlinMethod", "QuarkLinearMethod"
--]
--
--
--def adjust_marlin_shard(param, shard_size, shard_offset):
--    marlin_tile_size = getattr(param, "marlin_tile_size", None)
--    if marlin_tile_size is None:
--        return shard_size, shard_offset
--
--    return shard_size * marlin_tile_size, shard_offset * marlin_tile_size
--
--
--def adjust_bitsandbytes_4bit_shard(param: Parameter,
--                                   shard_offsets: dict[str, tuple[int, int]],
--                                   loaded_shard_id: str) -> tuple[int, int]:
--    """Adjust the quantization offsets and sizes for BitsAndBytes sharding."""
--
--    total, _ = shard_offsets["total"]
--    orig_offset, orig_size = shard_offsets[loaded_shard_id]
--
--    quantized_total = param.data.shape[0]
--    quantized_offset = orig_offset * quantized_total // total
--    quantized_size = orig_size * quantized_total // total
--
--    return quantized_size, quantized_offset
--
--
--def adjust_scalar_to_fused_array(param, loaded_weight, shard_id):
--    """For fused modules (QKV and MLP) we have an array of length
--    N that holds 1 scale for each "logical" matrix. So the param
--    is an array of length N. The loaded_weight corresponds to 
--    one of the shards on disk. Here, we slice the param based on 
--    the shard_id for loading.
--    """
--    qkv_idxs = {"q": 0, "k": 1, "v": 2}
--
--    if isinstance(shard_id, str):
--        shard_id = qkv_idxs[shard_id]
--    elif not isinstance(shard_id, int):
--        raise ValueError(f"Unknown Shard Id {shard_id}")
--
--    # AutoFP8 scales do not have a shape
--    # compressed-tensors scales do have a shape
--    if len(loaded_weight.shape) != 0:
--        assert loaded_weight.shape[0] == 1
--        loaded_weight = loaded_weight[0]
--
--    return param[shard_id], loaded_weight
--
--
--class LinearMethodBase(QuantizeMethodBase):
--    """Base class for different (maybe quantized) linear methods."""
--
--    @abstractmethod
--    def create_weights(self, layer: torch.nn.Module,
--                       input_size_per_partition: int,
--                       output_partition_sizes: list[int], input_size: int,
--                       output_size: int, params_dtype: torch.dtype,
--                       **extra_weight_attrs):
--        """Create weights for a linear layer. 
--           The weights will be set as attributes of the layer.
--
--        Args:
--            layer: The layer that is using the LinearMethodBase factory.
--            input_size_per_partition: Size of the weight input dim on rank X.
--            output_partition_sizes: Sizes of the output dim of each logical 
--                weight on rank X. E.g., output_partition_sizes for QKVLinear
--                is a list contains the width of Wq, Wk, Wv on rank X.
--            input_size: Size of the input dim of the weight across all ranks.
--            output_size: Size of the output dim of the weight across all ranks.
--            params_dtype: Datatype of the parameters.
--        """
--        raise NotImplementedError
--
--    @abstractmethod
--    def apply(self,
--              layer: torch.nn.Module,
--              x: torch.Tensor,
--              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
--        """Apply the weights in layer to the input tensor.
--        Expects create_weights to have been called before on the layer."""
--        raise NotImplementedError
--
--
--class UnquantizedLinearMethod(LinearMethodBase):
--    """Linear method without quantization."""
++                                stage1_config,
+                                 compute_type=compute_type,
+                                 use_fp8_w8a8=use_fp8_w8a8,
+                                 use_int8_w8a8=use_int8_w8a8,
+                                 use_int8_w8a16=use_int8_w8a16,
+                                 use_int4_w4a16=use_int4_w4a16,
++                                orig_acc_dtype=hidden_states.dtype,
+                                 per_channel_quant=per_channel_quant,
+                                 block_shape=block_shape)
+ 
+@@ -1370,14 +1509,21 @@ def fused_experts_impl(
+         else:
+             raise ValueError(f"Unsupported FusedMoe activation: {activation}")
+ 
+-        qintermediate_cache2, a2q_scale = moe_kernel_quantize_input(
+-            A=intermediate_cache2,
+-            A_scale=a2_scale,
+-            qtype=qtype,
+-            per_channel_quant=per_channel_quant,
+-            block_shape=block_shape)
 -
--    def create_weights(self, layer: torch.nn.Module,
--                       input_size_per_partition: int,
--                       output_partition_sizes: list[int], input_size: int,
--                       output_size: int, params_dtype: torch.dtype,
--                       **extra_weight_attrs):
+-        invoke_fused_moe_kernel(qintermediate_cache2,
++        
++        if (stage2_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and w2.shape[1] % 4 == 0 and w2.shape[2] % 8 == 0 and
++            (hidden_states.dtype == torch.bfloat16 or hidden_states.dtype == torch.float16)):
++            ops.fused_moe_kernel(intermediate_cache2, w2, intermediate_cache3,
++                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
++                                expert_ids, num_tokens_post_padded, True, 1, 0)
++        else:
++            qintermediate_cache2, a2q_scale = moe_kernel_quantize_input(
++                A=intermediate_cache2,
++                A_scale=a2_scale,
++                qtype=qtype,
++                per_channel_quant=per_channel_quant,
++                block_shape=block_shape)
++
++            invoke_fused_moe_kernel(qintermediate_cache2,
+                                 w2,
+                                 intermediate_cache3,
+                                 a2q_scale,
+@@ -1389,12 +1535,13 @@ def fused_experts_impl(
+                                 num_tokens_post_padded,
+                                 not apply_router_weight_on_input,
+                                 1,
+-                                config,
++                                stage2_config,
+                                 compute_type=compute_type,
+                                 use_fp8_w8a8=use_fp8_w8a8,
+                                 use_int8_w8a8=use_int8_w8a8,
+                                 use_int8_w8a16=use_int8_w8a16,
+                                 use_int4_w4a16=use_int4_w4a16,
++                                orig_acc_dtype=hidden_states.dtype,
+                                 per_channel_quant=per_channel_quant,
+                                 block_shape=block_shape)
+ 
+diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
+index cf8e4ee65..074e690e7 100644
+--- a/vllm/model_executor/layers/fused_moe/layer.py
++++ b/vllm/model_executor/layers/fused_moe/layer.py
+@@ -35,6 +35,12 @@ from vllm.utils import direct_register_custom_op
+ has_pplx = importlib.util.find_spec("pplx_kernels") is not None
+ has_deepep = importlib.util.find_spec("deep_ep") is not None
+ 
++if has_deepep:
++    try:
++        import deep_ep
++    except ImportError:
++        has_deepep = False
++
+ if current_platform.is_cuda_alike():
+     from .fused_batched_moe import BatchedTritonExperts
+     from .fused_moe import TritonExperts, fused_experts
+diff --git a/vllm/model_executor/layers/fused_moe/utils.py b/vllm/model_executor/layers/fused_moe/utils.py
+index 692482c2e..cb49594f0 100644
+--- a/vllm/model_executor/layers/fused_moe/utils.py
++++ b/vllm/model_executor/layers/fused_moe/utils.py
+@@ -62,7 +62,8 @@ def _int8_quantize(
+     if block_shape is None:
+         assert per_act_token, \
+             "int8 quantization only supports block or channel-wise"
+-        A, A_scale = per_token_quant_int8(A)
++        # A, A_scale = per_token_quant_int8(A)
++        A, A_scale, _ = ops.scaled_int8_quant(A, A_scale)
+     else:
+         assert len(block_shape) == 2
+         _, block_k = block_shape[0], block_shape[1]
+diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
+index 588aa8deb..cdb863ac4 100644
+--- a/vllm/model_executor/layers/linear.py
++++ b/vllm/model_executor/layers/linear.py
+@@ -28,6 +28,8 @@ from vllm.model_executor.parameter import (BasevLLMParameter,
+ # yapf: enable
+ from vllm.model_executor.utils import set_weight_attrs
+ 
++import vllm.envs as envs
++
+ logger = init_logger(__name__)
+ 
+ WEIGHT_LOADER_V2_SUPPORTED = [
+@@ -105,7 +107,11 @@ def adjust_scalar_to_fused_array(param, loaded_weight, shard_id):
+         assert loaded_weight.shape[0] == 1
+         loaded_weight = loaded_weight[0]
+ 
+-    return param[shard_id], loaded_weight
++    # Support gemm_tn->gemm_nn
++    if envs.MACA_VLLM_USE_TN_2_NN:
++        return param[shard_id], loaded_weight.t()
++    else:
++        return param[shard_id], loaded_weight
+ 
+ 
+ # TODO(Isotr0py): We might need a more flexible structure to handle
+@@ -187,10 +193,17 @@ class UnquantizedLinearMethod(LinearMethodBase):
+                        output_partition_sizes: list[int], input_size: int,
+                        output_size: int, params_dtype: torch.dtype,
+                        **extra_weight_attrs):
 -        weight = Parameter(torch.empty(sum(output_partition_sizes),
 -                                       input_size_per_partition,
--                                       dtype=params_dtype),
--                           requires_grad=False)
--        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
--        layer.register_parameter("weight", weight)
--        set_weight_attrs(weight, extra_weight_attrs)
--
--    def apply(self,
--              layer: torch.nn.Module,
--              x: torch.Tensor,
--              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
--
--        return F.linear(x, layer.weight, bias)
--
--
--class LinearBase(torch.nn.Module):
--    """Base linear layer.
--
--    Args:
--        input_size: input dimension of the linear layer.
--        output_size: output dimension of the linear layer.
--        bias: If true, add bias.
--        skip_bias_add: If true, skip adding bias but instead return it.
--        params_dtype: Data type for the parameters.
--        quant_config: Quantization configure.
--    """
--
--    def __init__(
--        self,
--        input_size: int,
--        output_size: int,
--        skip_bias_add: bool = False,
--        params_dtype: Optional[torch.dtype] = None,
--        quant_config: Optional[QuantizationConfig] = None,
--        prefix: str = "",
--    ):
--        super().__init__()
--
--        # Keep input parameters
--        self.input_size = input_size
--        self.output_size = output_size
--        self.skip_bias_add = skip_bias_add
--        if params_dtype is None:
--            params_dtype = torch.get_default_dtype()
--        self.params_dtype = params_dtype
--        if quant_config is None:
--            self.quant_method: Optional[
--                QuantizeMethodBase] = UnquantizedLinearMethod()
--        else:
--            self.quant_method = quant_config.get_quant_method(self,
--                                                              prefix=prefix)
--
--    def forward(self,
--                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
--        raise NotImplementedError
--
--
--class ReplicatedLinear(LinearBase):
--    """Replicated linear layer.
--
--    Args:
--        input_size: input dimension of the linear layer.
--        output_size: output dimension of the linear layer.
--        bias: If true, add bias.
--        skip_bias_add: If true, skip adding bias but instead return it.
--        params_dtype: Data type for the parameters.
--        quant_config: Quantization configure.
--        prefix: The name of the layer in the state dict, including all parents
--                        (e.g. model.layers.0.qkv_proj)
--    """
--
--    def __init__(self,
--                 input_size: int,
--                 output_size: int,
--                 bias: bool = True,
--                 skip_bias_add: bool = False,
--                 params_dtype: Optional[torch.dtype] = None,
--                 quant_config: Optional[QuantizationConfig] = None,
--                 prefix: str = ""):
--        super().__init__(input_size,
--                         output_size,
--                         skip_bias_add,
--                         params_dtype,
--                         quant_config,
--                         prefix=prefix)
--
--        # All the linear layer supports quant method.
--        assert self.quant_method is not None
--        self.quant_method.create_weights(self,
--                                         self.input_size, [self.output_size],
--                                         self.input_size,
--                                         self.output_size,
--                                         self.params_dtype,
--                                         weight_loader=self.weight_loader)
--
--        if bias:
--            self.bias = Parameter(
--                torch.empty(self.output_size, dtype=self.params_dtype))
--            set_weight_attrs(self.bias, {
--                "output_dim": 0,
--                "weight_loader": self.weight_loader,
--            })
--        else:
--            self.register_parameter("bias", None)
--
--    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
--        # If the weight on disk does not have a shape, give it one
--        # (such scales for AutoFp8).
--        if len(loaded_weight.shape) == 0:
--            loaded_weight = loaded_weight.reshape(1)
--
--        assert param.size() == loaded_weight.size()
--        param.data.copy_(loaded_weight)
--
--    def forward(self,
--                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
--        bias = self.bias if not self.skip_bias_add else None
--        assert self.quant_method is not None
--        output = self.quant_method.apply(self, x, bias)
--        output_bias = self.bias if self.skip_bias_add else None
--        return output, output_bias
--
--    def extra_repr(self) -> str:
--        s = f"in_features={self.input_size}"
--        s += f", output_features={self.output_size}"
--        s += f", bias={self.bias is not None}"
--        return s
--
--
--class ColumnParallelLinear(LinearBase):
--    """Linear layer with column parallelism.
--
--    The linear layer is defined as Y = XA + b. A is parallelized along
--    its second dimension as A = [A_1, ..., A_p].
--
--    Args:
--        input_size: first dimension of matrix A.
--        output_size: second dimension of matrix A.
--        bias: If true, add bias.
--        gather_output: If true, call all-gather on output and make Y available
--                       to all GPUs, otherwise, every GPU will have its output
--                       which is Y_i = XA_i
--        skip_bias_add: This was added to enable performance optimizations where
--                       bias can be fused with other element-wise operations. we
--                       skip adding bias but instead return it.
--        params_dtype: Data type for the parameters.
--        quant_config: Quantization configure.
--        output_sizes: list of output sizes packed into one output, like for QKV
--                       the list would be size 3.
--        prefix: The name of the layer in the state dict, including all parents
--                        (e.g. model.layers.0.qkv_proj) 
--    """
--
--    def __init__(self,
--                 input_size: int,
--                 output_size: int,
--                 bias: bool = True,
--                 gather_output: bool = False,
--                 skip_bias_add: bool = False,
--                 params_dtype: Optional[torch.dtype] = None,
--                 quant_config: Optional[QuantizationConfig] = None,
--                 output_sizes: Optional[list[int]] = None,
--                 prefix: str = ""):
--        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
--                         quant_config, prefix)
--
--        self.gather_output = gather_output
--
--        # Divide the weight matrix along the last dimension.
--        tp_size = get_tensor_model_parallel_world_size()
--        assert self.quant_method is not None
--        self.output_size_per_partition = divide(self.output_size, tp_size)
--        self.output_partition_sizes = [self.output_size_per_partition]
--        # If QKV or MergedColumn, use output size of each partition.
--        if hasattr(self, "output_sizes"):
--            self.output_partition_sizes = [
--                divide(output_size, tp_size)
--                for output_size in self.output_sizes
--            ]
--
--        if output_sizes is None:
--            output_sizes = [output_size]
--
--        self.quant_method.create_weights(
--            layer=self,
--            input_size_per_partition=self.input_size,
--            output_partition_sizes=self.output_partition_sizes,
--            input_size=self.input_size,
--            output_size=self.output_size,
--            params_dtype=self.params_dtype,
--            weight_loader=(
--                self.weight_loader_v2 if self.quant_method.__class__.__name__
--                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
--        if bias:
--            self.bias = Parameter(
--                torch.empty(self.output_size_per_partition,
--                            dtype=params_dtype))
--            set_weight_attrs(self.bias, {
--                "output_dim": 0,
--                "weight_loader": self.weight_loader,
--            })
--        else:
--            self.register_parameter("bias", None)
--
--    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
--        tp_rank = get_tensor_model_parallel_rank()
--        output_dim = getattr(param, "output_dim", None)
--
--        # Special case for GGUF
--        is_gguf_weight = getattr(param, "is_gguf_weight", False)
--        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
--        if is_gguf_weight_type:
--            param.weight_type = loaded_weight.item()
--
--        # Materialize GGUF UninitializedParameter
--        if is_gguf_weight and isinstance(param, UninitializedParameter):
--            param.materialize(loaded_weight.shape, dtype=loaded_weight.dtype)
--
--        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
--        is_sharded_weight = getattr(param, "is_sharded_weight", False)
--        # bitsandbytes loads the weights of the specific portion
--        # no need to narrow
--        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
++        # Support gemm_tn->gemm_nn here
++        if envs.MACA_VLLM_USE_TN_2_NN:
++            weight = Parameter(torch.empty(input_size_per_partition,
++                                       sum(output_partition_sizes),
+                                        dtype=params_dtype),
+                            requires_grad=False)
++        else:
++            weight = Parameter(torch.empty(sum(output_partition_sizes),
++                                           input_size_per_partition,
++                                           dtype=params_dtype),
++                           requires_grad=False)
+         set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
+         layer.register_parameter("weight", weight)
+         set_weight_attrs(weight, extra_weight_attrs)
+@@ -199,8 +212,11 @@ class UnquantizedLinearMethod(LinearMethodBase):
+               layer: torch.nn.Module,
+               x: torch.Tensor,
+               bias: Optional[torch.Tensor] = None) -> torch.Tensor:
 -
--        param_data = param.data
--        if output_dim is not None and not is_sharded_weight:
+-        return dispatch_unquantized_gemm()(x, layer.weight, bias)
++        # Support gemm_tn->gemm_nn here
++        if envs.MACA_VLLM_USE_TN_2_NN and x.shape[-1] == layer.weight.shape[0]:
++            return dispatch_unquantized_gemm()(x, layer.weight.t(), bias)
++        else:
++            return dispatch_unquantized_gemm()(x, layer.weight, bias)
+ 
+ 
+ class LinearBase(torch.nn.Module):
+@@ -321,6 +337,11 @@ class ReplicatedLinear(LinearBase):
+         if len(loaded_weight.shape) == 0:
+             loaded_weight = loaded_weight.reshape(1)
+ 
++        # Support gemm_tn->gemm_nn here
++        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
++        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
++            loaded_weight = loaded_weight.t()
++
+         assert param.size() == loaded_weight.size(), (
+             f"Tried to load weights of size {loaded_weight.size()}"
+             f"to a parameter of size {param.size()}")
+@@ -438,6 +459,8 @@ class ColumnParallelLinear(LinearBase):
+         # bitsandbytes loads the weights of the specific portion
+         # no need to narrow
+         is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
++        # Support gemm_tn->gemm_nn here
++        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+ 
+         # Special case for GGUF
+         is_gguf_weight = getattr(param, "is_gguf_weight", False)
+@@ -456,7 +479,13 @@ class ColumnParallelLinear(LinearBase):
+ 
+         param_data = param.data
+         if output_dim is not None and not is_sharded_weight:
 -            shard_size = param_data.shape[output_dim]
--            start_idx = tp_rank * shard_size
--            loaded_weight = loaded_weight.narrow(output_dim, start_idx,
--                                                 shard_size)
--
--        # Special case for loading scales off disk, which often do not
--        # have a shape (such as in the case of AutoFP8).
--        if len(loaded_weight.shape) == 0:
--            loaded_weight = loaded_weight.reshape(1)
--
--        assert param_data.shape == loaded_weight.shape
--        param_data.copy_(loaded_weight)
--
--    def weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor):
--        # Special case for loading scales off disk, which often do not
--        # have a shape (such as in the case of AutoFP8).
--        if len(loaded_weight.shape) == 0:
--            assert loaded_weight.numel() == 1
--            loaded_weight = loaded_weight.reshape(1)
--        param.load_column_parallel_weight(loaded_weight=loaded_weight)
--
--    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
--        bias = self.bias if not self.skip_bias_add else None
--
--        # Matrix multiply.
--        assert self.quant_method is not None
--        output_parallel = self.quant_method.apply(self, input_, bias)
--        if self.gather_output:
--            # All-gather across the partitions.
--            output = tensor_model_parallel_all_gather(output_parallel)
--        else:
--            output = output_parallel
--        output_bias = self.bias if self.skip_bias_add else None
--        return output, output_bias
--
--    def extra_repr(self) -> str:
--        s = f"in_features={self.input_size}"
--        s += f", output_features={self.output_size_per_partition}"
--        s += f", bias={self.bias is not None}"
--        s += f", tp_size={get_tensor_model_parallel_world_size()}"
--        s += f", gather_output={self.gather_output}"
--        return s
--
--
--class MergedColumnParallelLinear(ColumnParallelLinear):
--    """Packed linear layers with column parallelism.
--
--    Similar to ColumnParallelLinear, but the weight matrix is concatenated
--    along the output dimension. When the weight matrix is loaded, the
--    different partitions are sharded separately.
--
--    Args:
--        input_size: input dimension of the linear layer.
--        output_sizes: list of output dimensions of the linear layer.
--        bias: If true, add bias.
--        gather_output: If true, call all-gather on output and make the output
--                       available to all GPUs, otherwise, every GPU will have
--                       its own output.
--        skip_bias_add: This was added to enable performance optimizations where
--                       bias can be fused with other element-wise operations. we
--                       skip adding bias but instead return it.
--        params_dtype: Data type for the parameters.
--        quant_config: Quantization configure.
--        prefix: The name of the layer in the state dict, including all parents
--                        (e.g. model.layers.0.qkv_proj)
--    """
--
--    def __init__(self,
--                 input_size: int,
--                 output_sizes: list[int],
--                 bias: bool = True,
--                 gather_output: bool = False,
--                 skip_bias_add: bool = False,
--                 params_dtype: Optional[torch.dtype] = None,
--                 quant_config: Optional[QuantizationConfig] = None,
--                 prefix: str = ""):
--        self.output_sizes = output_sizes
--        tp_size = get_tensor_model_parallel_world_size()
--        assert all(output_size % tp_size == 0 for output_size in output_sizes)
--        super().__init__(input_size=input_size,
--                         output_size=sum(output_sizes),
--                         bias=bias,
--                         gather_output=gather_output,
--                         skip_bias_add=skip_bias_add,
--                         params_dtype=params_dtype,
--                         quant_config=quant_config,
--                         prefix=prefix)
--
--    def weight_loader(self,
--                      param: Parameter,
--                      loaded_weight: torch.Tensor,
--                      loaded_shard_id: Optional[int] = None):
--
--        # Special case for GGUF
--        # initialize GGUF param after we know the quantize type
--        is_gguf_weight = getattr(param, "is_gguf_weight", False)
--        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
--        if is_gguf_weight_type:
--            if loaded_shard_id is not None:
--                param.data[loaded_shard_id].copy_(loaded_weight)
--                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
--            else:
--                param.shard_weight_type = {
--                    i: loaded_weight.item()
--                    for i, _ in enumerate(self.output_sizes)
--                }
--            return
--
--        if is_gguf_weight:
--            tp_size = get_tensor_model_parallel_world_size()
--            tp_rank = get_tensor_model_parallel_rank()
--
--            output_dim = getattr(param, "output_dim", None)
--            shard_size = loaded_weight.size(output_dim) // tp_size
--            start_idx = tp_rank * shard_size
--
--            if loaded_shard_id is not None:
--                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
--                                                     shard_size)
--                param.shard_id.append(loaded_shard_id)
--                param.shard_id_map[loaded_shard_id] = len(param.data_container)
--                param.data_container.append(loaded_weight)
--                if len(param.data_container) == 2:
--                    self.qweight = param.materialize_nested()
--                return
--
--        param_data = param.data
--        output_dim = getattr(param, "output_dim", None)
--        # Special case for AQLM codebooks.
--        is_metadata = getattr(param, "is_metadata", False)
--        # Special case for per-tensor scale to load scalar into fused array.
--        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
--
--        if loaded_shard_id is None:
--            # Loaded weight is already fused on disk (mlp).
--            # (e.g., Phi-3's gate_up_proj).
--            if output_dim is None:
--                if needs_scalar_to_array:
--                    param_data, loaded_weight = adjust_scalar_to_fused_array(
--                        param_data, loaded_weight, 0)
--
--                assert param_data.shape == loaded_weight.shape
--                param_data.copy_(loaded_weight)
--                return
--            current_shard_offset = 0
--            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
--                                            False)
--            shard_offsets: list[tuple[int, int, int]] = []
--            for i, output_size in enumerate(self.output_sizes):
--                shard_offsets.append((i, current_shard_offset, output_size))
--                current_shard_offset += output_size
--            packed_dim = getattr(param, "packed_dim", None)
--            for shard_id, shard_offset, shard_size in shard_offsets:
--                # Special case for Quantization.
--                # If quantized, we need to adjust the offset and size to account
--                # for the packing.
--                if packed_dim == output_dim:
--                    shard_size = shard_size // param.pack_factor
--                    shard_offset = shard_offset // param.pack_factor
--                    # Special case for Marlin.
--                    shard_size, shard_offset = adjust_marlin_shard(
--                        param, shard_size, shard_offset)
--
--                if use_bitsandbytes_4bit:
--                    index = list(itertools.accumulate([0] + self.output_sizes))
--                    orig_offsets = {
--                        str(i): (index[i], size)
--                        for i, size in enumerate(self.output_sizes)
--                    }
--                    orig_offsets["total"] = (self.output_size, 0)
--                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
--                        param, orig_offsets, str(shard_id))
--
--                loaded_weight_shard = loaded_weight.narrow(
--                    output_dim, shard_offset, shard_size)
--                self.weight_loader(param, loaded_weight_shard, shard_id)
--            return
--
--        assert loaded_shard_id < len(self.output_sizes)
--        tp_rank = get_tensor_model_parallel_rank()
--        tp_size = get_tensor_model_parallel_world_size()
--        if output_dim is not None:
--            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
--            shard_size = self.output_sizes[loaded_shard_id] // tp_size
--            # Special case for quantization.
--            # If quantized, we need to adjust the offset and size to account
--            # for the packing.
--            packed_dim = getattr(param, "packed_dim", None)
--            if packed_dim == output_dim:
--                shard_size = shard_size // param.pack_factor
--                shard_offset = shard_offset // param.pack_factor
--                # Special case for Marlin.
--                shard_size, shard_offset = adjust_marlin_shard(
--                    param, shard_size, shard_offset)
--
--            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
--                                            False)
--            is_sharded_weight = getattr(param, "is_sharded_weight", False)
--            # bitsandbytes loads the weights of the specific portion
--            # no need to narrow
--            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
--
--            if use_bitsandbytes_4bit:
--                shard_size = loaded_weight.shape[output_dim]
--                shard_offset = loaded_weight.shape[output_dim] * \
--                    loaded_shard_id
++            
++            # Support gemm_tn->gemm_nn here
++            if not envs.MACA_VLLM_USE_TN_2_NN or len(param_data.shape)==1 or is_quantization:
++                shard_size = param_data.shape[output_dim] 
++            else:
++                shard_size = param_data.shape[int(not(output_dim))]
++                
+             start_idx = tp_rank * shard_size
+             loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+                                                  shard_size)
+@@ -466,6 +495,10 @@ class ColumnParallelLinear(LinearBase):
+         if len(loaded_weight.shape) == 0:
+             loaded_weight = loaded_weight.reshape(1)
+ 
++        # Support gemm_tn->gemm_nn here
++        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
++            loaded_weight = loaded_weight.t()
++
+         assert param_data.shape == loaded_weight.shape
+         param_data.copy_(loaded_weight)
+ 
+@@ -596,7 +629,10 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
+         is_metadata = getattr(param, "is_metadata", False)
+         # Special case for per-tensor scale to load scalar into fused array.
+         needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
 -
++        
++        # Support gemm_tn->gemm_nn here
++        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
++        
+         if loaded_shard_id is None:
+             # Loaded weight is already fused on disk (mlp).
+             # (e.g., Phi-3's gate_up_proj).
+@@ -676,8 +712,12 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
+                 shard_offset = loaded_weight.shape[output_dim] * \
+                     loaded_shard_id
+ 
 -            param_data = param_data.narrow(output_dim, shard_offset,
 -                                           shard_size)
--            start_idx = tp_rank * shard_size
--            if not is_sharded_weight:
--                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
--                                                     shard_size)
--        # Special case for AQLM codebooks.
--        elif is_metadata:
--            # metadata indicates fixed size concatenated along dim 0
--            shard_size = loaded_weight.shape[0]
--            shard_offset = loaded_shard_id * shard_size
--            param_data = param_data.narrow(0, shard_offset, shard_size)
--
--        # Special case for per-tensor scales in fused case.
--        elif needs_scalar_to_array:
--            param_data, loaded_weight = adjust_scalar_to_fused_array(
--                param_data, loaded_weight, loaded_shard_id)
--
--        else:
--            ignore_warning = getattr(param, "ignore_warning", False)
--            if not ignore_warning:
--                logger.warning(
--                    "Loading a weight without `output_dim` attribute in "
--                    "MergedColumnParallelLinear, assume the weight is "
--                    "the same for all partitions.")
--
--        assert param_data.shape == loaded_weight.shape
--        param_data.copy_(loaded_weight)
--
--    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
--                                           loaded_weight: torch.Tensor):
--        """
--        Handle special case for models where MLP layers are already
--        fused on disk. In this case, we have no shard id. This function
--        determmines the shard id by splitting these layers and then calls
--        the weight loader using the shard id.
--
--        An example of a model with these fused layers:
--        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
--        """
--
--        current_shard_offset = 0
--        shard_offsets: list[tuple[int, int, int]] = []
--        for i, output_size in enumerate(self.output_sizes):
--            shard_offsets.append((i, current_shard_offset, output_size))
--            current_shard_offset += output_size
--
--        for shard_id, shard_offset, shard_size in shard_offsets:
--            # Special case for Quantization.
--            # If quantized, we need to adjust the offset and size to account
--            # for the packing.
--            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
--                                  )) and param.packed_dim == param.output_dim:
--                shard_size, shard_offset = \
--                    param.adjust_shard_indexes_for_packing(
--                    shard_size=shard_size, shard_offset=shard_offset)
--
--            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
--                                                       shard_offset,
--                                                       shard_size)
--            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
--
--    def weight_loader_v2(self,
--                         param: BasevLLMParameter,
--                         loaded_weight: torch.Tensor,
--                         loaded_shard_id: Optional[int] = None):
--        if loaded_shard_id is None:
--            if isinstance(param, PerTensorScaleParameter):
--                param.load_merged_column_weight(loaded_weight=loaded_weight,
--                                                shard_id=0)
--                return
--            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
--                param.load_merged_column_weight(loaded_weight=loaded_weight)
--                return
--            # TODO: @dsikka - move to parameter.py
--            self._load_fused_module_from_checkpoint(param, loaded_weight)
--            return
--
--        assert loaded_shard_id < len(self.output_sizes)
--
--        tp_size = get_tensor_model_parallel_world_size()
--
--        if isinstance(param, BlockQuantScaleParameter):
--            from vllm.model_executor.layers.quantization.fp8 import (
--                Fp8LinearMethod, Fp8MoEMethod)
--            assert self.quant_method is not None
--            assert isinstance(self.quant_method,
--                              (Fp8LinearMethod, Fp8MoEMethod))
--            weight_block_size = self.quant_method.quant_config.weight_block_size
--            assert weight_block_size is not None
--            block_n, _ = weight_block_size[0], weight_block_size[1]
--            shard_offset = (
--                (sum(self.output_sizes[:loaded_shard_id]) + block_n - 1) //
--                block_n) // tp_size
--            shard_size = ((self.output_sizes[loaded_shard_id] + block_n - 1) //
--                          block_n // tp_size)
--        else:
--            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
--            shard_size = self.output_sizes[loaded_shard_id] // tp_size
--
--        param.load_merged_column_weight(loaded_weight=loaded_weight,
--                                        shard_id=loaded_shard_id,
--                                        shard_offset=shard_offset,
--                                        shard_size=shard_size)
--
--
--class QKVParallelLinear(ColumnParallelLinear):
--    """Linear layers for the attention's QKV transformation.
--
--    Linear layers for the linear transformation of the query, key, and value
--    vectors in the attention layer. The weight matrix is concatenated along
--    the output dimension. The layer is parallelized along the head dimension.
--    When the number of key/value heads is smaller than the number of query
--    heads (e.g., multi-query/grouped-query attention), the key/value head may
--    be replicated while the query heads are partitioned.
--
--    Args:
--        hidden_size: input hidden state size of the transformer.
--        head_size: size of each attention head.
--        total_num_heads: total number of attention query heads.
--        total_num_kv_heads: total number of attention key/value heads. If
--                            None, assume total_num_kv_heads = total_num_heads.
--        bias: If true, add bias.
--        skip_bias_add: This was added to enable performance optimizations where
--                       bias can be fused with other element-wise operations. we
--                       skip adding bias but instead return it.
--        params_dtype: Data type for the parameters.
--        quant_config: Quantization configure.
--        prefix: The name of the layer in the state dict, including all parents
--                        (e.g. model.layers.0.qkv_proj)
--    """
--
--    def __init__(self,
--                 hidden_size: int,
--                 head_size: int,
--                 total_num_heads: int,
--                 total_num_kv_heads: Optional[int] = None,
--                 bias: bool = True,
--                 skip_bias_add: bool = False,
--                 params_dtype: Optional[torch.dtype] = None,
--                 quant_config: Optional[QuantizationConfig] = None,
--                 prefix: str = ""):
--        self.hidden_size = hidden_size
--        self.head_size = head_size
--        self.total_num_heads = total_num_heads
--        if total_num_kv_heads is None:
--            total_num_kv_heads = total_num_heads
--        self.total_num_kv_heads = total_num_kv_heads
--        # Divide the weight matrix along the last dimension.
--        tp_size = get_tensor_model_parallel_world_size()
--        self.num_heads = divide(self.total_num_heads, tp_size)
--        if tp_size >= self.total_num_kv_heads:
--            self.num_kv_heads = 1
--            self.num_kv_head_replicas = divide(tp_size,
--                                               self.total_num_kv_heads)
--        else:
--            self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)
--            self.num_kv_head_replicas = 1
--        input_size = self.hidden_size
--        output_size = (self.num_heads +
--                       2 * self.num_kv_heads) * tp_size * self.head_size
--        self.output_sizes = [
--            self.num_heads * self.head_size * tp_size,  # q_proj
--            self.num_kv_heads * self.head_size * tp_size,  # k_proj
--            self.num_kv_heads * self.head_size * tp_size,  # v_proj 
--        ]
--
--        super().__init__(input_size=input_size,
--                         output_size=output_size,
--                         bias=bias,
--                         gather_output=False,
--                         skip_bias_add=skip_bias_add,
--                         params_dtype=params_dtype,
--                         quant_config=quant_config,
--                         prefix=prefix)
--
--    def _get_shard_offset_mapping(self, loaded_shard_id: str):
--        shard_offset_mapping = {
--            "q": 0,
--            "k": self.num_heads * self.head_size,
--            "v": (self.num_heads + self.num_kv_heads) * self.head_size,
--            "total": (self.num_heads + 2 * self.num_kv_heads) * self.head_size
--        }
--        return shard_offset_mapping.get(loaded_shard_id)
--
--    def _get_shard_size_mapping(self, loaded_shard_id: str):
--        shard_size_mapping = {
--            "q": self.num_heads * self.head_size,
--            "k": self.num_kv_heads * self.head_size,
--            "v": self.num_kv_heads * self.head_size,
--        }
--        return shard_size_mapping.get(loaded_shard_id)
--
--    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
--                                           loaded_weight: torch.Tensor):
--        """
--        Handle special case for models where QKV layers are already 
--        fused on disk. In this case, we have no shard id. This function
--        determmines the shard id by splitting these layers and then calls
--        the weight loader using the shard id.
--
--        An example of a model with these fused layers:
--        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
--        """
--        shard_offsets = [
--            # (shard_id, shard_offset, shard_size)
--            ("q", 0, self.total_num_heads * self.head_size),
--            ("k", self.total_num_heads * self.head_size,
--             self.total_num_kv_heads * self.head_size),
--            ("v",
--             (self.total_num_heads + self.total_num_kv_heads) * self.head_size,
--             self.total_num_kv_heads * self.head_size),
--        ]
--
--        for shard_id, shard_offset, shard_size in shard_offsets:
--            # Special case for Quantization.
--            # If quantized, we need to adjust the offset and size to account
--            # for the packing.
--            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
--                                  )) and param.packed_dim == param.output_dim:
--                shard_size, shard_offset = \
--                    param.adjust_shard_indexes_for_packing(
--                    shard_size=shard_size, shard_offset=shard_offset)
--
--            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
--                                                       shard_offset,
--                                                       shard_size)
--            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
--
--    def weight_loader_v2(self,
--                         param: BasevLLMParameter,
--                         loaded_weight: torch.Tensor,
--                         loaded_shard_id: Optional[str] = None):
--        if loaded_shard_id is None:  # special case for certain models
--            if isinstance(param, PerTensorScaleParameter):
--                param.load_qkv_weight(loaded_weight=loaded_weight, shard_id=0)
--                return
--            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
--                param.load_qkv_weight(loaded_weight=loaded_weight)
--                return
--            # TODO: @dsikka - move to parameter.py
--            self._load_fused_module_from_checkpoint(param, loaded_weight)
--            return
--
--        assert loaded_shard_id in ["q", "k", "v"]
--
--        shard_offset = self._get_shard_offset_mapping(loaded_shard_id)
--        shard_size = self._get_shard_size_mapping(loaded_shard_id)
--
--        param.load_qkv_weight(loaded_weight=loaded_weight,
--                              num_heads=self.num_kv_head_replicas,
--                              shard_id=loaded_shard_id,
--                              shard_offset=shard_offset,
--                              shard_size=shard_size)
--
--    def weight_loader(self,
--                      param: Parameter,
--                      loaded_weight: torch.Tensor,
--                      loaded_shard_id: Optional[str] = None):
--
--        # Special case for GGUF
--        # initialize GGUF param after we know the quantize type
--        is_gguf_weight = getattr(param, "is_gguf_weight", False)
--        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
--        if is_gguf_weight_type:
--            idx_map = {"q": 0, "k": 1, "v": 2}
--            if loaded_shard_id is not None:
--                param.data[idx_map[loaded_shard_id]].copy_(loaded_weight)
--                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
--            else:
--                param.shard_weight_type = {
--                    k: loaded_weight.item()
--                    for k in idx_map
--                }
--            return
--
--        if is_gguf_weight:
--            tp_size = get_tensor_model_parallel_world_size()
--            tp_rank = get_tensor_model_parallel_rank()
--
--            output_dim = getattr(param, "output_dim", None)
--            shard_size = loaded_weight.size(output_dim) // tp_size
--            start_idx = tp_rank * shard_size
--
--            if loaded_shard_id is not None:
--                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
--                                                     shard_size)
--                param.shard_id.append(loaded_shard_id)
--                param.shard_id_map[loaded_shard_id] = len(param.data_container)
--                param.data_container.append(loaded_weight)
--                if len(param.data_container) == 3:
--                    self.qweight = param.materialize_nested()
--                return
--
--        param_data = param.data
--        output_dim = getattr(param, "output_dim", None)
--        # Special case for AQLM codebooks.
--        is_metadata = getattr(param, "is_metadata", False)
--
--        # Special case for per-tensor scales in fused case.
--        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
--
--        if loaded_shard_id is None:
--            # Loaded weight is already fused on disk (qkv).
--            # (e.g., Phi-3's qkv_proj).
--            if output_dim is None:
--                if needs_scalar_to_array:
--                    param_data, loaded_weight = adjust_scalar_to_fused_array(
--                        param_data, loaded_weight, 0)
--
--                assert param_data.shape == loaded_weight.shape
--                param_data.copy_(loaded_weight)
--                return
--            shard_offsets = [
--                # (shard_id, shard_offset, shard_size)
--                ("q", 0, self.total_num_heads * self.head_size),
--                ("k", self.total_num_heads * self.head_size,
--                 self.total_num_kv_heads * self.head_size),
--                ("v", (self.total_num_heads + self.total_num_kv_heads) *
--                 self.head_size, self.total_num_kv_heads * self.head_size),
--            ]
--            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
--                                            False)
--
--            packed_dim = getattr(param, "packed_dim", None)
--            for shard_id, shard_offset, shard_size in shard_offsets:
--                # Special case for Quantized Weights.
--                # If quantized, we need to adjust the offset and size to account
--                # for the packing.
--                if packed_dim == output_dim:
--                    shard_size = shard_size // param.pack_factor
--                    shard_offset = shard_offset // param.pack_factor
--
--                    # Special case for Marlin.
--                    shard_size, shard_offset = adjust_marlin_shard(
--                        param, shard_size, shard_offset)
--
--                if use_bitsandbytes_4bit:
--                    orig_qkv_offsets = {
--                        "q": (0, self.total_num_heads * self.head_size),
--                        "k": (self.total_num_heads * self.head_size,
--                              self.total_num_kv_heads * self.head_size),
--                        "v":
--                        ((self.total_num_heads + self.total_num_kv_heads) *
--                         self.head_size,
--                         self.total_num_kv_heads * self.head_size),
--                        "total":
--                        ((self.total_num_heads + 2 * self.total_num_kv_heads) *
--                         self.head_size, 0)
--                    }
--
--                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
--                        param, orig_qkv_offsets, shard_id)
--
--                loaded_weight_shard = loaded_weight.narrow(
--                    output_dim, shard_offset, shard_size)
--                self.weight_loader(param, loaded_weight_shard, shard_id)
--            return
--
--        tp_rank = get_tensor_model_parallel_rank()
--        assert loaded_shard_id in ["q", "k", "v"]
--
--        # If output dim is defined, use the default loading process.
--        if output_dim is not None:
--            if loaded_shard_id == "q":
--                shard_offset = 0
--                shard_size = self.num_heads * self.head_size
--            elif loaded_shard_id == "k":
--                shard_offset = self.num_heads * self.head_size
--                shard_size = self.num_kv_heads * self.head_size
--            elif loaded_shard_id == "v":
--                shard_offset = (self.num_heads +
--                                self.num_kv_heads) * self.head_size
--                shard_size = self.num_kv_heads * self.head_size
--            # Special case for Quantized Weights.
--            # If quantized, we need to adjust the offset and size to account
--            # for the packing.
--            packed_dim = getattr(param, "packed_dim", None)
--            if packed_dim == output_dim:
--                shard_size = shard_size // param.pack_factor
--                shard_offset = shard_offset // param.pack_factor
--
--                # Special case for Marlin.
--                shard_size, shard_offset = adjust_marlin_shard(
--                    param, shard_size, shard_offset)
--
--            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
--                                            False)
--            is_sharded_weight = getattr(param, "is_sharded_weight", False)
--            # bitsandbytes loads the weights of the specific portion
--            # no need to narrow
--            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
--
--            if use_bitsandbytes_4bit:
--                orig_qkv_offsets = {
--                    "q": (0, self.num_heads * self.head_size),
--                    "k": (self.num_heads * self.head_size,
--                          self.num_kv_heads * self.head_size),
--                    "v":
--                    ((self.num_heads + self.num_kv_heads) * self.head_size,
--                     self.num_kv_heads * self.head_size),
--                    "total":
--                    ((self.num_heads + 2 * self.num_kv_heads) * self.head_size,
--                     0)
--                }
--                shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
--                    param, orig_qkv_offsets, loaded_shard_id)
++            # Support gemm_tn->gemm_nn here
++            if not envs.MACA_VLLM_USE_TN_2_NN or is_quantization:
++                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
++            else:
++                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
++                
+             start_idx = tp_rank * shard_size
+             if not is_sharded_weight:
+                 loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+@@ -701,7 +741,9 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
+                     "Loading a weight without `output_dim` attribute in "
+                     "MergedColumnParallelLinear, assume the weight is "
+                     "the same for all partitions.")
+-
++        # Support gemm_tn->gemm_nn here
++        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
++            loaded_weight = loaded_weight.t()
+         assert param_data.shape == loaded_weight.shape
+         param_data.copy_(loaded_weight)
+ 
+@@ -990,7 +1032,10 @@ class QKVParallelLinear(ColumnParallelLinear):
+ 
+         # Special case for per-tensor scales in fused case.
+         needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
 -
++        
++        # Support gemm_tn->gemm_nn here
++        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
++        
+         if loaded_shard_id is None:
+             # Loaded weight is already fused on disk (qkv).
+             # (e.g., Phi-3's qkv_proj).
+@@ -1097,8 +1142,12 @@ class QKVParallelLinear(ColumnParallelLinear):
+                 shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
+                     param, orig_qkv_offsets, loaded_shard_id)
+ 
 -            param_data = param_data.narrow(output_dim, shard_offset,
 -                                           shard_size)
--            if loaded_shard_id == "q":
--                shard_id = tp_rank
--            else:
--                shard_id = tp_rank // self.num_kv_head_replicas
--            start_idx = shard_id * shard_size
--
--            if not is_sharded_weight:
--                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
--                                                     shard_size)
--
--        # Special case for for AQLM codebooks.
--        elif is_metadata:
--            # metadata indicates fixed size concatenated along dim 0
--            shard_size = loaded_weight.shape[0]
--            shard_index = ["q", "k", "v"].index(loaded_shard_id)
--            param_data = param_data.narrow(0, shard_index * shard_size,
--                                           shard_size)
--        # Special case for per-tensor scales in fused case.
--        elif needs_scalar_to_array:
--            param_data, loaded_weight = adjust_scalar_to_fused_array(
--                param_data, loaded_weight, loaded_shard_id)
--        else:
--            ignore_warning = getattr(param, "ignore_warning", False)
--            if not ignore_warning:
--                logger.warning(
--                    "Loading a weight without `output_dim` attribute in "
--                    "QKVParallelLinear, assume the weight is the same "
--                    "for all partitions.")
--
--        assert param_data.shape == loaded_weight.shape
--        param_data.copy_(loaded_weight)
--
--
--class RowParallelLinear(LinearBase):
--    """Linear layer with row parallelism.
--
--    The linear layer is defined as Y = XA + b. A is parallelized along
--    its first dimension and X along its second dimension as:
--               -   -
--              | A_1 |
--              | .   |
--          A = | .   |        X = [X_1, ..., X_p]
--              | .   |
--              | A_p |
--               -   -
--    Arguments:
--        input_size: first dimension of matrix A.
--        output_size: second dimension of matrix A.
--        bias: If true, add bias. Note that bias is not parallelized.
--        input_is_parallel: If true, we assume that the input is already
--                           split across the GPUs and we do not split
--                           again.
--        skip_bias_add: This was added to enable performance optimization where
--                       bias can be fused with other element-wise operations.
--                       We skip adding bias but instead return it.
--        params_dtype: Data type for the parameters.
--        quant_config: Quantization configure.
--    """
--
--    def __init__(self,
--                 input_size: int,
--                 output_size: int,
--                 bias: bool = True,
--                 input_is_parallel: bool = True,
--                 skip_bias_add: bool = False,
--                 params_dtype: Optional[torch.dtype] = None,
--                 reduce_results: bool = True,
--                 quant_config: Optional[QuantizationConfig] = None,
--                 prefix: str = ""):
--        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
--                         quant_config, prefix)
--
--        self.input_is_parallel = input_is_parallel
--        self.reduce_results = reduce_results
--
--        # Divide the weight matrix along the last dimension.
--        self.tp_rank = get_tensor_model_parallel_rank()
--        self.tp_size = get_tensor_model_parallel_world_size()
--        self.input_size_per_partition = divide(input_size, self.tp_size)
--        assert self.quant_method is not None
--
--        self.quant_method.create_weights(
--            layer=self,
--            input_size_per_partition=self.input_size_per_partition,
--            output_partition_sizes=[self.output_size],
--            input_size=self.input_size,
--            output_size=self.output_size,
--            params_dtype=self.params_dtype,
--            weight_loader=(
--                self.weight_loader_v2 if self.quant_method.__class__.__name__
--                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
--        if not reduce_results and (bias and not skip_bias_add):
--            raise ValueError("When not reduce the results, adding bias to the "
--                             "results can lead to incorrect results")
--
--        if bias:
--            self.bias = Parameter(
--                torch.empty(self.output_size, dtype=params_dtype))
--            set_weight_attrs(self.bias, {
--                "output_dim": 0,
--                "weight_loader": self.weight_loader,
--            })
--        else:
--            self.register_parameter("bias", None)
--
--    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
--        tp_rank = get_tensor_model_parallel_rank()
--        tp_size = get_tensor_model_parallel_world_size()
--        input_dim = getattr(param, "input_dim", None)
--        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
--        is_sharded_weight = getattr(param, "is_sharded_weight", False)
--        # bitsandbytes loads the weights of the specific portion
--        # no need to narrow
--        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
--
--        # Special case for GGUF
--        is_gguf_weight = getattr(param, "is_gguf_weight", False)
--        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
--        if is_gguf_weight_type:
--            param.weight_type = loaded_weight.item()
--
--        # Materialize GGUF UninitializedParameter
--        if is_gguf_weight and isinstance(param, UninitializedParameter):
--            weight_shape = list(loaded_weight.shape)
--            if input_dim:
--                weight_shape[input_dim] = weight_shape[input_dim] // tp_size
--            param.materialize(tuple(weight_shape), dtype=loaded_weight.dtype)
--
--        param_data = param.data
--        if input_dim is not None and not is_sharded_weight:
--            shard_size = param_data.shape[input_dim]
--            start_idx = tp_rank * shard_size
--            loaded_weight = loaded_weight.narrow(input_dim, start_idx,
--                                                 shard_size)
--
--        # Special case for loading scales off disk, which often do not
--        # have a shape (such as in the case of AutoFP8).
--        if len(loaded_weight.shape) == 0:
--            loaded_weight = loaded_weight.reshape(1)
--
--        assert param_data.shape == loaded_weight.shape
--        param_data.copy_(loaded_weight)
--
--    def weight_loader_v2(self, param: BasevLLMParameter,
--                         loaded_weight: torch.Tensor):
--
--        # Special case for loading scales off disk, which often do not
--        # have a shape (such as in the case of AutoFP8).
--        if len(loaded_weight.shape) == 0:
--            assert loaded_weight.numel() == 1
--            loaded_weight = loaded_weight.reshape(1)
--
--        param.load_row_parallel_weight(loaded_weight=loaded_weight)
--
--    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
--        if self.input_is_parallel:
--            input_parallel = input_
--        else:
--            tp_rank = get_tensor_model_parallel_rank()
--            splitted_input = split_tensor_along_last_dim(
--                input_, num_partitions=self.tp_size)
--            input_parallel = splitted_input[tp_rank].contiguous()
--
--        # Matrix multiply.
--        assert self.quant_method is not None
--        # Only fuse bias add into GEMM for rank 0 (this ensures that
--        # bias will not get added more than once in TP>1 case)
--        bias_ = None if (self.tp_rank > 0 or self.skip_bias_add) else self.bias
--        output_parallel = self.quant_method.apply(self,
--                                                  input_parallel,
--                                                  bias=bias_)
--        if self.reduce_results and self.tp_size > 1:
--            output = tensor_model_parallel_all_reduce(output_parallel)
--        else:
--            output = output_parallel
--
--        output_bias = self.bias if self.skip_bias_add else None
--
--        return output, output_bias
--
--    def extra_repr(self) -> str:
--        s = f"input_features={self.input_size_per_partition}"
--        s += f", output_features={self.output_size}"
--        s += f", bias={self.bias is not None}"
--        s += f", tp_size={self.tp_size}"
--        s += f", reduce_results={self.reduce_results}"
--        return s
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# SPDX-License-Identifier: Apache-2.0
-+
-+import itertools
-+from abc import abstractmethod
-+from typing import Optional
-+
-+import torch
-+import torch.nn.functional as F
-+from torch.nn.parameter import Parameter, UninitializedParameter
-+
-+from vllm.distributed import (divide, get_tensor_model_parallel_rank,
-+                              get_tensor_model_parallel_world_size,
-+                              split_tensor_along_last_dim,
-+                              tensor_model_parallel_all_gather,
-+                              tensor_model_parallel_all_reduce)
-+from vllm.logger import init_logger
-+from vllm.model_executor.layers.quantization.base_config import (
-+    QuantizationConfig, QuantizeMethodBase)
-+# yapf: disable
-+from vllm.model_executor.parameter import (BasevLLMParameter,
-+                                           BlockQuantScaleParameter,
-+                                           PackedColumnParameter,
-+                                           PackedvLLMParameter,
-+                                           PerTensorScaleParameter,
-+                                           RowvLLMParameter)
-+# yapf: enable
-+from vllm.model_executor.utils import set_weight_attrs
-+
-+logger = init_logger(__name__)
-+
-+WEIGHT_LOADER_V2_SUPPORTED = [
-+    "CompressedTensorsLinearMethod", "AWQMarlinLinearMethod",
-+    "AWQLinearMethod", "GPTQMarlinLinearMethod", "Fp8LinearMethod",
-+    "MarlinLinearMethod", "QQQLinearMethod", "GPTQMarlin24LinearMethod",
-+    "TPUInt8LinearMethod", "GPTQLinearMethod", "FBGEMMFp8LinearMethod",
-+    "ModelOptFp8LinearMethod", "IPEXAWQLinearMethod", "IPEXGPTQLinearMethod",
-+    "HQQMarlinMethod", "QuarkLinearMethod"
-+]
-+
-+
-+def adjust_marlin_shard(param, shard_size, shard_offset):
-+    marlin_tile_size = getattr(param, "marlin_tile_size", None)
-+    if marlin_tile_size is None:
-+        return shard_size, shard_offset
-+
-+    return shard_size * marlin_tile_size, shard_offset * marlin_tile_size
-+
-+
-+def adjust_bitsandbytes_4bit_shard(param: Parameter,
-+                                   shard_offsets: dict[str, tuple[int, int]],
-+                                   loaded_shard_id: str) -> tuple[int, int]:
-+    """Adjust the quantization offsets and sizes for BitsAndBytes sharding."""
-+
-+    total, _ = shard_offsets["total"]
-+    orig_offset, orig_size = shard_offsets[loaded_shard_id]
-+
-+    quantized_total = param.data.shape[0]
-+    quantized_offset = orig_offset * quantized_total // total
-+    quantized_size = orig_size * quantized_total // total
-+
-+    return quantized_size, quantized_offset
-+
-+
-+def adjust_scalar_to_fused_array(param, loaded_weight, shard_id):
-+    """For fused modules (QKV and MLP) we have an array of length
-+    N that holds 1 scale for each "logical" matrix. So the param
-+    is an array of length N. The loaded_weight corresponds to 
-+    one of the shards on disk. Here, we slice the param based on 
-+    the shard_id for loading.
-+    """
-+    qkv_idxs = {"q": 0, "k": 1, "v": 2}
-+
-+    if isinstance(shard_id, str):
-+        shard_id = qkv_idxs[shard_id]
-+    elif not isinstance(shard_id, int):
-+        raise ValueError(f"Unknown Shard Id {shard_id}")
-+
-+    # AutoFP8 scales do not have a shape
-+    # compressed-tensors scales do have a shape
-+    if len(loaded_weight.shape) != 0:
-+        assert loaded_weight.shape[0] == 1
-+        loaded_weight = loaded_weight[0]
-+
-+    return param[shard_id], loaded_weight.t()
-+    #return param[shard_id], loaded_weight
-+
-+
-+class LinearMethodBase(QuantizeMethodBase):
-+    """Base class for different (maybe quantized) linear methods."""
-+
-+    @abstractmethod
-+    def create_weights(self, layer: torch.nn.Module,
-+                       input_size_per_partition: int,
-+                       output_partition_sizes: list[int], input_size: int,
-+                       output_size: int, params_dtype: torch.dtype,
-+                       **extra_weight_attrs):
-+        """Create weights for a linear layer. 
-+           The weights will be set as attributes of the layer.
-+
-+        Args:
-+            layer: The layer that is using the LinearMethodBase factory.
-+            input_size_per_partition: Size of the weight input dim on rank X.
-+            output_partition_sizes: Sizes of the output dim of each logical 
-+                weight on rank X. E.g., output_partition_sizes for QKVLinear
-+                is a list contains the width of Wq, Wk, Wv on rank X.
-+            input_size: Size of the input dim of the weight across all ranks.
-+            output_size: Size of the output dim of the weight across all ranks.
-+            params_dtype: Datatype of the parameters.
-+        """
-+        raise NotImplementedError
-+
-+    @abstractmethod
-+    def apply(self,
-+              layer: torch.nn.Module,
-+              x: torch.Tensor,
-+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
-+        """Apply the weights in layer to the input tensor.
-+        Expects create_weights to have been called before on the layer."""
-+        raise NotImplementedError
-+
-+
-+class UnquantizedLinearMethod(LinearMethodBase):
-+    """Linear method without quantization."""
-+
-+    def create_weights(self, layer: torch.nn.Module,
-+                       input_size_per_partition: int,
-+                       output_partition_sizes: list[int], input_size: int,
-+                       output_size: int, params_dtype: torch.dtype,
-+                       **extra_weight_attrs):
-+        weight = Parameter(torch.empty(input_size_per_partition,
-+                                       sum(output_partition_sizes),
-+                                       dtype=params_dtype),
-+                           requires_grad=False)
-+        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
-+        layer.register_parameter("weight", weight)
-+        set_weight_attrs(weight, extra_weight_attrs)
-+
-+    def apply(self,
-+              layer: torch.nn.Module,
-+              x: torch.Tensor,
-+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
-+
-+        #return F.linear(x, layer.weight, bias)
-+        if x.shape[-1]==layer.weight.shape[0]:
-+            return F.linear(x, layer.weight.t(), bias)
-+        else:
-+            return F.linear(x, layer.weight, bias)
-+
-+
-+class LinearBase(torch.nn.Module):
-+    """Base linear layer.
-+
-+    Args:
-+        input_size: input dimension of the linear layer.
-+        output_size: output dimension of the linear layer.
-+        bias: If true, add bias.
-+        skip_bias_add: If true, skip adding bias but instead return it.
-+        params_dtype: Data type for the parameters.
-+        quant_config: Quantization configure.
-+    """
-+
-+    def __init__(
-+        self,
-+        input_size: int,
-+        output_size: int,
-+        skip_bias_add: bool = False,
-+        params_dtype: Optional[torch.dtype] = None,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ):
-+        super().__init__()
-+
-+        # Keep input parameters
-+        self.input_size = input_size
-+        self.output_size = output_size
-+        self.skip_bias_add = skip_bias_add
-+        if params_dtype is None:
-+            params_dtype = torch.get_default_dtype()
-+        self.params_dtype = params_dtype
-+        if quant_config is None:
-+            self.quant_method: Optional[
-+                QuantizeMethodBase] = UnquantizedLinearMethod()
-+        else:
-+            self.quant_method = quant_config.get_quant_method(self,
-+                                                              prefix=prefix)
-+
-+    def forward(self,
-+                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
-+        raise NotImplementedError
-+
-+
-+class ReplicatedLinear(LinearBase):
-+    """Replicated linear layer.
-+
-+    Args:
-+        input_size: input dimension of the linear layer.
-+        output_size: output dimension of the linear layer.
-+        bias: If true, add bias.
-+        skip_bias_add: If true, skip adding bias but instead return it.
-+        params_dtype: Data type for the parameters.
-+        quant_config: Quantization configure.
-+        prefix: The name of the layer in the state dict, including all parents
-+                        (e.g. model.layers.0.qkv_proj)
-+    """
-+
-+    def __init__(self,
-+                 input_size: int,
-+                 output_size: int,
-+                 bias: bool = True,
-+                 skip_bias_add: bool = False,
-+                 params_dtype: Optional[torch.dtype] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 prefix: str = ""):
-+        super().__init__(input_size,
-+                         output_size,
-+                         skip_bias_add,
-+                         params_dtype,
-+                         quant_config,
-+                         prefix=prefix)
-+
-+        # All the linear layer supports quant method.
-+        assert self.quant_method is not None
-+        self.quant_method.create_weights(self,
-+                                         self.input_size, [self.output_size],
-+                                         self.input_size,
-+                                         self.output_size,
-+                                         self.params_dtype,
-+                                         weight_loader=self.weight_loader)
-+
-+        if bias:
-+            self.bias = Parameter(
-+                torch.empty(self.output_size, dtype=self.params_dtype))
-+            set_weight_attrs(self.bias, {
-+                "output_dim": 0,
-+                "weight_loader": self.weight_loader,
-+            })
-+        else:
-+            self.register_parameter("bias", None)
-+
-+    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
-+        # If the weight on disk does not have a shape, give it one
-+        # (such scales for AutoFp8).
-+        if len(loaded_weight.shape) == 0:
-+            loaded_weight = loaded_weight.reshape(1)
-+
-+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
-+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
-+
-+        assert param.size() == loaded_weight.size()
-+        param.data.copy_(loaded_weight)
-+
-+    def forward(self,
-+                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
-+        bias = self.bias if not self.skip_bias_add else None
-+        assert self.quant_method is not None
-+        output = self.quant_method.apply(self, x, bias)
-+        output_bias = self.bias if self.skip_bias_add else None
-+        return output, output_bias
-+
-+    def extra_repr(self) -> str:
-+        s = f"in_features={self.input_size}"
-+        s += f", output_features={self.output_size}"
-+        s += f", bias={self.bias is not None}"
-+        return s
-+
-+
-+class ColumnParallelLinear(LinearBase):
-+    """Linear layer with column parallelism.
-+
-+    The linear layer is defined as Y = XA + b. A is parallelized along
-+    its second dimension as A = [A_1, ..., A_p].
-+
-+    Args:
-+        input_size: first dimension of matrix A.
-+        output_size: second dimension of matrix A.
-+        bias: If true, add bias.
-+        gather_output: If true, call all-gather on output and make Y available
-+                       to all GPUs, otherwise, every GPU will have its output
-+                       which is Y_i = XA_i
-+        skip_bias_add: This was added to enable performance optimizations where
-+                       bias can be fused with other element-wise operations. we
-+                       skip adding bias but instead return it.
-+        params_dtype: Data type for the parameters.
-+        quant_config: Quantization configure.
-+        output_sizes: list of output sizes packed into one output, like for QKV
-+                       the list would be size 3.
-+        prefix: The name of the layer in the state dict, including all parents
-+                        (e.g. model.layers.0.qkv_proj) 
-+    """
-+
-+    def __init__(self,
-+                 input_size: int,
-+                 output_size: int,
-+                 bias: bool = True,
-+                 gather_output: bool = False,
-+                 skip_bias_add: bool = False,
-+                 params_dtype: Optional[torch.dtype] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 output_sizes: Optional[list[int]] = None,
-+                 prefix: str = ""):
-+        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
-+                         quant_config, prefix)
-+
-+        self.gather_output = gather_output
-+
-+        # Divide the weight matrix along the last dimension.
-+        tp_size = get_tensor_model_parallel_world_size()
-+        self.input_size_per_partition = self.input_size
-+        assert self.quant_method is not None
-+        self.output_size_per_partition = divide(self.output_size, tp_size)
-+        self.output_partition_sizes = [self.output_size_per_partition]
-+        # If QKV or MergedColumn, use output size of each partition.
-+        if hasattr(self, "output_sizes"):
-+            self.output_partition_sizes = [
-+                divide(output_size, tp_size)
-+                for output_size in self.output_sizes
-+            ]
-+
-+        if output_sizes is None:
-+            output_sizes = [output_size]
-+
-+        self.quant_method.create_weights(
-+            layer=self,
-+            input_size_per_partition=self.input_size,
-+            output_partition_sizes=self.output_partition_sizes,
-+            input_size=self.input_size,
-+            output_size=self.output_size,
-+            params_dtype=self.params_dtype,
-+            weight_loader=(
-+                self.weight_loader_v2 if self.quant_method.__class__.__name__
-+                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
-+        if bias:
-+            self.bias = Parameter(
-+                torch.empty(self.output_size_per_partition,
-+                            dtype=params_dtype))
-+            set_weight_attrs(self.bias, {
-+                "output_dim": 0,
-+                "weight_loader": self.weight_loader,
-+            })
-+        else:
-+            self.register_parameter("bias", None)
-+
-+    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
-+        tp_rank = get_tensor_model_parallel_rank()
-+        output_dim = getattr(param, "output_dim", None)
-+
-+        # Special case for GGUF
-+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-+        if is_gguf_weight_type:
-+            param.weight_type = loaded_weight.item()
-+
-+        # Materialize GGUF UninitializedParameter
-+        if is_gguf_weight and isinstance(param, UninitializedParameter):
-+            param.materialize(loaded_weight.shape, dtype=loaded_weight.dtype)
-+
-+        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
-+        is_sharded_weight = getattr(param, "is_sharded_weight", False)
-+        # bitsandbytes loads the weights of the specific portion
-+        # no need to narrow
-+        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
-+
-+        param_data = param.data
-+        if output_dim is not None and not is_sharded_weight:
-+            #shard_size = param_data.shape[output_dim]
-+            shard_size = param_data.shape[output_dim] if len(param_data.shape)==1 or is_quantization else param_data.shape[int(not(output_dim))]
-+            start_idx = tp_rank * shard_size
-+            loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-+                                                 shard_size)
-+
-+        # Special case for loading scales off disk, which often do not
-+        # have a shape (such as in the case of AutoFP8).
-+        if len(loaded_weight.shape) == 0:
-+            loaded_weight = loaded_weight.reshape(1)
-+
-+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
-+        assert param_data.shape == loaded_weight.shape
-+        param_data.copy_(loaded_weight)
-+
-+    def weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor):
-+        # Special case for loading scales off disk, which often do not
-+        # have a shape (such as in the case of AutoFP8).
-+        if len(loaded_weight.shape) == 0:
-+            assert loaded_weight.numel() == 1
-+            loaded_weight = loaded_weight.reshape(1)
-+        param.load_column_parallel_weight(loaded_weight=loaded_weight)
-+
-+    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
-+        bias = self.bias if not self.skip_bias_add else None
-+
-+        # Matrix multiply.
-+        assert self.quant_method is not None
-+        output_parallel = self.quant_method.apply(self, input_, bias)
-+        if self.gather_output:
-+            # All-gather across the partitions.
-+            output = tensor_model_parallel_all_gather(output_parallel)
-+        else:
-+            output = output_parallel
-+        output_bias = self.bias if self.skip_bias_add else None
-+        return output, output_bias
-+
-+    def extra_repr(self) -> str:
-+        s = f"in_features={self.input_size}"
-+        s += f", output_features={self.output_size_per_partition}"
-+        s += f", bias={self.bias is not None}"
-+        s += f", tp_size={get_tensor_model_parallel_world_size()}"
-+        s += f", gather_output={self.gather_output}"
-+        return s
-+
-+
-+class MergedColumnParallelLinear(ColumnParallelLinear):
-+    """Packed linear layers with column parallelism.
-+
-+    Similar to ColumnParallelLinear, but the weight matrix is concatenated
-+    along the output dimension. When the weight matrix is loaded, the
-+    different partitions are sharded separately.
-+
-+    Args:
-+        input_size: input dimension of the linear layer.
-+        output_sizes: list of output dimensions of the linear layer.
-+        bias: If true, add bias.
-+        gather_output: If true, call all-gather on output and make the output
-+                       available to all GPUs, otherwise, every GPU will have
-+                       its own output.
-+        skip_bias_add: This was added to enable performance optimizations where
-+                       bias can be fused with other element-wise operations. we
-+                       skip adding bias but instead return it.
-+        params_dtype: Data type for the parameters.
-+        quant_config: Quantization configure.
-+        prefix: The name of the layer in the state dict, including all parents
-+                        (e.g. model.layers.0.qkv_proj)
-+    """
-+
-+    def __init__(self,
-+                 input_size: int,
-+                 output_sizes: list[int],
-+                 bias: bool = True,
-+                 gather_output: bool = False,
-+                 skip_bias_add: bool = False,
-+                 params_dtype: Optional[torch.dtype] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 prefix: str = ""):
-+        self.output_sizes = output_sizes
-+        tp_size = get_tensor_model_parallel_world_size()
-+        assert all(output_size % tp_size == 0 for output_size in output_sizes)
-+        super().__init__(input_size=input_size,
-+                         output_size=sum(output_sizes),
-+                         bias=bias,
-+                         gather_output=gather_output,
-+                         skip_bias_add=skip_bias_add,
-+                         params_dtype=params_dtype,
-+                         quant_config=quant_config,
-+                         prefix=prefix)
-+
-+    def weight_loader(self,
-+                      param: Parameter,
-+                      loaded_weight: torch.Tensor,
-+                      loaded_shard_id: Optional[int] = None):
-+
-+        # Special case for GGUF
-+        # initialize GGUF param after we know the quantize type
-+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-+        if is_gguf_weight_type:
-+            if loaded_shard_id is not None:
-+                param.data[loaded_shard_id].copy_(loaded_weight)
-+                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
-+            else:
-+                param.shard_weight_type = {
-+                    i: loaded_weight.item()
-+                    for i, _ in enumerate(self.output_sizes)
-+                }
-+            return
-+
-+        if is_gguf_weight:
-+            tp_size = get_tensor_model_parallel_world_size()
-+            tp_rank = get_tensor_model_parallel_rank()
-+
-+            output_dim = getattr(param, "output_dim", None)
-+            shard_size = loaded_weight.size(output_dim) // tp_size
-+            start_idx = tp_rank * shard_size
-+
-+            if loaded_shard_id is not None:
-+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-+                                                     shard_size)
-+                param.shard_id.append(loaded_shard_id)
-+                param.shard_id_map[loaded_shard_id] = len(param.data_container)
-+                param.data_container.append(loaded_weight)
-+                if len(param.data_container) == 2:
-+                    self.qweight = param.materialize_nested()
-+                return
-+
-+        param_data = param.data
-+        output_dim = getattr(param, "output_dim", None)
-+        # Special case for AQLM codebooks.
-+        is_metadata = getattr(param, "is_metadata", False)
-+        # Special case for per-tensor scale to load scalar into fused array.
-+        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
-+
-+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
-+
-+        if loaded_shard_id is None:
-+            # Loaded weight is already fused on disk (mlp).
-+            # (e.g., Phi-3's gate_up_proj).
-+            if output_dim is None:
-+                if needs_scalar_to_array:
-+                    param_data, loaded_weight = adjust_scalar_to_fused_array(
-+                        param_data, loaded_weight, 0)
-+
-+                assert param_data.shape == loaded_weight.shape
-+                param_data.copy_(loaded_weight)
-+                return
-+            current_shard_offset = 0
-+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-+                                            False)
-+            shard_offsets: list[tuple[int, int, int]] = []
-+            for i, output_size in enumerate(self.output_sizes):
-+                shard_offsets.append((i, current_shard_offset, output_size))
-+                current_shard_offset += output_size
-+            packed_dim = getattr(param, "packed_dim", None)
-+            for shard_id, shard_offset, shard_size in shard_offsets:
-+                # Special case for Quantization.
-+                # If quantized, we need to adjust the offset and size to account
-+                # for the packing.
-+                if packed_dim == output_dim:
-+                    shard_size = shard_size // param.pack_factor
-+                    shard_offset = shard_offset // param.pack_factor
-+                    # Special case for Marlin.
-+                    shard_size, shard_offset = adjust_marlin_shard(
-+                        param, shard_size, shard_offset)
-+
-+                if use_bitsandbytes_4bit:
-+                    index = list(itertools.accumulate([0] + self.output_sizes))
-+                    orig_offsets = {
-+                        str(i): (index[i], size)
-+                        for i, size in enumerate(self.output_sizes)
-+                    }
-+                    orig_offsets["total"] = (self.output_size, 0)
-+                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
-+                        param, orig_offsets, str(shard_id))
-+
-+                loaded_weight_shard = loaded_weight.narrow(
-+                    output_dim, shard_offset, shard_size)
-+                self.weight_loader(param, loaded_weight_shard, shard_id)
-+            return
-+
-+        assert loaded_shard_id < len(self.output_sizes)
-+        tp_rank = get_tensor_model_parallel_rank()
-+        tp_size = get_tensor_model_parallel_world_size()
-+        if output_dim is not None:
-+            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
-+            shard_size = self.output_sizes[loaded_shard_id] // tp_size
-+            # Special case for quantization.
-+            # If quantized, we need to adjust the offset and size to account
-+            # for the packing.
-+            packed_dim = getattr(param, "packed_dim", None)
-+            if packed_dim == output_dim:
-+                shard_size = shard_size // param.pack_factor
-+                shard_offset = shard_offset // param.pack_factor
-+                # Special case for Marlin.
-+                shard_size, shard_offset = adjust_marlin_shard(
-+                    param, shard_size, shard_offset)
-+
-+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-+                                            False)
-+            is_sharded_weight = getattr(param, "is_sharded_weight", False)
-+            # bitsandbytes loads the weights of the specific portion
-+            # no need to narrow
-+            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-+
-+            if use_bitsandbytes_4bit:
-+                shard_size = loaded_weight.shape[output_dim]
-+                shard_offset = loaded_weight.shape[output_dim] * \
-+                    loaded_shard_id
-+
-+            #param_data = param_data.narrow(output_dim, shard_offset,
-+            #                               shard_size)
-+            if is_quantization:
-+                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
-+            else:
-+                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
-+
-+            start_idx = tp_rank * shard_size
-+            if not is_sharded_weight:
-+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-+                                                     shard_size)
-+        # Special case for AQLM codebooks.
-+        elif is_metadata:
-+            # metadata indicates fixed size concatenated along dim 0
-+            shard_size = loaded_weight.shape[0]
-+            shard_offset = loaded_shard_id * shard_size
-+            param_data = param_data.narrow(0, shard_offset, shard_size)
-+
-+        # Special case for per-tensor scales in fused case.
-+        elif needs_scalar_to_array:
-+            param_data, loaded_weight = adjust_scalar_to_fused_array(
-+                param_data, loaded_weight, loaded_shard_id)
-+
-+        else:
-+            ignore_warning = getattr(param, "ignore_warning", False)
-+            if not ignore_warning:
-+                logger.warning(
-+                    "Loading a weight without `output_dim` attribute in "
-+                    "MergedColumnParallelLinear, assume the weight is "
-+                    "the same for all partitions.")
-+
-+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
-+        assert param_data.shape == loaded_weight.shape
-+        param_data.copy_(loaded_weight)
-+
-+    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
-+                                           loaded_weight: torch.Tensor):
-+        """
-+        Handle special case for models where MLP layers are already
-+        fused on disk. In this case, we have no shard id. This function
-+        determmines the shard id by splitting these layers and then calls
-+        the weight loader using the shard id.
-+
-+        An example of a model with these fused layers:
-+        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
-+        """
-+
-+        current_shard_offset = 0
-+        shard_offsets: list[tuple[int, int, int]] = []
-+        for i, output_size in enumerate(self.output_sizes):
-+            shard_offsets.append((i, current_shard_offset, output_size))
-+            current_shard_offset += output_size
-+
-+        for shard_id, shard_offset, shard_size in shard_offsets:
-+            # Special case for Quantization.
-+            # If quantized, we need to adjust the offset and size to account
-+            # for the packing.
-+            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
-+                                  )) and param.packed_dim == param.output_dim:
-+                shard_size, shard_offset = \
-+                    param.adjust_shard_indexes_for_packing(
-+                    shard_size=shard_size, shard_offset=shard_offset)
-+
-+            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
-+                                                       shard_offset,
-+                                                       shard_size)
-+            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
-+
-+    def weight_loader_v2(self,
-+                         param: BasevLLMParameter,
-+                         loaded_weight: torch.Tensor,
-+                         loaded_shard_id: Optional[int] = None):
-+        if loaded_shard_id is None:
-+            if isinstance(param, PerTensorScaleParameter):
-+                param.load_merged_column_weight(loaded_weight=loaded_weight,
-+                                                shard_id=0)
-+                return
-+            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
-+                param.load_merged_column_weight(loaded_weight=loaded_weight)
-+                return
-+            # TODO: @dsikka - move to parameter.py
-+            self._load_fused_module_from_checkpoint(param, loaded_weight)
-+            return
-+
-+        assert loaded_shard_id < len(self.output_sizes)
-+
-+        tp_size = get_tensor_model_parallel_world_size()
-+
-+        if isinstance(param, BlockQuantScaleParameter):
-+            from vllm.model_executor.layers.quantization.fp8 import (
-+                Fp8LinearMethod, Fp8MoEMethod)
-+            assert self.quant_method is not None
-+            assert isinstance(self.quant_method,
-+                              (Fp8LinearMethod, Fp8MoEMethod))
-+            weight_block_size = self.quant_method.quant_config.weight_block_size
-+            assert weight_block_size is not None
-+            block_n, _ = weight_block_size[0], weight_block_size[1]
-+            shard_offset = (
-+                (sum(self.output_sizes[:loaded_shard_id]) + block_n - 1) //
-+                block_n) // tp_size
-+            shard_size = ((self.output_sizes[loaded_shard_id] + block_n - 1) //
-+                          block_n // tp_size)
-+        else:
-+            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
-+            shard_size = self.output_sizes[loaded_shard_id] // tp_size
-+
-+        param.load_merged_column_weight(loaded_weight=loaded_weight,
-+                                        shard_id=loaded_shard_id,
-+                                        shard_offset=shard_offset,
-+                                        shard_size=shard_size)
-+
-+
-+class QKVParallelLinear(ColumnParallelLinear):
-+    """Linear layers for the attention's QKV transformation.
-+
-+    Linear layers for the linear transformation of the query, key, and value
-+    vectors in the attention layer. The weight matrix is concatenated along
-+    the output dimension. The layer is parallelized along the head dimension.
-+    When the number of key/value heads is smaller than the number of query
-+    heads (e.g., multi-query/grouped-query attention), the key/value head may
-+    be replicated while the query heads are partitioned.
-+
-+    Args:
-+        hidden_size: input hidden state size of the transformer.
-+        head_size: size of each attention head.
-+        total_num_heads: total number of attention query heads.
-+        total_num_kv_heads: total number of attention key/value heads. If
-+                            None, assume total_num_kv_heads = total_num_heads.
-+        bias: If true, add bias.
-+        skip_bias_add: This was added to enable performance optimizations where
-+                       bias can be fused with other element-wise operations. we
-+                       skip adding bias but instead return it.
-+        params_dtype: Data type for the parameters.
-+        quant_config: Quantization configure.
-+        prefix: The name of the layer in the state dict, including all parents
-+                        (e.g. model.layers.0.qkv_proj)
-+    """
-+
-+    def __init__(self,
-+                 hidden_size: int,
-+                 head_size: int,
-+                 total_num_heads: int,
-+                 total_num_kv_heads: Optional[int] = None,
-+                 bias: bool = True,
-+                 skip_bias_add: bool = False,
-+                 params_dtype: Optional[torch.dtype] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 prefix: str = ""):
-+        self.hidden_size = hidden_size
-+        self.head_size = head_size
-+        self.total_num_heads = total_num_heads
-+        if total_num_kv_heads is None:
-+            total_num_kv_heads = total_num_heads
-+        self.total_num_kv_heads = total_num_kv_heads
-+        # Divide the weight matrix along the last dimension.
-+        tp_size = get_tensor_model_parallel_world_size()
-+        self.num_heads = divide(self.total_num_heads, tp_size)
-+        if tp_size >= self.total_num_kv_heads:
-+            self.num_kv_heads = 1
-+            self.num_kv_head_replicas = divide(tp_size,
-+                                               self.total_num_kv_heads)
-+        else:
-+            self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)
-+            self.num_kv_head_replicas = 1
-+        input_size = self.hidden_size
-+        output_size = (self.num_heads +
-+                       2 * self.num_kv_heads) * tp_size * self.head_size
-+        self.output_sizes = [
-+            self.num_heads * self.head_size * tp_size,  # q_proj
-+            self.num_kv_heads * self.head_size * tp_size,  # k_proj
-+            self.num_kv_heads * self.head_size * tp_size,  # v_proj 
-+        ]
-+
-+        super().__init__(input_size=input_size,
-+                         output_size=output_size,
-+                         bias=bias,
-+                         gather_output=False,
-+                         skip_bias_add=skip_bias_add,
-+                         params_dtype=params_dtype,
-+                         quant_config=quant_config,
-+                         prefix=prefix)
-+
-+    def _get_shard_offset_mapping(self, loaded_shard_id: str):
-+        shard_offset_mapping = {
-+            "q": 0,
-+            "k": self.num_heads * self.head_size,
-+            "v": (self.num_heads + self.num_kv_heads) * self.head_size,
-+            "total": (self.num_heads + 2 * self.num_kv_heads) * self.head_size
-+        }
-+        return shard_offset_mapping.get(loaded_shard_id)
-+
-+    def _get_shard_size_mapping(self, loaded_shard_id: str):
-+        shard_size_mapping = {
-+            "q": self.num_heads * self.head_size,
-+            "k": self.num_kv_heads * self.head_size,
-+            "v": self.num_kv_heads * self.head_size,
-+        }
-+        return shard_size_mapping.get(loaded_shard_id)
-+
-+    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
-+                                           loaded_weight: torch.Tensor):
-+        """
-+        Handle special case for models where QKV layers are already 
-+        fused on disk. In this case, we have no shard id. This function
-+        determmines the shard id by splitting these layers and then calls
-+        the weight loader using the shard id.
-+
-+        An example of a model with these fused layers:
-+        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
-+        """
-+        shard_offsets = [
-+            # (shard_id, shard_offset, shard_size)
-+            ("q", 0, self.total_num_heads * self.head_size),
-+            ("k", self.total_num_heads * self.head_size,
-+             self.total_num_kv_heads * self.head_size),
-+            ("v",
-+             (self.total_num_heads + self.total_num_kv_heads) * self.head_size,
-+             self.total_num_kv_heads * self.head_size),
-+        ]
-+
-+        for shard_id, shard_offset, shard_size in shard_offsets:
-+            # Special case for Quantization.
-+            # If quantized, we need to adjust the offset and size to account
-+            # for the packing.
-+            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
-+                                  )) and param.packed_dim == param.output_dim:
-+                shard_size, shard_offset = \
-+                    param.adjust_shard_indexes_for_packing(
-+                    shard_size=shard_size, shard_offset=shard_offset)
-+
-+            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
-+                                                       shard_offset,
-+                                                       shard_size)
-+            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
-+
-+    def weight_loader_v2(self,
-+                         param: BasevLLMParameter,
-+                         loaded_weight: torch.Tensor,
-+                         loaded_shard_id: Optional[str] = None):
-+        if loaded_shard_id is None:  # special case for certain models
-+            if isinstance(param, PerTensorScaleParameter):
-+                param.load_qkv_weight(loaded_weight=loaded_weight, shard_id=0)
-+                return
-+            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
-+                param.load_qkv_weight(loaded_weight=loaded_weight)
-+                return
-+            # TODO: @dsikka - move to parameter.py
-+            self._load_fused_module_from_checkpoint(param, loaded_weight)
-+            return
-+
-+        assert loaded_shard_id in ["q", "k", "v"]
-+
-+        shard_offset = self._get_shard_offset_mapping(loaded_shard_id)
-+        shard_size = self._get_shard_size_mapping(loaded_shard_id)
-+
-+        # Note(simon): This is needed for Qwen3's fp8 quantization.
-+        if isinstance(param, BlockQuantScaleParameter):
-+            assert self.quant_method is not None
-+            assert hasattr(self.quant_method, "quant_config")
-+            weight_block_size = self.quant_method.quant_config.weight_block_size
-+            block_n, _ = weight_block_size[0], weight_block_size[1]
-+            shard_offset = (shard_offset + block_n - 1) // block_n
-+            shard_size = (shard_size + block_n - 1) // block_n
-+
-+        param.load_qkv_weight(loaded_weight=loaded_weight,
-+                              num_heads=self.num_kv_head_replicas,
-+                              shard_id=loaded_shard_id,
-+                              shard_offset=shard_offset,
-+                              shard_size=shard_size)
-+
-+    def weight_loader(self,
-+                      param: Parameter,
-+                      loaded_weight: torch.Tensor,
-+                      loaded_shard_id: Optional[str] = None):
-+
-+        # Special case for GGUF
-+        # initialize GGUF param after we know the quantize type
-+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-+        if is_gguf_weight_type:
-+            idx_map = {"q": 0, "k": 1, "v": 2}
-+            if loaded_shard_id is not None:
-+                param.data[idx_map[loaded_shard_id]].copy_(loaded_weight)
-+                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
-+            else:
-+                param.shard_weight_type = {
-+                    k: loaded_weight.item()
-+                    for k in idx_map
-+                }
-+            return
-+
-+        if is_gguf_weight:
-+            tp_size = get_tensor_model_parallel_world_size()
-+            tp_rank = get_tensor_model_parallel_rank()
-+
-+            output_dim = getattr(param, "output_dim", None)
-+            shard_size = loaded_weight.size(output_dim) // tp_size
-+            start_idx = tp_rank * shard_size
-+
-+            if loaded_shard_id is not None:
-+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-+                                                     shard_size)
-+                param.shard_id.append(loaded_shard_id)
-+                param.shard_id_map[loaded_shard_id] = len(param.data_container)
-+                param.data_container.append(loaded_weight)
-+                if len(param.data_container) == 3:
-+                    self.qweight = param.materialize_nested()
-+                return
-+
-+        param_data = param.data
-+        output_dim = getattr(param, "output_dim", None)
-+        # Special case for AQLM codebooks.
-+        is_metadata = getattr(param, "is_metadata", False)
-+
-+        # Special case for per-tensor scales in fused case.
-+        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
-+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
-+
-+        if loaded_shard_id is None:
-+            # Loaded weight is already fused on disk (qkv).
-+            # (e.g., Phi-3's qkv_proj).
-+            if output_dim is None:
-+                if needs_scalar_to_array:
-+                    param_data, loaded_weight = adjust_scalar_to_fused_array(
-+                        param_data, loaded_weight, 0)
-+
-+                assert param_data.shape == loaded_weight.shape
-+                param_data.copy_(loaded_weight)
-+                return
-+            shard_offsets = [
-+                # (shard_id, shard_offset, shard_size)
-+                ("q", 0, self.total_num_heads * self.head_size),
-+                ("k", self.total_num_heads * self.head_size,
-+                 self.total_num_kv_heads * self.head_size),
-+                ("v", (self.total_num_heads + self.total_num_kv_heads) *
-+                 self.head_size, self.total_num_kv_heads * self.head_size),
-+            ]
-+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-+                                            False)
-+
-+            packed_dim = getattr(param, "packed_dim", None)
-+            for shard_id, shard_offset, shard_size in shard_offsets:
-+                # Special case for Quantized Weights.
-+                # If quantized, we need to adjust the offset and size to account
-+                # for the packing.
-+                if packed_dim == output_dim:
-+                    shard_size = shard_size // param.pack_factor
-+                    shard_offset = shard_offset // param.pack_factor
-+
-+                    # Special case for Marlin.
-+                    shard_size, shard_offset = adjust_marlin_shard(
-+                        param, shard_size, shard_offset)
-+
-+                if use_bitsandbytes_4bit:
-+                    orig_qkv_offsets = {
-+                        "q": (0, self.total_num_heads * self.head_size),
-+                        "k": (self.total_num_heads * self.head_size,
-+                              self.total_num_kv_heads * self.head_size),
-+                        "v":
-+                        ((self.total_num_heads + self.total_num_kv_heads) *
-+                         self.head_size,
-+                         self.total_num_kv_heads * self.head_size),
-+                        "total":
-+                        ((self.total_num_heads + 2 * self.total_num_kv_heads) *
-+                         self.head_size, 0)
-+                    }
-+
-+                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
-+                        param, orig_qkv_offsets, shard_id)
-+
-+                loaded_weight_shard = loaded_weight.narrow(
-+                    output_dim, shard_offset, shard_size)
-+                self.weight_loader(param, loaded_weight_shard, shard_id)
-+            return
-+
-+        tp_rank = get_tensor_model_parallel_rank()
-+        assert loaded_shard_id in ["q", "k", "v"]
-+
-+        # If output dim is defined, use the default loading process.
-+        if output_dim is not None:
-+            if loaded_shard_id == "q":
-+                shard_offset = 0
-+                shard_size = self.num_heads * self.head_size
-+            elif loaded_shard_id == "k":
-+                shard_offset = self.num_heads * self.head_size
-+                shard_size = self.num_kv_heads * self.head_size
-+            elif loaded_shard_id == "v":
-+                shard_offset = (self.num_heads +
-+                                self.num_kv_heads) * self.head_size
-+                shard_size = self.num_kv_heads * self.head_size
-+            # Special case for Quantized Weights.
-+            # If quantized, we need to adjust the offset and size to account
-+            # for the packing.
-+            packed_dim = getattr(param, "packed_dim", None)
-+            if packed_dim == output_dim:
-+                shard_size = shard_size // param.pack_factor
-+                shard_offset = shard_offset // param.pack_factor
-+
-+                # Special case for Marlin.
-+                shard_size, shard_offset = adjust_marlin_shard(
-+                    param, shard_size, shard_offset)
-+
-+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-+                                            False)
-+            is_sharded_weight = getattr(param, "is_sharded_weight", False)
-+            # bitsandbytes loads the weights of the specific portion
-+            # no need to narrow
-+            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-+
-+            if use_bitsandbytes_4bit:
-+                orig_qkv_offsets = {
-+                    "q": (0, self.num_heads * self.head_size),
-+                    "k": (self.num_heads * self.head_size,
-+                          self.num_kv_heads * self.head_size),
-+                    "v":
-+                    ((self.num_heads + self.num_kv_heads) * self.head_size,
-+                     self.num_kv_heads * self.head_size),
-+                    "total":
-+                    ((self.num_heads + 2 * self.num_kv_heads) * self.head_size,
-+                     0)
-+                }
-+                shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
-+                    param, orig_qkv_offsets, loaded_shard_id)
-+
-+            #param_data = param_data.narrow(output_dim, shard_offset,
-+            #                               shard_size)
-+            if len(param_data.shape)==1 or is_quantization:
-+                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
-+            else:
-+                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
-+
-+            if loaded_shard_id == "q":
-+                shard_id = tp_rank
-+            else:
-+                shard_id = tp_rank // self.num_kv_head_replicas
-+            start_idx = shard_id * shard_size
-+
-+            if not is_sharded_weight:
-+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-+                                                     shard_size)
-+
-+        # Special case for for AQLM codebooks.
-+        elif is_metadata:
-+            # metadata indicates fixed size concatenated along dim 0
-+            shard_size = loaded_weight.shape[0]
-+            shard_index = ["q", "k", "v"].index(loaded_shard_id)
-+            param_data = param_data.narrow(0, shard_index * shard_size,
-+                                           shard_size)
-+        # Special case for per-tensor scales in fused case.
-+        elif needs_scalar_to_array:
-+            param_data, loaded_weight = adjust_scalar_to_fused_array(
-+                param_data, loaded_weight, loaded_shard_id)
-+        else:
-+            ignore_warning = getattr(param, "ignore_warning", False)
-+            if not ignore_warning:
-+                logger.warning(
-+                    "Loading a weight without `output_dim` attribute in "
-+                    "QKVParallelLinear, assume the weight is the same "
-+                    "for all partitions.")
-+
-+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
-+        assert param_data.shape == loaded_weight.shape
-+        param_data.copy_(loaded_weight)
-+
-+
-+class RowParallelLinear(LinearBase):
-+    """Linear layer with row parallelism.
-+
-+    The linear layer is defined as Y = XA + b. A is parallelized along
-+    its first dimension and X along its second dimension as:
-+               -   -
-+              | A_1 |
-+              | .   |
-+          A = | .   |        X = [X_1, ..., X_p]
-+              | .   |
-+              | A_p |
-+               -   -
-+    Arguments:
-+        input_size: first dimension of matrix A.
-+        output_size: second dimension of matrix A.
-+        bias: If true, add bias. Note that bias is not parallelized.
-+        input_is_parallel: If true, we assume that the input is already
-+                           split across the GPUs and we do not split
-+                           again.
-+        skip_bias_add: This was added to enable performance optimization where
-+                       bias can be fused with other element-wise operations.
-+                       We skip adding bias but instead return it.
-+        params_dtype: Data type for the parameters.
-+        quant_config: Quantization configure.
-+    """
-+
-+    def __init__(self,
-+                 input_size: int,
-+                 output_size: int,
-+                 bias: bool = True,
-+                 input_is_parallel: bool = True,
-+                 skip_bias_add: bool = False,
-+                 params_dtype: Optional[torch.dtype] = None,
-+                 reduce_results: bool = True,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 prefix: str = ""):
-+        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
-+                         quant_config, prefix)
-+
-+        self.input_is_parallel = input_is_parallel
-+        self.reduce_results = reduce_results
-+
-+        # Divide the weight matrix along the last dimension.
-+        self.tp_rank = get_tensor_model_parallel_rank()
-+        self.tp_size = get_tensor_model_parallel_world_size()
-+        self.input_size_per_partition = divide(input_size, self.tp_size)
-+        assert self.quant_method is not None
-+
-+        self.quant_method.create_weights(
-+            layer=self,
-+            input_size_per_partition=self.input_size_per_partition,
-+            output_partition_sizes=[self.output_size],
-+            input_size=self.input_size,
-+            output_size=self.output_size,
-+            params_dtype=self.params_dtype,
-+            weight_loader=(
-+                self.weight_loader_v2 if self.quant_method.__class__.__name__
-+                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
-+        if not reduce_results and (bias and not skip_bias_add):
-+            raise ValueError("When not reduce the results, adding bias to the "
-+                             "results can lead to incorrect results")
-+
-+        if bias:
-+            self.bias = Parameter(
-+                torch.empty(self.output_size, dtype=params_dtype))
-+            set_weight_attrs(self.bias, {
-+                "output_dim": 0,
-+                "weight_loader": self.weight_loader,
-+            })
-+        else:
-+            self.register_parameter("bias", None)
-+
-+    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
-+        tp_rank = get_tensor_model_parallel_rank()
-+        tp_size = get_tensor_model_parallel_world_size()
-+        input_dim = getattr(param, "input_dim", None)
-+        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
-+        is_sharded_weight = getattr(param, "is_sharded_weight", False)
-+        # bitsandbytes loads the weights of the specific portion
-+        # no need to narrow
-+        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-+
-+        # Special case for GGUF
-+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-+        if is_gguf_weight_type:
-+            param.weight_type = loaded_weight.item()
-+
-+        # Materialize GGUF UninitializedParameter
-+        if is_gguf_weight and isinstance(param, UninitializedParameter):
-+            weight_shape = list(loaded_weight.shape)
-+            if input_dim:
-+                weight_shape[input_dim] = weight_shape[input_dim] // tp_size
-+            param.materialize(tuple(weight_shape), dtype=loaded_weight.dtype)
-+
-+        param_data = param.data
-+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
-+        if input_dim is not None and not is_sharded_weight:
-+            #shard_size = param_data.shape[input_dim]
-+            if is_quantization:
-+                shard_size = param_data.shape[input_dim]
-+            else:
-+                shard_size = param_data.shape[int(not(input_dim))]
-+            start_idx = tp_rank * shard_size
-+            loaded_weight = loaded_weight.narrow(input_dim, start_idx,
-+                                                 shard_size)
-+
-+        # Special case for loading scales off disk, which often do not
-+        # have a shape (such as in the case of AutoFP8).
-+        if len(loaded_weight.shape) == 0:
-+            loaded_weight = loaded_weight.reshape(1)
-+
-+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
-+        assert param_data.shape == loaded_weight.shape
-+        param_data.copy_(loaded_weight)
-+
-+    def weight_loader_v2(self, param: BasevLLMParameter,
-+                         loaded_weight: torch.Tensor):
-+
-+        # Special case for loading scales off disk, which often do not
-+        # have a shape (such as in the case of AutoFP8).
-+        if len(loaded_weight.shape) == 0:
-+            assert loaded_weight.numel() == 1
-+            loaded_weight = loaded_weight.reshape(1)
-+
-+        param.load_row_parallel_weight(loaded_weight=loaded_weight)
-+
-+    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
-+        if self.input_is_parallel:
-+            input_parallel = input_
-+        else:
-+            tp_rank = get_tensor_model_parallel_rank()
-+            splitted_input = split_tensor_along_last_dim(
-+                input_, num_partitions=self.tp_size)
-+            input_parallel = splitted_input[tp_rank].contiguous()
-+
-+        # Matrix multiply.
-+        assert self.quant_method is not None
-+        # Only fuse bias add into GEMM for rank 0 (this ensures that
-+        # bias will not get added more than once in TP>1 case)
-+        bias_ = None if (self.tp_rank > 0 or self.skip_bias_add) else self.bias
-+        output_parallel = self.quant_method.apply(self,
-+                                                  input_parallel,
-+                                                  bias=bias_)
-+        if self.reduce_results and self.tp_size > 1:
-+            output = tensor_model_parallel_all_reduce(output_parallel)
-+        else:
-+            output = output_parallel
-+
-+        output_bias = self.bias if self.skip_bias_add else None
-+
-+        return output, output_bias
-+
-+    def extra_repr(self) -> str:
-+        s = f"input_features={self.input_size_per_partition}"
-+        s += f", output_features={self.output_size}"
-+        s += f", bias={self.bias is not None}"
-+        s += f", tp_size={self.tp_size}"
-+        s += f", reduce_results={self.reduce_results}"
-+        return s
-diff --git a/vllm/model_executor/layers/logits_processor.py b/vllm/model_executor/layers/logits_processor.py
-index cdc67ca83..8230f7618 100644
---- a/vllm/model_executor/layers/logits_processor.py
-+++ b/vllm/model_executor/layers/logits_processor.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """A layer that compute logits from hidden_stats."""
- import inspect
-@@ -64,6 +65,11 @@ class LogitsProcessor(nn.Module):
-         sampling_metadata: Optional[SamplingMetadata] = None,
-         embedding_bias: Optional[torch.Tensor] = None,
-     ) -> Optional[torch.Tensor]:
-+    
-+        if sampling_metadata is None:
-+            logits = self._get_logits(hidden_states, lm_head, embedding_bias)
-+            return logits
++            # Support gemm_tn->gemm_nn here
++            if not envs.MACA_VLLM_USE_TN_2_NN or len(param_data.shape)==1 or is_quantization:
++                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
++            else:
++                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
++            
+             if loaded_shard_id == "q":
+                 shard_id = tp_rank
+             else:
+@@ -1128,6 +1177,10 @@ class QKVParallelLinear(ColumnParallelLinear):
+                     "QKVParallelLinear, assume the weight is the same "
+                     "for all partitions.")
+ 
++        # Support gemm_tn->gemm_nn here
++        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
++            loaded_weight = loaded_weight.t()
 +
-         if self.logits_as_input:
-             logits = hidden_states
-         else:
+         assert param_data.shape == loaded_weight.shape
+         param_data.copy_(loaded_weight)
+ 
+@@ -1245,8 +1298,15 @@ class RowParallelLinear(LinearBase):
+             param.materialize(tuple(weight_shape), dtype=loaded_weight.dtype)
+ 
+         param_data = param.data
++        # Support gemm_tn->gemm_nn here
++        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
++        
+         if input_dim is not None and not is_sharded_weight:
+-            shard_size = param_data.shape[input_dim]
++            # Support gemm_tn->gemm_nn here
++            if not envs.MACA_VLLM_USE_TN_2_NN or is_quantization:
++                shard_size = param_data.shape[input_dim]
++            else:
++                shard_size = param_data.shape[int(not(input_dim))]
+             start_idx = tp_rank * shard_size
+             loaded_weight = loaded_weight.narrow(input_dim, start_idx,
+                                                  shard_size)
+@@ -1256,6 +1316,9 @@ class RowParallelLinear(LinearBase):
+         if len(loaded_weight.shape) == 0:
+             loaded_weight = loaded_weight.reshape(1)
+ 
++        # Support gemm_tn->gemm_nn here
++        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
++            loaded_weight = loaded_weight.t()
+         assert param_data.shape == loaded_weight.shape
+         param_data.copy_(loaded_weight)
+ 
 diff --git a/vllm/model_executor/layers/quantization/__init__.py b/vllm/model_executor/layers/quantization/__init__.py
-index 6ded3874f..41b1f85b1 100644
+index 1cb23e7a1..c3c184a08 100644
 --- a/vllm/model_executor/layers/quantization/__init__.py
 +++ b/vllm/model_executor/layers/quantization/__init__.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from typing import Dict, List, Type
-@@ -8,6 +9,7 @@ from vllm.model_executor.layers.quantization.base_config import (
- QUANTIZATION_METHODS: List[str] = [
+@@ -9,6 +9,7 @@ from vllm.model_executor.layers.quantization.base_config import (
+ QuantizationMethods = Literal[
      "aqlm",
      "awq",
 +    "gptq",
      "deepspeedfp",
      "tpu_int8",
      "fp8",
-@@ -20,7 +22,7 @@ QUANTIZATION_METHODS: List[str] = [
-     "gptq_marlin_24",
+@@ -23,7 +24,7 @@ QuantizationMethods = Literal[
      "gptq_marlin",
+     "gptq_bitblas",
      "awq_marlin",
 -    "gptq",
-+    #"gptq",
++    # "gptq",
      "compressed-tensors",
      "bitsandbytes",
      "qqq",
-@@ -115,8 +117,10 @@ def get_quantization_config(quantization: str) -> Type[QuantizationConfig]:
-         "marlin": MarlinConfig,
+@@ -126,9 +127,9 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
+         "bitblas": BitBLASConfig,
          "gguf": GGUFConfig,
          "gptq_marlin_24": GPTQMarlin24Config,
 -        "gptq_marlin": GPTQMarlinConfig,
--        "awq_marlin": AWQMarlinConfig,
-+        #"gptq_marlin": GPTQMarlinConfig,
-+        #"awq_marlin": AWQMarlinConfig,
 +        "gptq_marlin": GPTQConfig,
+         "gptq_bitblas": GPTQBitBLASConfig,
+-        "awq_marlin": AWQMarlinConfig,
 +        "awq_marlin": AWQConfig,
          "gptq": GPTQConfig,
          "compressed-tensors": CompressedTensorsConfig,
          "bitsandbytes": BitsAndBytesConfig,
 diff --git a/vllm/model_executor/layers/quantization/awq.py b/vllm/model_executor/layers/quantization/awq.py
-index ff77af44d..fb07159f1 100644
+index f8bc3ab5e..4503e466e 100644
 --- a/vllm/model_executor/layers/quantization/awq.py
 +++ b/vllm/model_executor/layers/quantization/awq.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -13,7 +13,7 @@ from vllm.model_executor.layers.quantization.base_config import (
+     QuantizationConfig)
+ from vllm.model_executor.parameter import (GroupQuantScaleParameter,
+                                            PackedvLLMParameter)
+-
++from vllm.utils import direct_register_custom_op
  
- from typing import Any, Dict, List, Optional
-@@ -47,7 +48,7 @@ class AWQConfig(QuantizationConfig):
+ class AWQConfig(QuantizationConfig):
+     """Config class for AWQ.
+@@ -50,7 +50,7 @@ class AWQConfig(QuantizationConfig):
          return "awq"
  
-     def get_supported_act_dtypes(self) -> List[torch.dtype]:
+     def get_supported_act_dtypes(self) -> list[torch.dtype]:
 -        return [torch.half]
 +        return [torch.half, torch.bfloat16]
  
      @classmethod
      def get_min_capability(cls) -> int:
-@@ -157,6 +158,12 @@ class AWQLinearMethod(LinearMethodBase):
+@@ -86,6 +86,58 @@ class AWQConfig(QuantizationConfig):
+ def is_layer_skipped_awq(prefix: str, modules_to_not_convert: list[str]):
+     return any(module_name in prefix for module_name in modules_to_not_convert)
+ 
++def _apply_awq_fake(x: torch.Tensor,
++                    qweight: torch.Tensor,
++                    scales: torch.Tensor,
++                    qzeros: torch.Tensor,
++                    bias: torch.Tensor,
++                    pack_factor: int,
++                    group_size: int) -> torch.Tensor:
++    out_shape = ()
++    if group_size % 32:
++        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
++    else:
++        out_shape = (x.shape[:-1] + (qweight.shape[0], ))
++    return torch.empty(out_shape, dtype=x.dtype, device=x.device)
++
++def _apply_awq(x: torch.Tensor,
++               qweight: torch.Tensor,
++               scales: torch.Tensor,
++               qzeros: torch.Tensor,
++               bias: torch.Tensor,
++               pack_factor: int,
++               group_size: int) -> torch.Tensor:
++    out_shape = ()
++    reshaped_x = x.reshape(-1, x.shape[-1])
++    out = torch.empty(0)          
++    # num_tokens >= threshold
++    FP16_MATMUL_HEURISTIC_CONDITION = x.shape[:-1].numel() >= 256
++    # if (FP16_MATMUL_HEURISTIC_CONDITION and reshaped_x.dtype == torch.half) or self.quant_config.group_size != 128:
++    if group_size % 32:
++        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
++        out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)
++        out = torch.matmul(reshaped_x, out)
++    else:
++        num_out_channel = qweight.shape[0]
++        out_shape = (x.shape[:-1] + (num_out_channel, ))
++        temp_space = torch.empty(0, dtype=torch.float32, device=x.device)
++        if reshaped_x.dtype == torch.bfloat16:
++            temp_space = torch.zeros(reshaped_x.shape[0], num_out_channel,
++                                        dtype=torch.float32, device=x.device)
++        out = ops.awq_gemm(reshaped_x, qweight, qzeros, scales,
++                            pack_factor, temp_space,
++                            True if reshaped_x.dtype == torch.bfloat16 else False)
++    if bias is not None:
++        out.add_(bias)
++    return out.reshape(out_shape)
++
++direct_register_custom_op(
++    op_name="_apply_awq",
++    op_func=_apply_awq,
++    mutates_args=[],
++    fake_impl=_apply_awq_fake,
++    tags=(torch.Tag.needs_fixed_stride_order, ),
++)
+ 
+ class AWQLinearMethod(LinearMethodBase):
+     """Linear method for AWQ.
+@@ -168,6 +220,12 @@ class AWQLinearMethod(LinearMethodBase):
                                            requires_grad=False)
          layer.scales = torch.nn.Parameter(layer.scales.data,
                                            requires_grad=False)
 +        # warmup
-+        if self.quant_config.group_size != 128:
++        if self.quant_config.group_size % 32:
 +            pass
 +        else:
 +            qweight = ops.awq_to_gptq_4bit(layer.qweight)
@@ -27720,51 +19162,34 @@ index ff77af44d..fb07159f1 100644
  
      def apply(self,
                layer: torch.nn.Module,
-@@ -165,19 +172,28 @@ class AWQLinearMethod(LinearMethodBase):
-         qweight = layer.qweight
+@@ -177,18 +235,7 @@ class AWQLinearMethod(LinearMethodBase):
          scales = layer.scales
          qzeros = layer.qzeros
-+        out_shape = ()
          pack_factor = self.quant_config.pack_factor
 -        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
-         reshaped_x = x.reshape(-1, x.shape[-1])
-+        out = torch.empty(0)
- 
-         # num_tokens >= threshold
+-        reshaped_x = x.reshape(-1, x.shape[-1])
+-
+-        # num_tokens >= threshold
 -        FP16_MATMUL_HEURISTIC_CONDITION = x.shape[:-1].numel() >= 256
 -
 -        if FP16_MATMUL_HEURISTIC_CONDITION:
-+        FP16_MATMUL_HEURISTIC_CONDITION =x.shape[:-1].numel() >= 256
-+        # if (FP16_MATMUL_HEURISTIC_CONDITION and reshaped_x.dtype == torch.half) or self.quant_config.group_size != 128:
-+        if self.quant_config.group_size != 128:
-+            out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
-             out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)
-             out = torch.matmul(reshaped_x, out)
-         else:
+-            out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)
+-            out = torch.matmul(reshaped_x, out)
+-        else:
 -            out = ops.awq_gemm(reshaped_x, qweight, scales, qzeros,
 -                               pack_factor)
-+            num_out_channel = qweight.shape[0]
-+            out_shape = (x.shape[:-1] + (num_out_channel, ))
-+            temp_space = torch.empty(0, dtype=torch.float32, device=x.device)
-+            if reshaped_x.dtype == torch.bfloat16:
-+                temp_space = torch.zeros(reshaped_x.shape[0], num_out_channel,
-+                                         dtype=torch.float32, device=x.device)
-+            out = ops.awq_gemm(reshaped_x, qweight, qzeros, scales,
-+                               pack_factor, temp_space,
-+                               True if reshaped_x.dtype == torch.bfloat16 else False)
-         if bias is not None:
-             out.add_(bias)
-         return out.reshape(out_shape)
+-        if bias is not None:
+-            out.add_(bias)
+-        return out.reshape(out_shape)
++        group_size = self.quant_config.group_size
++        
++        return torch.ops.vllm._apply_awq(x, qweight, scales, qzeros, 
++                                        bias, pack_factor, group_size)
 diff --git a/vllm/model_executor/layers/quantization/base_config.py b/vllm/model_executor/layers/quantization/base_config.py
-index c0d8553c0..22ad62d96 100644
+index 78c5c75c0..0b50cef26 100644
 --- a/vllm/model_executor/layers/quantization/base_config.py
 +++ b/vllm/model_executor/layers/quantization/base_config.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import inspect
-@@ -103,7 +104,10 @@ class QuantizationConfig(ABC):
+@@ -113,7 +113,10 @@ class QuantizationConfig(ABC):
             this method should only be overwritten by subclasses in exceptional 
             circumstances
          """
@@ -27775,206 +19200,57 @@ index c0d8553c0..22ad62d96 100644
 +            return hf_quant_cfg["quant_method"]
  
      @staticmethod
-     def get_from_keys(config: Dict[str, Any], keys: List[str]) -> Any:
+     def get_from_keys(config: dict[str, Any], keys: list[str]) -> Any:
 diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
-index 6ee3e9362..16d2d7d31 100644
+index 28c62fc5e..1538c371e 100644
 --- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
 +++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from contextlib import suppress
-@@ -193,7 +194,8 @@ class CompressedTensorsConfig(QuantizationConfig):
-                                 min_capability: int,
-                                 error: bool = True) -> bool:
+@@ -197,7 +197,7 @@ class CompressedTensorsConfig(QuantizationConfig):
+                                 error: bool = True,
+                                 match_exact: bool = False) -> bool:
          capability_tuple = current_platform.get_device_capability()
 -
-+        
 +        """
          if capability_tuple is not None:
              capability = capability_tuple.to_int()
-             supported = capability >= min_capability
-@@ -205,6 +207,7 @@ class CompressedTensorsConfig(QuantizationConfig):
+             if match_exact:
+@@ -217,6 +217,8 @@ class CompressedTensorsConfig(QuantizationConfig):
              return supported
          else:
              return False
 +        """
++        return False
+ 
+     def _is_fp4a4_nvfp4(self, weight_quant: BaseModel, input_quant: BaseModel):
  
-     def _is_static_tensor_w8a8(self, weight_quant: BaseModel,
-                                input_quant: BaseModel) -> bool:
-@@ -262,6 +265,16 @@ class CompressedTensorsConfig(QuantizationConfig):
-             input_quant.strategy == QuantizationStrategy.TENSOR)
-         return is_symmetric_activation and is_per_tensor_activation
- 
-+    def _is_dynamic_token_int8_w8a8(self, weight_quant: BaseModel,
-+                     input_quant: BaseModel) -> bool:
-+        # Confirm weights and activations quantized.
-+        if weight_quant is None or input_quant is None:
-+            return False
-+            
-+        is_int8_w8a8 = (weight_quant.type == QuantizationType.INT and input_quant.type == QuantizationType.INT)
-+        return is_int8_w8a8 and self._is_dynamic_token_w8a8(weight_quant, input_quant)  
-+
-+
-     def _is_fp8_w8a16(self, weight_quant: BaseModel,
-                       input_quant: BaseModel) -> bool:
-         # Confirm weights quantized.
 diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
-index db8e8a4b6..53ee538ac 100644
+index bc9d399cf..4beb287f4 100644
 --- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
 +++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import enum
-@@ -51,10 +52,125 @@ class CompressedTensorsMoEMethod(FusedMoEMethodBase):
-             return CompressedTensorsWNA16MoEMethod(quant_config)
-         elif quant_config._is_fp8_w8a8(weight_quant, input_quant):
-             return CompressedTensorsW8A8Fp8MoEMethod(quant_config)
-+        elif quant_config._is_dynamic_token_int8_w8a8(weight_quant, input_quant):
-+            return CompressedTensorsW8A8Int8MoEMethod(quant_config)
-         else:
-             raise RuntimeError(
-                 f"Unsupported FusedMoe scheme: {weight_quant}, {input_quant}")
- 
-+class CompressedTensorsW8A8Int8MoEMethod(CompressedTensorsMoEMethod):
-+    def __init__(
-+        self,
-+        quant_config: "CompressedTensorsConfig"  # type: ignore # noqa E501
-+    ):
-+        self.quant_config = quant_config
-+        self.weight_quant = self.quant_config.target_scheme_map["Linear"].get(
-+                "weights")
-+        self.input_quant = self.quant_config.target_scheme_map["Linear"].get(
-+            "input_activations")
-+
-+        if not (self.weight_quant.strategy == QuantizationStrategy.CHANNEL
-+                and self.input_quant.strategy == QuantizationStrategy.TOKEN):
-+            raise ValueError(
-+                "For INT8 Fused MoE layers, only per-channel scales"
-+                "for activations and per-token scales for activations are supported. Found "
-+                f"{self.weight_quant}, {self.input_quant}")
-+
-+        self.static_input_scales = not self.input_quant.dynamic
-+
-+
-+    def create_weights(self, layer: torch.nn.Module, num_experts: int,
-+                       hidden_size: int, intermediate_size_per_partition: int,
-+                       params_dtype: torch.dtype, **extra_weight_attrs):
-+
-+        params_dtype = torch.int8
-+
-+        # WEIGHTS
-+        w13_weight = torch.nn.Parameter(torch.empty(num_experts,
-+                                                    2 * intermediate_size_per_partition,
-+                                                    hidden_size,
-+                                                    dtype=params_dtype),
-+                                        requires_grad=False)
-+
-+        layer.register_parameter("w13_weight", w13_weight)
-+        set_weight_attrs(w13_weight, extra_weight_attrs)
-+
-+        w2_weight = torch.nn.Parameter(torch.empty(num_experts,
-+                                                   hidden_size,
-+                                                   intermediate_size_per_partition,
-+                                                   dtype=params_dtype),
-+                                                    requires_grad=False)
-+        layer.register_parameter("w2_weight", w2_weight)
-+        set_weight_attrs(w2_weight, extra_weight_attrs)
-+
-+        w13_weight_scale = torch.nn.Parameter(torch.ones(num_experts,
-+                                                         2 * intermediate_size_per_partition,
-+                                                         1,
-+                                                         dtype=torch.float32),
-+                                                         requires_grad=False)
-+        layer.register_parameter("w13_weight_scale", w13_weight_scale)
-+
-+        w2_weight_scale = torch.nn.Parameter(torch.ones(num_experts,
-+                                                        hidden_size,
-+                                                        1,
-+                                                        dtype=torch.float32),
-+                                                        requires_grad=False)
-+        layer.register_parameter("w2_weight_scale", w2_weight_scale)
-+
-+        extra_weight_attrs.update({"quant_method": FusedMoeWeightScaleSupported.CHANNEL.value})
-+        set_weight_attrs(w13_weight_scale, extra_weight_attrs)
-+        set_weight_attrs(w2_weight_scale, extra_weight_attrs)
-+
-+        # INPUT_SCALES
-+        if self.static_input_scales:
-+            raise ValueError(
-+                "For INT8 Fused MoE layers, only dynamic scales"
-+                "for activations are supported. Found "
-+                f"{self.input_quant}")
-+        else:
-+            layer.w13_input_scale = None
-+            layer.w2_input_scale = None
-+
-+
-+    def apply(
-+        self,
-+        layer: torch.nn.Module,
-+        x: torch.Tensor,
-+        router_logits: torch.Tensor,
-+        top_k: int,
-+        renormalize: bool = True,
-+        use_grouped_topk: bool = False,
-+        num_expert_group: Optional[int] = None,
-+        topk_group: Optional[int] = None,
-+        custom_routing_function: Optional[Callable] = None,
-+        scoring_func: str = "softmax",
-+        e_score_correction_bias: Optional[torch.Tensor] = None,
-+    ) -> torch.Tensor:
-+
-+        from vllm.model_executor.layers.fused_moe import fused_experts
-+        topk_weights, topk_ids = FusedMoE.select_experts(
-+            hidden_states=x,
-+            router_logits=router_logits,
-+            use_grouped_topk=use_grouped_topk,
-+            top_k=top_k,
-+            renormalize=renormalize,
-+            topk_group=topk_group,
-+            num_expert_group=num_expert_group,
-+            custom_routing_function=custom_routing_function,
-+            scoring_func=scoring_func,
-+            e_score_correction_bias=e_score_correction_bias,)
-+
-+        return fused_experts(x,
-+                             layer.w13_weight,
-+                             layer.w2_weight,
-+                             topk_weights=topk_weights,
-+                             topk_ids=topk_ids,
-+                             inplace=True,
-+                             use_int8_w8a8=True,
-+                             w1_scale=layer.w13_weight_scale,
-+                             w2_scale=layer.w2_weight_scale,
-+                             a1_scale=layer.w13_input_scale,
-+                             a2_scale=layer.w2_input_scale)
+@@ -96,7 +96,6 @@ class CompressedTensorsMoEMethod(FusedMoEMethodBase):
+             raise RuntimeError(
+                 f"Unsupported FusedMoe scheme: {weight_quant}, {input_quant}")
  
+-
  class CompressedTensorsW8A8Fp8MoEMethod(CompressedTensorsMoEMethod):
  
+     def __init__(
 diff --git a/vllm/model_executor/layers/quantization/gptq.py b/vllm/model_executor/layers/quantization/gptq.py
-index 0cb77a754..65900e08d 100644
+index d3ab1be3b..492a9c931 100644
 --- a/vllm/model_executor/layers/quantization/gptq.py
 +++ b/vllm/model_executor/layers/quantization/gptq.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import enum
-@@ -55,7 +56,8 @@ class GPTQConfig(QuantizationConfig):
+@@ -85,8 +85,9 @@ class GPTQConfig(QuantizationConfig):
+         return "gptq"
  
      @classmethod
-     def get_supported_act_dtypes(cls) -> List[torch.dtype]:
++
+     def get_supported_act_dtypes(cls) -> list[torch.dtype]:
 -        return [torch.half]
 +        return [torch.half, torch.bfloat16]
-+        #return [torch.half]
  
      @classmethod
      # Need to figure it out
-@@ -219,7 +221,7 @@ class GPTQLinearMethod(LinearMethodBase):
+@@ -251,7 +252,7 @@ class GPTQLinearMethod(LinearMethodBase):
  
          # exllama needs to shuffle the weight after the weight is loaded
          # here we do the shuffle on first forward pass
@@ -27983,10 +19259,11 @@ index 0cb77a754..65900e08d 100644
              if self.quant_config.desc_act:
                  layer.g_idx.data = torch.argsort(layer.g_idx).to(torch.int)
              else:
-@@ -229,6 +231,41 @@ class GPTQLinearMethod(LinearMethodBase):
+@@ -261,6 +262,63 @@ class GPTQLinearMethod(LinearMethodBase):
              layer.exllama_state = ExllamaState.READY
              ops.gptq_shuffle(layer.qweight, layer.g_idx,
                               self.quant_config.weight_bits)
++            
 +            if layer.scales.dtype != torch.bfloat16:
 +                perm_space = torch.empty(0)
 +                temp_space = torch.empty(0)
@@ -28022,10 +19299,31 @@ index 0cb77a754..65900e08d 100644
 +                ops.gptq_shuffle(layer.qweight, layer.g_idx,
 +                                 self.quant_config.weight_bits)
 +
++                """
++                perm_space = torch.empty(0)
++                if self.quant_config.weight_bits == 4:
++                    # warmup
++                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*8, dtype=layer.scales.dtype, device="cuda")
++                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
++                                   layer.scales, layer.g_idx,
++                                   layer.exllama_state == ExllamaState.READY,
++                                   self.quant_config.weight_bits,
++                                   self.quant_config.group_size,
++                                   perm_space)
++                if self.quant_config.weight_bits == 8:
++                    # warmup
++                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*4, dtype=layer.scales.dtype, device="cuda")
++                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
++                                   layer.scales, layer.g_idx,
++                                   layer.exllama_state == ExllamaState.READY,
++                                   self.quant_config.weight_bits,
++                                   self.quant_config.group_size,
++                                   perm_space)
++                """
  
      def apply(self,
                layer: torch.nn.Module,
-@@ -237,10 +274,25 @@ class GPTQLinearMethod(LinearMethodBase):
+@@ -269,10 +327,25 @@ class GPTQLinearMethod(LinearMethodBase):
          out_shape = x.shape[:-1] + (layer.qweight.shape[-1], )
          reshaped_x = x.reshape(-1, x.shape[-1])
  
@@ -28036,6 +19334,7 @@ index 0cb77a754..65900e08d 100644
 +                if self.quant_config.desc_act:
 +                    perm_space = torch.empty(reshaped_x.shape[0], reshaped_x.shape[1],
 +                                             dtype=torch.float16, device="cuda")
++                    
 +                if reshaped_x.dtype == torch.bfloat16:
 +                    temp_space = torch.zeros(reshaped_x.shape[0], layer.qweight.shape[1],
 +                                             dtype=torch.float32, device="cuda")
@@ -28048,593 +19347,640 @@ index 0cb77a754..65900e08d 100644
 +                               self.quant_config.group_size,
 +                               perm_space, temp_space,
 +                               True if reshaped_x.dtype == torch.bfloat16 else False)
-+
          if bias is not None:
              output.add_(bias)
          return output.reshape(out_shape)
 diff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py
-index 56fa597e2..04b93eea1 100644
+index 3aa23f068..daae03c73 100644
 --- a/vllm/model_executor/layers/quantization/moe_wna16.py
 +++ b/vllm/model_executor/layers/quantization/moe_wna16.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- from typing import Any, Callable, Dict, List, Optional
-@@ -35,24 +36,24 @@ class MoeWNA16Config(QuantizationConfig):
-         self.linear_quant_method = linear_quant_method
-         self.full_config = full_config
-         self.use_marlin = False
--        if self.linear_quant_method == "gptq":
--            self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(
--                full_config)
--        elif self.linear_quant_method == "awq":
--            capability_tuple = current_platform.get_device_capability()
--            device_capability = (-1 if capability_tuple is None else
--                                 capability_tuple.to_int())
--            awq_min_capability = AWQConfig.get_min_capability()
--            if device_capability < awq_min_capability:
--                raise ValueError(
--                    "The quantization method moe_wna16 + awq is not supported "
--                    "for the current GPU. "
--                    f"Minimum capability: {awq_min_capability}. "
--                    f"Current capability: {device_capability}.")
--            self.use_marlin = AWQMarlinConfig.is_awq_marlin_compatible(
--                full_config)
--        else:
--            raise ValueError("moe_wna16 only support gptq and awq.")
-+        #if self.linear_quant_method == "gptq":
-+        #    self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(
-+        #        full_config)
-+        #elif self.linear_quant_method == "awq":
-+        #    capability_tuple = current_platform.get_device_capability()
-+        #    device_capability = (-1 if capability_tuple is None else
-+        #                         capability_tuple.to_int())
-+        #    awq_min_capability = AWQConfig.get_min_capability()
-+        #    if device_capability < awq_min_capability:
-+        #        raise ValueError(
-+        #            "The quantization method moe_wna16 + awq is not supported "
-+        #            "for the current GPU. "
-+        #            f"Minimum capability: {awq_min_capability}. "
-+        #            f"Current capability: {device_capability}.")
-+        #    self.use_marlin = AWQMarlinConfig.is_awq_marlin_compatible(
-+        #        full_config)
-+        #else:
-+        #    raise ValueError("moe_wna16 only support gptq and awq.")
- 
+@@ -41,6 +41,7 @@ class MoeWNA16Config(QuantizationConfig):
+             AWQMarlinConfig)
+         from vllm.model_executor.layers.quantization.gptq_marlin import (
+             GPTQMarlinConfig)
++        """
+         if self.linear_quant_method == "gptq":
+             self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(
+                 full_config)
+@@ -59,7 +60,7 @@ class MoeWNA16Config(QuantizationConfig):
+                 full_config)
+         else:
+             raise ValueError("moe_wna16 only support gptq and awq.")
+-
++        """
          if modules_to_not_convert is None:
              self.modules_to_not_convert = []
+         else:
+@@ -390,13 +391,22 @@ class MoeWNA16Method(FusedMoEMethodBase):
+                                     loaded_weight: torch.Tensor,
+                                     weight_name: str, shard_id: str,
+                                     expert_id: int):
++            if layer.ep_size > 1:
++                global_expert_id = expert_id
++                expert_id = layer._map_global_expert_id_to_local_expert_id(expert_id)
++                if expert_id == -1:
++                    return
++
+             if "g_idx" in weight_name:
+                 return
+             if not layer.quant_config.has_zp and "qzeros" in weight_name:
+                 return
+ 
+             device = get_tp_group().device
+-            tp_rank = get_tensor_model_parallel_rank()
++            if layer.ep_size > 1:
++                tp_rank = 0
++            else:
++                tp_rank = get_tensor_model_parallel_rank()
+             loaded_weight = loaded_weight.to(device)
+             shard_size = layer.intermediate_size_per_partition
+ 
+@@ -433,16 +443,26 @@ class MoeWNA16Method(FusedMoEMethodBase):
+                     layer.group_size_div_factor, 1)
+ 
+             if "w13_qzeros" in weight_name:
+-                tensor = loaded_weight.view(layer.tp_size, -1,
+-                                            loaded_weight.size(1))[tp_rank]
++                if layer.ep_size > 1 :
++                    tensor = loaded_weight.view(-1, param.data[expert_id].shape[0] // 2,
++                                                loaded_weight.size(1))[tp_rank]
++                else:
++                    tensor = loaded_weight.view(layer.tp_size, -1,
++                                                loaded_weight.size(1))[tp_rank]
+                 if shard_id == "w1":
+                     param.data[expert_id, :shard_size // 2] = tensor
+                 else:
+                     param.data[expert_id, shard_size // 2:] = tensor
+             elif "w2_qzeros" in weight_name:
+-                param.data[expert_id] = loaded_weight.view(
+-                    loaded_weight.size(0), layer.tp_size, -1)[:, tp_rank]
++                if layer.ep_size > 1 :
++                    param.data[expert_id] = loaded_weight.view(
++                        loaded_weight.size(0), -1, param.data[expert_id].shape[1])[:, tp_rank]
++                else:
++                    param.data[expert_id] = loaded_weight.view(
++                        loaded_weight.size(0), layer.tp_size, -1)[:, tp_rank]
+             else:
++                if layer.ep_size > 1:
++                    expert_id = global_expert_id
+                 weight_loader(param, loaded_weight, weight_name, shard_id,
+                               expert_id)
+ 
 diff --git a/vllm/model_executor/layers/quantization/utils/fp8_utils.py b/vllm/model_executor/layers/quantization/utils/fp8_utils.py
-index 9895537c2..5dc604ff2 100644
+index 08dc99e07..aaff93307 100644
 --- a/vllm/model_executor/layers/quantization/utils/fp8_utils.py
 +++ b/vllm/model_executor/layers/quantization/utils/fp8_utils.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Adapted from https://github.com/sgl-project/sglang/pull/2575
-@@ -28,7 +29,8 @@ current_platform_fp8_dtype = (torch.float8_e4m3fnuz
- def is_fp8(x: Union[torch.dtype, torch.Tensor]) -> bool:
-     if isinstance(x, torch.Tensor):
-         x = x.dtype
--    return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz
-+    #return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz
-+    return False
+@@ -5,7 +5,7 @@
+ import functools
+ import json
+ import os
+-from typing import Any, Callable, Optional, Union
++from typing import Any, List, Callable, Optional, Union
  
+ import torch
  
+@@ -33,7 +33,7 @@ def cutlass_scaled_mm(
+     B: torch.Tensor,
+     As: torch.Tensor,
+     Bs: torch.Tensor,
+-    block_size: list[int],
++    block_size: List[int],
+     output_dtype: torch.dtype = torch.float16,
+ ) -> torch.Tensor:
+     return ops.cutlass_scaled_mm(A,
+@@ -48,7 +48,7 @@ def rocm_aiter_gemm_w8a8_blockscale_impl(
+     B: torch.Tensor,
+     As: torch.Tensor,
+     Bs: torch.Tensor,
+-    block_size: list[int],
++    block_size: List[int],
+     output_dtype: torch.dtype = torch.float16,
+ ) -> torch.Tensor:
+     import aiter as rocm_aiter
+@@ -61,7 +61,7 @@ def rocm_aiter_gemm_w8a8_blockscale_fake(
+     B: torch.Tensor,
+     As: torch.Tensor,
+     Bs: torch.Tensor,
+-    block_size: list[int],
++    block_size: List[int],
+     output_dtype: torch.dtype = torch.float16,
+ ) -> torch.Tensor:
+ 
+@@ -103,7 +103,7 @@ def dispatch_w8a8_blockscale_func(
  def apply_w8a8_block_fp8_linear(
+     input: torch.Tensor,
+     weight: torch.Tensor,
+-    block_size: list[int],
++    block_size: List[int],
+     weight_scale: torch.Tensor,
+     input_scale: Optional[torch.Tensor] = None,
+     bias: Optional[torch.Tensor] = None,
+@@ -156,7 +156,7 @@ def apply_w8a8_block_fp8_linear(
+ def apply_w8a8_block_fp8_linear_fake(
+     input: torch.Tensor,
+     weight: torch.Tensor,
+-    block_size: list[int],
++    block_size: List[int],
+     weight_scale: torch.Tensor,
+     input_scale: Optional[torch.Tensor] = None,
+     bias: Optional[torch.Tensor] = None,
+@@ -536,7 +536,7 @@ def w8a8_block_fp8_matmul(
+     B: torch.Tensor,
+     As: torch.Tensor,
+     Bs: torch.Tensor,
+-    block_size: list[int],
++    block_size: List[int],
+     output_dtype: torch.dtype = torch.float16,
+ ) -> torch.Tensor:
+     """This function performs matrix multiplication with block-wise
+diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
+index 9de233896..53678ebeb 100644
+--- a/vllm/model_executor/layers/rotary_embedding.py
++++ b/vllm/model_executor/layers/rotary_embedding.py
+@@ -34,7 +34,7 @@ from vllm.model_executor.custom_op import CustomOp
+ from vllm.platforms import current_platform
+ 
+ if current_platform.is_cuda():
+-    from vllm.vllm_flash_attn.layers.rotary import apply_rotary_emb
++    from flash_attn.layers.rotary import apply_rotary_emb
+ 
+ 
+ def _rotate_neox(x: torch.Tensor) -> torch.Tensor:
 diff --git a/vllm/model_executor/layers/vocab_parallel_embedding.py b/vllm/model_executor/layers/vocab_parallel_embedding.py
-index e409094dd..493de2d5e 100644
+index 0f636d83a..489b621c9 100644
 --- a/vllm/model_executor/layers/vocab_parallel_embedding.py
 +++ b/vllm/model_executor/layers/vocab_parallel_embedding.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -6,6 +6,7 @@ from dataclasses import dataclass
+ from typing import Optional
  
- from dataclasses import dataclass
-@@ -135,13 +136,15 @@ class VocabParallelEmbeddingShardIndices:
+ import torch
++from vllm import envs
+ import torch.nn.functional as F
+ from torch.nn.parameter import Parameter, UninitializedParameter
+ 
+@@ -31,7 +32,13 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
+                        output_size: int, params_dtype: torch.dtype,
+                        **extra_weight_attrs):
+         """Create weights for embedding layer."""
+-        weight = Parameter(torch.empty(sum(output_partition_sizes),
++        if envs.MACA_VLLM_USE_TN_2_NN:
++            weight = Parameter(torch.empty(input_size_per_partition,
++                                       sum(output_partition_sizes),
++                                       dtype=params_dtype),
++                           requires_grad=False)
++        else:
++            weight = Parameter(torch.empty(sum(output_partition_sizes),
+                                        input_size_per_partition,
+                                        dtype=params_dtype),
+                            requires_grad=False)
+@@ -43,11 +50,17 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
+               layer: torch.nn.Module,
+               x: torch.Tensor,
+               bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+-        return dispatch_unquantized_gemm()(x, layer.weight, bias)
++        if envs.MACA_VLLM_USE_TN_2_NN and x.shape[-1] == layer.weight.shape[0]:
++            return dispatch_unquantized_gemm()(x, layer.weight.t(), bias)
++        else:
++            return dispatch_unquantized_gemm()(x, layer.weight, bias)
+ 
+     def embedding(self, layer: torch.nn.Module,
+                   input_: torch.Tensor) -> torch.Tensor:
+-        return F.embedding(input_, layer.weight)
++        if envs.MACA_VLLM_USE_TN_2_NN:
++            return F.embedding(input_, layer.weight.t())
++        else:
++            return F.embedding(input_, layer.weight)
+ 
+ 
+ def pad_vocab_size(vocab_size: int,
+@@ -138,13 +151,15 @@ class VocabParallelEmbeddingShardIndices:
          assert self.num_added_elements <= self.num_added_elements_padded
  
  
 -@torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
-+#@torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
++# @torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
 +@torch.jit.script
  def get_masked_input_and_mask(
          input_: torch.Tensor, org_vocab_start_index: int,
          org_vocab_end_index: int, num_org_vocab_padding: int,
          added_vocab_start_index: int,
-         added_vocab_end_index: int) -> Tuple[torch.Tensor, torch.Tensor]:
+         added_vocab_end_index: int) -> tuple[torch.Tensor, torch.Tensor]:
      # torch.compile will fuse all of the pointwise ops below
 +    # torch.jit.script will fuse all of the pointwise ops below
      # into a single kernel, making it very fast
      org_vocab_mask = (input_ >= org_vocab_start_index) & (
          input_ < org_vocab_end_index)
-@@ -260,13 +263,22 @@ class VocabParallelEmbedding(torch.nn.Module):
-             self.shard_indices.added_vocab_end_index -
+@@ -264,12 +279,12 @@ class VocabParallelEmbedding(torch.nn.Module):
              self.shard_indices.added_vocab_start_index)
  
--        self.linear_method.create_weights(self,
-+        if isinstance(self.linear_method, UnquantizedEmbeddingMethod):
-+            self.linear_method.create_weights(self,
-                                           self.embedding_dim,
-                                           [self.num_embeddings_per_partition],
-                                           self.embedding_dim,
-                                           self.num_embeddings_padded,
-                                           params_dtype=params_dtype,
-                                           weight_loader=self.weight_loader)
-+        else:
-+            self.linear_method.create_weights(self,
-+                                          self.num_embeddings_per_partition,
-+                                          [self.embedding_dim],
-+                                          self.embedding_dim,
-+                                          self.num_embeddings_padded,
-+                                          params_dtype=params_dtype,
-+                                          weight_loader=self.weight_loader)
+         self.quant_method.create_weights(self,
+-                                         self.embedding_dim,
+-                                         [self.num_embeddings_per_partition],
+-                                         self.embedding_dim,
+-                                         self.num_embeddings_padded,
+-                                         params_dtype=params_dtype,
+-                                         weight_loader=self.weight_loader)
++                                        self.embedding_dim,
++                                        [self.num_embeddings_per_partition],
++                                        self.embedding_dim,
++                                        self.num_embeddings_padded,
++                                        params_dtype=params_dtype,
++                                        weight_loader=self.weight_loader)
  
      @classmethod
      def _get_indices(cls, vocab_size_padded: int, org_vocab_size_padded: int,
-diff --git a/vllm/model_executor/model_loader/__init__.py b/vllm/model_executor/model_loader/__init__.py
-index 9048c70c7..ea7a063d9 100644
---- a/vllm/model_executor/model_loader/__init__.py
-+++ b/vllm/model_executor/model_loader/__init__.py
-@@ -1,8 +1,11 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -389,6 +404,13 @@ class VocabParallelEmbedding(torch.nn.Module):
+         # Copy the data. Select chunk corresponding to current shard.
+         loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
+ 
++        if envs.MACA_VLLM_USE_TN_2_NN:
++            loaded_weight = loaded_weight.t()
++            # we should padding last dimension after weight transpose
++            padding_needed = max(self.num_embeddings_per_partition - loaded_weight.size(-1), 0)
++            if padding_needed:
++                loaded_weight = torch.nn.functional.pad(loaded_weight, (0, padding_needed), value=0)
++
+         if current_platform.is_hpu():
+             # FIXME(kzawora): Weight copy with slicing bugs out on Gaudi here,
+             # so we're using a workaround. Remove this when fixed in
+diff --git a/vllm/model_executor/models/baichuan.py b/vllm/model_executor/models/baichuan.py
+index 0de5de5e8..c8dfa32a0 100644
+--- a/vllm/model_executor/models/baichuan.py
++++ b/vllm/model_executor/models/baichuan.py
+@@ -394,7 +394,7 @@ class BaiChuanBaseForCausalLM(nn.Module, SupportsLoRA, SupportsPP,
+         self.lm_head = ParallelLMHead(config.vocab_size,
+                                       config.hidden_size,
+                                       quant_config=quant_config)
+-        self.lm_head.weight.weight_loader = self.lm_head_weight_loader
++        # self.lm_head.weight.weight_loader = self.lm_head_weight_loader
+         if self.config.tie_word_embeddings:
+             self.lm_head.weight = self.model.embed_tokens.weight
+         self.logits_processor = LogitsProcessor(config.vocab_size)
+diff --git a/vllm/model_executor/models/deepseek.py b/vllm/model_executor/models/deepseek.py
+index 2f0202f1e..04d843f31 100644
+--- a/vllm/model_executor/models/deepseek.py
++++ b/vllm/model_executor/models/deepseek.py
+@@ -57,6 +57,7 @@ from .utils import (AutoWeightsLoader, extract_layer_index,
+                     make_empty_intermediate_tensors_factory, make_layers,
+                     maybe_prefix)
  
- from torch import nn
++import vllm.envs as envs
  
- from vllm.config import VllmConfig
-+from vllm.distributed.kv_transfer.kv_transfer_utils import (
-+    maybe_register_PD_disagg_hooks)
- from vllm.model_executor.model_loader.loader import (BaseModelLoader,
-                                                      get_model_loader)
- from vllm.model_executor.model_loader.utils import (
-@@ -10,8 +13,14 @@ from vllm.model_executor.model_loader.utils import (
+ class DeepseekMLP(nn.Module):
  
+@@ -152,8 +153,17 @@ class DeepseekMoE(nn.Module):
+         w2s = torch._utils._unflatten_dense_tensors(self.w2, w2)
+         for data, param in zip(w2s, w2):
+             param.data = data
+-
+         self.w2 = self.w2.view(len(w2), *w2s[0].shape)
++            
++        if envs.MACA_VLLM_USE_TN_2_NN:
++            self.w1 = self.w1.permute(0,2,1).contiguous()   
++            for expert, w in zip(self.experts, self.w1):
++                expert.gate_up_proj.weight.data = w.permute(1,0)
++                
++            self.w2 = self.w2.permute(0, 2, 1).contiguous()
++            for expert, w in zip(self.experts, self.w2):
++                expert.down_proj.weight.data = w.permute(1, 0)
++        
  
- def get_model(*, vllm_config: VllmConfig) -> nn.Module:
-+
-     loader = get_model_loader(vllm_config.load_config)
--    return loader.load_model(vllm_config=vllm_config)
-+
-+    model = loader.load_model(vllm_config=vllm_config)
-+
-+    maybe_register_PD_disagg_hooks(model, vllm_config)
-+
-+    return model
+     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+         num_tokens, hidden_dim = hidden_states.shape
+diff --git a/vllm/model_executor/models/deepseek_mtp.py b/vllm/model_executor/models/deepseek_mtp.py
+index 6e6e74b0d..9dd8c7930 100644
+--- a/vllm/model_executor/models/deepseek_mtp.py
++++ b/vllm/model_executor/models/deepseek_mtp.py
+@@ -52,10 +52,6 @@ class DeepSeekMultiTokenPredictorLayer(nn.Module):
+         quant_config: Optional[QuantizationConfig] = None,
+     ) -> None:
+         super().__init__()
+-        self.embed_tokens = VocabParallelEmbedding(
+-            config.vocab_size,
+-            config.hidden_size,
+-        )
  
+         self.enorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+         self.hnorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+@@ -74,8 +70,6 @@ class DeepSeekMultiTokenPredictorLayer(nn.Module):
+         inputs_embeds: Optional[torch.Tensor] = None,
+         spec_step_index: int = 0,
+     ) -> torch.Tensor:
+-        if inputs_embeds is None:
+-            inputs_embeds = self.embed_tokens(input_ids)
+         assert inputs_embeds is not None
+         # masking inputs at position 0, as not needed by MTP
+         inputs_embeds[positions == 0] = 0
+@@ -112,6 +106,10 @@ class DeepSeekMultiTokenPredictor(nn.Module):
+             for idx in range(self.mtp_start_layer_idx,
+                              self.mtp_start_layer_idx + self.num_mtp_layers)
+         })
++        self.embed_tokens = VocabParallelEmbedding(
++            config.vocab_size,
++            config.hidden_size,
++        )
  
- __all__ = [
-diff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py
-index 2a2c2523b..c8f9cb1d9 100644
---- a/vllm/model_executor/model_loader/loader.py
-+++ b/vllm/model_executor/model_loader/loader.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+         self.logits_processor = LogitsProcessor(config.vocab_size)
+ 
+@@ -123,6 +121,8 @@ class DeepSeekMultiTokenPredictor(nn.Module):
+         inputs_embeds: Optional[torch.Tensor] = None,
+         spec_step_idx: int = 0,
+     ) -> torch.Tensor:
++        if inputs_embeds is None:
++            inputs_embeds = self.embed_tokens(input_ids)
+         current_step_idx = (spec_step_idx % self.num_mtp_layers)
+         return self.layers[str(self.mtp_start_layer_idx + current_step_idx)](
+             input_ids,
+@@ -217,6 +217,11 @@ class DeepSeekMTP(nn.Module, SupportsPP):
+                 # Skip loading extra bias for GPTQ models.
+                 if name.endswith(".bias") and name not in params_dict:
+                     continue
++                # According to DeepSeek-V3 Technical Report, MTP modules
++                # shares embedding layer. We only load the first weights.
++                if (spec_layer != self.model.mtp_start_layer_idx
++                        and ".layers" not in name):
++                    continue
  
- # ruff: noqa: SIM117
-@@ -153,6 +154,30 @@ def _initialize_model(
-         return model_class(**kwargs)
- 
- 
-+def _process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
-+                                   target_device: torch.device) -> None:
-+    for _, module in model.named_modules():
-+        quant_method = getattr(module, "quant_method", None)
-+        if isinstance(quant_method, QuantizeMethodBase):
-+            # When quant methods need to process weights after loading
-+            # (for repacking, quantizing, etc), they expect parameters
-+            # to be on the global target device. This scope is for the
-+            # case where cpu offloading is used, where we will move the
-+            # parameters onto device for processing and back off after.
-+            with device_loading_context(module, target_device):
-+                quant_method.process_weights_after_loading(module)
-+
-+    # Currently only used by MLA.
-+    # NOTE: This intentionally happens after other modules so we can easily
-+    # decompress the weights for MLA.
-+    for _, module in model.named_modules():
-+        if isinstance(module, Attention) and \
-+            hasattr(module, "process_weights_after_loading"):
-+            # TODO(lucas): see if there is a way to unify the signatures
-+            # of process_weights_after_loading
-+            module.process_weights_after_loading(model_config.dtype)
-+
-+
- class BaseModelLoader(ABC):
-     """Base class for model loaders."""
- 
-@@ -394,23 +419,8 @@ class DefaultModelLoader(BaseModelLoader):
-                         "Following weights were not initialized from "
-                         f"checkpoint: {weights_not_loaded}")
- 
--            for _, module in model.named_modules():
--                quant_method = getattr(module, "quant_method", None)
--                if isinstance(quant_method, QuantizeMethodBase):
--                    # When quant methods need to process weights after loading
--                    # (for repacking, quantizing, etc), they expect parameters
--                    # to be on the global target device. This scope is for the
--                    # case where cpu offloading is used, where we will move the
--                    # parameters onto device for processing and back off after.
--                    with device_loading_context(module, target_device):
--                        quant_method.process_weights_after_loading(module)
--                if isinstance(module, Attention) and \
--                    hasattr(module, "process_weights_after_loading"):
--                    # When attention modules need to process weights after
--                    # currently only used by MLA
--                    # TODO(lucas): see if there is a way to unify the signatures
--                    # of process_weights_after_loading
--                    module.process_weights_after_loading(model_config.dtype)
-+            _process_weights_after_loading(model, model_config, target_device)
-+
-         return model.eval()
- 
- 
-@@ -429,29 +439,15 @@ class DummyModelLoader(BaseModelLoader):
-     def load_model(self, vllm_config: VllmConfig) -> nn.Module:
-         device_config = vllm_config.device_config
-         model_config = vllm_config.model_config
-+        target_device = torch.device(device_config.device)
-         with set_default_torch_dtype(model_config.dtype):
--            with torch.device(device_config.device):
-+            with target_device:
-                 model = _initialize_model(vllm_config=vllm_config)
-             # NOTE(woosuk): For accurate performance evaluation, we assign
-             # random values to the weights.
-             initialize_dummy_weights(model)
- 
--            for _, module in model.named_modules():
--                quant_method = getattr(module, "quant_method", None)
--                if quant_method is not None:
--                    # When quant methods need to process weights after loading
--                    # (for repacking, quantizing, etc), they expect parameters
--                    # to be on the global target device. This scope is for the
--                    # case where cpu offloading is used, where we will move the
--                    # parameters onto device for processing and back off after.
--                    with device_loading_context(
--                            module, torch.device(device_config.device)):
--                        quant_method.process_weights_after_loading(module)
--                if isinstance(module, Attention) and \
--                    hasattr(module, "process_weights_after_loading"):
--                    # When attention modules need to process weights after
--                    # currently only used by MLA
--                    module.process_weights_after_loading(model_config.dtype)
-+            _process_weights_after_loading(model, model_config, target_device)
-         return model.eval()
- 
- 
-@@ -632,6 +628,7 @@ class ShardedStateLoader(BaseModelLoader):
-     def load_model(self, vllm_config: VllmConfig) -> nn.Module:
-         device_config = vllm_config.device_config
-         model_config = vllm_config.model_config
-+        target_device = torch.device(device_config.device)
-         from safetensors.torch import safe_open
- 
-         from vllm.distributed import get_tensor_model_parallel_rank
-@@ -640,18 +637,10 @@ class ShardedStateLoader(BaseModelLoader):
-                                                  model_config.revision)
- 
-         with set_default_torch_dtype(model_config.dtype):
--            with torch.device(device_config.device):
-+            with target_device:
-                 model = _initialize_model(vllm_config=vllm_config)
--                for _, module in model.named_modules():
--                    quant_method = getattr(module, "quant_method", None)
--                    if quant_method is not None:
--                        quant_method.process_weights_after_loading(module)
--                    if isinstance(module, Attention) and \
--                        hasattr(module, "process_weights_after_loading"):
--                        # When attention modules need to process weights after
--                        # currently only used by MLA
--                        module.process_weights_after_loading(
--                            model_config.dtype)
-+                _process_weights_after_loading(model, model_config,
-+                                               target_device)
-             rank = get_tensor_model_parallel_rank()
-             pattern = os.path.join(
-                 local_model_path,
-@@ -1401,16 +1390,7 @@ class RunaiModelStreamerLoader(BaseModelLoader):
-                 self._get_weights_iterator(model_weights,
-                                            model_config.revision))
- 
--            for _, module in model.named_modules():
--                quant_method = getattr(module, "quant_method", None)
--                if quant_method is not None:
--                    with device_loading_context(module, target_device):
--                        quant_method.process_weights_after_loading(module)
--                if isinstance(module, Attention) and \
--                    hasattr(module, "process_weights_after_loading"):
--                    # When attention modules need to process weights after
--                    # currently only used by MLA
--                    module.process_weights_after_loading(model_config.dtype)
-+            _process_weights_after_loading(model, model_config, target_device)
-         return model.eval()
- 
- 
-diff --git a/vllm/model_executor/models/baichuan_moe.py b/vllm/model_executor/models/baichuan_moe.py
+                 param = params_dict[name]
+                 weight_loader = param.weight_loader
+@@ -253,17 +258,25 @@ class DeepSeekMTP(nn.Module, SupportsPP):
+         """
+         Rewrite the weight name to match the format of the original model.
+         Add .mtp_block for modules in transformer layer block for spec layer
++        and rename shared layer weights to be top level.
+         """
+         spec_layer_weight_names = [
+             "embed_tokens", "enorm", "hnorm", "eh_proj", "shared_head"
+         ]
++        shared_weight_names = ["embed_tokens"]
+         spec_layer_weight = False
++        shared_weight = False
+         for weight_name in spec_layer_weight_names:
+             if weight_name in name:
+                 spec_layer_weight = True
++                if weight_name in shared_weight_names:
++                    shared_weight = True
+                 break
+         if not spec_layer_weight:
+             # treat rest weights as weights for transformer layer block
+             name = name.replace(f"model.layers.{spec_layer}.",
+                                 f"model.layers.{spec_layer}.mtp_block.")
++        elif shared_weight:
++            # treat shared weights as top level weights
++            name = name.replace(f"model.layers.{spec_layer}.", "model.")
+         return name
+diff --git a/vllm/model_executor/models/ernie45.py b/vllm/model_executor/models/ernie45.py
 new file mode 100644
-index 000000000..a1c7c4679
+index 000000000..fcc7a1f17
 --- /dev/null
-+++ b/vllm/model_executor/models/baichuan_moe.py
-@@ -0,0 +1,689 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+import time
-+"""Inference-only Baichuan-MOE model."""
-+from transformers.configuration_utils import PretrainedConfig
-+class BaiChuanMoEConfig(PretrainedConfig):
-+    model_type = "baichuan-moe"
-+    keys_to_ignore_at_inference = ["past_key_values"]
++++ b/vllm/model_executor/models/ernie45.py
+@@ -0,0 +1,43 @@
++# SPDX-License-Identifier: Apache-2.0
++# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 +
-+    def __init__(
-+        self,
-+        vocab_size=64000,
-+        hidden_size=4096,
-+        intermediate_size=11008,
-+        num_hidden_layers=32,
-+        num_attention_heads=32,
-+        hidden_act="silu",
-+        max_position_embeddings=4096,
-+        initializer_range=0.02,
-+        rms_norm_eps=1e-6,
-+        rope_base=1e6,
-+        use_cache=True,
-+        pad_token_id=0,
-+        bos_token_id=1,
-+        eos_token_id=2,
-+        tie_word_embeddings=False,
-+        moe_experts_fixed=0,
-+        moe_experts_selected=2,
-+        moe_experts_routed=8,
-+        num_experts_fixed_per_layer=None, # "0,0,0,1,0,2"
-+        num_experts_selected_per_layer=None, # "1,2,1,1,1,2"
-+        num_experts_routed_per_layer=None, # "1,8,1,8,1,16"
-+        **kwargs,
-+    ):
-+        self.vocab_size = vocab_size
-+        self.max_position_embeddings = max_position_embeddings
-+        self.hidden_size = hidden_size
-+        self.intermediate_size = intermediate_size
-+        self.num_hidden_layers = num_hidden_layers
-+        self.num_attention_heads = num_attention_heads
-+        self.hidden_act = hidden_act
-+        self.initializer_range = initializer_range
-+        self.rms_norm_eps = rms_norm_eps
-+        self.rope_base = rope_base
-+        self.use_cache = use_cache
-+        self.moe_experts_fixed = moe_experts_fixed
-+        self.moe_experts_selected = moe_experts_selected
-+        self.moe_experts_routed = moe_experts_routed
-+        if num_experts_routed_per_layer:
-+            self.num_experts_routed_per_layer = [int(_.strip()) for _ in num_experts_routed_per_layer.split(",")]
-+            assert len(self.num_experts_routed_per_layer) == self.num_hidden_layers
-+            assert all([_ >= 1 for _ in self.num_experts_routed_per_layer])
-+        else:
-+            self.num_experts_routed_per_layer = None
++# Copyright 2025 The Baidu team.
++# Copyright 2023 The vLLM team.
++# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
++#
++# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
++# and OPT implementations in this library. It has been modified from its
++# original forms to accommodate minor architectural differences compared
++# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
++#
++# Licensed under the Apache License, Version 2.0 (the "License");
++# you may not use this file except in compliance with the License.
++# You may obtain a copy of the License at
++#
++#     http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing, software
++# distributed under the License is distributed on an "AS IS" BASIS,
++# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++# See the License for the specific language governing permissions and
++# limitations under the License.
++"""Inference-only Erine model compatible with HuggingFace weights."""
++from vllm.config import VllmConfig
++from vllm.model_executor.models.llama import LlamaForCausalLM
 +
-+        if num_experts_selected_per_layer:
-+            self.num_experts_selected_per_layer = [int(_.strip()) for _ in num_experts_selected_per_layer.split(",")]
-+            assert len(self.num_experts_selected_per_layer) == self.num_hidden_layers
-+            assert all([x >= y for x, y in zip(self.num_experts_routed_per_layer, self.num_experts_selected_per_layer)])
-+        else:
-+            self.num_experts_selected_per_layer = None
++from .utils import PPMissingLayer
 +
-+        if num_experts_fixed_per_layer:
-+            self.num_experts_fixed_per_layer = [int(_.strip()) for _ in num_experts_fixed_per_layer.split(",")]
-+            assert len(self.num_experts_fixed_per_layer) == self.num_hidden_layers
-+        else:
-+            self.num_experts_fixed_per_layer = None
-+
-+        super().__init__(
-+            pad_token_id=pad_token_id,
-+            bos_token_id=bos_token_id,
-+            eos_token_id=eos_token_id,
-+            tie_word_embeddings=tie_word_embeddings,
-+            **kwargs,
-+        )
 +
-+import math
-+import copy
-+from typing import List, Optional, Iterable, Tuple
++class Ernie4_5_ForCausalLM(LlamaForCausalLM):
++
++    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
++        super().__init__(vllm_config=vllm_config, prefix=prefix)
++        # Hack Llama model to fit HF format Ernie4.5 dense implementation
++        # Attention difference between Ernie and Llama:
++        # 1. rotary_dim and no Neox style.
++        # 2. There is no bias for o_proj in attention
++        for layer in self.model.layers:
++            if not isinstance(layer, PPMissingLayer):
++                layer.self_attn.rotary_emb.is_neox_style = False
++                layer.self_attn.o_proj.bias = None
++                layer.self_attn.o_proj.skip_bias_add = True
+\ No newline at end of file
+diff --git a/vllm/model_executor/models/ernie45_moe.py b/vllm/model_executor/models/ernie45_moe.py
+new file mode 100644
+index 000000000..fb6390275
+--- /dev/null
++++ b/vllm/model_executor/models/ernie45_moe.py
+@@ -0,0 +1,585 @@
++# SPDX-License-Identifier: Apache-2.0
++# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
++
++# Copyright 2025 The Baidu team.
++# Copyright 2023 The vLLM team.
++# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
++#
++# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
++# and OPT implementations in this library. It has been modified from its
++# original forms to accommodate minor architectural differences compared
++# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
++#
++# Licensed under the Apache License, Version 2.0 (the "License");
++# you may not use this file except in compliance with the License.
++# You may obtain a copy of the License at
++#
++#     http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing, software
++# distributed under the License is distributed on an "AS IS" BASIS,
++# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++# See the License for the specific language governing permissions and
++# limitations under the License.
++"""Inference-only ErineMoE model compatible with HuggingFace weights."""
++from collections.abc import Iterable
++from typing import Any, Optional, Union
++
 +import torch
 +from torch import nn
-+import torch.nn.functional as F
-+from transformers.activations import ACT2FN
++from transformers import PretrainedConfig
 +
-+from vllm.attention import Attention, AttentionMetadata
-+
-+from vllm.config import CacheConfig, LoRAConfig, VllmConfig
-+from vllm.model_executor.layers.fused_moe import fused_moe
++from vllm.attention import Attention
++from vllm.compilation.decorators import support_torch_compile
++from vllm.config import CacheConfig, VllmConfig
++from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
++from vllm.logger import init_logger
++from vllm.model_executor.layers.activation import SiluAndMul
++from vllm.model_executor.layers.fused_moe import FusedMoE
 +from vllm.model_executor.layers.layernorm import RMSNorm
-+from vllm.model_executor.layers.linear import (LinearMethodBase,
-+                                               ColumnParallelLinear,
++from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
 +                                               QKVParallelLinear,
 +                                               ReplicatedLinear,
-+                                               MergedColumnParallelLinear,
 +                                               RowParallelLinear)
-+from vllm.model_executor.layers.activation import SiluAndMul,GeluAndMul
 +from vllm.model_executor.layers.logits_processor import LogitsProcessor
-+from vllm.model_executor.layers.quantization.base_config import (
-+    QuantizationConfig)
++from vllm.model_executor.layers.quantization import QuantizationConfig
 +from vllm.model_executor.layers.rotary_embedding import get_rope
-+from vllm.model_executor.layers.sampler import Sampler, SamplerOutput 
 +from vllm.model_executor.layers.vocab_parallel_embedding import (
-+    DEFAULT_VOCAB_PADDING_SIZE, ParallelLMHead, VocabParallelEmbedding)
-+from vllm.distributed import (get_pp_group,
-+                              get_tensor_model_parallel_rank,
-+                              get_tensor_model_parallel_world_size,
-+                              tensor_model_parallel_all_reduce)
-+
++    ParallelLMHead, VocabParallelEmbedding)
++from vllm.model_executor.model_loader.weight_utils import (
++    default_weight_loader, maybe_remap_kv_scale_name)
 +from vllm.model_executor.sampling_metadata import SamplingMetadata
-+from vllm.model_executor.utils import set_weight_attrs
-+#from vllm.model_executor.weight_utils import (default_weight_loader,
-+#                                              hf_model_weights_iterator)
-+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
 +from vllm.sequence import IntermediateTensors
++
 +from .interfaces import SupportsPP
-+from .utils import PPMissingLayer, is_pp_missing_parameter, make_layers, make_empty_intermediate_tensors_factory, maybe_prefix
++from .utils import (PPMissingLayer, extract_layer_index,
++                    is_pp_missing_parameter,
++                    make_empty_intermediate_tensors_factory, make_layers,
++                    maybe_prefix)
++
++logger = init_logger(__name__)
++
++
++class Ernie4_5_MoeMLP(nn.Module):
 +
-+class MLP(nn.Module):
 +    def __init__(
-+            self,
-+            hidden_size: int,
-+            intermediate_size: int,
-+            hidden_act: str,
-+            quant_config: Optional[QuantizationConfig] = None,
-+            prefix: str = ""
-+    ):
++        self,
++        hidden_size: int,
++        intermediate_size: int,
++        hidden_act: str,
++        use_bias: bool = False,
++        quant_config: Optional[QuantizationConfig] = None,
++        reduce_results: bool = True,
++        prefix: str = "",
++    ) -> None:
 +        super().__init__()
 +        self.gate_up_proj = MergedColumnParallelLinear(
 +            hidden_size, [intermediate_size] * 2,
-+            bias=False,
++            bias=use_bias,
 +            quant_config=quant_config,
-+            prefix=f"{prefix}.gate_up_proj"
-+            )
++            prefix=f"{prefix}.gate_up_proj")
 +        self.down_proj = RowParallelLinear(intermediate_size,
 +                                           hidden_size,
-+                                           bias=False,
++                                           bias=use_bias,
 +                                           quant_config=quant_config,
-+                                           prefix=f"{prefix}.down_proj",
-+                                           )
-+        if hidden_act not in ["silu", "gelu"]:
++                                           reduce_results=reduce_results,
++                                           prefix=f"{prefix}.down_proj")
++        if hidden_act != "silu":
 +            raise ValueError(f"Unsupported activation: {hidden_act}. "
-+                             "Only silu and gelu are supported for now.")
-+        self.act_fn = SiluAndMul() if hidden_act == "silu" else GeluAndMul()
++                             "Only silu is supported for now.")
++        self.act_fn = SiluAndMul()
 +
 +    def forward(self, x):
 +        gate_up, _ = self.gate_up_proj(x)
 +        x = self.act_fn(gate_up)
-+        ret, _ = self.down_proj(x)
-+
-+        return ret
++        x, _ = self.down_proj(x)
++        return x
 +
 +
-+class MixtralMLP(nn.Module):
-+    """
-+    This implementation is
-+    strictly equivalent to standard MoE with full capacity (no
-+    dropped tokens). It's faster since it formulates MoE operations
-+    in terms of block-sparse operations to accomodate imbalanced
-+    assignments of tokens to experts, whereas standard MoE either
-+    (1) drop tokens at the cost of reduced performance or (2) set
-+    capacity factor to number of experts and thus waste computation
-+    and memory on padding.
-+    """
++class Ernie4_5_MoeMoE(nn.Module):
 +
-+    def __init__(self,
-+                hidden_size,
-+                intermediate_size,
-+                hidden_act,
-+                moe_experts_routed,
-+                moe_experts_selected,
-+                moe_experts_fixed,
-+                quant_config: Optional[QuantizationConfig] = None,
-+                params_dtype: Optional[torch.dtype] = None,
-+                tp_size: Optional[int] = None,
-+                prefix: str = ""):
++    def __init__(
++        self,
++        config: PretrainedConfig,
++        quant_config: Optional[QuantizationConfig] = None,
++        prefix: str = "",
++    ):
 +        super().__init__()
-+        self.tp_size = tp_size or get_tensor_model_parallel_world_size()
-+        self.num_experts_routed = moe_experts_routed
-+        self.num_local_experts_routed = self.num_experts_routed // 1
-+        self.top_k = moe_experts_selected
-+        self.hidden_size = hidden_size
-+        self.intermediate_size = intermediate_size // self.tp_size
 +
++        layer_idx = extract_layer_index(prefix)
++        self.layer_idx = layer_idx
++        self.tp_size = get_tensor_model_parallel_world_size()
++        self.moe_num_shared_experts = getattr(config, "moe_num_shared_experts",
++                                              None)
 +
-+        if params_dtype is None:
-+            params_dtype = torch.get_default_dtype()
-+        self.params_dtype = params_dtype
-+        self.router = ReplicatedLinear(self.hidden_size,
-+                                        self.num_experts_routed,
-+                                        bias=False,
-+                                        quant_config=quant_config,
-+                                        params_dtype=self.params_dtype,
-+                                        )
-+
-+        self.ws = nn.Parameter(
-+            torch.empty(self.num_experts_routed,
-+                        2 * self.intermediate_size,
-+                        self.hidden_size,
-+                        device="cuda",
-+                        dtype=self.params_dtype))
-+        self.w2s = nn.Parameter(
-+            torch.empty(self.num_experts_routed,
-+                        self.hidden_size,
-+                        self.intermediate_size,
-+                        device="cuda",
-+                        dtype=self.params_dtype))
-+        
-+        set_weight_attrs(self.ws, {
-+            "weight_loader": self.weight_loader,
-+        })
-+        set_weight_attrs(self.w2s, {
-+            "weight_loader": self.weight_loader,
-+        })
++        if self.tp_size > config.moe_num_experts:
++            raise ValueError(
++                f"Tensor parallel size {self.tp_size} is greater than "
++                f"the number of experts {config.moe_num_experts}.")
++
++        self.gate = ReplicatedLinear(config.hidden_size,
++                                     config.moe_num_experts,
++                                     bias=False,
++                                     quant_config=None,
++                                     prefix=f"{prefix}.gate")
++
++        self.experts = FusedMoE(num_experts=config.moe_num_experts,
++                                top_k=config.moe_k,
++                                hidden_size=config.hidden_size,
++                                intermediate_size=config.moe_intermediate_size,
++                                reduce_results=False,
++                                renormalize=True,
++                                quant_config=quant_config,
++                                prefix=f"{prefix}.experts")
++
++        if self.moe_num_shared_experts is not None:
++            intermediate_size = (config.moe_intermediate_size *
++                                 config.moe_num_shared_experts)
++            self.shared_experts = Ernie4_5_MoeMLP(
++                hidden_size=config.hidden_size,
++                intermediate_size=intermediate_size,
++                hidden_act=config.hidden_act,
++                quant_config=quant_config,
++                prefix=f"{prefix}.shared_experts",
++                reduce_results=self.experts.must_reduce_shared_expert_outputs(
++                ))
 +
++    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
++        orig_shape = hidden_states.shape
++        hidden_dim = hidden_states.shape[-1]
++        hidden_states = hidden_states.view(-1, hidden_dim)
++        if self.moe_num_shared_experts is not None:
++            shared_output = self.shared_experts(hidden_states)
 +
-+        if moe_experts_fixed >= 1:
-+            self.local_experts_fixed = MLP(hidden_size, intermediate_size*moe_experts_fixed, hidden_act, quant_config=quant_config, prefix=f"{prefix}.mlp")
-+        else:
-+            self.local_experts_fixed = None
-+
-+    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor,
-+                      weight_name: str, expert_id: int):
-+        tp_rank = get_tensor_model_parallel_rank()
-+        param_data = param.data
-+        shard_size = self.intermediate_size
-+        shard = slice(tp_rank * shard_size, (tp_rank + 1) * shard_size)
-+        if weight_name.endswith("gate_proj.weight"):
-+            param_data[expert_id, 0:shard_size, :] = loaded_weight[shard, :]
-+        if weight_name.endswith("up_proj.weight"):
-+            param_data[expert_id, shard_size:2 * shard_size, :] = loaded_weight[shard, :]
-+        if weight_name.endswith("down_proj.weight"):
-+            param_data[expert_id, :, :] = loaded_weight[:, shard]
++        router_logits, _ = self.gate(hidden_states)
 +
++        final_hidden_states = self.experts(hidden_states=hidden_states,
++                                           router_logits=router_logits)
 +
-+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-+        """ """
-+        num_tokens, hidden_size = hidden_states.shape
-+        hidden_states = hidden_states.view(-1, self.hidden_size)
-+        router_logits, _ = self.router(hidden_states)
-+        final_hidden_states = fused_moe(hidden_states,
-+                                        self.ws,
-+                                        self.w2s,
-+                                        router_logits,
-+                                        self.top_k,
-+                                        renormalize=True)
++        if self.moe_num_shared_experts is not None and \
++              shared_output is not None:
++            final_hidden_states = final_hidden_states + shared_output
 +
 +        if self.tp_size > 1:
-+            final_hidden_states = tensor_model_parallel_all_reduce(
-+                final_hidden_states)
-+        
-+        final_hidden_states = final_hidden_states.view(num_tokens, hidden_size)
-+        
-+        if self.local_experts_fixed:
-+            final_hidden_states += self.local_experts_fixed(hidden_states).reshape(num_tokens, hidden_size)
-+            final_hidden_states /= 2
-+        
-+        ret = final_hidden_states.reshape(num_tokens, hidden_size)
-+        return ret
++            final_hidden_states = (
++                self.experts.maybe_all_reduce_tensor_model_parallel(
++                    final_hidden_states))
 +
++        return final_hidden_states.view(orig_shape)
 +
-+class MixtralAttention(nn.Module):
 +
-+    def __init__(self,
-+                 hidden_size: int,
-+                 num_heads: int,
-+                 num_kv_heads: int,
-+                 max_position: int = 4096 * 32,
-+                 rope_theta: float = 10000,
-+                 cache_config: Optional[CacheConfig] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 prefix: str = "",) -> None:
++class Ernie4_5_MoeAttention(nn.Module):
++
++    def __init__(
++        self,
++        hidden_size: int,
++        num_heads: int,
++        num_kv_heads: int,
++        head_dim: Optional[int] = None,
++        rope_theta: float = 500000,
++        rope_scaling: Optional[dict[str, Any]] = None,
++        max_position_embeddings: int = 131072,
++        rms_norm_eps: float = 1e-05,
++        qkv_bias: bool = False,
++        cache_config: Optional[CacheConfig] = None,
++        quant_config: Optional[QuantizationConfig] = None,
++        prefix: str = "",
++    ) -> None:
 +        super().__init__()
++        layer_idx = extract_layer_index(prefix) if len(prefix) > 0 else 0
++        self.layer_idx = layer_idx
 +        self.hidden_size = hidden_size
 +        tp_size = get_tensor_model_parallel_world_size()
 +        self.total_num_heads = num_heads
 +        assert self.total_num_heads % tp_size == 0
 +        self.num_heads = self.total_num_heads // tp_size
++
 +        self.total_num_kv_heads = num_kv_heads
 +        if self.total_num_kv_heads >= tp_size:
 +            # Number of KV heads is greater than TP size, so we partition
@@ -28645,110 +19991,127 @@ index 000000000..a1c7c4679
 +            # the KV heads across multiple tensor parallel GPUs.
 +            assert tp_size % self.total_num_kv_heads == 0
 +        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
-+        self.head_dim = hidden_size // self.total_num_heads
++        self.head_dim = head_dim or (hidden_size // self.total_num_heads)
++
 +        self.q_size = self.num_heads * self.head_dim
 +        self.kv_size = self.num_kv_heads * self.head_dim
 +        self.scaling = self.head_dim**-0.5
 +        self.rope_theta = rope_theta
++        self.max_position_embeddings = max_position_embeddings
++
++        self.qkv_proj = QKVParallelLinear(hidden_size,
++                                          self.head_dim,
++                                          self.total_num_heads,
++                                          self.total_num_kv_heads,
++                                          bias=qkv_bias,
++                                          quant_config=quant_config,
++                                          prefix=f"{prefix}.qkv_proj")
++
++        self.o_proj = RowParallelLinear(self.total_num_heads * self.head_dim,
++                                        hidden_size,
++                                        bias=False,
++                                        quant_config=quant_config,
++                                        prefix=f"{prefix}.o_proj")
 +
-+        self.W_pack = QKVParallelLinear(
-+            hidden_size,
-+            self.head_dim,
-+            self.total_num_heads,
-+            self.total_num_kv_heads,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.qkv_proj",
-+        )
-+        self.o_proj = RowParallelLinear(
-+            self.total_num_heads * self.head_dim,
-+            hidden_size,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.o_proj",
-+        )
 +        self.rotary_emb = get_rope(
 +            self.head_dim,
 +            rotary_dim=self.head_dim,
-+            max_position=max_position,
-+            base=int(self.rope_theta),
-+            is_neox_style=True,
-+        )
-+        self.attn = Attention(
-+            self.num_heads,
-+            self.head_dim,
-+            self.scaling,
-+            num_kv_heads=self.num_kv_heads,
-+            cache_config=cache_config,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.attn"
++            max_position=max_position_embeddings,
++            base=rope_theta,
++            is_neox_style=False,
++            rope_scaling=rope_scaling,
 +        )
++        self.attn = Attention(self.num_heads,
++                              self.head_dim,
++                              self.scaling,
++                              num_kv_heads=self.num_kv_heads,
++                              cache_config=cache_config,
++                              quant_config=quant_config,
++                              prefix=f"{prefix}.attn")
 +
 +    def forward(
 +        self,
 +        positions: torch.Tensor,
 +        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
 +    ) -> torch.Tensor:
-+        qkv, _ = self.W_pack(hidden_states)
++
++        qkv, _ = self.qkv_proj(hidden_states)
++
 +        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
 +        q, k = self.rotary_emb(positions, q, k)
-+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
++
++        # Attention
++        attn_output = self.attn(q, k, v)
++        # Output projection
 +        output, _ = self.o_proj(attn_output)
 +        return output
 +
 +
-+class DecoderLayer(nn.Module):
++class Ernie4_5_MoeDecoderLayer(nn.Module):
++
 +    def __init__(
 +        self,
-+        config: BaiChuanMoEConfig,
++        config: PretrainedConfig,
 +        cache_config: Optional[CacheConfig] = None,
 +        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = ""
++        prefix: str = "",
 +    ) -> None:
 +        super().__init__()
 +        self.hidden_size = config.hidden_size
-+        # Requires transformers > 4.32.0
-+        rope_theta = getattr(config, "rope_base", 10000)
-+        self.self_attn = MixtralAttention(
++        rope_theta = getattr(config, "rope_theta", 500000)
++        rope_scaling = getattr(config, "rope_scaling", None)
++        max_position_embeddings = getattr(config, "max_position_embeddings",
++                                          131072)
++        self.self_attn = Ernie4_5_MoeAttention(
 +            hidden_size=self.hidden_size,
 +            num_heads=config.num_attention_heads,
-+            max_position=config.max_position_embeddings,
-+            num_kv_heads=config.num_attention_heads,
++            num_kv_heads=config.num_key_value_heads,
++            head_dim=getattr(config, 'head_dim', None),
 +            rope_theta=rope_theta,
++            rope_scaling=rope_scaling,
++            max_position_embeddings=max_position_embeddings,
++            rms_norm_eps=config.rms_norm_eps,
++            qkv_bias=getattr(config, 'use_bias', False),
 +            cache_config=cache_config,
 +            quant_config=quant_config,
-+            prefix=f"{prefix}.self_attn")
-+        
-+        
-+        # Dense
-+        if config.moe_experts_routed == 1:
-+            self.mlp = MLP(hidden_size=config.hidden_size,
-+                            intermediate_size=config.intermediate_size,
-+                            hidden_act=config.hidden_act, quant_config=quant_config,
-+                            prefix=f"{prefix}.mlp")
++            prefix=f"{prefix}.self_attn",
++        )
++
++        layer_idx = extract_layer_index(prefix)
++        self.layer_idx = layer_idx
++
 +        # MoE
++        moe_num_experts = getattr(config, "moe_num_experts", 0)
++        moe_layer_start_index = getattr(config, "moe_layer_start_index", 0)
++        moe_layer_end_index = getattr(config, "moe_layer_end_index",
++                                      config.num_hidden_layers - 1)
++        moe_layer_interval = getattr(config, "moe_layer_interval", 1)
++        use_moe = getattr(config, "use_moe", moe_num_experts > 0)
++
++        if (use_moe and ((layer_idx + 1) % moe_layer_interval == 0)
++                and layer_idx >= moe_layer_start_index
++                and layer_idx <= moe_layer_end_index):
++            self.mlp = Ernie4_5_MoeMoE(config=config,
++                                       quant_config=quant_config,
++                                       prefix=f"{prefix}.mlp")
 +        else:
-+            self.mlp = MixtralMLP(config.hidden_size,
-+                                    config.intermediate_size,
-+                                    config.hidden_act,
-+                                    config.moe_experts_routed,
-+                                    config.moe_experts_selected,
-+                                    config.moe_experts_fixed,
-+                                    quant_config=quant_config,
-+                                    prefix=f"{prefix}.mlp")
++            self.mlp = Ernie4_5_MoeMLP(
++                hidden_size=config.hidden_size,
++                intermediate_size=config.intermediate_size,
++                hidden_act=config.hidden_act,
++                use_bias=getattr(config, 'use_bias', False),
++                quant_config=quant_config,
++                prefix=f"{prefix}.mlp")
++
 +        self.input_layernorm = RMSNorm(config.hidden_size,
 +                                       eps=config.rms_norm_eps)
 +        self.post_attention_layernorm = RMSNorm(config.hidden_size,
 +                                                eps=config.rms_norm_eps)
-+    
++
 +    def forward(
 +        self,
 +        positions: torch.Tensor,
 +        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
 +        residual: Optional[torch.Tensor],
 +    ) -> torch.Tensor:
 +
@@ -28759,87 +20122,79 @@ index 000000000..a1c7c4679
 +        else:
 +            hidden_states, residual = self.input_layernorm(
 +                hidden_states, residual)
++
 +        hidden_states = self.self_attn(
 +            positions=positions,
 +            hidden_states=hidden_states,
-+            kv_cache=kv_cache,
-+            attn_metadata=attn_metadata,
 +        )
 +
 +        # Fully Connected
 +        hidden_states, residual = self.post_attention_layernorm(
 +            hidden_states, residual)
-+        
++
 +        hidden_states = self.mlp(hidden_states)
 +
 +        return hidden_states, residual
 +
-+def layer_function(prefix, config, cache_config, quant_config):
-+    index = int(prefix.split(".")[-1])  
-+    config_ = copy.deepcopy(config)
 +
-+    config_.moe_experts_fixed = config.num_experts_fixed_per_layer[index]
-+    config_.moe_experts_selected = config.num_experts_selected_per_layer[index]
-+    config_.moe_experts_routed = config.num_experts_routed_per_layer[index]
++@support_torch_compile
++class Ernie4_5_MoeModel(nn.Module):
 +
-+    return DecoderLayer(config=config_, cache_config=cache_config, quant_config=quant_config, prefix=prefix)
-+
-+class Model(nn.Module):
-+    def __init__(self, vllm_config: VllmConfig, prefix: str = "") -> None:
++    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
 +        super().__init__()
++
 +        config = vllm_config.model_config.hf_config
 +        cache_config = vllm_config.cache_config
 +        quant_config = vllm_config.quant_config
-+        lora_config = vllm_config.lora_config
 +
 +        self.padding_idx = config.pad_token_id
-+        lora_vocab = (lora_config.lora_extra_vocab_size *
-+                      (lora_config.max_loras or 1)) if lora_config else 0
-+        self.vocab_size = config.vocab_size + lora_vocab
-+        self.org_vocab_size = config.vocab_size
++        self.vocab_size = config.vocab_size
++        self.config = config
 +
-+        if get_pp_group().is_first_rank:
++        if get_pp_group().is_first_rank or (config.tie_word_embeddings
++                                            and get_pp_group().is_last_rank):
 +            self.embed_tokens = VocabParallelEmbedding(
-+                self.vocab_size,
++                config.vocab_size,
 +                config.hidden_size,
-+                org_num_embeddings=config.vocab_size,
-+            )
-+        else:
-+            self.embed_tokens = PPMissingLayer()
-+
-+        if config.num_experts_routed_per_layer:
-+            self.start_layer, self.end_layer, self.layers = make_layers(
-+                num_hidden_layers=config.num_hidden_layers,
-+                layer_fn=lambda prefix: layer_function(prefix, config, cache_config, quant_config),
-+                prefix=f"{prefix}.layers",  
-+            )
++                quant_config=quant_config,
++                prefix=f"{prefix}.embed_tokens")
 +        else:
-+            self.start_layer, self.end_layer, self.layers = make_layers(
-+                num_hidden_layers=config.num_hidden_layers,
-+                layer_fn = lambda prefix: DecoderLayer(
-+                    config=config,
-+                    cache_config=cache_config,
-+                    quant_config=quant_config,
-+                    prefix=prefix
-+                ),
-+                prefix=f"{prefix}.layers",  
-+            )
++            self.embed_tokens = PPMissingLayer()
++
++        self.start_layer, self.end_layer, self.layers = make_layers(
++            config.num_hidden_layers,
++            lambda prefix: Ernie4_5_MoeDecoderLayer(config=config,
++                                                    cache_config=cache_config,
++                                                    quant_config=quant_config,
++                                                    prefix=prefix),
++            prefix=f"{prefix}.layers",
++        )
 +
 +        if get_pp_group().is_last_rank:
 +            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
 +        else:
 +            self.norm = PPMissingLayer()
 +
++        self.make_empty_intermediate_tensors = (
++            make_empty_intermediate_tensors_factory(
++                ["hidden_states", "residual"], config.hidden_size))
++
++    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
++        return self.embed_tokens(input_ids)
++
 +    def forward(
 +        self,
 +        input_ids: torch.Tensor,
 +        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors],
-+    ) -> torch.Tensor:
++        intermediate_tensors: Optional[IntermediateTensors] = None,
++        inputs_embeds: Optional[torch.Tensor] = None,
++    ) -> Union[torch.Tensor, IntermediateTensors]:
++
 +        if get_pp_group().is_first_rank:
-+            hidden_states = self.embed_tokens(input_ids)
++            if inputs_embeds is not None:
++                hidden_states = inputs_embeds
++            else:
++                hidden_states = self.get_input_embeddings(input_ids)
 +            residual = None
 +        else:
 +            assert intermediate_tensors is not None
@@ -28848,2682 +20203,426 @@ index 000000000..a1c7c4679
 +
 +        for i in range(self.start_layer, self.end_layer):
 +            layer = self.layers[i]
-+            hidden_states, residual = layer(
-+                positions,
-+                hidden_states,
-+                kv_caches[i - self.start_layer],
-+                attn_metadata,
-+                residual,
-+            )
-+        
++            hidden_states, residual = layer(positions, hidden_states, residual)
++
 +        if not get_pp_group().is_last_rank:
 +            return IntermediateTensors({
 +                "hidden_states": hidden_states,
 +                "residual": residual
 +            })
-+        
++
 +        hidden_states, _ = self.norm(hidden_states, residual)
-+        
++
 +        return hidden_states
 +
-+class NormHead(nn.Module):
-+    def __init__(self, hidden_size, vocab_size, bias=False):
-+        super().__init__()
-+        self.weight = nn.Parameter(torch.empty((vocab_size, hidden_size)))
-+        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
-+        self.norm_weight = nn.functional.normalize(self.weight)
-+
-+    def forward(self, hidden_states):
-+        return nn.functional.linear(hidden_states, self.norm_weight)
-+
-+class BaiChuanMoEForCausalLM(nn.Module, SupportsPP):
-+    # packed_modules_mapping = {
-+    #     "qkv_proj": [
-+    #         "q_proj",
-+    #         "k_proj",
-+    #         "v_proj",
-+    #     ],
-+    # }
-+
-+    # # LoRA specific attributes
-+    # supported_lora_modules = [
-+    #     "qkv_proj",
-+    #     "o_proj",
-+    #     "embed_tokens",
-+    #     "lm_head",
-+    # ]
-+    embedding_modules = {
-+        "embed_tokens": "input_embeddings",
-+        "lm_head": "output_embeddings",
-+    }
-+    embedding_padding_modules = ["lm_head"]
-+
-+    def __init__(self, vllm_config: VllmConfig, prefix: str = "") -> None:
++
++class Ernie4_5_MoeForCausalLM(nn.Module, SupportsPP):
++    packed_modules_mapping = {
++        "qkv_proj": [
++            "q_proj",
++            "k_proj",
++            "v_proj",
++        ],
++        "gate_up_proj": [
++            "gate_proj",
++            "up_proj",
++        ],
++    }
++
++    fall_back_to_pt_during_load = False
++
++    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
 +        super().__init__()
 +        config = vllm_config.model_config.hf_config
-+        cache_config = vllm_config.cache_config
 +        quant_config = vllm_config.quant_config
-+        lora_config = vllm_config.lora_config
-+
 +        self.config = config
 +        self.quant_config = quant_config
-+        self.model = Model(vllm_config=vllm_config, prefix=maybe_prefix(prefix, "model"))
-+        if get_pp_group().is_last_rank:
-+            self.unpadded_vocab_size = config.vocab_size
-+            if lora_config:
-+                self.unpadded_vocab_size += lora_config.lora_extra_vocab_size
-+            self.lm_head = ParallelLMHead(
-+                self.unpadded_vocab_size,
-+                config.hidden_size,
-+                quant_config=quant_config,
-+                org_num_embeddings=config.vocab_size,
-+                padding_size=DEFAULT_VOCAB_PADDING_SIZE
-+                # We need bigger padding if using lora for kernel
-+                # compatibility
-+                if not lora_config else lora_config.lora_vocab_padding_size,
-+                prefix=maybe_prefix(prefix, "lm_head")
-+            )
-+            self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,
-+                                                config.vocab_size)
-+            self.sampler = Sampler()
-+        else:
-+            self.lm_head = PPMissingLayer()
-+    
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None
-+    ) -> torch.Tensor:
-+        hidden_states = self.model(input_ids, positions, kv_caches,
-+                                   attn_metadata, intermediate_tensors)
-+        return hidden_states
-+
-+    def compute_logits(self, hidden_states: torch.Tensor,
-+                       sampling_metadata: SamplingMetadata) -> torch.Tensor:
-+        logits = self.logits_processor(self.lm_head, hidden_states,
-+                                       sampling_metadata)
-+        return logits
-+    
-+    def make_empty_intermediate_tensors(
-+            self, batch_size: int, dtype: torch.dtype,
-+            device: torch.device) -> IntermediateTensors:
-+        return IntermediateTensors({
-+            "hidden_states":
-+            torch.zeros((batch_size, self.config.hidden_size),
-+                        dtype=dtype,
-+                        device=device),
-+            "residual":
-+            torch.zeros((batch_size, self.config.hidden_size),
-+                        dtype=dtype,
-+                        device=device),
-+        })
-+
-+    def sample(
-+        self,
-+        logits: Optional[torch.Tensor],
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[SamplerOutput]:
-+        next_tokens = self.sampler(logits, sampling_metadata)
-+        return next_tokens
-+
-+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
-+        stacked_params_mapping = [
-+            # (param_name, shard_name, shard_id)
-+            ("qkv_proj", "q_proj", "q"),
-+            ("qkv_proj", "k_proj", "k"),
-+            ("qkv_proj", "v_proj", "v"),
-+            ("mlp.gate_up_proj", "mlp.gate_proj", 0),
-+            ("mlp.gate_up_proj", "mlp.up_proj", 1),
-+            ("mlp.local_experts_fixed.gate_up_proj", "mlp.local_experts_fixed.gate_proj", 0),
-+            ("mlp.local_experts_fixed.gate_up_proj", "mlp.local_experts_fixed.up_proj", 1),
-+        ]
-+
-+        expert_params_mapping = [
-+            # (param_name, weight_name, expert_id)
-+            ("ws" if weight_name in ["gate_proj", "up_proj"] else "w2s",
-+             f"local_experts_routed.{expert_id}.{weight_name}.weight", expert_id)
-+            for expert_id in range(16)
-+            for weight_name in ["gate_proj", "down_proj", "up_proj"]
-+        ]
-+
-+        params_dict = dict(self.named_parameters())
-+        
-+        for name, loaded_weight in weights:
-+            if "rotary_emb.inv_freq" in name:
-+                continue
-+
-+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
-+                if weight_name not in name:
-+                    continue
-+                name = name.replace(weight_name, param_name)
-+                # Skip loading extra bias for GPTQ models.
-+                if name.endswith(".bias") and name not in params_dict:
-+                    continue
-+
-+                if is_pp_missing_parameter(name, self):
-+                    continue
-+
-+                param = params_dict[name]
-+                weight_loader = param.weight_loader
-+                weight_loader(param, loaded_weight, shard_id)
-+                break
-+            else:
-+                for param_name, weight_name, expert_id in expert_params_mapping:
-+                    if weight_name not in name:
-+                        continue
-+                    name = name.replace(weight_name, param_name)
-+
-+                    if is_pp_missing_parameter(name, self):
-+                        continue
-+
-+                    param = params_dict[name]
-+                    weight_loader = param.weight_loader
-+                    weight_loader(param,
-+                                  loaded_weight,
-+                                  weight_name,
-+                                  expert_id=expert_id)
-+                    break
-+                else:
-+                    # Skip loading extra bias for GPTQ models.
-+                    if name.endswith(".bias") and name not in params_dict:
-+                        continue
-+
-+                    if is_pp_missing_parameter(name, self):
-+                        continue
-+
-+                    param = params_dict.get(name, None)
-+
-+                    if name == "lm_head.weight":
-+                        # do norm
-+                        norm_weight = nn.functional.normalize(loaded_weight)
-+                        weight_loader = getattr(param, "weight_loader",
-+                                                default_weight_loader)
-+                        weight_loader(param, norm_weight)
-+                    else:
-+                        weight_loader = getattr(param, "weight_loader",
-+                                                default_weight_loader)
-+                        weight_loader(param, loaded_weight)
++        self.model = Ernie4_5_MoeModel(vllm_config=vllm_config,
++                                       prefix=maybe_prefix(prefix, "model"))
 +
-diff --git a/vllm/model_executor/models/deepseek.py b/vllm/model_executor/models/deepseek.py
-index 9599e1df6..9f3403bcc 100644
---- a/vllm/model_executor/models/deepseek.py
-+++ b/vllm/model_executor/models/deepseek.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Adapted from
-@@ -144,14 +145,20 @@ class DeepseekMoE(nn.Module):
-         w1s = torch._utils._unflatten_dense_tensors(self.w1, w1)
-         for data, param in zip(w1s, w1):
-             param.data = data
--        self.w1 = self.w1.view(len(w1), *w1s[0].shape)
-+        #self.w1 = self.w1.view(len(w1), *w1s[0].shape)
-+        self.w1 = self.w1.view(len(w1), *w1s[0].shape).permute(0, 2, 1).contiguous()
-+        for expert, w in zip(self.experts, self.w1):
-+            expert.gate_up_proj.weight.data = w.permute(1, 0)
- 
-         self.w2 = torch._utils._flatten_dense_tensors(w2)
-         w2s = torch._utils._unflatten_dense_tensors(self.w2, w2)
-         for data, param in zip(w2s, w2):
-             param.data = data
- 
--        self.w2 = self.w2.view(len(w2), *w2s[0].shape)
-+        #self.w2 = self.w2.view(len(w2), *w2s[0].shape)
-+        self.w2 = self.w2.view(len(w2), *w2s[0].shape).permute(0, 2, 1).contiguous()
-+        for expert, w in zip(self.experts, self.w2):
-+            expert.down_proj.weight.data = w.permute(1, 0)
- 
-     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-         num_tokens, hidden_dim = hidden_states.shape
-diff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py
-index 773f5abe7..45c1de169 100644
---- a/vllm/model_executor/models/deepseek_v2.py
-+++ b/vllm/model_executor/models/deepseek_v2.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Adapted from
-@@ -664,9 +665,12 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP):
-         self.quant_config = quant_config
-         self.model = DeepseekV2Model(vllm_config=vllm_config,
-                                      prefix=maybe_prefix(prefix, "model"))
--        self.lm_head = ParallelLMHead(config.vocab_size,
--                                      config.hidden_size,
--                                      quant_config=quant_config)
 +        if get_pp_group().is_last_rank:
-+            self.lm_head = ParallelLMHead(config.vocab_size,
++            if config.tie_word_embeddings:
++                self.lm_head = self.model.embed_tokens
++            else:
++                self.lm_head = ParallelLMHead(config.vocab_size,
 +                                          config.hidden_size,
-+                                          quant_config=quant_config)
-+        else:
-+            self.lm_head = PPMissingLayer()
-         self.logits_processor = LogitsProcessor(config.vocab_size)
-         self.sampler = get_sampler()
-         self.make_empty_intermediate_tensors = (
-diff --git a/vllm/model_executor/models/qwen.py b/vllm/model_executor/models/qwen.py
-index 897066124..075729e43 100644
---- a/vllm/model_executor/models/qwen.py
-+++ b/vllm/model_executor/models/qwen.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- # Adapted from
-@@ -724,8 +725,12 @@ class QWenVLProcessor:
- 
-         special_tokens: dict[str,
-                              int] = tokenizer.special_tokens  # type: ignore
--        self.img_start_id = special_tokens[IMG_START]
--        self.img_end_id = special_tokens[IMG_END]
-+        if IMG_START in special_tokens:
-+            self.img_start_id = special_tokens[IMG_START]
-+            self.img_end_id = special_tokens[IMG_END]
-+        else:
-+            self.img_start_id = None
-+            self.img_end_id = None
- 
-     def __call__(
-         self,
-@@ -873,9 +878,14 @@ class QWenVLMultiModalProcessor(BaseMultiModalProcessor[QWenVLProcessingInfo]):
-         special_tokens: dict[str,
-                              int] = tokenizer.special_tokens  # type: ignore
- 
--        img_start_id = special_tokens[IMG_START]
--        img_end_id = special_tokens[IMG_END]
--        img_pad_id = special_tokens[IMG_PAD]
-+        if IMG_START in special_tokens:
-+            img_start_id = special_tokens[IMG_START]
-+            img_end_id = special_tokens[IMG_END]
-+            img_pad_id = special_tokens[IMG_PAD]
-+        else:
-+            img_start_id = None
-+            img_end_id = None
-+            img_pad_id = None
- 
-         num_image_tokens = self.info.get_num_image_tokens()
-         image_tokens = [img_pad_id] * num_image_tokens
-diff --git a/vllm/model_executor/models/qwen2.py b/vllm/model_executor/models/qwen2.py
-index e3de6b64f..9c1821941 100644
---- a/vllm/model_executor/models/qwen2.py
-+++ b/vllm/model_executor/models/qwen2.py
-@@ -1,594 +1,600 @@
--# SPDX-License-Identifier: Apache-2.0
--
--# Adapted from
--# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/qwen2/modeling_qwen2.py
--# Copyright 2024 The Qwen team.
--# Copyright 2023 The vLLM team.
--# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
--#
--# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
--# and OPT implementations in this library. It has been modified from its
--# original forms to accommodate minor architectural differences compared
--# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
--#
--# Licensed under the Apache License, Version 2.0 (the "License");
--# you may not use this file except in compliance with the License.
--# You may obtain a copy of the License at
--#
--#     http://www.apache.org/licenses/LICENSE-2.0
--#
--# Unless required by applicable law or agreed to in writing, software
--# distributed under the License is distributed on an "AS IS" BASIS,
--# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
--# See the License for the specific language governing permissions and
--# limitations under the License.
--"""Inference-only Qwen2 model compatible with HuggingFace weights."""
--from typing import Iterable, List, Optional, Set, Tuple, Union
--
--import torch
--from torch import nn
--from transformers import Qwen2Config
--
--from vllm.attention import Attention, AttentionMetadata, AttentionType
--from vllm.compilation.decorators import support_torch_compile
--from vllm.config import CacheConfig, VllmConfig
--from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
--from vllm.logger import init_logger
--from vllm.model_executor.layers.activation import SiluAndMul
--from vllm.model_executor.layers.layernorm import RMSNorm
--from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
--                                               QKVParallelLinear,
--                                               RowParallelLinear)
--from vllm.model_executor.layers.logits_processor import LogitsProcessor
--from vllm.model_executor.layers.pooler import Pooler, PoolingType
--from vllm.model_executor.layers.quantization import QuantizationConfig
--from vllm.model_executor.layers.rotary_embedding import get_rope
--from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
--from vllm.model_executor.layers.vocab_parallel_embedding import (
--    ParallelLMHead, VocabParallelEmbedding)
--from vllm.model_executor.model_loader.weight_utils import (
--    default_weight_loader, maybe_remap_kv_scale_name)
--from vllm.model_executor.pooling_metadata import PoolingMetadata
--from vllm.model_executor.sampling_metadata import SamplingMetadata
--from vllm.sequence import IntermediateTensors, PoolerOutput
--
--from .interfaces import SupportsLoRA, SupportsPP
--from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
--                    is_pp_missing_parameter,
--                    make_empty_intermediate_tensors_factory, make_layers,
--                    maybe_prefix)
--
--logger = init_logger(__name__)
--
--
--class Qwen2MLP(nn.Module):
--
--    def __init__(
--        self,
--        hidden_size: int,
--        intermediate_size: int,
--        hidden_act: str,
--        quant_config: Optional[QuantizationConfig] = None,
--        prefix: str = "",
--    ) -> None:
--        super().__init__()
--        self.gate_up_proj = MergedColumnParallelLinear(
--            hidden_size,
--            [intermediate_size] * 2,
--            bias=False,
--            quant_config=quant_config,
--            prefix=f"{prefix}.gate_up_proj",
--        )
--        self.down_proj = RowParallelLinear(
--            intermediate_size,
--            hidden_size,
--            bias=False,
--            quant_config=quant_config,
--            prefix=f"{prefix}.down_proj",
--        )
--        if hidden_act != "silu":
--            raise ValueError(f"Unsupported activation: {hidden_act}. "
--                             "Only silu is supported for now.")
--        self.act_fn = SiluAndMul()
--
--    def forward(self, x):
--        gate_up, _ = self.gate_up_proj(x)
--        x = self.act_fn(gate_up)
--        x, _ = self.down_proj(x)
--        return x
--
--
--class Qwen2Attention(nn.Module):
--
--    def __init__(self,
--                 hidden_size: int,
--                 num_heads: int,
--                 num_kv_heads: int,
--                 max_position: int = 4096 * 32,
--                 rope_theta: float = 10000,
--                 cache_config: Optional[CacheConfig] = None,
--                 quant_config: Optional[QuantizationConfig] = None,
--                 rope_scaling: Optional[Tuple] = None,
--                 prefix: str = "",
--                 attn_type: str = AttentionType.DECODER) -> None:
--        super().__init__()
--        self.hidden_size = hidden_size
--        tp_size = get_tensor_model_parallel_world_size()
--        self.total_num_heads = num_heads
--        assert self.total_num_heads % tp_size == 0
--        self.num_heads = self.total_num_heads // tp_size
--        self.total_num_kv_heads = num_kv_heads
--        if self.total_num_kv_heads >= tp_size:
--            # Number of KV heads is greater than TP size, so we partition
--            # the KV heads across multiple tensor parallel GPUs.
--            assert self.total_num_kv_heads % tp_size == 0
--        else:
--            # Number of KV heads is less than TP size, so we replicate
--            # the KV heads across multiple tensor parallel GPUs.
--            assert tp_size % self.total_num_kv_heads == 0
--        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
--        self.head_dim = hidden_size // self.total_num_heads
--        self.q_size = self.num_heads * self.head_dim
--        self.kv_size = self.num_kv_heads * self.head_dim
--        self.scaling = self.head_dim**-0.5
--        self.rope_theta = rope_theta
--
--        self.qkv_proj = QKVParallelLinear(
--            hidden_size,
--            self.head_dim,
--            self.total_num_heads,
--            self.total_num_kv_heads,
--            bias=True,
--            quant_config=quant_config,
--            prefix=f"{prefix}.qkv_proj",
--        )
--        self.o_proj = RowParallelLinear(
--            self.total_num_heads * self.head_dim,
--            hidden_size,
--            bias=False,
--            quant_config=quant_config,
--            prefix=f"{prefix}.o_proj",
--        )
--
--        self.rotary_emb = get_rope(
--            self.head_dim,
--            rotary_dim=self.head_dim,
--            max_position=max_position,
--            base=self.rope_theta,
--            rope_scaling=rope_scaling,
--        )
--        self.attn = Attention(self.num_heads,
--                              self.head_dim,
--                              self.scaling,
--                              num_kv_heads=self.num_kv_heads,
--                              cache_config=cache_config,
--                              quant_config=quant_config,
--                              prefix=f"{prefix}.attn",
--                              attn_type=attn_type)
--
--    def forward(
--        self,
--        positions: torch.Tensor,
--        hidden_states: torch.Tensor,
--        kv_cache: torch.Tensor,
--        attn_metadata: AttentionMetadata,
--    ) -> torch.Tensor:
--        qkv, _ = self.qkv_proj(hidden_states)
--        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
--        q, k = self.rotary_emb(positions, q, k)
--        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
--        output, _ = self.o_proj(attn_output)
--        return output
--
--
--class Qwen2DecoderLayer(nn.Module):
--
--    def __init__(
--        self,
--        config: Qwen2Config,
--        cache_config: Optional[CacheConfig] = None,
--        quant_config: Optional[QuantizationConfig] = None,
--        prefix: str = "",
--    ) -> None:
--        super().__init__()
--        self.hidden_size = config.hidden_size
--        # Requires transformers > 4.32.0
--        rope_theta = getattr(config, "rope_theta", 1000000)
--        rope_scaling = getattr(config, "rope_scaling", None)
--
--        # By default, Qwen2 uses causal attention as it is a decoder-only model.
--        # You can override the HF config with `is_causal=False` to enable
--        # bidirectional attention, which is used in some embedding models
--        # (e.g. Alibaba-NLP/gte-Qwen2-7B-instruct)
--        if getattr(config, "is_causal", True):
--            attn_type = AttentionType.DECODER
--        else:
--            attn_type = AttentionType.ENCODER_ONLY
--
--        self.self_attn = Qwen2Attention(
--            hidden_size=self.hidden_size,
--            num_heads=config.num_attention_heads,
--            max_position=config.max_position_embeddings,
--            num_kv_heads=config.num_key_value_heads,
--            rope_theta=rope_theta,
--            cache_config=cache_config,
--            quant_config=quant_config,
--            rope_scaling=rope_scaling,
--            prefix=f"{prefix}.self_attn",
--            attn_type=attn_type,
--        )
--        self.mlp = Qwen2MLP(
--            hidden_size=self.hidden_size,
--            intermediate_size=config.intermediate_size,
--            hidden_act=config.hidden_act,
--            quant_config=quant_config,
--            prefix=f"{prefix}.mlp",
--        )
--        self.input_layernorm = RMSNorm(config.hidden_size,
--                                       eps=config.rms_norm_eps)
--        self.post_attention_layernorm = RMSNorm(config.hidden_size,
--                                                eps=config.rms_norm_eps)
--
--    def forward(
--        self,
--        positions: torch.Tensor,
--        hidden_states: torch.Tensor,
--        kv_cache: torch.Tensor,
--        attn_metadata: AttentionMetadata,
--        residual: Optional[torch.Tensor],
--    ) -> Tuple[torch.Tensor, torch.Tensor]:
--        # Self Attention
--        if residual is None:
--            residual = hidden_states
--            hidden_states = self.input_layernorm(hidden_states)
--        else:
--            hidden_states, residual = self.input_layernorm(
--                hidden_states, residual)
--        hidden_states = self.self_attn(
--            positions=positions,
--            hidden_states=hidden_states,
--            kv_cache=kv_cache,
--            attn_metadata=attn_metadata,
--        )
--
--        # Fully Connected
--        hidden_states, residual = self.post_attention_layernorm(
--            hidden_states, residual)
--        hidden_states = self.mlp(hidden_states)
--        return hidden_states, residual
--
--
--@support_torch_compile(
--    dynamic_arg_dims={
--        "input_ids": 0,
--        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
--        # otherwise (seq_len, ).
--        "positions": -1,
--        "intermediate_tensors": 0,
--        "inputs_embeds": 0,
--    })
--class Qwen2Model(nn.Module):
--
--    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
--        super().__init__()
--
--        config = vllm_config.model_config.hf_config
--        cache_config = vllm_config.cache_config
--        quant_config = vllm_config.quant_config
--
--        # TODO (@robertgshaw2): see if this can be moved out
--        if (cache_config.sliding_window is not None
--                and hasattr(config, "max_window_layers")):
--            raise ValueError("Sliding window for some but all layers is not "
--                             "supported. This model uses sliding window "
--                             "but `max_window_layers` = {} is less than "
--                             "`num_hidden_layers` = {}. Please open an issue "
--                             "to discuss this feature.".format(
--                                 config.max_window_layers,
--                                 config.num_hidden_layers,
--                             ))
--
--        self.config = config
--        self.quant_config = quant_config
--        self.padding_idx = config.pad_token_id
--        self.vocab_size = config.vocab_size
--
--        if get_pp_group().is_first_rank or (config.tie_word_embeddings
--                                            and get_pp_group().is_last_rank):
--            self.embed_tokens = VocabParallelEmbedding(
--                config.vocab_size,
--                config.hidden_size,
--                quant_config=quant_config,
--                prefix=f"{prefix}.embed_tokens",
--            )
--        else:
--            self.embed_tokens = PPMissingLayer()
--
--        self.start_layer, self.end_layer, self.layers = make_layers(
--            config.num_hidden_layers,
--            lambda prefix: Qwen2DecoderLayer(config=config,
--                                             cache_config=cache_config,
--                                             quant_config=quant_config,
--                                             prefix=prefix),
--            prefix=f"{prefix}.layers",
--        )
--
--        self.make_empty_intermediate_tensors = (
--            make_empty_intermediate_tensors_factory(
--                ["hidden_states", "residual"], config.hidden_size))
--        if get_pp_group().is_last_rank:
--            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
--        else:
--            self.norm = PPMissingLayer()
--
--    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
--        return self.embed_tokens(input_ids)
--
--    def forward(
--        self,
--        input_ids: torch.Tensor,
--        positions: torch.Tensor,
--        kv_caches: List[torch.Tensor],
--        attn_metadata: AttentionMetadata,
--        intermediate_tensors: Optional[IntermediateTensors] = None,
--        inputs_embeds: Optional[torch.Tensor] = None,
--    ) -> Union[torch.Tensor, IntermediateTensors]:
--        if get_pp_group().is_first_rank:
--            if inputs_embeds is not None:
--                hidden_states = inputs_embeds
--            else:
--                hidden_states = self.get_input_embeddings(input_ids)
--            residual = None
--        else:
--            assert intermediate_tensors is not None
--            hidden_states = intermediate_tensors["hidden_states"]
--            residual = intermediate_tensors["residual"]
--        for i in range(self.start_layer, self.end_layer):
--            layer = self.layers[i]
--            hidden_states, residual = layer(
--                positions,
--                hidden_states,
--                kv_caches[i - self.start_layer],
--                attn_metadata,
--                residual,
--            )
--        if not get_pp_group().is_last_rank:
--            return IntermediateTensors({
--                "hidden_states": hidden_states,
--                "residual": residual
--            })
--        hidden_states, _ = self.norm(hidden_states, residual)
--        return hidden_states
--
--    def load_weights(self, weights: Iterable[Tuple[str,
--                                                   torch.Tensor]]) -> Set[str]:
--        stacked_params_mapping = [
--            # (param_name, shard_name, shard_id)
--            ("qkv_proj", "q_proj", "q"),
--            ("qkv_proj", "k_proj", "k"),
--            ("qkv_proj", "v_proj", "v"),
--            ("gate_up_proj", "gate_proj", 0),
--            ("gate_up_proj", "up_proj", 1),
--        ]
--        params_dict = dict(self.named_parameters(remove_duplicate=False))
--        loaded_params: Set[str] = set()
--        for name, loaded_weight in weights:
--            if "rotary_emb.inv_freq" in name:
--                continue
--            if (self.quant_config is not None and
--                (scale_name := self.quant_config.get_cache_scale(name))):
--                # Loading kv cache quantization scales
--                param = params_dict[scale_name]
--                weight_loader = getattr(param, "weight_loader",
--                                        default_weight_loader)
--                loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
--                                 loaded_weight[0])
--                weight_loader(param, loaded_weight)
--                loaded_params.add(scale_name)
--                continue
--            for (param_name, weight_name, shard_id) in stacked_params_mapping:
--                if weight_name not in name:
--                    continue
--                name = name.replace(weight_name, param_name)
--                # Skip loading extra bias for GPTQ models.
--                if name.endswith(".bias") and name not in params_dict:
--                    continue
--                if is_pp_missing_parameter(name, self):
--                    continue
--                param = params_dict[name]
--                weight_loader = param.weight_loader
--                weight_loader(param, loaded_weight, shard_id)
--                break
--            else:
--                # Skip loading extra bias for GPTQ models.
--                if name.endswith(".bias") and name not in params_dict:
--                    continue
--                # Remapping the name of FP8 kv-scale.
--                name = maybe_remap_kv_scale_name(name, params_dict)
--                if name is None:
--                    continue
--                if is_pp_missing_parameter(name, self):
--                    continue
--                param = params_dict[name]
--                weight_loader = getattr(param, "weight_loader",
--                                        default_weight_loader)
--                weight_loader(param, loaded_weight)
--            loaded_params.add(name)
--        return loaded_params
--
--
--class Qwen2ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
--    packed_modules_mapping = {
--        "qkv_proj": [
--            "q_proj",
--            "k_proj",
--            "v_proj",
--        ],
--        "gate_up_proj": [
--            "gate_proj",
--            "up_proj",
--        ],
--    }
--
--    # LoRA specific attributes
--    supported_lora_modules = [
--        "qkv_proj",
--        "o_proj",
--        "gate_up_proj",
--        "down_proj",
--    ]
--    embedding_modules = {}
--    embedding_padding_modules = []
--
--    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
--        super().__init__()
--        config = vllm_config.model_config.hf_config
--        quant_config = vllm_config.quant_config
--        lora_config = vllm_config.lora_config
--
--        self.config = config
--        self.lora_config = lora_config
--
--        self.quant_config = quant_config
--        self.model = Qwen2Model(vllm_config=vllm_config,
--                                prefix=maybe_prefix(prefix, "model"))
--
--        if get_pp_group().is_last_rank:
--            if config.tie_word_embeddings:
--                self.lm_head = self.model.embed_tokens
--            else:
--                self.lm_head = ParallelLMHead(config.vocab_size,
--                                              config.hidden_size,
--                                              quant_config=quant_config,
--                                              prefix=maybe_prefix(
--                                                  prefix, "lm_head"))
--        else:
--            self.lm_head = PPMissingLayer()
--
--        self.logits_processor = LogitsProcessor(config.vocab_size)
--        self.sampler = get_sampler()
--
--        self.make_empty_intermediate_tensors = (
--            self.model.make_empty_intermediate_tensors)
--
--    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
--        return self.model.get_input_embeddings(input_ids)
--
--    def forward(
--        self,
--        input_ids: torch.Tensor,
--        positions: torch.Tensor,
--        kv_caches: List[torch.Tensor],
--        attn_metadata: AttentionMetadata,
--        intermediate_tensors: Optional[IntermediateTensors] = None,
--        inputs_embeds: Optional[torch.Tensor] = None,
--    ) -> Union[torch.Tensor, IntermediateTensors]:
--        hidden_states = self.model(input_ids, positions, kv_caches,
--                                   attn_metadata, intermediate_tensors,
--                                   inputs_embeds)
--        return hidden_states
--
--    def compute_logits(
--        self,
--        hidden_states: torch.Tensor,
--        sampling_metadata: SamplingMetadata,
--    ) -> Optional[torch.Tensor]:
--        logits = self.logits_processor(self.lm_head, hidden_states,
--                                       sampling_metadata)
--        return logits
--
--    def sample(
--        self,
--        logits: torch.Tensor,
--        sampling_metadata: SamplingMetadata,
--    ) -> Optional[SamplerOutput]:
--        next_tokens = self.sampler(logits, sampling_metadata)
--        return next_tokens
--
--    def load_weights(self, weights: Iterable[Tuple[str,
--                                                   torch.Tensor]]) -> Set[str]:
--        loader = AutoWeightsLoader(
--            self,
--            skip_prefixes=(["lm_head."]
--                           if self.config.tie_word_embeddings else None),
--        )
--        return loader.load_weights(weights)
--
--
--class Qwen2EmbeddingModel(nn.Module, SupportsLoRA, SupportsPP):
--    packed_modules_mapping = {
--        "qkv_proj": [
--            "q_proj",
--            "k_proj",
--            "v_proj",
--        ],
--        "gate_up_proj": [
--            "gate_proj",
--            "up_proj",
--        ],
--    }
--
--    # LoRA specific attributes
--    supported_lora_modules = [
--        "qkv_proj",
--        "o_proj",
--        "gate_up_proj",
--        "down_proj",
--    ]
--    embedding_modules = {}
--    embedding_padding_modules = []
--
--    hf_to_vllm_mapper = WeightsMapper(orig_to_new_prefix={"model.": ""})
--
--    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
--        super().__init__()
--        config = vllm_config.model_config.hf_config
--        quant_config = vllm_config.quant_config
--        lora_config = vllm_config.lora_config
--        pooler_config = vllm_config.model_config.pooler_config
--
--        self.config = config
--        self.lora_config = lora_config
--
--        self.quant_config = quant_config
--        self.model = Qwen2Model(vllm_config=vllm_config,
--                                prefix=maybe_prefix(prefix, "model"))
--
--        # TODO: Replace this model class with as_embedding_model(
--        # Qwen2ForCausalLM) after changing the default pooling method
--        if pooler_config.pooling_type is None:
--            logger.warning(
--                "This embedding model will default to last-token pooling in "
--                "an upcoming version. To avoid breaking changes, you should "
--                "pass `--override-pooler-config '{\"pooling_type\": \"MEAN\"}'`"
--                " explicitly.")
--
--        self._pooler = Pooler.from_config_with_defaults(
--            pooler_config,
--            pooling_type=PoolingType.MEAN,
--            normalize=True,
--            softmax=False)
--
--    def forward(
--        self,
--        input_ids: torch.Tensor,
--        positions: torch.Tensor,
--        kv_caches: List[torch.Tensor],
--        attn_metadata: AttentionMetadata,
--        intermediate_tensors: Optional[IntermediateTensors] = None,
--    ) -> torch.Tensor:
--        return self.model(input_ids, positions, kv_caches, attn_metadata,
--                          intermediate_tensors)
--
--    def pooler(
--        self,
--        hidden_states: torch.Tensor,
--        pooling_metadata: PoolingMetadata,
--    ) -> Optional[PoolerOutput]:
--        return self._pooler(hidden_states, pooling_metadata)
--
--    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
--        weights = self.hf_to_vllm_mapper.apply(weights)
--        weights = ((name, data) for name, data in weights
--                   if not name.startswith("lm_head."))
--        self.model.load_weights(weights)
-+# SPDX-License-Identifier: Apache-2.0
-+
-+# Adapted from
-+# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/qwen2/modeling_qwen2.py
-+# Copyright 2024 The Qwen team.
-+# Copyright 2023 The vLLM team.
-+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
-+#
-+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
-+# and OPT implementations in this library. It has been modified from its
-+# original forms to accommodate minor architectural differences compared
-+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
-+#
-+# Licensed under the Apache License, Version 2.0 (the "License");
-+# you may not use this file except in compliance with the License.
-+# You may obtain a copy of the License at
-+#
-+#     http://www.apache.org/licenses/LICENSE-2.0
-+#
-+# Unless required by applicable law or agreed to in writing, software
-+# distributed under the License is distributed on an "AS IS" BASIS,
-+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-+# See the License for the specific language governing permissions and
-+# limitations under the License.
-+"""Inference-only Qwen2 model compatible with HuggingFace weights."""
-+from typing import Iterable, List, Optional, Set, Tuple, Union
-+
-+import torch
-+from torch import nn
-+from transformers import Qwen2Config
-+
-+from vllm.attention import Attention, AttentionMetadata, AttentionType
-+from vllm.compilation.decorators import support_torch_compile
-+from vllm.config import CacheConfig, VllmConfig
-+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
-+from vllm.logger import init_logger
-+from vllm.model_executor.layers.activation import SiluAndMul
-+from vllm.model_executor.layers.layernorm import RMSNorm
-+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
-+                                               QKVParallelLinear,
-+                                               RowParallelLinear)
-+from vllm.model_executor.layers.logits_processor import LogitsProcessor
-+from vllm.model_executor.layers.pooler import Pooler, PoolingType
-+from vllm.model_executor.layers.quantization import QuantizationConfig
-+from vllm.model_executor.layers.rotary_embedding import get_rope
-+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
-+from vllm.model_executor.layers.vocab_parallel_embedding import (
-+    ParallelLMHead, VocabParallelEmbedding)
-+from vllm.model_executor.model_loader.weight_utils import (
-+    default_weight_loader, maybe_remap_kv_scale_name)
-+from vllm.model_executor.pooling_metadata import PoolingMetadata
-+from vllm.model_executor.sampling_metadata import SamplingMetadata
-+from vllm.sequence import IntermediateTensors, PoolerOutput
-+
-+from .interfaces import SupportsLoRA, SupportsPP
-+from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
-+                    is_pp_missing_parameter,
-+                    make_empty_intermediate_tensors_factory, make_layers,
-+                    maybe_prefix)
-+
-+logger = init_logger(__name__)
-+
-+
-+class Qwen2MLP(nn.Module):
-+
-+    def __init__(
-+        self,
-+        hidden_size: int,
-+        intermediate_size: int,
-+        hidden_act: str,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ) -> None:
-+        super().__init__()
-+        self.gate_up_proj = MergedColumnParallelLinear(
-+            hidden_size,
-+            [intermediate_size] * 2,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.gate_up_proj",
-+        )
-+        self.down_proj = RowParallelLinear(
-+            intermediate_size,
-+            hidden_size,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.down_proj",
-+        )
-+        if hidden_act != "silu":
-+            raise ValueError(f"Unsupported activation: {hidden_act}. "
-+                             "Only silu is supported for now.")
-+        self.act_fn = SiluAndMul()
-+
-+    def forward(self, x):
-+        gate_up, _ = self.gate_up_proj(x)
-+        x = self.act_fn(gate_up)
-+        x, _ = self.down_proj(x)
-+        return x
-+
-+
-+class Qwen2Attention(nn.Module):
-+
-+    def __init__(self,
-+                 hidden_size: int,
-+                 num_heads: int,
-+                 num_kv_heads: int,
-+                 max_position: int = 4096 * 32,
-+                 rope_theta: float = 10000,
-+                 cache_config: Optional[CacheConfig] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 rope_scaling: Optional[Tuple] = None,
-+                 prefix: str = "",
-+                 attn_type: str = AttentionType.DECODER) -> None:
-+        super().__init__()
-+        self.hidden_size = hidden_size
-+        tp_size = get_tensor_model_parallel_world_size()
-+        self.total_num_heads = num_heads
-+        assert self.total_num_heads % tp_size == 0
-+        self.num_heads = self.total_num_heads // tp_size
-+        self.total_num_kv_heads = num_kv_heads
-+        if self.total_num_kv_heads >= tp_size:
-+            # Number of KV heads is greater than TP size, so we partition
-+            # the KV heads across multiple tensor parallel GPUs.
-+            assert self.total_num_kv_heads % tp_size == 0
-+        else:
-+            # Number of KV heads is less than TP size, so we replicate
-+            # the KV heads across multiple tensor parallel GPUs.
-+            assert tp_size % self.total_num_kv_heads == 0
-+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
-+        self.head_dim = hidden_size // self.total_num_heads
-+        self.q_size = self.num_heads * self.head_dim
-+        self.kv_size = self.num_kv_heads * self.head_dim
-+        self.scaling = self.head_dim**-0.5
-+        self.rope_theta = rope_theta
-+
-+        self.qkv_proj = QKVParallelLinear(
-+            hidden_size,
-+            self.head_dim,
-+            self.total_num_heads,
-+            self.total_num_kv_heads,
-+            bias=True,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.qkv_proj",
-+        )
-+        self.o_proj = RowParallelLinear(
-+            self.total_num_heads * self.head_dim,
-+            hidden_size,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.o_proj",
-+        )
-+
-+        self.rotary_emb = get_rope(
-+            self.head_dim,
-+            rotary_dim=self.head_dim,
-+            max_position=max_position,
-+            base=self.rope_theta,
-+            rope_scaling=rope_scaling,
-+        )
-+        self.attn = Attention(self.num_heads,
-+                              self.head_dim,
-+                              self.scaling,
-+                              num_kv_heads=self.num_kv_heads,
-+                              cache_config=cache_config,
-+                              quant_config=quant_config,
-+                              prefix=f"{prefix}.attn",
-+                              attn_type=attn_type)
-+
-+    def forward(
-+        self,
-+        positions: torch.Tensor,
-+        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
-+    ) -> torch.Tensor:
-+        qkv, _ = self.qkv_proj(hidden_states)
-+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
-+        q, k = self.rotary_emb(positions, q, k)
-+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
-+        output, _ = self.o_proj(attn_output)
-+        return output
-+
-+
-+class Qwen2DecoderLayer(nn.Module):
-+
-+    def __init__(
-+        self,
-+        config: Qwen2Config,
-+        cache_config: Optional[CacheConfig] = None,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ) -> None:
-+        super().__init__()
-+        self.hidden_size = config.hidden_size
-+        # Requires transformers > 4.32.0
-+        rope_theta = getattr(config, "rope_theta", 1000000)
-+        rope_scaling = getattr(config, "rope_scaling", None)
-+
-+        # By default, Qwen2 uses causal attention as it is a decoder-only model.
-+        # You can override the HF config with `is_causal=False` to enable
-+        # bidirectional attention, which is used in some embedding models
-+        # (e.g. Alibaba-NLP/gte-Qwen2-7B-instruct)
-+        if getattr(config, "is_causal", True):
-+            attn_type = AttentionType.DECODER
-+        else:
-+            attn_type = AttentionType.ENCODER_ONLY
-+
-+        self.self_attn = Qwen2Attention(
-+            hidden_size=self.hidden_size,
-+            num_heads=config.num_attention_heads,
-+            max_position=config.max_position_embeddings,
-+            num_kv_heads=config.num_key_value_heads,
-+            rope_theta=rope_theta,
-+            cache_config=cache_config,
-+            quant_config=quant_config,
-+            rope_scaling=rope_scaling,
-+            prefix=f"{prefix}.self_attn",
-+            attn_type=attn_type,
-+        )
-+        self.mlp = Qwen2MLP(
-+            hidden_size=self.hidden_size,
-+            intermediate_size=config.intermediate_size,
-+            hidden_act=config.hidden_act,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.mlp",
-+        )
-+        self.input_layernorm = RMSNorm(config.hidden_size,
-+                                       eps=config.rms_norm_eps)
-+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
-+                                                eps=config.rms_norm_eps)
-+
-+    def forward(
-+        self,
-+        positions: torch.Tensor,
-+        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
-+        residual: Optional[torch.Tensor],
-+    ) -> Tuple[torch.Tensor, torch.Tensor]:
-+        # Self Attention
-+        if residual is None:
-+            residual = hidden_states
-+            hidden_states = self.input_layernorm(hidden_states)
-+        else:
-+            hidden_states, residual = self.input_layernorm(
-+                hidden_states, residual)
-+        hidden_states = self.self_attn(
-+            positions=positions,
-+            hidden_states=hidden_states,
-+            kv_cache=kv_cache,
-+            attn_metadata=attn_metadata,
-+        )
-+
-+        # Fully Connected
-+        hidden_states, residual = self.post_attention_layernorm(
-+            hidden_states, residual)
-+        hidden_states = self.mlp(hidden_states)
-+        return hidden_states, residual
-+
-+
-+@support_torch_compile(
-+    dynamic_arg_dims={
-+        "input_ids": 0,
-+        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
-+        # otherwise (seq_len, ).
-+        "positions": -1,
-+        "intermediate_tensors": 0,
-+        "inputs_embeds": 0,
-+    })
-+class Qwen2Model(nn.Module):
-+
-+    def __init__(self,
-+                 *,
-+                 vllm_config: VllmConfig,
-+                 prefix: str = "",
-+                 decoder_layer_type: type[nn.Module] = Qwen2DecoderLayer):
-+        super().__init__()
-+
-+        config = vllm_config.model_config.hf_config
-+        cache_config = vllm_config.cache_config
-+        quant_config = vllm_config.quant_config
-+
-+        # TODO (@robertgshaw2): see if this can be moved out
-+        if (cache_config.sliding_window is not None
-+                and hasattr(config, "max_window_layers")):
-+            raise ValueError("Sliding window for some but all layers is not "
-+                             "supported. This model uses sliding window "
-+                             "but `max_window_layers` = {} is less than "
-+                             "`num_hidden_layers` = {}. Please open an issue "
-+                             "to discuss this feature.".format(
-+                                 config.max_window_layers,
-+                                 config.num_hidden_layers,
-+                             ))
-+
-+        self.config = config
-+        self.quant_config = quant_config
-+        self.padding_idx = config.pad_token_id
-+        self.vocab_size = config.vocab_size
-+
-+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
-+                                            and get_pp_group().is_last_rank):
-+            self.embed_tokens = VocabParallelEmbedding(
-+                config.vocab_size,
-+                config.hidden_size,
-+                quant_config=quant_config,
-+                prefix=f"{prefix}.embed_tokens",
-+            )
-+        else:
-+            self.embed_tokens = PPMissingLayer()
-+
-+        # Use the provided decoder layer type or default to Qwen2DecoderLayer
-+        decoder_layer_type = decoder_layer_type or Qwen2DecoderLayer
-+        self.start_layer, self.end_layer, self.layers = make_layers(
-+            config.num_hidden_layers,
-+            lambda prefix: decoder_layer_type(config=config,
-+                                              cache_config=cache_config,
-+                                              quant_config=quant_config,
-+                                              prefix=prefix),
-+            prefix=f"{prefix}.layers",
-+        )
-+
-+        self.make_empty_intermediate_tensors = (
-+            make_empty_intermediate_tensors_factory(
-+                ["hidden_states", "residual"], config.hidden_size))
-+        if get_pp_group().is_last_rank:
-+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
-+        else:
-+            self.norm = PPMissingLayer()
-+
-+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-+        return self.embed_tokens(input_ids)
-+
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+        inputs_embeds: Optional[torch.Tensor] = None,
-+    ) -> Union[torch.Tensor, IntermediateTensors]:
-+        if get_pp_group().is_first_rank:
-+            if inputs_embeds is not None:
-+                hidden_states = inputs_embeds
-+            else:
-+                hidden_states = self.get_input_embeddings(input_ids)
-+            residual = None
-+        else:
-+            assert intermediate_tensors is not None
-+            hidden_states = intermediate_tensors["hidden_states"]
-+            residual = intermediate_tensors["residual"]
-+        for i in range(self.start_layer, self.end_layer):
-+            layer = self.layers[i]
-+            hidden_states, residual = layer(
-+                positions,
-+                hidden_states,
-+                kv_caches[i - self.start_layer],
-+                attn_metadata,
-+                residual,
-+            )
-+        if not get_pp_group().is_last_rank:
-+            return IntermediateTensors({
-+                "hidden_states": hidden_states,
-+                "residual": residual
-+            })
-+        hidden_states, _ = self.norm(hidden_states, residual)
-+        return hidden_states
-+
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
-+        stacked_params_mapping = [
-+            # (param_name, shard_name, shard_id)
-+            ("qkv_proj", "q_proj", "q"),
-+            ("qkv_proj", "k_proj", "k"),
-+            ("qkv_proj", "v_proj", "v"),
-+            ("gate_up_proj", "gate_proj", 0),
-+            ("gate_up_proj", "up_proj", 1),
-+        ]
-+        params_dict = dict(self.named_parameters(remove_duplicate=False))
-+        loaded_params: Set[str] = set()
-+        for name, loaded_weight in weights:
-+            if "rotary_emb.inv_freq" in name:
-+                continue
-+            if (self.quant_config is not None and
-+                (scale_name := self.quant_config.get_cache_scale(name))):
-+                # Loading kv cache quantization scales
-+                param = params_dict[scale_name]
-+                weight_loader = getattr(param, "weight_loader",
-+                                        default_weight_loader)
-+                loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
-+                                 loaded_weight[0])
-+                weight_loader(param, loaded_weight)
-+                loaded_params.add(scale_name)
-+                continue
-+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
-+                if weight_name not in name:
-+                    continue
-+                name = name.replace(weight_name, param_name)
-+                # Skip loading extra bias for GPTQ models.
-+                if name.endswith(".bias") and name not in params_dict:
-+                    continue
-+                if is_pp_missing_parameter(name, self):
-+                    continue
-+                param = params_dict[name]
-+                weight_loader = param.weight_loader
-+                weight_loader(param, loaded_weight, shard_id)
-+                break
-+            else:
-+                # Skip loading extra bias for GPTQ models.
-+                if name.endswith(".bias") and name not in params_dict:
-+                    continue
-+                # Remapping the name of FP8 kv-scale.
-+                name = maybe_remap_kv_scale_name(name, params_dict)
-+                if name is None:
-+                    continue
-+                if is_pp_missing_parameter(name, self):
-+                    continue
-+                param = params_dict[name]
-+                weight_loader = getattr(param, "weight_loader",
-+                                        default_weight_loader)
-+                weight_loader(param, loaded_weight)
-+            loaded_params.add(name)
-+        return loaded_params
-+
-+
-+class Qwen2ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
-+    packed_modules_mapping = {
-+        "qkv_proj": [
-+            "q_proj",
-+            "k_proj",
-+            "v_proj",
-+        ],
-+        "gate_up_proj": [
-+            "gate_proj",
-+            "up_proj",
-+        ],
-+    }
-+
-+    # LoRA specific attributes
-+    supported_lora_modules = [
-+        "qkv_proj",
-+        "o_proj",
-+        "gate_up_proj",
-+        "down_proj",
-+    ]
-+    embedding_modules = {}
-+    embedding_padding_modules = []
-+
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        super().__init__()
-+        config = vllm_config.model_config.hf_config
-+        quant_config = vllm_config.quant_config
-+        lora_config = vllm_config.lora_config
-+
-+        self.config = config
-+        self.lora_config = lora_config
-+
-+        self.quant_config = quant_config
-+        self.model = Qwen2Model(vllm_config=vllm_config,
-+                                prefix=maybe_prefix(prefix, "model"))
-+
-+        if get_pp_group().is_last_rank:
-+            if config.tie_word_embeddings:
-+                self.lm_head = self.model.embed_tokens
-+            else:
-+                self.lm_head = ParallelLMHead(config.vocab_size,
-+                                              config.hidden_size,
-+                                              quant_config=quant_config,
-+                                              prefix=maybe_prefix(
-+                                                  prefix, "lm_head"))
-+        else:
-+            self.lm_head = PPMissingLayer()
-+
-+        self.logits_processor = LogitsProcessor(config.vocab_size)
-+        self.sampler = get_sampler()
-+
-+        self.make_empty_intermediate_tensors = (
-+            self.model.make_empty_intermediate_tensors)
-+
-+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-+        return self.model.get_input_embeddings(input_ids)
-+
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+        inputs_embeds: Optional[torch.Tensor] = None,
-+    ) -> Union[torch.Tensor, IntermediateTensors]:
-+        hidden_states = self.model(input_ids, positions, kv_caches,
-+                                   attn_metadata, intermediate_tensors,
-+                                   inputs_embeds)
-+        return hidden_states
-+
-+    def compute_logits(
-+        self,
-+        hidden_states: torch.Tensor,
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[torch.Tensor]:
-+        logits = self.logits_processor(self.lm_head, hidden_states,
-+                                       sampling_metadata)
-+        return logits
-+
-+    def sample(
-+        self,
-+        logits: torch.Tensor,
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[SamplerOutput]:
-+        next_tokens = self.sampler(logits, sampling_metadata)
-+        return next_tokens
-+
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
-+        loader = AutoWeightsLoader(
-+            self,
-+            skip_prefixes=(["lm_head."]
-+                           if self.config.tie_word_embeddings else None),
-+        )
-+        return loader.load_weights(weights)
-+
-+
-+class Qwen2EmbeddingModel(nn.Module, SupportsLoRA, SupportsPP):
-+    packed_modules_mapping = {
-+        "qkv_proj": [
-+            "q_proj",
-+            "k_proj",
-+            "v_proj",
-+        ],
-+        "gate_up_proj": [
-+            "gate_proj",
-+            "up_proj",
-+        ],
-+    }
-+
-+    # LoRA specific attributes
-+    supported_lora_modules = [
-+        "qkv_proj",
-+        "o_proj",
-+        "gate_up_proj",
-+        "down_proj",
-+    ]
-+    embedding_modules = {}
-+    embedding_padding_modules = []
-+
-+    hf_to_vllm_mapper = WeightsMapper(orig_to_new_prefix={"model.": ""})
-+
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        super().__init__()
-+        config = vllm_config.model_config.hf_config
-+        quant_config = vllm_config.quant_config
-+        lora_config = vllm_config.lora_config
-+        pooler_config = vllm_config.model_config.pooler_config
-+
-+        self.config = config
-+        self.lora_config = lora_config
-+
-+        self.quant_config = quant_config
-+        self.model = Qwen2Model(vllm_config=vllm_config,
-+                                prefix=maybe_prefix(prefix, "model"))
-+
-+        # TODO: Replace this model class with as_embedding_model(
-+        # Qwen2ForCausalLM) after changing the default pooling method
-+        if pooler_config.pooling_type is None:
-+            logger.warning(
-+                "This embedding model will default to last-token pooling in "
-+                "an upcoming version. To avoid breaking changes, you should "
-+                "pass `--override-pooler-config '{\"pooling_type\": \"MEAN\"}'`"
-+                " explicitly.")
-+
-+        self._pooler = Pooler.from_config_with_defaults(
-+            pooler_config,
-+            pooling_type=PoolingType.MEAN,
-+            normalize=True,
-+            softmax=False)
-+
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+    ) -> torch.Tensor:
-+        return self.model(input_ids, positions, kv_caches, attn_metadata,
-+                          intermediate_tensors)
-+
-+    def pooler(
-+        self,
-+        hidden_states: torch.Tensor,
-+        pooling_metadata: PoolingMetadata,
-+    ) -> Optional[PoolerOutput]:
-+        return self._pooler(hidden_states, pooling_metadata)
-+
-+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
-+        weights = self.hf_to_vllm_mapper.apply(weights)
-+        weights = ((name, data) for name, data in weights
-+                   if not name.startswith("lm_head."))
-+        self.model.load_weights(weights)
-diff --git a/vllm/model_executor/models/qwen3.py b/vllm/model_executor/models/qwen3.py
-new file mode 100644
-index 000000000..54831961a
---- /dev/null
-+++ b/vllm/model_executor/models/qwen3.py
-@@ -0,0 +1,337 @@
-+# SPDX-License-Identifier: Apache-2.0
-+
-+# Copyright 2024 The Qwen team.
-+# Copyright 2023 The vLLM team.
-+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
-+#
-+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
-+# and OPT implementations in this library. It has been modified from its
-+# original forms to accommodate minor architectural differences compared
-+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
-+#
-+# Licensed under the Apache License, Version 2.0 (the "License");
-+# you may not use this file except in compliance with the License.
-+# You may obtain a copy of the License at
-+#
-+#     http://www.apache.org/licenses/LICENSE-2.0
-+#
-+# Unless required by applicable law or agreed to in writing, software
-+# distributed under the License is distributed on an "AS IS" BASIS,
-+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-+# See the License for the specific language governing permissions and
-+# limitations under the License.
-+"""Inference-only Qwen3 model compatible with HuggingFace weights."""
-+from typing import Iterable, List, Optional, Set, Tuple, Union
-+
-+import torch
-+from torch import nn
-+from transformers import Qwen3Config
-+
-+from vllm.attention import Attention, AttentionMetadata, AttentionType
-+from vllm.compilation.decorators import support_torch_compile
-+from vllm.config import CacheConfig, VllmConfig
-+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
-+from vllm.logger import init_logger
-+from vllm.model_executor.layers.layernorm import RMSNorm
-+from vllm.model_executor.layers.linear import (QKVParallelLinear,
-+                                               RowParallelLinear)
-+from vllm.model_executor.layers.logits_processor import LogitsProcessor
-+from vllm.model_executor.layers.quantization import QuantizationConfig
-+from vllm.model_executor.layers.rotary_embedding import get_rope
-+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
-+from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead
-+from vllm.model_executor.sampling_metadata import SamplingMetadata
-+from vllm.sequence import IntermediateTensors
-+
-+from .interfaces import SupportsLoRA, SupportsPP
-+from .qwen2 import Qwen2MLP as Qwen3MLP
-+from .qwen2 import Qwen2Model
-+from .utils import AutoWeightsLoader, PPMissingLayer, maybe_prefix
-+
-+logger = init_logger(__name__)
-+
-+
-+class Qwen3Attention(nn.Module):
-+
-+    def __init__(self,
-+                 hidden_size: int,
-+                 num_heads: int,
-+                 num_kv_heads: int,
-+                 max_position: int = 4096 * 32,
-+                 head_dim: Optional[int] = None,
-+                 rms_norm_eps: float = 1e-06,
-+                 qkv_bias: bool = False,
-+                 rope_theta: float = 10000,
-+                 cache_config: Optional[CacheConfig] = None,
-+                 quant_config: Optional[QuantizationConfig] = None,
-+                 rope_scaling: Optional[Tuple] = None,
-+                 prefix: str = "",
-+                 attn_type: str = AttentionType.DECODER) -> None:
-+        super().__init__()
-+        self.hidden_size = hidden_size
-+        tp_size = get_tensor_model_parallel_world_size()
-+        self.total_num_heads = num_heads
-+        assert self.total_num_heads % tp_size == 0
-+        self.num_heads = self.total_num_heads // tp_size
-+        self.total_num_kv_heads = num_kv_heads
-+        if self.total_num_kv_heads >= tp_size:
-+            # Number of KV heads is greater than TP size, so we partition
-+            # the KV heads across multiple tensor parallel GPUs.
-+            assert self.total_num_kv_heads % tp_size == 0
-+        else:
-+            # Number of KV heads is less than TP size, so we replicate
-+            # the KV heads across multiple tensor parallel GPUs.
-+            assert tp_size % self.total_num_kv_heads == 0
-+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
-+        self.head_dim = head_dim or hidden_size // self.total_num_heads
-+        self.q_size = self.num_heads * self.head_dim
-+        self.kv_size = self.num_kv_heads * self.head_dim
-+        self.scaling = self.head_dim**-0.5
-+        self.rope_theta = rope_theta
-+
-+        self.qkv_proj = QKVParallelLinear(
-+            hidden_size,
-+            self.head_dim,
-+            self.total_num_heads,
-+            self.total_num_kv_heads,
-+            bias=qkv_bias,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.qkv_proj",
-+        )
-+        self.o_proj = RowParallelLinear(
-+            self.total_num_heads * self.head_dim,
-+            hidden_size,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.o_proj",
-+        )
-+
-+        self.rotary_emb = get_rope(
-+            self.head_dim,
-+            rotary_dim=self.head_dim,
-+            max_position=max_position,
-+            base=self.rope_theta,
-+            rope_scaling=rope_scaling,
-+        )
-+        self.attn = Attention(self.num_heads,
-+                              self.head_dim,
-+                              self.scaling,
-+                              num_kv_heads=self.num_kv_heads,
-+                              cache_config=cache_config,
-+                              quant_config=quant_config,
-+                              prefix=f"{prefix}.attn",
-+                              attn_type=attn_type)
-+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
-+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
-+
-+    def forward(
-+        self,
-+        positions: torch.Tensor,
-+        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
-+    ) -> torch.Tensor:
-+        qkv, _ = self.qkv_proj(hidden_states)
-+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
-+        # Add qk-norm
-+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
-+                           self.head_dim)
-+        q_by_head = self.q_norm.forward_native(q_by_head)
-+        q = q_by_head.view(q.shape)
-+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
-+                           self.head_dim)
-+        k_by_head = self.k_norm.forward_native(k_by_head)
-+        k = k_by_head.view(k.shape)
-+        q, k = self.rotary_emb(positions, q, k)
-+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
-+        output, _ = self.o_proj(attn_output)
-+        return output
-+
-+
-+class Qwen3DecoderLayer(nn.Module):
-+
-+    def __init__(
-+        self,
-+        config: Qwen3Config,
-+        cache_config: Optional[CacheConfig] = None,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ) -> None:
-+        super().__init__()
-+        self.hidden_size = config.hidden_size
-+        # Requires transformers > 4.32.0
-+        rope_theta = getattr(config, "rope_theta", 1000000)
-+        rope_scaling = getattr(config, "rope_scaling", None)
-+
-+        # By default, Qwen3 uses causal attention as it is a decoder-only model.
-+        # You can override the HF config with `is_causal=False` to enable
-+        # bidirectional attention, which is used in some embedding models
-+        # (e.g. Alibaba-NLP/gte-Qwen3-7B-instruct)
-+        if getattr(config, "is_causal", True):
-+            attn_type = AttentionType.DECODER
-+        else:
-+            attn_type = AttentionType.ENCODER_ONLY
-+
-+        self.self_attn = Qwen3Attention(
-+            hidden_size=self.hidden_size,
-+            num_heads=config.num_attention_heads,
-+            max_position=config.max_position_embeddings,
-+            num_kv_heads=config.num_key_value_heads,
-+            rope_theta=rope_theta,
-+            rms_norm_eps=config.rms_norm_eps,
-+            qkv_bias=getattr(config, 'attention_bias', False),
-+            head_dim=getattr(config, 'head_dim', None),
-+            cache_config=cache_config,
-+            quant_config=quant_config,
-+            rope_scaling=rope_scaling,
-+            prefix=f"{prefix}.self_attn",
-+            attn_type=attn_type,
-+        )
-+        self.mlp = Qwen3MLP(
-+            hidden_size=self.hidden_size,
-+            intermediate_size=config.intermediate_size,
-+            hidden_act=config.hidden_act,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.mlp",
-+        )
-+        self.input_layernorm = RMSNorm(config.hidden_size,
-+                                       eps=config.rms_norm_eps)
-+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
-+                                                eps=config.rms_norm_eps)
-+
-+    def forward(
-+        self,
-+        positions: torch.Tensor,
-+        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
-+        residual: Optional[torch.Tensor],
-+    ) -> Tuple[torch.Tensor, torch.Tensor]:
-+        # Self Attention
-+        if residual is None:
-+            residual = hidden_states
-+            hidden_states = self.input_layernorm(hidden_states)
-+        else:
-+            hidden_states, residual = self.input_layernorm(
-+                hidden_states, residual)
-+        hidden_states = self.self_attn(
-+            positions=positions,
-+            hidden_states=hidden_states,
-+            kv_cache=kv_cache,
-+            attn_metadata=attn_metadata,
-+        )
-+
-+        # Fully Connected
-+        hidden_states, residual = self.post_attention_layernorm(
-+            hidden_states, residual)
-+        hidden_states = self.mlp(hidden_states)
-+        return hidden_states, residual
-+
-+
-+ALL_DECODER_LAYER_TYPES = {
-+    "attention": Qwen3DecoderLayer,
-+}
-+
-+
-+@support_torch_compile(
-+    dynamic_arg_dims={
-+        "input_ids": 0,
-+        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
-+        # otherwise (seq_len, ).
-+        "positions": -1,
-+        "intermediate_tensors": 0,
-+        "inputs_embeds": 0,
-+    })
-+class Qwen3Model(Qwen2Model):
-+
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        super().__init__(vllm_config=vllm_config,
-+                         prefix=prefix,
-+                         decoder_layer_type=Qwen3DecoderLayer)
-+
-+
-+class Qwen3ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
-+    packed_modules_mapping = {
-+        "qkv_proj": [
-+            "q_proj",
-+            "k_proj",
-+            "v_proj",
-+        ],
-+        "gate_up_proj": [
-+            "gate_proj",
-+            "up_proj",
-+        ],
-+    }
-+
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        super().__init__()
-+        config = vllm_config.model_config.hf_config
-+        quant_config = vllm_config.quant_config
-+        lora_config = vllm_config.lora_config
-+
-+        self.config = config
-+        self.lora_config = lora_config
-+
-+        self.quant_config = quant_config
-+        self.model = Qwen3Model(vllm_config=vllm_config,
-+                                prefix=maybe_prefix(prefix, "model"))
-+
-+        if get_pp_group().is_last_rank:
-+            if config.tie_word_embeddings:
-+                self.lm_head = self.model.embed_tokens
-+            else:
-+                self.lm_head = ParallelLMHead(config.vocab_size,
-+                                              config.hidden_size,
-+                                              quant_config=quant_config,
-+                                              prefix=maybe_prefix(
-+                                                  prefix, "lm_head"))
-+        else:
-+            self.lm_head = PPMissingLayer()
-+
-+        self.logits_processor = LogitsProcessor(config.vocab_size)
-+        self.sampler = get_sampler()
-+
-+        self.make_empty_intermediate_tensors = (
-+            self.model.make_empty_intermediate_tensors)
-+
-+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-+        return self.model.get_input_embeddings(input_ids)
-+
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+        inputs_embeds: Optional[torch.Tensor] = None,
-+    ) -> Union[torch.Tensor, IntermediateTensors]:
-+        hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors,
-+                                   inputs_embeds)
-+        return hidden_states
-+
-+    def compute_logits(
-+        self,
-+        hidden_states: torch.Tensor,
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[torch.Tensor]:
-+        logits = self.logits_processor(self.lm_head, hidden_states,
-+                                       sampling_metadata)
-+        return logits
-+
-+    def sample(
-+        self,
-+        logits: torch.Tensor,
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[SamplerOutput]:
-+        next_tokens = self.sampler(logits, sampling_metadata)
-+        return next_tokens
-+
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
-+        loader = AutoWeightsLoader(
-+            self,
-+            skip_prefixes=(["lm_head."]
-+                           if self.config.tie_word_embeddings else None),
-+        )
-+        return loader.load_weights(weights)
-diff --git a/vllm/model_executor/models/qwen3_moe.py b/vllm/model_executor/models/qwen3_moe.py
-new file mode 100644
-index 000000000..4ef45b75b
---- /dev/null
-+++ b/vllm/model_executor/models/qwen3_moe.py
-@@ -0,0 +1,550 @@
-+# SPDX-License-Identifier: Apache-2.0
-+
-+# Copyright 2024 The Qwen team.
-+# Copyright 2023 The vLLM team.
-+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
-+#
-+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
-+# and OPT implementations in this library. It has been modified from its
-+# original forms to accommodate minor architectural differences compared
-+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
-+#
-+# Licensed under the Apache License, Version 2.0 (the "License");
-+# you may not use this file except in compliance with the License.
-+# You may obtain a copy of the License at
-+#
-+#     http://www.apache.org/licenses/LICENSE-2.0
-+#
-+# Unless required by applicable law or agreed to in writing, software
-+# distributed under the License is distributed on an "AS IS" BASIS,
-+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-+# See the License for the specific language governing permissions and
-+# limitations under the License.
-+"""Inference-only Qwen3MoE model compatible with HuggingFace weights."""
-+from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union
-+
-+import torch
-+from torch import nn
-+from transformers import PretrainedConfig
-+
-+from vllm.attention import Attention, AttentionMetadata
-+from vllm.compilation.decorators import support_torch_compile
-+from vllm.config import CacheConfig, VllmConfig
-+from vllm.distributed import (get_pp_group,
-+                              get_tensor_model_parallel_world_size,
-+                              tensor_model_parallel_all_reduce)
-+from vllm.logger import init_logger
-+from vllm.model_executor.layers.activation import SiluAndMul
-+from vllm.model_executor.layers.fused_moe import FusedMoE
-+from vllm.model_executor.layers.layernorm import RMSNorm
-+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
-+                                               QKVParallelLinear,
-+                                               ReplicatedLinear,
-+                                               RowParallelLinear)
-+from vllm.model_executor.layers.logits_processor import LogitsProcessor
-+from vllm.model_executor.layers.quantization import QuantizationConfig
-+from vllm.model_executor.layers.rotary_embedding import get_rope
-+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
-+from vllm.model_executor.layers.vocab_parallel_embedding import (
-+    ParallelLMHead, VocabParallelEmbedding)
-+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
-+from vllm.model_executor.sampling_metadata import SamplingMetadata
-+from vllm.sequence import IntermediateTensors
-+
-+from .interfaces import SupportsPP
-+from .utils import (AutoWeightsLoader, extract_layer_index,
-+                    is_pp_missing_parameter,
-+                    make_empty_intermediate_tensors_factory, make_layers,
-+                    maybe_prefix)
-+
-+logger = init_logger(__name__)
-+
-+
-+class Qwen3MoeMLP(nn.Module):
-+
-+    def __init__(
-+        self,
-+        hidden_size: int,
-+        intermediate_size: int,
-+        hidden_act: str,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        reduce_results: bool = True,
-+        prefix: str = "",
-+    ) -> None:
-+        super().__init__()
-+        self.gate_up_proj = MergedColumnParallelLinear(
-+            hidden_size, [intermediate_size] * 2,
-+            bias=False,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.gate_up_proj")
-+        self.down_proj = RowParallelLinear(intermediate_size,
-+                                           hidden_size,
-+                                           bias=False,
-+                                           quant_config=quant_config,
-+                                           reduce_results=reduce_results,
-+                                           prefix=f"{prefix}.down_proj")
-+        if hidden_act != "silu":
-+            raise ValueError(f"Unsupported activation: {hidden_act}. "
-+                             "Only silu is supported for now.")
-+        self.act_fn = SiluAndMul()
-+
-+    def forward(self, x):
-+        gate_up, _ = self.gate_up_proj(x)
-+        x = self.act_fn(gate_up)
-+        x, _ = self.down_proj(x)
-+        return x
-+
-+
-+class Qwen3MoeSparseMoeBlock(nn.Module):
-+
-+    def __init__(
-+        self,
-+        config: PretrainedConfig,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ):
-+        super().__init__()
-+        self.tp_size = get_tensor_model_parallel_world_size()
-+
-+        if self.tp_size > config.num_experts:
-+            raise ValueError(
-+                f"Tensor parallel size {self.tp_size} is greater than "
-+                f"the number of experts {config.num_experts}.")
-+
-+        self.experts = FusedMoE(num_experts=config.num_experts,
-+                                top_k=config.num_experts_per_tok,
-+                                hidden_size=config.hidden_size,
-+                                intermediate_size=config.moe_intermediate_size,
-+                                reduce_results=False,
-+                                renormalize=config.norm_topk_prob,
-+                                quant_config=quant_config,
-+                                prefix=f"{prefix}.experts")
-+
-+        self.gate = ReplicatedLinear(config.hidden_size,
-+                                     config.num_experts,
-+                                     bias=False,
-+                                     quant_config=None,
-+                                     prefix=f"{prefix}.gate")
-+
-+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-+        # NOTE: hidden_states can have either 1D or 2D shape.
-+        orig_shape = hidden_states.shape
-+        hidden_dim = hidden_states.shape[-1]
-+        hidden_states = hidden_states.view(-1, hidden_dim)
-+
-+        # router_logits: (num_tokens, n_experts)
-+        router_logits, _ = self.gate(hidden_states)
-+        final_hidden_states = self.experts(hidden_states=hidden_states,
-+                                           router_logits=router_logits)
-+        final_hidden_states = final_hidden_states
-+        if self.tp_size > 1:
-+            final_hidden_states = tensor_model_parallel_all_reduce(
-+                final_hidden_states)
-+
-+        return final_hidden_states.view(orig_shape)
-+
-+
-+class Qwen3MoeAttention(nn.Module):
-+
-+    def __init__(
-+        self,
-+        hidden_size: int,
-+        num_heads: int,
-+        num_kv_heads: int,
-+        rope_theta: float = 10000,
-+        rope_scaling: Optional[Dict[str, Any]] = None,
-+        max_position_embeddings: int = 8192,
-+        head_dim: Optional[int] = None,
-+        rms_norm_eps: float = 1e-06,
-+        qkv_bias: bool = False,
-+        cache_config: Optional[CacheConfig] = None,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ) -> None:
-+        super().__init__()
-+        self.hidden_size = hidden_size
-+        tp_size = get_tensor_model_parallel_world_size()
-+        self.total_num_heads = num_heads
-+        assert self.total_num_heads % tp_size == 0
-+        self.num_heads = self.total_num_heads // tp_size
-+        self.total_num_kv_heads = num_kv_heads
-+        if self.total_num_kv_heads >= tp_size:
-+            # Number of KV heads is greater than TP size, so we partition
-+            # the KV heads across multiple tensor parallel GPUs.
-+            assert self.total_num_kv_heads % tp_size == 0
-+        else:
-+            # Number of KV heads is less than TP size, so we replicate
-+            # the KV heads across multiple tensor parallel GPUs.
-+            assert tp_size % self.total_num_kv_heads == 0
-+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
-+        self.head_dim = head_dim or (hidden_size // self.total_num_heads)
-+        self.q_size = self.num_heads * self.head_dim
-+        self.kv_size = self.num_kv_heads * self.head_dim
-+        self.scaling = self.head_dim**-0.5
-+        self.rope_theta = rope_theta
-+        self.max_position_embeddings = max_position_embeddings
-+
-+        self.qkv_proj = QKVParallelLinear(hidden_size,
-+                                          self.head_dim,
-+                                          self.total_num_heads,
-+                                          self.total_num_kv_heads,
-+                                          bias=qkv_bias,
-+                                          quant_config=quant_config,
-+                                          prefix=f"{prefix}.qkv_proj")
-+
-+        self.o_proj = RowParallelLinear(self.total_num_heads * self.head_dim,
-+                                        hidden_size,
-+                                        bias=False,
-+                                        quant_config=quant_config,
-+                                        prefix=f"{prefix}.o_proj")
-+
-+        self.rotary_emb = get_rope(
-+            self.head_dim,
-+            rotary_dim=self.head_dim,
-+            max_position=max_position_embeddings,
-+            base=rope_theta,
-+            rope_scaling=rope_scaling,
-+        )
-+        self.attn = Attention(self.num_heads,
-+                              self.head_dim,
-+                              self.scaling,
-+                              num_kv_heads=self.num_kv_heads,
-+                              cache_config=cache_config,
-+                              quant_config=quant_config,
-+                              prefix=f"{prefix}.attn")
-+
-+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
-+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
-+
-+    def forward(
-+        self,
-+        positions: torch.Tensor,
-+        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
-+    ) -> torch.Tensor:
-+        qkv, _ = self.qkv_proj(hidden_states)
-+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
-+        # Add qk-norm
-+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
-+                           self.head_dim)
-+        q_by_head = self.q_norm.forward_native(q_by_head)
-+        q = q_by_head.view(q.shape)
-+
-+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
-+                           self.head_dim)
-+        k_by_head = self.k_norm.forward_native(k_by_head)
-+        k = k_by_head.view(k.shape)
-+        q, k = self.rotary_emb(positions, q, k)
-+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
-+        output, _ = self.o_proj(attn_output)
-+        return output
-+
-+
-+class Qwen3MoeDecoderLayer(nn.Module):
-+
-+    def __init__(
-+        self,
-+        config: PretrainedConfig,
-+        cache_config: Optional[CacheConfig] = None,
-+        quant_config: Optional[QuantizationConfig] = None,
-+        prefix: str = "",
-+    ) -> None:
-+        super().__init__()
-+        self.hidden_size = config.hidden_size
-+        rope_theta = getattr(config, "rope_theta", 10000)
-+        rope_scaling = getattr(config, "rope_scaling", None)
-+        max_position_embeddings = getattr(config, "max_position_embeddings",
-+                                          8192)
-+        self.self_attn = Qwen3MoeAttention(
-+            hidden_size=self.hidden_size,
-+            num_heads=config.num_attention_heads,
-+            num_kv_heads=config.num_key_value_heads,
-+            rope_theta=rope_theta,
-+            rope_scaling=rope_scaling,
-+            max_position_embeddings=max_position_embeddings,
-+            rms_norm_eps=config.rms_norm_eps,
-+            qkv_bias=getattr(config, 'attention_bias', False),
-+            head_dim=getattr(config, 'head_dim', None),
-+            cache_config=cache_config,
-+            quant_config=quant_config,
-+            prefix=f"{prefix}.self_attn",
-+        )
-+
-+        # `mlp_only_layers` in the config.
-+        layer_idx = extract_layer_index(prefix)
-+        mlp_only_layers = ([] if not hasattr(config, "mlp_only_layers") else
-+                           config.mlp_only_layers)
-+        if (layer_idx not in mlp_only_layers) and (
-+                config.num_experts > 0 and
-+            (layer_idx + 1) % config.decoder_sparse_step == 0):
-+            self.mlp = Qwen3MoeSparseMoeBlock(config=config,
-+                                              quant_config=quant_config,
-+                                              prefix=f"{prefix}.mlp")
-+        else:
-+            self.mlp = Qwen3MoeMLP(hidden_size=config.hidden_size,
-+                                   intermediate_size=config.intermediate_size,
-+                                   hidden_act=config.hidden_act,
-+                                   quant_config=quant_config,
-+                                   prefix=f"{prefix}.mlp")
-+        self.input_layernorm = RMSNorm(config.hidden_size,
-+                                       eps=config.rms_norm_eps)
-+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
-+                                                eps=config.rms_norm_eps)
-+
-+    def forward(
-+        self,
-+        positions: torch.Tensor,
-+        hidden_states: torch.Tensor,
-+        kv_cache: torch.Tensor,
-+        attn_metadata: AttentionMetadata,
-+        residual: Optional[torch.Tensor],
-+    ) -> torch.Tensor:
-+        # Self Attention
-+        if residual is None:
-+            residual = hidden_states
-+            hidden_states = self.input_layernorm(hidden_states)
-+        else:
-+            hidden_states, residual = self.input_layernorm(
-+                hidden_states, residual)
-+        hidden_states = self.self_attn(
-+            positions=positions,
-+            hidden_states=hidden_states,
-+            kv_cache=kv_cache,
-+            attn_metadata=attn_metadata,
-+        )
-+
-+        # Fully Connected
-+        hidden_states, residual = self.post_attention_layernorm(
-+            hidden_states, residual)
-+        hidden_states = self.mlp(hidden_states)
-+        return hidden_states, residual
-+
-+
-+@support_torch_compile
-+class Qwen3MoeModel(nn.Module):
-+
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        super().__init__()
-+
-+        config = vllm_config.model_config.hf_config
-+        cache_config = vllm_config.cache_config
-+        quant_config = vllm_config.quant_config
-+
-+        self.padding_idx = config.pad_token_id
-+        self.vocab_size = config.vocab_size
-+        self.config = config
-+        self.embed_tokens = VocabParallelEmbedding(
-+            config.vocab_size,
-+            config.hidden_size,
-+            prefix=f"{prefix}.embed_tokens")
-+        self.start_layer, self.end_layer, self.layers = make_layers(
-+            config.num_hidden_layers,
-+            lambda prefix: Qwen3MoeDecoderLayer(config=config,
-+                                                cache_config=cache_config,
-+                                                quant_config=quant_config,
-+                                                prefix=prefix),
-+            prefix=f"{prefix}.layers",
-+        )
-+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
-+        self.make_empty_intermediate_tensors = (
-+            make_empty_intermediate_tensors_factory(
-+                ["hidden_states", "residual"], config.hidden_size))
-+
-+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-+        return self.embed_tokens(input_ids)
-+
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+        inputs_embeds: Optional[torch.Tensor] = None,
-+    ) -> Union[torch.Tensor, IntermediateTensors]:
-+        if get_pp_group().is_first_rank:
-+            if inputs_embeds is not None:
-+                hidden_states = inputs_embeds
-+            else:
-+                hidden_states = self.get_input_embeddings(input_ids)
-+            residual = None
-+        else:
-+            assert intermediate_tensors is not None
-+            hidden_states = intermediate_tensors["hidden_states"]
-+            residual = intermediate_tensors["residual"]
-+        for i in range(self.start_layer, self.end_layer):
-+            layer = self.layers[i]
-+            hidden_states, residual = layer(positions, hidden_states, kv_caches[i - self.start_layer],
-+                attn_metadata, residual)
-+        if not get_pp_group().is_last_rank:
-+            return IntermediateTensors({
-+                "hidden_states": hidden_states,
-+                "residual": residual
-+            })
-+        hidden_states, _ = self.norm(hidden_states, residual)
-+        return hidden_states
-+
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
-+        stacked_params_mapping = [
-+            # (param_name, shard_name, shard_id)
-+            ("qkv_proj", "q_proj", "q"),
-+            ("qkv_proj", "k_proj", "k"),
-+            ("qkv_proj", "v_proj", "v"),
-+            ("gate_up_proj", "gate_proj", 0),
-+            ("gate_up_proj", "up_proj", 1),
-+        ]
-+
-+        # Params for weights, fp8 weight scales, fp8 activation scales
-+        # (param_name, weight_name, expert_id, shard_id)
-+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
-+            ckpt_gate_proj_name="gate_proj",
-+            ckpt_down_proj_name="down_proj",
-+            ckpt_up_proj_name="up_proj",
-+            num_experts=self.config.num_experts)
-+
-+        params_dict = dict(self.named_parameters())
-+        loaded_params: Set[str] = set()
-+        for name, loaded_weight in weights:
-+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
-+                # Skip non-stacked layers and experts (experts handled below).
-+                if weight_name not in name:
-+                    continue
-+                # We have mlp.experts[0].gate_proj in the checkpoint.
-+                # Since we handle the experts below in expert_params_mapping,
-+                # we need to skip here BEFORE we update the name, otherwise
-+                # name will be updated to mlp.experts[0].gate_up_proj, which
-+                # will then be updated below in expert_params_mapping
-+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
-+                if "mlp.experts" in name:
-+                    continue
-+                name = name.replace(weight_name, param_name)
-+                # Skip loading extra bias for GPTQ models.
-+                if ((name.endswith(".bias") or name.endswith("_bias"))
-+                        and name not in params_dict):
-+                    continue
-+                # Skip layers on other devices.
-+                if is_pp_missing_parameter(name, self):
-+                    continue
-+                if name not in params_dict:
-+                    continue
-+
-+                param = params_dict[name]
-+                weight_loader = param.weight_loader
-+                weight_loader(param, loaded_weight, shard_id)
-+                break
-+            else:
-+                for mapping in expert_params_mapping:
-+                    param_name, weight_name, expert_id, shard_id = mapping
-+                    if weight_name not in name:
-+                        continue
-+                    name = name.replace(weight_name, param_name)
-+                    # Skip layers on other devices.
-+                    if is_pp_missing_parameter(name, self):
-+                        continue
-+                    # Skip loading extra bias for GPTQ models.
-+                    if ((name.endswith(".bias") or name.endswith("_bias"))
-+                            and name not in params_dict):
-+                        continue
-+                    param = params_dict[name]
-+                    weight_loader = param.weight_loader
-+                    weight_loader(param,
-+                                  loaded_weight,
-+                                  name,
-+                                  shard_id=shard_id,
-+                                  expert_id=expert_id)
-+                    break
-+                else:
-+                    # Skip loading extra bias for GPTQ models.
-+                    if ((name.endswith(".bias") or name.endswith("_bias"))
-+                            and name not in params_dict):
-+                        continue
-+                    # Skip layers on other devices.
-+                    if is_pp_missing_parameter(name, self):
-+                        continue
-+                    # Remapping the name of FP8 kv-scale.
-+                    if name.endswith("kv_scale"):
-+                        remapped_kv_scale_name = name.replace(
-+                            ".kv_scale", ".attn.kv_scale")
-+                        if remapped_kv_scale_name not in params_dict:
-+                            logger.warning_once(
-+                                "Found kv scale in the checkpoint "
-+                                f"(e.g. {name}), but not found the expected "
-+                                f"name in the model "
-+                                f"(e.g. {remapped_kv_scale_name}). "
-+                                "kv-scale is not loaded.")
-+                            continue
-+                        else:
-+                            name = remapped_kv_scale_name
-+                    param = params_dict[name]
-+                    weight_loader = getattr(param, "weight_loader",
-+                                            default_weight_loader)
-+                    weight_loader(param, loaded_weight)
-+            loaded_params.add(name)
-+        return loaded_params
-+
-+ 
-+class Qwen3MoeForCausalLM(nn.Module, SupportsPP):
-+
-+    fall_back_to_pt_during_load = False
-+
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        super().__init__()
-+        config = vllm_config.model_config.hf_config
-+        quant_config = vllm_config.quant_config
-+        self.config = config
-+        self.quant_config = quant_config
-+        self.model = Qwen3MoeModel(vllm_config=vllm_config,
-+                                   prefix=maybe_prefix(prefix, "model"))
-+        self.lm_head = ParallelLMHead(config.vocab_size,
-+                                      config.hidden_size,
-+                                      quant_config=quant_config)
-+        if self.config.tie_word_embeddings:
-+            self.lm_head.weight = self.model.embed_tokens.weight
-+        self.logits_processor = LogitsProcessor(config.vocab_size)
-+        self.sampler = get_sampler()
-+        self.make_empty_intermediate_tensors = (
-+            self.model.make_empty_intermediate_tensors)
-+
-+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-+        return self.model.get_input_embeddings(input_ids)
-+
-+    def forward(
-+        self,
-+        input_ids: torch.Tensor,
-+        positions: torch.Tensor,
-+        kv_caches: List[torch.Tensor],
-+        attn_metadata: AttentionMetadata,
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+        inputs_embeds: Optional[torch.Tensor] = None,
-+    ) -> Union[torch.Tensor, IntermediateTensors]:
-+        hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors,
-+                                   inputs_embeds)
-+        return hidden_states
-+
-+    def compute_logits(
-+        self,
-+        hidden_states: torch.Tensor,
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[torch.Tensor]:
-+        logits = self.logits_processor(self.lm_head, hidden_states,
-+                                       sampling_metadata)
-+        return logits
-+
-+    def sample(
-+        self,
-+        logits: Optional[torch.Tensor],
-+        sampling_metadata: SamplingMetadata,
-+    ) -> Optional[SamplerOutput]:
-+        next_tokens = self.sampler(logits, sampling_metadata)
-+        return next_tokens
-+    
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
-+        loader = AutoWeightsLoader(
-+            self,
-+            skip_prefixes=(["rotary_emb.inv_freq"]),
-+        )
-+        return loader.load_weights(weights)
-+
-diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
-index 3b2a7069e..6af50ae2b 100644
---- a/vllm/model_executor/models/registry.py
-+++ b/vllm/model_executor/models/registry.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """
- Whenever you add an architecture to this page, please also update
-@@ -37,6 +38,7 @@ _TEXT_GENERATION_MODELS = {
-     "BaiChuanForCausalLM": ("baichuan", "BaiChuanForCausalLM"),
-     # baichuan-13b, lower case 'c' in the class name
-     "BaichuanForCausalLM": ("baichuan", "BaichuanForCausalLM"),
-+    "BaiChuanMoEForCausalLM": ("baichuan_moe", "BaiChuanMoEForCausalLM"), # baichuan-moe
-     "BloomForCausalLM": ("bloom", "BloomForCausalLM"),
-     # ChatGLMModel supports multimodal
-     "CohereForCausalLM": ("commandr", "CohereForCausalLM"),
-@@ -92,12 +94,15 @@ _TEXT_GENERATION_MODELS = {
-     # QWenLMHeadModel supports multimodal
-     "Qwen2ForCausalLM": ("qwen2", "Qwen2ForCausalLM"),
-     "Qwen2MoeForCausalLM": ("qwen2_moe", "Qwen2MoeForCausalLM"),
-+    "Qwen3ForCausalLM": ("qwen3", "Qwen3ForCausalLM"),
-+    "Qwen3MoeForCausalLM": ("qwen3_moe", "Qwen3MoeForCausalLM"),
-     "RWForCausalLM": ("falcon", "FalconForCausalLM"),
-     "StableLMEpochForCausalLM": ("stablelm", "StablelmForCausalLM"),
-     "StableLmForCausalLM": ("stablelm", "StablelmForCausalLM"),
-     "Starcoder2ForCausalLM": ("starcoder2", "Starcoder2ForCausalLM"),
-     "SolarForCausalLM": ("solar", "SolarForCausalLM"),
-     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
-+    "TelechatForCausalLM": ("telechat", "TelechatForCausalLM"),
-     "XverseForCausalLM": ("llama", "LlamaForCausalLM"),
-     # [Encoder-decoder]
-     "BartModel": ("bart", "BartForConditionalGeneration"),
-@@ -130,6 +135,7 @@ _EMBEDDING_MODELS = {
-     "Qwen2ForRewardModel": ("qwen2_rm", "Qwen2ForRewardModel"),
-     "Qwen2ForProcessRewardModel": ("qwen2_rm", "Qwen2ForProcessRewardModel"),
-     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
-+    "TelechatForCausalLM": ("telechat", "TelechatForCausalLM"),
-     # [Multimodal]
-     "LlavaNextForConditionalGeneration": ("llava_next", "LlavaNextForConditionalGeneration"),  # noqa: E501
-     "Phi3VForCausalLM": ("phi3v", "Phi3VForCausalLM"),
-diff --git a/vllm/model_executor/models/telechat.py b/vllm/model_executor/models/telechat.py
-new file mode 100644
-index 000000000..58fdcbd43
---- /dev/null
-+++ b/vllm/model_executor/models/telechat.py
-@@ -0,0 +1,139 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
-+# Copyright 2023 The vLLM team.
-+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
-+#
-+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
-+# and OPT implementations in this library. It has been modified from its
-+# original forms to accommodate minor architectural differences compared
-+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
-+#
-+# Licensed under the Apache License, Version 2.0 (the "License");
-+# you may not use this file except in compliance with the License.
-+# You may obtain a copy of the License at
-+#
-+#     http://www.apache.org/licenses/LICENSE-2.0
-+#
-+# Unless required by applicable law or agreed to in writing, software
-+# distributed under the License is distributed on an "AS IS" BASIS,
-+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-+# See the License for the specific language governing permissions and
-+# limitations under the License.
-+from typing import Iterable, Set, Tuple
-+
-+import torch
-+
-+from vllm.config import VllmConfig
-+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
-+from vllm.model_executor.models.llama import LlamaForCausalLM, LlamaModel
-+
-+from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
-+                    is_pp_missing_parameter)
++                                          quant_config=quant_config)
++        else:
++            self.lm_head = PPMissingLayer()
 +
++        self.logits_processor = LogitsProcessor(config.vocab_size)
++        self.make_empty_intermediate_tensors = (
++            self.model.make_empty_intermediate_tensors)
 +
-+class TeleChatModel(LlamaModel):
++    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
++        return self.model.get_input_embeddings(input_ids)
 +
-+    hf_to_vllm_mapper = WeightsMapper(
-+        orig_to_new_prefix={
-+            "transformer.": "model.",
-+        },
-+        # for different telechat model: telechat-12B & telechat-7B
-+        orig_to_new_substr={
-+            ".h.": ".layers.",
-+            "h.": "model.layers.",
-+            ".self_attention.": ".self_attn.",
-+            ".word_embeddings.": ".embed_tokens.",
-+            "word_embeddings.": "model.embed_tokens.",
-+            ".dense.": ".o_proj.",
-+            ".ln_f.": ".norm.",
-+            "ln_f.": "model.norm.",
-+        },
-+    )
++    def forward(
++        self,
++        input_ids: torch.Tensor,
++        positions: torch.Tensor,
++        intermediate_tensors: Optional[IntermediateTensors] = None,
++        inputs_embeds: Optional[torch.Tensor] = None,
++    ) -> Union[torch.Tensor, IntermediateTensors]:
++        hidden_states = self.model(input_ids, positions, intermediate_tensors,
++                                   inputs_embeds)
++        return hidden_states
 +
-+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-+        # 1. Initialize the LlamaModel with bias
-+        vllm_config.model_config.hf_config.bias = True
-+        vllm_config.model_config.hf_config.mlp_bias = True
-+        vllm_config.model_config.hf_config.num_key_value_heads = vllm_config.model_config.hf_config.num_attention_heads
-+        super().__init__(vllm_config=vllm_config, prefix=prefix)
-+        # 2. Remove the bias from the qkv_proj and gate_up_proj based on config
-+        # Telechat2's gate_up_proj and qkv_proj don't have bias
-+        # see: https://github.com/vllm-project/vllm/pull/10311#issuecomment-2490297566
-+        for layer in self.layers:
-+            if not isinstance(layer, PPMissingLayer):
-+                layer.self_attn.qkv_proj.bias = None
-+                layer.self_attn.qkv_proj.skip_bias_add = True
-+                layer.mlp.gate_up_proj.bias = None
-+                layer.mlp.gate_up_proj.skip_bias_add = True
++    def compute_logits(
++        self,
++        hidden_states: torch.Tensor,
++        sampling_metadata: SamplingMetadata,
++    ) -> Optional[torch.Tensor]:
++        logits = self.logits_processor(self.lm_head, hidden_states,
++                                       sampling_metadata)
++        return logits
 +
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
++    def load_weights(self, weights: Iterable[tuple[str,
++                                                   torch.Tensor]]) -> set[str]:
 +        stacked_params_mapping = [
-+            ('gate_up_proj', 'gate_proj', 0),
-+            ('gate_up_proj', 'up_proj', 1),
++            # (param_name, shard_name, shard_id)
++            ("qkv_proj", "q_proj", "q"),
++            ("qkv_proj", "k_proj", "k"),
++            ("qkv_proj", "v_proj", "v"),
++            ("gate_up_proj", "gate_proj", 0),
++            ("gate_up_proj", "up_proj", 1),
 +        ]
++
++        # Params for weights, fp8 weight scales, fp8 activation scales
++        # (param_name, weight_name, expert_id, shard_id)
++        expert_params_mapping = FusedMoE.make_expert_params_mapping(
++            ckpt_gate_proj_name="gate_proj",
++            ckpt_down_proj_name="down_proj",
++            ckpt_up_proj_name="up_proj",
++            num_experts=self.config.moe_num_experts)
++
 +        params_dict = dict(self.named_parameters())
-+        loaded_params: Set[str] = set()
-+        total_num_heads = self.config.n_head
-+        head_dim = self.config.hidden_size // total_num_heads
++        loaded_params: set[str] = set()
 +        for name, loaded_weight in weights:
-+            if "self_attn.key_value" in name:
-+                k_weight = []
-+                v_weight = []
-+                for i in range(total_num_heads):
-+                    start = i * head_dim * 2
-+                    k_weight.append(loaded_weight[start:start + head_dim, :])
-+                    v_weight.append(loaded_weight[start + head_dim:start +
-+                                                  2 * head_dim:])
-+                k_weight = torch.cat(k_weight, dim=0)
-+                v_weight = torch.cat(v_weight, dim=0)
-+                name = name.replace("key_value", "qkv_proj")
-+                if is_pp_missing_parameter(name, self):
++            if self.config.tie_word_embeddings and name.endswith(
++                    "lm_head.weight"):
++                continue
++            # MTP will be supported soon.
++            if "mtp" in name:
++                continue
++
++            for (param_name, weight_name, shard_id) in stacked_params_mapping:
++                # Skip non-stacked layers and experts (experts handled below).
++                if weight_name not in name:
 +                    continue
-+                param = params_dict[name]
-+                weight_loader = param.weight_loader
-+                weight_loader(param, k_weight, "k")
-+                weight_loader(param, v_weight, "v")
-+            elif "query" in name:
-+                name = name.replace("query", "qkv_proj")
++
++                if (("mlp.experts." in name) and name not in params_dict):
++                    continue
++                name = name.replace(weight_name, param_name)
++                # Skip loading extra bias for GPTQ models.
++                if ((name.endswith(".bias") or name.endswith("_bias"))
++                        and name not in params_dict):
++                    continue
++                # Skip layers on other devices.
 +                if is_pp_missing_parameter(name, self):
 +                    continue
++
 +                param = params_dict[name]
 +                weight_loader = param.weight_loader
-+                weight_loader(param, loaded_weight, "q")
++                weight_loader(param, loaded_weight, shard_id)
++                break
 +            else:
-+                for param_name, weight_name, shard_id in stacked_params_mapping:
++                for mapping in expert_params_mapping:
++                    param_name, weight_name, expert_id, shard_id = mapping
++
 +                    if weight_name not in name:
 +                        continue
++
 +                    name = name.replace(weight_name, param_name)
++                    # Skip layers on other devices.
 +                    if is_pp_missing_parameter(name, self):
 +                        continue
++
++                    # Skip loading extra bias for GPTQ models.
++                    if ((name.endswith(".bias") or name.endswith("_bias"))
++                            and name not in params_dict):
++                        continue
 +                    param = params_dict[name]
++
 +                    weight_loader = param.weight_loader
-+                    weight_loader(param, loaded_weight, shard_id)
++                    weight_loader(param,
++                                  loaded_weight,
++                                  name,
++                                  shard_id=shard_id,
++                                  expert_id=expert_id)
 +                    break
 +                else:
++                    # Skip loading extra bias for GPTQ models.
++                    if ((name.endswith(".bias") or name.endswith("_bias"))
++                            and name not in params_dict):
++                        continue
++                    # Skip layers on other devices.
 +                    if is_pp_missing_parameter(name, self):
 +                        continue
++                    # Remapping the name of FP8 kv-scale.
++                    name = maybe_remap_kv_scale_name(name, params_dict)
++                    if name is None:
++                        continue
++
 +                    param = params_dict[name]
 +                    weight_loader = getattr(param, "weight_loader",
 +                                            default_weight_loader)
 +                    weight_loader(param, loaded_weight)
 +            loaded_params.add(name)
 +        return loaded_params
+\ No newline at end of file
+diff --git a/vllm/model_executor/models/minicpmv.py b/vllm/model_executor/models/minicpmv.py
+index 4100fee0e..ca1df5c7d 100644
+--- a/vllm/model_executor/models/minicpmv.py
++++ b/vllm/model_executor/models/minicpmv.py
+@@ -38,6 +38,8 @@ from typing_extensions import TypeVar
+ 
+ from vllm.config import VllmConfig
+ from vllm.model_executor.layers.quantization import QuantizationConfig
++from vllm.model_executor.layers.quantization.awq import AWQConfig
++from vllm.model_executor.layers.quantization.awq_marlin import AWQMarlinConfig
+ from vllm.model_executor.layers.resampler import (BaseResampler, Resampler2,
+                                                   get_2d_sincos_pos_embed)
+ from vllm.model_executor.model_loader.utils import set_default_torch_dtype
+@@ -341,7 +343,9 @@ class MiniCPMVProcessingInfo(BaseProcessingInfo):
+ 
+     def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:
+         mm_limits = {"image": None}
+-        if self.get_model_version() == (2, 6):
++        if self.get_model_version() == (2,
++                                        6) or self.get_model_version() == (4,
++                                                                           0):
+             mm_limits["video"] = None
+ 
+         return mm_limits
+@@ -616,7 +620,8 @@ class MiniCPMVMultiModalProcessor(BaseMultiModalProcessor[_I]):
+         out_keys: set[str],
+     ) -> dict[str, NestedTensors]:
+         # This processor supports zipping prompt and mm_data together
+-        if self.info.get_model_version() == (2, 6):
++        if self.info.get_model_version() == (
++                2, 6) or self.info.get_model_version() == (4, 0):
+             inputs = super()._call_hf_processor(
+                 prompt=prompts,  # type: ignore
+                 mm_data=mm_data,
+@@ -671,10 +676,18 @@ class MiniCPMVMultiModalProcessor(BaseMultiModalProcessor[_I]):
+         hf_processor_mm_kwargs: Mapping[str, object],
+         out_mm_kwargs: MultiModalKwargs,
+     ) -> Sequence[PromptUpdate]:
+-        placeholder = {
+-            "image": self.info.image_pattern,
+-            "video": self.info.video_pattern,
+-        }
++        placeholders = [("image", self.info.image_pattern),
++                        ("video", self.info.video_pattern)]
++
++        # hard code for inconsistency of encode-decode image_pattern
++        additional_placeholders = []
++        tokenizer = self.info.get_tokenizer()
++        for modality, pattern in placeholders:
++            sub_pattern = tokenizer.decode(
++                tokenizer.encode(pattern, add_special_tokens=False))
++            if sub_pattern != pattern:
++                additional_placeholders.append((modality, sub_pattern))
++        placeholders += additional_placeholders
+ 
+         def get_image_replacement(item_idx: int):
+             images = mm_items.get_items(
+@@ -706,9 +719,9 @@ class MiniCPMVMultiModalProcessor(BaseMultiModalProcessor[_I]):
+ 
+         return [
+             PromptReplacement(modality=modality,
+-                              target=placeholder[modality],
++                              target=pattern,
+                               replacement=get_replacement[modality])
+-            for modality in ("image", "video")
++            for modality, pattern in placeholders
+         ]
+ 
+     def _get_mm_fields_config(
+@@ -1244,11 +1257,124 @@ class MiniCPMV2_6(MiniCPMVBaseModel, SupportsLoRA):
+ 
+         return self.resampler(vision_embedding, tgt_sizes)
+ 
++    def load_weights(self, weights: Iterable[tuple[str,
++                                                   torch.Tensor]]) -> set[str]:
++        loader = AutoWeightsLoader(self,
++                                   skip_prefixes=["apm.", "audio", "tts"])
++        return loader.load_weights(weights)
 +
 +
-+class TelechatForCausalLM(LlamaForCausalLM):
++class MiniCPMV4_0(MiniCPMVBaseModel, SupportsLoRA):
++    packed_modules_mapping = {
++        "qkv_proj": [
++            "q_proj",
++            "k_proj",
++            "v_proj",
++        ],
++        "gate_up_proj": [
++            "gate_proj",
++            "up_proj",
++        ],
++    }
++
++    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
++        super().__init__(vllm_config=vllm_config, prefix=prefix)
++        assert self.version == (4, 0)
 +
-+    def _init_model(self, vllm_config: VllmConfig, prefix: str = ""):
-+        return TeleChatModel(vllm_config=vllm_config, prefix=prefix)
++    def _maybe_ignore_quant_config(self, quant_config: QuantizationConfig):
++        if isinstance(quant_config, (AWQConfig, AWQMarlinConfig)):
++            return None
++        return quant_config
 +
-+    def load_weights(self, weights: Iterable[Tuple[str,
-+                                                   torch.Tensor]]) -> Set[str]:
++    def init_llm(
++        self,
++        vllm_config: VllmConfig,
++        prefix: str = "",
++    ) -> nn.Module:
++        return LlamaForCausalLM(vllm_config=vllm_config, prefix=prefix)
 +
-+        loader = AutoWeightsLoader(
-+            self,
-+            skip_prefixes=(["lm_head."]
-+                           if self.config.tie_word_embeddings else None),
++    def init_vision_module(
++        self,
++        config: PretrainedConfig,
++        quant_config: Optional[QuantizationConfig] = None,
++        prefix: str = "",
++    ) -> nn.Module:
++        quant_config = self._maybe_ignore_quant_config(quant_config)
++        model = Idefics2VisionTransformer(config.vision_config,
++                                          quant_config=quant_config,
++                                          prefix=prefix)
++        if self.config.drop_vision_last_layer:
++            model.encoder.layers = model.encoder.layers[:-1]
++        return model
++
++    def init_resampler(
++        self,
++        embed_dim: int,
++        vision_dim: int,
++        quant_config: Optional[QuantizationConfig] = None,
++        prefix: str = "",
++    ) -> nn.Module:
++        quant_config = self._maybe_ignore_quant_config(quant_config)
++        with set_default_torch_dtype(torch.float16):
++            # The resampler in 4.0 remains consistent with the one in 2.5/2.6.
++            resampler = Resampler2_5(num_queries=self.config.query_num,
++                                     embed_dim=embed_dim,
++                                     num_heads=embed_dim // 128,
++                                     kv_dim=vision_dim,
++                                     quant_config=quant_config,
++                                     prefix=prefix)
++
++        return resampler.to(device=current_platform.device_type,
++                            dtype=torch.get_default_dtype())
++
++    def get_vision_hidden_states(
++            self, data: MiniCPMVImagePixelInputs) -> torch.Tensor:
++        pixel_values = data["pixel_values"]
++        tgt_sizes = data["tgt_sizes"]
++
++        B = len(pixel_values)
++        P = pixel_values[0].shape[-2]
++        L = max(item.shape[-1] for item in pixel_values)
++        device = pixel_values[0].device
++        dtype = pixel_values[0].dtype
++
++        all_pixel_values = torch.zeros((B, 3, P, L),
++                                       dtype=dtype,
++                                       device=device)
++        for i, pixel_values_item in enumerate(pixel_values):
++            L_item = pixel_values_item.shape[-1]
++            all_pixel_values[i, ..., :L_item] = pixel_values_item
++
++        num_patches = tgt_sizes.prod(-1)
++        max_patches = num_patches.max().item()
++        assert isinstance(max_patches, int)
++
++        patch_attn_mask = torch.zeros((B, max_patches),
++                                      dtype=torch.bool,
++                                      device=device)
++        for i, num_patches_item in enumerate(num_patches):
++            patch_attn_mask[i, :num_patches_item] = True
++
++        vision_embedding = self.vpm(
++            all_pixel_values,
++            patch_attention_mask=patch_attn_mask.unsqueeze(1),
++            tgt_sizes=tgt_sizes,
 +        )
-+        return loader.load_weights(weights, mapper=self.model.hf_to_vllm_mapper)
-diff --git a/vllm/platforms/__init__.py b/vllm/platforms/__init__.py
-index e4767a378..750786b26 100644
---- a/vllm/platforms/__init__.py
-+++ b/vllm/platforms/__init__.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
++
++        return self.resampler(vision_embedding, tgt_sizes)
++
++    def load_weights(self, weights: Iterable[tuple[str,
++                                                   torch.Tensor]]) -> set[str]:
++        loader = AutoWeightsLoader(self,
++                                   skip_prefixes=["apm.", "audio", "tts"])
++        return loader.load_weights(weights)
++
  
- import logging
-@@ -33,11 +34,14 @@ def cuda_platform_plugin() -> Optional[str]:
-     is_cuda = False
+ _SUPPORT_VERSION = {
+     (2, 0): MiniCPMV2_0,
+     (2, 5): MiniCPMV2_5,
+     (2, 6): MiniCPMV2_6,
++    (4, 0): MiniCPMV4_0,
+ }
  
-     try:
--        from vllm.utils import import_pynvml
--        pynvml = import_pynvml()
-+        #from vllm.utils import import_pynvml
-+        #pynvml = import_pynvml()
-+        import vllm.platforms.pynvml as pynvml
-         pynvml.nvmlInit()
-         try:
--            if pynvml.nvmlDeviceGetCount() > 0:
-+            import torch
-+            #if pynvml.nvmlDeviceGetCount() > 0:
-+            if torch.cuda.device_count() > 0:
-                 is_cuda = True
-         finally:
-             pynvml.nvmlShutdown()
+ 
+@@ -1276,8 +1402,10 @@ class MiniCPMV(MiniCPMVBaseModel, SupportsMultiModal, SupportsLoRA):
+         # Dispatch class based on version
+         instance_cls = _SUPPORT_VERSION.get(version)
+         if instance_cls is None:
+-            raise ValueError(
+-                "Currently, MiniCPMV only supports versions 2.0, 2.5, and 2.6")
++            supported_versions = ", ".join(
++                [f"{v[0]}.{v[1]}" for v in sorted(_SUPPORT_VERSION.keys())])
++            raise ValueError(f"Currently, MiniCPMV only supports versions "
++                             f"{supported_versions}. Got version: {version}")
+ 
+         # quant_config references base class members,
+         # so update values before init is called
+diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
+index a4f8a361e..5414e2370 100644
+--- a/vllm/model_executor/models/qwen2_vl.py
++++ b/vllm/model_executor/models/qwen2_vl.py
+@@ -236,7 +236,7 @@ def apply_rotary_pos_emb_vision(t: torch.Tensor,
+     sin = freqs.sin()
+     apply_rotary_emb = apply_rotary_emb_torch
+     if current_platform.is_cuda():
+-        from vllm.vllm_flash_attn.layers.rotary import apply_rotary_emb
++        from flash_attn.layers.rotary import apply_rotary_emb
+     output = apply_rotary_emb(t_, cos, sin).type_as(t)
+     return output
+ 
+diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
+index e82e36638..6703c3539 100644
+--- a/vllm/model_executor/models/registry.py
++++ b/vllm/model_executor/models/registry.py
+@@ -51,6 +51,8 @@ _TEXT_GENERATION_MODELS = {
+     "DeepseekForCausalLM": ("deepseek", "DeepseekForCausalLM"),
+     "DeepseekV2ForCausalLM": ("deepseek_v2", "DeepseekV2ForCausalLM"),
+     "DeepseekV3ForCausalLM": ("deepseek_v2", "DeepseekV3ForCausalLM"),
++    "Ernie4_5_ForCausalLM": ("ernie45", "Ernie4_5_ForCausalLM"),
++    "Ernie4_5_MoeForCausalLM": ("ernie45_moe", "Ernie4_5_MoeForCausalLM"),
+     "ExaoneForCausalLM": ("exaone", "ExaoneForCausalLM"),
+     "FalconForCausalLM": ("falcon", "FalconForCausalLM"),
+     "Fairseq2LlamaForCausalLM": ("fairseq2_llama", "Fairseq2LlamaForCausalLM"),
 diff --git a/vllm/platforms/cuda.py b/vllm/platforms/cuda.py
-index 991d55ac8..b40acdec1 100644
+index 48d1aacba..dee2bb7a5 100644
 --- a/vllm/platforms/cuda.py
 +++ b/vllm/platforms/cuda.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Code inside this file can safely assume cuda platform, e.g. importing
- pynvml. However, it should not initialize cuda context.
-@@ -33,7 +34,7 @@ pynvml = import_pynvml()
- 
- # pytorch 2.5 uses cudnn sdpa by default, which will cause crash on some models
- # see https://github.com/huggingface/diffusers/issues/9704 for details
--torch.backends.cuda.enable_cudnn_sdp(False)
-+#torch.backends.cuda.enable_cudnn_sdp(False)
- 
- 
- def device_id_to_physical_device_id(device_id: int) -> int:
-@@ -204,9 +205,14 @@ class CudaPlatformBase(Platform):
+@@ -291,7 +291,7 @@ class CudaPlatformBase(Platform):
          # installed.
          if target_backend == _Backend.FLASH_ATTN:
              try:
 -                import vllm.vllm_flash_attn  # noqa: F401
--                from vllm.attention.backends.flash_attn import (  # noqa: F401
--                    FlashAttentionBackend)
 +                import flash_attn  # noqa: F401
-+                pg_method = os.getenv("MACA_VLLM_PG_OPT", None)
-+                if pg_method is None:
-+                    from vllm.attention.backends.flash_attn import (  # noqa: F401
-+                        FlashAttentionBackend)
-+                else:
-+                    from vllm.attention.backends.flash_attn_pg import (  # noqa: F401
-+                        FlashAttentionBackend)
- 
-                 supported_sizes = \
-                     FlashAttentionBackend.get_supported_head_sizes()
-@@ -228,7 +234,12 @@ class CudaPlatformBase(Platform):
-             return "vllm.attention.backends.xformers.XFormersBackend"
- 
-         logger.info("Using Flash Attention backend.")
--        return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
-+        pg_method = os.getenv("MACA_VLLM_PG_OPT", None)
-+        if pg_method is None:
-+            return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
-+        else:
-+            return "vllm.attention.backends.flash_attn_pg.FlashAttentionBackend"
-+        #return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
+                 from vllm.attention.backends.flash_attn import (  # noqa: F401
+                     FlashAttentionBackend, flash_attn_supports_fp8)
  
-     @classmethod
-     def get_punica_wrapper(cls) -> str:
-@@ -379,7 +390,8 @@ finally:
-     if nvml_available:
-         pynvml.nvmlShutdown()
- 
--CudaPlatform = NvmlCudaPlatform if nvml_available else NonNvmlCudaPlatform
-+#CudaPlatform = NvmlCudaPlatform if nvml_available else NonNvmlCudaPlatform
-+CudaPlatform = NonNvmlCudaPlatform
- 
- try:
-     from sphinx.ext.autodoc.mock import _MockModule
-diff --git a/vllm/platforms/pynvml.py b/vllm/platforms/pynvml.py
+diff --git a/vllm/third_party/pymcml.py b/vllm/third_party/pymcml.py
 new file mode 100644
-index 000000000..76aa09e97
+index 000000000..097007ea0
 --- /dev/null
-+++ b/vllm/platforms/pynvml.py
-@@ -0,0 +1,5440 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
++++ b/vllm/third_party/pymcml.py
+@@ -0,0 +1,5439 @@
 +# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 +#####
 +# Copyright (c) 2011-2023, NVIDIA Corporation.  All rights reserved.
@@ -33858,7 +22957,7 @@ index 000000000..76aa09e97
 +## Device get functions
 +def nvmlDeviceGetCount():
 +    c_count = c_uint()
-+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCount_v2")
++    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCount")
 +    ret = fn(byref(c_count))
 +    _nvmlCheckReturn(ret)
 +    return c_count.value
@@ -34385,9 +23484,9 @@ index 000000000..76aa09e97
 +def nvmlDeviceGetCudaComputeCapability(handle):
 +    c_major = c_int()
 +    c_minor = c_int()
-+    #fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCudaComputeCapability")
-+    #ret = fn(handle, byref(c_major), byref(c_minor))
-+    #_nvmlCheckReturn(ret)
++    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeCapability")
++    ret = fn(handle, byref(c_major), byref(c_minor))
++    _nvmlCheckReturn(ret)
 +    return (c_major.value, c_minor.value)
 +
 +def nvmlDeviceGetEccMode(handle):
@@ -36852,705 +25951,1865 @@ index 000000000..76aa09e97
 +MXSMLEX_GPU_FABRIC_STATE_IN_PROGRESS   = 2
 +MXSMLEX_GPU_FABRIC_STATE_COMPLETED     = 3
 +
-+class c_nvmlGpuFabricInfo_t(_PrintableStructure):
-+    _fields_ = [
-+        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
-+        ("status", _nvmlReturn_t),
-+        ("cliqueId", c_uint32),
-+        ("state", _nvmlGpuFabricState_t)
-+    ]
++class c_nvmlGpuFabricInfo_t(_PrintableStructure):
++    _fields_ = [
++        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
++        ("status", _nvmlReturn_t),
++        ("cliqueId", c_uint32),
++        ("state", _nvmlGpuFabricState_t)
++    ]
++
++nvmlGpuFabricInfo_v2 = 0x02000024
++
++class c_nvmlGpuFabricInfoV_t(_PrintableStructure):
++    _fields_ = [
++        ("version", c_uint),
++        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
++        ("status", _nvmlReturn_t),
++        ("cliqueId", c_uint32),
++        ("state", _nvmlGpuFabricState_t),
++        ("healthMask", c_uint32)
++    ]
++
++    def __init__(self):
++        super(c_nvmlGpuFabricInfoV_t, self).__init__(version=nvmlGpuFabricInfo_v2)
++
++def nvmlDeviceGetGpuFabricInfo(device, gpuFabricInfo):
++    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfo");
++    ret = fn(device, gpuFabricInfo)
++    _nvmlCheckReturn(ret)
++    return ret
++
++def nvmlDeviceGetGpuFabricInfoV(device, gpuFabricInfo):
++    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfoV");
++    ret = fn(device, gpuFabricInfo)
++    _nvmlCheckReturn(ret)
++    return ret
++
++######################
++## Enums/defines
++#### MXSMLEX GPU NVLINK BW MODE
++MXSMLEX_GPU_NVLINK_BW_MODE_FULL      = 0x0
++MXSMLEX_GPU_NVLINK_BW_MODE_OFF       = 0x1
++MXSMLEX_GPU_NVLINK_BW_MODE_MIN       = 0x2
++MXSMLEX_GPU_NVLINK_BW_MODE_HALF      = 0x3
++MXSMLEX_GPU_NVLINK_BW_MODE_3QUARTER  = 0x4
++MXSMLEX_GPU_NVLINK_BW_MODE_COUNT     = 0x5
++
++def nvmlSystemSetNvlinkBwMode(mode):
++    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetNvlinkBwMode")
++    ret = fn(mode)
++    _nvmlCheckReturn(ret)
++    return ret
++
++def nvmlSystemGetNvlinkBwMode():
++    mode = c_uint()
++    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetNvlinkBwMode")
++    ret = fn(byref(mode))
++    _nvmlCheckReturn(ret)
++    return mode.value
++
++_nvmlPowerScopeType_t = c_uint
++MXSMLEX_POWER_SCOPE_GPU     = 0
++MXSMLEX_POWER_SCOPE_MODULE  = 1
++MXSMLEX_POWER_SCOPE_MEMORY  = 2
++
++class c_nvmlPowerValue_v2_t(_PrintableStructure):
++    _fields_ = [
++        ('version', c_uint),
++        ('powerScope', _nvmlPowerScopeType_t),
++        ('powerValueMw', c_uint),
++    ]
++    _fmt_ = {'<default>': "%d B"}
++
++nvmlPowerValue_v2 = 0x0200000C
++
++def nvmlDeviceSetPowerManagementLimit_v2(device, powerScope, powerLimit, version=nvmlPowerValue_v2):
++    c_powerScope = _nvmlPowerScopeType_t(powerScope)
++    c_powerValue = c_nvmlPowerValue_v2_t()
++    c_powerValue.version = c_uint(version)
++    c_powerValue.powerScope = c_powerScope
++    c_powerValue.powerValueMw = c_uint(powerLimit)
++    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPowerManagementLimit_v2")
++    ret = fn(device, byref(c_powerValue))
++    return ret
++
++class c_nvmlEccSramErrorStatus_v1_t(_PrintableStructure):
++    _fields_ = [
++        ('version', c_uint),
++        ('aggregateUncParity', c_ulonglong),
++        ('aggregateUncSecDed', c_ulonglong),
++        ('aggregateCor', c_ulonglong),
++        ('volatileUncParity', c_ulonglong),
++        ('volatileUncSecDed', c_ulonglong),
++        ('volatileCor', c_ulonglong),
++        ('aggregateUncBucketL2', c_ulonglong),
++        ('aggregateUncBucketSm', c_ulonglong),
++        ('aggregateUncBucketPcie', c_ulonglong),
++        ('aggregateUncBucketMcu', c_ulonglong),
++        ('aggregateUncBucketOther', c_ulonglong),
++        ('bThresholdExceeded', c_uint)
++    ]
++
++    def __init__(self):
++        super(c_nvmlEccSramErrorStatus_v1_t, self).__init__(version=nvmlEccSramErrorStatus_v1)
++
++nvmlEccSramErrorStatus_v1 = 0x1000068
++
++def nvmlDeviceGetSramEccErrorStatus(device, status):
++    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSramEccErrorStatus")
++    ret = fn(device, status)
++    _nvmlCheckReturn(ret)
++    return ret
++
+diff --git a/vllm/utils.py b/vllm/utils.py
+index d8dd5f284..12590b707 100644
+--- a/vllm/utils.py
++++ b/vllm/utils.py
+@@ -1118,7 +1118,7 @@ def find_nccl_library() -> str:
+             so_file)
+     else:
+         if torch.version.cuda is not None:
+-            so_file = "libnccl.so.2"
++            so_file = "libmccl.so"
+         elif torch.version.hip is not None:
+             so_file = "librccl.so.1"
+         else:
+@@ -2698,7 +2698,7 @@ def import_pynvml():
+     After all the troubles, we decide to copy the official `pynvml`
+     module to our codebase, and use it directly.
+     """
+-    import vllm.third_party.pynvml as pynvml
++    import vllm.third_party.pymcml as pynvml
+     return pynvml
+ 
+ 
+diff --git a/vllm/v1/attention/backends/cpu_attn.py b/vllm/v1/attention/backends/cpu_attn.py
+index d7a580c28..1c4604cc2 100644
+--- a/vllm/v1/attention/backends/cpu_attn.py
++++ b/vllm/v1/attention/backends/cpu_attn.py
+@@ -7,7 +7,8 @@ from vllm.attention.backends.torch_sdpa import (TorchSDPABackendImpl,
+                                                 TorchSDPAMetadata)
+ from vllm.attention.backends.utils import CommonAttentionState
+ from vllm.attention.ops.ipex_attn import PagedAttention
+-from vllm.v1.attention.backends.utils import CommonAttentionMetadata
++from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
++                                              CommonAttentionMetadata)
+ from vllm.v1.core.sched.output import SchedulerOutput
+ from vllm.v1.kv_cache_interface import AttentionSpec
+ from vllm.v1.worker.block_table import BlockTable
+@@ -53,7 +54,7 @@ class TorchSDPABackend:
+         return False
+ 
+ 
+-class TorchSDPAMetadataBuilderV1:
++class TorchSDPAMetadataBuilderV1(AttentionMetadataBuilder[TorchSDPAMetadata]):
+ 
+     def __init__(self, runner: CPUModelRunner, kv_cache_spec: AttentionSpec,
+                  block_table: BlockTable) -> None:
+@@ -118,9 +119,12 @@ class TorchSDPAMetadataBuilderV1:
+ 
+         return True
+ 
+-    def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
+-              common_prefix_len: int,
++    def build(self, common_prefix_len: int,
+               common_attn_metadata: CommonAttentionMetadata):
++        num_reqs = common_attn_metadata.num_reqs
++        num_actual_tokens = common_attn_metadata.num_actual_tokens
++        max_query_len = common_attn_metadata.max_query_len
++
+         runner = self.runner
+         block_table = self.block_table
+         seq_lens_np = runner.seq_lens_np[:num_reqs]
+diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
+index 91a7c43cd..22d361c9b 100755
+--- a/vllm/v1/attention/backends/flash_attn.py
++++ b/vllm/v1/attention/backends/flash_attn.py
+@@ -2,7 +2,7 @@
+ # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ """Attention layer with FlashAttention."""
+ from dataclasses import dataclass
+-from typing import TYPE_CHECKING, Any, Optional
++from typing import TYPE_CHECKING, Any, ClassVar, Optional
+ 
+ import numpy as np
+ import torch
+@@ -13,29 +13,32 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+                                               is_quantized_kv_cache)
+ from vllm.attention.layer import Attention
+ from vllm.attention.ops.merge_attn_states import merge_attn_states
+-from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
+-                                           get_flash_attn_version)
++# from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
++#                                            get_flash_attn_version)
+ from vllm.config import VllmConfig, get_layers_from_vllm_config
+ from vllm.distributed.kv_transfer.kv_connector.utils import (
+     get_kv_connector_cache_layout)
+ from vllm.logger import init_logger
+ from vllm.platforms import current_platform
+ from vllm.utils import cdiv
+-from vllm.v1.attention.backends.utils import CommonAttentionMetadata
++from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
++                                              CommonAttentionMetadata)
+ from vllm.v1.kv_cache_interface import AttentionSpec
+ from vllm.v1.worker.block_table import BlockTable
+ 
+ if TYPE_CHECKING:
+-    from vllm.v1.core.sched.output import SchedulerOutput
+-    from vllm.v1.worker.gpu_input_batch import InputBatch
+     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
+ 
+ if current_platform.is_cuda():
+-    from vllm.vllm_flash_attn import (flash_attn_varlen_func,
+-                                      get_scheduler_metadata)
++    from flash_attn import (flash_attn_varlen_func, flash_attn_with_kvcache)
+ 
+ logger = init_logger(__name__)
+ 
++def flash_attn_supports_fp8() -> bool:
++    return False
++
++def get_flash_attn_version():
++    return None
+ 
+ class FlashAttentionBackend(AttentionBackend):
+ 
+@@ -43,7 +46,7 @@ class FlashAttentionBackend(AttentionBackend):
+ 
+     @staticmethod
+     def get_supported_head_sizes() -> list[int]:
+-        return [32, 64, 96, 128, 160, 192, 224, 256]
++        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
+ 
+     @staticmethod
+     def get_name() -> str:
+@@ -102,6 +105,22 @@ class FlashAttentionMetadata:
+     query_start_loc: torch.Tensor
+     max_seq_len: int
+     seq_lens: torch.Tensor
++
++    # For handling prefill decode split
++    num_decodes: int
++    num_decode_tokens: int
++    decode_query_start_loc: torch.Tensor
++    decode_max_seq_len: int
++    decode_seq_lens: torch.Tensor
++    decode_block_table: torch.Tensor
++
++    num_prefills: int
++    num_prefill_tokens: int
++    prefill_query_start_loc: torch.Tensor
++    prefill_max_seq_len: int
++    prefill_seq_lens: torch.Tensor
++    prefill_block_table: torch.Tensor
++
+     block_table: torch.Tensor
+     slot_mapping: torch.Tensor
+ 
+@@ -306,7 +325,9 @@ def _get_sliding_window_configs(
+     return sliding_window_configs
+ 
+ 
+-class FlashAttentionMetadataBuilder:
++class FlashAttentionMetadataBuilder(
++        AttentionMetadataBuilder[FlashAttentionMetadata]):
++    full_cudagraph_supported: ClassVar[bool] = True  # Decode-only
+ 
+     def __init__(self, runner: "GPUModelRunner", kv_cache_spec: AttentionSpec,
+                  block_table: BlockTable):
+@@ -325,9 +346,9 @@ class FlashAttentionMetadataBuilder:
+ 
+         self.aot_schedule = (get_flash_attn_version() == 3)
+         self.use_full_cuda_graph = compilation_config.full_cuda_graph
+-        if self.use_full_cuda_graph and not self.aot_schedule:
+-            raise ValueError("Full CUDA graph mode requires AOT scheduling, "
+-                             "which requires FlashAttention 3.")
++        # if self.use_full_cuda_graph and not self.aot_schedule:
++        #     raise ValueError("Full CUDA graph mode requires AOT scheduling, "
++        #                      "which requires FlashAttention 3.")
+         self.scheduler_metadata = torch.zeros(self.runner.max_num_reqs + 1,
+                                               dtype=torch.int32,
+                                               device=self.runner.device)
+@@ -338,11 +359,95 @@ class FlashAttentionMetadataBuilder:
+ 
+     def reorder_batch(self, input_batch: "InputBatch",
+                       scheduler_output: "SchedulerOutput") -> bool:
+-        return False
++        # We now want to reorder the batch so that the "decode" requests are and
++        # the front and the "prefill" requests are at the using the least amount
++        # swaps possible. (NOTE for now we loosely use "decode" to mean requests
++        # where attention is likely memory-bound and "prefill" to mean requests
++        # where attention is likely compute-bound, TODO(lucas): figure out a
++        # better naming here)
++        decodes = []
++        prefills = []
++        num_decode_tokens = 0
++        num_prefill_tokens = 0
++
++        for i, req_id in enumerate(input_batch.req_ids):
++            num_tokens = scheduler_output.num_scheduled_tokens[req_id]
++            # for now treat 1 scheduled token as "decode" even if its not,
++            # we should update this to something like < 8 in the future but
++            # currently the decode run only supports num_tokens = 1
++            if num_tokens == 1:
++                decodes.append(i)
++                num_decode_tokens += num_tokens
++            else:
++                prefills.append(i)
++                num_prefill_tokens += num_tokens
++
++        # We hope that this is fairly minimal since decodes
++        # should be around for a number of iterations so hopefully they are
++        # relatively stationary (and new request are generally appended to the
++        # persistent batch so already should be at the back)
++        # To achieve this we loop over the decodes in descending order and
++        # the prefills in ascending order. We swap decodes from the  "back"
++        # i.e. past where the last decode should be in the reodorered with
++        # prefills from the front of the batch.
++        # `decodes` and `prefills` are already in ascending order just based on
++        # the above loop
++        num_decodes = len(decodes)
++        num_prefills = len(prefills)
++        modified_batch = False
++
++        for i in range(1, min(num_decodes, num_prefills) + 1):
++            # If the decode is at the "back" of the batch, i, we can swap it
++            # with the prefill closest to the front of the batch
++            decode_idx = decodes[num_decodes - i]
++            if decode_idx < num_decodes:
++                break
++
++            input_batch.swap_states(prefills[i - 1], decode_idx)
++            modified_batch = True
++
++        # Save for next `build` call
++        # TODO(lucas): this is a bit of a hack, we should probably have a
++        # better way of doing this
++        self._num_decodes = num_decodes
++        self._num_prefills = num_prefills
++        self._num_decode_tokens = num_decode_tokens
++        self._num_prefill_tokens = num_prefill_tokens
 +
-+nvmlGpuFabricInfo_v2 = 0x02000024
++        return modified_batch
 +
-+class c_nvmlGpuFabricInfoV_t(_PrintableStructure):
-+    _fields_ = [
-+        ("version", c_uint),
-+        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
-+        ("status", _nvmlReturn_t),
-+        ("cliqueId", c_uint32),
-+        ("state", _nvmlGpuFabricState_t),
-+        ("healthMask", c_uint32)
-+    ]
++    def build_for_cudagraph_capture(
++            self, common_attn_metadata: CommonAttentionMetadata) -> FlashAttentionMetadata:
++        """
++        This method builds the metadata for full cudagraph capture.
++        Currently, only decode is supported for full cudagraphs with MLA.
++        """
++        m = common_attn_metadata
++        assert m.num_reqs == m.num_actual_tokens, \
++            "MLA only supports decode-only full CUDAGraph capture. " \
++            "Make sure all cudagraph capture sizes <= max_num_seq."
++
++        m.max_query_len = 1  # decode-only
++
++        # Update state usually set in reorder_batch.
++        self._num_decodes = m.num_reqs
++        self._num_decode_tokens = m.num_actual_tokens
++        self._num_prefills = 0
++        self._num_prefill_tokens = 0
++        return self.build(0, m)
++
++    def build(
++        self, common_prefix_len: int,
++        common_attn_metadata: CommonAttentionMetadata
++    ) -> FlashAttentionMetadata:
++        num_reqs = common_attn_metadata.num_reqs
++        num_actual_tokens = common_attn_metadata.num_actual_tokens
++        max_query_len = common_attn_metadata.max_query_len
++
++        assert self._num_decodes + self._num_prefills == num_reqs
++        assert (self._num_decode_tokens +
++                self._num_prefill_tokens == num_actual_tokens)
+ 
+-    def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
+-              common_prefix_len: int,
+-              common_attn_metadata: CommonAttentionMetadata):
+         max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())
+         query_start_loc = common_attn_metadata.query_start_loc
+         seq_lens = common_attn_metadata.seq_lens
+@@ -358,6 +463,30 @@ class FlashAttentionMetadataBuilder:
+ 
+         slot_mapping = block_table.slot_mapping[:num_actual_tokens]
+ 
++        # For handling prefill decode split
++        if self._num_decodes > 0:
++            decode_max_seq_len = int(self.runner.seq_lens_np[:self._num_decodes].max())
++            decode_query_start_loc = common_attn_metadata.query_start_loc[:self._num_decodes + 1]
++            decode_seq_lens = common_attn_metadata.seq_lens[:self._num_decodes]
++            decode_block_table_tensor = block_table.get_device_tensor()[:self._num_decodes]
++        else:
++            decode_max_seq_len = 0
++            decode_query_start_loc = None
++            decode_seq_lens = None
++            decode_block_table_tensor = None
++
++        if self._num_prefills > 0:
++            prefill_max_seq_len = int(self.runner.seq_lens_np[self._num_decodes:num_reqs].max())
++            prefill_query_start_loc = (common_attn_metadata.query_start_loc[self._num_decodes:num_reqs + 1] -
++                                       common_attn_metadata.query_start_loc[self._num_decodes])
++            prefill_seq_lens = common_attn_metadata.seq_lens[self._num_decodes:num_reqs]
++            prefill_block_table_tensor = block_table.get_device_tensor()[self._num_decodes:num_reqs]
++        else:
++            prefill_max_seq_len = 0
++            prefill_query_start_loc = None
++            prefill_seq_lens = None
++            prefill_block_table_tensor = None
++        
+         if self.aot_sliding_window is None:
+             self.aot_sliding_window = (-1, -1)
+             # For the AOT scheduler we need the sliding window value to be
+@@ -376,20 +505,20 @@ class FlashAttentionMetadataBuilder:
+ 
+         def schedule(batch_size, cu_query_lens, max_query_len, seqlens,
+                      max_seq_len, causal):
+-            if self.aot_schedule:
+-                return get_scheduler_metadata(
+-                    batch_size=batch_size,
+-                    max_seqlen_q=max_query_len,
+-                    max_seqlen_k=max_seq_len,
+-                    cache_seqlens=seqlens,
+-                    num_heads_q=self.num_heads_q,
+-                    num_heads_kv=self.num_heads_kv,
+-                    headdim=self.headdim,
+-                    page_size=self.block_size,
+-                    cu_seqlens_q=cu_query_lens,
+-                    causal=causal,
+-                    window_size=self.aot_sliding_window,
+-                )
++            # if self.aot_schedule:
++            #     return get_scheduler_metadata(
++            #         batch_size=batch_size,
++            #         max_seqlen_q=max_query_len,
++            #         max_seqlen_k=max_seq_len,
++            #         cache_seqlens=seqlens,
++            #         num_heads_q=self.num_heads_q,
++            #         num_heads_kv=self.num_heads_kv,
++            #         headdim=self.headdim,
++            #         page_size=self.block_size,
++            #         cu_seqlens_q=cu_query_lens,
++            #         causal=causal,
++            #         window_size=self.aot_sliding_window,
++            #     )
+             return None
+ 
+         # for local attention
+@@ -465,17 +594,17 @@ class FlashAttentionMetadataBuilder:
+                                           max_seq_len=max_seq_len,
+                                           causal=True)
+ 
+-        if self.use_full_cuda_graph:
+-            assert scheduler_metadata is not None
+-            n = scheduler_metadata.shape[0]
+-            self.scheduler_metadata[:n].copy_(scheduler_metadata,
+-                                              non_blocking=True)
+-            # NOTE(woosuk): We should zero out the rest of the scheduler
+-            # metadata to guarantee the correctness. Otherwise, some thread
+-            # blocks may use the invalid scheduler metadata and overwrite the
+-            # output buffer.
+-            self.scheduler_metadata[n:] = 0
+-            scheduler_metadata = self.scheduler_metadata[:n]
++        # if self.use_full_cuda_graph:
++        #     assert scheduler_metadata is not None
++        #     n = scheduler_metadata.shape[0]
++        #     self.scheduler_metadata[:n].copy_(scheduler_metadata,
++        #                                       non_blocking=True)
++        #     # NOTE(woosuk): We should zero out the rest of the scheduler
++        #     # metadata to guarantee the correctness. Otherwise, some thread
++        #     # blocks may use the invalid scheduler metadata and overwrite the
++        #     # output buffer.
++        #     self.scheduler_metadata[n:] = 0
++        #     scheduler_metadata = self.scheduler_metadata[:n]
+ 
+         attn_metadata = FlashAttentionMetadata(
+             num_actual_tokens=num_actual_tokens,
+@@ -483,6 +612,19 @@ class FlashAttentionMetadataBuilder:
+             query_start_loc=query_start_loc,
+             max_seq_len=max_seq_len,
+             seq_lens=seq_lens,
++            # For handling prefill decode split
++            num_decodes=self._num_decodes,
++            num_decode_tokens=self._num_decode_tokens,
++            decode_query_start_loc=decode_query_start_loc,
++            decode_max_seq_len=decode_max_seq_len,
++            decode_seq_lens=decode_seq_lens,
++            decode_block_table=decode_block_table_tensor,
++            num_prefills=self._num_prefills,
++            num_prefill_tokens=self._num_prefill_tokens,
++            prefill_query_start_loc=prefill_query_start_loc,
++            prefill_max_seq_len=prefill_max_seq_len,
++            prefill_seq_lens=prefill_seq_lens,
++            prefill_block_table=prefill_block_table_tensor,
+             block_table=block_table_tensor,
+             slot_mapping=slot_mapping,
+             use_cascade=use_cascade,
+@@ -496,6 +638,10 @@ class FlashAttentionMetadataBuilder:
+         )
+         return attn_metadata
+ 
++    def can_run_in_cudagraph(
++            self, common_attn_metadata: CommonAttentionMetadata) -> bool:
++        return common_attn_metadata.max_query_len == 1
 +
-+    def __init__(self):
-+        super(c_nvmlGpuFabricInfoV_t, self).__init__(version=nvmlGpuFabricInfo_v2)
+     def use_cascade_attention(self, *args, **kwargs) -> bool:
+         return use_cascade_attention(*args, **kwargs)
+ 
+@@ -634,6 +780,43 @@ class FlashAttentionImpl(AttentionImpl):
+         # Compute attention and update output up to `num_actual_tokens`.
+         use_local_attn = \
+             (self.use_irope and attn_metadata.local_attn_metadata is not None)
++        
++        # For handling prefill decode split
++        if not attn_metadata.use_cascade and not use_local_attn:
++            num_decode_tokens = attn_metadata.num_decode_tokens
++            if attn_metadata.num_prefills > 0:
++                cu_prefix_kv_lens = torch.tensor([0] + attn_metadata.prefill_seq_lens.tolist(), device=attn_metadata.prefill_seq_lens.device, dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
++                output[num_decode_tokens:num_actual_tokens] = flash_attn_varlen_func(
++                    q=query[num_decode_tokens:num_actual_tokens],
++                    k=key_cache,
++                    v=value_cache,
++                    block_table=attn_metadata.prefill_block_table,
++                    cu_seqlens_q=attn_metadata.prefill_query_start_loc,
++                    cu_seqlens_k=cu_prefix_kv_lens,
++                    max_seqlen_q=attn_metadata.max_query_len,
++                    max_seqlen_k=attn_metadata.prefill_max_seq_len,
++                    softmax_scale=self.scale,
++                    causal=True,
++                    window_size=self.sliding_window,
++                    alibi_slopes=self.alibi_slopes,
++                    softcap=self.logits_soft_cap,
++                )
++            if attn_metadata.num_decodes > 0:
++                # Use flash_attn_with_kvcache for normal decoding.
++                decode_query = query[:num_decode_tokens]
++                output[:num_decode_tokens] = flash_attn_with_kvcache(
++                    q=decode_query.unsqueeze(1),
++                    k_cache=key_cache,
++                    v_cache=value_cache,
++                    block_table=attn_metadata.decode_block_table,
++                    cache_seqlens=attn_metadata.decode_seq_lens,
++                    softmax_scale=self.scale,
++                    causal=True,
++                    window_size=self.sliding_window,
++                    alibi_slopes=self.alibi_slopes,
++                    softcap=self.logits_soft_cap,
++                ).squeeze(1)
++            return output
+ 
+         if not attn_metadata.use_cascade or use_local_attn:
+             if use_local_attn:
+@@ -653,16 +836,18 @@ class FlashAttentionImpl(AttentionImpl):
+                 block_table = attn_metadata.block_table
+                 scheduler_metadata = attn_metadata.scheduler_metadata
+ 
+-            descale_shape = (cu_seqlens_q.shape[0] - 1, key.shape[1])
+-
+-            flash_attn_varlen_func(
++            # descale_shape = (cu_seqlens_q.shape[0] - 1, key.shape[1])
++            cu_prefix_kv_lens = torch.tensor([0] + seqused_k.tolist(), seqused_k.device, 
++                                             dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
++            output[:num_actual_tokens] = flash_attn_varlen_func(
+                 q=query[:num_actual_tokens],
+                 k=key_cache,
+                 v=value_cache,
+-                out=output[:num_actual_tokens],
++                # out=output[:num_actual_tokens],
+                 cu_seqlens_q=cu_seqlens_q,
+                 max_seqlen_q=max_seqlen_q,
+-                seqused_k=seqused_k,
++                # seqused_k=seqused_k,
++                cu_seqlens_k=cu_prefix_kv_lens,
+                 max_seqlen_k=max_seqlen_k,
+                 softmax_scale=self.scale,
+                 causal=True,
+@@ -670,11 +855,11 @@ class FlashAttentionImpl(AttentionImpl):
+                 window_size=self.sliding_window,
+                 block_table=block_table,
+                 softcap=self.logits_soft_cap,
+-                scheduler_metadata=scheduler_metadata,
+-                fa_version=self.vllm_flash_attn_version,
+-                q_descale=layer._q_scale.expand(descale_shape),
+-                k_descale=layer._k_scale.expand(descale_shape),
+-                v_descale=layer._v_scale.expand(descale_shape),
++                # scheduler_metadata=scheduler_metadata,
++                # fa_version=self.vllm_flash_attn_version,
++                # q_descale=layer._q_scale.expand(descale_shape),
++                # k_descale=layer._k_scale.expand(descale_shape),
++                # v_descale=layer._v_scale.expand(descale_shape),
+             )
+             return output
+ 
+@@ -813,12 +998,15 @@ def cascade_attention(
+     descale_shape = (cu_prefix_query_lens.shape[0] - 1, key_cache.shape[-2])
+ 
+     # Process shared prefix.
++    cu_prefix_kv_lens = torch.tensor([0] + prefix_kv_lens.tolist(), 
++                                     device=prefix_kv_lens.device, 
++                                     dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
+     prefix_output, prefix_lse = flash_attn_varlen_func(
+         q=query,
+         k=key_cache,
+         v=value_cache,
+         cu_seqlens_q=cu_prefix_query_lens,
+-        seqused_k=prefix_kv_lens,
++        cu_seqlens_k=cu_prefix_kv_lens,
+         max_seqlen_q=num_tokens,
+         max_seqlen_k=common_prefix_len,
+         softmax_scale=softmax_scale,
+@@ -827,25 +1015,28 @@ def cascade_attention(
+         block_table=block_table[:1],
+         softcap=logits_soft_cap,
+         return_softmax_lse=True,
+-        scheduler_metadata=prefix_scheduler_metadata,
+-        fa_version=fa_version,
+-        q_descale=q_descale.expand(descale_shape)
+-        if q_descale is not None else None,
+-        k_descale=k_descale.expand(descale_shape)
+-        if k_descale is not None else None,
+-        v_descale=v_descale.expand(descale_shape)
+-        if v_descale is not None else None,
++        # scheduler_metadata=prefix_scheduler_metadata,
++        # fa_version=fa_version,
++        # q_descale=q_descale.expand(descale_shape)
++        # if q_descale is not None else None,
++        # k_descale=k_descale.expand(descale_shape)
++        # if k_descale is not None else None,
++        # v_descale=v_descale.expand(descale_shape)
++        # if v_descale is not None else None,
+     )
+ 
+     descale_shape = (cu_query_lens.shape[0] - 1, key_cache.shape[-2])
+ 
+     # Process suffix per query.
++    cu_suffix_kv_lens = torch.tensor([0] + suffix_kv_lens.tolist(), 
++                                     device=suffix_kv_lens.device, 
++                                     dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
+     suffix_output, suffix_lse = flash_attn_varlen_func(
+         q=query,
+         k=key_cache,
+         v=value_cache,
+         cu_seqlens_q=cu_query_lens,
+-        seqused_k=suffix_kv_lens,
++        cu_seqlens_k=cu_suffix_kv_lens,
+         max_seqlen_q=max_query_len,
+         max_seqlen_k=max_kv_len - common_prefix_len,
+         softmax_scale=softmax_scale,
+@@ -854,14 +1045,14 @@ def cascade_attention(
+         block_table=block_table[:, num_common_kv_blocks:],
+         softcap=logits_soft_cap,
+         return_softmax_lse=True,
+-        scheduler_metadata=suffix_scheduler_metadata,
+-        fa_version=fa_version,
+-        q_descale=q_descale.expand(descale_shape)
+-        if q_descale is not None else None,
+-        k_descale=k_descale.expand(descale_shape)
+-        if k_descale is not None else None,
+-        v_descale=v_descale.expand(descale_shape)
+-        if v_descale is not None else None,
++        # scheduler_metadata=suffix_scheduler_metadata,
++        # fa_version=fa_version,
++        # q_descale=q_descale.expand(descale_shape)
++        # if q_descale is not None else None,
++        # k_descale=k_descale.expand(descale_shape)
++        # if k_descale is not None else None,
++        # v_descale=v_descale.expand(descale_shape)
++        # if v_descale is not None else None,
+     )
+ 
+     # Merge prefix and suffix outputs, and store the result in output.
+diff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py
+index b15bb4b31..f8455b5dc 100755
+--- a/vllm/v1/attention/backends/flashinfer.py
++++ b/vllm/v1/attention/backends/flashinfer.py
+@@ -8,8 +8,8 @@ from typing import TYPE_CHECKING, Any, Optional
+ 
+ import torch
+ from flashinfer import (BatchDecodeWithPagedKVCacheWrapper,
+-                        BatchPrefillWithPagedKVCacheWrapper,
+-                        MultiLevelCascadeAttentionWrapper)
++                        BatchPrefillWithPagedKVCacheWrapper)
++                        #MultiLevelCascadeAttentionWrapper
+ 
+ import vllm.envs as envs
+ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+@@ -18,7 +18,8 @@ from vllm.attention.layer import Attention
+ from vllm.config import VllmConfig, get_layers_from_vllm_config
+ from vllm.logger import init_logger
+ from vllm.v1.attention.backends.flash_attn import use_cascade_attention
+-from vllm.v1.attention.backends.utils import CommonAttentionMetadata
++from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
++                                              CommonAttentionMetadata)
+ from vllm.v1.kv_cache_interface import AttentionSpec
+ from vllm.v1.worker.block_table import BlockTable
+ 
+@@ -184,7 +185,7 @@ class FlashInferMetadata:
+ 
+     prefill_wrapper: Optional[BatchPrefillWithPagedKVCacheWrapper] = None
+     decode_wrapper: Optional[BatchDecodeWithPagedKVCacheWrapper] = None
+-    cascade_wrapper: Optional[MultiLevelCascadeAttentionWrapper] = None
++    # cascade_wrapper: Optional[MultiLevelCascadeAttentionWrapper] = None
+ 
+     @property
+     def query_start_loc(self):
+@@ -202,7 +203,7 @@ class FlashInferMetadata:
+                 f" received {self.head_dim}.")
+ 
+ 
+-class FlashInferMetadataBuilder:
++class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):
+ 
+     def __init__(self, runner: GPUModelRunner, kv_cache_spec: AttentionSpec,
+                  block_table: BlockTable):
+@@ -306,17 +307,17 @@ class FlashInferMetadataBuilder:
+                 use_tensor_cores=use_tensor_cores)
+         return self._decode_wrapper
+ 
+-    def _get_cascade_wrapper(self):
+-        if self._cascade_wrapper is None:
+-            self._cascade_wrapper = MultiLevelCascadeAttentionWrapper(
+-                2, self._get_workspace_buffer(), "NHD")
+-        return self._cascade_wrapper
++    # def _get_cascade_wrapper(self):
++    #     if self._cascade_wrapper is None:
++    #         self._cascade_wrapper = MultiLevelCascadeAttentionWrapper(
++    #             2, self._get_workspace_buffer(), "NHD")
++    #     return self._cascade_wrapper
+ 
+     def _plan(self, attn_metadata: FlashInferMetadata):
+         if self.global_hyperparameters is None:
+             self.global_hyperparameters = infer_global_hyperparameters(
+                 get_per_layer_parameters(self.vllm_config))
+-        if attn_metadata.use_cascade:
++        if attn_metadata.use_cascade and False: # not supported
+             attn_metadata.cascade_wrapper = self._get_cascade_wrapper()
+             attn_metadata.cascade_wrapper.plan(
+                 [attn_metadata.shared_qo_indptr, attn_metadata.qo_indptr],
+@@ -399,9 +400,11 @@ class FlashInferMetadataBuilder:
+                     kv_data_type=attn_metadata.data_type,
+                 )
+ 
+-    def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
+-              common_prefix_len: int,
++    def build(self, common_prefix_len: int,
+               common_attn_metadata: CommonAttentionMetadata):
++        num_reqs = common_attn_metadata.num_reqs
++        num_actual_tokens = common_attn_metadata.num_actual_tokens
++
+         assert self._num_decodes + self._num_prefills == num_reqs
+         assert (self._num_decode_tokens +
+                 self._num_prefill_tokens == num_actual_tokens)
+@@ -486,6 +489,9 @@ class FlashInferMetadataBuilder:
+         return attn_metadata
+ 
+     def use_cascade_attention(self, *args, **kwargs) -> bool:
++        logger.warning_once(
++                "Using cascade attention in FlashInfer is not supported yet")
++        return False
+         if self.kv_cache_spec.dtype != self.runner.model_config.dtype:
+             # TODO: The cascade wrapper currently does not support setting
+             # kv cache dtype to something different from query dtype.
+@@ -603,11 +609,11 @@ class FlashInferImpl(AttentionImpl):
+         output_padded = output
+         output = output[:num_actual_tokens]
+ 
+-        if attn_metadata.use_cascade:
+-            # Cascade attention (rare case).
+-            assert attn_metadata.cascade_wrapper is not None
+-            output.copy_(attn_metadata.cascade_wrapper.run(query, kv_cache))
+-            return output
++        # if attn_metadata.use_cascade:
++        #     # Cascade attention (rare case).
++        #     assert attn_metadata.cascade_wrapper is not None
++        #     output.copy_(attn_metadata.cascade_wrapper.run(query, kv_cache))
++        #     return output
+ 
+         num_decode_tokens = attn_metadata.num_decode_tokens
+         num_prefill_tokens = attn_metadata.num_prefill_tokens
+diff --git a/vllm/v1/attention/backends/flex_attention.py b/vllm/v1/attention/backends/flex_attention.py
+index 5b473b146..c3efb938e 100644
+--- a/vllm/v1/attention/backends/flex_attention.py
++++ b/vllm/v1/attention/backends/flex_attention.py
+@@ -15,7 +15,8 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+                                               is_quantized_kv_cache)
+ from vllm.logger import init_logger
+ from vllm.platforms import current_platform
+-from vllm.v1.attention.backends.utils import CommonAttentionMetadata
++from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
++                                              CommonAttentionMetadata)
+ from vllm.v1.kv_cache_interface import AttentionSpec
+ from vllm.v1.worker.block_table import BlockTable
+ 
+@@ -25,8 +26,6 @@ if current_platform.is_cuda():
+ logger = init_logger(__name__)
+ 
+ if TYPE_CHECKING:
+-    from vllm.v1.core.sched.output import SchedulerOutput
+-    from vllm.v1.worker.gpu_input_batch import InputBatch
+     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
+ 
+ create_block_mask_compiled = torch.compile(create_block_mask,
+@@ -256,7 +255,8 @@ class FlexAttentionMetadata:
+         self.block_mask = self.build_block_mask()
+ 
+ 
+-class FlexAttentionMetadataBuilder:
++class FlexAttentionMetadataBuilder(
++        AttentionMetadataBuilder[FlexAttentionMetadata]):
+ 
+     def __init__(self, runner: "GPUModelRunner", kv_cache_spec: AttentionSpec,
+                  block_table: BlockTable):
+@@ -272,13 +272,12 @@ class FlexAttentionMetadataBuilder:
+         self.kv_cache_spec = kv_cache_spec
+         self.block_table = block_table
+ 
+-    def reorder_batch(self, input_batch: "InputBatch",
+-                      scheduler_output: "SchedulerOutput") -> bool:
+-        return False
+-
+-    def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
+-              common_prefix_len: int,
++    def build(self, common_prefix_len: int,
+               common_attn_metadata: CommonAttentionMetadata):
++        num_reqs = common_attn_metadata.num_reqs
++        num_actual_tokens = common_attn_metadata.num_actual_tokens
++        max_query_len = common_attn_metadata.max_query_len
++
+         max_seq_len = self.runner.seq_lens_np[:num_reqs].max()
+         query_start_loc = common_attn_metadata.query_start_loc
+         seq_lens = common_attn_metadata.seq_lens
+@@ -332,9 +331,6 @@ class FlexAttentionMetadataBuilder:
+         )
+         return out
+ 
+-    def use_cascade_attention(self, *args, **kwargs) -> bool:
+-        return False
+-
+ 
+ class FlexAttentionImpl(AttentionImpl):
+     sliding_window: Optional[tuple[int, int]]
+diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py
+index e6b4f6404..6f28c0f47 100644
+--- a/vllm/v1/attention/backends/mla/common.py
++++ b/vllm/v1/attention/backends/mla/common.py
+@@ -200,14 +200,15 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionLayer,
+                                               MLAAttentionImpl)
+ from vllm.attention.backends.utils import get_mla_dims
+ from vllm.attention.ops.merge_attn_states import merge_attn_states
+-from vllm.attention.utils.fa_utils import get_flash_attn_version
++# from vllm.attention.utils.fa_utils import get_flash_attn_version
+ from vllm.logger import init_logger
+ from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                                LinearBase,
+                                                UnquantizedLinearMethod)
+ from vllm.platforms import current_platform
+ from vllm.utils import cdiv, round_down
+-from vllm.v1.attention.backends.utils import CommonAttentionMetadata
++from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
++                                              CommonAttentionMetadata)
+ from vllm.v1.kv_cache_interface import AttentionSpec
+ from vllm.v1.worker.block_table import BlockTable
+ 
+@@ -223,9 +224,13 @@ if TYPE_CHECKING:
+     from vllm.v1.core.sched.output import SchedulerOutput
+     from vllm.v1.worker.gpu_input_batch import InputBatch
+     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
++    
++from vllm import envs
+ 
+ logger = init_logger(__name__)
+ 
++def get_flash_attn_version():
++    return None
+ 
+ class MLACommonBackend(AttentionBackend):
+ 
+@@ -329,7 +334,7 @@ class MLACommonMetadata(Generic[D]):
+ M = TypeVar("M", bound=MLACommonMetadata)
+ 
+ 
+-class MLACommonMetadataBuilder(Generic[M]):
++class MLACommonMetadataBuilder(AttentionMetadataBuilder[M]):
+     """
+     NOTE: Please read the comment at the top of the file before trying to
+     understand this class
+@@ -350,7 +355,7 @@ class MLACommonMetadataBuilder(Generic[M]):
+         self.num_heads = model_config.get_num_attention_heads(
+             runner.parallel_config)
+         self.mla_dims = get_mla_dims(model_config)
+-        self.aot_schedule = current_platform.is_cuda()
++        self.aot_schedule = False # current_platform.is_cuda()
+         self.kv_cache_spec = kv_cache_spec
+ 
+         # Dont try to access the runner on AMD
+@@ -450,9 +455,32 @@ class MLACommonMetadataBuilder(Generic[M]):
+             seq_lens=seq_lens,
+         )
+ 
+-    def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
+-              common_prefix_len: int,
++    def build_for_cudagraph_capture(
++            self, common_attn_metadata: CommonAttentionMetadata) -> M:
++        """
++        This method builds the metadata for full cudagraph capture.
++        Currently, only decode is supported for full cudagraphs with MLA.
++        """
++        m = common_attn_metadata
++        assert m.num_reqs == m.num_actual_tokens, \
++            "MLA only supports decode-only full CUDAGraph capture. " \
++            "Make sure all cudagraph capture sizes <= max_num_seq."
++
++        m.max_query_len = 1  # decode-only
++
++        # Update state usually set in reorder_batch.
++        self._num_decodes = m.num_reqs
++        self._num_decode_tokens = m.num_actual_tokens
++        self._num_prefills = 0
++        self._num_prefill_tokens = 0
++        return self.build(0, m)
++
++    def build(self, common_prefix_len: int,
+               common_attn_metadata: CommonAttentionMetadata) -> M:
++        num_reqs = common_attn_metadata.num_reqs
++        num_actual_tokens = common_attn_metadata.num_actual_tokens
++        max_query_len = common_attn_metadata.max_query_len
++
+         assert self._num_decodes + self._num_prefills == num_reqs
+ 
+         # Note(simon): be careful about the CPU <> GPU memory movement in this
+@@ -461,8 +489,11 @@ class MLACommonMetadataBuilder(Generic[M]):
+         device = self.runner.device
+         block_table = self.block_table
+         block_table_tensor = block_table.get_device_tensor()[:num_reqs]
+-        slot_mapping = block_table.slot_mapping_cpu[:num_actual_tokens].to(
+-            device, non_blocking=True).long()
++        block_table.slot_mapping[:num_actual_tokens].copy_(
++            block_table.slot_mapping_cpu[:num_actual_tokens],
++            non_blocking=True)
++        block_table.slot_mapping[num_actual_tokens:].fill_(-1)
++        slot_mapping = block_table.slot_mapping[:num_actual_tokens]
+ 
+         query_start_loc = common_attn_metadata.query_start_loc
+         seq_lens = common_attn_metadata.seq_lens
+@@ -564,8 +595,9 @@ class MLACommonMetadataBuilder(Generic[M]):
+             decode=decode_metadata,
+         )
+ 
+-    def use_cascade_attention(self, *args, **kwargs) -> bool:
+-        return False
++    def can_run_in_cudagraph(
++            self, common_attn_metadata: CommonAttentionMetadata) -> bool:
++        return common_attn_metadata.max_query_len == 1
+ 
+ 
+ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
+@@ -644,14 +676,25 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
+             maybe_padded_v = torch.nn.functional.pad(
+                 v, [0, q.shape[-1] - v.shape[-1]], value=0)
+ 
+-        attn_out = self.flash_attn_varlen_func(
+-            q=q,
+-            k=k,
+-            v=maybe_padded_v,
+-            return_softmax_lse=return_softmax_lse,
+-            softmax_scale=softmax_scale,
+-            **kwargs,
+-        )
++        if is_vllm_fa:
++            attn_out = self.flash_attn_varlen_func(
++                q=q,
++                k=k,
++                v=maybe_padded_v,
++                return_softmax_lse=return_softmax_lse,
++                softmax_scale=softmax_scale,
++                **kwargs,
++            )
++        else:
++            # Use return_attn_probs instead of return_softmax_lse for RoCM
++            attn_out = self.flash_attn_varlen_func(
++                q=q,
++                k=k,
++                v=maybe_padded_v,
++                return_attn_probs=return_softmax_lse,
++                softmax_scale=softmax_scale,
++                **kwargs,
++            )
+ 
+         # Unpack the output if there is multiple results
+         lse = None
+@@ -695,7 +738,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
+                 del eye
+                 # standardize to (output, input)
+                 return dequant_weights.T
+-            return layer.weight
++            return layer.weight if not envs.MACA_VLLM_USE_TN_2_NN else layer.weight.T
+ 
+         # we currently do not have quantized bmm's which are needed for
+         # `W_UV` and `W_UK_T`, we we just store fp16/bf16 copies and perform
+@@ -805,7 +848,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
+     ) -> torch.Tensor:
+         assert attn_metadata.prefill is not None
+ 
+-        has_context = attn_metadata.prefill.chunked_context is not None
++        has_context = False # attn_metadata.prefill.chunked_context is not None
+         kv_nope = self.kv_b_proj(kv_c_normed)[0].view(\
+             -1, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)
+         k_nope, v = kv_nope\
+@@ -823,7 +866,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
+             max_seqlen_k=attn_metadata.prefill.max_query_len,
+             softmax_scale=self.scale,
+             causal=True,
+-            return_softmax_lse=has_context,
++            # return_softmax_lse=has_context,
+         )
+ 
+         if has_context:
+@@ -842,9 +885,10 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
+ 
+         # unpad if necessary
+         if self._pad_v:
+-            output = output[..., :v.shape[-1]]
++            output = output.view(-1, self.num_heads, q.shape[-1])[..., :v.shape[-1]] \
++                .reshape(-1, self.num_heads * v.shape[-1])
+ 
+-        return output.flatten(start_dim=-2)
++        return output
+ 
+     @abstractmethod
+     def _forward_decode(
+diff --git a/vllm/v1/attention/backends/mla/flashmla.py b/vllm/v1/attention/backends/mla/flashmla.py
+index 318b8ede1..87733c54f 100644
+--- a/vllm/v1/attention/backends/mla/flashmla.py
++++ b/vllm/v1/attention/backends/mla/flashmla.py
+@@ -2,7 +2,7 @@
+ # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
+ from dataclasses import dataclass
+-from typing import Any, Optional
++from typing import Any, ClassVar, Optional
+ 
+ import torch
+ 
+@@ -44,7 +44,7 @@ class FlashMLABackend(MLACommonBackend):
+ 
+ @dataclass
+ class FlashMLADecodeMetadata(MLACommonDecodeMetadata):
+-    tile_scheduler_metadata: tuple[torch.Tensor, torch.Tensor]
++    tile_scheduler_metadata: torch.Tensor
+     num_splits: torch.Tensor
+ 
+ 
+@@ -54,13 +54,17 @@ class FlashMLAMetadata(MLACommonMetadata[FlashMLADecodeMetadata]):
+ 
+ 
+ class FlashMLAMetadataBuilder(MLACommonMetadataBuilder[FlashMLAMetadata]):
++    full_cudagraph_supported: ClassVar[bool] = True  # Decode-only
+ 
+     def __init__(self, runner, kv_cache_spec: AttentionSpec,
+                  block_table: BlockTable):
+-        super().__init__(runner, kv_cache_spec, block_table)
++        super().__init__(runner, kv_cache_spec, block_table, FlashMLAMetadata)
+ 
+         self.num_q_heads = self.runner.model_config.get_num_attention_heads(
+             self.runner.parallel_config)
++        
++        self.cg_buf_tile_scheduler_metadata = None
++        self.cg_buf_num_splits = None
+ 
+     def _build_decode(self, block_table_tensor: torch.Tensor,
+                       seq_lens: torch.Tensor) -> FlashMLADecodeMetadata:
+@@ -70,6 +74,30 @@ class FlashMLAMetadataBuilder(MLACommonMetadataBuilder[FlashMLAMetadata]):
+             self.num_q_heads,
+             1, # MQA for the decode path
+         )
++        
++        if self.runner.full_cuda_graph:
++            # First time around (CUDAGraph capture), allocate the static buffer
++            if self.cg_buf_tile_scheduler_metadata is None:
++                self.cg_buf_tile_scheduler_metadata = tile_scheduler_metadata
++                self.cg_buf_num_splits = num_splits
++            else:
++                assert self.cg_buf_num_splits is not None
++
++                # Metadata per-SM, fixed size (#SMs, TileMetadataSize)
++                assert (self.cg_buf_tile_scheduler_metadata.size() ==
++                        tile_scheduler_metadata.size())
++                self.cg_buf_tile_scheduler_metadata.\
++                    copy_(tile_scheduler_metadata)
++                tile_scheduler_metadata = self.cg_buf_tile_scheduler_metadata
++
++                # Num splits is per-batch, varying size (batch_size,)
++                n = num_splits.size(0)
++                # make sure static buffer is large enough
++                assert n <= self.cg_buf_num_splits.size(0)
++                num_splits_view = self.cg_buf_num_splits[:n]
++                num_splits_view.copy_(num_splits)
++                self.cg_buf_num_splits[n:].fill_(0)  # fill the rest with 0s
++                num_splits = num_splits_view
+ 
+         return FlashMLADecodeMetadata(
+             block_table=block_table_tensor,
+diff --git a/vllm/v1/attention/backends/mla/rocm_aiter_mla.py b/vllm/v1/attention/backends/mla/rocm_aiter_mla.py
+index 1f0406a7a..9fbca2e95 100644
+--- a/vllm/v1/attention/backends/mla/rocm_aiter_mla.py
++++ b/vllm/v1/attention/backends/mla/rocm_aiter_mla.py
+@@ -66,7 +66,7 @@ class AiterMLAMetadataBuilder(MLACommonMetadataBuilder[AiterMLAMetadata]):
+ 
+     def __init__(self, runner, kv_cache_spec: AttentionSpec,
+                  block_table: BlockTable):
+-        super().__init__(runner, kv_cache_spec, block_table)
++        super().__init__(runner, kv_cache_spec, block_table, AiterMLAMetadata)
+         assert self.kv_cache_spec.block_size == 1, "AITER MLA" \
+             "only supports block size 1."
+ 
+diff --git a/vllm/v1/attention/backends/mla/triton_mla.py b/vllm/v1/attention/backends/mla/triton_mla.py
+index e26d79091..523f17263 100644
+--- a/vllm/v1/attention/backends/mla/triton_mla.py
++++ b/vllm/v1/attention/backends/mla/triton_mla.py
+@@ -1,6 +1,7 @@
+ # SPDX-License-Identifier: Apache-2.0
+ # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+ 
++from dataclasses import dataclass
+ from typing import Any, Optional
+ 
+ import torch
+@@ -10,11 +11,20 @@ from vllm.attention.backends.abstract import (AttentionType,
+ from vllm.attention.ops.triton_decode_attention import decode_attention_fwd
+ from vllm.logger import init_logger
+ from vllm.v1.attention.backends.mla.common import (MLACommonBackend,
++                                                   MLACommonDecodeMetadata,
+                                                    MLACommonImpl,
+-                                                   MLACommonMetadata)
+-
++                                                   MLACommonMetadata,
++                                                   MLACommonMetadataBuilder)
++from vllm.attention.backends.triton_mla import (load_config,
++                                                find_best_mla_para)
+ logger = init_logger(__name__)
+ 
++import os
++# TODO: Configure environment variables temporarily. New versions do not need to be configured
++os.environ['TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP'] = '1'
++os.environ['TRITON_ENABLE_MACA_CHAIN_DOT_OPT'] = '1'
 +
-+def nvmlDeviceGetGpuFabricInfo(device, gpuFabricInfo):
-+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfo");
-+    ret = fn(device, gpuFabricInfo)
-+    _nvmlCheckReturn(ret)
-+    return ret
++JSON_DATA = load_config()
+ 
+ class TritonMLABackend(MLACommonBackend):
+ 
+@@ -22,10 +32,42 @@ class TritonMLABackend(MLACommonBackend):
+     def get_name() -> str:
+         return "TRITON_MLA_VLLM_V1"
+ 
++    @staticmethod
++    def get_metadata_cls() -> type["TritonMLAMetadata"]:
++        return TritonMLAMetadata
 +
-+def nvmlDeviceGetGpuFabricInfoV(device, gpuFabricInfo):
-+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfoV");
-+    ret = fn(device, gpuFabricInfo)
-+    _nvmlCheckReturn(ret)
-+    return ret
++    @staticmethod
++    def get_builder_cls() -> type["TritonMLAMetadataBuilder"]:
++        return TritonMLAMetadataBuilder
 +
-+######################
-+## Enums/defines
-+#### MXSMLEX GPU NVLINK BW MODE
-+MXSMLEX_GPU_NVLINK_BW_MODE_FULL      = 0x0
-+MXSMLEX_GPU_NVLINK_BW_MODE_OFF       = 0x1
-+MXSMLEX_GPU_NVLINK_BW_MODE_MIN       = 0x2
-+MXSMLEX_GPU_NVLINK_BW_MODE_HALF      = 0x3
-+MXSMLEX_GPU_NVLINK_BW_MODE_3QUARTER  = 0x4
-+MXSMLEX_GPU_NVLINK_BW_MODE_COUNT     = 0x5
+     @staticmethod
+     def get_impl_cls() -> type["TritonMLAImpl"]:
+         return TritonMLAImpl
+-
++@dataclass
++class TritonMLADecodeMetadata(MLACommonDecodeMetadata):
++    num_kv_splits: int
++    num_stages: int
 +
-+def nvmlSystemSetNvlinkBwMode(mode):
-+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetNvlinkBwMode")
-+    ret = fn(mode)
-+    _nvmlCheckReturn(ret)
-+    return ret
++@dataclass
++class TritonMLAMetadata(MLACommonMetadata[TritonMLADecodeMetadata]):
++    pass
 +
-+def nvmlSystemGetNvlinkBwMode():
-+    mode = c_uint()
-+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetNvlinkBwMode")
-+    ret = fn(byref(mode))
-+    _nvmlCheckReturn(ret)
-+    return mode.value
++class TritonMLAMetadataBuilder(MLACommonMetadataBuilder[TritonMLAMetadata]):
++    def _build_decode(self, block_table_tensor: torch.Tensor,
++                      seq_lens: torch.Tensor) -> TritonMLADecodeMetadata:
++        if seq_lens is not None:
++            batch = seq_lens.shape[0]
++            max_seq_len = int(seq_lens.max())
++            num_kv_splits, num_stages = find_best_mla_para(JSON_DATA, batch, max_seq_len, 8)
++        else:
++            num_kv_splits = 4
++            num_stages = 1
++        return TritonMLADecodeMetadata(
++            block_table=block_table_tensor,
++            seq_lens=seq_lens,
++            num_kv_splits=num_kv_splits,
++            num_stages=num_stages,
++        )
+ 
+ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+ 
+@@ -90,14 +132,12 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+                         dtype=q.dtype,
+                         device=q.device)
+ 
+-        num_kv_splits = 4  # TODO: heuristic
+-
+         # TODO(lucas) Allocate ahead of time
+         attn_logits = torch.empty(
+             (
+                 B,
+                 self.num_heads,
+-                num_kv_splits,
++                attn_metadata.decode.num_kv_splits,
+                 # NOTE(lucas) idk why the +1 is here but sglang has it so we
+                 # just mirror that
+                 self.kv_lora_rank + 1,
+@@ -115,6 +155,8 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+         decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,
+                              attn_metadata.decode.block_table,
+                              attn_metadata.decode.seq_lens, attn_logits,
+-                             num_kv_splits, self.scale, PAGE_SIZE)
++                             attn_metadata.decode.num_kv_splits,
++                             attn_metadata.decode.num_stages,
++                             self.scale, PAGE_SIZE)
+ 
+         return self._v_up_proj(o)
+diff --git a/vllm/v1/attention/backends/utils.py b/vllm/v1/attention/backends/utils.py
+index 72c764353..c53674f25 100644
+--- a/vllm/v1/attention/backends/utils.py
++++ b/vllm/v1/attention/backends/utils.py
+@@ -1,15 +1,22 @@
+ # SPDX-License-Identifier: Apache-2.0
+ # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
++import abc
++from abc import abstractmethod
+ from dataclasses import dataclass
++from typing import TYPE_CHECKING, ClassVar, Generic, TypeVar
+ 
++import numpy as np
+ import torch
+ 
++if TYPE_CHECKING:
++    from vllm.v1.core.sched.output import SchedulerOutput
++    from vllm.v1.worker.gpu_input_batch import InputBatch
+ 
+ @dataclass
+ class CommonAttentionMetadata:
+     """
+-    Attention metadata attributes that can be shared by layers in different KV
+-    cache groups and thus having different block table.
++    Per-batch attention metadata, shared across layers and backends.
++    AttentionMetadataBuilder instances use it to construct per-layer metadata.
+     """
+ 
+     query_start_loc: torch.Tensor
+@@ -18,6 +25,65 @@ class CommonAttentionMetadata:
+     """(batch_size,), the length of each request including both computed tokens
+     and newly scheduled tokens"""
+ 
++    num_reqs: int
++    """Number of requests"""
++    num_actual_tokens: int
++    """Total number of tokens in batch"""
++    max_query_len: int
++    """Longest query in batch"""
 +
-+_nvmlPowerScopeType_t = c_uint
-+MXSMLEX_POWER_SCOPE_GPU     = 0
-+MXSMLEX_POWER_SCOPE_MODULE  = 1
-+MXSMLEX_POWER_SCOPE_MEMORY  = 2
++M = TypeVar("M")
 +
-+class c_nvmlPowerValue_v2_t(_PrintableStructure):
-+    _fields_ = [
-+        ('version', c_uint),
-+        ('powerScope', _nvmlPowerScopeType_t),
-+        ('powerValueMw', c_uint),
-+    ]
-+    _fmt_ = {'<default>': "%d B"}
 +
-+nvmlPowerValue_v2 = 0x0200000C
++class AttentionMetadataBuilder(abc.ABC, Generic[M]):
++    # Does this backend/builder support CUDA Graphs for attention.
++    full_cudagraph_supported: ClassVar[bool] = False
 +
-+def nvmlDeviceSetPowerManagementLimit_v2(device, powerScope, powerLimit, version=nvmlPowerValue_v2):
-+    c_powerScope = _nvmlPowerScopeType_t(powerScope)
-+    c_powerValue = c_nvmlPowerValue_v2_t()
-+    c_powerValue.version = c_uint(version)
-+    c_powerValue.powerScope = c_powerScope
-+    c_powerValue.powerValueMw = c_uint(powerLimit)
-+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPowerManagementLimit_v2")
-+    ret = fn(device, byref(c_powerValue))
-+    return ret
++    @abstractmethod
++    def build(self, common_prefix_len: int,
++              common_attn_metadata: CommonAttentionMetadata) -> M:
++        """
++        Central method that builds attention metadata.
++        Some builders (MLA) require reorder_batch to be called prior to build.
++        """
++        raise NotImplementedError
 +
-+class c_nvmlEccSramErrorStatus_v1_t(_PrintableStructure):
-+    _fields_ = [
-+        ('version', c_uint),
-+        ('aggregateUncParity', c_ulonglong),
-+        ('aggregateUncSecDed', c_ulonglong),
-+        ('aggregateCor', c_ulonglong),
-+        ('volatileUncParity', c_ulonglong),
-+        ('volatileUncSecDed', c_ulonglong),
-+        ('volatileCor', c_ulonglong),
-+        ('aggregateUncBucketL2', c_ulonglong),
-+        ('aggregateUncBucketSm', c_ulonglong),
-+        ('aggregateUncBucketPcie', c_ulonglong),
-+        ('aggregateUncBucketMcu', c_ulonglong),
-+        ('aggregateUncBucketOther', c_ulonglong),
-+        ('bThresholdExceeded', c_uint)
-+    ]
++    def can_run_in_cudagraph(
++            self, common_attn_metadata: CommonAttentionMetadata) -> bool:
++        """
++        Can this batch (with given metadata) use CUDA Graphs for attention.
++        """
++        return False
 +
-+    def __init__(self):
-+        super(c_nvmlEccSramErrorStatus_v1_t, self).__init__(version=nvmlEccSramErrorStatus_v1)
++    def build_for_cudagraph_capture(
++            self, common_attn_metadata: CommonAttentionMetadata) -> M:
++        """
++        Build attention metadata for CUDA graph capture. Uses build by default.
++        Subclasses that override this method should call self.build or
++        super().build_for_cudagraph_capture.
++        """
++        return self.build(common_prefix_len=0,
++                          common_attn_metadata=common_attn_metadata)
 +
-+nvmlEccSramErrorStatus_v1 = 0x1000068
++    def use_cascade_attention(
++        self,
++        common_prefix_len: int,
++        query_lens: np.ndarray,
++        num_query_heads: int,
++        num_kv_heads: int,
++        use_alibi: bool,
++        use_sliding_window: bool,
++        num_sms: int,
++    ) -> bool:
++        return False
++
++    def reorder_batch(self, input_batch: "InputBatch",
++                      scheduler_output: "SchedulerOutput") -> bool:
++        """
++        This method can reorder the batch if desired by the backend.
++        :return: Has the batch been reordered (default False).
++        """
++        return False
+ 
+ def validate_kv_sharing_target(current_layer_name, target_layer_name,
+                                static_forward_context):
+diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
+index f36a491a1..dd8730aed 100644
+--- a/vllm/v1/engine/core.py
++++ b/vllm/v1/engine/core.py
+@@ -799,6 +799,7 @@ class DPEngineCoreProc(EngineCoreProc):
+             str(current_platform.device_id_to_physical_device_id(i))
+             for i in range(local_dp_rank * world_size, (local_dp_rank + 1) *
+                            world_size))
++        os.environ["MACA_VISIBLE_DEVICES"] = os.environ[device_control_env_var]
+ 
+         self.dp_rank = dp_rank
+         self.dp_group = vllm_config.parallel_config.stateless_init_dp_group()
+@@ -917,7 +918,7 @@ class DPEngineCoreActor(DPEngineCoreProc):
+         # Ray sets CUDA_VISIBLE_DEVICES to empty string,
+         # we clean this up to be able to properly initialize
+         # data parallel groups.
+-        del os.environ['CUDA_VISIBLE_DEVICES']
++        # del os.environ['CUDA_VISIBLE_DEVICES']
+ 
+         super().__init__(vllm_config, on_head_node, "", executor_class,
+                          log_stats)
+diff --git a/vllm/v1/engine/detokenizer.py b/vllm/v1/engine/detokenizer.py
+index c6fe2d339..9ec2f5ce2 100644
+--- a/vllm/v1/engine/detokenizer.py
++++ b/vllm/v1/engine/detokenizer.py
+@@ -17,6 +17,13 @@ from vllm.v1.engine import EngineCoreRequest
+ 
+ logger = init_logger(__name__)
+ 
++# Only tokenizers >= 0.21.1 supports DecodeStream used for
++# FastIncrementalDetokenizer.
++USE_FAST_DETOKENIZER = version.parse(
++    tokenizers.__version__) >= version.parse("0.21.1")
 +
-+def nvmlDeviceGetSramEccErrorStatus(device, status):
-+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSramEccErrorStatus")
-+    ret = fn(device, status)
-+    _nvmlCheckReturn(ret)
-+    return ret
++# Error string from https://github.com/huggingface/tokenizers/blob/909fdde2a4ffedd9295206f705eb612be2a91b12/tokenizers/src/tokenizer/mod.rs#L1042
++INVALID_PREFIX_ERR_MSG = "Invalid prefix encountered"
+ 
+ class IncrementalDetokenizer:
+ 
+@@ -46,10 +53,9 @@ class IncrementalDetokenizer:
+             # No tokenizer => skipping detokenization.
+             return IncrementalDetokenizer()
+ 
+-        if (isinstance(tokenizer, PreTrainedTokenizerFast) and version.parse(
+-                tokenizers.__version__) >= version.parse("0.21.1")):
++        if USE_FAST_DETOKENIZER and isinstance(tokenizer,
++                                               PreTrainedTokenizerFast):
+             # Fast tokenizer => use tokenizers library DecodeStream.
+-            # And only tokenizers >= 0.21.1 supports Fast Detokenizer.
+             return FastIncrementalDetokenizer(tokenizer, request)
+ 
+         # Fall back to slow python-based incremental detokenization.
+@@ -157,8 +163,11 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
+         super().__init__(request)
+ 
+         sampling_params = request.sampling_params
 +
-diff --git a/vllm/triton_utils/custom_cache_manager.py b/vllm/triton_utils/custom_cache_manager.py
-index 4163969c9..df44adda0 100644
---- a/vllm/triton_utils/custom_cache_manager.py
-+++ b/vllm/triton_utils/custom_cache_manager.py
-@@ -1,9 +1,11 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
++        self.request_id = request.request_id
++        self.skip_special_tokens = sampling_params.skip_special_tokens
+         self.stream = DecodeStream(
+-            skip_special_tokens=sampling_params.skip_special_tokens)
++            skip_special_tokens=self.skip_special_tokens)
  
- import os
+         self.tokenizer: Tokenizer = tokenizer._tokenizer
  
--from triton.runtime.cache import (FileCacheManager, default_cache_dir,
--                                  default_dump_dir, default_override_dir)
-+#from triton.runtime.cache import (FileCacheManager, default_cache_dir,
-+#                                  default_dump_dir, default_override_dir)
-+from triton.runtime.cache import FileCacheManager, default_cache_dir
+@@ -174,7 +183,7 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
  
- from vllm.logger import init_logger
+         # Prime the stream.
+         for tid in prompt_suffix:
+-            self.stream.step(self.tokenizer, tid)
++            self._protected_step(tid)
  
-diff --git a/vllm/utils.py b/vllm/utils.py
-index 8b9269598..676ad5ff5 100644
---- a/vllm/utils.py
-+++ b/vllm/utils.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+         self.spaces_between_special_tokens = (
+             sampling_params.skip_special_tokens
+@@ -199,7 +208,7 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
+                 self.spaces_between_special_tokens = True
  
- import argparse
-@@ -151,6 +152,7 @@ STR_DTYPE_TO_TORCH_DTYPE = {
-     "fp8": torch.uint8,
-     "fp8_e4m3": torch.uint8,
-     "fp8_e5m2": torch.uint8,
-+    "int8": torch.int8,
- }
+     def decode_next(self, next_token_id: int) -> str:
+-        token = self.stream.step(self.tokenizer, next_token_id)
++        token = self._protected_step(next_token_id)
  
- TORCH_DTYPE_TO_NUMPY_DTYPE = {
-@@ -658,6 +660,7 @@ def create_kv_caches_with_random(
-     model_dtype: Optional[Union[str, torch.dtype]] = None,
-     seed: int = 0,
-     device: Optional[str] = "cuda",
-+    new_layerout:Optional[bool] = False,
- ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
- 
-     if cache_dtype == "fp8" and head_size % 16:
-@@ -685,6 +688,11 @@ def create_kv_caches_with_random(
-             raise ValueError(
-                 f"Does not support key cache of type {cache_dtype}")
-         key_caches.append(key_cache)
-+        if new_layerout:
-+            key_cache_new = torch.empty(size=key_cache_shape,
-+                                        dtype=torch_dtype,
-+                                        device=device)
-+            key_caches.append(key_cache_new)
- 
-     value_cache_shape = (num_blocks, num_heads, head_size, block_size)
-     value_caches: List[torch.Tensor] = []
-@@ -700,6 +708,12 @@ def create_kv_caches_with_random(
-             raise ValueError(
-                 f"Does not support value cache of type {cache_dtype}")
-         value_caches.append(value_cache)
-+        if new_layerout:
-+            value_cache_new = torch.empty(size=value_cache_shape,
-+                                    dtype=torch_dtype,
-+                                    device=device)
-+            value_caches.append(value_cache_new)
-+
-     return key_caches, value_caches
- 
- 
-@@ -942,7 +956,7 @@ def find_nccl_library() -> str:
-             so_file)
+         if not self.spaces_between_special_tokens:
+             special_token = self.added_token_ids.get(next_token_id)
+@@ -211,6 +220,22 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
+ 
+         return token or ""
+ 
++    def _protected_step(self, next_token_id: int) -> Optional[str]:
++        try:
++            token = self.stream.step(self.tokenizer, next_token_id)
++        except Exception as e:
++            if str(e) != INVALID_PREFIX_ERR_MSG:
++                raise e
++            # Recover from edge case where tokenizer can produce non-monotonic,
++            # invalid UTF-8 output, which breaks the internal state of
++            # tokenizers' DecodeStream.
++            # See https://github.com/vllm-project/vllm/issues/17448.
++            logger.warning(
++                "Encountered invalid prefix detokenization error"
++                " for request %s, resetting decode stream.", self.request_id)
++            self.stream = DecodeStream(self.skip_special_tokens)
++            token = self.stream.step(self.tokenizer, next_token_id)
++        return token
+ 
+ class SlowIncrementalDetokenizer(BaseIncrementalDetokenizer):
+ 
+diff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py
+index b2354c533..efaa85145 100644
+--- a/vllm/v1/sample/rejection_sampler.py
++++ b/vllm/v1/sample/rejection_sampler.py
+@@ -446,7 +446,7 @@ def rejection_greedy_sample_kernel(
+         is_greedy = True
      else:
-         if torch.version.cuda is not None:
--            so_file = "libnccl.so.2"
-+            so_file = "libmccl.so"
-         elif torch.version.hip is not None:
-             so_file = "librccl.so.1"
-         else:
-@@ -1875,12 +1889,12 @@ def direct_register_custom_op(
- 
-     if not supports_custom_op():
-         from vllm.platforms import current_platform
--        assert not current_platform.is_cuda_alike(), (
--            "cuda platform needs torch>=2.4 to support custom op, "
--            "chances are you are using an old version of pytorch "
--            "or a custom build of pytorch. It is recommended to "
--            "use vLLM in a fresh new environment and let it install "
--            "the required dependencies.")
-+        #assert not current_platform.is_cuda_alike(), (
-+        #    "cuda platform needs torch>=2.4 to support custom op, "
-+        #    "chances are you are using an old version of pytorch "
-+        #    "or a custom build of pytorch. It is recommended to "
-+        #    "use vLLM in a fresh new environment and let it install "
-+        #    "the required dependencies.")
+         is_greedy = tl.load(is_greedy_ptr + req_idx)
+-    if not is_greedy:
++    if is_greedy is None:
+         # Early exit for non-greedy sampling requests.
          return
  
-     import torch.library
-@@ -2246,7 +2260,8 @@ def import_pynvml():
-     `pynvml.py` module from the `nvidia-ml-py` package.
-     """
-     if TYPE_CHECKING:
--        import pynvml
-+        #import pynvml
-+        import vllm.platforms.pynvml as pynvml
-         return pynvml
-     if "pynvml" in sys.modules:
-         import pynvml
-diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
-old mode 100755
-new mode 100644
-index 837d7faf4..4bcb6a3dc
---- a/vllm/v1/attention/backends/flash_attn.py
-+++ b/vllm/v1/attention/backends/flash_attn.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- """Attention layer with FlashAttention."""
- from dataclasses import dataclass
-@@ -14,9 +15,11 @@ from vllm.envs import VLLM_FLASH_ATTN_VERSION
- from vllm.logger import init_logger
- from vllm.platforms import current_platform
- from vllm.utils import cdiv
--from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
--                                  flash_attn_varlen_func,
--                                  is_fa_version_supported)
-+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
-+#                                  flash_attn_varlen_func,
-+#                                  is_fa_version_supported)
-+from flash_attn import (flash_attn_varlen_func,
-+                        flash_attn_with_kvcache)
+@@ -494,7 +494,7 @@ def rejection_random_sample_kernel(
+ ):
+     req_idx = tl.program_id(0)
+     is_greedy = tl.load(is_greedy_ptr + req_idx)
+-    if is_greedy:
++    if is_greedy is not None:
+         # Early exit for greedy sampling requests.
+         return
  
- logger = init_logger(__name__)
+diff --git a/vllm/v1/spec_decode/eagle.py b/vllm/v1/spec_decode/eagle.py
+index 4b5c9b7ec..47b0fbf54 100644
+--- a/vllm/v1/spec_decode/eagle.py
++++ b/vllm/v1/spec_decode/eagle.py
+@@ -138,15 +138,17 @@ class EagleProposer:
+             max_query_len = query_lens.max().item()
+ 
+             common_attn_metadata = CommonAttentionMetadata(
+-                query_start_loc=cu_num_tokens, seq_lens=seq_lens)
++                query_start_loc=cu_num_tokens,
++                seq_lens=seq_lens,
++                num_reqs=batch_size,
++                num_actual_tokens=num_tokens,
++                max_query_len=max_query_len,
++            )
  
-@@ -27,7 +30,7 @@ class FlashAttentionBackend(AttentionBackend):
+             assert self.runner is not None
  
-     @staticmethod
-     def get_supported_head_sizes() -> List[int]:
--        return [32, 64, 96, 128, 160, 192, 224, 256]
-+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
+             # FIXME: need to consider multiple kv_cache_groups
+-            attn_metadata = self.runner.attn_metadata_builder.build(
+-                num_reqs=batch_size,
+-                num_actual_tokens=num_tokens,
+-                max_query_len=max_query_len,
++            attn_metadata = self.runner.attn_metadata_builders[0].build(
+                 common_prefix_len=0,
+                 common_attn_metadata=common_attn_metadata,
+             )
+diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
+index b1bc727e1..88d1d35ba 100644
+--- a/vllm/v1/worker/gpu_model_runner.py
++++ b/vllm/v1/worker/gpu_model_runner.py
+@@ -15,10 +15,8 @@ import torch.nn as nn
+ 
+ import vllm.envs as envs
+ from vllm.attention import AttentionType, get_attn_backend
+-from vllm.attention.backends.abstract import (AttentionBackend,
+-                                              AttentionMetadataBuilder)
++from vllm.attention.backends.abstract import AttentionBackend
+ from vllm.attention.layer import Attention
+-from vllm.attention.utils.fa_utils import get_flash_attn_version
+ from vllm.config import (CompilationLevel, VllmConfig,
+                          get_layers_from_vllm_config)
+ from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+@@ -40,7 +38,8 @@ from vllm.sequence import IntermediateTensors
+ from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,
+                         GiB_bytes, LazyLoader, async_tensor_h2d, cdiv,
+                         check_use_alibi, is_pin_memory_available)
+-from vllm.v1.attention.backends.utils import CommonAttentionMetadata
++from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
++                                              CommonAttentionMetadata)
+ from vllm.v1.core.encoder_cache_manager import compute_encoder_budget
+ from vllm.v1.kv_cache_interface import (AttentionSpec, FullAttentionSpec,
+                                         KVCacheConfig, KVCacheSpec,
+@@ -74,6 +73,24 @@ else:
+ logger = init_logger(__name__)
  
-     @staticmethod
-     def get_name() -> str:
-@@ -139,6 +142,7 @@ class FlashAttentionImpl(AttentionImpl):
-         # if hopper default to FA3, otherwise stick to FA2 for now
-         # TODO(lucas): profile FA3 on ampere to see if it makes sense to
-         #  use FA3 as default for both
-+        """
-         if current_platform.get_device_capability()[0] >= 9:
-             self.fa_version = 3 if is_fa_version_supported(3) else 2
-         else:
-@@ -154,6 +158,7 @@ class FlashAttentionImpl(AttentionImpl):
-                          fa_version_unsupported_reason(self.fa_version))
  
-         assert is_fa_version_supported(self.fa_version)
-+        """
++# --- FLAGSCALE MODIFICATION BEG ---
++# Know more about FlagGems: https://github.com/FlagOpen/FlagGems
++import os
++if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
++     try:
++        print("Try to using FLAGGEMS...")
++        import flag_gems
++        flag_gems.enable(record=True, unused=["exponential_", "softmax", "masked_fill_", "index"], path="/tmp/gems_oplist.log.txt")
++        logger.info("Successfully enabled flag_gems as default ops implementation.")
++     except ImportError as e:
++        # Throw an exception directly if failure occurs
++        raise ImportError("Failed to import 'flag_gems'. Please install flag_gems or set USE_FLAGGEMS=false to disable it.") from e
++     except Exception as e:
++        # Throw an exception directly if failure occurs
++        raise RuntimeError(f"Failed to enable 'flag_gems': {e}. Please check your flag_gems installation or set USE_FLAGGEMS=false to disable it.") from e
++# --- FLAGSCALE MODIFICATION END ---
++
++
+ class GPUModelRunner(LoRAModelRunnerMixin):
+ 
+     def __init__(
+@@ -84,6 +101,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         self.vllm_config = vllm_config
+         self.model_config = vllm_config.model_config
+         self.cache_config = vllm_config.cache_config
++        self.compilation_config = vllm_config.compilation_config
+         self.lora_config = vllm_config.lora_config
+         self.load_config = vllm_config.load_config
+         self.parallel_config = vllm_config.parallel_config
+@@ -192,7 +210,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+             block_sizes=[self.cache_config.block_size],
+         )
  
-     def forward(
+-        self.use_cuda_graph = (self.vllm_config.compilation_config.level
++        self.use_cuda_graph = (self.compilation_config.level
+                                == CompilationLevel.PIECEWISE
+                                and not self.model_config.enforce_eager)
+         # TODO(woosuk): Provide an option to tune the max cudagraph batch size.
+@@ -200,8 +218,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         # self.cudagraph_batch_sizes sorts in ascending order.
+         # The batch sizes in the config are in descending order.
+         self.cudagraph_batch_sizes = list(
+-            reversed(
+-                self.vllm_config.compilation_config.cudagraph_capture_sizes))
++           reversed(self.compilation_config.cudagraph_capture_sizes))
++
++        self.full_cuda_graph = self.compilation_config.full_cuda_graph
+ 
+         # Cache the device properties.
+         self._init_device_properties()
+@@ -552,7 +571,15 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+     def _prepare_inputs(
          self,
-@@ -212,14 +217,18 @@ class FlashAttentionImpl(AttentionImpl):
-         # Compute attention and update output up to `num_actual_tokens`.
-         if not attn_metadata.use_cascade:
-             # Regular attention (common case).
--            flash_attn_varlen_func(
-+            #print("is ok")
-+            cu_prefix_kv_lens = torch.tensor([0] + attn_metadata.seq_lens.tolist(), device='cuda:0',dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
-+            #print(key_cache)
-+            output[:num_actual_tokens] = flash_attn_varlen_func(
-                 q=query[:num_actual_tokens],
-                 k=key_cache,
-                 v=value_cache,
--                out=output[:num_actual_tokens],
-+                #out=output[:num_actual_tokens],
-                 cu_seqlens_q=attn_metadata.query_start_loc,
-                 max_seqlen_q=attn_metadata.max_query_len,
--                seqused_k=attn_metadata.seq_lens,
-+                #seqused_k=attn_metadata.seq_lens,
-+                cu_seqlens_k=cu_prefix_kv_lens,     #Maybe error
-                 max_seqlen_k=attn_metadata.max_seq_len,
-                 softmax_scale=self.scale,
-                 causal=True,
-@@ -227,7 +236,7 @@ class FlashAttentionImpl(AttentionImpl):
-                 window_size=self.sliding_window,
-                 block_table=attn_metadata.block_table,
-                 softcap=self.logits_soft_cap,
--                fa_version=self.fa_version,
-+                #fa_version=self.fa_version,
-             )
-             return output
+         scheduler_output: "SchedulerOutput",
+-    ) -> tuple[dict[str, Any], torch.Tensor, Optional[SpecDecodeMetadata]]:
++     ) -> tuple[dict[str, Any], bool, torch.Tensor,
++               Optional[SpecDecodeMetadata]]:
++        """
++        :return: tuple[
++            attn_metadata: layer-to-attention_metadata mapping,
++            attention_cuda_graphs: whether attention can run in cudagraph
++            logits_indices, spec_decode_metadata
++        ]
++        """
+         total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+         assert total_num_scheduled_tokens > 0
+         num_reqs = self.input_batch.num_reqs
+@@ -666,7 +693,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         seq_lens = self.seq_lens[:num_reqs]
+ 
+         common_attn_metadata = CommonAttentionMetadata(
+-            query_start_loc=query_start_loc, seq_lens=seq_lens)
++            query_start_loc=query_start_loc,
++            seq_lens=seq_lens,
++            num_reqs=num_reqs,
++            num_actual_tokens=total_num_scheduled_tokens,
++            max_query_len=max_num_scheduled_tokens,
++        )
  
-@@ -249,7 +258,7 @@ class FlashAttentionImpl(AttentionImpl):
-             logits_soft_cap=self.logits_soft_cap,
-             block_table=attn_metadata.block_table,
-             common_prefix_len=attn_metadata.common_prefix_len,
--            fa_version=self.fa_version,
-+            #fa_version=self.fa_version,
-         )
-         return output
+         attn_metadata: dict[str, Any] = {}
+         # Prepare the attention metadata for each KV cache group and make layers
+@@ -676,25 +708,28 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+ 
+             # Prepare for cascade attention if enabled & beneficial.
+             common_prefix_len = 0
++            builder = self.attn_metadata_builders[kv_cache_group_id]
+             if self.cascade_attn_enabled:
+                 common_prefix_len = self._compute_cascade_attn_prefix_len(
+                     num_scheduled_tokens,
+                     scheduler_output.
+                     num_common_prefix_blocks[kv_cache_group_id],
+                     kv_cache_group_spec.kv_cache_spec,
+-                    self.attn_metadata_builders[kv_cache_group_id],
++                    builder,
+                 )
  
-@@ -353,12 +362,15 @@ def cascade_attention(
-     assert num_common_kv_blocks > 0
+-            attn_metadata_i = (
+-                self.attn_metadata_builders[kv_cache_group_id].build(
+-                    num_reqs=num_reqs,
+-                    num_actual_tokens=total_num_scheduled_tokens,
+-                    max_query_len=max_num_scheduled_tokens,
+-                    common_prefix_len=common_prefix_len,
+-                    common_attn_metadata=common_attn_metadata))
++            attn_metadata_i = (builder.build(
++                common_prefix_len=common_prefix_len,
++                common_attn_metadata=common_attn_metadata,
++            ))
++
+             for layer_name in kv_cache_group_spec.layer_names:
+                 attn_metadata[layer_name] = attn_metadata_i
+ 
++        attention_cuda_graphs = all(
++            b.can_run_in_cudagraph(common_attn_metadata)
++            for b in self.attn_metadata_builders)
++
+         use_spec_decode = len(
+             scheduler_output.scheduled_spec_decode_tokens) > 0
+         if not use_spec_decode:
+@@ -723,7 +758,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         if self.lora_config:
+             self.set_active_loras(self.input_batch, num_scheduled_tokens)
+ 
+-        return attn_metadata, logits_indices, spec_decode_metadata
++        return (attn_metadata, attention_cuda_graphs, logits_indices,
++                spec_decode_metadata)
+ 
+     def _compute_cascade_attn_prefix_len(
+         self,
+@@ -1115,7 +1151,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         assert self.intermediate_tensors is not None
+ 
+         tp = self.vllm_config.parallel_config.tensor_parallel_size
+-        enabled_sp = self.vllm_config.compilation_config.pass_config. \
++        enabled_sp = self.compilation_config.pass_config. \
+             enable_sequence_parallelism
+         if enabled_sp:
+             # When sequence parallelism is enabled, we always pad num_tokens
+@@ -1183,8 +1219,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+             return self.kv_connector_no_forward(scheduler_output)
+ 
+         # Prepare the decoder inputs.
+-        attn_metadata, logits_indices, spec_decode_metadata = (
+-            self._prepare_inputs(scheduler_output))
++        (attn_metadata, attention_cuda_graphs, logits_indices,
++         spec_decode_metadata) = (self._prepare_inputs(scheduler_output))
+         num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+         if (self.use_cuda_graph
+                 and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):
+@@ -1197,7 +1233,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+             # Pad tokens to multiple of tensor_parallel_size when
+             # enabled collective fusion for SP
+             tp_size = self.vllm_config.parallel_config.tensor_parallel_size
+-            if self.vllm_config.compilation_config.pass_config. \
++            if self.compilation_config.pass_config. \
+                 enable_sequence_parallelism and tp_size > 1:
+                 from vllm.utils import round_up
+                 num_input_tokens = round_up(num_scheduled_tokens, tp_size)
+@@ -1249,12 +1285,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+             intermediate_tensors = self.sync_and_slice_intermediate_tensors(
+                 num_input_tokens, intermediate_tensors, True)
+ 
++        # Some attention backends only support CUDA Graphs in pure decode.
++        # If attention doesn't support CUDA Graphs for this batch, but we
++        # compiled with full CUDA graphs, we have to skip them entirely.
++        skip_cuda_graphs = self.full_cuda_graph and not attention_cuda_graphs
++
+         # Run the decoder.
+         # Use persistent buffers for CUDA graphs.
+-        with set_forward_context(attn_metadata,
+-                                 self.vllm_config,
+-                                 num_tokens=num_input_tokens,
+-                                 num_tokens_across_dp=num_tokens_across_dp):
++        with set_forward_context(
++                attn_metadata,
++                self.vllm_config,
++                num_tokens=num_input_tokens,
++                num_tokens_across_dp=num_tokens_across_dp,
++                skip_cuda_graphs=skip_cuda_graphs,
++            ):
+             self.maybe_setup_kv_connector(scheduler_output)
+ 
+             model_output = self.model(
+@@ -1763,7 +1807,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+     def _dummy_run(
+         self,
+         num_tokens: int,
+-        skip_attn: bool = True,
++        capture_attn_cudagraph: bool = False,
+     ) -> torch.Tensor:
  
-     # Process shared prefix.
--    prefix_output, prefix_lse = flash_attn_varlen_func(
-+    #prefix_output, prefix_lse = flash_attn_varlen_func(
-+    cu_prefix_kv_lens = torch.tensor([0] + prefix_kv_lens.tolist(), dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
-+    prefix_output, prefix_lse, prefix_s_dmask = flash_attn_varlen_func(
-         q=query,
-         k=key_cache,
-         v=value_cache,
-         cu_seqlens_q=cu_prefix_query_lens,
--        seqused_k=prefix_kv_lens,
-+        cu_seqlens_k=cu_prefix_kv_lens,
-+        #seqused_k=prefix_kv_lens,
-         max_seqlen_q=num_tokens,
-         max_seqlen_k=common_prefix_len,
-         softmax_scale=softmax_scale,
-@@ -366,17 +378,21 @@ def cascade_attention(
-         window_size=sliding_window,
-         block_table=block_table[:1],
-         softcap=logits_soft_cap,
--        return_softmax_lse=True,
--        fa_version=fa_version,
-+        return_attn_probs=True,
-+        #return_softmax_lse=True,
-+        #fa_version=fa_version,
-     )
+         # Padding for DP
+@@ -1784,9 +1828,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         num_scheduled_tokens = np.array(num_scheduled_tokens_list,
+                                         dtype=np.int32)
  
-     # Process suffix per query.
--    suffix_output, suffix_lse = flash_attn_varlen_func(
-+    #suffix_output, suffix_lse = flash_attn_varlen_func(
-+    cu_suffix_kv_lens = torch.tensor([0] + suffix_kv_lens.tolist(), dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
-+    suffix_output, suffix_lse, suffix_s_dmask = flash_attn_varlen_func(
-         q=query,
-         k=key_cache,
-         v=value_cache,
-         cu_seqlens_q=cu_query_lens,
--        seqused_k=suffix_kv_lens,
-+        cu_seqlens_k=cu_suffix_kv_lens,
-+        #seqused_k=suffix_kv_lens,
-         max_seqlen_q=max_query_len,
-         max_seqlen_k=max_kv_len - common_prefix_len,
-         softmax_scale=softmax_scale,
-@@ -384,8 +400,9 @@ def cascade_attention(
-         window_size=sliding_window,
-         block_table=block_table[:, num_common_kv_blocks:],
-         softcap=logits_soft_cap,
--        return_softmax_lse=True,
--        fa_version=fa_version,
-+        return_attn_probs=True,
-+        #return_softmax_lse=True,
-+        #fa_version=fa_version,
-     )
+-        if skip_attn:
+-            attn_metadata: Optional[dict[str, Any]] = None
+-        else:
++        attn_metadata: Optional[dict[str, Any]] = None
++        if capture_attn_cudagraph:
++            attn_metadata = {}
+             query_start_loc = self.query_start_loc[:num_reqs + 1]
+             # Make sure max_model_len is used at the graph capture time.
+             self.seq_lens_np[:num_reqs] = self.max_model_len
+@@ -1796,19 +1840,18 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+             seq_lens = self.seq_lens[:num_reqs]
+ 
+             common_attn_metadata = CommonAttentionMetadata(
+-                query_start_loc=query_start_loc, seq_lens=seq_lens)
++                query_start_loc=query_start_loc,
++                seq_lens=seq_lens,
++                num_reqs=num_reqs,
++                num_actual_tokens=num_tokens,
++                max_query_len=num_tokens,
++            )
  
-     # Merge prefix and suffix outputs, and store the result in output.
-diff --git a/vllm/v1/core/scheduler.py b/vllm/v1/core/scheduler.py
-index fb5e83fe0..e2981940b 100644
---- a/vllm/v1/core/scheduler.py
-+++ b/vllm/v1/core/scheduler.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+-            attn_metadata = {}
+             for kv_cache_group_id, kv_cache_group_spec in enumerate(
+                     self.kv_cache_config.kv_cache_groups):
+-                attn_metadata_i = (
+-                    self.attn_metadata_builders[kv_cache_group_id].build(
+-                        num_reqs=num_reqs,
+-                        num_actual_tokens=num_tokens,
+-                        max_query_len=num_tokens,
+-                        common_prefix_len=0,
+-                        common_attn_metadata=common_attn_metadata,
+-                    ))
++                attn_metadata_i = self.attn_metadata_builders[
++                    kv_cache_group_id].build_for_cudagraph_capture(
++                        common_attn_metadata)
+                 for layer_name in kv_cache_group_spec.layer_names:
+                     attn_metadata[layer_name] = attn_metadata_i
+ 
+@@ -1880,7 +1923,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+ 
+         dummy_metadata = SamplingMetadata(
+             temperature=dummy_tensors(0.5),
+-            all_greedy=False,
++            all_greedy=True,
+             all_random=False,
+             top_p=dummy_tensors(0.9),
+             top_k=dummy_tensors(logits.size(1) - 1),
+@@ -2033,12 +2076,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+         # Capture the large shapes first so that the smaller shapes
+         # can reuse the memory pool allocated for the large shapes.
+         with graph_capture(device=self.device):
+-            skip_attn = not self.vllm_config.compilation_config.full_cuda_graph
++            full_cg = self.full_cuda_graph
+             for num_tokens in reversed(self.cudagraph_batch_sizes):
+-                for _ in range(self.vllm_config.compilation_config.
+-                               cudagraph_num_of_warmups):
+-                    self._dummy_run(num_tokens, skip_attn=skip_attn)
+-                self._dummy_run(num_tokens, skip_attn=skip_attn)
++                for _ in range(
++                        self.compilation_config.cudagraph_num_of_warmups):
++                    self._dummy_run(num_tokens, capture_attn_cudagraph=full_cg)
++                self._dummy_run(num_tokens, capture_attn_cudagraph=full_cg)
+ 
+         end_time = time.perf_counter()
+         end_free_gpu_memory = torch.cuda.mem_get_info()[0]
+@@ -2081,20 +2124,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+                     "Non-Attention backend is not supported by V1 "
+                     "GPUModelRunner.")
+ 
+-            if self.vllm_config.compilation_config.full_cuda_graph:
+-                attn_backend_name = attn_backend_i.__name__
+-                flash_attn_version = get_flash_attn_version()
+-                if attn_backend_name != "FlashAttentionBackend" or \
+-                    flash_attn_version != 3:
+-                    raise ValueError(
+-                        f"full_cuda_graph is only supported with "
+-                        f"FA3. Current attention backend is "
+-                        f"{attn_backend_name}, FlashAttention version is "
+-                        f"{flash_attn_version}.")
+-
+             block_table_i = self.input_batch.block_table[i]
+             attn_metadata_builder_i = attn_backend_i.get_builder_cls()(
+-                weakref.proxy(self), kv_cache_spec, block_table_i)
++                weakref.proxy(self),
++                kv_cache_spec,
++                block_table_i,
++            )
++
++            if (self.full_cuda_graph
++                and not attn_metadata_builder_i.full_cudagraph_supported):
++                raise ValueError(
++                    f"Full CUDAGraph not supported for "
++                    f"{attn_backend_i.__name__}. Turn off CompilationConfig."
++                    f"full_cuda_graph or use a different attention backend.")
++
+             self.attn_backends.append(attn_backend_i)
+             self.attn_metadata_builders.append(attn_metadata_builder_i)
  
- from collections import deque
-diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
-index 29a9ac186..9fb13df28 100644
---- a/vllm/v1/engine/core.py
-+++ b/vllm/v1/engine/core.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -2237,10 +2280,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
+                 kv_caches,
+             )
  
- import pickle
-diff --git a/vllm/v1/executor/abstract.py b/vllm/v1/executor/abstract.py
-index ac10d43eb..6d6fd312a 100644
---- a/vllm/v1/executor/abstract.py
-+++ b/vllm/v1/executor/abstract.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+-        bind_kv_cache(
+-            kv_caches,
+-            self.vllm_config.compilation_config.static_forward_context,
+-            self.kv_caches)
++        bind_kv_cache(kv_caches,
++                      self.compilation_config.static_forward_context,
++                      self.kv_caches)
+         return kv_caches
  
- from typing import Type
+     def initialize_kv_cache(self, kv_cache_config: KVCacheConfig) -> None:
 diff --git a/vllm/version.py b/vllm/version.py
-index 70cd0289b..d5c02d75d 100644
+index 6c88b1b5a..11e21ccd5 100644
 --- a/vllm/version.py
 +++ b/vllm/version.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- try:
-@@ -5,9 +6,9 @@ try:
+@@ -6,11 +6,11 @@ try:
  except Exception as e:
      import warnings
  
 -    warnings.warn(f"Failed to read commit hash:\n{e}",
 -                  RuntimeWarning,
 -                  stacklevel=2)
-+    #warnings.warn(f"Failed to read commit hash:\n{e}",
-+    #              RuntimeWarning,
-+    #              stacklevel=2)
++    # warnings.warn(f"Failed to read commit hash:\n{e}",
++    #               RuntimeWarning,
++    #               stacklevel=2)
  
 -    __version__ = "dev"
-+    __version__ = "0.7.2"
++    __version__ = "0.9.1"
      __version_tuple__ = (0, 0, __version__)
-diff --git a/vllm/worker/cache_engine.py b/vllm/worker/cache_engine.py
-index 3960392cf..32f1895a6 100644
---- a/vllm/worker/cache_engine.py
-+++ b/vllm/worker/cache_engine.py
-@@ -154,8 +154,7 @@ class CacheEngine:
- 
-         key_cache_entry = num_heads * head_size
-         if CacheEngine._align_cache(model_config):
--            key_cache_entry = align_to_256bytes(key_cache_entry,
--                                                model_config.dtype)
-+            key_cache_entry = align_to_256bytes(key_cache_entry, dtype)
- 
-         # For MLA there is no value cache, since the latent vector
-         # is joint keys and values.
+ 
+ 
 diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
-index 12baecde6..bb315094d 100644
+index 82db6617b..dcae1a098 100644
 --- a/vllm/worker/model_runner.py
 +++ b/vllm/worker/model_runner.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
+@@ -78,6 +78,24 @@ torch._dynamo.config.cache_size_limit = 128
+ torch._dynamo.config.accumulated_cache_size_limit = 128
  
- import dataclasses
-@@ -72,8 +73,21 @@ TModelInputForGPU = TypeVar('TModelInputForGPU', bound="ModelInputForGPU")
  
- # For now, bump up cache limits for recompilations during CUDA graph warmups.
- torch._dynamo.config.cache_size_limit = 128
--torch._dynamo.config.accumulated_cache_size_limit = 128
--
-+#torch._dynamo.config.accumulated_cache_size_limit = 128
-+
 +# --- FLAGSCALE MODIFICATION BEG ---
 +# Know more about FlagGems: https://github.com/FlagOpen/FlagGems
 +import os
 +if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
-+    try:
++     try:
++        print("Try to using FLAGGEMS...")
 +        import flag_gems
-+        flag_gems.enable()
++        flag_gems.enable(record=True, unused=["exponential_", "softmax", "masked_fill_", "index"], path="/tmp/gems_oplist.log.txt")
 +        logger.info("Successfully enabled flag_gems as default ops implementation.")
-+    except ImportError:
-+        logger.warning("Failed to import 'flag_gems'. Falling back to default implementation.")
-+    except Exception as e:
-+        logger.warning(f"Failed to enable 'flag_gems': {e}. Falling back to default implementation.")
++     except ImportError as e:
++        # Throw an exception directly if failure occurs
++        raise ImportError("Failed to import 'flag_gems'. Please install flag_gems or set USE_FLAGGEMS=false to disable it.") from e
++     except Exception as e:
++        # Throw an exception directly if failure occurs
++        raise RuntimeError(f"Failed to enable 'flag_gems': {e}. Please check your flag_gems installation or set USE_FLAGGEMS=false to disable it.") from e
 +# --- FLAGSCALE MODIFICATION END ---
- 
- @dataclass(frozen=True)
- class ModelInputForGPU(ModelRunnerInputBase):
-@@ -1699,7 +1713,8 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
-                     # layers.
-                     model_executable,
-                     model_input,
--                    kv_caches=kv_caches
-+                    kv_caches=kv_caches,
-+                    block_size=self.block_size,
-                 )
- 
-         multi_modal_kwargs = model_input.multi_modal_kwargs or {}
-@@ -1732,16 +1747,16 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
- 
-         # Sending KV cache in distributed KV cache transfer setting
-         # NOTE: the send operation is non-blocking
--        if self.need_send_kv(model_input, kv_caches):
--            get_kv_transfer_group().send_kv_caches_and_hidden_states(
--                # model_executable is used to know which layer the current
--                # worker is working on, so that we can send KV for only those
--                # layers.
--                model_executable,
--                model_input,
--                kv_caches,
--                hidden_or_intermediate_states,
--            )
-+        #if self.need_send_kv(model_input, kv_caches):
-+        #    get_kv_transfer_group().send_kv_caches_and_hidden_states(
-+        #        # model_executable is used to know which layer the current
-+        #        # worker is working on, so that we can send KV for only those
-+        #        # layers.
-+        #        model_executable,
-+        #        model_input,
-+        #        kv_caches,
-+        #        hidden_or_intermediate_states,
-+        #    )
- 
-         # Compute the logits in the last pipeline stage.
-         if not get_pp_group().is_last_rank:
-@@ -1809,6 +1824,179 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
-             output.hidden_states = hidden_states
- 
-         return [output]
-+    
-+    @torch.inference_mode()
-+    def execute_model_return_hidden(
-+        self,
-+        model_input: ModelInputForGPUWithSamplingMetadata,
-+        kv_caches: List[torch.Tensor],
-+        intermediate_tensors: Optional[IntermediateTensors] = None,
-+        num_steps: int = 1,
-+    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:
-+        if num_steps > 1:
-+            raise ValueError("num_steps > 1 is not supported in ModelRunner")
-+
-+        if self.lora_config:
-+            assert model_input.lora_requests is not None
-+            assert model_input.lora_mapping is not None
-+            self.set_active_loras(model_input.lora_requests,
-+                                  model_input.lora_mapping)
-+
-+        if self.prompt_adapter_config:
-+            assert model_input.prompt_adapter_requests is not None
-+            assert model_input.prompt_adapter_mapping is not None
-+            self.set_active_prompt_adapters(
-+                model_input.prompt_adapter_requests,
-+                model_input.prompt_adapter_mapping)
-+
-+        self.attn_state.begin_forward(model_input)
-+
-+        # Currently cuda graph is only supported by the decode phase.
-+        assert model_input.attn_metadata is not None
-+        prefill_meta = model_input.attn_metadata.prefill_metadata
-+        decode_meta = model_input.attn_metadata.decode_metadata
-+        # TODO(andoorve): We can remove this once all
-+        # virtual engines share the same kv cache.
-+        virtual_engine = model_input.virtual_engine
-+        if prefill_meta is None and decode_meta.use_cuda_graph:
-+            assert model_input.input_tokens is not None
-+            graph_batch_size = model_input.input_tokens.shape[0]
-+            model_executable = self.graph_runners[virtual_engine][
-+                graph_batch_size]
-+        else:
-+            model_executable = self.model
-+
-+        # Receive KV cache in distributed KV cache transfer setting
-+        # In disagg prefill setting, it will also recv hidden states and bypass
-+        # model forwarding
-+        # In KV cache database setting, it will change the model input so that
-+        # we can skip prefilling on tokens that successfully received KV caches
-+        # NOTE: The receive operation is blocking
-+        bypass_model_exec = False
-+        if self.need_recv_kv(model_input, kv_caches):
-+            hidden_or_intermediate_states, bypass_model_exec, model_input = \
-+                get_kv_transfer_group().recv_kv_caches_and_hidden_states(
-+                    # model is used to know which layer the current worker
-+                    # is working on, so that we can receive KV for only those
-+                    # layers.
-+                    model_executable,
-+                    model_input,
-+                    kv_caches=kv_caches
-+                )
-+
-+        multi_modal_kwargs = model_input.multi_modal_kwargs or {}
-+        seqlen_agnostic_kwargs = {
-+            "finished_requests_ids": model_input.finished_requests_ids,
-+            "request_ids_to_seq_ids": model_input.request_ids_to_seq_ids,
-+        } if self.has_inner_state else {}
-+        if (self.observability_config is not None
-+                and self.observability_config.collect_model_forward_time):
-+            model_forward_start = torch.cuda.Event(enable_timing=True)
-+            model_forward_end = torch.cuda.Event(enable_timing=True)
-+            model_forward_start.record()
-+
-+        if not bypass_model_exec:
-+            with set_forward_context(model_input.attn_metadata,
-+                                     self.vllm_config):
-+                hidden_or_intermediate_states = model_executable(
-+                    input_ids=model_input.input_tokens,
-+                    positions=model_input.input_positions,
-+                    kv_caches=kv_caches,
-+                    attn_metadata=model_input.attn_metadata,
-+                    intermediate_tensors=intermediate_tensors,
-+                    **MultiModalKwargs.as_kwargs(multi_modal_kwargs,
-+                                                 device=self.device),
-+                    **seqlen_agnostic_kwargs)
-+
-+        if (self.observability_config is not None
-+                and self.observability_config.collect_model_forward_time):
-+            model_forward_end.record()
-+
-+        # Sending KV cache in distributed KV cache transfer setting
-+        # NOTE: the send operation is non-blocking
-+        if self.need_send_kv(model_input, kv_caches):
-+            get_kv_transfer_group().send_kv_caches_and_hidden_states(
-+                # model_executable is used to know which layer the current
-+                # worker is working on, so that we can send KV for only those
-+                # layers.
-+                model_executable,
-+                model_input,
-+                kv_caches,
-+                hidden_or_intermediate_states,
-+            )
-+
-+        # Compute the logits in the last pipeline stage.
-+        if not get_pp_group().is_last_rank:
-+            if (self.is_driver_worker
-+                    and hidden_or_intermediate_states is not None
-+                    and isinstance(hidden_or_intermediate_states,
-+                                   IntermediateTensors)
-+                    and self.observability_config is not None
-+                    and self.observability_config.collect_model_forward_time):
-+                model_forward_end.synchronize()
-+                model_forward_time = model_forward_start.elapsed_time(
-+                    model_forward_end)
-+                orig_model_forward_time = 0.0
-+                if intermediate_tensors is not None:
-+                    orig_model_forward_time = intermediate_tensors.tensors.get(
-+                        "model_forward_time", torch.tensor(0.0)).item()
-+                hidden_or_intermediate_states.tensors["model_forward_time"] = (
-+                    torch.tensor(model_forward_time + orig_model_forward_time))
-+            return hidden_or_intermediate_states
-+
-+        logits = self.model.compute_logits(hidden_or_intermediate_states,
-+                                           model_input.sampling_metadata)
-+
-+        if model_input.input_positions.flatten()[0] == 0:
-+            logits_rt = self.model.compute_logits(hidden_or_intermediate_states, None)
-+        else:
-+            logits_rt = logits
-+        
-+        if not self.is_driver_worker:
-+            return []
 +
-+        if model_input.async_callback is not None:
-+            model_input.async_callback()
 +
-+        # Sample the next token.
-+        output: SamplerOutput = self.model.sample(
-+            logits=logits,
-+            sampling_metadata=model_input.sampling_metadata,
-+        )
-+        if (self.observability_config is not None
-+                and self.observability_config.collect_model_forward_time
-+                and output is not None):
-+            model_forward_end.synchronize()
-+            model_forward_time = model_forward_start.elapsed_time(
-+                model_forward_end)
-+            orig_model_forward_time = 0.0
-+            if intermediate_tensors is not None:
-+                orig_model_forward_time = intermediate_tensors.tensors.get(
-+                    "model_forward_time", torch.tensor(0.0)).item()
-+            # If there are multiple workers, we are still tracking the latency
-+            # from the start time of the driver worker to the end time of the
-+            # driver worker. The model forward time will then end up covering
-+            # the communication time as well.
-+            output.model_forward_time = (orig_model_forward_time +
-+                                         model_forward_time)
-+
-+        if self.return_hidden_states or True:
-+            # we only need to pass hidden states of most recent token
-+            # assert model_input.sampling_metadata is not None
-+            # indices = model_input.sampling_metadata.selected_token_indices
-+            # if model_input.is_prompt:
-+            #     hidden_states = hidden_or_intermediate_states.index_select(
-+            #         0, indices)
-+            #     output.prefill_hidden_states = hidden_or_intermediate_states
-+            # elif decode_meta.use_cuda_graph:
-+            #     hidden_states = hidden_or_intermediate_states[:len(indices)]
-+            # else:
-+            #     hidden_states = hidden_or_intermediate_states
-+
-+            # output.hidden_states = hidden_states
-+            output.hidden_states = logits_rt
-+
-+        return [output]
- 
-     def need_recv_kv(self, model_input, kv_caches) -> bool:
-         """Check if we need to receive kv-cache from the other worker.
-@@ -1858,8 +2046,8 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
-         is_prefill_run = prefill_meta is not None
- 
-         return self.vllm_config.kv_transfer_config.is_kv_producer and (
--            not is_profile_run) and is_prefill_run
--
-+            not is_profile_run) and is_prefill_run and (
-+            not self.vllm_config.kv_transfer_config.is_layerwise_kv_transfer)
- 
- # NOTE: this is nn.Module so the profiler can properly capture/group
- #  kernels calls made within the graph
+ @dataclass(frozen=True)
+ class ModelInputForGPU(ModelRunnerInputBase):
+     """
 diff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py
-index 90771e8ac..4eaad8449 100644
+index cc0cc855e..927475bbe 100644
 --- a/vllm/worker/multi_step_model_runner.py
 +++ b/vllm/worker/multi_step_model_runner.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import dataclasses
-@@ -32,7 +33,7 @@ if TYPE_CHECKING:
+@@ -34,7 +34,7 @@ if TYPE_CHECKING:
  logger = init_logger(__name__)
  
  MULTI_STEP_ATTENTION_BACKENDS = [
@@ -37560,120 +27819,15 @@ index 90771e8ac..4eaad8449 100644
  MULTI_STEP_CHUNKED_PREFILL_ATTENTION_BACKENDS = ["FLASH_ATTN", "FLASHINFER"]
  
 diff --git a/vllm/worker/worker_base.py b/vllm/worker/worker_base.py
-index 819b81fbf..d08e92966 100644
+index 0b37caa71..7db5f1496 100644
 --- a/vllm/worker/worker_base.py
 +++ b/vllm/worker/worker_base.py
-@@ -1,3 +1,4 @@
-+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
- # SPDX-License-Identifier: Apache-2.0
- 
- import dataclasses
-@@ -82,6 +83,7 @@ class WorkerBase(ABC):
+@@ -540,6 +540,8 @@ class WorkerWrapperBase:
+             # suppress the warning in `update_environment_variables`
+             del os.environ[key]
+         update_environment_variables(envs)
++        if key in os.environ:
++            os.environ["MACA_VISIBLE_DEVICES"] = os.environ[key]
+ 
+     def init_worker(self, all_kwargs: List[Dict[str, Any]]) -> None:
          """
-         raise NotImplementedError
- 
-+
-     def start_worker_execution_loop(self) -> None:
-         """Execute model loop in parallel worker.
- 
-@@ -94,6 +96,18 @@ class WorkerBase(ABC):
-                 if output is None:
-                     return None
- 
-+
-+    def start_worker_execution_loop_return_hidden(self) -> None:
-+        """Execute model loop in parallel worker.
-+
-+        You can stop the loop by executing a driver worker with an empty output.
-+        See `stop_remote_worker_execution_loop` for more details.
-+        """
-+        while True:
-+            output = self.execute_model_return_hidden(execute_model_req=None)
-+            if output is None:
-+                return None
-+    
-     @abstractmethod
-     def get_model(self) -> nn.Module:
-         raise NotImplementedError
-@@ -104,6 +118,13 @@ class WorkerBase(ABC):
-         execute_model_req: Optional[ExecuteModelRequest] = None
-     ) -> Optional[List[SamplerOutput]]:
-         raise NotImplementedError
-+    
-+    @abstractmethod
-+    def execute_model_return_hidden(
-+        self,
-+        execute_model_req: Optional[ExecuteModelRequest] = None
-+    ) -> Optional[List[SamplerOutput]]:
-+        raise NotImplementedError
- 
-     @abstractmethod
-     def get_cache_block_size_bytes(self) -> int:
-@@ -439,6 +460,67 @@ class LocalOrDistributedWorkerBase(WorkerBase):
- 
-         # output is List[SamplerOutput]
-         return output
-+    
-+    def execute_model_return_hidden(
-+        self,
-+        execute_model_req: Optional[ExecuteModelRequest] = None,
-+    ) -> Optional[List[SamplerOutput]]:
-+        """Executes at least one model step on the given sequences, unless no
-+        sequences are provided."""
-+        start_time = time.perf_counter()
-+
-+        inputs = self.prepare_input(execute_model_req)
-+        if inputs is None:
-+            return None
-+
-+        model_input, worker_input, kwargs = inputs
-+        num_steps = worker_input.num_steps
-+
-+        self.execute_worker(worker_input)
-+
-+        # If there is no input, we don't need to execute the model.
-+        if worker_input.num_seq_groups == 0:
-+            return []
-+
-+        intermediate_tensors = None
-+        orig_model_execute_time = 0.0
-+        if not get_pp_group().is_first_rank:
-+            intermediate_tensors = IntermediateTensors(
-+                get_pp_group().recv_tensor_dict(
-+                    all_gather_group=get_tp_group()))
-+            if (self.observability_config is not None
-+                    and self.observability_config.collect_model_execute_time):
-+                orig_model_execute_time = intermediate_tensors.tensors.get(
-+                    "model_execute_time", torch.tensor(0)).item()
-+
-+        output = self.model_runner.execute_model_return_hidden(
-+            model_input=model_input,
-+            kv_caches=self.kv_cache[worker_input.virtual_engine]
-+            if self.kv_cache is not None else None,
-+            intermediate_tensors=intermediate_tensors,
-+            num_steps=num_steps,
-+            **kwargs,
-+        )
-+
-+        model_execute_time = time.perf_counter() - start_time
-+        if not get_pp_group().is_last_rank:
-+            # output is IntermediateTensors
-+            if (self.observability_config is not None
-+                    and self.observability_config.collect_model_execute_time):
-+                output.tensors["model_execute_time"] = torch.tensor(
-+                    model_execute_time + orig_model_execute_time)
-+            get_pp_group().send_tensor_dict(output.tensors,
-+                                            all_gather_group=get_tp_group())
-+            return [None]
-+        if (self.observability_config is not None
-+                and self.observability_config.collect_model_execute_time
-+                and output is not None):
-+            for o in output:
-+                o.model_execute_time = (orig_model_execute_time +
-+                                        model_execute_time)
-+
-+        # output is List[SamplerOutput]
-+        return output
- 
-     def _execute_model_spmd(
-         self,

