diff --git a/examples/qwen3/conf/train/30b_a3b.yaml b/examples/qwen3/conf/train/30b_a3b.yaml
index 642c2cb4..c7fdf487 100644
--- a/examples/qwen3/conf/train/30b_a3b.yaml
+++ b/examples/qwen3/conf/train/30b_a3b.yaml
@@ -1,19 +1,14 @@
 system:
   no_shared_fs: ${experiment.runner.no_shared_fs}
-  num_workers: 16
-  tensor_model_parallel_size: 2
+  num_workers: 2
+  tensor_model_parallel_size: 1
   pipeline_model_parallel_size: 2
-  expert_model_parallel_size: 2
+  expert_model_parallel_size: 4
   context_parallel_size: 1
-  disable_bias_linear: true
-  reset_position_ids: True
-  reset_attention_mask: True
-  qk_layernorm: true
   sequence_parallel: true
   use_distributed_optimizer: true
-  overlap_grad_reduce: true
-  overlap_param_gather: true
-  finetune: false
+  # overlap_grad_reduce: true
+  # overlap_param_gather: true
   precision:
     bf16: true
     attention_softmax_in_fp32: true
@@ -38,8 +33,9 @@ model:
   transformer_impl: transformer_engine
   num_layers: 48
   hidden_size: 2048
-  kv_channels: 128
   num_attention_heads: 32
+  kv_channels: 128
+  group_query_attention: true
   num_query_groups: 4 # num_key_value_heads
   seq_length: 4096
   max_position_embeddings: 40960
@@ -48,14 +44,14 @@ model:
   rotary_base: 1000000
   swiglu: true
   normalization: RMSNorm
+  qk_layernorm: true
   init_method_std: 0.02
   attention_dropout: 0.0
   hidden_dropout: 0.0
-  clip_grad: 1.0
-  position_embedding_type: rope
   untie_embeddings_and_output_weights: true
   no_position_embedding: true
   no_rope_fusion: true
+  disable_bias_linear: true
 
   # moe args ===================
   ffn_hidden_size: 6144
@@ -71,24 +67,26 @@ model:
   seed: ${experiment.seed}
   # finetune: false
   micro_batch_size: 1
-  global_batch_size: 2048
+  global_batch_size: 128 #2048
   eval_iters: 0
-  train_samples: 244141056 #1T #29297664 #120B tokens
+  train_iters: 102400
 
   optimizer:
+    clip_grad: 1.0
     weight_decay: 0.1
     adam_beta1: 0.9
     adam_beta2: 0.95
     lr_scheduler:
       lr: 3.0e-3
       min_lr: 3.0e-4
-      lr_warmup_samples: 2048000
+      lr_warmup_fraction: 0.01
       lr_decay_style: WSD
       lr_wsd_decay_style: cosine
-      lr_wsd_decay_samples: 2048
-
+      lr_wsd_decay_iters: 10
 
 data:
+  reset_position_ids: True
+  reset_attention_mask: True
   data_path: /path
   split: 1
   no_mmap_bin_files: true

