diff --git a/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml b/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
index a8957bc3..a90ea8b6 100644
--- a/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
+++ b/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
@@ -1,15 +1,14 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /share/Qwen2.5-VL-32B-Instruct # should be customized
-    served_model_name: qwenvl32-nv-flagos
-    tensor_parallel_size: 8
-    max_model_len: 32768
-    pipeline_parallel_size: 1
-    max_num_seqs: 8 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
-    gpu_memory_utilization: 0.9
-    limit_mm_per_prompt: image=18 # should be customized, 18 images/request is enough for most scenarios
-    port: 9010
+    model: /share/project/zhaodeming/Qwen2.5-VL-32B-Instruct # should be customized
+    served_model_name: qwenvl32
+    swap_space: 16
+    port: 7925
     trust_remote_code: true
-    enforce_eager: true # better compare to FlagGems
-    enable_chunked_prefill: true
+    block_size: 16
+    tensor_parallel_size: 2
+    max_model_len: 4096
+    max_seq_len_to_capture: 4096
+    max_num_batched_tokens: 65536
+    dtype: bfloat16

