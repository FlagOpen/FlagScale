diff --git a/examples/qwen2_5_vl/conf/serve.yaml b/examples/qwen2_5_vl/conf/serve.yaml
index 4f460cb3..82e9d4c5 100644
--- a/examples/qwen2_5_vl/conf/serve.yaml
+++ b/examples/qwen2_5_vl/conf/serve.yaml
@@ -11,7 +11,7 @@ experiment:
   runner:
     ssh_port: 22
   envs:
-    CUDA_DEVICE_MAX_CONNECTIONS: 1
+    VLLM_LATENCY_DEBUG: true
 action: run
 hydra:
   run:
diff --git a/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml b/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
index a8957bc3..a90ea8b6 100644
--- a/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
+++ b/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
@@ -1,15 +1,14 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /share/Qwen2.5-VL-32B-Instruct # should be customized
-    served_model_name: qwenvl32-nv-flagos
-    tensor_parallel_size: 8
-    max_model_len: 32768
-    pipeline_parallel_size: 1
-    max_num_seqs: 8 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
-    gpu_memory_utilization: 0.9
-    limit_mm_per_prompt: image=18 # should be customized, 18 images/request is enough for most scenarios
-    port: 9010
+    model: /share/project/zhaodeming/Qwen2.5-VL-32B-Instruct # should be customized
+    served_model_name: qwenvl32
+    swap_space: 16
+    port: 7925
     trust_remote_code: true
-    enforce_eager: true # better compare to FlagGems
-    enable_chunked_prefill: true
+    block_size: 16
+    tensor_parallel_size: 2
+    max_model_len: 4096
+    max_seq_len_to_capture: 4096
+    max_num_batched_tokens: 65536
+    dtype: bfloat16
diff --git a/examples/qwen3_moe/conf/serve.yaml b/examples/qwen3_moe/conf/serve.yaml
new file mode 100644
index 00000000..71ce27b3
--- /dev/null
+++ b/examples/qwen3_moe/conf/serve.yaml
@@ -0,0 +1,23 @@
+defaults:
+  - _self_
+  - serve: 235b
+experiment:
+  exp_name: qwen3_235b_a22b
+  exp_dir: outputs/${experiment.exp_name}
+  task:
+    type: serve
+  deploy:
+    use_fs_serve: false
+  runner:
+    ssh_port: 22
+  envs:
+    VLLM_LATENCY_DEBUG: true
+    CNCL_CNPX_DOMAIN_ENABLE: 0
+    PYTORCH_MLU_ALLOC_CONF: "expandable_segments:True"
+    EXPERT_PARALLEL_EN: true
+    VLLM_GRAPH_DEBUG: false
+    VLLM_AVG_MOE_EN: 1
+action: run
+hydra:
+  run:
+    dir: ${experiment.exp_dir}/hydra
diff --git a/examples/qwen3_moe/conf/serve/235b.yaml b/examples/qwen3_moe/conf/serve/235b.yaml
new file mode 100644
index 00000000..01213cfe
--- /dev/null
+++ b/examples/qwen3_moe/conf/serve/235b.yaml
@@ -0,0 +1,16 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /share/project/zhaodeming/Qwen3-235B-A22B # should be customized
+    served_model_name: qwen3_moe
+    swap_space: 16
+    tensor_parallel_size: 8
+    moe_ep_size: 8
+    port: 7925
+    trust_remote_code: true
+    max_model_len: 4096
+    num_scheduler_steps: 10
+    max_seq_len_to_capture: 4096
+    max_num_batched_tokens: 65536
+    tokenizer: /share/project/zhaodeming/Qwen3-235B-A22B
+    dtype: bfloat16
diff --git a/flagscale/inference/inference_aquila.py b/flagscale/inference/inference_aquila.py
index 246c5a23..8a9e1ed3 100644
--- a/flagscale/inference/inference_aquila.py
+++ b/flagscale/inference/inference_aquila.py
@@ -44,6 +44,7 @@ def inference(cfg):
         print(f"{output.prompt=}")
         print(f"{output.outputs[0].text=}")
         print(f"{output.outputs[0].token_ids=}")
+    print("#" * 50)
 
 
 if __name__ == "__main__":
