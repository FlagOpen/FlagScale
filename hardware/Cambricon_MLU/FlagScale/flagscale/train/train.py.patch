diff --git a/flagscale/train/train.py b/flagscale/train/train.py
index 3eb11e71..e1778c88 100644
--- a/flagscale/train/train.py
+++ b/flagscale/train/train.py
@@ -10,7 +10,7 @@ import logging
 import math
 import os
 import sys
-from typing import List
+from typing import List, Optional
 
 import torch.distributed
 from megatron.training.log_handler import CustomHandler
@@ -33,6 +33,12 @@ try:
 except ImportError:
     has_nvidia_modelopt = False
 
+try:
+    from nvidia_resiliency_ext.inprocess import CallWrapper
+except ImportError:
+    CallWrapper = type(None)
+
+
 from megatron.core import mpu, tensor_parallel
 from megatron.core.utils import (
     check_param_hashes_across_dp_replicas,
@@ -116,6 +122,7 @@ from megatron.training.global_vars import (
     get_tensorboard_writer,
     get_wandb_writer,
     get_one_logger,
+    get_energy_monitor,
 )
 from megatron.training import one_logger_utils
 
@@ -130,6 +137,10 @@ from flagscale.train.theoretical_memory_usage import report_theoretical_memory a
 
 stimer = StragglerDetector()
 
+from megatron.core.msc_utils import MultiStorageClientFeature, open_file
+
+from flagscale.train.peft.peft import PEFT
+from flagscale.train.peft.lora import LoRA
 
 def destroy_global_state():
     destroy_global_vars()
@@ -484,12 +495,12 @@ def num_floating_point_operations_fs(args, batch_size):
                 * args.hidden_size
             )
         else:
-            # Attention projection size.
-            query_projection_size = args.kv_channels * args.num_attention_heads
-            kv_projection_size = args.kv_channels * args.num_query_groups
             # Group Query Attention.
             if not args.group_query_attention:
                 args.num_query_groups = args.num_attention_heads
+            # Attention projection size.
+            query_projection_size = args.kv_channels * args.num_attention_heads
+            kv_projection_size = args.kv_channels * args.num_query_groups
             # qkv proj
             num_flops_attn = (
                 2
@@ -513,7 +524,11 @@ def num_floating_point_operations_fs(args, batch_size):
             )
 
         # Part 2: MLP or MoE =====================================================================
-        moe_ffn_hidden_size = args.moe_ffn_hidden_size if args.moe_ffn_hidden_size is not None else args.ffn_hidden_size
+        moe_ffn_hidden_size = (
+            args.moe_ffn_hidden_size
+            if args.moe_ffn_hidden_size is not None
+            else args.ffn_hidden_size
+        )
         shared_expert_ffn_hidden_size = (
             0
             if args.moe_shared_expert_intermediate_size is None
@@ -539,11 +554,16 @@ def num_floating_point_operations_fs(args, batch_size):
                 moe_layer_pattern = args.moe_layer_freq
             else:
                 raise RuntimeError("Illegal --moe-layer-freq argument provided!")
-            assert len(moe_layer_pattern) == args.num_layers
+            assert len(moe_layer_pattern) == args.num_layers, (
+                f"Invalid length of moe_layer_pattern: {len(moe_layer_pattern)}, "
+                f"expected {args.num_layers}, "
+                f"current moe layer pattern: {args.moe_layer_freq}"
+            )
             num_moe_layers = sum(moe_layer_pattern)  # Number of 1s in `moe_layer_pattern`.
             num_dense_layers = args.num_layers - num_moe_layers
             num_experts_routed_to = args.moe_router_topk
             num_experts = args.num_experts
+            last_layer_is_moe = moe_layer_pattern[-1]
 
         num_flops_dense_mlp = (
             4 # mlp (two linear)
@@ -584,8 +604,16 @@ def num_floating_point_operations_fs(args, batch_size):
         )
 
         # Part3: MTP =============================================================================
+        if args.mtp_num_layers is not None:
+            mtp_num_layers = args.mtp_num_layers
+            num_moe_layers += last_layer_is_moe * mtp_num_layers
+            num_dense_layers += (1 - last_layer_is_moe) * mtp_num_layers
+            num_layers = args.num_layers + mtp_num_layers
+        else:
+            mtp_num_layers = 0
+            num_layers = args.num_layers
+
         num_flops_mtp = 0
-        mtp_num_layers = 0 if not getattr(args, "mtp_num_layers", None) else args.mtp_num_layers
         if mtp_num_layers > 0:
             num_flops_mtp = (
                 # MTP eh norm + final nrom
@@ -633,7 +661,7 @@ def get_start_time_from_progress_log():
     def _get_field(string, type):
         return type(string.split(': ')[1])
 
-    with open(progress_log_filename, 'r') as f:
+    with open_file(progress_log_filename, 'r') as f:
         for line in f:
             line = line.strip()
             line_tokens = line.split('\t')
@@ -857,7 +885,9 @@ def pretrain(
     get_embedding_ranks=None,
     get_position_embedding_ranks=None,
     non_loss_data_func=None,
-    extra_valid_dataset_provider=None,
+    store=None,
+    inprocess_call_wrapper: Optional[CallWrapper] = None,
+    extra_valid_dataset_provider=None, ###### FlagScale Add ##########
 ):
     """Main training program.
 
@@ -890,14 +920,23 @@ def pretrain(
         get_position_embedding_ranks (TODO):
         non_loss_data_func (callable): A custom function to call during evaluation.
             It can run e.g. benchmarks.
+        store: an optional instance of torch.distributed.Store, to be used by
+            torch.distributed.init_process_group
+        inprocess_call_wrapper: an optional instance of inprocess.CallWrapper,
+            it is automatically injected when in-process restart is in use
     """
 
+    if inprocess_call_wrapper is not None:
+        iteration = inprocess_call_wrapper.iteration
+        store = torch.distributed.PrefixStore(str(iteration), store)
+
     # Initalize and get arguments, timers, and Tensorboard writer.
     initialize_megatron(
         extra_args_provider=extra_args_provider,
         args_defaults=args_defaults,
         get_embedding_ranks=get_embedding_ranks,
         get_position_embedding_ranks=get_position_embedding_ranks,
+        store=store,
     )
 
     args = get_args()
@@ -919,10 +958,12 @@ def pretrain(
     # This will be closer to what scheduler will see (outside of
     # image ... launches.
     global _TRAIN_START_TIME
+    ########## FlagScale Begin ##########
     if "cpu:gloo" == torch.distributed.get_backend():
         start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device='cpu')
     else:
         start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device='cuda')
+    ########## FlagScale Begin ##########
     torch.distributed.all_reduce(start_time_tensor, op=torch.distributed.ReduceOp.MIN)
     _TRAIN_START_TIME = start_time_tensor.item()
 
@@ -998,11 +1039,20 @@ def pretrain(
         valid_data_iterator = []
         test_data_iterator = []
         for i in range(len(model)):
-            mpu.set_virtual_pipeline_model_parallel_rank(i)
             iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
             train_data_iterator.append(iterators[0])
             valid_data_iterator.append(iterators[1])
             test_data_iterator.append(iterators[2])
+    elif args.use_dualpipev:
+        train_data_iterator = []
+        valid_data_iterator = []
+        test_data_iterator = []
+        for _ in range(2):
+            iterators = build_train_valid_test_data_iterators(
+                train_valid_test_dataset_provider)
+            train_data_iterator.append(iterators[0])
+            valid_data_iterator.append(iterators[1])
+            test_data_iterator.append(iterators[2])
     else:
         train_data_iterator, valid_data_iterator, test_data_iterator = (
             build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
@@ -1056,7 +1106,7 @@ def pretrain(
 
         print_datetime('after training is done')
 
-        if not args.auto_tune:
+        if not args.auto_tune: ########## FlagScale Add ##########
             if args.save and iteration != 0 and iteration % args.save_interval != 0:
                 save_checkpoint(
                     iteration,
@@ -1118,6 +1168,12 @@ def pretrain(
                 mpu.set_virtual_pipeline_model_parallel_rank(i)
                 extra_iterators = build_extra_valid_data_iterators(extra_valid_dataset_provider)
                 extra_valid_data_iterator.append(extra_iterators)
+        elif args.use_dualpipev:
+            extra_valid_data_iterator = []
+            for _ in range(2):
+                extra_iterators = build_extra_valid_data_iterators(
+                    extra_valid_dataset_provider)
+                extra_valid_data_iterator.append(extra_iterators)
         else:
             extra_valid_data_iterator = (
                 build_extra_valid_data_iterators(extra_valid_dataset_provider)
@@ -1205,18 +1261,39 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
                 ), "Interleaved schedule not supported for model with encoder on separate PP rank"
             model = []
             for i in range(args.virtual_pipeline_model_parallel_size):
-                mpu.set_virtual_pipeline_model_parallel_rank(i)
                 # Set pre_process and post_process only after virtual rank is set.
-                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False)
-                post_process = mpu.is_pipeline_last_stage(ignore_virtual=False)
+                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False, vp_stage=i)
+                post_process = mpu.is_pipeline_last_stage(ignore_virtual=False, vp_stage=i)
                 this_model = model_provider_func(
                     pre_process=pre_process, post_process=post_process, vp_stage=i)
                 this_model.model_type = model_type
                 this_model.vp_stage = i
                 model.append(this_model)
+        elif args.use_dualpipev:
+            model = []
+
+            pre_process, post_process = False, False
+            if mpu.is_pipeline_first_stage():
+                pre_process = True
+
+            first_model = model_provider_func(
+                pre_process=pre_process,
+                post_process=post_process,
+                is_dualpipev_first_chunk=True,
+            )
+            first_model.model_type = model_type
+            model.append(first_model)
+
+            second_model = model_provider_func(
+                pre_process=post_process,
+                post_process=pre_process,
+                is_dualpipev_first_chunk=False,
+            )
+            second_model.model_type = model_type
+            model.append(second_model)
         else:
-            pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False)
-            post_process = mpu.is_pipeline_last_stage(ignore_virtual=False)
+            pre_process = mpu.is_pipeline_first_stage()
+            post_process = mpu.is_pipeline_last_stage()
             add_encoder = True
             add_decoder = True
             if model_type == ModelType.encoder_and_decoder:
@@ -1248,6 +1325,25 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
     if not isinstance(model, list):
         model = [model]
 
+    ######## FLAGSCALE BEGIN ########
+    # 1) init
+    config = get_model_config(model[0])
+    if config.peft_type is not None:
+        peft = PEFT.from_config(config)
+        unwrapped_model = unwrap_model(model)
+        if not isinstance(unwrapped_model, list):
+            unwrapped_model = [unwrapped_model]
+
+        for model_module in unwrapped_model:
+            # 2) transform
+            peft.apply_transform(model_module)
+            # 3) freeze model
+            peft.freeze_model(model_module)
+            # 4) load state dict
+            peft.load_state_dict_pre_hooks(model_module)
+            peft.load_state_dict_post_hooks(model_module)
+    ######## FLAGSCALE END   ########
+
     # Set tensor model parallel attributes if not set.
     # Only parameters that are already tensor model parallel have these
     # attributes set for them. We should make sure the default attributes
@@ -1260,7 +1356,7 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
     num_parameters = sum(
         [sum([p.nelement() for p in model_module.parameters()]) for model_module in model]
     )
-    if mpu.get_data_parallel_rank() == 0:
+    if mpu.get_data_parallel_rank() == 0 and mpu.get_context_parallel_rank() == 0:
         print(
             ' > number of parameters on (tensor, pipeline) '
             'model parallel rank ({}, {}): {}'.format(
@@ -1397,7 +1493,7 @@ def get_optimizer_param_scheduler(optimizer):
             lr_warmup_steps = args.lr_warmup_samples
     else:
         raise Exception('either train-iters or train-samples should be provided.')
-
+    ########## FlagScale Begin ##########
     stablelm2_scheduler_config = None
     if args.lr_decay_style == 'stablelm2-scheduler':
         stablelm2_scheduler_config = StableLM2SchedulerConfig(
@@ -1409,7 +1505,7 @@ def get_optimizer_param_scheduler(optimizer):
           cosine_max_lr=args.lr_decay_stablelm2_cosine_max_lr,
           cosine_period_samples=args.lr_decay_stablelm2_cosine_period_samples,
           decay_samples=args.lr_decay_stablelm2_decay_samples)
-
+    ########## FlagScale End ##########
     opt_param_scheduler = OptimizerParamScheduler(
         optimizer,
         init_lr=args.lr_warmup_init,
@@ -1447,7 +1543,7 @@ def setup_model_and_optimizer(
 
     model = get_model(model_provider_func, model_type)
     unwrapped_model = unwrap_model(model)
-
+    ########## FlagScale Begin ##########
     config = None
     para_ctx = get_parallel_context()
     if para_ctx is not None:
@@ -1459,6 +1555,7 @@ def setup_model_and_optimizer(
             if hasattr(args, f.name):
                 kwargs[f.name] = getattr(args, f.name)
         config = OptimizerConfig(**kwargs)
+    ########## FlagScale End ##########
     config.timers = timers
     optimizer = get_megatron_optimizer(
         config,
@@ -1478,13 +1575,25 @@ def setup_model_and_optimizer(
             "Upcycling should only be set for the first run when converting the dense model. "
             "All subsequent runs should remove this flag. "
         )
+        # before changing moe related global args, save them in local variables
         num_experts = args.num_experts
-        args.num_experts = None
         expert_model_parallel_size = args.expert_model_parallel_size
+        moe_ffn_hidden_size = args.ffn_hidden_size
+
+        # set dense model related args in to global args before getting dense model
+        args.num_experts = None
         args.expert_model_parallel_size = 1
+        args.ffn_hidden_size = moe_ffn_hidden_size * args.moe_upcycling_granularity
+
+        # get dense model
         dense_model_for_upcycling = get_model(model_provider_func, model_type)
+
+        # recover moe upcycling related args in global args before executing upcycling
         args.num_experts = num_experts
         args.expert_model_parallel_size = expert_model_parallel_size
+        args.ffn_hidden_size = moe_ffn_hidden_size
+
+        # execute upcycling
         _, args.num_floating_point_operations_so_far = upcycling_utils.load_and_upcycle_model(
             load_checkpoint,
             unwrapped_model,
@@ -1685,20 +1794,38 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
         if args.use_distributed_optimizer and args.overlap_param_gather:
             cuda_graph_set_manual_hooks(model)
 
-    if mpu.is_pipeline_last_stage(ignore_virtual=True):
+    is_last_stage = mpu.is_pipeline_last_stage(ignore_virtual=True)
+    if args.use_dualpipev:
+        is_last_stage = mpu.is_pipeline_first_stage(ignore_virtual=True)
+    if is_last_stage:
         # Average loss across microbatches.
         loss_reduced = {}
+
         for key in losses_reduced[0].keys():
             val = [x[key].view(-1) for x in losses_reduced]
             if val[0].numel() == 2:
-                # there is one dict per microbatch. in new reporting, we average
-                # over the total number of tokens across the global batch.
-                val = torch.vstack(val).sum(dim=0)
-                torch.distributed.all_reduce(
-                    val,
-                    group=mpu.get_data_parallel_group(with_context_parallel=True)
-                )
-                loss_reduced[key] = val[0] / val[1]
+                if args.sft:
+                    # in mcore the normalization happens on micro batch instead of global
+                    val = torch.vstack(val)
+                    val = val[:, 0] / val[:, 1]
+                    val = val.mean()
+                    torch.distributed.all_reduce(
+                        val,
+                        group=mpu.get_data_parallel_group(with_context_parallel=True)
+                    )
+                    val /= torch.distributed.get_world_size(
+                        group=mpu.get_data_parallel_group(with_context_parallel=True)
+                    )
+                    loss_reduced[key] = val
+                else:
+                    # there is one dict per microbatch. in new reporting, we average
+                    # over the total number of tokens across the global batch.
+                    val = torch.vstack(val).sum(dim=0)
+                    torch.distributed.all_reduce(
+                        val,
+                        group=mpu.get_data_parallel_group(with_context_parallel=True)
+                    )
+                    loss_reduced[key] = val[0] / val[1]
             elif val[0].numel() == 1:
                 # legacy behavior, we average over the number of microbatches
                 val = torch.cat(val).mean()
@@ -1736,6 +1863,7 @@ def training_log(
     writer = get_tensorboard_writer()
     wandb_writer = get_wandb_writer()
     one_logger = get_one_logger()
+    energy_monitor = get_energy_monitor()
 
     # Advanced, skipped, and Nan iterations.
     advanced_iters_key = 'advanced iterations'
@@ -1950,6 +2078,17 @@ def training_log(
                     writer.add_scalar('throughput', throughput, iteration)
                 if wandb_writer:
                     wandb_writer.log({'throughput': throughput}, iteration)
+        if args.log_energy:
+            energy = (energy_monitor.lap() / total_iterations) / args.world_size
+            power = energy / elapsed_time_per_iteration
+            log_string += f' energy per GPU (J/iter/GPU): {energy:.1f} |'
+            log_string += f' power per GPU (W/GPU): {power:.1f} |'
+            if writer:
+                writer.add_scalar('iter-energy/gpu', energy, iteration)
+                writer.add_scalar('power/gpu', power, iteration)
+            if wandb_writer:
+                wandb_writer.log({'iter-energy/gpu': energy}, iteration)
+                wandb_writer.log({'power/gpu': power}, iteration)
         # Decoupled_learning_rate should be not None only on first and last pipeline stage.
         log_string += f' learning rate: {learning_rate:.6E} |'
         if args.decoupled_lr is not None and (
@@ -1984,7 +2123,7 @@ def training_log(
         total_loss_dict[skipped_iters_key] = 0
         total_loss_dict[nan_iters_key] = 0
         print_rank_last(log_string)
-        if not args.auto_tune:
+        if not args.auto_tune: ########## FlagScale Add ##########
             if report_memory_flag:
                 # Report memory after optimizer state has been initialized.
                 if torch.distributed.get_rank() == 0:
@@ -1993,6 +2132,8 @@ def training_log(
                 report_memory(f'(after {iteration} iterations)')
                 report_memory_flag = False
         else:
+            num_microbatches = get_num_microbatches()
+            fs_report_theoretical_memory(args, num_microbatches=num_microbatches, verbose=True)
             report_memory(f'(after {iteration} iterations)')
             report_memory_flag = False
         timers.log(timers_to_log, normalizer=args.log_interval)
@@ -2057,9 +2198,12 @@ def save_checkpoint_and_time(
 ):
     args = get_args()
     timers = get_timers()
+    energy_monitor = get_energy_monitor()
 
     # Stop timer to get accurate train interval time and exclude checkpointing duration
     timers('interval-time').stop()
+    energy_monitor.pause()
+
     # Extra barrier is added to make sure all ranks report the max time.
     timer_key = 'save-checkpoint-non-persistent' if non_persistent_ckpt else 'save-checkpoint'
     timers(timer_key, log_level=0).start(barrier=True)
@@ -2095,6 +2239,7 @@ def save_checkpoint_and_time(
         )
 
     # Recover timing
+    energy_monitor.resume()
     timers('interval-time', log_level=0).start(barrier=True)
 
 
@@ -2138,6 +2283,7 @@ def post_training_step_callbacks(
         check_adlr_autoresume_termination(iteration, model, optimizer, opt_param_scheduler)
 
     # Profiling.
+    torch.cuda.nvtx.range_pop() # for iteratrion
     if (
         args.profile
         and iteration == args.profile_step_end
@@ -2278,6 +2424,7 @@ def train(
     """Training function: run train_step desired number of times, run validation, checkpoint."""
     args = get_args()
     timers = get_timers()
+    energy_monitor = get_energy_monitor()
     one_logger = get_one_logger()
 
     if args.run_workload_inspector_server:
@@ -2345,6 +2492,10 @@ def train(
             config.param_sync_func = config.param_sync_func[0]
     config.finalize_model_grads_func = finalize_model_grads
 
+    if args.log_energy:
+        energy_monitor.setup()
+        energy_monitor.resume()
+
     timers('interval-time', log_level=0).start(barrier=True)
     print_datetime('before the start of training step')
     report_memory_flag = True
@@ -2459,7 +2610,7 @@ def train(
             elif iteration == args.profile_step_start:
                 torch.cuda.cudart().cudaProfilerStart()
                 torch.autograd.profiler.emit_nvtx(record_shapes=True).__enter__()
-
+        torch.cuda.nvtx.range_push(f"iteration num {iteration}") # NOTE(lizhiyu): add iteration num tag for profile
         ft_integration.on_checkpointing_start()
         maybe_finalize_async_save(blocking=False)
         ft_integration.on_checkpointing_end(is_async_finalization=True)
@@ -2627,6 +2778,8 @@ def train(
 
         # Evaluation.
         if args.eval_interval and iteration % args.eval_interval == 0 and args.do_valid:
+            if args.log_energy:
+                energy_monitor.pause()
             timers('interval-time').stop()
             if should_disable_forward_pre_hook(args):
                 disable_forward_pre_hook(model)
@@ -2660,8 +2813,10 @@ def train(
                 enable_forward_pre_hook(model)
                 pre_hook_enabled = True
             timers('interval-time', log_level=0).start(barrier=True)
+            if args.log_energy:
+                energy_monitor.resume()
 
-
+        ######## FlagScale Begin ########
         # Extra Evaluation =====================================================================
         if args.extra_eval_interval and iteration % args.extra_eval_interval == 0:
             # NOTE(zhaoyinglia): Must rebuild the dataloaders for extra validation here,
@@ -2673,6 +2828,12 @@ def train(
                     mpu.set_virtual_pipeline_model_parallel_rank(i)
                     extra_iterators = build_extra_valid_data_iterators(extra_valid_dataset_provider)
                     extra_valid_data_iterator.append(extra_iterators)
+            elif args.use_dualpipev:
+                extra_valid_data_iterator = []
+                for _ in range(2):
+                    extra_iterators = build_extra_valid_data_iterators(
+                        extra_valid_dataset_provider)
+                    extra_valid_data_iterator.append(extra_iterators)
             else:
                 extra_valid_data_iterator = (
                     build_extra_valid_data_iterators(extra_valid_dataset_provider)
@@ -2717,7 +2878,7 @@ def train(
                     pre_hook_enabled = True
                 timers('interval-time', log_level=0).start(barrier=True)
         # =======================================================================================
-
+        ######## FlagScale End ########
         # Miscellaneous post-training-step functions (e.g., FT heartbeats, GC).
         # Some of these only happen at specific iterations.
         post_training_step_callbacks(
@@ -2761,6 +2922,12 @@ def train(
     if args.enable_ft_package and ft_integration.get_rank_monitor_client() is not None:
         ft_integration.get_rank_monitor_client().shutdown_workload_monitoring()
 
+    if args.log_energy:
+        energy_monitor.lap()
+        total_energy = energy_monitor.get_total()
+        print_rank_0(f"Total training energy (GPU): {total_energy / 1e6} MJ")
+        energy_monitor.shutdown()
+
     # If any exit conditions (signal handler, duration, iterations) have been reached, exit.
     if should_exit:
         wandb_writer = get_wandb_writer()
@@ -2809,21 +2976,24 @@ def evaluate(
     eval_batch_size = args.global_batch_size
     eval_num_microbatches = eval_batch_size // (args.micro_batch_size * args.data_parallel_size)
 
+    ######## FlagScale Begin ########
     if extra_valid_index is not None:
         assert getattr(args, "extra_eval_iters_list") is not None, \
             "extra_eval_iters_list must be provided if extra_valid_index is not None"
         eval_iters = args.extra_eval_iters_list[extra_valid_index]
     else:
         eval_iters = args.eval_iters
-
+    ######## FlagScale End ########
     with torch.no_grad():
         iteration = 0
         if verbose:
+            ######## FlagScale Begin ########
             print_rank_0(f'Evaluating on {eval_iters * eval_batch_size} samples')
         while iteration < eval_iters:
             iteration += 1
             if verbose:
                 print_rank_0(f'Evaluating iter {iteration}/{eval_iters}')
+            ######## FlagScale End ########
 
             forward_backward_func = get_forward_backward_func()
             # Don't care about timing during evaluation
@@ -2854,13 +3024,29 @@ def evaluate(
                             [0.0, 0.0], dtype=torch.float
                         ).cuda()
                     val = [x[key].view(-1) for x in loss_dicts]
+
                     if val[0].numel() == 2:
-                        val = torch.vstack(val).sum(dim=0)
-                        torch.distributed.all_reduce(
-                            val,
-                            group=mpu.get_data_parallel_group(with_context_parallel=True)
-                        )
-                        total_loss_dict[key] += val
+                        if args.sft:
+                            # normalize over micro batch instead of global
+                            val = torch.vstack(val)
+                            val = val[:, 0] / val[:, 1]
+                            val = val.mean()
+                            torch.distributed.all_reduce(
+                                val,
+                                group=mpu.get_data_parallel_group(with_context_parallel=True)
+                            )
+                            val /= torch.distributed.get_world_size(
+                                group=mpu.get_data_parallel_group(with_context_parallel=True)
+                            )
+                            total_loss_dict[key][0] += val
+                            total_loss_dict[key][1] += 1
+                        else :
+                            val = torch.vstack(val).sum(dim=0)
+                            torch.distributed.all_reduce(
+                                val,
+                                group=mpu.get_data_parallel_group(with_context_parallel=True)
+                            )
+                            total_loss_dict[key] += val
                     elif val[0].numel() == 1:
                         val = torch.cat(val).sum()
                         total_loss_dict[key][0] += val

