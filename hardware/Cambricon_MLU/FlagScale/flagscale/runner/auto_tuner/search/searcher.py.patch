diff --git a/flagscale/runner/auto_tuner/search/searcher.py b/flagscale/runner/auto_tuner/search/searcher.py
index 5f2cfa89..76c95e38 100644
--- a/flagscale/runner/auto_tuner/search/searcher.py
+++ b/flagscale/runner/auto_tuner/search/searcher.py
@@ -71,6 +71,24 @@ _DEFAULT_SERVE_TUNE_SPACE = {
 }
 
 
+def get_first_last_num_layers_for_pp(num_layers, pp_size):
+    if pp_size == 2:
+        half = num_layers // 2
+        return half, num_layers - half
+
+    avg_layers = num_layers / pp_size
+    middle_layers = round(avg_layers)
+    remaining_layers = num_layers - middle_layers * (pp_size - 2)
+    if remaining_layers < 2:
+        middle_layers -= 1
+        remaining_layers = num_layers - middle_layers * (pp_size - 2)
+
+    first_num_layers = remaining_layers // 2
+    last_num_layers = remaining_layers - first_num_layers
+
+    return first_num_layers, last_num_layers
+
+
 class Searcher:
 
     def __init__(self, config):
@@ -108,6 +126,9 @@ class Searcher:
 
             for strategy in self.strategies:
                 strategy["memory_model"] = default_model(strategy, self.config)
+                strategy["gpu_utilization"] = self.config.experiment.auto_tuner.memory_model.get(
+                    "gpu_utilization", [0.2, 0.8]
+                )
                 self.logger.info(
                     "Searcher: strategy is {}, memory model is {} MB".format(
                         strategy, strategy["memory_model"]
@@ -132,6 +153,7 @@ class Searcher:
                 "use_distributed_optimizer",
                 "tensor_model_parallel_size",
                 "pipeline_model_parallel_size",
+                "expert_model_parallel_size",
                 "recompute_method",
                 "recompute_num_layers",
                 "context_parallel_size",
@@ -157,6 +179,7 @@ class Searcher:
         priority = config.experiment.auto_tuner.algo.get("priority", None)
         if config.experiment.auto_tuner.platform.get("airs_switch", False):
             priority = "memory"
+
         # Set data parallel degree
         space["data_parallel_size"] = (
             [i for i in range(1, cards + 1)]
@@ -223,6 +246,7 @@ class Searcher:
             else config.experiment.auto_tuner.space.use_recompute
         )
         self._sort("use_recompute", space["use_recompute"], priority)
+
         # Set recompute method
         space["recompute_method"] = (
             ["uniform", "block"]
@@ -269,14 +293,18 @@ class Searcher:
         self._sort("context_parallel_size", space["context_parallel_size"], priority)
 
         # Set expert parallel degree
-        # NOTE: Expert parallel degree is not supported now
-        space["expert_model_parallel_size"] = (
-            [1]
-            if "expert_model_parallel_size" not in config.experiment.auto_tuner.space
-            or config.experiment.auto_tuner.space.expert_model_parallel_size == "auto"
-            else config.experiment.auto_tuner.space.expert_model_parallel_size
-        )
+        if not hasattr(config.train.model, "num_experts"):
+            space["expert_model_parallel_size"] = [1]
+        else:
+            space["expert_model_parallel_size"] = (
+                [i for i in range(1, cards + 1)]
+                if "expert_model_parallel_size" not in config.experiment.auto_tuner.space
+                or config.experiment.auto_tuner.space.expert_model_parallel_size == "auto"
+                else config.experiment.auto_tuner.space.expert_model_parallel_size
+            )
+
         self._sort("expert_model_parallel_size", space["expert_model_parallel_size"], priority)
+
         return space
 
     def build_strategies(self, space, config):
@@ -298,8 +326,9 @@ class Searcher:
 
     def _product_parallel_dims(self, space, config):
         # Avoid space explosion after product
-        product_parallelism_dims = []
         cards = config.experiment.auto_tuner.cards
+
+        product_parallelism_dims = []
         for data_parallel_size in space["data_parallel_size"]:
             dims = {}
             if not divisible(cards, data_parallel_size):
@@ -308,17 +337,20 @@ class Searcher:
             gbs = config.train.model.global_batch_size
             if not divisible(gbs, data_parallel_size):
                 continue
+
             for tensor_model_parallel_size in space["tensor_model_parallel_size"]:
                 if not divisible(cards, tensor_model_parallel_size):
                     continue
                 if not divisible(cards, data_parallel_size * tensor_model_parallel_size):
                     continue
+
                 hidden_size = config.train.model.hidden_size
                 num_attention_size = config.train.model.num_attention_heads
                 if not divisible(hidden_size, tensor_model_parallel_size):
                     continue
                 if not divisible(num_attention_size, tensor_model_parallel_size):
                     continue
+
                 for pipeline_model_parallel_size in space["pipeline_model_parallel_size"]:
                     if not divisible(cards, pipeline_model_parallel_size):
                         continue
@@ -329,9 +361,23 @@ class Searcher:
                         * pipeline_model_parallel_size,
                     ):
                         continue
+
                     num_layers = config.train.model.num_layers
                     if not divisible(num_layers, pipeline_model_parallel_size):
-                        continue
+                        first_num_layers, last_num_layers = get_first_last_num_layers_for_pp(
+                            num_layers, pipeline_model_parallel_size
+                        )
+                        if first_num_layers == 0 or last_num_layers == 0:
+                            continue
+                        if (
+                            first_num_layers + last_num_layers >= num_layers
+                            and pipeline_model_parallel_size > 2
+                        ):
+                            continue
+                    else:
+                        first_num_layers = None
+                        last_num_layers = None
+
                     for context_parallel_size in space["context_parallel_size"]:
                         if not divisible(cards, context_parallel_size):
                             continue
@@ -343,40 +389,47 @@ class Searcher:
                             * context_parallel_size,
                         ):
                             continue
+                        if (
+                            data_parallel_size
+                            * tensor_model_parallel_size
+                            * pipeline_model_parallel_size
+                            * context_parallel_size
+                            != cards
+                        ):
+                            continue
                         seq_length = config.train.model.seq_length
                         if not divisible(seq_length, context_parallel_size):
                             continue
+
                         for expert_model_parallel_size in space["expert_model_parallel_size"]:
+                            expert_tensor_parallel_size = tensor_model_parallel_size
                             if not divisible(cards, expert_model_parallel_size):
                                 continue
                             if not divisible(
                                 cards,
-                                data_parallel_size
-                                * tensor_model_parallel_size
-                                * pipeline_model_parallel_size
-                                * context_parallel_size
-                                * expert_model_parallel_size,
-                            ):
-                                continue
-                            if (
-                                data_parallel_size
-                                * tensor_model_parallel_size
-                                * pipeline_model_parallel_size
-                                * context_parallel_size
+                                expert_tensor_parallel_size
                                 * expert_model_parallel_size
-                                != cards
+                                * pipeline_model_parallel_size,
                             ):
                                 continue
+                            if expert_model_parallel_size > 1:
+                                num_experts = config.train.model.num_experts
+                                if not divisible(num_experts, expert_model_parallel_size):
+                                    continue
 
                             dims["data_parallel_size"] = data_parallel_size
                             dims["tensor_model_parallel_size"] = tensor_model_parallel_size
                             dims["pipeline_model_parallel_size"] = pipeline_model_parallel_size
                             dims["expert_model_parallel_size"] = expert_model_parallel_size
                             dims["context_parallel_size"] = context_parallel_size
+                            dims["decoder_first_pipeline_num_layers"] = first_num_layers
+                            dims["decoder_last_pipeline_num_layers"] = (
+                                last_num_layers if pipeline_model_parallel_size > 2 else None
+                            )
                             copied_dims = copy.deepcopy(dims)
                             product_parallelism_dims.append(copied_dims)
-        product_dist_opt_dims = []
 
+        product_dist_opt_dims = []
         for use_distributed_optimizer in space["use_distributed_optimizer"]:
             dims = {}
             dims["use_distributed_optimizer"] = use_distributed_optimizer
@@ -384,7 +437,6 @@ class Searcher:
             product_dist_opt_dims.append(copied_dims)
 
         product_sp_dims = []
-
         for sequence_parallel in space["sequence_parallel"]:
             dims = {}
             dims["sequence_parallel"] = sequence_parallel
@@ -398,11 +450,10 @@ class Searcher:
             product_dim = {}
             product_dim.update(product_parallelism_dim)
             if product_parallelism_dim["data_parallel_size"] == 1:
-                product_dim["use_distributed_optimizer"] = None
+                product_dim["use_distributed_optimizer"] = False
                 if product_parallelism_dim["tensor_model_parallel_size"] == 1:
-                    product_dim["sequence_parallel"] = None
+                    product_dim["sequence_parallel"] = False
                     self._append(result, unique_result, product_dim)
-
                 else:
                     for product_sp_dim in product_sp_dims:
                         if product_sp_dim["sequence_parallel"]:
@@ -420,7 +471,7 @@ class Searcher:
                     ]
 
                     if product_parallelism_dim["tensor_model_parallel_size"] == 1:
-                        product_dim["sequence_parallel"] = None
+                        product_dim["sequence_parallel"] = False
                         self._append(result, unique_result, product_dim)
                     else:
                         for product_sp_dim in product_sp_dims:
@@ -500,22 +551,28 @@ class Searcher:
                         self._append(result, unique_result, product_dim)
                     else:
                         pipeline_model_parallel_size = parallelism["pipeline_model_parallel_size"]
-                        layers = config.train.model.num_layers
                         if (
                             pipeline_model_parallel_size <= 2
                             and num_layers_per_virtual_pipeline_stage >= 1
                         ):
                             continue
+                        if not divisible(num_layers, pipeline_model_parallel_size):
+                            continue
 
-                        layers_per_pp_stage = layers // pipeline_model_parallel_size
+                        layers_per_pp_stage = num_layers // pipeline_model_parallel_size
                         if not divisible(
                             layers_per_pp_stage, num_layers_per_virtual_pipeline_stage
                         ):
                             continue
 
-                        # Micro batches should divide pp size
+                        virtual_pipeline_model_parallel_size = (
+                            layers_per_pp_stage // num_layers_per_virtual_pipeline_stage
+                        )
+                        if virtual_pipeline_model_parallel_size == 1:
+                            continue
+                        # Micro batches should divide vpp size
                         if not divisible(
-                            product_dim["micro_batch_size"], num_layers_per_virtual_pipeline_stage
+                            product_dim["acc_step"], virtual_pipeline_model_parallel_size
                         ):
                             continue
 

