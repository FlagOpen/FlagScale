diff --git a/flagscale/serve/engine.py b/flagscale/serve/engine.py
index 4a7bc51c..213b0ff9 100644
--- a/flagscale/serve/engine.py
+++ b/flagscale/serve/engine.py
@@ -1,57 +1,209 @@
 import importlib
+import importlib.util
+import inspect
 import json
+import logging
 import os
 import subprocess
 import sys
+import threading
 
 from pathlib import Path
+from typing import Any, Dict
 
 import matplotlib.pyplot as plt
 import numpy as np
 import omegaconf
 import ray
 import uvicorn
+import yaml
 
 from dag_utils import check_and_get_port
 from fastapi import FastAPI, HTTPException, Request
-from pydantic import create_model
-from ray import workflow
+from pydantic import BaseModel, create_model
+from ray import serve
+from ray.serve.handle import DeploymentHandle
+
+# from flagscale.logger import logger
+logger = logging.getLogger("ray.serve")
+logger.setLevel(logging.INFO)
+
+
+def make_task_manager():
+    local_app = FastAPI()
+
+    class TaskUpdate(BaseModel):
+        id: str
+        status: str
+
+    class TaskQuery(BaseModel):
+        id: str
+
+    @serve.deployment(num_replicas=1)
+    @serve.ingress(local_app)
+    class TaskManager:
+        def __init__(self):
+            self.task_status = {}
+            self._lock = threading.Lock()
+
+        @local_app.post("/set_task_status")
+        async def set_task_status(self, req: TaskUpdate):
+            with self._lock:
+                self.task_status[req.id] = req.status
+            return {"ok": True}
+
+        @local_app.post("/get_task_status")
+        async def get_task_status(self, req: TaskQuery):
+            with self._lock:
+                status = self.task_status.get(req.id, "unknown")
+            return {"id": req.id, "status": status}
+
+        @local_app.post("/delete_task_status")
+        async def delete_task_status(self, req: TaskQuery):
+            with self._lock:
+                self.task_status.pop(req.id, None)
+            return {"ok": True}
+
+    return TaskManager.bind()
+
+
+def load_class_from_file(file_path: str, class_name: str):
+    file_path = os.path.abspath(file_path)
+    module_dir = os.path.dirname(file_path)
+    logger.info(f"Loading class {class_name} from file: {file_path}")
+    sys.path.insert(0, module_dir)
+    try:
+        module_name = os.path.splitext(os.path.basename(file_path))[0]
+        spec = importlib.util.spec_from_file_location(module_name, file_path)
+        if spec is None:
+            raise ImportError(f"Cannot create module spec from {file_path}")
+        module = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(module)
+        if not hasattr(module, class_name):
+            raise ImportError(f"Class {class_name} not found in {file_path}")
+        return getattr(module, class_name)
+    finally:
+        if sys.path[0] == module_dir:
+            sys.path.pop(0)
+
+
+def make_deployment(logic_cls, **deploy_kwargs):
+    @serve.deployment(**deploy_kwargs)
+    class WrappedModel:
+        def __init__(self):
+            self.logic = logic_cls()
+
+        async def forward(self, *args, **kwargs):
+            if inspect.iscoroutinefunction(self.logic.forward):
+                return await self.logic.forward(*args, **kwargs)
+            return self.logic.forward(*args, **kwargs)
+
+    return WrappedModel
+
+
+@serve.deployment
+class FinalModel:
+    def __init__(
+        self,
+        graph_config: Dict[str, Any],
+        handles: Dict[str, DeploymentHandle],
+        config: omegaconf.DictConfig,
+    ):
+        self.graph_config = graph_config
+        self.handles = handles
+
+        # determine return nodes
+        all_nodes = set(graph_config.keys())
+        dep_nodes = {dep for cfg in graph_config.values() for dep in cfg.get("depends", [])}
+        self.roots = list(all_nodes - dep_nodes)
+        assert len(self.roots) == 1, "Only one return node is allowed"
+        request_config = config.experiment.runner.deploy.request
+        self.request_base = create_model(
+            "Request",
+            **{
+                field: (type_, ...)
+                for field, type_ in zip(request_config.args, request_config.types)
+            },
+        )
+
+    async def __call__(self, http_request):
+        origin_request = await http_request.json()
+        request_data = self.request_base(**origin_request).dict()
+
+        results_cache = {}
+
+        async def run_node(node_name, **input_data):
+            if node_name in results_cache:
+                return results_cache[node_name]
 
-from flagscale.logger import logger
+            node_cfg = self.graph_config[node_name]
+            handle = self.handles[node_name]
+
+            if node_cfg.get("depends"):
+                dep_results = []
+                for dep in node_cfg["depends"]:
+                    res = await run_node(dep, **input_data)
+                    dep_results.append(res)
+                if len(dep_results) == 1:
+                    result = await handle.forward.remote(dep_results[0])
+                else:
+                    result = await handle.forward.remote(*dep_results)
+            else:
+                result = await handle.forward.remote(**input_data)
+
+            results_cache[node_name] = result
+            return result
+
+        final_results = {}
+        root = self.roots[0]
+        final_results[root] = await run_node(root, **request_data)
+        return final_results[root]
+
+
+def build_graph(config):
+    connection_list = config.serve
+    # Convert list of dicts with 'serve_id' to dict keyed by serve_id
+    connection = {
+        cfg["serve_id"]: {k: v for k, v in cfg.items() if k != "serve_id"}
+        for cfg in connection_list
+    }
+
+    handles = {}
+    deployments = {}
+    for name, cfg in connection.items():
+        logic_cls = load_class_from_file(cfg["module"], cfg["name"])
+        resources = cfg.get("resources", {})
+        ray_actor_options = {}
+        if "num_gpus" in resources:
+            ray_actor_options["num_gpus"] = resources["num_gpus"]
+        deploy_kwargs = {
+            "num_replicas": resources.get("num_replicas", 1),
+            "ray_actor_options": ray_actor_options,
+        }
+        deployments[name] = make_deployment(logic_cls, **deploy_kwargs)
+        handles[name] = deployments[name].bind()
+
+    root_model = FinalModel.bind(connection, handles, config)
+    return root_model
 
 
 class ServeEngine:
     def __init__(self, config):
-        self.config = config.serve
+        self.config = config
+        self.model_config = config.serve
         self.exp_config = config.experiment
-        self.check_config(self.config)
-        self.tasks = {}
+        self.check_task(self.exp_config)
+        self.init_task()
 
-    def check_config(self, config):
-        if not config.get("deploy", None):
+    def check_task(self, config):
+        if not config.get("runner", {}).get("deploy", None):
             raise ValueError("key deploy is missing for deployment configuration.")
-        if not config.deploy.get("models", None):
-            raise ValueError("key models is missing for building dag pipeline.")
-
-    def find_final_node(self):
-        whole_nodes = set(self.config["deploy"]["models"].keys())
-        dependencies = set()
-
-        for model_alias, model_config in self.config["deploy"]["models"].items():
-            if len(model_config.get("depends", [])) > 0:
-                dependencies.update(model_config.depends)
-
-        output_node = whole_nodes - dependencies
-        if len(output_node) != 1:
-            raise ValueError(
-                f"There should only have one final node but there are {len(output_node)} nodes {output_node}."
-            )
-        return list(output_node)[0]
+        self.check_dag()
 
     def check_dag(self, visibilization=True):
         # Ensure that all dependencies are valid
         dag = {}
-        for model_alias, model_config in self.config["deploy"]["models"].items():
+        for model_alias, model_config in ((k, v) for d in self.model_config for k, v in d.items()):
             dependencies = []
             if "depends" in model_config:
                 deps = model_config["depends"]
@@ -61,7 +213,7 @@ class ServeEngine:
             dag[model_alias] = dependencies
 
             for dep in dependencies:
-                if dep not in self.config["deploy"]["models"]:
+                if dep not in self.model_config["deploy"]["models"]:
                     raise ValueError(
                         f"Dependency {dep} for model {model_alias} not found in config['deploy']['models']"
                     )
@@ -180,207 +332,46 @@ class ServeEngine:
             _visualize_dag_with_force_directed_layout(dag, dag_img_path)
 
     def init_task(self, pythonpath=""):
-
-        hostfile = self.config.get("hostfile", None)
-        address = "auto"
-        exp_path = os.path.join(self.exp_config.exp_dir, "ray_workflow")
-        ray_path = os.path.abspath(exp_path)
-        if hostfile:
-            head_ip, head_port = next(
-                (
-                    (node.master.ip, node.master.get("port", None))
-                    for node in hostfile.nodes
-                    if "master" in node
-                ),
-                (None, None),
-            )
-            if head_ip is None:
-                raise ValueError(
-                    f"Failed to start Ray cluster using hostfile {hostfile} due to master node missing. Please ensure that the file exists and has the correct format."
-                )
-            if head_port is None:
-                port = check_and_get_port()
-            else:
-                port = check_and_get_port(target_port=int(head_port))
-            cmd = ["ray", "start", "--head", f"--port={port}", f"--storage={ray_path}"]
-            logger.info(f"head node command: {cmd}")
-            head_result = subprocess.run(
-                cmd, check=True, capture_output=True, text=True, encoding="utf-8", errors="replace"
-            )
-            if head_result.returncode != 0:
-                logger.warning(
-                    f"Head Node cmd {ssh_cmd} failed with return code {head_result.returncode}."
-                )
-                logger.warning(f"Output: {head_result.stdout}")
-                logger.warning(f"Error: {head_result.stderr}")
-                sys.exit(head_result.returncode)
-            address = f"{head_ip}:{port}"
-
-            for item in hostfile.nodes:
-                if "node" in item:
-                    node = item.node
-                    if node.type == "gpu":
-                        node_cmd = f"ray start --address={address} --num-gpus={node.slots}"
-
-                    elif node.type == "cpu":
-                        node_cmd = f"ray start --address={address} --num-cpus={node.slots}"
-                    else:
-                        resource = json.dumps({node.type: node.slots}).replace('"', '\\"')
-                        node_cmd = f"ray start --address={address} --resources='{resource}'"
-                    if self.exp_config.get("cmds", "") and self.exp_config.cmds.get(
-                        "before_start", ""
-                    ):
-                        before_start_cmd = self.exp_config.cmds.before_start
-                        node_cmd = (
-                            f"export RAY_STORAGE={ray_path} && {before_start_cmd} && " + node_cmd
-                        )
-
-                    if node.get("port", None):
-                        ssh_cmd = f'ssh -n -p {node.port} {node.ip} "{node_cmd}"'
-                    else:
-                        ssh_cmd = f'ssh -n {node.ip} "{node_cmd}"'
-
-                    logger.info(f"worker node command: {cmd}")
-
-                    result = subprocess.run(
-                        ssh_cmd,
-                        shell=True,
-                        check=True,
-                        capture_output=True,
-                        text=True,
-                        encoding="utf-8",
-                        errors="replace",
-                    )
-                    if result.returncode != 0:
-                        logger.warning(
-                            f"SSH command {ssh_cmd} failed with return code {result.returncode}."
-                        )
-                        logger.warning(f"Output: {result.stdout}")
-                        logger.warning(f"Error: {result.stderr}")
-                        sys.exit(result.returncode)
-        else:
-            port = check_and_get_port()
-            head_ip = "127.0.0.1"
-            cmd = ["ray", "start", "--head", f"--port={port}", f"--storage={ray_path}"]
-            logger.info(f"head node command: {cmd}")
-            head_result = subprocess.run(
-                cmd, check=True, capture_output=True, text=True, encoding="utf-8", errors="replace"
-            )
-            if head_result.returncode != 0:
-                logger.warning(
-                    f"local command {cmd} failed with return code {head_result.returncode}."
-                )
-                logger.warning(f"Output: {head_result.stdout}")
-                logger.warning(f"Error: {head_result.stderr}")
-                sys.exit(head_result.returncode)
-            address = f"{head_ip}:{port}"
-
         logger.info(f" =========== pythonpath {pythonpath} -----------------------")
+        runtime_env = {}
+        working_dir = (
+            self.exp_config.runner.deploy.get("working_dir", "") or self.exp_config.exp_dir
+        )
         if pythonpath:
-            ray.init(address=address, runtime_env={"env_vars": {"PYTHONPATH": pythonpath}})
+            runtime_env["env_vars"] = {"PYTHONPATH": pythonpath}
+
+        if working_dir:
+            runtime_env["working_dir"] = working_dir
+            runtime_env["excludes"] = [
+                "*.log",
+                "*.out",
+                "*.output",
+                "*.ckpt",
+                "*.safetensors",
+                "*.pth",
+                "*.pt",
+                "*.bin",
+                "*.pyc",
+                "**/.git/**",
+                "**/__pycache__/**",
+            ]
+        if runtime_env:
+            ray.init(runtime_env=runtime_env)
         else:
-            ray.init(address=address)
-
-    def build_task(self):
-        self.check_dag()
-        pythonpath_tmp = set()
-        for model_alias, model_config in self.config["deploy"]["models"].items():
-            module_name = model_config["module"]
-            path = Path(module_name)
-            module_dir = str(path.parent)
-            pythonpath_tmp.add(os.path.abspath(module_dir))
-        pythonpath = ":".join(pythonpath_tmp)
-        self.init_task(pythonpath=pythonpath)
-
-        for model_alias, model_config in self.config["deploy"]["models"].items():
-            module_name = model_config["module"]
-            model_name = model_config["name"]
-            path = Path(module_name)
-            module_tmp = path.stem
-            module_dir = str(path.parent)
-            sys.path.append(module_dir)
-            module = importlib.import_module(module_tmp)
-            model = getattr(module, model_name)
-            resources = model_config.resources
-            num_gpus = resources.get("gpu", 0)
-            num_cpus = resources.get("cpu", 1)
-            customs = {res: resources[res] for res in resources if res not in ["gpu", "cpu"]}
-            self.tasks[model_alias] = ray.remote(model).options(
-                num_cpus=num_cpus, num_gpus=num_gpus, resources=customs
-            )
-        return
-
-    def run_task(self, *input_data):
-        assert len(self.tasks) > 0
-        models_to_process = list(self.config["deploy"]["models"].keys())
-        model_nodes = {}
-
-        while models_to_process:
-            progress = False
-            for model_alias in list(models_to_process):
-                model_config = self.config["deploy"]["models"][model_alias]
-                dependencies = []
-                if "depends" in model_config:
-                    deps = model_config["depends"]
-                    if not isinstance(deps, (list, omegaconf.listconfig.ListConfig)):
-                        deps = [deps]
-                    dependencies = deps
-                else:
-                    dependencies = []
-
-                if all(dep in model_nodes for dep in dependencies):
-                    if dependencies:
-                        if len(dependencies) > 1:
-                            inputs = [model_nodes[dep] for dep in dependencies]
-                            model_nodes[model_alias] = self.tasks[model_alias].bind(*inputs)
-                        else:
-                            model_nodes[model_alias] = self.tasks[model_alias].bind(
-                                model_nodes[dependencies[0]]
-                            )
-                    else:
-                        if len(input_data) == 0:
-                            model_nodes[model_alias] = self.tasks[model_alias].bind()
-                        else:
-                            model_nodes[model_alias] = self.tasks[model_alias].bind(*input_data)
-                    models_to_process.remove(model_alias)
-                    progress = True
-            if not progress:
-                raise ValueError("Circular dependency detected in model configuration")
-
-        logger.info(f" =========== deploy model_nodes {model_nodes} ============= ")
-        find_final_node = self.find_final_node()
-
-        final_node = model_nodes[find_final_node]
-        # pydot is required to plot DAG, install it with `pip install pydot`.
-        # ray.dag.vis_utils.plot(final_node, "output.jpg")
-        final_result = workflow.run(final_node)
-        return final_result
-
-    def run_router_task(self, method="post"):
-        router_config = self.config["deploy"].get("service")
-        assert router_config and len(router_config) > 0
-
-        name = router_config["name"]
-        port = router_config["port"]
-        request_names = router_config["request"]["names"]
-        request_types = router_config["request"]["types"]
-
-        RequestData = create_model(
-            "Request", **{field: (type_, ...) for field, type_ in zip(request_names, request_types)}
+            ray.init()
+
+    def run_task(self):
+        graph = build_graph(self.config)
+        task_manager = make_task_manager()
+        serve.start(http_options={"port": self.exp_config.runner.deploy.get("port", 8000)})
+        manager_prefix_name = "/manager"
+        serve_prefix_name = self.exp_config.runner.deploy.get("name", "/")
+        assert (
+            manager_prefix_name != serve_prefix_name
+        ), "router /manager exists, use another router name instead"
+        serve.run(
+            task_manager, name="task_manager", route_prefix=manager_prefix_name, blocking=False
+        )
+        serve.run(
+            graph, name=self.exp_config.exp_name, route_prefix=serve_prefix_name, blocking=True
         )
-        app = FastAPI()
-
-        if method.lower() == "post":
-
-            @app.post(name)
-            async def route_handler(request_data: RequestData):
-                input_data = tuple(getattr(request_data, field) for field in request_names)
-                try:
-                    response = self.run_task(*input_data)
-                    return response
-                except Exception as e:
-                    raise HTTPException(status_code=400, detail=str(e))
-
-        else:
-            raise ValueError(f"Unsupported HTTP method: {method}")
-        uvicorn.run(app, host="127.0.0.1", port=port)

