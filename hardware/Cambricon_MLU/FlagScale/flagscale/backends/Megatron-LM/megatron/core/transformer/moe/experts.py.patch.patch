diff --git a/flagscale/backends/Megatron-LM/megatron/core/transformer/moe/experts.py.patch b/flagscale/backends/Megatron-LM/megatron/core/transformer/moe/experts.py.patch
new file mode 100644
index 00000000..ad03c325
--- /dev/null
+++ b/flagscale/backends/Megatron-LM/megatron/core/transformer/moe/experts.py.patch
@@ -0,0 +1,17 @@
+diff --git a/megatron/core/transformer/moe/experts.py b/megatron/core/transformer/moe/experts.py
+index c215c43e..200be4f9 100644
+--- a/megatron/core/transformer/moe/experts.py
++++ b/megatron/core/transformer/moe/experts.py
+@@ -226,6 +226,12 @@ class GroupedMLP(MegatronModule):
+         setattr(self.weight1, 'allreduce', not self.expert_parallel)
+         setattr(self.weight2, 'allreduce', not self.expert_parallel)
+ 
++        # NOTE(lizhiyu): The following code is for hetro-expert training when one of the expert parallel degree is 1.
++        #                 But there are other codes need to be modified to make it work.
++        # if config.enable_hetero:
++        #     setattr(self.weight1, 'allreduce', False)
++        #     setattr(self.weight2, 'allreduce', False)
++
+         def remove_extra_states_check(self, incompatible_keys):
+             """
+             Remove _extra_state from unexpected keys.

