diff --git a/flagscale/backends/Megatron-LM/megatron/core/models/common/language_module/language_module.py.patch b/flagscale/backends/Megatron-LM/megatron/core/models/common/language_module/language_module.py.patch
new file mode 100644
index 00000000..0010c88d
--- /dev/null
+++ b/flagscale/backends/Megatron-LM/megatron/core/models/common/language_module/language_module.py.patch
@@ -0,0 +1,32 @@
+diff --git a/megatron/core/models/common/language_module/language_module.py b/megatron/core/models/common/language_module/language_module.py
+index 6000e35b..5272dbe6 100644
+--- a/megatron/core/models/common/language_module/language_module.py
++++ b/megatron/core/models/common/language_module/language_module.py
+@@ -171,9 +171,24 @@ class LanguageModule(MegatronModule):
+             ):
+                 weight = self.shared_embedding_or_output_weight()
+                 weight.data = weight.data.cuda()
+-                torch.distributed.all_reduce(
+-                    weight.data, group=parallel_state.get_embedding_group()
+-                )
++                embedding_group = parallel_state.get_embedding_group()
++                if not isinstance(embedding_group, list):
++                    torch.distributed.all_reduce(
++                        weight.data, group=parallel_state.get_embedding_group()
++                    )
++                else: # for multiple embedding groups in heterogeneous mode
++                    with torch.no_grad():
++                        original_dtype = weight.dtype
++                        if (original_dtype == torch.bfloat16) and torch.distributed.get_backend(group=embedding_group[0])=="cpu:gloo": # gloo backend doesn't support bfloat16
++                            weight = weight.to(torch.float32)
++                            weight.data = weight.data.cpu()
++                        original_weight = weight.clone().detach().data
++                        for group in embedding_group:
++                            weight.data.copy_(original_weight)
++                            torch.distributed.all_reduce(weight.data, group=group)
++                        if original_dtype != weight.dtype:
++                            weight = weight.to(original_dtype)
++                            weight.data = weight.data.cuda()
+ 
+         elif not getattr(LanguageModule, "embedding_warning_printed", False):
+             logging.getLogger(__name__).warning(

