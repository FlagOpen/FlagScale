diff --git a/flagscale/backends/Megatron-LM/megatron/training/tokenizer/tokenizer.py.patch b/flagscale/backends/Megatron-LM/megatron/training/tokenizer/tokenizer.py.patch
new file mode 100644
index 00000000..46c11791
--- /dev/null
+++ b/flagscale/backends/Megatron-LM/megatron/training/tokenizer/tokenizer.py.patch
@@ -0,0 +1,339 @@
+diff --git a/megatron/training/tokenizer/tokenizer.py b/megatron/training/tokenizer/tokenizer.py
+index 5cf222cc..14188a2c 100644
+--- a/megatron/training/tokenizer/tokenizer.py
++++ b/megatron/training/tokenizer/tokenizer.py
+@@ -14,6 +14,7 @@ from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer
+ 
+ from .bert_tokenization import FullTokenizer as FullBertTokenizer
+ from .gpt2_tokenization import GPT2Tokenizer
++from .gpt2_tokenization import AquilaTokenizer
+ from megatron.training.tokenizer.multimodal_tokenizer import MultimodalTokenizer
+ from megatron.training.tokenizer.sft_tokenizer import SFTTokenizer
+ 
+@@ -102,6 +103,31 @@ def build_tokenizer(args, **kwargs):
+     elif args.tokenizer_type == 'NullMultimodalTokenizer':
+         assert args.vocab_size is not None
+         tokenizer = _NullMultimodalTokenizer(args.vocab_size)
++    elif args.tokenizer_type == 'AquilaTokenizerFS':
++        assert args.vocab_file is not None
++        assert args.merge_file is not None
++        assert args.special_tokens_file is not None
++        tokenizer = _AquilaTokenizerFS(args.vocab_file, args.merge_file,
++                                     args.special_tokens_file)
++    elif args.tokenizer_type == "HFTokenizerFS":
++        assert args.tokenizer_path is not None
++        tokenizer = _HFTokenizerFS(args.tokenizer_path)
++    elif args.tokenizer_type == "Llama3TokenizerFS":
++        assert args.tokenizer_path is not None
++        tokenizer = _Llama3TokenizerFS(args.tokenizer_path)
++    elif args.tokenizer_type == "QwenTokenizerFS":
++        assert args.tokenizer_path is not None
++        tokenizer = _QwenTokenizerFS(args.tokenizer_path)
++    elif args.tokenizer_type == "HFTokenizersTokenizerFS":
++        assert args.tokenizer_path is not None
++        tokenizer = _HFTokenizersTokenizerFS(args.tokenizer_path)
++    elif args.tokenizer_type == "Qwen2TokenizerFS":
++        assert args.tokenizer_path is not None
++        tokenizer = _Qwen2TokenizerFS(args.tokenizer_path, args)
++    elif args.tokenizer_type == 'Qwen2VLTokenizer':
++        assert args.tokenizer_path is not None
++        tokenizer = _Qwen2VLTokenizer(args.tokenizer_path, args.extra_vocab_size)
++        args.padded_vocab_size = tokenizer.vocab_size # no padding
+     else:
+         raise NotImplementedError('{} tokenizer is not ' 'implemented.'.format(args.tokenizer_type))
+ 
+@@ -596,6 +622,16 @@ class _Llama2Tokenizer(_SentencePieceTokenizer):
+             t = t + [self.eos_id]
+         return t
+ 
++    def instruct_tokenize(self, s: str, bos=True, eos=False):
++        '''Default args for text completion, not chat/dialog.'''
++        assert type(s) is str
++        t = self.tokenizer.encode(s)
++        if bos:
++            t = [self.bos_id] + t
++        if eos:
++            t = t + [self.eos_id]
++        return t
++
+     def detokenize(self, ids):
+         return self.tokenizer.decode_ids(ids)
+ 
+@@ -909,3 +945,276 @@ class _NullMultimodalTokenizer(MegatronTokenizer):
+     @property
+     def additional_special_tokens_ids(self):
+         return None
++
++
++class _AquilaTokenizerFS(MegatronTokenizer):
++    """Aquila tokenizer."""
++
++    def __init__(self, vocab_file, merge_file, special_tokens_file):
++        super().__init__(vocab_file, merge_file, special_tokens_file)
++
++        special_tokens = []
++        if special_tokens_file:
++            special_tokens = open(special_tokens_file, encoding='utf-8').read().split('\n')[:-1]
++
++        self.tokenizer = AquilaTokenizer(vocab_file, merge_file, errors='replace',
++                                         special_tokens=special_tokens, max_len=None)
++        self.eod_id = self.tokenizer.encoder['</s>']
++        self.cls_id = self.tokenizer.encoder['[CLS]']
++        self.pad_id = self.tokenizer.encoder['<|endoftext|>']
++
++    @property
++    def vocab_size(self):
++        return len(self.tokenizer.encoder)
++
++    @property
++    def vocab(self):
++        return self.tokenizer.encoder
++
++    @property
++    def inv_vocab(self):
++        return self.tokenizer.decoder
++
++    def tokenize(self, text):
++        return self.tokenizer.encode(text)
++
++    def detokenize(self, token_ids):
++        return self.tokenizer.decode(token_ids)
++
++    @property
++    def eod(self):
++        return self.eod_id
++
++    @property
++    def cls(self):
++        return self.cls_id
++
++    @property
++    def pad(self):
++        return self.pad_id
++
++
++class _HFTokenizerFS(MegatronTokenizer):
++    """Huggingface tokenizer."""
++
++    def __init__(self, tokenizer_path):
++        name = 'HFTokenizer'
++        super().__init__(name)
++        
++        from transformers import AutoTokenizer
++        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
++
++        self.eod_id = self.tokenizer.eos_token_id
++        self.cls_id = self.tokenizer.bos_token_id
++        self.pad_id = self.tokenizer.pad_token_id
++
++        self._inv_vocab = None
++        
++    @property
++    def vocab_size(self):
++        return self.tokenizer.vocab_size
++
++    @property
++    def vocab(self):
++        return self.tokenizer.get_vocab()
++
++    @property
++    def inv_vocab(self):
++        vocab = self.vocab()
++        if self._inv_vocab is None:
++            self._inv_vocab = {v: k for k, v in vocab.items()}
++        return self._inv_vocab
++    
++    def tokenize(self, text):
++        return self.tokenizer.encode(text)
++
++    def detokenize(self, token_ids):
++        return self.tokenizer.decode(token_ids)
++
++    @property
++    def eod(self):
++        return self.eod_id
++
++    @property
++    def cls(self):
++        return self.cls_id
++
++    @property
++    def pad(self):
++        return self.pad_id
++
++
++class _Llama3TokenizerFS(_HFTokenizerFS):
++
++    def __init__(self, tokenizer_path):
++        super().__init__(tokenizer_path)
++
++    @property
++    def vocab_size(self):
++        return self.tokenizer.vocab_size + len(self.tokenizer.get_added_vocab())
++
++
++class _QwenTokenizerFS(_HFTokenizerFS):
++    """Adapted Qwen tokenizer."""
++    
++    def __init__(self, tokenizer_path):
++        super().__init__(tokenizer_path)
++        self.eod_id = self.tokenizer.encode('<|extra_204|>')[0]
++        self.cls_id = self.tokenizer.encode('<|extra_203|>')[0]
++        self.pad_id = self.tokenizer.encode('<|endoftext|>')[0]
++
++
++class _HFTokenizersTokenizerFS(MegatronTokenizer):
++    """Tokenizer from HuggingFace Tokenizers."""
++
++    def __init__(self, json_file):
++        super().__init__(json_file)
++
++        from tokenizers import Tokenizer
++        self.tokenizer = Tokenizer.from_file(json_file)
++
++        print(f"Vocab size: {self.tokenizer.get_vocab_size()}")
++
++        self.eod_id = self.tokenizer.token_to_id("<|endoftext|>")
++        self.pad_id = self.tokenizer.token_to_id("<|padding|>")
++
++        self._inv_vocab = None
++
++    @property
++    def vocab_size(self):
++        return self.tokenizer.get_vocab_size() 
++
++    @property
++    def vocab(self):
++        return self.tokenizer.get_vocab()
++
++    @property
++    def inv_vocab(self):
++        # return self.tokenizer.decoder
++        vocab = self.vocab()
++        if self._inv_vocab is None:
++            self._inv_vocab = {v: k for k, v in vocab.items()}
++        return self._inv_vocab 
++
++    def tokenize(self, text):
++        return self.tokenizer.encode(text)
++
++    def detokenize(self, token_ids):
++        return self.tokenizer.decode(token_ids)
++
++    @property
++    def eod(self):
++        return self.eod_id
++
++    @property
++    def pad(self):
++        return self.pad_id
++
++
++class _Qwen2TokenizerFS(_HFTokenizerFS):
++    """Adapted Qwen tokenizer."""
++
++    def __init__(self, tokenizer_path, args):
++        super().__init__(tokenizer_path)
++        self.eod_id = self.tokenizer.encode('<|extra_204|>')[0]
++        self.cls_id = self.tokenizer.encode('<|extra_203|>')[0]
++        self.pad_id = self.tokenizer.encode('<|endoftext|>')[0]
++        assert args.vocab_size is not None
++        self._vocab_size = args.vocab_size
++
++    @property
++    def vocab_size(self):
++        return self._vocab_size
++
++
++class _Qwen2VLTokenizer(MegatronTokenizer):
++    def __init__(self, tokenizer_path, extra_vocab_size):
++        super().__init__(tokenizer_path)
++        from transformers import AutoTokenizer
++        self.tokenizer = AutoTokenizer.from_pretrained(
++            tokenizer_path,
++            padding_side="right",
++            use_fast=True,
++            split_special_tokens=False,
++            trust_remote_code=True,
++            revision = "main",
++            token = None,
++        )
++        self.extra_vocab_size = extra_vocab_size
++        self.special_tokens_map = {k:v for k, v in zip(self.tokenizer.all_special_tokens, self.tokenizer.all_special_ids)}
++        self.image_token = '<|image_pad|>'
++        self.video_token = '<|video_pad|>'
++        self.vision_start_token = '<|vision_start|>'
++        self.vision_end_token = '<|vision_end|>'
++
++        from transformers import AutoProcessor
++        self.processor = AutoProcessor.from_pretrained(
++            tokenizer_path,
++            revision = "main",
++            token = None,
++        )
++        # NOTE: In Qwen2-VL, template in chat_template.json is same within tokenizer_config.json and both can be used.
++        # However, in Qwen 2.5-VL, the two templates are different and only the one in chat_template.json is OK.
++        self.chat_template = self.processor.chat_template
++
++    def __call__(self, text, return_tensors=None,
++                    padding=None, max_length=None, truncation=None, add_special_tokens=None):
++
++        return self.tokenizer(text, return_tensors=return_tensors, padding=padding,
++                max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)
++
++    def apply_chat_template(self, conversations, tokenize:bool=True, **kwargs):
++        return self.tokenizer.apply_chat_template(conversations, tokenize=tokenize, chat_template=self.chat_template, **kwargs)
++    
++    @property
++    def vocab_size(self):
++        return self.tokenizer.vocab_size + self.extra_vocab_size
++
++    @property
++    def vocab(self):
++        return self.tokenizer.vocab
++
++    @property
++    def inv_vocab(self):
++        return self.tokenizer.decoder
++
++    def tokenize(self, text):
++        return self.tokenizer.tokenize(text)
++
++    def detokenize(self, token_ids):
++        return self.tokenizer.decode(token_ids)
++
++    @property
++    def eod(self):
++        return self.tokenizer.eos_token_id
++
++    @property
++    def eos_token(self):
++        return self.tokenizer.eos_token
++
++    @property
++    def pad_token_id(self):
++        return self.tokenizer.pad_token_id
++
++    @property
++    def eos_token_id(self):
++        return self.tokenizer.eos_token_id
++    
++    @property
++    def image_token_id(self):
++        return self.special_tokens_map[self.image_token]
++    
++    @property
++    def video_token_id(self):
++        return self.special_tokens_map[self.video_token]
++    
++    @property
++    def vision_start_token_id(self):
++        return self.special_tokens_map[self.vision_start_token]
++    
++    @property
++    def vision_end_token_id(self):
++        return self.special_tokens_map[self.vision_end_token]
++    
++    def encode(self, x):
++        return self.tokenizer.encode(x)
+\ No newline at end of file

