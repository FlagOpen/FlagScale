diff --git a/flagscale/backends/Megatron-LM/tests/unit_tests/export/trtllm/test_distributed_fp8.py.patch b/flagscale/backends/Megatron-LM/tests/unit_tests/export/trtllm/test_distributed_fp8.py.patch
new file mode 100644
index 00000000..c249778e
--- /dev/null
+++ b/flagscale/backends/Megatron-LM/tests/unit_tests/export/trtllm/test_distributed_fp8.py.patch
@@ -0,0 +1,22 @@
+diff --git a/tests/unit_tests/export/trtllm/test_distributed_fp8.py b/tests/unit_tests/export/trtllm/test_distributed_fp8.py
+index cf47a864..ba83ad96 100644
+--- a/tests/unit_tests/export/trtllm/test_distributed_fp8.py
++++ b/tests/unit_tests/export/trtllm/test_distributed_fp8.py
+@@ -104,7 +104,16 @@ def _forward_step_func(data_iterator, model):
+ 
+     return output_tensor, partial(loss_func, loss_mask)
+ 
+-
++"""
++Author: phoenixdong
++Date: 2024-12-17
++Action: Add class-level skip decorator
++Reason: Skip all tests in this class if the device does not support CUDA or if its compute capability is less than 8.9.
++"""
++@pytest.mark.skipif(
++    not torch.cuda.is_available() or torch.cuda.get_device_capability(0) < (8, 9),
++    reason="Device compute capability 8.9 or higher required for FP8 execution"
++)
+ class TestTRTLLMSingleDeviceConverterFP8:
+     QUANTIZED_LAYERS = [
+         'transformer.layers.*.attention.dense.weight',

