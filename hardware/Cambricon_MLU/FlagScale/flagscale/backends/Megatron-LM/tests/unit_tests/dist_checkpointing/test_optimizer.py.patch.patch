diff --git a/flagscale/backends/Megatron-LM/tests/unit_tests/dist_checkpointing/test_optimizer.py.patch b/flagscale/backends/Megatron-LM/tests/unit_tests/dist_checkpointing/test_optimizer.py.patch
new file mode 100644
index 00000000..c3db148f
--- /dev/null
+++ b/flagscale/backends/Megatron-LM/tests/unit_tests/dist_checkpointing/test_optimizer.py.patch
@@ -0,0 +1,24 @@
+diff --git a/tests/unit_tests/dist_checkpointing/test_optimizer.py b/tests/unit_tests/dist_checkpointing/test_optimizer.py
+index 7c1e5f15..596cf97d 100644
+--- a/tests/unit_tests/dist_checkpointing/test_optimizer.py
++++ b/tests/unit_tests/dist_checkpointing/test_optimizer.py
+@@ -196,6 +196,19 @@ def initialize_1d_flatten_tensor_model(
+     return Model1dFlattenTensor()
+ 
+ 
++def init_mock_args(args):
++    args.data_parallel_random_init = False
++    args.virtual_pipeline_model_parallel_size = None
++    args.bf16 = True
++    args.accumulate_allreduce_grads_in_fp32 = False
++    args.overlap_grad_reduce = False
++    args.use_distributed_optimizer = True
++    args.ddp_bucket_size = None
++    args.load = None
++    args.save_param_index_maps_only = False
++    return args
++
++
+ def load_checkpoint_no_arg_checks(*args, **kwargs):
+     with mock.patch('megatron.training.checkpointing.check_checkpoint_args'):
+         with mock.patch('megatron.training.checkpointing.update_num_microbatches'):

