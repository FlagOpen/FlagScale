diff --git a/tools/datasets/qwenvl/data/image_processing.py b/tools/datasets/qwenvl/data/image_processing.py
index 2b446613..822b3fc3 100644
--- a/tools/datasets/qwenvl/data/image_processing.py
+++ b/tools/datasets/qwenvl/data/image_processing.py
@@ -11,17 +11,24 @@ from PIL import Image, ImageDraw
 from torchvision import transforms as T
 from torchvision.transforms import Compose, RandAugment, RandomResizedCrop, Resize, ToPILImage
 
+# config :https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/blob/main/preprocessor_config.json
 # Imagenet's mean and std.
-pixel_mean = [123.675, 116.28, 103.53]
-pixel_std = [58.395, 57.12, 57.375]
+pixel_mean = [0.48145466, 0.4578275, 0.40821073]
+pixel_std = [0.26862954, 0.26130258, 0.27577711]
 
 # Reshape for broadcasting.
 pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)
 pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)
 
 
-def convert_to_rgb(image):
-    return image.convert("RGB")
+# https://github.com/QwenLM/Qwen2.5-VL/blob/477fd9d4317266508705366ce36cac5b68d70936/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L89C1-L95C40
+def convert_to_rgb(pil_image: Image.Image) -> Image.Image:
+    if pil_image.mode == 'RGBA':
+        white_background = Image.new("RGB", pil_image.size, (255, 255, 255))
+        white_background.paste(pil_image, mask=pil_image.split()[3])  # Use alpha channel as mask
+        return white_background
+    else:
+        return pil_image.convert("RGB")
 
 
 def _transform_train_aug():
@@ -57,24 +64,20 @@ def _transform_test():
 
 def standardize_image(img):
     """Standardize image pixel values."""
-    return (torch.Tensor(np.array(img)).permute(2, 0, 1) - pixel_mean) / pixel_std
+    return (T.ToTensor()(img) - pixel_mean) / pixel_std
 
 
 def get_visual_transform(
-    img,
+    img,  # Path
     factor: int = 28,
-    min_pixels: int = 56 * 56,
-    max_pixels: int = 14 * 14 * 4 * 1280,
+    min_pixels: int = 4 * 28 * 28,
+    max_pixels: int = 16384 * 28 * 28,
     augment=False,
 ):
-    img = np.array(img)
-
-    if augment:
-        visual_transform = _transform_train_aug()
-    else:
-        visual_transform = _transform_test()
-
-    img = visual_transform(img)
+    # TODO(lizhiyu): Need to limit the aspect ratio of the image.
+    # (reference https://github.com/QwenLM/Qwen2.5-VL/blob/477fd9d4317266508705366ce36cac5b68d70936/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L72)
+    img = Image.open(img)
+    img = convert_to_rgb(img)
     w, h = img.size
     h_bar, w_bar = smart_resize(h, w, factor, min_pixels, max_pixels)
     img = img.resize((w_bar, h_bar))
@@ -90,8 +93,8 @@ def smart_resize(
     height: int,
     width: int,
     factor: int = 28,
-    min_pixels: int = 56 * 56,
-    max_pixels: int = 14 * 14 * 4 * 1280,
+    min_pixels: int = 4 * 28 * 28,
+    max_pixels: int = 16384 * 28 * 28,
 ):
     """Rescales the image so that the following conditions are met:
 

