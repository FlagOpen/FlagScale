diff --git a/tools/checkpoint/mistral/ckpt.py b/tools/checkpoint/mistral/ckpt.py
index efabe085..89758e1c 100644
--- a/tools/checkpoint/mistral/ckpt.py
+++ b/tools/checkpoint/mistral/ckpt.py
@@ -100,6 +100,9 @@ def get_attn_ckpt(message, models, layer_id, args):
         message["qkv bias"] = torch.cat(qkv_bias, dim=0)
     if args.add_bias_linear:
         message["proj bias"] = proj_bias
+    if args.qk_layernorm:
+        message["q norm weight"] = tf_layer.self_attention.q_layernorm.weight.data
+        message["k norm weight"] = tf_layer.self_attention.k_layernorm.weight.data
 
 
 def get_mlp_ckpt(message, models, layer_id, args):
@@ -160,6 +163,9 @@ def set_attn_ckpt(message, models, layer_id, md, args):
         qkv_bias = torch.chunk(message.pop("qkv bias"), tp_size, dim=0)
     if md.add_bias_linear:
         proj_bias = message.pop("proj bias")
+    if md.qk_layernorm:
+        q_norm_weight = message.pop("q norm weight")
+        k_norm_weight = message.pop("k norm weight")
 
     # set data to transformer layer's self-attention
     for tp_rank, model in enumerate(models):
@@ -175,6 +181,9 @@ def set_attn_ckpt(message, models, layer_id, md, args):
             tf_layer.self_attention.linear_qkv.bias.data.copy_(qkv_bias[tp_rank])
         if md.add_bias_linear:
             tf_layer.self_attention.linear_proj.bias.data.copy_(proj_bias)
+        if md.qk_layernorm:
+            tf_layer.self_attention.q_layernorm.weight.data.copy_(q_norm_weight)
+            tf_layer.self_attention.k_layernorm.weight.data.copy_(k_norm_weight)
 
 
 def set_mlp_ckpt(message, models, layer_id, md, args):

