diff --git a/tools/checkpoint/mixtral/ckpt.py b/tools/checkpoint/mixtral/ckpt.py
index 469aef00..7cb34e4a 100644
--- a/tools/checkpoint/mixtral/ckpt.py
+++ b/tools/checkpoint/mixtral/ckpt.py
@@ -48,6 +48,9 @@ def get_hf_attn_ckpt(message, model, layer_id, args):
         )
     if args.add_bias_linear:
         message["proj bias"] = tf_layer.self_attn.o_proj.bias.data
+    if args.qk_layernorm:
+        message["q norm weight"] = tf_layer.self_attn.q_norm.weight.data
+        message["k norm weight"] = tf_layer.self_attn.k_norm.weight.data
 
 
 def get_hf_mlp_ckpt(message, model, layer_id, args):
@@ -90,6 +93,9 @@ def set_hf_attn_ckpt(message, model, layer_id, md, args):
         qkv_bias = message.pop("qkv bias")
     if md.add_bias_linear:
         proj_bias = message.pop("proj bias")
+    if md.qk_layernorm:
+        q_norm_weight = message.pop("q norm weight")
+        k_norm_weight = message.pop("k norm weight")
 
     nh = args.num_attention_heads
     ng = args.num_query_groups if args.group_query_attention else args.num_attention_heads
@@ -118,6 +124,9 @@ def set_hf_attn_ckpt(message, model, layer_id, md, args):
         tf_layer.self_attn.v_proj.bias.data.copy_(qkv_bias[2].reshape(-1))
     if md.add_bias_linear:
         tf_layer.self_attn.o_proj.bias.data.copy_(proj_bias)
+    if md.qk_layernorm:
+        tf_layer.self_attn.q_norm.weight.data.copy_(q_norm_weight)
+        tf_layer.self_attn.k_norm.weight.data.copy_(k_norm_weight)
 
 
 def set_hf_mlp_ckpt(message, model, layer_id, md, args):
@@ -253,6 +262,9 @@ def get_attn_ckpt(message, models, layer_id, args):
         message["qkv bias"] = torch.cat(qkv_bias, dim=0)
     if args.add_bias_linear:
         message["proj bias"] = proj_bias
+    if args.qk_layernorm:
+        message["q norm weight"] = tf_layer.self_attention.q_layernorm.weight.data
+        message["k norm weight"] = tf_layer.self_attention.k_layernorm.weight.data
 
 
 def get_mlp_ckpt(message, models, layer_id, args):
@@ -369,6 +381,9 @@ def set_attn_ckpt(message, models, layer_id, md, args):
         qkv_bias = torch.chunk(message.pop("qkv bias"), tp_size, dim=0)
     if md.add_bias_linear:
         proj_bias = message.pop("proj bias")
+    if md.qk_layernorm:
+        q_norm_weight = message.pop("q norm weight")
+        k_norm_weight = message.pop("k norm weight")
 
     # set data to transformer layer's self-attention
     for tp_ep_rank, model in enumerate(models):
@@ -385,6 +400,9 @@ def set_attn_ckpt(message, models, layer_id, md, args):
             tf_layer.self_attention.linear_qkv.bias.data.copy_(qkv_bias[tp_rank])
         if md.add_bias_linear:
             tf_layer.self_attention.linear_proj.bias.data.copy_(proj_bias)
+        if md.qk_layernorm:
+            tf_layer.self_attention.q_layernorm.weight.data.copy_(q_norm_weight)
+            tf_layer.self_attention.k_layernorm.weight.data.copy_(k_norm_weight)
 
 
 def set_mlp_ckpt(message, models, layer_id, md, args):

