diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index fa493fefb..47ddef178 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -831,11 +831,12 @@ def init_distributed_environment(
         # adjust the world size to take into account data parallelism
         world_size = parallel_config.world_size_across_dp
         ip = parallel_config.data_parallel_master_ip
-        port = parallel_config.get_next_dp_init_port()
+        # port = parallel_config.get_next_dp_init_port()
+        port = parallel_config.data_parallel_master_port
         distributed_init_method = f"tcp://{ip}:{port}"  # noqa
         logger.info(
-            "Adjusting world_size=%d rank=%d distributed_init_method=%s for DP",
-            world_size, rank, distributed_init_method)
+            "Adjusting world_size=%d rank=%d local_rank=%d distributed_init_method=%s for DP",
+            world_size, rank, local_rank, distributed_init_method)
     if not torch.distributed.is_initialized():
         assert distributed_init_method is not None, (
             "distributed_init_method must be provided when initializing "
@@ -1089,7 +1090,10 @@ def cleanup_dist_env_and_memory(shutdown_ray: bool = False):
     gc.collect()
     from vllm.platforms import current_platform
     if not current_platform.is_cpu():
-        torch.cuda.empty_cache()
+        if current_platform.is_out_of_tree():
+            current_platform.empty_cache()
+        else:
+            torch.cuda.empty_cache()
     try:
         torch._C._host_emptyCache()
     except AttributeError:

