diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 86e6d9752..94d49eae7 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -74,6 +74,19 @@ TModelInputForGPU = TypeVar('TModelInputForGPU', bound="ModelInputForGPU")
 torch._dynamo.config.cache_size_limit = 128
 torch._dynamo.config.accumulated_cache_size_limit = 128
 
+# --- FLAGSCALE MODIFICATION BEG ---
+# Know more about FlagGems: https://github.com/FlagOpen/FlagGems
+import os
+if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
+    try:
+        import flag_gems
+        flag_gems.enable()
+        logger.info("Successfully enabled flag_gems as default ops implementation.")
+    except ImportError:
+        logger.warning("Failed to import 'flag_gems'. Falling back to default implementation.")
+    except Exception as e:
+        logger.warning(f"Failed to enable 'flag_gems': {e}. Falling back to default implementation.")
+# --- FLAGSCALE MODIFICATION END ---
 
 @dataclass(frozen=True)
 class ModelInputForGPU(ModelRunnerInputBase):

