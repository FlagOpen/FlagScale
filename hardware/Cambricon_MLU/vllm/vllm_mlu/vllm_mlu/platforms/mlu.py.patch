diff --git a/vllm_mlu/vllm_mlu/platforms/mlu.py b/vllm_mlu/vllm_mlu/platforms/mlu.py
new file mode 100644
index 000000000..31938da64
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/platforms/mlu.py
@@ -0,0 +1,175 @@
+from functools import lru_cache
+from typing import TYPE_CHECKING, Optional, Tuple
+
+import os
+import torch
+
+import vllm.envs as envs
+from vllm.logger import init_logger
+from vllm.platforms.interface import (DeviceCapability, Platform,
+                                      PlatformEnum)
+
+from vllm_mlu.model_executor.layers.quantization import (
+    register_fake_mlu_quantization_methods
+)
+
+if TYPE_CHECKING:
+    from vllm.config import ModelConfig, VllmConfig
+    from vllm.utils import FlexibleArgumentParser
+else:
+    ModelConfig = None
+    VllmConfig = None
+    FlexibleArgumentParser = None
+
+from vllm_mlu._mlu_utils import VLLM_LOGITS_USE_ALL_GATHER
+
+logger = init_logger(__name__)
+
+
+envs.environment_variables.update({
+    "MLU_VISIBLE_DEVICES":
+    lambda: os.environ.get("MLU_VISIBLE_DEVICES", None)
+})
+
+
+register_fake_mlu_quantization_methods()
+
+
+class MLUPlatform(Platform):
+    _enum = PlatformEnum.OOT
+    device_name: str = "mlu"
+    device_type: str = "mlu"
+    dispatch_key: str = "MLU"
+    ray_device_key: str = "GPU"
+    device_control_env_var: str = "MLU_VISIBLE_DEVICES"
+    simple_compile_backend: str = "eager"  # Disable torch.compile()
+
+    supported_quantization: list[str] = ["weightonly", "smoothquant",
+                                         "awq_mlu", "gptq_mlu", "fp8"]
+    additional_env_vars: list[str] = ["VLLM_LATENCY_DEBUG",
+                                      "VLLM_LATENCY_DEBUG_NO_DEVICE",
+                                      "MLU_GRAPH_CAPTURE_LIST",
+                                      "DPSK_MCC_PARALLEL_NUM",
+                                      "VLLM_LOGITS_USE_ALL_GATHER"]
+
+    @classmethod
+    def pre_register_and_update(cls,
+                                parser: Optional[FlexibleArgumentParser] = None
+                                ) -> None:
+        pass
+
+    @classmethod
+    def get_attn_backend_cls(cls, selected_backend, head_size, dtype,
+                             kv_cache_dtype, block_size, use_v1, use_mla) -> str:
+        if use_v1:
+            logger.warning(
+                f"AttentionBackend v1 is not supported on MLU now, forcing AttentionBackend to v0.")
+        if use_mla:
+            logger.info(f"Select MLUMLAFlashAttentionBackend.")
+            return "vllm_mlu.attention.backends.mlu_flash_attn.MLUMLAFlashAttentionBackend"
+        logger.info(f"Select MLUFlashAttentionBackend.")
+        return "vllm_mlu.attention.backends.mlu_flash_attn.MLUFlashAttentionBackend"
+
+    @classmethod
+    @lru_cache(maxsize=8)
+    def get_device_capability(cls, device_id: int = 0) -> DeviceCapability:
+        major, minor = torch.mlu.get_device_capability(device_id)
+        return DeviceCapability(major=major, minor=minor)
+
+    @classmethod
+    @lru_cache(maxsize=8)
+    def get_device_name(cls, device_id: int = 0) -> str:
+        return torch.mlu.get_device_name(device_id)
+
+    @classmethod
+    def get_device_total_memory(cls, device_id: int = 0) -> int:
+        device_props = torch.mlu.get_device_properties(device_id)
+        return device_props.total_memory
+
+    @classmethod
+    def set_device(cls, device: torch.device):
+        torch.mlu.set_device(device)
+
+    @classmethod
+    def empty_cache(cls):
+        torch.mlu.empty_cache()
+
+    @classmethod
+    def synchronize(cls):
+        torch.mlu.synchronize()
+
+    @classmethod
+    def mem_get_info(cls) -> Tuple[int, int]:
+        return torch.mlu.mem_get_info()
+
+    @classmethod
+    def is_async_output_supported(cls, enforce_eager: Optional[bool]) -> bool:
+        return True
+
+    @classmethod
+    def inference_mode(cls):
+        return torch.no_grad()
+
+    @classmethod
+    def check_and_update_config(cls, vllm_config: VllmConfig) -> None:
+        from vllm.config import CompilationLevel
+        compilation_config = vllm_config.compilation_config
+        if compilation_config.level != CompilationLevel.NO_COMPILATION:
+            logger.warning(
+                "Compilation level %s is not supported on MLU now, forcing compilation level to NO_COMPILATION",
+                compilation_config.level)
+            compilation_config.level = CompilationLevel.NO_COMPILATION
+
+        parallel_config = vllm_config.parallel_config
+        scheduler_config = vllm_config.scheduler_config
+
+        if parallel_config.worker_cls == "auto":
+            if scheduler_config.is_multi_step:
+                if envs.VLLM_USE_V1:
+                    raise NotImplementedError(f"vllm v1 is not supported on mlu platform.")
+                else:
+                    parallel_config.worker_cls = \
+                        "vllm_mlu.worker.multi_step_mlu_worker.MLUMultiStepWorker"
+            elif vllm_config.speculative_config:
+                parallel_config.worker_cls = \
+                        "vllm.spec_decode.spec_decode_worker.create_spec_worker"
+                parallel_config.sd_worker_cls = \
+                        "vllm_mlu.worker.mlu_worker.MLUWorker"
+            else:
+                if envs.VLLM_USE_V1:
+                    raise NotImplementedError(f"vllm v1 is not supported on mlu platform.")
+                else:
+                    parallel_config.worker_cls = "vllm_mlu.worker.mlu_worker.MLUWorker"
+
+        cache_config = vllm_config.cache_config
+        if cache_config and cache_config.block_size is None:
+            cache_config.block_size = 16
+
+    @classmethod
+    def get_current_memory_usage(cls,
+                                 device: Optional[torch.types.Device] = None
+                                 ) -> float:
+        torch.mlu.reset_peak_memory_stats(device)
+        return torch.mlu.max_memory_allocated(device)
+
+    @classmethod
+    def get_punica_wrapper(cls) -> str:
+        return "vllm_mlu.lora.punica_wrapper.punica_mlu.PunicaWrapperMLU"
+
+    @classmethod
+    def get_device_communicator_cls(cls) -> str:
+        return "vllm_mlu.distributed.device_communicators.mlu_communicator.MLUCommunicator"
+
+    @classmethod
+    def use_all_gather(cls) -> bool:
+        from vllm.config import get_current_vllm_config
+        parallel_config = get_current_vllm_config().parallel_config
+        return (VLLM_LOGITS_USE_ALL_GATHER or parallel_config.distributed_executor_backend
+                == "external_launcher")
+
+    @classmethod
+    def supports_v1(cls, model_config: ModelConfig) -> bool:
+        """Returns whether the current platform can support v1 for the supplied
+        model configuration.
+        """
+        return False

