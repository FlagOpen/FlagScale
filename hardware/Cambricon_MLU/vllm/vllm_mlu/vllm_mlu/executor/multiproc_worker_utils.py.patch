diff --git a/vllm_mlu/vllm_mlu/executor/multiproc_worker_utils.py b/vllm_mlu/vllm_mlu/executor/multiproc_worker_utils.py
new file mode 100644
index 000000000..1ad09a58a
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/executor/multiproc_worker_utils.py
@@ -0,0 +1,95 @@
+# SPDX-License-Identifier: Apache-2.0
+
+import os
+import sys
+from multiprocessing import Queue
+from typing import Any, Callable
+
+import torch
+
+from vllm.config import VllmConfig
+from vllm.logger import init_logger
+from vllm.utils import get_mp_context, run_method
+from vllm.executor import multiproc_worker_utils
+from vllm.executor.multiproc_worker_utils import (_TERMINATE,
+                                                  Result, _add_prefix)
+from vllm.platforms import current_platform
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+logger = init_logger(__name__)
+
+
+def vllm__executor__multiproc_worker_utils___run_worker_process(
+    worker_factory: Callable[[VllmConfig, int], Any],
+    task_queue: Queue,
+    result_queue: Queue,
+    vllm_config: VllmConfig,
+    rank: int,
+) -> None:
+    """Worker process event loop"""
+
+    # Add process-specific prefix to stdout and stderr
+    process_name = get_mp_context().current_process().name
+    pid = os.getpid()
+    _add_prefix(sys.stdout, process_name, pid)
+    _add_prefix(sys.stderr, process_name, pid)
+
+    # Initialize worker
+    worker = worker_factory(vllm_config, rank)
+    del worker_factory
+
+    # Accept tasks from the engine in task_queue
+    # and return task output in result_queue
+    logger.info("Worker ready; awaiting tasks")
+    try:
+        for items in iter(task_queue.get, _TERMINATE):
+            output = None
+            exception = None
+            task_id, method, args, kwargs = items
+            try:
+                output = run_method(worker, method, args, kwargs)
+            except SystemExit:
+                raise
+            except KeyboardInterrupt:
+                break
+            except BaseException as e:
+                logger.exception(
+                    "Exception in worker %s while processing method %s.",
+                    process_name, method)
+                exception = e
+            result_queue.put(
+                Result(task_id=task_id, value=output, exception=exception))
+    except KeyboardInterrupt:
+        pass
+    except Exception:
+        logger.exception("Worker failed")
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: avoid torch gpu migration go into cuda
+    '''
+    if not current_platform.is_out_of_tree():
+        # Flush TunableOp results when TunableOp is enabled and
+        # online (in situ) tuning is enabled.
+        # Offline tuning API (record_untuned_is_enabled()) only
+        # available in PyTorch 2.6 or later.
+        if torch.cuda.is_available():
+            import torch.cuda.tunable as tunable
+            if (tunable.is_enabled() and tunable.tuning_is_enabled()
+                    and not tunable.record_untuned_is_enabled()):
+                tunable.write_file()
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    logger.info("Worker exiting")
+
+
+MluHijackObject.apply_hijack(
+    multiproc_worker_utils,
+    multiproc_worker_utils._run_worker_process,
+    vllm__executor__multiproc_worker_utils___run_worker_process
+)
\ No newline at end of file

