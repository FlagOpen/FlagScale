diff --git a/vllm_mlu/vllm_mlu/executor/executor_base.py b/vllm_mlu/vllm_mlu/executor/executor_base.py
new file mode 100644
index 000000000..0cf55e36c
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/executor/executor_base.py
@@ -0,0 +1,40 @@
+from vllm.utils import run_method
+from vllm.executor.executor_base import ExecutorBase
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__executor__executor_base__ExecutorBase__get_latency(self) -> float:
+    '''
+    requires that torch.mlu.synchronize() be executed before this function
+    for getting an accurate reading
+    '''
+    latency = run_method(self.driver_worker,
+                         "get_latency", args=[], kwargs={})
+    return latency
+
+
+def vllm__executor__executor_base__ExecutorBase__recapture_model(
+    self,
+    enable_context_mlugraph,
+    context_batch_size_to_capture,
+    context_seq_len_to_capture
+) -> None:
+    return self.collective_rpc("recapture_model", args=(
+        enable_context_mlugraph, context_batch_size_to_capture, context_seq_len_to_capture))
+
+
+def vllm__executor__executor_base__ExecutorBase__get_memory_usage(self):
+    memory_usage = run_method(self.driver_worker,
+                              "get_memory_usage", args=[], kwargs={})
+    return memory_usage
+
+
+MluHijackObject.apply_hijack(ExecutorBase,
+                             "get_latency",
+                             vllm__executor__executor_base__ExecutorBase__get_latency)
+MluHijackObject.apply_hijack(ExecutorBase,
+                             "recapture_model",
+                             vllm__executor__executor_base__ExecutorBase__recapture_model)
+MluHijackObject.apply_hijack(ExecutorBase,
+                             "get_memory_usage",
+                             vllm__executor__executor_base__ExecutorBase__get_memory_usage)
\ No newline at end of file

