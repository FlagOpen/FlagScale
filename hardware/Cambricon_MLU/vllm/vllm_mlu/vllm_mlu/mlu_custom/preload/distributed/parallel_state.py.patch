diff --git a/vllm_mlu/vllm_mlu/mlu_custom/preload/distributed/parallel_state.py b/vllm_mlu/vllm_mlu/mlu_custom/preload/distributed/parallel_state.py
new file mode 100644
index 000000000..99c513d71
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/mlu_custom/preload/distributed/parallel_state.py
@@ -0,0 +1,109 @@
+import torch
+import torch.distributed as dist
+
+from vllm.distributed.parallel_state import GroupCoordinator
+from vllm.distributed.device_communicators.base_device_communicator import (
+    DeviceCommunicatorBase
+)
+
+from vllm_mlu.distributed.device_communicators.mlu_communicator import MLUCommunicator
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__distributed__parallel_state__GroupCoordinator__all_reduce(
+    self,
+    input_: torch.Tensor,
+    async_op: bool = False,
+) -> torch.Tensor:
+    """
+    User-facing all-reduce function before we actually call the
+    all-reduce operation.
+
+    We need this because Dynamo does not support passing an arbitrary
+    object (`self` in this case) to a custom op. We need to pass the
+        group name as a string, and then look up the group coordinator from
+        the group name, dispatch the all-reduce operation to the group
+        coordinator.
+
+    In addition, PyTorch custom ops do not support mutation or returning
+    a new tensor in the same op. So we always make the all-reduce operation
+    out-of-place.
+    """
+    # Bypass the function if we are using only 1 GPU.
+    if self.world_size == 1:
+        return input_
+
+    if self.use_custom_op_call:
+        return torch.ops.vllm.all_reduce(input_,
+                                            group_name=self.unique_name)
+    else:
+        '''
+        =============================
+        Modify by custom vllm_mlu
+        =============================
+        @brief: use async all reduce when preload weights.
+        '''
+        return self._all_reduce_out_place(input_, async_op=async_op)
+        '''
+        ==================
+        End of custom MLU Hijack
+        ==================
+        '''
+
+
+def vllm__distributed__parallel_state__GroupCoordinator___all_reduce_out_place(
+    self,
+    input_: torch.Tensor,
+    async_op: bool = False,
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by custom vllm_mlu
+    =============================
+    @brief: use async all reduce when preload weights.
+    '''
+    return self.device_communicator.all_reduce(input_, async_op=async_op)
+    '''
+    ==================
+    End of custom MLU Hijack
+    ==================
+    '''
+
+
+def vllm__distributed__device_communicators__base_device_communicator__DeviceCommunicatorBase__all_reduce(
+    self,
+    input_: torch.Tensor,
+    async_op: bool = False,
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by custom vllm_mlu
+    =============================
+    @brief: use async all reduce when preload weights.
+    '''
+    handle = dist.all_reduce(input_, group=self.device_group, async_op=async_op)
+    if async_op:
+        return handle
+    return input_
+    '''
+    ==================
+    End of custom MLU Hijack
+    ==================
+    '''
+
+
+MluHijackObject.apply_hijack(
+    GroupCoordinator,
+    GroupCoordinator.all_reduce,
+    vllm__distributed__parallel_state__GroupCoordinator__all_reduce
+)
+MluHijackObject.apply_hijack(
+    GroupCoordinator,
+    GroupCoordinator._all_reduce_out_place,
+    vllm__distributed__parallel_state__GroupCoordinator___all_reduce_out_place
+)
+MluHijackObject.apply_hijack(
+    DeviceCommunicatorBase,
+    DeviceCommunicatorBase.all_reduce,
+    vllm__distributed__device_communicators__base_device_communicator__DeviceCommunicatorBase__all_reduce
+)
\ No newline at end of file

