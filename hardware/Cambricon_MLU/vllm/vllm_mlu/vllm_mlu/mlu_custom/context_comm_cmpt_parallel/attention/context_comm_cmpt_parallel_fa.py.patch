diff --git a/vllm_mlu/vllm_mlu/mlu_custom/context_comm_cmpt_parallel/attention/context_comm_cmpt_parallel_fa.py b/vllm_mlu/vllm_mlu/mlu_custom/context_comm_cmpt_parallel/attention/context_comm_cmpt_parallel_fa.py
new file mode 100644
index 000000000..35ad68ce8
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/mlu_custom/context_comm_cmpt_parallel/attention/context_comm_cmpt_parallel_fa.py
@@ -0,0 +1,231 @@
+import torch
+from typing import List, Optional
+
+from vllm.attention.backends.abstract import AttentionType
+from vllm.attention.backends.utils import get_num_prefill_decode_query_kv_tokens
+
+from vllm_mlu._mlu_utils import *
+from vllm_mlu import _mlu_ops as mlu_ops
+from vllm_mlu.attention.backends.mlu_flash_attn import (MLUFlashAttentionMetadata,
+                                                        _get_query_key_seq_metadata,
+                                                        _get_causal_option)
+
+
+def context_attn_comm_cmpt_parallel_flash_attention(
+    query: torch.Tensor,
+    key: torch.Tensor,
+    value: torch.Tensor,
+    num_heads: int,
+    head_size: int,
+    num_kv_heads: int,
+    attn_metadata: MLUFlashAttentionMetadata,
+    kv_cache: List[torch.Tensor],
+    kv_cache_dtype: str,
+    softmax_scale: float,
+    cncl_comm: int,
+    smooth: torch.Tensor,
+    qweight: torch.Tensor,
+    per_channel_scale: torch.Tensor,
+    parallel_num: int,
+    residual: Optional[torch.Tensor] = None,
+    window_size: Optional[List[int]] = None,
+    alibi_slopes: Optional[torch.Tensor] = None,
+    attn_type: Optional[AttentionType] = AttentionType.DECODER,
+    use_mla: bool = False,
+) -> torch.Tensor:
+    """Forward pass with FlashAttention.
+
+    Args:
+        query: shape = [num_tokens, num_heads, head_size]
+        key: shape = [num_tokens, num_kv_heads, head_size]
+        value: shape = [num_tokens, num_kv_heads, head_size]
+        output: shape = [num_tokens, num_heads, head_size]
+        kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
+            NOTE: kv_cache will be an empty tensor with shape [0]
+            for profiling run.
+        attn_metadata: Metadata for attention.
+    NOTE: It in-place updates the output tensor.
+    """
+
+    # Reshape the query, key, and value tensors.
+    query = query.view(-1, num_heads, head_size)
+    if (key is not None) and (key is not None):
+        key = key.view(-1, num_kv_heads, head_size)
+        value = value.view(-1, num_kv_heads, head_size)
+
+    if (attn_type == AttentionType.ENCODER
+            and (not attn_metadata.is_all_encoder_attn_metadata_set)):
+        raise AttributeError("Encoder attention requires setting "
+                                "encoder metadata attributes.")
+    elif (attn_type == AttentionType.ENCODER_DECODER
+            and (not attn_metadata.is_all_cross_attn_metadata_set)):
+        raise AttributeError("Encoder/decoder cross-attention "
+                                "requires setting cross-attention "
+                                "metadata attributes.")
+
+    if kv_cache[0].numel() > 0:
+        kv_cache_, kv_cache_scale_ = kv_cache
+        key_cache = kv_cache_[0]
+        value_cache = None if use_mla else kv_cache_[1]
+        key_cache_scale, value_cache_scale = None, None
+        if kv_cache_scale_.numel() > 0:
+            key_cache_scale = kv_cache_scale_[0]
+            value_cache_scale = None if use_mla else kv_cache_scale_[1]
+
+        # We skip updating the KV cache under two conditions:
+        #  a. When the Attention Type is ENCODER. In this phase, we compute
+        #     only the encoder attention without updating the cache.
+        #  b. When both Key and Value are None. This occurs during
+        #     cross-attention computation in the decoding phase, where the
+        #     KV cache is already populated with the cross-attention
+        #     tensor. Thus, we skip cache updates during this time.
+        if (attn_type != AttentionType.ENCODER) and (key is not None) and (
+                value is not None):
+            if attn_type == AttentionType.ENCODER_DECODER:
+                # Update cross-attention KV cache (prefill-only)
+                updated_slot_mapping = attn_metadata.cross_slot_mapping
+            else:
+                # Update self-attention KV cache (prefill/decode)
+                updated_slot_mapping = attn_metadata.slot_mapping
+
+            # Reshape the input keys and values and store them in the cache.
+            # If kv_cache is not provided, the new key and value tensors are
+            # not cached. This happens during the initial memory
+            # profiling run.
+            value_to_cache = None if use_mla else value
+            if use_mla and attn_metadata.prefill_metadata:
+                # MLA save cache info in models before flashattn
+                pass
+            else:
+                if kv_cache_dtype == 'int8':
+                    mlu_ops.quant_to_paged_cache(
+                        key,
+                        value_to_cache,
+                        key_cache,
+                        value_cache,
+                        key_cache_scale,
+                        value_cache_scale,
+                        updated_slot_mapping.flatten()
+                    )
+                else:
+                    mlu_ops.reshape_paged_cache(
+                        key,
+                        value_to_cache,
+                        key_cache,
+                        value_cache,
+                        updated_slot_mapping.flatten()
+                    )
+
+    (num_prefill_query_tokens, num_prefill_kv_tokens,
+    num_decode_query_tokens) = \
+        get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)
+    decode_query = query[num_prefill_query_tokens:]
+    # QKV for prefill.
+    query = query[:num_prefill_query_tokens]
+    assert query.shape[0] == num_prefill_query_tokens
+    assert decode_query.shape[0] == num_decode_query_tokens
+
+    if prefill_meta := attn_metadata.prefill_metadata:
+        alibi_slopes = None if alibi_slopes is None else \
+                            alibi_slopes.repeat(attn_metadata.num_prefills, 1)
+        # Prompt run.
+        if (kv_cache[0].numel() == 0 or prefill_meta.block_tables is None
+                or prefill_meta.block_tables.numel() == 0):
+            # normal attention
+            # When block_tables are not filled, it means q and k are the
+            # prompt, and they have the same length.
+            q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len = \
+                _get_query_key_seq_metadata(prefill_meta, True, attn_type)
+
+            key = key[:num_prefill_kv_tokens]
+            value = value[:num_prefill_kv_tokens]
+
+            output = mlu_ops.flash_attn_sq_mm_allreduce(
+                cncl_comm,
+                query,
+                key,
+                value,
+                q_seq_start_loc,
+                k_seq_start_loc,
+                alibi_slopes,
+                None,
+                smooth,
+                qweight,
+                per_channel_scale,
+                None,
+                q_seq_len,
+                k_seq_len,
+                softmax_scale,
+                _get_causal_option(attn_type),
+                -1 if window_size is None else window_size[0],
+                -1 if window_size is None else window_size[1],
+                torch.float,
+                parallel_num
+            )
+        else:
+            # prefix-enabled attention
+            assert attn_type == AttentionType.DECODER, (
+                "Only decoder-only models support prefix caching")
+            assert prefill_meta.seq_lens is not None
+            max_seq_len = max(prefill_meta.seq_lens)
+
+            if kv_cache_dtype == 'int8' and \
+                    prefill_meta.chunked_prefill_enabled:
+                _, head_num_kv, _, head_size_qk = key_cache.shape
+                total_seqlens = prefill_meta.seq_start_loc[-1].item()
+                key_cache_float = torch.zeros((total_seqlens, head_num_kv, head_size_qk),
+                                                dtype=query.dtype,
+                                                device=key_cache.device)
+                value_cache_float = None
+                if value_cache is not None:
+                    _, head_num_kv, _, head_size_v = value_cache.shape
+                    value_cache_float = torch.zeros((total_seqlens, head_num_kv, head_size_v),
+                                                    dtype=query.dtype,
+                                                    device=value_cache.device)
+                mlu_ops.dequant_from_paged_cache(
+                    key=key_cache_float,
+                    value=value_cache_float,
+                    key_cache=key_cache,
+                    value_cache=value_cache,
+                    key_cache_quant_scale=key_cache_scale,
+                    value_cache_quant_scale=value_cache_scale,
+                    context_lengths=prefill_meta.seq_lens_tensor,
+                    max_context_len=prefill_meta.max_prefill_seq_len,
+                    context_seq_offset=None,
+                    block_tables=prefill_meta.block_tables,
+                    quant_mode=1,
+                    quant_bit=8
+                )
+                block_tables = None
+            else:
+                key_cache_float = key_cache
+                value_cache_float = value_cache
+                block_tables = prefill_meta.block_tables
+
+            output = mlu_ops.flash_attn_sq_mm_allreduce(
+                cncl_comm,
+                query,
+                key_cache_float,
+                value_cache_float,
+                prefill_meta.query_start_loc,
+                prefill_meta.seq_start_loc,
+                alibi_slopes,
+                None,
+                smooth,
+                qweight,
+                per_channel_scale,
+                None,
+                prefill_meta.max_query_len,
+                max_seq_len,
+                softmax_scale,
+                True,
+                -1 if window_size is None else window_size[0],
+                -1 if window_size is None else window_size[1],
+                torch.float,
+                parallel_num
+            )
+
+    # Add residual.
+    if residual is not None:
+        output = output + residual
+    return output

