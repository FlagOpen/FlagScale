diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen3_moe.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen3_moe.py
new file mode 100644
index 000000000..e92dbf35b
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen3_moe.py
@@ -0,0 +1,444 @@
+import torch
+import re
+import torch.nn.functional as F
+
+from typing import Optional, Tuple, Iterable, Set
+from transformers import PretrainedConfig
+
+from vllm.config import CacheConfig
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.distributed import tensor_model_parallel_all_reduce
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.layers.linear import ReplicatedLinear
+from vllm.model_executor.models.qwen3_moe import (
+    Qwen3MoeAttention, Qwen3MoeDecoderLayer, Qwen3MoeForCausalLM, Qwen3MoeModel)
+from vllm.model_executor.models.utils import (extract_layer_index,
+                                              is_pp_missing_parameter)
+from vllm.sequence import IntermediateTensors
+from vllm.logger import init_logger
+
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_per_tensor_smoothquant, is_per_token_smoothquant,
+    quant_fusion_with_rmsnorm)
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+logger = init_logger(__name__)
+
+class Qwen3MoeSparseMoeBlock(SparseMoeMlp):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__(num_experts=config.num_experts,
+                         top_k=config.num_experts_per_tok,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.moe_intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         renormalize=config.norm_topk_prob,
+                         hidden_act=config.hidden_act,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True)
+
+    def forward(self, hidden_states: torch.Tensor, residual: Optional[torch.Tensor] = None) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.forward_experts(hidden_states, router_logits, residual)
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
+
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+def vllm__module_executor__models__qwen3moe__Qwen3MoeAttention__forward(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    residual: Optional[torch.Tensor] = None,
+    smooth_quant_scale: Optional[torch.Tensor] = None,
+) -> torch.Tensor:
+    qkv, _ = self.qkv_proj(hidden_states, smooth_quant_scale)
+    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+    # Add qk-norm
+    q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                       self.head_dim)
+    q_by_head = self.q_norm.forward_native(q_by_head)
+    q = q_by_head.view(q.shape)
+
+    k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                       self.head_dim)
+    k_by_head = self.k_norm.forward_native(k_by_head)
+    k = k_by_head.view(k.shape)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: pack q & k to fit tmo.apply_rotary
+    '''
+    qk = torch.cat([q, k], dim=-1)
+    self.rotary_emb(positions, qk.view(-1, self.num_heads + self.num_kv_heads, self.head_dim))
+    q, k = qk.split([self.q_size, self.kv_size], dim=-1)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    attn_output = self.attn(q, k, v)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add residual
+    '''
+    output, _ = self.o_proj(attn_output, residual)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return output
+
+def vllm__module_executor__models__qwen3moe__Qwen3MoeDecoderLayer____init__(
+    self,
+    config: PretrainedConfig,
+    cache_config: Optional[CacheConfig] = None,
+    quant_config: Optional[QuantizationConfig] = None,
+    prefix: str = "",
+) -> None:
+    super(Qwen3MoeDecoderLayer, self).__init__()
+    self.hidden_size = config.hidden_size
+    rope_theta = getattr(config, "rope_theta", 10000)
+    rope_scaling = getattr(config, "rope_scaling", None)
+    max_position_embeddings = getattr(config, "max_position_embeddings",
+                                      8192)
+    self.self_attn = Qwen3MoeAttention(
+        hidden_size=self.hidden_size,
+        num_heads=config.num_attention_heads,
+        num_kv_heads=config.num_key_value_heads,
+        rope_theta=rope_theta,
+        rope_scaling=rope_scaling,
+        max_position_embeddings=max_position_embeddings,
+        rms_norm_eps=config.rms_norm_eps,
+        qkv_bias=getattr(config, 'attention_bias', False),
+        head_dim=getattr(config, 'head_dim', None),
+        cache_config=cache_config,
+        quant_config=quant_config,
+        prefix=f"{prefix}.self_attn",
+    )
+
+    # `mlp_only_layers` in the config.
+    layer_idx = extract_layer_index(prefix)
+    mlp_only_layers = ([] if not hasattr(config, "mlp_only_layers") else
+                       config.mlp_only_layers)
+    if (layer_idx not in mlp_only_layers) and (
+            config.num_experts > 0 and
+        (layer_idx + 1) % config.decoder_sparse_step == 0):
+        self.mlp = Qwen3MoeSparseMoeBlock(config=config,
+                                          quant_config=quant_config,
+                                          prefix=f"{prefix}.mlp")
+    else:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use FeedForward instead of MLP
+        '''
+        self.mlp = FeedForward(
+            hidden_size=config.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            up_proj_name='gate_up_proj',
+            is_gated=True,
+            down_proj_name='down_proj',
+            bias=False,
+            quant_config=quant_config,
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+    self.input_layernorm = RMSNorm(config.hidden_size,
+                                   eps=config.rms_norm_eps)
+    self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                            eps=config.rms_norm_eps)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: prepare to perf per-tensor sq cases if suitable. For moe
+        model, we only do quant fusion in attn block.
+    '''
+    self.is_per_tesnor_sq_perf_cases = is_per_tensor_smoothquant(quant_config)
+    self.is_per_token_sq_perf_cases = is_per_token_smoothquant(quant_config)
+    if self.is_per_tesnor_sq_perf_cases or self.is_per_token_sq_perf_cases:
+        self.self_attn.qkv_proj.quant_method.skip_quant_input = True
+        self.quant_fusion_attn_layernorm = None
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__module_executor__models__qwen3moe__Qwen3MoeForCausalLM__load_weights(
+    self,
+    weights: Iterable[Tuple[str, torch.Tensor]]
+):
+    start_expert_id = 0
+    stacked_params_mapping = [
+        # (param_name, shard_name, shard_id)
+        ("qkv_proj", "q_proj", "q"),
+        ("qkv_proj", "k_proj", "k"),
+        ("qkv_proj", "v_proj", "v"),
+        ("gate_up_proj", "gate_proj", 0),
+        ("gate_up_proj", "up_proj", 1),
+    ]
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: delete expert_params_mapping for useless
+    '''
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    params_dict = dict(self.named_parameters())
+    loaded_params: Set[str] = set()
+    for name, loaded_weight in weights:
+        if "rotary_emb.inv_freq" in name:
+            continue
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace expert_id in weight to named_expert_id in params_dict
+        '''
+        if start_expert_id > 0 and "mlp.experts." in name:
+            expert_str = re.search(r'experts\.\d+', name).group(0)
+            expert_id = int(expert_str.split(".")[1])
+            named_expert_id = expert_id - start_expert_id
+            old_expert_name = f"experts.{expert_id}"
+            new_expert_name = f"experts.{named_expert_id}"
+            name = name.replace(old_expert_name, new_expert_name)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        for (param_name, weight_name, shard_id) in stacked_params_mapping:
+            if weight_name not in name:
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: delete if "mlp.experts" in name: continue condition
+            '''
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            name = name.replace(weight_name, param_name)
+            # Skip loading extra bias for GPTQ models.
+            if ((name.endswith(".bias") or name.endswith("_bias"))
+                        and name not in params_dict):
+                continue
+            # Skip layers on other devices.
+            if is_pp_missing_parameter(name, self):
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+            '''
+            # Skip experts that are not assigned to this worker.
+            if (("mlp.experts." in name or "mlp.shared_expert." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            param = params_dict[name]
+            weight_loader = param.weight_loader
+            weight_loader(param, loaded_weight, shard_id)
+            break
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: delete for mapping in expert_params_mapping condition
+            '''
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # Skip loading extra bias for GPTQ models.
+            if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                continue
+            # Skip layers on other devices.
+            if is_pp_missing_parameter(name, self):
+                continue
+            # Remapping the name of FP8 kv-scale.
+            if name.endswith("kv_scale"):
+                remapped_kv_scale_name = name.replace(
+                    ".kv_scale", ".attn.kv_scale")
+                if remapped_kv_scale_name not in params_dict:
+                    logger.warning_once(
+                        "Found kv scale in the checkpoint "
+                        f"(e.g. {name}), but not found the expected "
+                        f"name in the model "
+                        f"(e.g. {remapped_kv_scale_name}). "
+                        "kv-scale is not loaded.")
+                    continue
+                else:
+                    name = remapped_kv_scale_name
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition
+            '''
+            # Skip experts that are not assigned to this worker.
+            if (("mlp.experts." in name or "mlp.shared_expert." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            param = params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
+        loaded_params.add(name)
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: pack params
+    '''
+    for name, m in self.model.named_modules():
+        if isinstance(m, SparseMoeMlp):
+            m.pack_params()
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return loaded_params
+
+
+def vllm__module_executor__models__qwen3moe__Qwen3MoeDecoderLayer__forward(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: perf model by:
+    1) add residual in matmul;
+    2) fuse quantization in layernorm in per-tensor sq case;
+    '''
+    attn_layernorm = self.input_layernorm
+    if self.is_per_tesnor_sq_perf_cases or self.is_per_token_sq_perf_cases:
+        if self.quant_fusion_attn_layernorm is None:
+            if self.is_per_token_sq_perf_cases:
+                attn_quant_scale = self.self_attn.qkv_proj.smooth
+            else:
+                attn_quant_scale = self.self_attn.qkv_proj.scale_to_int
+            self.quant_fusion_attn_layernorm = quant_fusion_with_rmsnorm(
+                self.input_layernorm, attn_quant_scale,
+                dynamic_quant=self.is_per_token_sq_perf_cases)
+        attn_layernorm = self.quant_fusion_attn_layernorm
+
+    return decoder_layer_forward_base(
+        positions=positions,
+        hidden_states=hidden_states,
+        input_layernorm=attn_layernorm,
+        self_attn=self.self_attn,
+        post_layernorm=self.post_attention_layernorm,
+        mlp=self.mlp,
+        input_norm_fuse_en=self.is_per_token_sq_perf_cases
+    )
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__module_executor__models__qwen3moe__Qwen3MoeModel__forward(
+    self,
+    input_ids: torch.Tensor,
+    positions: torch.Tensor,
+    intermediate_tensors: Optional[IntermediateTensors] = None,
+    inputs_embeds: Optional[torch.Tensor] = None,
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    '''
+    return decoder_model_forward_base_pp(
+        input_ids=input_ids,
+        positions=positions,
+        intermediate_tensors=intermediate_tensors,
+        layers=self.layers,
+        start_layer=self.start_layer,
+        end_layer=self.end_layer,
+        get_input_embeddings=self.embed_tokens,
+        norm=self.norm,
+        inputs_embeds=inputs_embeds,
+    )
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+MluHijackObject.apply_hijack(Qwen3MoeAttention,
+                             Qwen3MoeAttention.forward,
+                             vllm__module_executor__models__qwen3moe__Qwen3MoeAttention__forward)
+MluHijackObject.apply_hijack(Qwen3MoeDecoderLayer,
+                             Qwen3MoeDecoderLayer.__init__,
+                             vllm__module_executor__models__qwen3moe__Qwen3MoeDecoderLayer____init__)
+MluHijackObject.apply_hijack(Qwen3MoeForCausalLM,
+                             Qwen3MoeForCausalLM.load_weights,
+                             vllm__module_executor__models__qwen3moe__Qwen3MoeForCausalLM__load_weights)
+MluHijackObject.apply_hijack(Qwen3MoeDecoderLayer,
+                             Qwen3MoeDecoderLayer.forward,
+                             vllm__module_executor__models__qwen3moe__Qwen3MoeDecoderLayer__forward)
+MluHijackObject.apply_hijack(Qwen3MoeModel,
+                             Qwen3MoeModel.forward,
+                             vllm__module_executor__models__qwen3moe__Qwen3MoeModel__forward)

