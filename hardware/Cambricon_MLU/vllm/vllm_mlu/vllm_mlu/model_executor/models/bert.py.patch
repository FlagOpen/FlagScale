diff --git a/vllm_mlu/vllm_mlu/model_executor/models/bert.py b/vllm_mlu/vllm_mlu/model_executor/models/bert.py
new file mode 100644
index 000000000..b73b850a9
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/bert.py
@@ -0,0 +1,54 @@
+import torch
+
+from vllm.model_executor.models.bert import BertSelfOutput, BertOutput
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__module_executor__models__bert__BertSelfOutput__forward(
+    self,
+    hidden_states: torch.Tensor,
+    input_tensor: torch.Tensor
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: apply residual fusion
+    '''
+    hidden_states, _ = self.dense(hidden_states, input_tensor)
+    hidden_states = self.LayerNorm(hidden_states)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return hidden_states
+
+
+def vllm__module_executor__models__bert__BertOutput__forward(
+    self,
+    hidden_states: torch.Tensor,
+    input_tensor: torch.Tensor
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: apply residual fusion
+    '''
+    hidden_states, _ = self.dense(hidden_states, input_tensor)
+    hidden_states = self.LayerNorm(hidden_states)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return hidden_states
+
+
+MluHijackObject.apply_hijack(BertSelfOutput,
+                             BertSelfOutput.forward,
+                             vllm__module_executor__models__bert__BertSelfOutput__forward)
+MluHijackObject.apply_hijack(BertOutput,
+                             BertOutput.forward,
+                             vllm__module_executor__models__bert__BertOutput__forward)

