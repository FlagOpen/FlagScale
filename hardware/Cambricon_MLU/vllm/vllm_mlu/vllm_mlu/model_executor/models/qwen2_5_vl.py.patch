diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen2_5_vl.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen2_5_vl.py
new file mode 100644
index 000000000..5fa98254c
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen2_5_vl.py
@@ -0,0 +1,276 @@
+from typing import Optional
+import torch
+import torch.nn.functional as F
+from einops import rearrange, repeat
+from vllm.logger import init_logger
+from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLVisionConfig
+from vllm.model_executor.models.qwen2_5_vl import (
+    Qwen2_5_VisionTransformer, Qwen2_5_VisionAttention)
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.attention.selector import _Backend
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+logger = init_logger(__name__)
+
+vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionTransformer__init_org = Qwen2_5_VisionTransformer.__init__
+
+def vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionTransformer____init__(
+    self,
+    vision_config: Qwen2_5_VLVisionConfig,
+    norm_eps: float = 1e-6,
+    quant_config: Optional[QuantizationConfig] = None,
+    prefix: str = ""
+):
+    vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionTransformer__init_org(
+        self, vision_config, norm_eps, quant_config, prefix)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: use mlu_ops.flash_atten for better performance
+    '''
+    self.attn_backend = _Backend.FLASH_ATTN
+
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+def vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionTransformer__forward(
+    self,
+    x: torch.Tensor,
+    grid_thw: torch.Tensor
+):
+    # patchify
+    hidden_states = x.to(device=self.device, dtype=self.dtype)
+    hidden_states = self.patch_embed(hidden_states)
+
+    # compute position embedding
+    rotary_pos_emb = self.rot_pos_emb(grid_thw)
+
+    # windows attention
+    window_index, cu_window_seqlens = self.get_window_index(grid_thw)
+    cu_window_seqlens = torch.tensor(
+        cu_window_seqlens,
+        device=hidden_states.device,
+        dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32)
+    cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)
+    seq_len, _ = hidden_states.size()
+    hidden_states = hidden_states.reshape(
+        seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
+    hidden_states = hidden_states[window_index, :, :]
+    hidden_states = hidden_states.reshape(seq_len, -1)
+    rotary_pos_emb = rotary_pos_emb.reshape(
+        seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
+    rotary_pos_emb = rotary_pos_emb[window_index, :, :]
+    rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: compute cos and sin once for rope
+    '''
+    # compute cos sin for apply_rope
+    cos = rotary_pos_emb.cos()
+    sin = rotary_pos_emb.sin()
+    cos = repeat(cos, "... d -> ... (2 d)")
+    sin = repeat(sin, "... d -> ... (2 d)")
+    rotary_pos_emb.cos = cos
+    rotary_pos_emb.sin = sin
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    # compute cu_seqlens
+    cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
+                                         grid_thw[:, 0]).cumsum(
+                                             dim=0, dtype=torch.int32)
+    cu_seqlens = F.pad(cu_seqlens, (1, 0), "constant", 0)
+
+    # transformers
+    hidden_states = hidden_states.unsqueeze(1)
+
+    # pre-compute seqlens for window/full attn to reduce cuMemcpy operations
+    max_seqlen_full, seqlens_full = self.compute_attn_mask_seqlen(
+        cu_seqlens)
+    max_seqlen_window, seqlens_window = self.compute_attn_mask_seqlen(
+        cu_window_seqlens)
+    for layer_num, blk in enumerate(self.blocks):
+        if layer_num in self.fullatt_block_indexes:
+            cu_seqlens_now = cu_seqlens
+            max_seqlen_now = max_seqlen_full
+            seqlens_now = seqlens_full
+        else:
+            cu_seqlens_now = cu_window_seqlens
+            max_seqlen_now = max_seqlen_window
+            seqlens_now = seqlens_window
+
+        hidden_states = blk(
+            hidden_states,
+            cu_seqlens=cu_seqlens_now,
+            rotary_pos_emb=rotary_pos_emb,
+            max_seqlen=max_seqlen_now,
+            seqlens=seqlens_now,
+        )
+
+    # For Qwen2.5-VL-3B, float16 will overflow at last block
+    # for long visual tokens sequences.
+    if hidden_states.dtype == torch.float16:
+        hidden_states = cast_overflow_tensors(hidden_states)
+
+    # adapter
+    hidden_states = self.merger(hidden_states)
+    reverse_indices = torch.argsort(window_index)
+    hidden_states = hidden_states[reverse_indices, :]
+    return hidden_states
+
+def vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionAttention__forward(
+    self,
+    x: torch.Tensor,
+    cu_seqlens: torch.Tensor,
+    rotary_pos_emb: torch.Tensor,
+    max_seqlen: Optional[int] = None,  # Only used for Flash Attention
+    seqlens: Optional[list[int]] = None,  # Only used for xFormers
+):
+    # [s, b, c] --> [s, b, 3 * head * head_dim]
+    x, _ = self.qkv(x)
+
+    # [s, b, 3 * head * head_dim] -> 3 * [s, b, head, head_dim]
+    q, k, v = self.split_qkv(x)
+    batch_size = q.shape[1]
+
+    q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
+               for x in (q, k, v))
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: apply mlu_ops.apply_rotary
+    '''
+    head_dim = q.shape[-1]
+    if rotary_pos_emb is not None:
+        sin = rotary_pos_emb.sin
+        cos = rotary_pos_emb.cos
+        from vllm_mlu import _mlu_ops as mlu_ops
+        q = q.float()
+        q = mlu_ops.rotary_embedding(
+            q, sin, cos, None, None, False, False, False, q.shape[1]
+        )
+        k = k.float()
+        k = mlu_ops.rotary_embedding(
+            k, sin, cos, None, None, False, False, False, k.shape[1]
+        )
+        q = q.type_as(v)
+        k = k.type_as(v)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    if self.attn_backend == _Backend.FLASH_ATTN:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: apply mlu_ops.flash_attention
+        '''
+        q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+        output = mlu_ops.flash_attention(q,
+                                         k,
+                                         v,
+                                         out=None,
+                                         cu_seq_lens_q=cu_seqlens,
+                                         cu_seq_lens_kv=cu_seqlens,
+                                         max_seq_len_q=max_seqlen,
+                                         max_seq_len_kv=max_seqlen,
+                                         alibi_slope=None,
+                                         attn_bias=None,
+                                         softmax_scale=head_dim ** -0.5,
+                                         is_causal=False)
+        context_layer = rearrange(output,
+                                  "(b s) ... -> b s ...",
+                                  b=batch_size)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+    elif self.attn_backend == _Backend.TORCH_SDPA:
+        # Execute attention entry by entry for speed & less VRAM.
+        outputs = []
+        for i in range(1, len(cu_seqlens)):
+            start_idx = cu_seqlens[i - 1]
+            end_idx = cu_seqlens[i]
+            q_i = q[:, start_idx:end_idx]
+            k_i = k[:, start_idx:end_idx]
+            v_i = v[:, start_idx:end_idx]
+            q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
+                             for x in [q_i, k_i, v_i])
+            output_i = F.scaled_dot_product_attention(q_i,
+                                                      k_i,
+                                                      v_i,
+                                                      dropout_p=0.0)
+            output_i = rearrange(output_i, "b h s d -> b s h d ")
+            outputs.append(output_i)
+        context_layer = torch.cat(outputs, dim=1)
+    elif self.attn_backend == _Backend.XFORMERS:
+        from xformers import ops as xops
+        from xformers.ops.fmha.attn_bias import BlockDiagonalMask
+
+        seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
+        attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,
+                                                   kv_seqlen=None)
+
+        context_layer = xops.memory_efficient_attention_forward(
+            q, k, v, attn_bias=attn_bias, p=0, scale=None)
+    context_layer = rearrange(context_layer,
+                              "b s h d -> s b (h d)").contiguous()
+
+    output, _ = self.proj(context_layer)
+    return output
+
+
+vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionAttention__init_org = Qwen2_5_VisionAttention.__init__
+
+def vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionAttention____init__(
+    self,
+    embed_dim: Optional[int] = None,
+    num_heads: Optional[int] = None,
+    projection_size: Optional[int] = None,
+    quant_config: Optional[QuantizationConfig] = None,
+    prefix: str = "",
+) -> None:
+    vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionAttention__init_org(
+            self, embed_dim, num_heads, projection_size, quant_config, prefix)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: use mlu_ops.flash_atten for better performance
+    '''
+    self.attn_backend = _Backend.FLASH_ATTN
+
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+MluHijackObject.apply_hijack(Qwen2_5_VisionTransformer,
+                             Qwen2_5_VisionTransformer.__init__,
+                             vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionTransformer____init__)
+MluHijackObject.apply_hijack(Qwen2_5_VisionTransformer,
+                             Qwen2_5_VisionTransformer.forward,
+                             vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionTransformer__forward)
+MluHijackObject.apply_hijack(Qwen2_5_VisionAttention,
+                             Qwen2_5_VisionAttention.forward,
+                             vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionAttention__forward)
+MluHijackObject.apply_hijack(Qwen2_5_VisionAttention,
+                             Qwen2_5_VisionAttention.__init__,
+                             vllm__module_executor__models__qwen2_5_vl__Qwen2_5_VisionAttention____init__)

