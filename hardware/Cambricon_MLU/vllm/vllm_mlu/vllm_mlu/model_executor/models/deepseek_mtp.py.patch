diff --git a/vllm_mlu/vllm_mlu/model_executor/models/deepseek_mtp.py b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_mtp.py
new file mode 100644
index 000000000..12dec4ac6
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_mtp.py
@@ -0,0 +1,135 @@
+# SPDX-License-Identifier: Apache-2.0
+from typing import Iterable, Set, Tuple
+
+import torch
+
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.deepseek_v2 import get_spec_layer_idx_from_weight_name, DeepseekV2MLAAttention
+from vllm.model_executor.models.deepseek_mtp import DeepSeekMTP
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__load_weights(
+        self, weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]:
+    stacked_params_mapping = [
+        ("gate_up_proj", "gate_proj", 0),
+        ("gate_up_proj", "up_proj", 1),
+    ]
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: delete expert_params_mapping for useless
+    '''
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    params_dict = dict(self.named_parameters())
+    loaded_params: Set[str] = set()
+    for name, loaded_weight in weights:
+        if "rotary_emb.inv_freq" in name:
+            continue
+        spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+        if spec_layer is None:
+            continue
+        name = self._rewrite_spec_layer_name(spec_layer, name)
+        for (param_name, weight_name, shard_id) in stacked_params_mapping:
+            # Skip non-stacked layers and experts (experts handled below).
+            if weight_name not in name:
+                continue
+            # We have mlp.experts[0].gate_proj in the checkpoint.
+            # Since we handle the experts below in expert_params_mapping,
+            # we need to skip here BEFORE we update the name, otherwise
+            # name will be updated to mlp.experts[0].gate_up_proj, which
+            # will then be updated below in expert_params_mapping
+            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+            '''
+            name = name.replace(weight_name, param_name)
+            if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # Skip loading extra bias for GPTQ models.
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+
+            param = params_dict[name]
+            weight_loader = param.weight_loader
+            weight_loader(param, loaded_weight, shard_id)
+            break
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: delete expert_params_mapping weight load
+            '''
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # Skip loading extra bias for GPTQ models.
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition
+            '''
+            if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+
+            param = params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
+
+        loaded_params.add(name)
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: pack_params
+    '''
+    for name, m in self.model.named_modules():
+        if isinstance(m, SparseMoeMlp) or isinstance(m, DeepseekV2MLAAttention):
+            m.pack_params()
+        if (isinstance(m, DeepseekV2MLAAttention)
+                 and m.use_fused_mla_qkv
+                 and self.config.q_lora_rank is not None):
+            m.weight_c = m.w_kc.view(m.num_local_heads, m.kv_lora_rank, -1).contiguous()
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    return loaded_params
+
+
+MluHijackObject.apply_hijack(DeepSeekMTP,
+                             DeepSeekMTP.load_weights,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__load_weights)

