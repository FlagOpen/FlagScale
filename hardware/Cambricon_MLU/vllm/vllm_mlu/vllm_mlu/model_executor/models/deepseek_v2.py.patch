diff --git a/vllm_mlu/vllm_mlu/model_executor/models/deepseek_v2.py b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_v2.py
new file mode 100644
index 000000000..056851de9
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_v2.py
@@ -0,0 +1,996 @@
+import types
+import torch
+from torch import nn
+from typing import Any, Dict, Iterable, Optional, Tuple, Set
+from transformers import PretrainedConfig
+
+from vllm.attention.backends.abstract import AttentionType
+from vllm.attention import Attention, AttentionMetadata
+from vllm.attention.backends.utils import get_num_prefill_decode_query_kv_tokens
+from vllm.config import CacheConfig, ModelConfig
+from vllm.forward_context import (ForwardContext,
+                                  get_forward_context)
+from vllm.distributed import (get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               ReplicatedLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.models.utils import is_pp_missing_parameter
+from vllm.model_executor.models.deepseek_v2 import (
+    DeepseekV2Attention, DeepseekV2MLAAttention, DeepseekV2DecoderLayer,
+    DeepseekV2ForCausalLM, yarn_get_mscale, get_spec_layer_idx_from_weight_name)
+from vllm.model_executor.layers.quantization.fp8 import Fp8Config
+from vllm.model_executor.layers.quantization.utils.fp8_utils import scaled_dequantize
+
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp
+from vllm_mlu.model_executor.models.layer_utils import quant_fusion_with_rmsnorm, is_per_token_smoothquant
+from vllm_mlu import _mlu_ops as mlu_ops
+
+
+class DeepseekV2MoE(SparseMoeMlp):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__(num_experts=config.n_routed_experts,
+                         top_k=config.num_experts_per_tok,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.moe_intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         renormalize=config.norm_topk_prob,
+                         hidden_act=config.hidden_act,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True,
+                         expert_group=config.n_group,
+                         topk_group=config.topk_group,
+                         scoring_func=config.scoring_func,
+                         topk_method=config.topk_method,
+                         routed_scaling_factor=config.routed_scaling_factor)
+        self.config = config
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.n_shared_experts = config.n_shared_experts
+        self.routed_scaling_factor = config.routed_scaling_factor
+        if self.tp_size > config.n_routed_experts:
+            raise ValueError(
+                f"Tensor parallel size {self.tp_size} is greater than "
+                f"the number of experts {config.n_routed_experts}.")
+
+        if config.hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {config.hidden_act}. "
+                             "Only silu is supported for now.")
+
+        self.gate = ReplicatedLinear(config.hidden_size,
+                                     config.n_routed_experts,
+                                     bias=False,
+                                     quant_config=None,
+                                     prefix=f"{prefix}.gate")
+        if config.topk_method == "noaux_tc":
+            self.gate.e_score_correction_bias = nn.Parameter(
+                torch.empty(config.n_routed_experts))
+        else:
+            self.gate.e_score_correction_bias = None
+        if config.n_shared_experts is not None:
+            intermediate_size = (config.moe_intermediate_size *
+                                 config.n_shared_experts)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: replace MLP with FeedForward.
+            '''
+            self.shared_experts = FeedForward(hidden_size=config.hidden_size,
+                                             intermediate_size=intermediate_size,
+                                             hidden_act=config.hidden_act,
+                                             up_proj_name='gate_up_proj',
+                                             is_gated=True,
+                                             down_proj_name='down_proj',
+                                             bias=False,
+                                             quant_config=quant_config,
+                                             reduce_results=False)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        if self.is_fp8_block_wise:
+            self.experts.e_score_correction_bias = self.gate.e_score_correction_bias
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+        if self.n_shared_experts is not None:
+            shared_output = self.shared_experts(hidden_states)
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace experts() with forward_experts, which defined by SparseMoeMlp.
+        '''
+        final_hidden_states = self.forward_experts(
+            hidden_states, router_logits, shared_output = shared_output)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+
+def _compute_prefill_context(
+    self,
+    q: torch.Tensor,
+    kv_c_and_k_pe_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+):
+    prefill_metadata = attn_metadata.prefill_metadata
+    assert prefill_metadata is not None
+    assert prefill_metadata.context_chunk_seq_tot is not None
+    assert prefill_metadata.context_chunk_cu_seq_lens is not None
+    assert prefill_metadata.context_chunk_starts is not None
+    assert prefill_metadata.context_chunk_max_seq_lens is not None
+    assert prefill_metadata.context_lens_tensor is not None
+
+    output = None
+    iters = len(prefill_metadata.context_chunk_seq_tot)
+
+    # Fetch from attn_metadata directly, since it late bound by
+    # MLAAttentionState, grabbing it directly `attn_metadata` can avoid
+    # any weirdness around prefill_metadata caching
+    assert attn_metadata.context_chunk_workspace is not None
+    workspace = attn_metadata.context_chunk_workspace
+
+    for i in range(iters):
+        toks = prefill_metadata.context_chunk_seq_tot[i]
+
+        mlu_ops.gather_cache(
+            kv_cache=kv_c_and_k_pe_cache,
+            dst=workspace,
+            block_table=prefill_metadata.block_tables,
+            cu_seq_lens=prefill_metadata.context_chunk_cu_seq_lens[i],
+            batch_size=prefill_metadata.num_prefills,
+            seq_starts=prefill_metadata.context_chunk_starts[i],
+            kv_cache_dtype=self.attn.kv_cache_dtype
+        )
+
+        kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]
+        k_pe = workspace[:toks][..., self.kv_lora_rank:].unsqueeze(1)
+
+        kv_nope = self.kv_b_proj(kv_c_normed)[0].view(
+            -1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)
+        k_nope, v = kv_nope.split([self.qk_nope_head_dim, self.v_head_dim],
+                                  dim=-1)
+
+        k = torch.cat((k_nope, k_pe.expand((*k_nope.shape[:-1], -1))), dim=-1)
+
+        q = q.reshape(-1, self.num_local_heads * self.qk_head_dim)
+        k = k.reshape(-1, self.num_local_heads * self.qk_head_dim)
+        v = v.reshape(-1, self.num_local_heads * self.v_head_dim)
+
+        prefill_kwargs = {
+            "only_prefill": True,
+            "prefill_causal": False,
+            "cu_seq_lens_q": prefill_metadata.query_start_loc,
+            "cu_seq_lens_kv": prefill_metadata.context_chunk_cu_seq_lens[i],
+            "max_seq_len_q": prefill_metadata.max_query_len,
+            "max_seq_len_kv": prefill_metadata.context_chunk_max_seq_lens[i],
+            "return_lse": True,
+        }
+        attn_output, attn_softmax_lse = self.attn(q, k, v, kwargs=prefill_kwargs)
+        attn_output = attn_output.reshape(-1, self.num_local_heads, self.v_head_dim)
+        attn_softmax_lse = mlu_ops.extract_uncausal_lse(
+            attn_softmax_lse, prefill_metadata.query_start_loc)
+        if output is None:
+            output = attn_output
+            output_lse = attn_softmax_lse
+        else:
+            output_tmp = torch.empty_like(output)
+            output_lse_tmp = torch.empty_like(output_lse)
+            mlu_ops.merge_attn_states(
+                output=output_tmp,
+                output_lse=output_lse_tmp,
+                prefix_output=output,
+                prefix_lse=output_lse,
+                suffix_output=attn_output,
+                suffix_lse=attn_softmax_lse,
+            )
+            output_lse = output_lse_tmp
+
+    return output, output_lse
+
+
+def forward_prefill(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    kv_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+) -> torch.Tensor:
+    if self.q_lora_rank is not None:
+        q = self.q_a_proj(hidden_states)[0]
+        q_scale = None
+        if hasattr(self.q_b_proj.quant_method,"quant_config") and \
+                is_per_token_smoothquant(self.q_b_proj.quant_method.quant_config):
+            self.q_b_proj.quant_method.skip_quant_input = True
+            quant_scale = self.q_b_proj.smooth
+            self.quant_fusion_attn_layernorm = quant_fusion_with_rmsnorm(
+                        self.q_a_layernorm, quant_scale, dynamic_quant=True)
+            q, q_scale = self.quant_fusion_attn_layernorm(q)
+            q = self.q_b_proj(q, q_scale)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+        else:
+            q = self.q_a_layernorm(q)
+            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads,
+                                         self.qk_head_dim)
+    else:
+        q = self.q_proj(hidden_states)[0].view(-1, self.num_local_heads,
+                                               self.qk_head_dim)
+    q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim],
+                           dim=-1)
+    latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+    kv_a, _ = latent_cache.split(
+        [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
+    latent_cache = latent_cache.unsqueeze(1)
+    kv_a = self.kv_a_layernorm(kv_a)
+    kv = self.kv_b_proj(kv_a)[0]
+    kv = kv.view(-1, self.num_local_heads,
+                 self.qk_nope_head_dim + self.v_head_dim)
+    k_nope, v = kv.split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)
+    k_pe = latent_cache[:, :, self.kv_lora_rank:]
+    q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe, only_prefill=True)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: MLA save cache before flashattn
+    '''
+    if len(kv_cache) != 0  and kv_cache[0].numel() > 0:
+        key_cache = kv_cache[0][0]
+        key_value = torch.concat((kv_a.unsqueeze(1), k_pe), dim=-1)
+        updated_slot_mapping = attn_metadata.slot_mapping[:key_value.size(0)]
+        if self.attn.kv_cache_dtype == 'int8':
+            key_cache_scale = kv_cache[1][0]
+            mlu_ops.quant_to_paged_cache(key_value,
+                                            None,
+                                            key_cache,
+                                            None,
+                                            key_cache_scale,
+                                            None,
+                                            updated_slot_mapping.flatten())
+        else:
+            mlu_ops.reshape_paged_cache(key_value,
+                                        None,
+                                        key_cache,
+                                        None,
+                                        updated_slot_mapping.flatten())
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    k = torch.empty_like(q)
+    k[..., :self.qk_nope_head_dim] = k_nope
+    k[..., self.qk_nope_head_dim:] = k_pe
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: mlu attention not pad but qk_head_dim 192 v_head_dim 128.
+    '''
+    q = q.reshape(-1, self.num_local_heads * self.qk_head_dim)
+    k = k.reshape(-1, self.num_local_heads * self.qk_head_dim)
+    v = v.reshape(-1, self.num_local_heads * self.v_head_dim)
+
+    cu_seq_lens_q = attn_metadata.prefill_metadata.query_start_loc
+    max_seq_len_q = attn_metadata.prefill_metadata.max_query_len
+    prefill_kwargs = {
+        "only_prefill": True,
+        "prefill_causal": True,
+        "cu_seq_lens_q": cu_seq_lens_q,
+        "cu_seq_lens_kv": cu_seq_lens_q,
+        "max_seq_len_q": max_seq_len_q,
+        "max_seq_len_kv": max_seq_len_q,
+        "return_lse": attn_metadata.prefill_metadata.context_chunk_seq_tot is not None,
+    }
+    attn_output = self.attn(q, k, v, kwargs=prefill_kwargs)
+    # chunked prefill
+    if attn_metadata.prefill_metadata.context_chunk_seq_tot is not None:
+        suffix_output, suffix_lse = attn_output
+        suffix_output = suffix_output.reshape(-1, self.num_local_heads, self.v_head_dim)
+        suffix_lse = mlu_ops.extract_uncausal_lse(suffix_lse, cu_seq_lens_q)
+        context_output, context_lse = self._compute_prefill_context(q, kv_cache, attn_metadata)
+        attn_output = torch.empty_like(suffix_output)
+        mlu_ops.merge_attn_states(
+            output=attn_output,
+            prefix_output=context_output,
+            prefix_lse=context_lse,
+            suffix_output=suffix_output,
+            suffix_lse=suffix_lse,
+        )
+    attn_output = attn_output.reshape(-1, self.num_local_heads * self.v_head_dim)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    output, _ = self.o_proj(attn_output)
+    return output
+
+
+def forward_decoder(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    kv_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+) -> torch.Tensor:
+    q_len = hidden_states.shape[0]
+    q_input = hidden_states.new_empty(
+        q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
+    )
+    if self.q_lora_rank is not None:
+        q = self.q_a_proj(hidden_states)[0]
+        q_scale = None
+        if hasattr(self.q_b_proj.quant_method,"quant_config") and \
+                is_per_token_smoothquant(self.q_b_proj.quant_method.quant_config):
+            self.q_b_proj.quant_method.skip_quant_input = True
+            quant_scale = self.q_b_proj.smooth
+            self.quant_fusion_attn_layernorm = quant_fusion_with_rmsnorm(
+                        self.q_a_layernorm, quant_scale, dynamic_quant=True)
+            q, q_scale = self.quant_fusion_attn_layernorm(q)
+            q = self.q_b_proj(q, q_scale)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+        else:
+            q = self.q_a_layernorm(q)
+            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+    else:
+        q = self.q_proj(hidden_states)[0].view(
+            -1, self.num_local_heads, self.qk_head_dim
+        )
+    q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
+    torch.bmm(q_nope.transpose(0, 1), self.w_kc, out=q_input[..., : self.kv_lora_rank].transpose(0, 1))
+
+    latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+    v_input = latent_cache[..., : self.kv_lora_rank]
+    k_input = latent_cache
+    self.kv_a_layernorm(v_input, out=k_input[..., : self.kv_lora_rank])
+
+    k_input = k_input.unsqueeze(1)
+    k_pe = k_input[..., self.kv_lora_rank :]
+    q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe, only_decode=True)
+    q_input[..., self.kv_lora_rank :] = q_pe
+
+    q_input = q_input.reshape(q_input.shape[0], -1)
+    k_input = k_input.reshape(k_input.shape[0], -1)
+    v_input = v_input.reshape(v_input.shape[0], -1)
+    decode_kwargs = {"only_decode":True}
+    attn_output = self.attn_decoder(q_input, k_input, v_input, kwargs=decode_kwargs)
+    attn_output = attn_output.reshape(-1, self.num_local_heads,
+                                      self.kv_lora_rank)
+    attn_bmm_output = torch.empty(
+        q_len, self.num_local_heads, self.v_head_dim, device=attn_output.device, dtype=attn_output.dtype)
+    torch.bmm(attn_output.transpose(0, 1), self.w_vc.transpose(1, 2), out=attn_bmm_output.transpose(0, 1))
+    attn_output = attn_bmm_output.flatten(1, 2)
+    output, _ = self.o_proj(attn_output)
+    return output
+
+
+def forward_decoder_fused_mla_q(self, q, output, position):
+    if q.dim() == 2:
+        batch, input_size = q.shape
+        seq = 1
+    else:
+        batch, seq, input_size = q.shape
+    q = q.view(batch, seq, -1)
+
+    if output is not None:
+        output = output.view(batch, seq, output.shape[1], output.shape[2])
+
+    gamma = self.q_a_layernorm.weight.data
+    smooth_quant_scale = None
+    weight_b_scale = None
+    if hasattr(self.q_b_proj.quant_method,"quant_config") and \
+            is_per_token_smoothquant(self.q_b_proj.quant_method.quant_config):
+        smooth_quant_scale = self.q_b_proj.smooth
+        weight_b_scale = self.q_b_proj.per_channel_scale
+        weight_b = self.q_b_proj.qweight.data
+    else:
+        weight_b = self.q_b_proj.qweight.data
+
+    position_id, interleaved, _ = self.rotary_emb.get_param(position)
+    sin = self.rotary_emb.sin_
+    cos = self.rotary_emb.cos_
+    eps = self.q_a_layernorm.variance_epsilon
+
+    return mlu_ops.fused_mla_q(q, gamma, smooth_quant_scale, weight_b, weight_b_scale, self.weight_c, sin, cos,
+                               position_id, output, eps, interleaved)
+
+
+def forward_decoder_fused_mla_kv(self, kv_latent_cache, position, kv_cache, attn_metadata):
+    head_num = 1
+    if kv_latent_cache.dim() == 2:
+        batch, head_size = kv_latent_cache.shape
+        seq = 1
+    else:
+        batch, seq, head_size = kv_latent_cache.shape
+
+    kv = kv_latent_cache.view(batch, seq, head_num, head_size)
+
+    position_id, interleaved, _ = self.rotary_emb.get_param(position)
+    sin = self.rotary_emb.sin_
+    cos = self.rotary_emb.cos_
+
+    gamma = self.kv_a_layernorm.weight.data
+    key_cache = None
+    key_cache_scale = None
+    if kv_cache[0].numel() > 0:
+        kv_cache_, kv_cache_scale_ = kv_cache
+        key_cache = kv_cache_[0]
+        if kv_cache_scale_.numel() > 0:
+            key_cache_scale = kv_cache_scale_[0]
+
+    is_paged_cache = True
+    slot_mapping = None
+    cache_bs_id = None
+    cache_seq_offset = None
+    slot_mapping = attn_metadata.slot_mapping
+    slot_mapping = slot_mapping[attn_metadata.num_prefill_tokens:].view(batch, seq)
+    eps = self.kv_a_layernorm.variance_epsilon
+    return mlu_ops.fused_mla_kv(kv, sin, cos, position_id, gamma, key_cache, key_cache_scale, slot_mapping, cache_bs_id,
+                                cache_seq_offset, is_paged_cache, eps, interleaved)
+
+
+def forward_decoder_fused(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    kv_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+) -> torch.Tensor:
+    q_len = hidden_states.shape[0]
+    q_input = hidden_states.new_empty(
+        q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
+    )
+    is_fused_mla_q = False
+    if self.q_lora_rank is not None:
+        if self.use_fused_qkv_a:
+            assert self.pack_params_done, "q_a_proj and kv_a_proj weights hasn't merged"
+            qkv_latent_cache = self.qkv_a_proj(hidden_states)[0]
+            q = qkv_latent_cache[..., : self.q_lora_rank]
+            latent_cache = qkv_latent_cache[..., self.q_lora_rank:]
+        else:
+            q = self.q_a_proj(hidden_states)[0]
+            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+
+        if hasattr(self.q_b_proj.quant_method,"quant_config") and \
+                is_per_token_smoothquant(self.q_b_proj.quant_method.quant_config):
+            forward_decoder_fused_mla_q(self, q, q_input, positions)
+            is_fused_mla_q = True
+        else:
+            q = self.q_a_layernorm(q)
+            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+    else:
+        q = self.q_proj(hidden_states)[0].view(
+            -1, self.num_local_heads, self.qk_head_dim
+        )
+        latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+
+    if is_fused_mla_q is False:
+        q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
+        torch.bmm(q_nope.transpose(0, 1), self.w_kc, out=q_input[..., :self.kv_lora_rank].transpose(0, 1))
+        q_pe, _ = self.rotary_emb(positions, q_pe, None, only_decode=True)
+        q_input[..., self.kv_lora_rank:] = q_pe
+
+    forward_decoder_fused_mla_kv(self, latent_cache, positions, kv_cache, attn_metadata)
+
+    k_input = latent_cache
+    k_input = k_input.unsqueeze(1)
+
+    v_input = latent_cache[..., : self.kv_lora_rank]
+
+    q_input = q_input.reshape(q_input.shape[0], -1)
+    k_input = k_input.reshape(k_input.shape[0], -1)
+    v_input = v_input.reshape(v_input.shape[0], -1)
+    decode_kwargs = {"only_decode":True}
+    attn_output = self.attn_decoder(q_input, k_input, v_input, kwargs=decode_kwargs)
+    attn_output = attn_output.reshape(-1, self.num_local_heads,
+                                      self.kv_lora_rank)
+    attn_bmm_output = torch.empty(
+        q_len, self.num_local_heads, self.v_head_dim, device=attn_output.device, dtype=attn_output.dtype)
+    torch.bmm(attn_output.transpose(0, 1), self.w_vc.transpose(1, 2), out=attn_bmm_output.transpose(0, 1))
+    attn_output = attn_bmm_output.flatten(1, 2)
+    output, _ = self.o_proj(attn_output)
+    return output
+
+
+def vllm__module_executor__models__deepseek_v2__DeepseekV2Attention__pack_params(self):
+    if self.pack_params_done:
+        return
+    if self.use_fused_qkv_a:
+        self.q_a_proj.weight.data = self.qkv_a_proj.weight.data[:self.q_lora_rank, ...]
+        self.kv_a_proj_with_mqa.weight.data = self.qkv_a_proj.weight.data[self.q_lora_rank:, ...]
+    if self.is_fp8_block_wise:
+        kv_b_proj_dequant_weight = scaled_dequantize(self.kv_b_proj.weight, self.kv_b_proj.weight_scale_inv, self.kv_b_proj.quant_method.quant_config.weight_block_size, self.kv_b_proj.params_dtype)
+        w_kc, w_vc = kv_b_proj_dequant_weight.unflatten(0, (-1, self.qk_nope_head_dim + self.v_head_dim)).split([self.qk_nope_head_dim, self.v_head_dim], dim=1)
+        self.w_kc = w_kc
+        self.w_vc = w_vc
+
+    self.pack_params_done = True
+
+
+def vllm__module_executor__models__deepseek_v2__DeepseekV2Attention__init(
+    self,
+    config: PretrainedConfig,
+    hidden_size: int,
+    num_heads: int,
+    qk_nope_head_dim: int,
+    qk_rope_head_dim: int,
+    v_head_dim: int,
+    q_lora_rank: int,
+    kv_lora_rank: int,
+    rope_theta: float = 10000,
+    rope_scaling: Optional[Dict[str, Any]] = None,
+    max_position_embeddings: int = 8192,
+    cache_config: Optional[CacheConfig] = None,
+    quant_config: Optional[QuantizationConfig] = None,
+    prefix: str = "",
+) -> None:
+    super(DeepseekV2MLAAttention, self).__init__()
+    self.hidden_size = hidden_size
+    self.qk_nope_head_dim = qk_nope_head_dim
+    self.qk_rope_head_dim = qk_rope_head_dim
+    self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim
+    self.v_head_dim = v_head_dim
+    self.q_lora_rank = q_lora_rank
+    self.kv_lora_rank = kv_lora_rank
+    self.num_heads = num_heads
+    tp_size = get_tensor_model_parallel_world_size()
+    assert num_heads % tp_size == 0
+    self.num_local_heads = num_heads // tp_size
+    self.scaling = self.qk_head_dim**-0.5
+    self.rope_theta = rope_theta
+    self.max_position_embeddings = max_position_embeddings
+    self.use_fused_mla_qkv = False
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: skip q_a_proj, kv_a_proj_with_mqa, kv_b_proj weight quant,
+            split kv_b_proj weight if not is_fp8_block_wise
+    '''
+    self.is_fp8_block_wise = isinstance(quant_config, Fp8Config) and quant_config.weight_block_size is not None
+
+    if self.q_lora_rank is not None:
+        self.q_a_proj = ReplicatedLinear(self.hidden_size,
+                                         self.q_lora_rank,
+                                         bias=False,
+                                         quant_config=quant_config if self.is_fp8_block_wise else None,
+                                         prefix=f"{prefix}.q_a_proj")
+        self.q_a_layernorm = RMSNorm(self.q_lora_rank,
+                                     eps=config.rms_norm_eps)
+        self.q_b_proj = ColumnParallelLinear(q_lora_rank,
+                                             self.num_heads *
+                                             self.qk_head_dim,
+                                             bias=False,
+                                             quant_config=quant_config,
+                                             prefix=f"{prefix}.q_b_proj")
+    else:
+        self.q_proj = ColumnParallelLinear(self.hidden_size,
+                                           self.num_heads *
+                                           self.qk_head_dim,
+                                           bias=False,
+                                           quant_config=quant_config,
+                                           prefix=f"{prefix}.q_proj")
+
+    self.kv_a_proj_with_mqa = ReplicatedLinear(
+        self.hidden_size,
+        self.kv_lora_rank + self.qk_rope_head_dim,
+        bias=False,
+        quant_config=quant_config if self.is_fp8_block_wise else None,
+        prefix=f"{prefix}.kv_a_proj_with_mqa")
+    self.kv_a_layernorm = RMSNorm(self.kv_lora_rank,
+                                  eps=config.rms_norm_eps)
+    self.kv_b_proj = ColumnParallelLinear(
+        self.kv_lora_rank,
+        self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),
+        bias=False,
+        quant_config=quant_config if self.is_fp8_block_wise else None,
+        prefix=f"{prefix}.kv_b_proj")
+    kv_b_proj_weight = self.kv_b_proj.weight
+    w_kc, w_vc = kv_b_proj_weight.unflatten(
+        0, (-1, self.qk_nope_head_dim + self.v_head_dim)
+        ).split([self.qk_nope_head_dim, self.v_head_dim], dim=1)
+    self.w_kc = w_kc
+    self.w_vc = w_vc
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    # O projection.
+    self.o_proj = RowParallelLinear(self.num_heads * self.v_head_dim,
+                                    self.hidden_size,
+                                    bias=False,
+                                    quant_config=quant_config,
+                                    prefix=f"{prefix}.o_proj")
+    if rope_scaling:
+        rope_scaling["rope_type"] = 'deepseek_yarn'
+        self.use_normal_rope = False
+    else:
+        self.use_normal_rope = True
+    self.rotary_emb = get_rope(qk_rope_head_dim,
+                               rotary_dim=qk_rope_head_dim,
+                               max_position=max_position_embeddings,
+                               base=rope_theta,
+                               rope_scaling=rope_scaling,
+                               is_neox_style=False)
+
+    if rope_scaling:
+        mscale_all_dim = rope_scaling.get("mscale_all_dim", False)
+        scaling_factor = rope_scaling["factor"]
+        mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))
+        self.scaling = self.scaling * mscale * mscale
+
+    # self.attn = Attention(self.num_heads,
+    #                       self.qk_head_dim,
+    #                       self.scaling,
+    #                       num_kv_heads=self.num_heads)
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add fused mla_q/kv and fused_qkv_a.
+    @brief: mlu attention support head_size 192.
+    '''
+    self.use_fused_mla_qkv = cache_config.cache_dtype != 'int8' and not self.is_fp8_block_wise
+    self.use_fused_qkv_a = False and self.use_fused_mla_qkv and self.q_lora_rank is not None and quant_config is not None
+
+    self.attn = Attention(self.num_local_heads,
+                          self.qk_nope_head_dim + self.qk_rope_head_dim,
+                          self.scaling,
+                          num_kv_heads=self.num_local_heads,
+                          cache_config=cache_config,
+                          quant_config=quant_config,
+                          prefix=f"{prefix}.attn",
+                          use_mla=True,
+                          v_head_dim=self.v_head_dim)
+    self.attn_decoder = Attention(self.num_local_heads,
+                          self.kv_lora_rank + self.qk_rope_head_dim,
+                          self.scaling,
+                          num_kv_heads=1,
+                          cache_config=cache_config,
+                          quant_config=quant_config,
+                          prefix=f"{prefix}.mla_attn",
+                          use_mla=True,
+                          v_head_dim=self.kv_lora_rank,
+                          use_fused_mla_qkv=self.use_fused_mla_qkv)
+    self.forward_prefill = types.MethodType(forward_prefill, self)
+    self.forward_decoder = types.MethodType(forward_decoder, self)
+    self._compute_prefill_context = types.MethodType(_compute_prefill_context, self)
+    if self.use_fused_mla_qkv:
+        self.forward_decoder_fused = types.MethodType(forward_decoder_fused, self)
+        self.weight_c = self.w_kc.transpose(-2, -1).contiguous()
+        if self.use_fused_qkv_a:
+            # self.qkv_a_proj is the fusion of self.q_a_proj and self.kv_a_proj_with_mqa
+            self.qkv_a_proj = ReplicatedLinear(self.hidden_size,
+                                               self.q_lora_rank + self.kv_lora_rank + self.qk_rope_head_dim,
+                                               bias=False,
+                                               quant_config=None,
+                                               prefix=f"{prefix}.qkv_a_proj")
+    self.pack_params_done = False
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__module_executor__models__deepseek_v2__DeepseekV2Attention__forward(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: use normal computation for prefill and use weight absorption for extend/decode.
+            pack_params() is called for dummy model
+    '''
+    forward_context: ForwardContext = get_forward_context()
+    attn_metadata = forward_context.attn_metadata
+
+    if attn_metadata.is_profile_run and attn_metadata.chunked_prefill_enabled and \
+        attn_metadata.context_chunk_workspace is not None:
+        # During the profile run try to simulate to worse case output size
+        # for `self.kv_b_proj(kv_c_normed)` in `_compute_prefill_context`
+        # since this can be large
+        _ = torch.empty(
+            (attn_metadata.context_chunk_workspace.shape[0],
+             self.num_heads, self.qk_nope_head_dim + self.v_head_dim),
+            device=hidden_states.device,
+            dtype=hidden_states.dtype,
+        )
+
+    num_prefill_tokens = attn_metadata.num_prefill_tokens
+    kv_cache = self.attn.kv_cache[forward_context.virtual_engine]
+    self.pack_params()
+    output = torch.empty_like(hidden_states)
+    if attn_metadata.prefill_metadata:
+        attn_metadata.prefill_metadata.compute_dtype = torch.float16
+        prefill_positions = positions[:num_prefill_tokens, ...]
+        prefill_hidden_states = hidden_states[:num_prefill_tokens, ...]
+        output[:num_prefill_tokens] = self.forward_prefill(prefill_positions, prefill_hidden_states, kv_cache,
+                                                           attn_metadata)
+    if attn_metadata.decode_metadata:
+        attn_metadata.decode_metadata.compute_dtype = torch.float16
+        forward_decoder_func = self.forward_decoder_fused if self.use_fused_mla_qkv else self.forward_decoder
+        decode_positions = positions[num_prefill_tokens:, ...]
+        decode_hidden_states = hidden_states[num_prefill_tokens:, ...]
+        output[num_prefill_tokens:] = forward_decoder_func(decode_positions, decode_hidden_states, kv_cache,
+                                                           attn_metadata)
+    return output
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__module_executor__models__deepseek_v2__DeepseekV2DecoderLayer__init__(
+    self,
+    config: PretrainedConfig,
+    prefix: str,
+    model_config: ModelConfig,
+    cache_config: Optional[CacheConfig] = None,
+    quant_config: Optional[QuantizationConfig] = None,
+) -> None:
+    super(DeepseekV2DecoderLayer, self).__init__()
+    self.hidden_size = config.hidden_size
+    rope_theta = getattr(config, "rope_theta", 10000)
+    rope_scaling = getattr(config, "rope_scaling", None)
+    max_position_embeddings = getattr(config, "max_position_embeddings",
+                                      8192)
+    # DecoderLayers are created with `make_layers` which passes the prefix
+    # with the layer's index.
+    layer_idx = int(prefix.split(sep='.')[-1])
+    if model_config.use_mla:
+        attn_cls = DeepseekV2MLAAttention
+    else:
+        attn_cls = DeepseekV2Attention
+    self.self_attn = attn_cls(
+        config=config,
+        hidden_size=self.hidden_size,
+        num_heads=config.num_attention_heads,
+        qk_nope_head_dim=config.qk_nope_head_dim,
+        qk_rope_head_dim=config.qk_rope_head_dim,
+        v_head_dim=config.v_head_dim,
+        q_lora_rank=config.q_lora_rank
+        if hasattr(config, "q_lora_rank") else None,
+        kv_lora_rank=config.kv_lora_rank,
+        rope_theta=rope_theta,
+        rope_scaling=rope_scaling,
+        max_position_embeddings=max_position_embeddings,
+        cache_config=cache_config,
+        quant_config=quant_config,
+        prefix=f"{prefix}.self_attn",
+    )
+
+    if (config.n_routed_experts is not None
+            and layer_idx >= config.first_k_dense_replace
+            and layer_idx % config.moe_layer_freq == 0):
+        self.mlp = DeepseekV2MoE(
+            config=config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mlp",
+        )
+    else:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace MLP with FeedForward.
+        '''
+        self.mlp = FeedForward(hidden_size=config.hidden_size,
+                               intermediate_size=config.intermediate_size,
+                               hidden_act=config.hidden_act,
+                               up_proj_name='gate_up_proj',
+                               is_gated=True,
+                               down_proj_name='down_proj',
+                               bias=False,
+                               quant_config=quant_config,
+                               prefix=f"{prefix}.mlp")
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+    self.input_layernorm = RMSNorm(config.hidden_size,
+                                   eps=config.rms_norm_eps)
+    self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                            eps=config.rms_norm_eps)
+    self.routed_scaling_factor = config.routed_scaling_factor
+
+
+def vllm__module_executor__models__deepseek_v2__DeepseekV2ForCausalLM__load_weights(
+    self,
+    weights: Iterable[Tuple[str, torch.Tensor]]
+) -> Set[str]:
+
+    stacked_params_mapping = [
+        # (param_name, shard_name, shard_id)
+        ("gate_up_proj", "gate_proj", 0),
+        ("gate_up_proj", "up_proj", 1),
+    ]
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: expert_params_mapping for fp8 block_wise
+    '''
+    is_fp8_block_wise = isinstance(self.quant_config, Fp8Config) and self.quant_config.weight_block_size is not None
+    expert_params_mapping = FusedMoE.make_expert_params_mapping(
+        ckpt_gate_proj_name="gate_proj",
+        ckpt_down_proj_name="down_proj",
+        ckpt_up_proj_name="up_proj",
+        num_experts=self.config.n_routed_experts) if is_fp8_block_wise else {}
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    params_dict = dict(self.named_parameters())
+    loaded_params: Set[str] = set()
+    for name, loaded_weight in weights:
+        import re
+        pattern = r'layers\.([0-9]*)\.'
+        match = re.search(pattern, name)
+        if match:
+            layer_id = int(match.group(1))
+            if layer_id >= self.config.num_hidden_layers:
+                continue
+        if "rotary_emb.inv_freq" in name:
+            continue
+
+        spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+        if spec_layer is not None:
+            continue  # skip spec decode layers for main model
+
+        for (param_name, weight_name, shard_id) in stacked_params_mapping:
+            # Skip non-stacked layers and experts (experts handled below).
+            if weight_name not in name:
+                continue
+            # We have mlp.experts[0].gate_proj in the checkpoint.
+            # Since we handle the experts below in expert_params_mapping,
+            # we need to skip here BEFORE we update the name, otherwise
+            # name will be updated to mlp.experts[0].gate_up_proj, which
+            # will then be updated below in expert_params_mapping
+            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+            '''
+            if is_fp8_block_wise and ("mlp.experts." in name) and name not in params_dict:
+                continue
+            name = name.replace(weight_name, param_name)
+            if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # Skip loading extra bias for GPTQ models.
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+
+            if is_pp_missing_parameter(name, self):
+                continue
+
+            param = params_dict[name]
+            weight_loader = param.weight_loader
+            weight_loader(param, loaded_weight, shard_id)
+            break
+        else:
+            for mapping in expert_params_mapping:
+                param_name, weight_name, expert_id, shard_id = mapping
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param,
+                              loaded_weight,
+                              name,
+                              shard_id=shard_id,
+                              expert_id=expert_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+
+                if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                        and name not in params_dict):
+                    continue
+
+                # Remapping the name of FP8 kv-scale.
+                name = maybe_remap_kv_scale_name(name, params_dict)
+                if name is None:
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+        loaded_params.add(name)
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: pack params
+    '''
+    for name, m in self.model.named_modules():
+        if isinstance(m, SparseMoeMlp) or isinstance(m, DeepseekV2MLAAttention):
+            m.pack_params()
+        if (isinstance(m, DeepseekV2MLAAttention)
+                 and m.use_fused_mla_qkv
+                 and self.config.q_lora_rank is not None):
+            m.weight_c = m.w_kc.transpose(-2, -1).contiguous()
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    return loaded_params
+
+
+MluHijackObject.apply_hijack(DeepseekV2MLAAttention,
+                             DeepseekV2MLAAttention.forward,
+                             vllm__module_executor__models__deepseek_v2__DeepseekV2Attention__forward)
+MluHijackObject.apply_hijack(DeepseekV2MLAAttention,
+                             DeepseekV2MLAAttention.__init__,
+                             vllm__module_executor__models__deepseek_v2__DeepseekV2Attention__init)
+MluHijackObject.apply_hijack(DeepseekV2MLAAttention,
+                             "pack_params",
+                             vllm__module_executor__models__deepseek_v2__DeepseekV2Attention__pack_params)
+MluHijackObject.apply_hijack(DeepseekV2DecoderLayer,
+                             DeepseekV2DecoderLayer.__init__,
+                             vllm__module_executor__models__deepseek_v2__DeepseekV2DecoderLayer__init__)
+MluHijackObject.apply_hijack(DeepseekV2ForCausalLM,
+                             DeepseekV2ForCausalLM.load_weights,
+                             vllm__module_executor__models__deepseek_v2__DeepseekV2ForCausalLM__load_weights)

