diff --git a/vllm_mlu/vllm_mlu/model_executor/layers/sparse_moe_mlp.py b/vllm_mlu/vllm_mlu/model_executor/layers/sparse_moe_mlp.py
new file mode 100644
index 000000000..000ea8d81
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/layers/sparse_moe_mlp.py
@@ -0,0 +1,613 @@
+"""Inference-only MOE model."""
+from typing import Optional
+
+import torch
+from torch import nn
+
+from vllm.distributed import (get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              get_tensor_model_parallel_group,
+                              tensor_model_parallel_all_reduce)
+from vllm.model_executor.layers.linear import ReplicatedLinear
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm.model_executor.layers.quantization.fp8 import Fp8Config
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.fused_moe.fused_moe import grouped_topk
+
+from vllm_mlu._mlu_utils import *
+from vllm_mlu import _mlu_ops as mlu_ops
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.quantization.weightonly import WeightOnlyConfig
+from vllm_mlu.model_executor.layers.quantization.smoothquant import SmoothQuantConfig
+
+class SparseMoeMlp(nn.Module):
+    """
+    Tensor Parallel evenly splits each expert's weight and distributes them to different ranks,
+    which means each rank holds partial weight of all experts.
+    While Expert Parallel evenly distributes some of the experts' full weight to different ranks,
+    which means each rank holds part of the experts' full weight.
+
+    As a result, each rank in the Tensor Parallel group receives all tokens' hidden states for all experts,
+    then computes using the partial weights, while for Expert Parallel, each rank only receives
+    part of tokens' hidden states for experts on this rank, then computes using the full weights.
+
+    When both Tensor Parallel and Expert Parallel are enabled, each rank handles
+    a portion of the expert weights matrices (as in EP mode) and these weights are further sliced
+    across ranks (as in TP mode). This hybrid approach aims to balance the workload more evenly across ranks,
+    enhancing efficiency and reducing the likelihood of bottlenecks associated with EP mode alone.
+    """
+    reduce_weight : torch.Tensor = None
+    expert_id     : torch.Tensor = None
+    is_expert_avg : bool = False
+    max_batched_token : int = 2048
+
+    def __init__(
+        self,
+        num_experts: int,
+        top_k: int,
+        hidden_size: int,
+        intermediate_size: int,
+        up_proj_name: str,
+        is_gated: bool,
+        down_proj_name: str,
+        has_bias: bool,
+        skip_bias_add: bool = False,
+        renormalize:bool = False,
+        hidden_act: str = "silu",
+        params_dtype: Optional[torch.dtype] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        is_use_fused_moe: bool = False,
+        expert_group: Optional[int] = 1,
+        topk_group: Optional[int] = 1,
+        scoring_func: str = "softmax",
+        topk_method: str = "",
+        routed_scaling_factor: float = 1.0,
+    ):
+        super().__init__()
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.tp_group = get_tensor_model_parallel_group()
+        self.num_total_experts = num_experts
+        self.top_k = top_k
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size
+        self.up_proj_name = up_proj_name
+        self.is_gated = is_gated
+        self.down_proj_name = down_proj_name
+        self.has_bias = has_bias
+        self.renormalize = renormalize
+        self.hidden_act = hidden_act
+        self.quant_config = quant_config
+        self.is_use_fused_moe = is_use_fused_moe
+        self.expert_group = expert_group
+        self.topk_group = topk_group
+        self.scoring_func = scoring_func
+        self.routed_scaling_factor = routed_scaling_factor
+        # fused_moe doesn't support weightonly quantization
+        if isinstance(quant_config, WeightOnlyConfig):
+            self.is_use_fused_moe = False
+
+        if params_dtype is None:
+            params_dtype = torch.get_default_dtype()
+        self.params_dtype = params_dtype
+
+        if VLLM_AVG_MOE_EN and not SparseMoeMlp.is_expert_avg:
+            n_tokens = SparseMoeMlp.max_batched_token
+            val = 1.0 / float(num_experts)
+            SparseMoeMlp.reduce_weight    = torch.full((n_tokens, top_k), val, device="mlu", dtype=torch.float32)
+            #avg 0,20,40,60,80...120,140,  1,21,...121,141, 2...142,  ......  19,...159,  0,20,......
+            import math
+            batch_table = math.ceil(n_tokens * top_k / num_experts) * num_experts
+            hi_val = batch_table // num_experts
+            table = (torch.arange(hi_val * num_experts, device="mlu", dtype=torch.int32) % num_experts).view(hi_val, expert_group, num_experts // expert_group).transpose(1, 2)
+            SparseMoeMlp.expert_id        = table.flatten()[:n_tokens*top_k].view(n_tokens, top_k)
+            SparseMoeMlp.is_expert_avg = True
+
+        # NOTE: The bias for fc2 is only applied on tp_rank 0. If we added it on all nodes the allreduce() would
+        # contain multiple copies of the bias. The bias on other node will be ignored, and may be set to nullptr
+        self.skip_bias_add = True if self.tp_rank > 0 else False
+
+        assert self.intermediate_size % self.tp_size == 0, (
+            f"need intermediate_size:{self.intermediate_size} % tp_size:{self.tp_size} == 0")
+
+        self.num_experts_per_rank = self.num_total_experts
+
+        self.start_expert_id = 0
+        self.end_expert_id = self.start_expert_id + self.num_experts_per_rank
+
+        # Gate always runs at half / full precision for now.
+        self.gate = ReplicatedLinear(self.hidden_size,
+                                     self.num_total_experts,
+                                     bias=False,
+                                     params_dtype=self.params_dtype,
+                                     quant_config=None)
+        if topk_method == "noaux_tc":
+            self.gate.e_score_correction_bias = nn.Parameter(
+                torch.empty(self.num_total_experts, device="mlu"))
+        else:
+            self.gate.e_score_correction_bias = None
+        self.is_fp8_block_wise = isinstance(self.quant_config, Fp8Config) and self.quant_config.weight_block_size is not None
+        if self.is_fp8_block_wise:
+            self.experts = FusedMoE(
+                num_experts=self.num_experts_per_rank,
+                top_k=self.top_k,
+                hidden_size=self.hidden_size,
+                intermediate_size=self.intermediate_size,
+                reduce_results=False,
+                renormalize=self.renormalize,
+                quant_config=self.quant_config,
+                use_grouped_topk=True,
+                num_expert_group=self.expert_group,
+                topk_group=self.topk_group,
+                prefix=f"experts",
+                scoring_func=self.scoring_func,
+                e_score_correction_bias=self.gate.e_score_correction_bias)
+        else:
+            self.experts = nn.ModuleList([
+                FeedForward(hidden_size=self.hidden_size,
+                            intermediate_size=self.intermediate_size,
+                        hidden_act=self.hidden_act,
+                        up_proj_name=self.up_proj_name,
+                        is_gated=self.is_gated,
+                        down_proj_name=self.down_proj_name,
+                        bias=self.has_bias,
+                        quant_config=self.quant_config,
+                        skip_bias_add=self.skip_bias_add,
+                        reduce_results=False,
+                        prefix=f"experts.{idx}") for idx in range(self.num_experts_per_rank)
+        ])
+
+        self.init_pack_param()
+
+
+    def init_pack_param(self):
+        self.w13 = None
+        self.w2 = None
+        self.b13 = None
+        self.b2 = None
+        self.w13_scale = None
+        self.w2_scale = None
+        self.a13_scale = None
+        self.a2_scale = None
+        self.pack_params_done = False
+
+
+    def map_param_data(self, param_list, is_use_first_data=False):
+        if len(param_list) == 0:
+            return None
+
+        if is_use_first_data or len(param_list) == 1:
+            first_data = param_list[0].data
+            for param in param_list[1: -1]:
+                param.data = first_data
+            if is_use_first_data:
+                out_param = first_data.view_as(param_list[0])
+            else:
+                out_param = first_data.view(len(param_list), *first_data.shape)
+        else:
+            packed_param = torch._utils._flatten_dense_tensors(param_list)
+            data_list = torch._utils._unflatten_dense_tensors(packed_param, param_list)
+            for data, param in zip(data_list, param_list):
+                param.data = data
+            out_param = packed_param.view(len(param_list), *data_list[0].shape)
+
+        torch.mlu.empty_cache()
+
+        return out_param
+
+
+    def pack_unquantized_params(self, w13, w2, b13, b2):
+        for expert in self.experts:
+            up_proj = getattr(expert, self.up_proj_name)
+            down_proj = getattr(expert, self.down_proj_name)
+            w13.append(up_proj.weight)
+            w2.append(down_proj.weight)
+            if self.has_bias:
+                b13.append(up_proj.bias)
+                b2.append(down_proj.bias)
+
+
+    def pack_smoothquant_params(self, w13, w2, b13, b2, w13_scale, w2_scale, a13_scale, a2_scale):
+        for expert in self.experts:
+            up_proj = getattr(expert, self.up_proj_name)
+            down_proj = getattr(expert, self.down_proj_name)
+            w13.append(up_proj.qweight)
+            w2.append(down_proj.qweight)
+            if self.has_bias:
+                b13.append(up_proj.bias)
+                b2.append(down_proj.bias)
+            w13_scale.append(up_proj.per_channel_scale)
+            w2_scale.append(down_proj.per_channel_scale)
+            if self.quant_config.input_quant_method == "per_token":
+                a13_scale.append(up_proj.smooth)
+                a2_scale.append(down_proj.smooth)
+            else:
+                a13_scale.append(up_proj.scale_to_int)
+                a2_scale.append(down_proj.scale_to_int)
+
+
+    def pack_weightonly_params(self, w13, w2, b13, b2, w13_scale, w2_scale):
+        for expert in self.experts:
+            up_proj = getattr(expert, self.up_proj_name)
+            down_proj = getattr(expert, self.down_proj_name)
+            w13.append(up_proj.qweight)
+            w2.append(down_proj.qweight)
+            if self.has_bias:
+                b13.append(up_proj.bias)
+                b2.append(down_proj.bias)
+            w13_scale.append(up_proj.scales)
+            w2_scale.append(down_proj.scales)
+
+    def pack_fp8_params_without_activation_scheme(self, w13, w2, b13, b2, w13_scale, w2_scale):
+        for expert in self.experts:
+            up_proj = getattr(expert, self.up_proj_name)
+            down_proj = getattr(expert, self.down_proj_name)
+            w13.append(up_proj.weight)
+            w2.append(down_proj.weight)
+            if self.has_bias:
+                b13.append(up_proj.bias)
+                b2.append(down_proj.bias)
+            w13_scale.append(up_proj.weight_scale)
+            w2_scale.append(down_proj.weight_scale)
+
+
+    def pack_params(self):
+        if self.pack_params_done or self.is_fp8_block_wise:
+            return
+
+        w13 = []
+        w2 = []
+        b13 = []
+        b2 = []
+        w13_scale = []
+        w2_scale = []
+        a13_scale = []
+        a2_scale = []
+
+        if self.quant_config is None:
+            self.pack_unquantized_params(w13, w2, b13, b2)
+        elif isinstance(self.quant_config, SmoothQuantConfig):
+            self.pack_smoothquant_params(w13, w2, b13, b2, w13_scale, w2_scale, a13_scale, a2_scale)
+        elif isinstance(self.quant_config, WeightOnlyConfig):
+            self.pack_weightonly_params(w13, w2, b13, b2, w13_scale, w2_scale)
+        elif isinstance(self.quant_config, Fp8Config) and self.quant_config.activation_scheme == 'dynamic':
+            self.pack_fp8_params_without_activation_scheme(w13, w2, b13, b2, w13_scale, w2_scale)
+        else:
+            raise ValueError(f'Unsupported quantization:{self.quant_config}')
+
+        # pack weight
+        self.w13 = self.map_param_data(w13)
+        self.w2 = self.map_param_data(w2)
+
+        # pack bias
+        if self.has_bias:
+            self.b13 = self.map_param_data(b13)
+            # NOTE: The bias for fc2 is only applied on tp_rank 0. If we added it on all nodes the allreduce() would
+            # contain multiple copies of the bias. The bias on other node will be ignored, and may be set to nullptr
+            if self.skip_bias_add is False:
+                self.b2 = self.map_param_data(b2)
+
+
+        # pack weight scale
+        if len(w13_scale) > 0:
+            self.w13_scale = self.map_param_data(w13_scale)
+        if len(w2_scale) > 0:
+            self.w2_scale = self.map_param_data(w2_scale)
+
+        # pack activate scale
+        if len(a13_scale) > 0:
+            self.a13_scale = self.map_param_data(a13_scale)
+        if len(a2_scale) > 0:
+            self.a2_scale = self.map_param_data(a2_scale)
+
+        if isinstance(self.quant_config, SmoothQuantConfig) and self.quant_config.group_size > 1 and self.is_use_fused_moe:
+            assert self.w13_scale is not None and self.w2_scale is not None, "w13_scale and w2_scale must be not None"
+            self.w13_scale = self.w13_scale.permute(2, 0, 1).contiguous()
+            self.w2_scale = self.w2_scale.permute(2, 0, 1).contiguous()
+
+        # pack smooth variables for moe_quantize if fp8
+        # FIXME: replace smooth to None after tmo supports.
+        if isinstance(self.quant_config, Fp8Config):
+            expert_size = self.w13.shape[0]
+            fp8_smooth_2_hidden_size = self.w13.shape[1] // 2 if self.is_gated else self.w13.shape[1]
+            self.fp8_smooth_1 = torch.ones([expert_size, self.hidden_size], device=self.w13.device, dtype=torch.float32)
+            self.fp8_smooth_2 = torch.ones([expert_size, fp8_smooth_2_hidden_size], device=self.w13.device, dtype=torch.float32)
+
+        self.pack_params_done = True
+
+
+    def forward(self, hidden_states: torch.Tensor, residual: Optional[torch.Tensor] = None) -> torch.Tensor:
+        orig_hidden_states_shape = hidden_states.shape
+        hidden_states = hidden_states.view(-1, self.hidden_size)
+        # expert_logits: [num_tokens, self.num_experts_per_rank]
+        expert_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.forward_experts(hidden_states, expert_logits, residual)
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+
+        output = final_hidden_states.view(orig_hidden_states_shape)
+        return output
+
+
+    def forward_experts(self, hidden_states, expert_logits, residual: Optional[torch.Tensor] = None,
+                        shared_output: Optional[torch.Tensor] = None):
+        assert not (residual is not None and shared_output is not None)
+        residual_ = None if self.tp_rank > 0 else residual
+
+        # change only for deepseek_model without residual_
+        if shared_output is not None:
+            residual_ = shared_output
+        if self.is_use_fused_moe and self.expert_group != 1:
+            final_hidden_states = self.forward_group_experts(hidden_states, expert_logits, residual_)
+        elif self.is_use_fused_moe:
+            self.pack_params()
+            final_hidden_states = mlu_ops.fused_moe(hidden_states=hidden_states,
+                                                    gating_output=expert_logits,
+                                                    w1=self.w13,
+                                                    w2=self.w2,
+                                                    bias1=self.b13,
+                                                    bias2=self.b2,
+                                                    residual=residual_,
+                                                    input_smooth=self.a13_scale,
+                                                    act_smooth=self.a2_scale,
+                                                    w1_scale=self.w13_scale,
+                                                    w2_scale=self.w2_scale,
+                                                    topk=self.top_k,
+                                                    renormalize=self.renormalize,
+                                                    gated=self.is_gated,
+                                                    act_mode=self.hidden_act,
+                                                    start_expert_id=self.start_expert_id)
+        else:
+            final_hidden_states = self.forward_experts_nofused(hidden_states, expert_logits)
+            if residual_ is not None:
+                final_hidden_states = final_hidden_states + residual_
+        return final_hidden_states
+
+
+    def forward_experts_nofused(self, hidden_states, expert_logits):
+        hidden_states_shape = hidden_states.shape
+        if self.scoring_func == "softmax":
+            topk_values, topk_indices = self.topk_softmax(expert_logits)
+        elif self.scoring_func == "sigmoid":
+            gating_output = expert_logits.to(torch.float32)
+            gating_output = gating_output.view(-1, gating_output.size(-1))
+            topk_values, topk_indices = grouped_topk(hidden_states, gating_output, self.top_k, self.renormalize,
+                                                    self.expert_group, self.topk_group, self.scoring_func,
+                                                    self.gate.e_score_correction_bias)
+            topk_values = topk_values.to(hidden_states.dtype)
+            topk_indices = topk_indices.to(torch.int64)
+        expand_gather_idx, scatter_idx, expand_token_count, cusum_token_count = self.generate_gather_idx(
+            topk_indices)
+        # no expert is routed, then expand_gather_idx, expand_scatter_idx has no item,
+        # expand_token_count and expand_cusum_token_count has item but the value is all zero
+        # so this rank should only return final_hidden_states with zero value
+        if expand_gather_idx.numel() == 0:
+            final_hidden_states = torch.zeros_like(hidden_states,
+                                                   dtype=hidden_states.dtype,
+                                                   device=hidden_states.device)
+            return final_hidden_states
+
+        expand_hidden_states = self.expand_input(hidden_states, expand_gather_idx)
+
+        expand_output_list = []
+        expand_cusum_token_count = cusum_token_count[self.start_expert_id:self.end_expert_id +
+                                                     1] - cusum_token_count[self.start_expert_id]
+        for expert_idx, num_tokens_per_expert in enumerate(expand_token_count):
+            if num_tokens_per_expert > 0:
+                expert_hidden_states = expand_hidden_states[
+                    expand_cusum_token_count[expert_idx]:expand_cusum_token_count[expert_idx + 1]]
+                expert_output = self.experts[expert_idx](expert_hidden_states)
+                expert_output = expert_output[0] if isinstance(expert_output, (tuple, list)) else expert_output
+                expand_output_list.append(expert_output)
+        expand_output = torch.cat(expand_output_list, dim=0)
+        final_hidden_states = self.combine_moe(expand_output, scatter_idx, cusum_token_count, hidden_states_shape,
+                                               topk_values)
+
+        return final_hidden_states
+
+    def forward_group_experts(self, hidden_states, expert_logits, residual_):
+        if self.is_fp8_block_wise:
+            output = self.experts(hidden_states=hidden_states,
+                                router_logits=expert_logits) * self.routed_scaling_factor
+            if residual_ is not None:
+                output = output + residual_
+            return output
+        is_fp8_quant = isinstance(self.quant_config, Fp8Config)
+        ori_input_shape = hidden_states.shape
+        dtype = hidden_states.dtype
+        self.pack_params()
+        gating_output=expert_logits.to(torch.float32)
+        w1=self.w13
+        w2=self.w2
+        bias1=self.b13
+        bias2=self.b2
+        input_smooth=self.a13_scale
+        act_smooth=self.a2_scale
+        w1_scale=self.w13_scale
+        w2_scale=self.w2_scale
+        topk=self.top_k
+        renormalized=self.renormalize
+        gated=self.is_gated
+        act_mode=self.hidden_act
+        quant_input=None
+
+        start_expert_id=self.start_expert_id
+        expert_num = gating_output.size(-1)
+        expert_size = w1.size(0)
+        max_m = hidden_states.shape[0]
+        hidden_states = hidden_states.view(-1, hidden_states.size(-1))
+        gating_output = gating_output.view(-1, gating_output.size(-1))
+        residual_ = residual_.view(-1, residual_.size(-1)) if residual_ is not None else None
+        # Check smooth quant parameters.
+        per_token_sq = False
+        if not is_fp8_quant:
+            check_list = [input_smooth, act_smooth, w1_scale, w2_scale]
+            if all(x is not None for x in check_list):
+                per_token_sq = True
+
+            if not (all(x is None for x in check_list) or all(x is not None for x in check_list)):
+                raise ValueError("input_smooth, act_smooth, w1_scale and w2_scale must be present "
+                                "and absent at the same time.")
+
+        # softmax_topk
+        if self.scoring_func == "softmax":
+            reduce_weight, expert_id = mlu_ops.moe_softmax_topk(gating_output, topk, renormalized, self.expert_group,
+                                                                self.topk_group, route_scale=self.routed_scaling_factor)
+        elif self.scoring_func == "sigmoid":
+            reduce_weight, expert_id = mlu_ops.moe_sigmoid_topk(gating_output, topk, renormalized,
+                                                                self.expert_group, self.topk_group,
+                                                                self.routed_scaling_factor,
+                                                                self.gate.e_score_correction_bias)
+        else:
+            raise ValueError(f"Unsupported scoring function: {self.scoring_func}")
+
+        if VLLM_AVG_MOE_EN:
+            n_tokens = hidden_states.shape[0]
+            reduce_weight = SparseMoeMlp.reduce_weight[:n_tokens]
+            expert_id     = SparseMoeMlp.expert_id[:n_tokens]
+        # gen_idx
+        expand_idx, combine_idx, token_count, cusum_token_count = mlu_ops.moe_gen_idx(expert_id, expert_num)
+        # check quant
+        if is_fp8_quant and self.quant_config.activation_quant_method == 'per_token':
+            quant_input, input_scale = mlu_ops.moe_quantize(
+                hidden_states,
+                self.fp8_smooth_1,
+                zero=None,
+                token_count=token_count[start_expert_id:start_expert_id+expert_size],
+                gather_index=expand_idx,
+                gather_index_start_position=cusum_token_count[start_expert_id].unsqueeze(0),
+                output=None,
+                output_scale=None,
+                dynamic_quant=True,
+                quant_type=torch.float8_e4m3fn
+            )
+        elif per_token_sq:
+            quant_input, input_scale = mlu_ops.moe_quantize(hidden_states,
+                input_smooth, None, token_count[start_expert_id:start_expert_id+expert_size], expand_idx,
+                cusum_token_count[start_expert_id].unsqueeze(0))
+        else:
+            expand_hidden_states = mlu_ops.moe_expand_input(hidden_states, expand_idx,
+                    cusum_token_count, start_expert_id, expert_size)
+
+        if (is_fp8_quant and self.quant_config.activation_quant_method == 'per_token') or per_token_sq:
+            gemm_out = mlu_ops.smooth_quant_group_gemm(quant_input, w1,
+                                                        token_count[start_expert_id:start_expert_id+expert_size],
+                                                        None, None, None, None,
+                                                        input_scale, w1_scale, dtype, max_m)
+        else:
+            gemm_out = mlu_ops.group_gemm(expand_hidden_states, w1,
+                                       token_count[start_expert_id:start_expert_id+expert_size],
+                                       None, None, None, None, max_m)
+        # add_bias_active
+        if is_fp8_quant and self.quant_config.activation_quant_method == 'per_token':
+            act_out = mlu_ops.moe_active(gemm_out, act_mode, gated, gemm_out[:,:gemm_out.shape[-1]//2], bias=bias1, cusum_token_count=cusum_token_count, start_expert_id=start_expert_id, expert_size=expert_size)
+            quant_input, input_scale = mlu_ops.moe_quantize(
+                act_out,
+                self.fp8_smooth_2,
+                zero=None,
+                token_count=token_count[start_expert_id:start_expert_id+expert_size],
+                gather_index=None,
+                gather_index_start_position=None,
+                output=quant_input[:,:act_out.shape[-1]],
+                output_scale=None,
+                dynamic_quant=True,
+                quant_type=torch.float8_e4m3fn
+            )
+        elif per_token_sq:
+            quant_input = quant_input[:, :gemm_out.shape[-1] // 2]
+            input_scale = input_scale[:gemm_out.shape[0]]
+            quant_input, input_scale = mlu_ops.moe_quantize(gemm_out, act_smooth, None,
+                                                            token_count[start_expert_id:start_expert_id+expert_size],
+                                                            output=quant_input,
+                                                            output_scale=input_scale,
+                                                            act_mode=act_mode,
+                                                            is_gated=self.is_gated)
+
+        if (is_fp8_quant and self.quant_config.activation_quant_method == 'per_token') or per_token_sq:
+            # Remove the reference to gemm_out tensor.
+            # If that was the only reference, the tensorâ€™s memory becomes eligible for deallocation
+            # So that we can reuse this memory for the new allocation of next gemm operation
+            del gemm_out
+            gemm_out = mlu_ops.smooth_quant_group_gemm(quant_input, w2,
+                                                    token_count[start_expert_id:start_expert_id+expert_size],
+                                                    None, None, None, None, input_scale, w2_scale, dtype, max_m)
+        else:
+            act_out = mlu_ops.moe_active(gemm_out, act_mode, gated, gemm_out[:,:gemm_out.shape[-1]//2], bias1, cusum_token_count, start_expert_id, expert_size)
+            gemm_out = mlu_ops.group_gemm(act_out, w2,
+                                       token_count[start_expert_id:start_expert_id+expert_size],
+                                       None, None, None, None, max_m)
+
+        # we reuse the memory of hidden_states to store the output
+        output = mlu_ops.moe_combine_result(gemm_out, reduce_weight, combine_idx,
+                                        residual_, cusum_token_count, start_expert_id,
+                                        expert_size, bias2, output=hidden_states)
+        return output.view(ori_input_shape)
+
+
+    def topk_softmax(self, expert_logits):
+        # expert_logits: [num_tokens, self.num_experts_per_rank]
+        # topk_values: [num_tokens, self.top_k]
+        # topk_indices: [num_tokens, self.top_k]
+        if self.renormalize:
+            topk_values, topk_indices = torch.topk(expert_logits, self.top_k, dim=-1)
+            topk_values = torch.softmax(topk_values, -1)
+        else:
+            router_probs = torch.softmax(expert_logits, -1)
+            topk_values, topk_indices = torch.topk(router_probs, self.top_k, dim=-1)
+
+        return topk_values, topk_indices
+
+
+    def generate_gather_idx(self, topk_indices):
+        device = topk_indices.device
+        # gather_expand_idx: [num_tokens * self.top_k]
+        sorted_expert_id, indices = topk_indices.flatten().sort()
+        gather_idx = indices // self.top_k
+
+        seqs = torch.arange(indices.numel(), dtype=indices.dtype, device=indices.device)
+        scatter_idx=torch.zeros((indices.numel(),), dtype=seqs.dtype, device=seqs.device).scatter(0, indices, seqs)
+
+        # token_count: [self.num_experts_per_rank]
+        partial_token_index, partial_token_count = sorted_expert_id.unique(sorted=True, return_counts=True)
+        zero_token_count = torch.zeros(self.num_total_experts, dtype=partial_token_count.dtype, device=device)
+        token_count = zero_token_count.scatter(dim=0, index=partial_token_index, src=partial_token_count)
+        # cusum_token_count: [self.num_experts_per_rank + 1]
+        cusum_token_count = torch.cat(
+            [torch.tensor([0], dtype=token_count.dtype, device=device),
+             token_count.cumsum(dim=0)])
+
+        num_tokens_before_expert = cusum_token_count[self.start_expert_id]
+        num_tokens_including_expert = cusum_token_count[self.end_expert_id]
+
+        expand_gather_idx = gather_idx[num_tokens_before_expert:num_tokens_including_expert]
+        expand_token_count = token_count[self.start_expert_id:self.end_expert_id]
+
+        return expand_gather_idx, scatter_idx, expand_token_count, cusum_token_count
+
+
+    def expand_input(self, hidden_states, expand_gather_idx):
+        expand_hidden_states = hidden_states[expand_gather_idx]
+        return expand_hidden_states
+
+
+    def combine_moe(self, expand_output, scatter_idx, cusum_token_count, hidden_states_shape, topk_values):
+        num_tokens, hidden_size = hidden_states_shape
+        num_tokens_before_expert = cusum_token_count[self.start_expert_id]
+        num_tokens_after_expert = cusum_token_count[-1] - cusum_token_count[self.end_expert_id]
+
+        expand_output_before_expert = torch.zeros((num_tokens_before_expert, hidden_size),
+                                                   dtype=expand_output.dtype,
+                                                   device=expand_output.device)
+        expand_output_after_expert = torch.zeros((num_tokens_after_expert, hidden_size),
+                                                   dtype=expand_output.dtype,
+                                                   device=expand_output.device)
+        unscatted_output = torch.cat([expand_output_before_expert, expand_output, expand_output_after_expert], dim=0)
+        scatter_output = unscatted_output[scatter_idx]
+        hidden_states_weight = topk_values.flatten().unsqueeze(-1)
+        weighted_hidden_states = scatter_output * hidden_states_weight
+        unreduced_hidden_states = weighted_hidden_states.view(num_tokens, self.top_k, hidden_size)
+        final_hidden_states = unreduced_hidden_states.sum(dim=1)
+
+        return final_hidden_states

