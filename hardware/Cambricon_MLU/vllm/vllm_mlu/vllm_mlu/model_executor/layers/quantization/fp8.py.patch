diff --git a/vllm_mlu/vllm_mlu/model_executor/layers/quantization/fp8.py b/vllm_mlu/vllm_mlu/model_executor/layers/quantization/fp8.py
new file mode 100644
index 000000000..3ff56f3af
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/layers/quantization/fp8.py
@@ -0,0 +1,561 @@
+import torch
+from torch.nn import Module
+from torch.nn.parameter import Parameter
+
+from typing import Any, Dict, List, Optional, Callable
+from vllm import envs
+from vllm import _custom_ops as ops
+from vllm.distributed.parallel_state import get_tensor_model_parallel_world_size
+from vllm.logger import init_logger
+from vllm.model_executor.layers.quantization.fp8 import (
+    ACTIVATION_SCHEMES, Fp8Config, Fp8LinearMethod, Fp8MoEMethod)
+from vllm.model_executor.layers.quantization.utils.marlin_utils_fp8 import (
+    apply_fp8_marlin_linear, prepare_fp8_layer_for_marlin)
+from vllm.model_executor.layers.quantization.utils.w8a8_utils import (
+    convert_to_channelwise, cutlass_block_fp8_supported, cutlass_fp8_supported,
+    normalize_e4m3fn_to_e4m3fnuz, requantize_with_max_scale,
+    maybe_create_device_identity, Fp8LinearOp)
+from vllm.model_executor.parameter import (
+    BlockQuantScaleParameter, ChannelQuantScaleParameter,
+    ModelWeightParameter, PerTensorScaleParameter)
+from vllm.platforms import current_platform
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+import vllm_mlu._mlu_ops as mlu_ops
+
+logger = init_logger(__name__)
+
+
+def vllm__model_executor__layers__quantization__fp8__Fp8Config__get_min_capability(cls) -> int:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: For Cambricon MLU, the minimum capability is 60.
+    '''
+    return 60
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__model_executor__layers__quantization__fp8__Fp8Config____init__(
+    self,
+    is_checkpoint_fp8_serialized: bool = False,
+    activation_scheme: str = "dynamic",
+    ignored_layers: Optional[List[str]] = None,
+    weight_block_size: Optional[List[int]] = None,
+    activation_quant_method: Optional[str] = None,
+    weight_quant_method: Optional[str] = None,
+) -> None:
+    self.is_checkpoint_fp8_serialized = is_checkpoint_fp8_serialized
+    if is_checkpoint_fp8_serialized:
+        logger.warning("Detected fp8 checkpoint. Please note that the "
+                        "format is experimental and subject to change.")
+    if activation_scheme not in ACTIVATION_SCHEMES:
+        raise ValueError(
+            f"Unsupported activation scheme {activation_scheme}")
+    self.activation_scheme = activation_scheme
+    self.ignored_layers = ignored_layers or []
+    if weight_block_size is not None:
+        if not is_checkpoint_fp8_serialized:
+            raise ValueError(
+                "The block-wise quantization only supports fp8-serialized "
+                "checkpoint for now.")
+        if len(weight_block_size) != 2:
+            raise ValueError(
+                "The quantization block size of weight must have 2 "
+                f"dimensions, but got {len(weight_block_size)} dimensions")
+        if activation_scheme != "dynamic":
+            raise ValueError("The block-wise quantization only supports "
+                                "dynamic activation scheme for now, but got "
+                                f"{activation_scheme} activation scheme.")
+    self.weight_block_size = weight_block_size
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: Add class members activation_quant_method and weight_quant_method to
+    indicate the granularity of quantization.
+    '''
+    self.activation_quant_method = activation_quant_method
+    self.weight_quant_method = weight_quant_method
+
+    assert (self.weight_block_size or \
+        self.activation_quant_method == "per_token" and self.weight_quant_method == "per_channel"
+        and self.activation_scheme == "dynamic"), "Only support block-wise quantization, or "\
+            "input dynamic per-token output per-channel quantization yet."
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+@classmethod
+def vllm__model_executor__layers__quantization__fp8__Fp8Config__from_config(
+    cls, config: Dict[str, Any]
+) -> "Fp8Config":
+    quant_method = cls.get_from_keys(config, ["quant_method"])
+    is_checkpoint_fp8_serialized = ("fp8" in quant_method)
+    activation_scheme = cls.get_from_keys(config, ["activation_scheme"])
+    ignored_layers = cls.get_from_keys_or(config, ["ignored_layers"], None)
+    weight_block_size = cls.get_from_keys_or(config, ["weight_block_size"],
+                                                None)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: Add config members activation_quant_method and weight_quant_method to
+    indicate the granularity of quantization.
+    '''
+    activation_quant_method = cls.get_from_keys_or(config, ["activation_quant_method"], 'per_token')
+    weight_quant_method = cls.get_from_keys_or(config, ["weight_quant_method"], None)
+    return cls(is_checkpoint_fp8_serialized=is_checkpoint_fp8_serialized,
+                activation_scheme=activation_scheme,
+                ignored_layers=ignored_layers,
+                weight_block_size=weight_block_size,
+                activation_quant_method=activation_quant_method,
+                weight_quant_method=weight_quant_method)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod__create_weights(
+    self,
+    layer: torch.nn.Module,
+    input_size_per_partition: int,
+    output_partition_sizes: List[int],
+    input_size: int,
+    output_size: int,
+    params_dtype: torch.dtype,
+    **extra_weight_attrs,
+):
+    maybe_create_device_identity()
+
+    output_size_per_partition = sum(output_partition_sizes)
+    weight_loader = extra_weight_attrs.get("weight_loader")
+
+    if self.block_quant:
+        tp_size = get_tensor_model_parallel_world_size()
+        assert self.quant_config.weight_block_size is not None
+        block_n, block_k = (
+            self.quant_config.weight_block_size[0],
+            self.quant_config.weight_block_size[1],
+        )
+        # Required by row parallel
+        if (tp_size > 1
+                and input_size // input_size_per_partition == tp_size
+                and input_size_per_partition % block_k != 0):
+            raise ValueError(
+                f"Weight input_size_per_partition = "
+                f"{input_size_per_partition} is not divisible by "
+                f"weight quantization block_k = {block_k}.")
+        # Required by column parallel or enabling merged weights
+        if (tp_size > 1 and output_size // output_size_per_partition
+                == tp_size) or len(output_partition_sizes) > 1:
+            for output_partition_size in output_partition_sizes:
+                if output_partition_size % block_n != 0:
+                    raise ValueError(
+                        f"Weight output_partition_size = "
+                        f"{output_partition_size} is not divisible by "
+                        f"weight quantization block_n = {block_n}.")
+
+    layer.logical_widths = output_partition_sizes
+
+    layer.input_size_per_partition = input_size_per_partition
+    layer.output_size_per_partition = output_size_per_partition
+    layer.orig_dtype = params_dtype
+
+    # WEIGHT
+    weight_dtype = (torch.float8_e4m3fn
+                    if self.quant_config.is_checkpoint_fp8_serialized else
+                    params_dtype)
+
+    weight = ModelWeightParameter(data=torch.empty(
+        output_size_per_partition,
+        input_size_per_partition,
+        dtype=weight_dtype),
+                                    input_dim=1,
+                                    output_dim=0,
+                                    weight_loader=weight_loader)
+    layer.register_parameter("weight", weight)
+
+    # If checkpoint is serialized fp8, load them.
+    # Otherwise, wait until process_weights_after_loading.
+    if self.quant_config.is_checkpoint_fp8_serialized:
+        # WEIGHT SCALE
+        if not self.block_quant:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: Support weight per channel quantization.
+            '''
+            if self.weight_per_channel:
+                scale = ChannelQuantScaleParameter(
+                    data=torch.empty(sum(output_partition_sizes), dtype=torch.float32),
+                    output_dim=0,
+                    weight_loader=weight_loader,
+                )
+            else:
+                scale = PerTensorScaleParameter(
+                    data=torch.empty(len(output_partition_sizes),
+                                        dtype=torch.float32),
+                    weight_loader=weight_loader,
+                )
+            scale[:] = torch.finfo(torch.float32).min
+            layer.register_parameter("weight_scale", scale)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        else:
+            assert self.quant_config.activation_scheme == "dynamic"
+            scale = BlockQuantScaleParameter(
+                data=torch.empty(
+                    (output_size_per_partition + block_n - 1) // block_n,
+                    (input_size_per_partition + block_k - 1) // block_k,
+                    dtype=torch.float32,
+                ),
+                input_dim=1,
+                output_dim=0,
+                weight_loader=weight_loader,
+            )
+            scale[:] = torch.finfo(torch.float32).min
+            # The weight_scale_inv name is intentional for deepseekv3
+            layer.register_parameter("weight_scale_inv", scale)
+
+        # INPUT ACTIVATION SCALE
+        if self.quant_config.activation_scheme == "static":
+            scale = PerTensorScaleParameter(data=torch.empty(
+                len(output_partition_sizes), dtype=torch.float32),
+                                            weight_loader=weight_loader)
+
+            scale[:] = torch.finfo(torch.float32).min
+            layer.register_parameter("input_scale", scale)
+        else:
+            layer.register_parameter("input_scale", None)
+
+
+def vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod____init__(
+    self,
+    quant_config: Fp8Config
+):
+    self.quant_config = quant_config
+    self.cutlass_fp8_supported = cutlass_fp8_supported()
+    self.cutlass_block_fp8_supported = cutlass_block_fp8_supported()
+
+    # For GPUs that lack FP8 hardware support, we can leverage the Marlin
+    # kernel for fast weight-only FP8 quantization
+    self.use_marlin = (not current_platform.has_device_capability(89)
+                        or envs.VLLM_TEST_FORCE_FP8_MARLIN)
+    # Disable marlin for rocm
+    if current_platform.is_rocm():
+        self.use_marlin = False
+
+    self.block_quant = self.quant_config.weight_block_size is not None
+    if self.block_quant:
+        # Marlin doesn't support block-wise fp8
+        self.use_marlin = False
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: Add config members activation_quant_method and weight_quant_method to
+    indicate the granularity of quantization.
+    '''
+    self.weight_per_channel = (self.quant_config.weight_quant_method == 'per_channel')
+    self.activation_per_token = (self.quant_config.activation_quant_method == 'per_token')
+    if self.weight_per_channel and  self.activation_per_token:
+        self.use_marlin = False
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    self.fp8_linear = Fp8LinearOp(
+        # Default to using per_token quantization if cutlass is supported
+        use_per_token_if_dynamic=cutlass_fp8_supported())
+
+
+def vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod__process_weights_after_loading(
+    self,
+    layer: Module,
+) -> None:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: For dynamic activation and channel-wise weight quantization,
+    additional processing is not needed.
+    '''
+    if self.quant_config.is_checkpoint_fp8_serialized and \
+        self.weight_per_channel and \
+        self.quant_config.activation_scheme == "dynamic":
+        return
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    # TODO(rob): refactor block quant into separate class.
+    if self.block_quant:
+        assert self.quant_config.activation_scheme == "dynamic"
+        if current_platform.is_fp8_fnuz():
+            weight, weight_scale_inv, _ = \
+                normalize_e4m3fn_to_e4m3fnuz(
+                    weight=layer.weight,
+                    weight_scale=layer.weight_scale_inv)
+        else:
+            weight = layer.weight.data
+            weight_scale_inv = layer.weight_scale_inv.data
+
+        weight = self._maybe_pad_weight(weight)
+
+        # Torch.compile cannot use Parameter subclasses.
+        layer.weight = Parameter(weight, requires_grad=False)
+        layer.weight_scale_inv = Parameter(weight_scale_inv,
+                                            requires_grad=False)
+        return
+
+    # If checkpoint not serialized fp8, quantize the weights.
+    if not self.quant_config.is_checkpoint_fp8_serialized:
+        qweight, weight_scale = ops.scaled_fp8_quant(layer.weight,
+                                                        scale=None)
+
+        # If using marlin (w8a16), kernel uses channelwise weights,
+        # so extend the weight scales to be channelwise.
+        if self.use_marlin:
+            assert weight_scale.numel() == 1
+            weight_scale = convert_to_channelwise(
+                weight_scale.expand(len(layer.logical_widths)),
+                layer.logical_widths)
+
+        # Update the layer with the new values.
+        layer.weight = Parameter(qweight.t(), requires_grad=False)
+        layer.weight_scale = Parameter(weight_scale, requires_grad=False)
+        layer.input_scale = None
+
+    # If checkpoint is fp8, handle that there are N scales for N
+    # shards in a fused module
+    else:
+        layer.weight_scale = torch.nn.Parameter(layer.weight_scale.data,
+                                                requires_grad=False)
+        if self.quant_config.activation_scheme == "static":
+            layer.input_scale = torch.nn.Parameter(layer.input_scale.data,
+                                                    requires_grad=False)
+        # If using marlin (w8a16), kernel uses channelwise weights,
+        # so extend the weight scales to be channelwise.
+        if self.use_marlin:
+            weight = layer.weight
+            weight_scale = convert_to_channelwise(layer.weight_scale,
+                                                    layer.logical_widths)
+
+        # If using w8a8, torch._scaled_mm needs per tensor, so
+        # requantize the logical shards as a single weight.
+        else:
+            # Dequant -> Quant with max scale so we can run per tensor.
+            weight = layer.weight
+            weight_scale = layer.weight_scale
+
+            if current_platform.is_fp8_fnuz():
+                weight, weight_scale, input_scale = \
+                    normalize_e4m3fn_to_e4m3fnuz(
+                        weight=weight,
+                        weight_scale=weight_scale,
+                        input_scale=layer.input_scale)
+                if input_scale is not None:
+                    layer.input_scale = Parameter(input_scale,
+                                                    requires_grad=False)
+
+            weight_scale, weight = requantize_with_max_scale(
+                weight=weight,
+                weight_scale=weight_scale,
+                logical_widths=layer.logical_widths,
+            )
+
+        weight = self._maybe_pad_weight(weight)
+        # Update layer with new values.
+        layer.weight = Parameter(weight.t(), requires_grad=False)
+        layer.weight_scale = Parameter(weight_scale, requires_grad=False)
+        if self.quant_config.activation_scheme == "static":
+            layer.input_scale = Parameter(layer.input_scale.max(),
+                                            requires_grad=False)
+
+    if self.use_marlin:
+        prepare_fp8_layer_for_marlin(layer)
+        # Activations not quantized for marlin.
+        del layer.input_scale
+
+def vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod__apply(
+    self,
+    layer: torch.nn.Module,
+    x: torch.Tensor,
+    bias: Optional[torch.Tensor] = None,
+    residual: Optional[torch.Tensor] = None,
+) -> torch.Tensor:
+    assert residual is None, "residual is not supported yet."
+
+    if self.use_marlin:
+        return apply_fp8_marlin_linear(
+            input=x,
+            weight=layer.weight,
+            weight_scale=layer.weight_scale,
+            workspace=layer.workspace,
+            size_n=layer.output_size_per_partition,
+            size_k=layer.input_size_per_partition,
+            bias=bias)
+
+    # Note: lazy import to avoid triton import error.
+    from vllm_mlu.model_executor.layers.quantization.utils.fp8_utils import (
+        apply_w8a8_block_fp8_linear)
+    if self.block_quant:
+        assert self.quant_config.weight_block_size is not None
+        return apply_w8a8_block_fp8_linear(
+            input=x,
+            weight=layer.weight,
+            block_size=self.quant_config.weight_block_size,
+            weight_scale=layer.weight_scale_inv,
+            input_scale=layer.input_scale,
+            bias=bias,
+            cutlass_block_fp8_supported=self.cutlass_block_fp8_supported,
+        )
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: Use activation per token quantization based on quantization config.
+    '''
+    return self.fp8_linear.apply(
+        input=x,
+        weight=layer.weight,
+        weight_scale=layer.weight_scale,
+        input_scale=layer.input_scale,
+        bias=bias,
+        use_per_token_if_dynamic=self.activation_per_token,
+        weight_per_channel=self.weight_per_channel,
+        activation_per_token=self.activation_per_token)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+def vllm__model_executor__layers__quantization__fp8__Fp8MoEMethod__apply(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        router_logits: torch.Tensor,
+        top_k: int,
+        renormalize: bool,
+        use_grouped_topk: bool = False,
+        topk_group: Optional[int] = None,
+        num_expert_group: Optional[int] = None,
+        global_num_experts: int = -1,
+        expert_map: Optional[torch.Tensor] = None,
+        custom_routing_function: Optional[Callable] = None,
+        scoring_func: str = "softmax",
+        e_score_correction_bias: Optional[torch.Tensor] = None,
+        apply_router_weight_on_input: bool = False,
+        activation: str = "silu",
+    ) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: Use moe_softmax_topk and moe_sigmoid_topk of mlu_ops to implement FusedMoE.select_experts
+    '''
+    from vllm_mlu.model_executor.layers.fused_moe.fused_moe import fused_experts
+    routed_scaling_factor = 1.
+    if scoring_func == "softmax":
+        topk_weights, topk_ids = mlu_ops.moe_softmax_topk(router_logits,
+                                                            top_k,
+                                                            renormalize,
+                                                            num_expert_group,
+                                                            topk_group,
+                                                            route_scale=routed_scaling_factor)
+    elif scoring_func == "sigmoid":
+        topk_weights, topk_ids = mlu_ops.moe_sigmoid_topk(router_logits, top_k, renormalize,
+                                                            num_expert_group, topk_group,
+                                                            routed_scaling_factor,
+                                                            e_score_correction_bias)
+    else:
+        raise ValueError(f"Unsupported scoring function: {scoring_func}")
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    return fused_experts(
+        x,
+        layer.w13_weight,
+        layer.w2_weight,
+        topk_weights=topk_weights,
+        topk_ids=topk_ids,
+        inplace=True,
+        activation=activation,
+        use_fp8_w8a8=True,
+        global_num_experts=global_num_experts,
+        apply_router_weight_on_input=apply_router_weight_on_input,
+        expert_map=expert_map,
+        w1_scale=(layer.w13_weight_scale_inv
+                    if self.block_quant else layer.w13_weight_scale),
+        w2_scale=(layer.w2_weight_scale_inv
+                    if self.block_quant else layer.w2_weight_scale),
+        a1_scale=layer.w13_input_scale,
+        a2_scale=layer.w2_input_scale,
+        block_shape=self.quant_config.weight_block_size,
+        allow_deep_gemm=self.allow_deep_gemm,
+    )
+
+
+
+MluHijackObject.apply_hijack(
+    Fp8LinearMethod,
+    Fp8LinearMethod.apply,
+    vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod__apply
+)
+MluHijackObject.apply_hijack(
+    Fp8Config,
+    Fp8Config.get_min_capability,
+    vllm__model_executor__layers__quantization__fp8__Fp8Config__get_min_capability
+)
+MluHijackObject.apply_hijack(
+    Fp8Config,
+    Fp8Config.__init__,
+    vllm__model_executor__layers__quantization__fp8__Fp8Config____init__
+)
+MluHijackObject.apply_hijack(
+    Fp8Config,
+    Fp8Config.from_config,
+    vllm__model_executor__layers__quantization__fp8__Fp8Config__from_config
+)
+MluHijackObject.apply_hijack(
+    Fp8LinearMethod,
+    Fp8LinearMethod.create_weights,
+    vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod__create_weights
+)
+MluHijackObject.apply_hijack(
+    Fp8LinearMethod,
+    Fp8LinearMethod.__init__,
+    vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod____init__
+)
+MluHijackObject.apply_hijack(
+    Fp8LinearMethod,
+    Fp8LinearMethod.process_weights_after_loading,
+    vllm__model_executor__layers__quantization__fp8__Fp8LinearMethod__process_weights_after_loading
+)
+MluHijackObject.apply_hijack(
+    Fp8MoEMethod,
+    Fp8MoEMethod.apply,
+    vllm__model_executor__layers__quantization__fp8__Fp8MoEMethod__apply
+)

