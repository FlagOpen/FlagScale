diff --git a/vllm_mlu/vllm_mlu/model_executor/layers/fused_moe/layer.py b/vllm_mlu/vllm_mlu/model_executor/layers/fused_moe/layer.py
new file mode 100644
index 000000000..28012543f
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/layers/fused_moe/layer.py
@@ -0,0 +1,52 @@
+from typing import Optional, Callable
+
+import torch
+
+from vllm.model_executor.layers.fused_moe.layer import UnquantizedFusedMoEMethod
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__model_executor__layers__fused_moe__layer__UnquantizedFusedMoEMethod__forward_oot(
+    self,
+    layer: torch.nn.Module,
+    x: torch.Tensor,
+    router_logits: torch.Tensor,
+    top_k: int,
+    renormalize: bool,
+    use_grouped_topk: bool = False,
+    topk_group: Optional[int] = None,
+    num_expert_group: Optional[int] = None,
+    global_num_experts: int = -1,
+    expert_map: Optional[torch.Tensor] = None,
+    custom_routing_function: Optional[Callable] = None,
+    scoring_func: str = "softmax",
+    e_score_correction_bias: Optional[torch.Tensor] = None,
+    apply_router_weight_on_input: bool = False,
+    activation: str = "silu",
+) -> torch.Tensor:
+    from vllm_mlu import _mlu_ops as mlu_ops
+
+    assert use_grouped_topk is False and num_expert_group is None and topk_group is None, \
+        f"Following params: use_grouped_topk, num_expert_group, topk_group are not support yet."
+    return mlu_ops.fused_moe(
+        x,
+        router_logits,
+        layer.w13_weight, layer.w2_weight,
+        None, None, # bias1, bias2
+        None, # residual
+        None, # input_smooth
+        None, # act_smooth
+        None, None, # w1_scale, w2_scale
+        top_k,
+        renormalize,
+        True, # gated
+        activation
+    )
+
+
+MluHijackObject.apply_hijack(
+    UnquantizedFusedMoEMethod,
+    UnquantizedFusedMoEMethod.forward_oot,
+    vllm__model_executor__layers__fused_moe__layer__UnquantizedFusedMoEMethod__forward_oot
+)
\ No newline at end of file

