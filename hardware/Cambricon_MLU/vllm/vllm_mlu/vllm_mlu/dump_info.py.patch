diff --git a/vllm_mlu/vllm_mlu/dump_info.py b/vllm_mlu/vllm_mlu/dump_info.py
new file mode 100644
index 000000000..3309700db
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/dump_info.py
@@ -0,0 +1,380 @@
+import os
+from vllm.logger import init_logger
+from vllm_mlu.mlu_hijack_utils import get_is_gated, TypedDict
+import json
+from vllm.transformers_utils.config import get_config
+from vllm.entrypoints.llm import LLM
+from vllm_mlu._mlu_utils import VLLM_DUMP_MLU_INFO_EN
+logger = init_logger(__name__)
+
+def get_deepseek_v2_flops(bcfg, batch, seq_len, hidden_size):
+    qk_nope_head_dim = bcfg["qk_nope_head_dim"]
+    qk_rope_head_dim = bcfg["qk_rope_head_dim"]
+    v_head_dim       = bcfg["v_head_dim"]
+    q_lora_rank      = bcfg["q_lora_rank"]
+    kv_lora_rank     = bcfg["kv_lora_rank"]
+    context_atn_pre = 2 * batch * seq_len * \
+                 (hidden_size * q_lora_rank + \
+                 hidden_size * (kv_lora_rank + qk_rope_head_dim) + \
+                 q_lora_rank * bcfg["head_num"] * (qk_nope_head_dim + qk_rope_head_dim) + \
+                 kv_lora_rank * bcfg["head_num"] * (qk_nope_head_dim + v_head_dim))
+    context_atn_qk = 2 * batch * seq_len * seq_len * bcfg["head_num"] * (qk_nope_head_dim + qk_rope_head_dim)
+    context_atn_qkv = 2 * batch * seq_len * seq_len * bcfg["head_num"] * v_head_dim
+    context_atn_post = 2 * batch * seq_len * bcfg["head_num"] * v_head_dim * hidden_size
+    return context_atn_pre, context_atn_qk, context_atn_qkv, context_atn_post
+
+
+FlopsInfo = {
+    "context_flops":0.0,
+    "decoder_flops":0.0
+    }
+
+
+class LLMDumpInfo:
+    def __init__(self,
+                 tensor_parallel_size=None,
+                 dtype=None, kv_cache_dtype=None,
+                 quantization=None,
+                 model=None, batch_size=None,
+                 input_len=None,
+                 output_len=None,
+                 trust_remote_code=None)->None:
+        self.hfu_info = None
+        self.flops_info = None
+        self.hfu_model_config = TypedDict
+        self.io_efficiency = 0
+        self.context_latency_device = 0
+        self.generate_latency_device = 0
+
+        self.tensor_parallel_size = tensor_parallel_size
+        self.dtype = dtype
+        self.kv_cache_dtype = kv_cache_dtype
+        self.quantization = quantization
+        self.batch_size = batch_size
+        self.input_len = input_len
+        self.output_len = output_len
+        self.model = model
+        self.model_config = None
+        self.trust_remote_code = trust_remote_code
+
+
+    def init_param(self,
+                   tensor_parallel_size=None,
+                   dtype=None,
+                   kv_cache_dtype=None,
+                   quantization=None,
+                   model=None,
+                   batch_size=None,
+                   input_len=None,
+                   output_len=None,
+                   trust_remote_code=None,
+                   context_latency_device=None,
+                   generate_latency_device=None):
+        if tensor_parallel_size != None:
+            self.tensor_parallel_size = tensor_parallel_size
+        if dtype != None:
+            self.dtype = dtype
+        if kv_cache_dtype != None:
+            self.kv_cache_dtype = kv_cache_dtype
+        if quantization != None:
+            self.quantization = quantization
+        if model != None:
+            self.model = model
+        if batch_size != None:
+            self.batch_size = batch_size
+        if input_len != None:
+            self.input_len = input_len
+        if output_len != None:
+            self.output_len = output_len
+        if trust_remote_code != None:
+            self.trust_remote_code = trust_remote_code
+        if context_latency_device != None:
+            self.context_latency_device = context_latency_device
+        if generate_latency_device != None:
+            self.generate_latency_device = generate_latency_device
+
+        # paser the model config
+        if self.model_config == None and self.model != None and self.trust_remote_code != None:
+            self.model_config = get_config(self.model, self.trust_remote_code)
+
+    def initialize_hfu_model_config(self):
+        # prepare input
+        if hasattr(self.model_config, "hidden_size"):
+            self.hfu_model_config["hidden_size"] = self.model_config.hidden_size
+        elif hasattr(self.model_config, "audio_config") and hasattr(self.model_config.audio_config, "d_model"):
+            self.hfu_model_config["hidden_size"] = self.model_config.audio_config.d_model
+        else:
+            logger.error("The model's config.json does not contain hidden_size or audio_config.d_model.")
+        self.hfu_model_config["vocab_size"]   = self.model_config.vocab_size
+        self.hfu_model_config["cla_coeffient"]   = 1.0
+
+        possible_keys_ffn_size = [
+            # chatglm3-6b-32k
+            "ffn_hidden_size",
+            # llama3-8b-hf
+            "intermediate_size",
+        ]
+        possible_kv_heads = [
+            # chatglm3-6b-32k
+            "multi_query_group_num",
+            # llama3-8b-hf
+            "num_key_value_heads",
+            # falcon-180B-chat
+            "num_kv_heads",
+        ]
+        possible_num_attention_heads = [
+            "num_attention_heads",
+            "n_heads",
+        ]
+        moe_size=None
+        ffn_size=None
+        if getattr(self.model_config, "moe_intermediate_size", None):
+            moe_size = getattr(self.model_config, "moe_intermediate_size", None)
+        for key in possible_keys_ffn_size:
+            ffn_size = getattr(self.model_config, key, None)
+            if ffn_size is not None:
+                break
+        if self.model_config.model_type in ['bloom'] and ffn_size is None:
+            ffn_size = self.model_config.hidden_size * 4
+        if self.model_config.model_type in ['qwen']:
+           ffn_size = self.model_config.intermediate_size // 2
+        if ffn_size is None and moe_size is None:
+            logger.warning("The model's config.json does not contain any of the following"
+                        "keys to determine the ffn_size or moe_size: "
+                        f"{possible_keys_ffn_size}. ")
+
+        for key in possible_num_attention_heads:
+            num_attention_heads = getattr(self.model_config, key, None)
+            if num_attention_heads is not None:
+                break
+
+        if num_attention_heads is None:
+            if hasattr(self.model_config, "audio_config") and hasattr(self.model_config.audio_config, "encoder_attention_heads"):
+                num_attention_heads = self.model_config.audio_config.encoder_attention_heads
+            else:
+                logger.error("The model's config.json does not contain any of the following"
+                            "keys to determine the num_attention_heads: "
+                            f"{possible_num_attention_heads}. ")
+
+        for key in possible_kv_heads:
+            kv_heads = getattr(self.model_config, key, None)
+            if kv_heads is not None:
+                break
+
+        if kv_heads is None:
+            logger.warning("The model's config.json does not contain any of the following"
+                        "keys to determine the kv_heads: "
+                        f"{possible_kv_heads}, use num_attention_heads to replace")
+            kv_heads = num_attention_heads
+        self.hfu_model_config["ffn_inner_size"] =  0 if ffn_size is None else ffn_size
+        self.hfu_model_config["moe_inner_size"] =  0 if moe_size is None else moe_size
+        self.hfu_model_config["moe_layer_num"]  =  0 if moe_size is None else self.model_config.num_hidden_layers
+        if getattr(self.model_config, "num_hidden_layers", None):
+            num_hidden_layers = getattr(self.model_config, "num_hidden_layers", None)
+        elif hasattr(self.model_config, "audio_config") and hasattr(self.model_config.audio_config, "encoder_layers"):
+            num_hidden_layers = self.model_config.audio_config.encoder_layers
+        else:
+            logger.error("The model's config.json does not contain num_hidden_layers or audio_config.encoder_layers.")
+        self.hfu_model_config["layer_num"] = num_hidden_layers
+        self.hfu_model_config["head_num"]       =  num_attention_heads
+        self.hfu_model_config["head_size"]      =  self.hfu_model_config["hidden_size"] / self.hfu_model_config["head_num"]
+        self.hfu_model_config["head_num_kv"]    =  kv_heads
+        self.hfu_model_config["tp_num"]         =  self.tensor_parallel_size
+        if hasattr(self.model_config, "shared_expert_intermediate_size") and self.model_config.shared_expert_intermediate_size is not None:
+            self.hfu_model_config["shared_expert_intermediate_size"] = self.model_config.shared_expert_intermediate_size
+        else:
+            self.hfu_model_config["shared_expert_intermediate_size"] = 0
+        self.hfu_model_config["use_gated_ffn"]   =  get_is_gated()
+        if hasattr(self.model_config, "n_shared_experts") and self.model_config.n_shared_experts is not None:
+            self.hfu_model_config["shared_expert_intermediate_size"] = self.model_config.n_shared_experts * moe_size
+        else:
+            self.hfu_model_config["shared_experts"]  = 0
+        if hasattr(self.model_config, "num_experts") and self.model_config.num_experts is not None:
+            self.hfu_model_config["experts_num"]     =  self.model_config.num_experts
+            if self.model_config.model_type == 'hunyuan':
+                self.hfu_model_config["topk_num"]        =  self.model_config.moe_topk
+            else:
+                self.hfu_model_config["topk_num"]        =  self.model_config.num_experts_per_tok
+        elif hasattr(self.model_config, "num_local_experts"):
+            self.hfu_model_config["experts_num"]     =  self.model_config.num_local_experts
+            if self.model_config.model_type == 'hunyuan':
+                self.hfu_model_config["topk_num"]        =  self.model_config.moe_topk
+            else:
+                self.hfu_model_config["topk_num"]        =  self.model_config.num_experts_per_tok
+        elif hasattr(self.model_config, "n_routed_experts"):
+            self.hfu_model_config["experts_num"]     =  self.model_config.n_routed_experts
+            if self.model_config.model_type == 'hunyuan':
+                self.hfu_model_config["topk_num"]        =  self.model_config.moe_topk
+            else:
+                self.hfu_model_config["topk_num"]        =  self.model_config.num_experts_per_tok
+        else:
+            self.hfu_model_config["experts_num"]     =  0
+        if hasattr(self.model_config, "model_type") and self.model_config.model_type is not None:
+            self.hfu_model_config["model_type"] = self.model_config.model_type
+            # when adding a moe model, need fix moe/ffn info, like
+            # moe_inner_size, ffn_inner_size, moe_layer_num, shared_expert_intermediate_size.
+            # add for mixtral
+            if self.model_config.model_type == "mixtral":
+                self.hfu_model_config["moe_inner_size"] = ffn_size
+                self.hfu_model_config["ffn_inner_size"] =  0
+                self.hfu_model_config["moe_layer_num"]  = self.model_config.num_hidden_layers
+            # add for deepseek-v2
+            if self.model_config.model_type in ["deepseek_v2", "deepseek_v3"]:
+                if hasattr(self.model_config, "first_k_dense_replace") and self.model_config.first_k_dense_replace is not None:
+                    self.hfu_model_config["moe_layer_num"] = self.model_config.num_hidden_layers - self.model_config.first_k_dense_replace
+                if hasattr(self.model_config, "qk_nope_head_dim") and self.model_config.qk_nope_head_dim is not None:
+                    self.hfu_model_config["qk_nope_head_dim"] = self.model_config.qk_nope_head_dim
+                if hasattr(self.model_config, "qk_rope_head_dim") and self.model_config.qk_rope_head_dim is not None:
+                    self.hfu_model_config["qk_rope_head_dim"] = self.model_config.qk_rope_head_dim
+                if hasattr(self.model_config, "v_head_dim") and self.model_config.v_head_dim is not None:
+                    self.hfu_model_config["v_head_dim"] = self.model_config.v_head_dim
+                if hasattr(self.model_config, "q_lora_rank") and self.model_config.q_lora_rank is not None:
+                    self.hfu_model_config["q_lora_rank"] = self.model_config.q_lora_rank
+                else:
+                    self.hfu_model_config["q_lora_rank"] = 0
+                if hasattr(self.model_config, "kv_lora_rank") and self.model_config.kv_lora_rank is not None:
+                    self.hfu_model_config["kv_lora_rank"] = self.model_config.kv_lora_rank
+            # add for Hunyuan
+            if self.model_config.model_type == "hunyuan":
+                self.hfu_model_config["cla_coeffient"] = 0.5 # huanyuan model use CLA2
+                if hasattr(self.model_config, "num_shared_expert") and self.model_config.num_shared_expert is not None:
+                    self.hfu_model_config["shared_expert_intermediate_size"] = self.model_config.num_shared_expert * self.model_config.intermediate_size
+                if not self.hfu_model_config["moe_inner_size"] and self.model_config.intermediate_size is not None:
+                    self.hfu_model_config["moe_inner_size"] = self.model_config.intermediate_size
+                if not self.hfu_model_config["moe_layer_num"] and hasattr(self.model_config, "num_experts"):
+                    self.hfu_model_config["moe_layer_num"] = self.model_config.num_hidden_layers
+
+        self.hfu_model_config["use_causal_mask"]     =  True  # the flash attention is only use causal_mask in vllm
+
+        if self.dtype == "auto":
+            self.hfu_model_config["data_type"] = "float16"
+        else:
+            self.hfu_model_config["data_type"] = self.dtype
+
+        if self.quantization != None:
+            config = None
+            if os.path.exists(self.model + "/quantize_config.json"):
+                with open(self.model + "/quantize_config.json", 'r') as file:
+                    config = json.load(file)
+            elif os.path.exists(self.model + "/config.json"):
+                with open(self.model + "/config.json", 'r') as file:
+                    config = json.load(file).get("quantization_config", None)
+            assert config is not None, "The model's quantize_config.json does not exist."
+
+            if config["quant_mode"] == "SmoothQuant":
+                self.hfu_model_config["smooth_quant_type"] = b"SmoothQuant"
+            else:
+                self.hfu_model_config["smooth_quant_type"] = "invalid"
+            if self.quantization == 'fp8':
+                self.hfu_model_config["filter_data_type"] = f"fp8_{config['fmt']}"
+            else:
+                self.hfu_model_config["filter_data_type"] = ("int" +  str(config['bits']))
+        else:
+            self.hfu_model_config["smooth_quant_type"] = "invalid"
+            self.hfu_model_config["filter_data_type"] = self.hfu_model_config["data_type"]
+
+        if self.kv_cache_dtype == "auto":
+            self.hfu_model_config["kv_cache_dtype"]      =  self.hfu_model_config["data_type"]
+        else:
+            self.hfu_model_config["kv_cache_dtype"]      =  self.kv_cache_dtype
+
+
+    def get_flops(self):
+        self.batch_size = self.batch_size
+        seq_len = self.input_len
+        hidden_size = self.hfu_model_config["hidden_size"]
+        voc_size = self.hfu_model_config["vocab_size"]
+        ffn_size = self.hfu_model_config["ffn_inner_size"]
+        moe_size = self.hfu_model_config["moe_inner_size"]
+        shared_expert_intermediate_size = self.hfu_model_config["shared_expert_intermediate_size"]
+        layer_num = self.hfu_model_config["layer_num"]
+        out_seq = self.output_len
+        seq_len_decode = seq_len + out_seq / 2
+        r = self.hfu_model_config["head_num"] / self.hfu_model_config["head_num_kv"]
+        bsh2 = self.batch_size * seq_len * hidden_size * hidden_size
+        cla_coeffient = self.hfu_model_config["cla_coeffient"]
+        if self.hfu_model_config["model_type"] in ["deepseek_v2", "deepseek_v3"]:
+            context_atn_pre, context_atn_qk, context_atn_qkv, context_atn_post = (
+                get_deepseek_v2_flops(self.hfu_model_config, self.batch_size, seq_len, hidden_size)
+            )
+        else:
+            context_atn_pre = 2 * bsh2 + 4 * bsh2 / r * cla_coeffient
+            context_atn_qk = 2 * self.batch_size * seq_len * seq_len * hidden_size
+            context_atn_qkv = 2 * self.batch_size * seq_len * seq_len * hidden_size
+            context_atn_post = 2 * self.batch_size * seq_len * hidden_size * hidden_size
+        context_lm_head = 2 * self.batch_size * seq_len * hidden_size * voc_size
+        context_ffn = 0
+        bh2 = self.batch_size * hidden_size * hidden_size
+        decode_atn_pre = 2 * bh2 + 4 * bh2 / r * cla_coeffient
+        decode_atn_qk = 2 * self.batch_size * seq_len_decode * hidden_size
+        decode_atn_qkv = 2 * self.batch_size * seq_len_decode * hidden_size
+        decode_atn_post = 2 * self.batch_size * hidden_size * hidden_size
+        decode_lm_head = 2 * self.batch_size * hidden_size * voc_size
+        decode_ffn = 0
+        coeffient = 6 if self.hfu_model_config["use_gated_ffn"] else 4
+        if self.hfu_model_config["experts_num"] == 0:
+            context_ffn = coeffient * self.batch_size * seq_len * hidden_size * ffn_size
+            decode_ffn = coeffient * self.batch_size * hidden_size * ffn_size
+        else:
+            context_ffn = self.batch_size * seq_len * hidden_size * (coeffient * (moe_size * self.hfu_model_config["topk_num"] + shared_expert_intermediate_size) + 2 * self.hfu_model_config["experts_num"])
+            decode_ffn = self.batch_size * hidden_size * (coeffient * (moe_size * self.hfu_model_config["topk_num"] + shared_expert_intermediate_size) + 2 * self.hfu_model_config["experts_num"])
+
+        if self.hfu_model_config["use_causal_mask"]:
+            c = 0.5
+            context_atn_qk *= c
+            context_atn_qkv *= c
+
+        self.flops_info["context_flops"] = context_lm_head
+        self.flops_info["decoder_flops"] = decode_lm_head
+        if self.hfu_model_config["kv_cache_dtype"] != b"int8":
+            self.flops_info["context_flops"] += (layer_num * (context_atn_qk + context_atn_qkv))
+            self.flops_info["decoder_flops"] += (layer_num * (decode_atn_qk + decode_atn_qkv))
+        else:
+            self.flops_info["context_flops"] += (layer_num * (context_atn_qk + context_atn_qkv))
+            self.flops_info["decoder_flops"] += (layer_num * (decode_atn_qk + decode_atn_qkv))
+
+        if self.hfu_model_config["smooth_quant_type"] == b"invalid":
+            self.flops_info["context_flops"] += (layer_num * (context_atn_pre + context_atn_post + context_ffn))
+            self.flops_info["decoder_flops"] += (layer_num * (decode_atn_pre + decode_atn_post + decode_ffn))
+        else:
+            self.flops_info["context_flops"] += (layer_num * (context_atn_pre + context_atn_post + context_ffn))
+            self.flops_info["decoder_flops"] += (layer_num * (decode_atn_pre + decode_atn_post + decode_ffn))
+
+
+    def get_decoder_io_efficiency(self):
+        try:
+            from vllm_mlu.device_info import get_decoder_io_efficiency
+            self.io_efficiency = get_decoder_io_efficiency(self.hfu_model_config, self.batch_size, self.input_len, self.output_len, self.generate_latency_device)
+        except:
+            logger.info("Unsupport io_efficiency get_decoder_io_efficiency function")
+
+    def get_device_output_info(self):
+        self.initialize_hfu_model_config()
+
+        if VLLM_DUMP_MLU_INFO_EN:
+            from vllm_mlu.device_info import get_flops_inner, HFUInfo
+            self.hfu_info = HFUInfo
+            get_flops_inner(self.hfu_model_config, self.batch_size, self.input_len, self.output_len, self.tensor_parallel_size, self.hfu_info)
+            self.get_decoder_io_efficiency()
+        else:
+            self.flops_info = FlopsInfo
+            self.get_flops()
+
+    def has_information_dump(self):
+        if VLLM_DUMP_MLU_INFO_EN:
+            try:
+                import vllm_mlu.device_info
+                return True
+            except:
+                return False
+        return False
+
+    def dump(self):
+        self.get_device_output_info()
+
+    def dump_performance_info(self):
+        try:
+            from vllm_mlu.device_info import dump_information
+            dump_information(LLM.dump_info)
+        except:
+            logger.info("Unsupport dump performance information")

