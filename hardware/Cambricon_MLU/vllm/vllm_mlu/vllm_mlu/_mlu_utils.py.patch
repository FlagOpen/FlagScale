diff --git a/vllm_mlu/vllm_mlu/_mlu_utils.py b/vllm_mlu/vllm_mlu/_mlu_utils.py
new file mode 100644
index 000000000..7d02d142e
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/_mlu_utils.py
@@ -0,0 +1,86 @@
+import os
+import torch
+
+
+def _check_env(env, default=False):
+    if env in os.environ:
+        return os.environ[env].lower() in ["true", "1"]
+    return default
+
+
+def _check_env_value(env, default=0):
+    if env in os.environ:
+        if not os.environ[env].isdigit():
+            raise ValueError(f"'{env}' should be set with integer")
+        value = int(os.environ[env])
+        return value
+    return default
+
+
+# VLLM_LATENCY_DEBUG: Get more kernel info for benchmark latency.
+VLLM_LATENCY_DEBUG = _check_env("VLLM_LATENCY_DEBUG", default=False)
+
+# VLLM_LATENCY_DEBUG_NO_DEVICE: Get more kernel info(without device) for benchmark latency.
+VLLM_LATENCY_DEBUG_NO_DEVICE = _check_env("VLLM_LATENCY_DEBUG_NO_DEVICE", default=False)
+
+# VLLM_DUMP_TENSORS: Dump each layer outputs when running vLLM inference.
+VLLM_DUMP_OUTPUTS = _check_env("VLLM_DUMP_OUTPUTS", default=False)
+
+# VLLM_DUMP_MLU_INFO: Get device info when running vLLM inference.
+VLLM_DUMP_MLU_INFO = _check_env("VLLM_DUMP_MLU_INFO", default=False)
+
+# VLLM_SCHEDULER_PROFILE: Profiling vLLM scheduler.
+VLLM_SCHEDULER_PROFILE = _check_env("VLLM_SCHEDULER_PROFILE", default=False)
+
+# VLLM_GRAPH_DEBUG: Debug the graph status when running decoder, default value is True.
+# Set to False to disable warning messages.
+VLLM_GRAPH_DEBUG = _check_env("VLLM_GRAPH_DEBUG", default=True)
+
+# CHUNKED_PIPELINE_PARALLEL_EN: use chunked pipeline parallel, default value is False.
+CHUNKED_PIPELINE_PARALLEL_EN = _check_env("CHUNKED_PIPELINE_PARALLEL_EN", default=False)
+
+# CONTEXT_PARALLEL_EN: use context parallel, default value is False.
+CONTEXT_PARALLEL_EN = _check_env("CONTEXT_PARALLEL_EN", default=False)
+
+# EXPERT_PARALLEL_EN: use expert parallel, default value is False.
+EXPERT_PARALLEL_EN = _check_env("EXPERT_PARALLEL_EN", default=False)
+
+# ATTN_DATA_PARALLEL_EN: use attn data parallel, default value is False.
+ATTN_DATA_PARALLEL_EN = _check_env("ATTN_DATA_PARALLEL_EN", default=False)
+
+# VLLM_AVG_MOE_EN: make moe experts workload balance, default value is False.
+VLLM_AVG_MOE_EN = _check_env("VLLM_AVG_MOE_EN", default=False)
+
+# VLLM_LOGITS_USE_ALL_GATHER: use allgather for logits collection, default value is False.
+VLLM_LOGITS_USE_ALL_GATHER = _check_env("VLLM_LOGITS_USE_ALL_GATHER", default=False)
+
+VLLM_LATENCY_DEBUG_EN = (VLLM_LATENCY_DEBUG or VLLM_LATENCY_DEBUG_NO_DEVICE)
+VLLM_LATENCY_DEBUG_WITH_DEVICE_EN = (VLLM_LATENCY_DEBUG and not VLLM_LATENCY_DEBUG_NO_DEVICE)
+VLLM_DUMP_MLU_INFO_EN = (VLLM_LATENCY_DEBUG_WITH_DEVICE_EN and VLLM_DUMP_MLU_INFO)
+CUSTOM_VLLM_HIJACK_EN = (CHUNKED_PIPELINE_PARALLEL_EN or CONTEXT_PARALLEL_EN
+                         or EXPERT_PARALLEL_EN or ATTN_DATA_PARALLEL_EN)
+
+VLLM_PRELOAD_SIZE = _check_env_value("VLLM_PRELOAD_SIZE", default=0)
+
+# ATTN_PARALLEL_NUM & FFN_PARALLEL_NUM: use context comm cmpt parallel.
+ATTN_PARALLEL_NUM = 'ATTN_PARALLEL_NUM'
+FFN_PARALLEL_NUM = 'FFN_PARALLEL_NUM'
+
+
+def check_context_comm_cmpt_parallel():
+    return (ATTN_PARALLEL_NUM in os.environ) or (FFN_PARALLEL_NUM in os.environ)
+
+
+def set_is_prompt(flag):
+    global IS_PROMPT
+    IS_PROMPT=flag
+
+
+def get_is_prompt():
+    return IS_PROMPT
+
+def get_dpsk_mcc_max_parallel_num():
+    return _check_env_value("DPSK_MCC_PARALLEL_NUM", default=1)
+
+def is_dpsk_mcc_enabled():
+    return get_dpsk_mcc_max_parallel_num() > 1

