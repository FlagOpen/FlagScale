diff --git a/vllm_mlu/vllm_mlu/device_info.py b/vllm_mlu/vllm_mlu/device_info.py
new file mode 100644
index 000000000..a59138040
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/device_info.py
@@ -0,0 +1,297 @@
+import os
+from vllm.logger import init_logger
+from vllm_mlu.mlu_hijack_utils import TypedDict
+from vllm_mlu._mlu_utils import VLLM_DUMP_MLU_INFO_EN
+from typing import List, Tuple, Dict, Union
+import re
+import subprocess
+import logging
+from vllm_mlu.dump_info import get_deepseek_v2_flops
+
+logger = init_logger(__name__)
+
+data_type_byte_width_map = {
+                            "int8":1,
+                            "int4":0.5,
+                            "float16":2,
+                            "float32":4,
+                            "int32":4,
+                            "bfloat16":2
+                            }
+
+HFUInfo = {
+    "context_hfu":0.0,
+    "decoder_hfu":0.0
+    }
+
+def str_to_data_type(param):
+    data_type_map = {
+        "float16": "CNNL_DTYPE_HALF",
+        "float32": "CNNL_DTYPE_FLOAT",
+        "int32": "CNNL_DTYPE_INT32",
+        "int8": "CNNL_DTYPE_INT8",
+        "int4": "CNNL_DTYPE_INT8",
+        "bfloat16": "CNNL_DTYPE_BFLOAT16",
+        "invalid": "CNNL_DTYPE_INVALID",
+        "fp8_e4m3fn": "CNNL_DTYPE_FLOAT8_E4M3FN",
+        "fp8_e5m2": "CNNL_DTYPE_FLOAT8_E5M2",
+    }
+
+    return data_type_map.get(param, "CNNL_DTYPE_INVALID")
+
+def get_deepseek_v2_model_param(bcfg, expert_param, ffn_param):
+    num_attention_heads = bcfg["num_attention_heads"]
+    qk_nope_head_dim = bcfg["qk_nope_head_dim"]
+    qk_rope_head_dim = bcfg["qk_rope_head_dim"]
+    v_head_dim = bcfg["v_head_dim"]
+    q_lora_rank = bcfg["q_lora_rank"]
+    kv_lora_rank = bcfg["kv_lora_rank"]
+    hidden_size = bcfg["hidden_size"]
+    layer_num = bcfg["layer_num"]
+
+    qa_params = hidden_size * q_lora_rank
+    kva_params = hidden_size * (kv_lora_rank + qk_rope_head_dim)
+    qb_params = q_lora_rank * num_attention_heads * (qk_nope_head_dim + qk_rope_head_dim)
+    kvb_params = kv_lora_rank * num_attention_heads * (qk_nope_head_dim + v_head_dim)
+    o_params = v_head_dim * num_attention_heads * hidden_size
+
+    mla_weight_params = (qa_params + kva_params + qb_params + kvb_params + o_params) * layer_num
+    model_params = mla_weight_params + expert_param + ffn_param
+
+    return model_params
+
+def get_model_param(bcfg, once_batch):
+    batch = once_batch
+    hidden_size = bcfg["hidden_size"]
+    ffn_inner_size = bcfg["ffn_inner_size"]
+    moe_inner_size = bcfg["moe_inner_size"]
+    layer_num = bcfg["layer_num"]
+    r = bcfg["head_num"] / bcfg["head_num_kv"]
+    layer_moe_num = bcfg["moe_layer_num"]
+    layer_ffn_num = layer_num - layer_moe_num
+    experts_num = bcfg["experts_num"]
+    topk_num = bcfg["topk_num"]
+    shared_expert_intermediate_size = bcfg["shared_expert_intermediate_size"]
+    expert_num = min(experts_num, batch * topk_num)
+    ffn_num = 3.0 if bcfg["use_gated_ffn"] else 2.0
+
+    # shared_expert_intermediate_size = shared_expert_num * moe_intermediate_size,
+    # when adding a new moe model, must fix it when parse model.json
+    # in moe model may ffn & moe layers appear at the same time, need compute them devided
+    expert_param = layer_moe_num * ffn_num * hidden_size * (expert_num * moe_inner_size + shared_expert_intermediate_size)
+    ffn_param = layer_ffn_num * ffn_num * hidden_size * ffn_inner_size
+
+    if bcfg["model_type"] == "deepseek_v2":
+        # for deepseek_v2, using MLA replace attentionï¼Œcompute mla_weight_param here.
+        return get_deepseek_v2_model_param(bcfg, expert_param, ffn_param)
+
+    cla_coeffient = bcfg["cla_coeffient"]
+    attn_param = (2.0 + 2.0 / r * cla_coeffient) * layer_num * hidden_size * hidden_size
+
+    return expert_param + ffn_param + attn_param
+
+def get_deepseek_v2_kv_cache(bcfg, input_seq_len, output_seq_len, layer_num, kv_cache_byte_width):
+    num_attention_heads = bcfg["head_num"]
+    qk_nope_head_dim = bcfg["qk_nope_head_dim"]
+    qk_rope_head_dim = bcfg["qk_rope_head_dim"]
+    v_head_dim = bcfg["v_head_dim"]
+
+    kv_cache = num_attention_heads * (qk_nope_head_dim + qk_rope_head_dim + v_head_dim) * \
+               (input_seq_len + output_seq_len / 2) * layer_num * kv_cache_byte_width
+    return kv_cache
+
+def get_kv_cache(bcfg, once_batch, input_seq_len_, output_length):
+    batch          = once_batch
+    head_num_kv    = bcfg["head_num_kv"]
+    head_size      = bcfg["head_size"]
+    tp_num         = bcfg["tp_num"]
+    layer_num      = bcfg["layer_num"]
+    input_seq_len  = input_seq_len_
+    output_seq_len = output_length
+    kv_cache_byte_width = data_type_byte_width_map[bcfg["kv_cache_dtype"]]
+    if bcfg["model_type"] == "deepseek_v2":
+        return get_deepseek_v2_kv_cache(bcfg, input_seq_len,
+                                    output_seq_len, layer_num, kv_cache_byte_width)
+    cla_coeffient = bcfg["cla_coeffient"]
+    kv_cache = cla_coeffient * batch * max(head_num_kv, tp_num) * head_size * (input_seq_len + output_seq_len / 2) * 2 * layer_num * kv_cache_byte_width
+    return kv_cache
+
+
+def get_decoder_io_efficiency(hfu_model_config, batch_size, input_len, output_len, generate_latency):
+    model_param = get_model_param(hfu_model_config, batch_size)
+    model_byte_width = data_type_byte_width_map[hfu_model_config["filter_data_type"]]
+    param_scale = model_param * model_byte_width
+    kv_cache = get_kv_cache(hfu_model_config, batch_size, input_len, output_len)
+    # '(param_scale + kv_cache)': the number of bytes loaded during the decoder stage
+    # '(param_scale + kv_cache) / unit / unit ': the number of Megabytes loaded during the decoder stage
+    # '(param_scale + kv_cache) / unit / unit / bandwidth * unit': the last 'unit' is used to convert seconds to milliseconds of theoretical loading time
+    unit = 1000.0
+    _, _, _, bandwidth = get_device_attribute()
+    memory_MB = (param_scale + kv_cache) / unit / unit
+    io_efficiency = memory_MB / bandwidth * unit / generate_latency / hfu_model_config["tp_num"]
+
+    return io_efficiency
+
+
+def get_flops(bcfg, once_batch, input_seq_len, output_length, card_num, each_peak_compute, hfu_info):
+    batch = once_batch
+    seq_len = input_seq_len
+    hidden_size = bcfg["hidden_size"]
+    voc_size = bcfg["vocab_size"]
+    ffn_size = bcfg["ffn_inner_size"]
+    moe_size = bcfg["moe_inner_size"]
+    shared_expert_intermediate_size = bcfg["shared_expert_intermediate_size"]
+    layer_num = bcfg["layer_num"]
+    out_seq = output_length
+    seq_len_decode = seq_len + out_seq / 2
+    r = bcfg["head_num"] / bcfg["head_num_kv"]
+    bsh2 = batch * seq_len * hidden_size * hidden_size
+    cla_coeffient = bcfg["cla_coeffient"]
+
+    if bcfg["model_type"] in ['deepseek_v2','deepseek_v3']:
+        context_atn_pre, context_atn_qk, context_atn_qkv, context_atn_post = (
+            get_deepseek_v2_flops(bcfg, batch, seq_len, hidden_size)
+        )
+    else:
+        context_atn_pre = 2 * bsh2 + 4 * bsh2 / r * cla_coeffient
+        context_atn_qk = 2 * batch * seq_len * seq_len * hidden_size
+        context_atn_qkv = 2 * batch * seq_len * seq_len * hidden_size
+        context_atn_post = 2 * batch * seq_len * hidden_size * hidden_size
+    context_lm_head = 2 * batch * seq_len * hidden_size * voc_size
+    context_ffn = 0
+    bh2 = batch * hidden_size * hidden_size
+    decode_atn_pre = 2 * bh2 + 4 * bh2 / r * cla_coeffient
+    decode_atn_qk = 2 * batch * seq_len_decode * hidden_size
+    decode_atn_qkv = 2 * batch * seq_len_decode * hidden_size
+    decode_atn_post = 2 * batch * hidden_size * hidden_size
+    decode_lm_head = 2 * batch * hidden_size * voc_size
+    decode_ffn = 0
+    coeffient = 6 if bcfg["use_gated_ffn"] else 4
+    if bcfg["experts_num"] == 0:
+        context_ffn = coeffient * batch * seq_len * hidden_size * ffn_size
+        decode_ffn = coeffient * batch * hidden_size * ffn_size
+    else:
+        context_ffn = batch * seq_len * hidden_size * (coeffient * (moe_size * bcfg["topk_num"] + shared_expert_intermediate_size) + 2 * bcfg["experts_num"])
+        decode_ffn = batch * hidden_size * (coeffient * (moe_size * bcfg["topk_num"] + shared_expert_intermediate_size) + 2 * bcfg["experts_num"])
+
+    if bcfg["use_causal_mask"]:
+        c = 0.5
+        context_atn_qk *= c
+        context_atn_qkv *= c
+
+    total_peak_compute = each_peak_compute
+    for i in range(len(total_peak_compute)):
+        total_peak_compute[i] = card_num * each_peak_compute[i] + 1e-6
+
+    hfu_info["context_hfu"] = context_lm_head / total_peak_compute[1]
+    hfu_info["decoder_hfu"] = decode_lm_head / total_peak_compute[1]
+    if bcfg["kv_cache_dtype"] != "int8":
+        hfu_info["context_hfu"] += (layer_num * (context_atn_qk + context_atn_qkv) / total_peak_compute[1])
+        hfu_info["decoder_hfu"] += (layer_num * (decode_atn_qk + decode_atn_qkv) / total_peak_compute[1])
+    else:
+        hfu_info["context_hfu"] += (layer_num * (context_atn_qk + context_atn_qkv) / total_peak_compute[0])
+        hfu_info["decoder_hfu"] += (layer_num * (decode_atn_qk + decode_atn_qkv) / total_peak_compute[0])
+
+    if bcfg["smooth_quant_type"] == "invalid":
+        hfu_info["context_hfu"] += (layer_num * (context_atn_pre + context_atn_post + context_ffn) / total_peak_compute[1])
+        hfu_info["decoder_hfu"] += (layer_num * (decode_atn_pre + decode_atn_post + decode_ffn) / total_peak_compute[1])
+    else:
+        hfu_info["context_hfu"] += (layer_num * (context_atn_pre + context_atn_post + context_ffn) / total_peak_compute[2])
+        hfu_info["decoder_hfu"] += (layer_num * (decode_atn_pre + decode_atn_post + decode_ffn) / total_peak_compute[2])
+
+
+def get_flops_inner(hfu_model_config, batch_size, input_seq_len, output_length, tp_num, hfu_info):
+    if VLLM_DUMP_MLU_INFO_EN:
+        each_peak_compute = [1.0, 1.0, 1.0]
+        get_hfu_peak_flops(hfu_model_config, each_peak_compute)
+        tmp = [f"{num:.5e}" for num in each_peak_compute]
+        get_flops(hfu_model_config, batch_size, input_seq_len, output_length, tp_num, each_peak_compute, hfu_info)
+    else:
+        each_peak_compute = [1.0, 1.0, 1.0]
+        get_flops(hfu_model_config, batch_size, input_seq_len, output_length, 1, hfu_info)
+    return hfu_info
+
+def dump_information(dump_info):
+    if VLLM_DUMP_MLU_INFO_EN:
+        millisecond2second_unit = 1000
+        dump_info.hfu_info["context_hfu"] = dump_info.hfu_info["context_hfu"] / (dump_info.context_latency_device / millisecond2second_unit)
+        dump_info.hfu_info["decoder_hfu"] = dump_info.hfu_info["decoder_hfu"] / (dump_info.generate_latency_device / millisecond2second_unit)
+
+        print(f"Context HFU-visible:   {dump_info.hfu_info['context_hfu']:.3%}")
+        print(f"Decoder HFU-visible:   {dump_info.hfu_info['decoder_hfu']:.3%}")
+        print(f"Decoder IO Efficiency: {dump_info.io_efficiency:.3%}")
+
+def get_device_attribute():
+    result = subprocess.run(['cnmon', 'info'],
+                                stdout=subprocess.PIPE,
+                                stderr=subprocess.PIPE,
+                                text=True,
+                                check=True)
+
+    pattern = r"MLU\s+\d+-\d+"
+    ranges = re.findall(pattern, result.stdout)
+    unique_ranges = list(dict.fromkeys(ranges))
+    cluster_num = len(unique_ranges)
+    first_range = unique_ranges[0]
+    match = re.search(r"(\d+)-(\d+)", first_range)
+    start = int(match.group(1))
+    end = int(match.group(2))
+    core_num = end - start + 1
+    match = re.search(r"Default\s+:\s+(\d+)\s+MHz", result.stdout)
+    frequency = int(match.group(1))
+    match = re.search(r"MEM BandWidth\s+:\s+(\d+)\s+MB/s", result.stdout)
+    if match is None:
+        match = re.search(r"DDR BandWidth\s+:\s+(\d+)\s+MB/s", result.stdout)
+    bandwidth = int(match.group(1))
+    return core_num, cluster_num, frequency, bandwidth
+
+
+def get_hfu_peak_flops(hfu_model_config, each_peak_compute):
+    device_id = 0
+    device_name = os.popen(f"cnmon info -c {device_id} | grep 'Product Name' | cut -d: -f2").read().strip()
+
+    input_data_type = hfu_model_config["data_type"]
+    filter_data_type = hfu_model_config["filter_data_type"]
+
+    base_value_map: Dict[Tuple[str, str], float] = {
+        ("U5", "CNNL_DTYPE_FLOAT"): int("10000000000", 2),
+        ("U5", "CNNL_DTYPE_HALF"): int("1000000000000", 2),
+        ("U5", "CNNL_DTYPE_INT16"): int("100000000000", 2),
+        ("U5", "CNNL_DTYPE_BFLOAT16"): int("1000000000000", 2),
+        ("U5", "CNNL_DTYPE_INT8"): int("10000000000000", 2),
+        ("0B", "CNNL_DTYPE_INT8"): int("100000000000000", 2),
+        ("0B", "CNNL_DTYPE_INT16"): int("10000000000000", 2),
+        ("0B", "CNNL_DTYPE_HALF"): int("10000000000000", 2),
+        ("0B", "CNNL_DTYPE_BFLOAT16"): int("10000000000000", 2),
+        ("0B", "CNNL_DTYPE_FLOAT"): int("10000000000", 2),
+        ("0B", "CNNL_DTYPE_FLOAT8_E4M3FN"): int("100000000000000", 2),
+        ("0B", "CNNL_DTYPE_FLOAT8_E5M2"): int("100000000000000", 2),
+    }
+
+    cur_plat = ""
+    if "0B" in device_name or "80-X" in device_name:
+        cur_plat = "0B"
+    elif "U5" in device_name:
+        cur_plat = "U5"
+    else:
+        raise TypeError(f"current platform {device_name} not support HFU info get!")
+
+
+    if cur_plat:
+        keys = [
+            (cur_plat, "CNNL_DTYPE_INT16"),  # kv8
+            (cur_plat, str_to_data_type(input_data_type)),
+            (cur_plat, str_to_data_type(filter_data_type)),
+        ]
+        freq_unit = 1000.0
+        for i, key in enumerate(keys):
+            if key not in base_value_map:
+                raise TypeError(f"current data type {key} not support! input_data_type: {input_data_type}, filter_data_type: {filter_data_type}")
+            else:
+                each_peak_compute[i] = base_value_map[key]
+                core_num, cluster_num, frequency, _ = get_device_attribute()
+                each_peak_compute[i] *= frequency * freq_unit * freq_unit * cluster_num * core_num
+    else:
+        raise TypeError(f"current platform {device_name} not support HFU info get!")
+
+    return each_peak_compute

