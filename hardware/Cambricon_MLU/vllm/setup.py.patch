diff --git a/setup.py b/setup.py
index b0cc2f481..2a2728e83 100755
--- a/setup.py
+++ b/setup.py
@@ -13,11 +13,26 @@ from shutil import which
 
 import torch
 from packaging.version import Version, parse
-from setuptools import Extension, setup
+from setuptools import Extension, setup, find_packages
 from setuptools.command.build_ext import build_ext
 from setuptools_scm import get_version
 from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME
+from setuptools.command.install import install
 
+class InstallWithMlu(install):
+    def run(self):
+        # 执行原始的install命令安装vllm
+        super().run()
+
+        # 安装vllm_mlu
+        mlu_path = os.path.join(ROOT_DIR, "vllm_mlu")
+        if os.path.exists(mlu_path):
+            print("Installing vllm_mlu after vllm...")
+            subprocess.check_call([
+                sys.executable, "-m", "pip", "install", "."
+            ], cwd=mlu_path)
+        else:
+            print(f"vllm_mlu directory not found at {mlu_path}")
 
 def load_module_from_path(module_name, path):
     spec = importlib.util.spec_from_file_location(module_name, path)
@@ -30,6 +45,8 @@ def load_module_from_path(module_name, path):
 ROOT_DIR = Path(__file__).parent
 logger = logging.getLogger(__name__)
 
+os.environ['VLLM_TARGET_DEVICE'] = 'empty'
+
 # cannot import envs directly because it depends on vllm,
 #  which is not installed yet
 envs = load_module_from_path('envs', os.path.join(ROOT_DIR, 'vllm', 'envs.py'))
@@ -538,7 +555,8 @@ def get_gaudi_sw_version():
 
 
 def get_vllm_version() -> str:
-    version = get_version(write_to="vllm/_version.py")
+    # version = get_version(write_to="vllm/_version.py")
+    version = "0.8.3"
     sep = "+" if "+" not in version else "."  # dev versions might contain +
 
     if _no_device():
@@ -655,11 +673,13 @@ if _is_cuda():
 if _build_custom_ops():
     ext_modules.append(CMakeExtension(name="vllm._C"))
 
+packages = find_packages(include=['vllm', 'vllm.*'])
 package_data = {
     "vllm": [
         "py.typed",
         "model_executor/layers/fused_moe/configs/*.json",
         "model_executor/layers/quantization/utils/configs/*.json",
+        "**/*",
     ]
 }
 
@@ -667,7 +687,7 @@ if _no_device():
     ext_modules = []
 
 if not ext_modules:
-    cmdclass = {}
+    cmdclass = {"install": InstallWithMlu}
 else:
     cmdclass = {
         "build_ext":
@@ -686,6 +706,12 @@ setup(
         "audio": ["librosa", "soundfile"],  # Required for audio processing
         "video": []  # Kept for backwards compatibility
     },
+    entry_points={
+        'console_scripts': [
+            'vllm = vllm.entrypoints.cli.main:main',
+        ],
+    },
     cmdclass=cmdclass,
     package_data=package_data,
+    packages=packages,
 )

