diff --git a/megatron/core/datasets/gpt_dataset.py b/megatron/core/datasets/gpt_dataset.py
index b80caaf6c..8e0f7bf85 100644
--- a/megatron/core/datasets/gpt_dataset.py
+++ b/megatron/core/datasets/gpt_dataset.py
@@ -14,7 +14,7 @@ from megatron.core.datasets.indexed_dataset import IndexedDataset
 from megatron.core.datasets.megatron_dataset import MegatronDataset
 from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer
 from megatron.core.datasets.object_storage_utils import ObjectStorageConfig, is_object_storage_path
-from megatron.core.datasets.utils import Split
+from megatron.core.datasets.utils import Split, is_built_on_zero_rank
 from megatron.core.utils import log_single_rank
 
 logger = logging.getLogger(__name__)
@@ -199,12 +199,14 @@ class GPTDataset(MegatronDataset):
             )
             if self.masks_and_position_ids_are_cacheable:
                 self.cached_attention_mask = attention_mask
-                self.cached_loss_mask = loss_mask
+                #NOTE(mlu): cached_loss_mask and loss mask do not reuse the same address.
+                self.cached_loss_mask = loss_mask.clone()
                 self.cached_position_ids = position_ids
                 self.masks_and_position_ids_are_cached = True
         else:
             attention_mask = self.cached_attention_mask
-            loss_mask = self.cached_loss_mask
+            #NOTE(mlu): cached_loss_mask and loss mask do not reuse the same address.
+            loss_mask = self.cached_loss_mask.clone()
             position_ids = self.cached_position_ids
 
         # For padded sequences, mask the loss
@@ -355,7 +357,7 @@ class GPTDataset(MegatronDataset):
 
         if not path_to_cache or (
             not cache_hit
-            and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0)
+            and (not torch.distributed.is_initialized() or is_built_on_zero_rank())
         ):
             log_single_rank(
                 logger,

