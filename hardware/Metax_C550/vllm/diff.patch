diff --git a/CMakeLists.txt b/CMakeLists.txt
old mode 100755
new mode 100644
index c823c9ff8..9b1c192e8
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -19,6 +19,8 @@ set(VLLM_TARGET_DEVICE "cuda" CACHE STRING "Target device backend for vLLM")
 message(STATUS "Build type: ${CMAKE_BUILD_TYPE}")
 message(STATUS "Target device: ${VLLM_TARGET_DEVICE}")
 
+option(USE_MACA          "enable MACA"  ON)
+
 include(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)
 
 # Suppress potential warnings about unused manually-specified variables
@@ -46,7 +48,8 @@ set(HIP_SUPPORTED_ARCHS "gfx906;gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx11
 # requirements.txt files and should be kept consistent.  The ROCm torch
 # versions are derived from Dockerfile.rocm
 #
-set(TORCH_SUPPORTED_VERSION_CUDA "2.5.1")
+set(TORCH_SUPPORTED_VERSION_CUDA "2.1.2")
+#set(TORCH_SUPPORTED_VERSION_CUDA "2.5.1")
 set(TORCH_SUPPORTED_VERSION_ROCM "2.5.1")
 
 #
@@ -80,6 +83,8 @@ endif()
 #
 find_package(Torch REQUIRED)
 
+add_compile_definitions(USE_MACA)
+
 #
 # Forward the non-CUDA device extensions to external CMake scripts.
 #
@@ -224,6 +229,13 @@ set(VLLM_EXT_SRC
   "csrc/prepare_inputs/advance_step.cu"
   "csrc/torch_bindings.cpp")
 
+# support opt of gptq-marlin
+set_source_files_properties(
+  "csrc/quantization/gptq/q_gemm.cu"
+  PROPERTIES
+  COMPILE_FLAGS "-mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128"
+)
+
 if(VLLM_GPU_LANG STREQUAL "CUDA")
   SET(CUTLASS_ENABLE_HEADERS_ONLY ON CACHE BOOL "Enable only the header library")
 
@@ -235,6 +247,10 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     set(VLLM_CUTLASS_SRC_DIR $ENV{VLLM_CUTLASS_SRC_DIR})
   endif()
 
+  if (USE_MACA)
+    set(VLLM_CUTLASS_SRC_DIR $ENV{MACA_PATH}/include) # For MACA cutlass header files.
+  endif()
+
   if(VLLM_CUTLASS_SRC_DIR)
     if(NOT IS_ABSOLUTE VLLM_CUTLASS_SRC_DIR)
       get_filename_component(VLLM_CUTLASS_SRC_DIR "${VLLM_CUTLASS_SRC_DIR}" ABSOLUTE)
@@ -259,7 +275,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
   list(APPEND VLLM_EXT_SRC
     "csrc/mamba/mamba_ssm/selective_scan_fwd.cu"
     "csrc/mamba/causal_conv1d/causal_conv1d.cu"
-    "csrc/quantization/aqlm/gemm_kernels.cu"
+    #"csrc/quantization/aqlm/gemm_kernels.cu"
     "csrc/quantization/awq/gemm_kernels.cu"
     "csrc/custom_all_reduce.cu"
     "csrc/permute_cols.cu"
@@ -272,10 +288,17 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     SRCS "${VLLM_EXT_SRC}"
     CUDA_ARCHS "${CUDA_ARCHS}")
 
+  # support opt of gptq-marlin
+  set_source_files_properties(
+    "csrc/quantization/awq/gemm_kernels.cu"
+    PROPERTIES
+    COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128"
+  )
+
   # Only build Marlin kernels if we are building for at least some compatible archs.
   # Keep building Marlin for 9.0 as there are some group sizes and shapes that
   # are not supported by Machete yet.
-  cuda_archs_loose_intersection(MARLIN_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}")
+  # cuda_archs_loose_intersection(MARLIN_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}") # Not support
   if (MARLIN_ARCHS)
     set(MARLIN_SRCS
        "csrc/quantization/fp8/fp8_marlin.cu"
@@ -297,6 +320,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
 
   # The cutlass_scaled_mm kernels for Hopper (c3x, i.e. CUTLASS 3.x) require
   # CUDA 12.0 or later (and only work on Hopper, 9.0a for now).
+  if(0)
   cuda_archs_loose_intersection(SCALED_MM_3X_ARCHS "9.0a" "${CUDA_ARCHS}")
   if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_3X_ARCHS)
     set(SRCS 
@@ -326,6 +350,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     # build any 3x kernels
     set(SCALED_MM_3X_ARCHS)
   endif()
+  endif()
 
   #
   # For the cutlass_scaled_mm kernels we want to build the c2x (CUTLASS 2.x)
@@ -475,14 +500,19 @@ target_compile_definitions(_C PRIVATE CUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1)
 set(VLLM_MOE_EXT_SRC
   "csrc/moe/torch_bindings.cpp"
   "csrc/moe/moe_align_sum_kernels.cu"
-  "csrc/moe/topk_softmax_kernels.cu")
+  "csrc/moe/topk_softmax_kernels.cu"
+  "csrc/moe/moe_ops.cpp")
+
+
+set(MCBLAS_INCLUDE_DIR $ENV{MACA_PATH}/include/mcblas)
+set(MCBLAS_LIB $ENV{MACA_PATH}/lib/libmcblas.so)
 
 set_gencode_flags_for_srcs(
   SRCS "${VLLM_MOE_EXT_SRC}"
   CUDA_ARCHS "${CUDA_ARCHS}")
 
 if(VLLM_GPU_LANG STREQUAL "CUDA")
-  cuda_archs_loose_intersection(MARLIN_MOE_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}")
+  # cuda_archs_loose_intersection(MARLIN_MOE_ARCHS "8.0;8.6;8.7;8.9;9.0" "${CUDA_ARCHS}") # Not support
   if (MARLIN_MOE_ARCHS)
     set(MARLIN_MOE_SRC
         "csrc/moe/marlin_kernels/marlin_moe_kernel.h"
@@ -514,6 +544,8 @@ define_gpu_extension_target(
   SOURCES ${VLLM_MOE_EXT_SRC}
   COMPILE_FLAGS ${VLLM_GPU_FLAGS}
   ARCHITECTURES ${VLLM_GPU_ARCHES}
+  INCLUDE_DIRECTORIES ${MCBLAS_INCLUDE_DIR}
+  LIBRARIES ${MCBLAS_LIB}
   USE_SABI 3
   WITH_SOABI)
 
@@ -567,46 +599,46 @@ endif()
 # This is to enable local development of vllm-flash-attn within vLLM.
 # It can be set as an environment variable or passed as a cmake argument.
 # The environment variable takes precedence.
-if (DEFINED ENV{VLLM_FLASH_ATTN_SRC_DIR})
-  set(VLLM_FLASH_ATTN_SRC_DIR $ENV{VLLM_FLASH_ATTN_SRC_DIR})
-endif()
-
-if(VLLM_FLASH_ATTN_SRC_DIR)
-  FetchContent_Declare(
-          vllm-flash-attn SOURCE_DIR 
-          ${VLLM_FLASH_ATTN_SRC_DIR}
-          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
-  )
-else()
-  FetchContent_Declare(
-          vllm-flash-attn
-          GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git
-          GIT_TAG d4e09037abf588af1ec47d0e966b237ee376876c
-          GIT_PROGRESS TRUE
-          # Don't share the vllm-flash-attn build between build types
-          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
-  )
-endif()
+#if (DEFINED ENV{VLLM_FLASH_ATTN_SRC_DIR})
+#  set(VLLM_FLASH_ATTN_SRC_DIR $ENV{VLLM_FLASH_ATTN_SRC_DIR})
+#endif()
+
+#if(VLLM_FLASH_ATTN_SRC_DIR)
+#  FetchContent_Declare(
+#          vllm-flash-attn SOURCE_DIR 
+#          ${VLLM_FLASH_ATTN_SRC_DIR}
+#          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
+#  )
+#else()
+#  FetchContent_Declare(
+#          vllm-flash-attn
+#          GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git
+#          GIT_TAG d4e09037abf588af1ec47d0e966b237ee376876c
+#          GIT_PROGRESS TRUE
+#          # Don't share the vllm-flash-attn build between build types
+#          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
+#  )
+#endif()
 
 
 # Fetch the vllm-flash-attn library
-FetchContent_MakeAvailable(vllm-flash-attn)
-message(STATUS "vllm-flash-attn is available at ${vllm-flash-attn_SOURCE_DIR}")
+# FetchContent_MakeAvailable(vllm-flash-attn)
+# message(STATUS "vllm-flash-attn is available at ${vllm-flash-attn_SOURCE_DIR}")
 
 # Copy over the vllm-flash-attn python files (duplicated for fa2 and fa3, in
 # case only one is built, in the case both are built redundant work is done)
-install(
-  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
-  DESTINATION vllm_flash_attn
-  COMPONENT _vllm_fa2_C
-  FILES_MATCHING PATTERN "*.py"
-)
-
-install(
-  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
-  DESTINATION vllm_flash_attn
-  COMPONENT _vllm_fa3_C
-  FILES_MATCHING PATTERN "*.py"
-)
+#install(
+#  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
+#  DESTINATION vllm_flash_attn
+#  COMPONENT _vllm_fa2_C
+#  FILES_MATCHING PATTERN "*.py"
+#)
+
+#install(
+#  DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/
+#  DESTINATION vllm_flash_attn
+#  COMPONENT _vllm_fa3_C
+#  FILES_MATCHING PATTERN "*.py"
+#)
 
 # Nothing after vllm-flash-attn, see comment about macros above
diff --git a/NOTICE b/NOTICE
new file mode 100644
index 000000000..aa8efcedb
--- /dev/null
+++ b/NOTICE
@@ -0,0 +1,130 @@
+The following files may have been Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. in 2025. 
+
+CMakeLists.txt
+benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
+benchmarks/kernels/benchmark_moe.py
+cmake/utils.cmake
+csrc/activation_kernels.cu
+csrc/attention/attention_kernels.cuh
+csrc/attention/attention_utils.cuh
+csrc/attention/dtype_float16.cuh
+csrc/attention/paged_attention_v1.cu
+csrc/attention/paged_attention_v2.cu
+csrc/cache.h
+csrc/cache_kernels.cu
+csrc/core/scalar_type.hpp
+csrc/cuda_compat.h
+csrc/custom_all_reduce.cuh
+csrc/cutlass_extensions/common.hpp
+csrc/mamba/causal_conv1d/causal_conv1d.cu
+csrc/mamba/mamba_ssm/selective_scan_fwd.cu
+csrc/moe/moe_align_sum_kernels.cu
+csrc/moe/moe_ops.h
+csrc/ops.h
+csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
+csrc/quantization/awq/awq_4bits.cuh
+csrc/quantization/awq/dequant.cuh
+csrc/quantization/awq/dequantize.cuh
+csrc/quantization/awq/gemm_kernels.cu
+csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
+csrc/quantization/awq/hgemv_selector.hpp
+csrc/quantization/compressed_tensors/int8_quant_kernels.cu
+csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
+csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
+csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
+csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
+csrc/quantization/fp8/common.cu
+csrc/quantization/fp8/nvidia/quant_utils.cuh
+csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
+csrc/quantization/fused_kernels/quant_conversions.cuh
+csrc/quantization/gguf/ggml-common.h
+csrc/quantization/gptq/Hgemm_common.cuh
+csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
+csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
+csrc/quantization/gptq/dequant.cuh
+csrc/quantization/gptq/gptq.cuh
+csrc/quantization/gptq/hgemm_gptq.h
+csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
+csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
+csrc/quantization/gptq/hgemv_selector.hpp
+csrc/quantization/gptq/q_gemm.cu
+csrc/quantization/gptq/qdq_4.cuh
+csrc/quantization/gptq/scalar_type.hpp
+csrc/quantization/vectorization.cuh
+csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
+examples/llm_engine_example_bytemlperf.py
+requirements-cuda.txt
+setup.py
+tests/distributed/test_pynccl.py
+tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
+tests/kernels/test_awq.py
+tests/kernels/test_causal_conv1d.py
+tests/kernels/test_cutlass.py
+tests/kernels/test_fused_quant_layernorm.py
+tests/kernels/test_int8_quant.py
+tests/kernels/test_mamba_ssm.py
+tests/kernels/test_marlin_gemm.py
+tests/kernels/utils.py
+vllm/__init__.py
+vllm/_custom_ops.py
+vllm/attention/backends/flash_attn.py
+vllm/attention/backends/flash_attn_pg.py
+vllm/attention/backends/mla/utils.py
+vllm/attention/backends/triton_mla.py
+vllm/attention/layer.py
+vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
+vllm/attention/ops/blocksparse_attention/interface.py
+vllm/attention/ops/paged_attn.py
+vllm/attention/ops/triton_decode_attention.py
+vllm/config.py
+vllm/distributed/device_communicators/pynccl_wrapper.py
+vllm/distributed/kv_transfer/kv_connector/base.py
+vllm/distributed/kv_transfer/kv_connector/factory.py
+vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
+vllm/distributed/kv_transfer/kv_connector/simple_connector.py
+vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
+vllm/distributed/kv_transfer/kv_transfer_agent.py
+vllm/distributed/kv_transfer/kv_transfer_utils.py
+vllm/distributed/parallel_state.py
+vllm/distributed/utils.py
+vllm/engine/arg_utils.py
+vllm/engine/llm_engine.py
+vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
+vllm/envs.py
+vllm/executor/executor_base.py
+vllm/executor/mp_distributed_executor.py
+vllm/executor/ray_distributed_executor.py
+vllm/model_executor/layers/fused_moe/fused_moe.py
+vllm/model_executor/layers/linear.py
+vllm/model_executor/layers/logits_processor.py
+vllm/model_executor/layers/quantization/__init__.py
+vllm/model_executor/layers/quantization/awq.py
+vllm/model_executor/layers/quantization/base_config.py
+vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
+vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
+vllm/model_executor/layers/quantization/gptq.py
+vllm/model_executor/layers/quantization/moe_wna16.py
+vllm/model_executor/layers/quantization/utils/fp8_utils.py
+vllm/model_executor/layers/vocab_parallel_embedding.py
+vllm/model_executor/model_loader/__init__.py
+vllm/model_executor/model_loader/loader.py
+vllm/model_executor/models/baichuan_moe.py
+vllm/model_executor/models/deepseek.py
+vllm/model_executor/models/deepseek_v2.py
+vllm/model_executor/models/qwen.py
+vllm/model_executor/models/registry.py
+vllm/model_executor/models/telechat.py
+vllm/platforms/__init__.py
+vllm/platforms/cuda.py
+vllm/platforms/pynvml.py
+vllm/triton_utils/custom_cache_manager.py
+vllm/utils.py
+vllm/v1/attention/backends/flash_attn.py
+vllm/v1/core/scheduler.py
+vllm/v1/engine/core.py
+vllm/v1/executor/abstract.py
+vllm/v1/executor/ray_distributed_executor.py
+vllm/version.py
+vllm/worker/model_runner.py
+vllm/worker/multi_step_model_runner.py
+vllm/worker/worker_base.py
diff --git a/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh b/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
old mode 100644
new mode 100755
index eb5d891d0..bd1c1e1b5
--- a/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
+++ b/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
@@ -3,7 +3,7 @@
 # Requirement: 2x GPUs.
 
 
-# Model: meta-llama/Meta-Llama-3.1-8B-Instruct
+# Model: /pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct
 # Query: 1024 input tokens, 6 output tokens, QPS 2/4/6/8, 100 requests
 # Resource: 2x GPU
 # Approaches:
@@ -34,7 +34,7 @@ wait_for_server() {
 
 
 launch_chunked_prefill() {
-  model="meta-llama/Meta-Llama-3.1-8B-Instruct"
+  model="/pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct"
   # disagg prefill
   CUDA_VISIBLE_DEVICES=0 python3 \
     -m vllm.entrypoints.openai.api_server \
@@ -58,7 +58,7 @@ launch_chunked_prefill() {
 
 
 launch_disagg_prefill() {
-  model="meta-llama/Meta-Llama-3.1-8B-Instruct" 
+  model="/pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct" 
   # disagg prefill
   CUDA_VISIBLE_DEVICES=0 python3 \
     -m vllm.entrypoints.openai.api_server \
@@ -66,8 +66,9 @@ launch_disagg_prefill() {
     --port 8100 \
     --max-model-len 10000 \
     --gpu-memory-utilization 0.6 \
+    --enforce-eager \
     --kv-transfer-config \
-    '{"kv_connector":"PyNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
+    '{"kv_connector":"LayerwisePyNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
 
   CUDA_VISIBLE_DEVICES=1 python3 \
     -m vllm.entrypoints.openai.api_server \
@@ -76,7 +77,7 @@ launch_disagg_prefill() {
     --max-model-len 10000 \
     --gpu-memory-utilization 0.6 \
     --kv-transfer-config \
-    '{"kv_connector":"PyNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
+    '{"kv_connector":"LayerwisePyNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2,"kv_buffer_size":5e9}' &
 
   wait_for_server 8100
   wait_for_server 8200
@@ -87,7 +88,7 @@ launch_disagg_prefill() {
 
 benchmark() {
   results_folder="./results"
-  model="meta-llama/Meta-Llama-3.1-8B-Instruct"
+  model="/pde_ai/models/llm/Llama/Meta-Llama-3.1-8B-Instruct"
   dataset_name="sonnet"
   dataset_path="../sonnet_4x.txt"
   num_prompts=100
@@ -143,11 +144,11 @@ main() {
 
   export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')
 
-  launch_chunked_prefill
-  for qps in 2 4 6 8; do
-  benchmark $qps $default_output_len chunked_prefill
-  done
-  kill_gpu_processes
+  #launch_chunked_prefill
+  #for qps in 2 4 6 8; do
+  #benchmark $qps $default_output_len chunked_prefill
+  #done
+  #kill_gpu_processes
 
   launch_disagg_prefill
   for qps in 2 4 6 8; do
diff --git a/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py b/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
index 980e68668..d067a9b5a 100644
--- a/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
+++ b/benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import os
@@ -58,6 +59,34 @@ async def handle_request():
         print(e)
         print("".join(traceback.format_exception(*exc_info)))
 
+@app.route('/metrics', methods=['GET'])
+async def handle_request_metrics():
+    try:
+        original_request_data = await request.get_json()
+
+        generator = forward_request('http://localhost:8100/metrics',
+                                    original_request_data)
+        print("getting prefill metrics...")
+        response = await make_response(generator)
+        #if response.success:
+        print("metrics prefill getted")
+        # return decode
+        generator = forward_request('http://localhost:8200/metrics',
+                                    original_request_data)
+        print("getting decode metrics...")
+        response = await make_response(generator)
+        #if response.success:
+        print("metrics decode getted")
+
+        return response
+
+    except Exception as e:
+        import sys
+        import traceback
+        exc_info = sys.exc_info()
+        print("Error occurred in disagg prefill metrics start proxy server")
+        print(e)
+        print("".join(traceback.format_exception(*exc_info)))
 
 if __name__ == '__main__':
     app.run(port=8000)
diff --git a/benchmarks/kernels/benchmark_moe.py b/benchmarks/kernels/benchmark_moe.py
index a4a45c9cb..990f77291 100644
--- a/benchmarks/kernels/benchmark_moe.py
+++ b/benchmarks/kernels/benchmark_moe.py
@@ -1,560 +1,618 @@
-# SPDX-License-Identifier: Apache-2.0
-
-import argparse
-import time
-from datetime import datetime
-from itertools import product
-from typing import Any, Dict, List, Tuple, TypedDict
-
-import ray
-import torch
-import triton
-from ray.experimental.tqdm_ray import tqdm
-from transformers import AutoConfig
-
-from vllm.model_executor.layers.fused_moe.fused_moe import *
-from vllm.platforms import current_platform
-from vllm.utils import FlexibleArgumentParser
-
-FP8_DTYPE = torch.float8_e4m3fnuz if current_platform.is_rocm(
-) else torch.float8_e4m3fn
-
-
-class BenchmarkConfig(TypedDict):
-    BLOCK_SIZE_M: int
-    BLOCK_SIZE_N: int
-    BLOCK_SIZE_K: int
-    GROUP_SIZE_M: int
-    num_warps: int
-    num_stages: int
-
-
-def benchmark_config(
-    config: BenchmarkConfig,
-    num_tokens: int,
-    num_experts: int,
-    shard_intermediate_size: int,
-    hidden_size: int,
-    topk: int,
-    dtype: torch.dtype,
-    use_fp8_w8a8: bool,
-    use_int8_w8a16: bool,
-    num_iters: int = 100,
-) -> float:
-    init_dtype = torch.float16 if use_fp8_w8a8 else dtype
-    x = torch.randn(num_tokens, hidden_size, dtype=dtype)
-    if use_int8_w8a16:
-        w1 = torch.randint(-127,
-                           127, (
-                               num_experts,
-                               shard_intermediate_size,
-                               hidden_size,
-                           ),
-                           dtype=torch.int8)
-        w2 = torch.randint(-127,
-                           127, (
-                               num_experts,
-                               hidden_size,
-                               shard_intermediate_size // 2,
-                           ),
-                           dtype=torch.int8)
-    else:
-        w1 = torch.randn(num_experts,
-                         shard_intermediate_size,
-                         hidden_size,
-                         dtype=init_dtype)
-        w2 = torch.randn(num_experts,
-                         hidden_size,
-                         shard_intermediate_size // 2,
-                         dtype=init_dtype)
-    gating_output = torch.randn(num_iters,
-                                num_tokens,
-                                num_experts,
-                                dtype=torch.float32)
-
-    w1_scale = None
-    w2_scale = None
-    a1_scale = None
-    a2_scale = None
-    if use_int8_w8a16:
-        w1_scale = torch.randn((num_experts, 2 * shard_intermediate_size),
-                               dtype=torch.float32)
-        w2_scale = torch.randn((hidden_size, num_experts), dtype=torch.float32)
-    if use_fp8_w8a8:
-        w1_scale = torch.randn(num_experts, dtype=torch.float32)
-        w2_scale = torch.randn(num_experts, dtype=torch.float32)
-        a1_scale = torch.randn(1, dtype=torch.float32)
-        a2_scale = torch.randn(1, dtype=torch.float32)
-
-        w1 = w1.to(FP8_DTYPE)
-        w2 = w2.to(FP8_DTYPE)
-
-    input_gating = torch.empty(num_tokens, num_experts, dtype=torch.float32)
-
-    def prepare(i: int):
-        input_gating.copy_(gating_output[i])
-
-    def run():
-        from vllm.model_executor.layers.fused_moe import override_config
-        with override_config(config):
-            fused_moe(
-                x,
-                w1,
-                w2,
-                input_gating,
-                topk,
-                renormalize=True,
-                inplace=True,
-                use_fp8_w8a8=use_fp8_w8a8,
-                use_int8_w8a16=use_int8_w8a16,
-                w1_scale=w1_scale,
-                w2_scale=w2_scale,
-                a1_scale=a1_scale,
-                a2_scale=a2_scale,
-            )
-
-    # JIT compilation & warmup
-    run()
-    torch.cuda.synchronize()
-
-    # Capture 10 invocations with CUDA graph
-    graph = torch.cuda.CUDAGraph()
-    with torch.cuda.graph(graph):
-        for _ in range(10):
-            run()
-    torch.cuda.synchronize()
-
-    # Warmup
-    for _ in range(5):
-        graph.replay()
-    torch.cuda.synchronize()
-
-    start_event = torch.cuda.Event(enable_timing=True)
-    end_event = torch.cuda.Event(enable_timing=True)
-
-    latencies: List[float] = []
-    for i in range(num_iters):
-        prepare(i)
-        torch.cuda.synchronize()
-
-        start_event.record()
-        graph.replay()
-        end_event.record()
-        end_event.synchronize()
-        latencies.append(start_event.elapsed_time(end_event))
-    avg = sum(latencies) / (num_iters * 10) * 1000  # us
-    graph.reset()
-    return avg
-
-
-def get_rocm_tuning_space(use_fp16):
-    block_mn_range = [16, 32, 64, 128, 256]
-    block_k_range = [16, 32, 64, 128, 256]
-    if not use_fp16:
-        block_k_range.remove(16)  # BLOCK_K=16 not supported for fp8
-    num_warps_range = [1, 2, 4, 8]
-    group_m_range = [1, 4, 8, 16, 32]
-    num_stage_range = [2]
-    waves_per_eu_range = [0]
-    matrix_instr_nonkdim_range = [16, 32] if use_fp16 else []
-    kpack_range = [1, 2] if use_fp16 else []
-
-    param_ranges = {
-        "BLOCK_SIZE_M": block_mn_range,
-        "BLOCK_SIZE_N": block_mn_range,
-        "BLOCK_SIZE_K": block_k_range,
-        "GROUP_SIZE_M": group_m_range,
-        "num_warps": num_warps_range,
-        "num_stages": num_stage_range,
-        "waves_per_eu": waves_per_eu_range,
-    }
-    if use_fp16:
-        param_ranges["matrix_instr_nonkdim"] = matrix_instr_nonkdim_range
-        param_ranges["kpack"] = kpack_range
-
-    return param_ranges
-
-
-def get_configs_compute_bound(use_fp16) -> List[Dict[str, int]]:
-    configs: List[BenchmarkConfig] = []
-
-    if current_platform.is_rocm():
-        param_ranges = get_rocm_tuning_space(use_fp16)
-    else:
-        # Reduced search space for faster tuning.
-        # TODO(woosuk): Increase the search space and use a performance model to
-        # prune the search space.
-        block_m_range = [16, 32, 64, 128, 256]
-        block_n_range = [32, 64, 128, 256]
-        block_k_range = [64, 128, 256]
-        num_warps_range = [4, 8]
-        group_m_range = [1, 16, 32, 64]
-        num_stage_range = [2, 3, 4, 5]
-
-        param_ranges = {
-            "BLOCK_SIZE_M": block_m_range,
-            "BLOCK_SIZE_N": block_n_range,
-            "BLOCK_SIZE_K": block_k_range,
-            "GROUP_SIZE_M": group_m_range,
-            "num_warps": num_warps_range,
-            "num_stages": num_stage_range,
-        }
-
-    keys, values = zip(*param_ranges.items())
-    for config_values in product(*values):
-        config = dict(zip(keys, config_values))
-        configs.append(config)
-    return configs
-
-
-def prune_rocm_search_space(num_tokens, shard_intermediate_size, hidden_size,
-                            search_space, is_fp16):
-    N1, K1 = shard_intermediate_size, hidden_size
-    N2, K2 = hidden_size, shard_intermediate_size // 2
-    pruned_space_1 = prune_rocm_configs(num_tokens * 2, N1, K1, search_space,
-                                        is_fp16)
-    pruned_space_2 = prune_rocm_configs(num_tokens * 2, N2, K2, search_space,
-                                        is_fp16)
-    search_space = merge_unique_dicts(pruned_space_1, pruned_space_2)
-    return search_space
-
-
-# The following code is inspired by ROCm/Triton GEMM tuning script:
-# https://github.com/ROCm/triton/blob/triton-mlir/scripts/amd/gemm/tune_gemm.py#L89
-def prune_rocm_configs(M, N, K, configs, is_fp16=True):
-    pruned_configs = []
-    elemBytes_a = 2 if is_fp16 else 1
-    elemBytes_b = 2 if is_fp16 else 1
-
-    mfma = 16 if M < 32 or N < 32 else 32
-
-    # TODO (zhanglx): figure out the boundary between large and small gemms
-    large_gemm = False
-    if M >= 2048 and N >= 2048:
-        large_gemm = True
-
-    for config in configs:
-        BLOCK_SIZE_M = config.get("BLOCK_SIZE_M")
-        BLOCK_SIZE_N = config.get("BLOCK_SIZE_N")
-        BLOCK_SIZE_K = config.get("BLOCK_SIZE_K")
-        num_warps = config.get("num_warps")
-
-        if is_fp16:
-            matrix_instr_nonkdim = config.get("matrix_instr_nonkdim")
-            if matrix_instr_nonkdim > mfma:
-                continue
-        if mfma == 4 and BLOCK_SIZE_K < 64:
-            continue
-        # some layouts could not work properly in case
-        # number elements per thread is less 1
-        if BLOCK_SIZE_M * BLOCK_SIZE_N < 64:
-            continue
-        SPLIT_K = config.get("SPLIT_K", 1)
-        GROUP_M = config.get("GROUP_SIZE_M")
-        if is_fp16:
-            if (matrix_instr_nonkdim > BLOCK_SIZE_M
-                    or matrix_instr_nonkdim > BLOCK_SIZE_N):
-                continue
-            if (matrix_instr_nonkdim >= M
-                    and matrix_instr_nonkdim != BLOCK_SIZE_M):
-                continue
-            if (matrix_instr_nonkdim >= N
-                    and matrix_instr_nonkdim != BLOCK_SIZE_N):
-                continue
-        # Skip BLOCK_SIZE that is too large compare to M/N
-        # unless BLOCK_SIZE is already small enough
-        if M * 2 < BLOCK_SIZE_M and BLOCK_SIZE_M != 16:
-            continue
-        if N * 2 < BLOCK_SIZE_N and BLOCK_SIZE_N != 16:
-            continue
-        # skip large split_k when not necessary
-        if SPLIT_K != 1 and not need_split_k(M, N, K):
-            continue
-        # skip split_k that leads to EVEN_K = false
-        leap = SPLIT_K * BLOCK_SIZE_K
-        modv = K % leap
-        if modv != 0:
-            continue
-        # skip large GROUP_M
-        if GROUP_M * BLOCK_SIZE_M > M and GROUP_M != 1:
-            continue
-        # out of shared memory resource
-        # TODO (zhanglx): This does not consider the LDS usage in the epilogue
-        LDS = (BLOCK_SIZE_K * BLOCK_SIZE_M * elemBytes_a +
-               BLOCK_SIZE_K * BLOCK_SIZE_N * elemBytes_b)
-        if LDS > 65536:
-            continue
-        # Skip small block sizes and num_warps for large gemm
-        # For fp16 and f8, we want to only use BLOCK_SIZE >= 64
-        if large_gemm:
-            if BLOCK_SIZE_M < 64 or BLOCK_SIZE_N < 64:
-                continue
-            if BLOCK_SIZE_K < 64:
-                continue
-            if num_warps < 4:
-                continue
-
-        pruned_configs.append(config)
-
-    return pruned_configs
-
-
-def need_split_k(SIZE_M, SIZE_N, SIZE_K):
-    return (SIZE_M < 64 or SIZE_N < 64) and SIZE_K > 1024
-
-
-def merge_unique_dicts(list1, list2):
-    result = []
-    combined_list = list1.copy()
-    combined_list.extend(list2)
-    for dictionary in combined_list:
-        if dictionary not in result:
-            result.append(dictionary)
-    return result
-
-
-@ray.remote(num_gpus=1)
-class BenchmarkWorker:
-
-    def __init__(self, seed: int) -> None:
-        torch.set_default_device("cuda")
-        current_platform.seed_everything(seed)
-        self.seed = seed
-        # Get the device ID to allocate tensors and kernels
-        # on the respective GPU. This is required for Ray to work
-        # correctly with multi-GPU tuning on the ROCm platform.
-        self.device_id = int(ray.get_gpu_ids()[0])
-
-    def benchmark(
-        self,
-        num_tokens: int,
-        num_experts: int,
-        shard_intermediate_size: int,
-        hidden_size: int,
-        topk: int,
-        dtype: torch.dtype,
-        use_fp8_w8a8: bool,
-        use_int8_w8a16: bool,
-    ) -> Tuple[Dict[str, int], float]:
-        current_platform.seed_everything(self.seed)
-        dtype_str = get_config_dtype_str(dtype,
-                                         use_int8_w8a16=use_int8_w8a16,
-                                         use_fp8_w8a8=use_fp8_w8a8)
-        # NOTE(woosuk): The current naming convention uses w2.shape[2], which
-        # is the intermediate size after silu_and_mul.
-        op_config = get_moe_configs(num_experts, shard_intermediate_size // 2,
-                                    dtype_str)
-        if op_config is None:
-            config = get_default_config(num_tokens,
-                                        num_experts,
-                                        shard_intermediate_size,
-                                        hidden_size,
-                                        topk,
-                                        dtype_str,
-                                        is_marlin=False)
-        else:
-            config = op_config[min(op_config.keys(),
-                                   key=lambda x: abs(x - num_tokens))]
-        kernel_time = benchmark_config(config, num_tokens, num_experts,
-                                       shard_intermediate_size, hidden_size,
-                                       topk, dtype, use_fp8_w8a8,
-                                       use_int8_w8a16)
-        return config, kernel_time
-
-    def tune(
-        self,
-        num_tokens: int,
-        num_experts: int,
-        shard_intermediate_size: int,
-        hidden_size: int,
-        topk: int,
-        dtype: torch.dtype,
-        use_fp8_w8a8: bool,
-        use_int8_w8a16: bool,
-        search_space: List[Dict[str, int]],
-    ) -> Dict[str, int]:
-        best_config = None
-        best_time = float("inf")
-        if current_platform.is_rocm():
-            is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16)
-            search_space = prune_rocm_search_space(num_tokens,
-                                                   shard_intermediate_size,
-                                                   hidden_size, search_space,
-                                                   is_fp16)
-
-        with torch.cuda.device(self.device_id):
-            for config in tqdm(search_space):
-                try:
-                    kernel_time = benchmark_config(config,
-                                                   num_tokens,
-                                                   num_experts,
-                                                   shard_intermediate_size,
-                                                   hidden_size,
-                                                   topk,
-                                                   dtype,
-                                                   use_fp8_w8a8,
-                                                   use_int8_w8a16,
-                                                   num_iters=20)
-                except triton.runtime.autotuner.OutOfResources:
-                    # Some configurations may be invalid and fail to compile.
-                    continue
-
-                if kernel_time < best_time:
-                    best_time = kernel_time
-                    best_config = config
-        now = datetime.now()
-        print(f"{now.ctime()}] Completed tuning for batch_size={num_tokens}")
-        assert best_config is not None
-        return best_config
-
-
-def sort_config(config: BenchmarkConfig) -> BenchmarkConfig:
-    return {
-        "BLOCK_SIZE_M":
-        config["BLOCK_SIZE_M"],
-        "BLOCK_SIZE_N":
-        config["BLOCK_SIZE_N"],
-        "BLOCK_SIZE_K":
-        config["BLOCK_SIZE_K"],
-        "GROUP_SIZE_M":
-        config["GROUP_SIZE_M"],
-        "num_warps":
-        config["num_warps"],
-        "num_stages":
-        config["num_stages"],
-        **({
-            "waves_per_eu": config["waves_per_eu"]
-        } if "waves_per_eu" in config else {}),
-        **({
-            "matrix_instr_nonkdim": config["matrix_instr_nonkdim"]
-        } if "matrix_instr_nonkdim" in config else {}),
-        **({
-            "kpack": config["kpack"]
-        } if "kpack" in config else {}),
-    }
-
-
-def save_configs(configs: Dict[int, BenchmarkConfig], num_experts: int,
-                 shard_intermediate_size: int, hidden_size: int, topk: int,
-                 dtype: torch.dtype, use_fp8_w8a8: bool,
-                 use_int8_w8a16: bool) -> None:
-    dtype_str = get_config_dtype_str(dtype,
-                                     use_int8_w8a16=use_int8_w8a16,
-                                     use_fp8_w8a8=use_fp8_w8a8)
-
-    # NOTE(woosuk): The current naming convention uses w2.shape[2], which
-    # is the intermediate size after silu_and_mul.
-    filename = get_config_file_name(num_experts, shard_intermediate_size // 2,
-                                    dtype_str)
-
-    print(f"Writing best config to {filename}...")
-    with open(filename, "w") as f:
-        json.dump(configs, f, indent=4)
-        f.write("\n")
-
-
-def main(args: argparse.Namespace):
-    print(args)
-
-    config = AutoConfig.from_pretrained(
-        args.model, trust_remote_code=args.trust_remote_code)
-    if config.architectures[0] == "DbrxForCausalLM":
-        E = config.ffn_config.moe_num_experts
-        topk = config.ffn_config.moe_top_k
-        intermediate_size = config.ffn_config.ffn_hidden_size
-        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-    elif config.architectures[0] == "JambaForCausalLM":
-        E = config.num_experts
-        topk = config.num_experts_per_tok
-        intermediate_size = config.intermediate_size
-        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-    elif config.architectures[0] == "DeepseekV3ForCausalLM":
-        E = config.n_routed_experts
-        topk = config.num_experts_per_tok
-        intermediate_size = config.moe_intermediate_size
-        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-    else:
-        # Default: Mixtral.
-        E = config.num_local_experts
-        topk = config.num_experts_per_tok
-        intermediate_size = config.intermediate_size
-        shard_intermediate_size = 2 * intermediate_size // args.tp_size
-
-    hidden_size = config.hidden_size
-    dtype = torch.float16 if current_platform.is_rocm() else config.torch_dtype
-    use_fp8_w8a8 = args.dtype == "fp8_w8a8"
-    use_int8_w8a16 = args.dtype == "int8_w8a16"
-
-    if args.batch_size is None:
-        batch_sizes = [
-            1, 2, 4, 8, 16, 24, 32, 48, 64, 96, 128, 256, 512, 1024, 1536,
-            2048, 3072, 4096
-        ]
-    else:
-        batch_sizes = [args.batch_size]
-
-    ray.init()
-    num_gpus = int(ray.available_resources()["GPU"])
-    workers = [BenchmarkWorker.remote(args.seed) for _ in range(num_gpus)]
-
-    def _distribute(method: str, inputs: List[Any]) -> List[Any]:
-        outputs = []
-        worker_idx = 0
-        for input_args in inputs:
-            worker = workers[worker_idx]
-            worker_method = getattr(worker, method)
-            output = worker_method.remote(*input_args)
-            outputs.append(output)
-            worker_idx = (worker_idx + 1) % num_gpus
-        return ray.get(outputs)
-
-    if args.tune:
-        is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16)
-        search_space = get_configs_compute_bound(is_fp16)
-        print(f"Start tuning over {len(search_space)} configurations...")
-
-        start = time.time()
-        configs = _distribute(
-            "tune", [(batch_size, E, shard_intermediate_size, hidden_size,
-                      topk, dtype, use_fp8_w8a8, use_int8_w8a16, search_space)
-                     for batch_size in batch_sizes])
-        best_configs = {
-            M: sort_config(config)
-            for M, config in zip(batch_sizes, configs)
-        }
-        save_configs(best_configs, E, shard_intermediate_size, hidden_size,
-                     topk, dtype, use_fp8_w8a8, use_int8_w8a16)
-        end = time.time()
-        print(f"Tuning took {end - start:.2f} seconds")
-    else:
-        outputs = _distribute(
-            "benchmark", [(batch_size, E, shard_intermediate_size, hidden_size,
-                           topk, dtype, use_fp8_w8a8, use_int8_w8a16)
-                          for batch_size in batch_sizes])
-
-        for batch_size, (config, kernel_time) in zip(batch_sizes, outputs):
-            print(f"Batch size: {batch_size}, config: {config}")
-            print(f"Kernel time: {kernel_time:.2f} us")
-
-
-if __name__ == "__main__":
-    parser = FlexibleArgumentParser()
-    parser.add_argument("--model",
-                        type=str,
-                        default="mistralai/Mixtral-8x7B-Instruct-v0.1")
-    parser.add_argument("--tp-size",
-                        "-tp",
-                        "--tensor-parallel-size",
-                        type=int,
-                        default=2)
-    parser.add_argument("--dtype",
-                        type=str,
-                        choices=["auto", "fp8_w8a8", "int8_w8a16"],
-                        default="auto")
-    parser.add_argument("--seed", type=int, default=0)
-    parser.add_argument("--batch-size", type=int, required=False)
-    parser.add_argument("--tune", action="store_true")
-    parser.add_argument("--trust-remote-code", action="store_true")
-    args = parser.parse_args()
-
-    main(args)
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# SPDX-License-Identifier: Apache-2.0
+
+import argparse
+import time
+from datetime import datetime
+from itertools import product
+from typing import Any, Dict, List, Tuple, TypedDict
+
+import ray
+import torch
+import triton
+from ray.experimental.tqdm_ray import tqdm
+from transformers import AutoConfig
+
+from vllm.model_executor.layers.fused_moe.fused_moe import *
+from vllm.platforms import current_platform
+from vllm.utils import FlexibleArgumentParser
+
+FP8_DTYPE = torch.float8_e4m3fnuz if current_platform.is_rocm(
+) else torch.float8_e4m3fn
+
+
+class BenchmarkConfig(TypedDict):
+    BLOCK_SIZE_M: int
+    BLOCK_SIZE_N: int
+    BLOCK_SIZE_K: int
+    GROUP_SIZE_M: int
+    num_warps: int
+    num_stages: int
+
+
+def benchmark_config(
+    config: BenchmarkConfig,
+    num_tokens: int,
+    num_experts: int,
+    shard_intermediate_size: int,
+    hidden_size: int,
+    topk: int,
+    dtype: torch.dtype,
+    use_fp8_w8a8: bool,
+    use_int8_w8a16: bool,
+    use_int4_w4a16: bool, 
+    use_int8_w8a8: bool,
+    num_iters: int = 100,
+) -> float:
+    init_dtype = torch.float16 if use_fp8_w8a8 else dtype
+    x = torch.randn(num_tokens, hidden_size, dtype=dtype)
+    if use_int8_w8a16 or use_int8_w8a8:
+        w1 = torch.randint(-127,
+                           127, (
+                               num_experts,
+                               shard_intermediate_size,
+                               hidden_size,
+                           ),
+                           dtype=torch.int8)
+        w2 = torch.randint(-127,
+                           127, (
+                               num_experts,
+                               hidden_size,
+                               shard_intermediate_size // 2,
+                           ),
+                           dtype=torch.int8)
+    elif use_int4_w4a16:
+        w1 = torch.randint(-127,
+                           127, (
+                               num_experts,
+                               shard_intermediate_size,
+                               hidden_size//2,
+                           ),
+                           dtype=torch.int8)
+        w2 = torch.randint(-127,
+                           127, (
+                               num_experts,
+                               hidden_size,
+                               shard_intermediate_size // 4,
+                           ),
+                           dtype=torch.int8)
+
+    else:
+        w1 = torch.randn(num_experts,
+                         shard_intermediate_size,
+                         hidden_size,
+                         dtype=init_dtype)
+        w2 = torch.randn(num_experts,
+                         hidden_size,
+                         shard_intermediate_size // 2,
+                         dtype=init_dtype)
+    gating_output = torch.randn(num_iters,
+                                num_tokens,
+                                num_experts,
+                                dtype=torch.float32)
+
+    w1_scale = None
+    w2_scale = None
+    a1_scale = None
+    a2_scale = None
+    w1_zp = None
+    w2_zp = None
+    block_shape = None
+
+    if use_int8_w8a16 :
+        w1_scale = torch.randn((num_experts, 2 * shard_intermediate_size),
+                               dtype=torch.float32)
+        w2_scale = torch.randn((hidden_size, num_experts), dtype=torch.float32)
+
+    if use_fp8_w8a8:
+        w1_scale = torch.randn(num_experts, dtype=torch.float32)
+        w2_scale = torch.randn(num_experts, dtype=torch.float32)
+        a1_scale = torch.randn(1, dtype=torch.float32)
+        a2_scale = torch.randn(1, dtype=torch.float32)
+
+        w1 = w1.to(FP8_DTYPE)
+        w2 = w2.to(FP8_DTYPE)
+
+    if use_int4_w4a16:
+        w1_scale = torch.randn(num_experts, shard_intermediate_size, hidden_size // 128, dtype=torch.float16)
+        w2_scale = torch.randn((num_experts,hidden_size,shard_intermediate_size // 256,), dtype=torch.float16)
+        w1_zp = torch.randint(-127,127, (num_experts, shard_intermediate_size//2, hidden_size // 128),dtype=torch.int8)
+        w2_zp = torch.randint(-127,127, (num_experts,hidden_size//2,shard_intermediate_size // 256,),dtype=torch.int8)
+        block_shape=[0, 128]
+
+    if use_int8_w8a8:
+        w1_scale = torch.randn((num_experts, shard_intermediate_size, 1),dtype=torch.float32) 
+        w2_scale = torch.randn((num_experts, hidden_size,  1), dtype=torch.float32) 
+        
+    input_gating = torch.empty(num_tokens, num_experts, dtype=torch.float32)
+
+    def prepare(i: int):
+        input_gating.copy_(gating_output[i])
+
+    def run():
+        from vllm.model_executor.layers.fused_moe import override_config
+        with override_config(config):
+            fused_moe(
+                x,
+                w1,
+                w2,
+                input_gating,
+                topk,
+                renormalize=True,
+                inplace=True,
+                use_fp8_w8a8=use_fp8_w8a8,
+                use_int8_w8a16=use_int8_w8a16,
+                use_int4_w4a16=use_int4_w4a16, 
+                use_int8_w8a8=use_int8_w8a8,
+                w1_scale=w1_scale,
+                w2_scale=w2_scale,
+                a1_scale=a1_scale,
+                a2_scale=a2_scale,
+                w1_zp=w1_zp, 
+                w2_zp=w2_zp, 
+                block_shape=block_shape
+            )
+
+    # JIT compilation & warmup
+    run()
+    torch.cuda.synchronize()
+
+    # Capture 10 invocations with CUDA graph
+    graph = torch.cuda.CUDAGraph()
+    with torch.cuda.graph(graph):
+        for _ in range(10):
+            run()
+    torch.cuda.synchronize()
+    
+
+    # Warmup
+    for _ in range(5):
+        graph.replay()
+    torch.cuda.synchronize()
+
+    start_event = torch.cuda.Event(enable_timing=True)
+    end_event = torch.cuda.Event(enable_timing=True)
+
+    latencies: List[float] = []
+    for i in range(num_iters):
+        prepare(i)
+        torch.cuda.synchronize()
+
+        start_event.record()
+        graph.replay()
+        end_event.record()
+        end_event.synchronize()
+        latencies.append(start_event.elapsed_time(end_event))
+    avg = sum(latencies) / (num_iters * 10) * 1000  # us
+    # graph.reset()
+    return avg
+
+
+def get_rocm_tuning_space(use_fp16):
+    block_mn_range = [16, 32, 64, 128, 256]
+    block_k_range = [16, 32, 64, 128, 256]
+    if not use_fp16:
+        block_k_range.remove(16)  # BLOCK_K=16 not supported for fp8
+    num_warps_range = [1, 2, 4, 8]
+    group_m_range = [1, 4, 8, 16, 32]
+    num_stage_range = [2]
+    waves_per_eu_range = [0]
+    matrix_instr_nonkdim_range = [16, 32] if use_fp16 else []
+    kpack_range = [1, 2] if use_fp16 else []
+
+    param_ranges = {
+        "BLOCK_SIZE_M": block_mn_range,
+        "BLOCK_SIZE_N": block_mn_range,
+        "BLOCK_SIZE_K": block_k_range,
+        "GROUP_SIZE_M": group_m_range,
+        "num_warps": num_warps_range,
+        "num_stages": num_stage_range,
+        "waves_per_eu": waves_per_eu_range,
+    }
+    if use_fp16:
+        param_ranges["matrix_instr_nonkdim"] = matrix_instr_nonkdim_range
+        param_ranges["kpack"] = kpack_range
+
+    return param_ranges
+
+
+def get_configs_compute_bound(use_fp16) -> List[Dict[str, int]]:
+    configs: List[BenchmarkConfig] = []
+
+    if current_platform.is_rocm():
+        param_ranges = get_rocm_tuning_space(use_fp16)
+    else:
+        # Reduced search space for faster tuning.
+        # TODO(woosuk): Increase the search space and use a performance model to
+        # prune the search space.
+        block_m_range = [16, 32, 64, 128, 256]
+        block_n_range = [32, 64, 128, 256]
+        block_k_range = [64, 128, 256]
+        num_warps_range = [4, 8]
+        group_m_range = [1, 16, 32, 64]
+        num_stage_range = [2, 3, 4, 5]
+
+        param_ranges = {
+            "BLOCK_SIZE_M": block_m_range,
+            "BLOCK_SIZE_N": block_n_range,
+            "BLOCK_SIZE_K": block_k_range,
+            "GROUP_SIZE_M": group_m_range,
+            "num_warps": num_warps_range,
+            "num_stages": num_stage_range,
+        }
+
+    keys, values = zip(*param_ranges.items())
+    for config_values in product(*values):
+        config = dict(zip(keys, config_values))
+        configs.append(config)
+    return configs
+
+
+def prune_rocm_search_space(num_tokens, shard_intermediate_size, hidden_size,
+                            search_space, is_fp16):
+    N1, K1 = shard_intermediate_size, hidden_size
+    N2, K2 = hidden_size, shard_intermediate_size // 2
+    pruned_space_1 = prune_rocm_configs(num_tokens * 2, N1, K1, search_space,
+                                        is_fp16)
+    pruned_space_2 = prune_rocm_configs(num_tokens * 2, N2, K2, search_space,
+                                        is_fp16)
+    search_space = merge_unique_dicts(pruned_space_1, pruned_space_2)
+    return search_space
+
+
+# The following code is inspired by ROCm/Triton GEMM tuning script:
+# https://github.com/ROCm/triton/blob/triton-mlir/scripts/amd/gemm/tune_gemm.py#L89
+def prune_rocm_configs(M, N, K, configs, is_fp16=True):
+    pruned_configs = []
+    elemBytes_a = 2 if is_fp16 else 1
+    elemBytes_b = 2 if is_fp16 else 1
+
+    mfma = 16 if M < 32 or N < 32 else 32
+
+    # TODO (zhanglx): figure out the boundary between large and small gemms
+    large_gemm = False
+    if M >= 2048 and N >= 2048:
+        large_gemm = True
+
+    for config in configs:
+        BLOCK_SIZE_M = config.get("BLOCK_SIZE_M")
+        BLOCK_SIZE_N = config.get("BLOCK_SIZE_N")
+        BLOCK_SIZE_K = config.get("BLOCK_SIZE_K")
+        num_warps = config.get("num_warps")
+
+        if is_fp16:
+            matrix_instr_nonkdim = config.get("matrix_instr_nonkdim")
+            if matrix_instr_nonkdim > mfma:
+                continue
+        if mfma == 4 and BLOCK_SIZE_K < 64:
+            continue
+        # some layouts could not work properly in case
+        # number elements per thread is less 1
+        if BLOCK_SIZE_M * BLOCK_SIZE_N < 64:
+            continue
+        SPLIT_K = config.get("SPLIT_K", 1)
+        GROUP_M = config.get("GROUP_SIZE_M")
+        if is_fp16:
+            if (matrix_instr_nonkdim > BLOCK_SIZE_M
+                    or matrix_instr_nonkdim > BLOCK_SIZE_N):
+                continue
+            if (matrix_instr_nonkdim >= M
+                    and matrix_instr_nonkdim != BLOCK_SIZE_M):
+                continue
+            if (matrix_instr_nonkdim >= N
+                    and matrix_instr_nonkdim != BLOCK_SIZE_N):
+                continue
+        # Skip BLOCK_SIZE that is too large compare to M/N
+        # unless BLOCK_SIZE is already small enough
+        if M * 2 < BLOCK_SIZE_M and BLOCK_SIZE_M != 16:
+            continue
+        if N * 2 < BLOCK_SIZE_N and BLOCK_SIZE_N != 16:
+            continue
+        # skip large split_k when not necessary
+        if SPLIT_K != 1 and not need_split_k(M, N, K):
+            continue
+        # skip split_k that leads to EVEN_K = false
+        leap = SPLIT_K * BLOCK_SIZE_K
+        modv = K % leap
+        if modv != 0:
+            continue
+        # skip large GROUP_M
+        if GROUP_M * BLOCK_SIZE_M > M and GROUP_M != 1:
+            continue
+        # out of shared memory resource
+        # TODO (zhanglx): This does not consider the LDS usage in the epilogue
+        LDS = (BLOCK_SIZE_K * BLOCK_SIZE_M * elemBytes_a +
+               BLOCK_SIZE_K * BLOCK_SIZE_N * elemBytes_b)
+        if LDS > 65536:
+            continue
+        # Skip small block sizes and num_warps for large gemm
+        # For fp16 and f8, we want to only use BLOCK_SIZE >= 64
+        if large_gemm:
+            if BLOCK_SIZE_M < 64 or BLOCK_SIZE_N < 64:
+                continue
+            if BLOCK_SIZE_K < 64:
+                continue
+            if num_warps < 4:
+                continue
+
+        pruned_configs.append(config)
+
+    return pruned_configs
+
+
+def need_split_k(SIZE_M, SIZE_N, SIZE_K):
+    return (SIZE_M < 64 or SIZE_N < 64) and SIZE_K > 1024
+
+
+def merge_unique_dicts(list1, list2):
+    result = []
+    combined_list = list1.copy()
+    combined_list.extend(list2)
+    for dictionary in combined_list:
+        if dictionary not in result:
+            result.append(dictionary)
+    return result
+
+
+@ray.remote(num_gpus=1)
+class BenchmarkWorker:
+
+    def __init__(self, seed: int) -> None:
+        torch.set_default_device("cuda")
+        current_platform.seed_everything(seed)
+        self.seed = seed
+        # Get the device ID to allocate tensors and kernels
+        # on the respective GPU. This is required for Ray to work
+        # correctly with multi-GPU tuning on the ROCm platform.
+        self.device_id = int(ray.get_gpu_ids()[0])
+
+    def benchmark(
+        self,
+        num_tokens: int,
+        num_experts: int,
+        shard_intermediate_size: int,
+        hidden_size: int,
+        topk: int,
+        dtype: torch.dtype,
+        use_fp8_w8a8: bool,
+        use_int8_w8a16: bool,
+    ) -> Tuple[Dict[str, int], float]:
+        current_platform.seed_everything(self.seed)
+        dtype_str = get_config_dtype_str(dtype,
+                                         use_int8_w8a16=use_int8_w8a16,
+                                         use_fp8_w8a8=use_fp8_w8a8)
+        # NOTE(woosuk): The current naming convention uses w2.shape[2], which
+        # is the intermediate size after silu_and_mul.
+        op_config = get_moe_configs(num_experts, shard_intermediate_size // 2,
+                                    dtype_str)
+        if op_config is None:
+            config = get_default_config(num_tokens,
+                                        num_experts,
+                                        shard_intermediate_size,
+                                        hidden_size,
+                                        topk,
+                                        dtype_str,
+                                        is_marlin=False)
+        else:
+            config = op_config[min(op_config.keys(),
+                                   key=lambda x: abs(x - num_tokens))]
+        kernel_time = benchmark_config(config, num_tokens, num_experts,
+                                       shard_intermediate_size, hidden_size,
+                                       topk, dtype, use_fp8_w8a8,
+                                       use_int8_w8a16)
+        return config, kernel_time
+
+    def tune(
+        self,
+        num_tokens: int,
+        num_experts: int,
+        shard_intermediate_size: int,
+        hidden_size: int,
+        topk: int,
+        dtype: torch.dtype,
+        use_fp8_w8a8: bool,
+        use_int8_w8a16: bool,
+        use_int4_w4a16: bool, 
+        use_int8_w8a8: bool,
+        search_space: List[Dict[str, int]],
+    ) -> Dict[str, int]:
+        best_config = None
+        best_time = float("inf")
+        if current_platform.is_rocm():
+            is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16)
+            search_space = prune_rocm_search_space(num_tokens,
+                                                   shard_intermediate_size,
+                                                   hidden_size, search_space,
+                                                   is_fp16)
+        with torch.cuda.device(self.device_id):
+            for config in tqdm(search_space):
+                try:
+                    kernel_time = benchmark_config(config,
+                                                   num_tokens,
+                                                   num_experts,
+                                                   shard_intermediate_size,
+                                                   hidden_size,
+                                                   topk,
+                                                   dtype,
+                                                   use_fp8_w8a8,
+                                                   use_int8_w8a16,
+                                                   use_int4_w4a16, 
+                                                   use_int8_w8a8,
+                                                   num_iters=20)
+                except triton.runtime.autotuner.OutOfResources:
+                    # Some configurations may be invalid and fail to compile.
+                    continue
+
+                if kernel_time < best_time:
+                    best_time = kernel_time
+                    best_config = config
+        now = datetime.now()
+        print(f"{now.ctime()}] Completed tuning for batch_size={num_tokens}")
+        assert best_config is not None
+        return best_config
+
+
+def sort_config(config: BenchmarkConfig) -> BenchmarkConfig:
+    return {
+        "BLOCK_SIZE_M":
+        config["BLOCK_SIZE_M"],
+        "BLOCK_SIZE_N":
+        config["BLOCK_SIZE_N"],
+        "BLOCK_SIZE_K":
+        config["BLOCK_SIZE_K"],
+        "GROUP_SIZE_M":
+        config["GROUP_SIZE_M"],
+        "num_warps":
+        config["num_warps"],
+        "num_stages":
+        config["num_stages"],
+        **({
+            "waves_per_eu": config["waves_per_eu"]
+        } if "waves_per_eu" in config else {}),
+        **({
+            "matrix_instr_nonkdim": config["matrix_instr_nonkdim"]
+        } if "matrix_instr_nonkdim" in config else {}),
+        **({
+            "kpack": config["kpack"]
+        } if "kpack" in config else {}),
+    }
+
+
+def save_configs(configs: Dict[int, BenchmarkConfig], num_experts: int,
+                 shard_intermediate_size: int, hidden_size: int, topk: int,
+                 dtype: torch.dtype, use_fp8_w8a8: bool,
+                 use_int8_w8a16: bool, use_int4_w4a16: bool, use_int8_w8a8: bool) -> None:
+    dtype_str = get_config_dtype_str(dtype,
+                                     use_int8_w8a16=use_int8_w8a16,
+                                     use_fp8_w8a8=use_fp8_w8a8, 
+                                     use_int4_w4a16 = use_int4_w4a16,
+                                     use_int8_w8a8 = use_int8_w8a8)
+
+    # NOTE(woosuk): The current naming convention uses w2.shape[2], which
+    # is the intermediate size after silu_and_mul.
+    filename = get_config_file_name(num_experts, shard_intermediate_size // 2,
+                                    dtype_str)
+
+    print(f"Writing best config to {filename}...")
+    with open(filename, "w") as f:
+        json.dump(configs, f, indent=4)
+        f.write("\n")
+
+
+def main(args: argparse.Namespace):
+    print(args)
+
+    config = AutoConfig.from_pretrained(
+        args.model, trust_remote_code=args.trust_remote_code)
+    if config.architectures[0] == "DbrxForCausalLM":
+        E = config.ffn_config.moe_num_experts
+        topk = config.ffn_config.moe_top_k
+        intermediate_size = config.ffn_config.ffn_hidden_size
+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
+    elif config.architectures[0] == "JambaForCausalLM":
+        E = config.num_experts
+        topk = config.num_experts_per_tok
+        intermediate_size = config.intermediate_size
+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
+    elif config.architectures[0] == "DeepseekV3ForCausalLM":
+        E = config.n_routed_experts
+        topk = config.num_experts_per_tok
+        intermediate_size = config.moe_intermediate_size
+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
+    elif config.architectures[0] in [
+            "Qwen2MoeForCausalLM", "Qwen3MoeForCausalLM"
+    ]:
+        E = config.num_experts
+        topk = config.num_experts_per_tok
+        intermediate_size = config.moe_intermediate_size
+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
+ 
+    else:
+        # Default: Mixtral.
+        E = config.num_local_experts
+        topk = config.num_experts_per_tok
+        intermediate_size = config.intermediate_size
+        shard_intermediate_size = 2 * intermediate_size // args.tp_size
+
+    hidden_size = config.hidden_size
+    dtype = torch.float16 if current_platform.is_rocm() else config.torch_dtype
+    use_fp8_w8a8 = args.dtype == "fp8_w8a8"
+    use_int8_w8a16 = args.dtype == "int8_w8a16"
+    use_int4_w4a16 = args.dtype == "int4_w4a16"
+    use_int8_w8a8 = args.dtype == "int8_w8a8"
+
+    if args.batch_size is None:
+        batch_sizes = [
+            1, 2, 4, 8, 16, 24, 32, 48, 64, 96, 128, 256, 512, 1024, 1536,
+            2048, 3072, 4096
+        ]
+    else:
+        batch_sizes = [args.batch_size]
+
+    ray.init()
+    num_gpus = int(ray.available_resources()["GPU"])
+    workers = [BenchmarkWorker.remote(args.seed) for _ in range(num_gpus)]
+
+    def _distribute(method: str, inputs: List[Any]) -> List[Any]:
+        outputs = []
+        worker_idx = 0
+        for input_args in inputs:
+            worker = workers[worker_idx]
+            worker_method = getattr(worker, method)
+            output = worker_method.remote(*input_args)
+            outputs.append(output)
+            worker_idx = (worker_idx + 1) % num_gpus
+        return ray.get(outputs)
+
+    if args.tune:
+        is_fp16 = not (use_fp8_w8a8 or use_int8_w8a16 or use_int4_w4a16)
+        search_space = get_configs_compute_bound(is_fp16)
+        print(f"Start tuning over {len(search_space)} configurations...")
+
+        start = time.time()
+        configs = _distribute(
+            "tune", [(batch_size, E, shard_intermediate_size, hidden_size,
+                      topk, dtype, use_fp8_w8a8, use_int8_w8a16, use_int4_w4a16, use_int8_w8a8, search_space)
+                     for batch_size in batch_sizes])
+        best_configs = {
+            M: sort_config(config)
+            for M, config in zip(batch_sizes, configs)
+        }
+        save_configs(best_configs, E, shard_intermediate_size, hidden_size,
+                     topk, dtype, use_fp8_w8a8, use_int8_w8a16, use_int4_w4a16, use_int8_w8a8)
+        end = time.time()
+        print(f"Tuning took {end - start:.2f} seconds")
+    else:
+        outputs = _distribute(
+            "benchmark", [(batch_size, E, shard_intermediate_size, hidden_size,
+                           topk, dtype, use_fp8_w8a8, use_int8_w8a16)
+                          for batch_size in batch_sizes])
+
+        for batch_size, (config, kernel_time) in zip(batch_sizes, outputs):
+            print(f"Batch size: {batch_size}, config: {config}")
+            print(f"Kernel time: {kernel_time:.2f} us")
+
+
+if __name__ == "__main__":
+    parser = FlexibleArgumentParser()
+    parser.add_argument("--model",
+                        type=str,
+                        default="mistralai/Mixtral-8x7B-Instruct-v0.1")
+    parser.add_argument("--tp-size",
+                        "-tp",
+                        "--tensor-parallel-size",
+                        type=int,
+                        default=2)
+    parser.add_argument("--dtype",
+                        type=str,
+                        choices=["auto", "fp8_w8a8", "int8_w8a16", "int4_w4a16", "int8_w8a8"],
+                        default="auto")
+    parser.add_argument("--seed", type=int, default=0)
+
+    parser.add_argument("--batch-size", type=int, required=False)
+    parser.add_argument("--tune", action="store_true")
+    parser.add_argument("--trust-remote-code", action="store_true")
+    args = parser.parse_args()
+
+    main(args)
+
diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index 1c1c53981..724930ab1 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -94,9 +94,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
     #
     # Get common NVCC flags from torch.
     #
-    run_python(GPU_FLAGS
-      "from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))"
-      "Failed to determine torch nvcc compiler flags")
+    set(GPU_FLAGS "-D__CUDA_NO_HALF_OPERATORS__;-D__CUDA_NO_HALF_CONVERSIONS__;-D__CUDA_NO_HALF2_OPERATORS__;--expt-relaxed-constexpr")
+    list(APPEND GPU_FLAGS -mllvm -metaxgpu-GridDim-UseLdu)
+    #run_python(GPU_FLAGS
+    #  "from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))"
+    #  "Failed to determine torch nvcc compiler flags")
 
     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)
       list(APPEND GPU_FLAGS "-DENABLE_FP8")
@@ -105,7 +107,6 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
       list(REMOVE_ITEM GPU_FLAGS
         "-D__CUDA_NO_HALF_OPERATORS__"
         "-D__CUDA_NO_HALF_CONVERSIONS__"
-        "-D__CUDA_NO_BFLOAT16_CONVERSIONS__"
         "-D__CUDA_NO_HALF2_OPERATORS__")
     endif()
 
@@ -148,6 +149,15 @@ macro(clear_cuda_arches CUDA_ARCH_FLAGS)
     string(REGEX MATCHALL "-gencode arch=[^ ]+" CUDA_ARCH_FLAGS
       ${CMAKE_CUDA_FLAGS})
 
+    # change the config's value of metaxgpu-disable-bsm-offset
+    string(REPLACE "-metaxgpu-disable-bsm-offset=1" "-metaxgpu-disable-bsm-offset=0"
+            CMAKE_CUDA_FLAGS ${CMAKE_CUDA_FLAGS})
+
+    # support opt of gptq-marlin
+    # string(APPEND CMAKE_CUDA_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128")
+    # opt of cutlass w8a8
+    string(APPEND CMAKE_CUDA_FLAGS " -mllvm -structurizecfg-skip-uniform-regions=true")
+
     # Remove all `-gencode` flags from `CMAKE_CUDA_FLAGS` since they will be modified
     # and passed back via the `CUDA_ARCHITECTURES` property.
     string(REGEX REPLACE "-gencode arch=[^ ]+ *" "" CMAKE_CUDA_FLAGS
diff --git a/csrc/activation_kernels.cu b/csrc/activation_kernels.cu
index 88275dbdd..fc416fb78 100644
--- a/csrc/activation_kernels.cu
+++ b/csrc/activation_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <ATen/cuda/CUDAContext.h>
 #include <torch/all.h>
 #include <c10/cuda/CUDAGuard.h>
@@ -31,10 +32,120 @@ __global__ void act_and_mul_kernel(
   }
 }
 
+template<typename scalar_t, typename VT, int N, scalar_t (*ACT_FN)(const scalar_t&),
+        bool act_first>
+__global__ void act_and_mul_kernel_bd_opt(
+  scalar_t* __restrict__ out,               // [..., d]
+  const scalar_t* __restrict__ input,       // [..., 2, d]
+  const int d,
+  const int blockDim_x
+){
+    const int64_t token_idx = blockIdx.y;
+    int x_offset = (blockIdx.x * blockDim_x + threadIdx.x) * N;
+    if(x_offset >= d) return;
+    int64_t offset0 = token_idx * d;
+    int64_t offset1 = offset0 << 1;
+    const scalar_t* ptr_input = input + offset1;
+    const scalar_t* ptr_input0 = ptr_input + x_offset;
+    const scalar_t* ptr_input1 = ptr_input0 + d;
+    scalar_t* ptr_output = out + offset0 + x_offset;
+    VT vsrc0 = *(VT*)(ptr_input0);
+    VT vsrc1 = *(VT*)(ptr_input1);
+    VT vdst;
+    scalar_t* ptr_src0 = (scalar_t*)&vsrc0;
+    scalar_t* ptr_src1 = (scalar_t*)&vsrc1;
+    scalar_t* ptr_dst = (scalar_t*)&vdst;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+        ptr_dst[i] = compute<scalar_t, ACT_FN, act_first>(ptr_src0[i], ptr_src1[i]);
+    }
+    *(VT*)(ptr_output) = vdst;
+}
+
+template<typename scalar_t, typename VT, int N, scalar_t (*ACT_FN)(const scalar_t&),
+        bool act_first>
+__global__ void act_and_mul_kernel_sd_opt(
+  scalar_t* __restrict__ out,               // [..., d]
+  const scalar_t* __restrict__ input,       // [..., 2, d]
+  const int d,
+  const int blockDim_x,
+  const int gridDim_x,
+  const int token_per_block,
+  const int max_token_num) {
+    __shared__ int8_t sm_buffer[16384];
+    int token_offset = blockIdx.x * token_per_block;
+    int out_offset = token_offset * d;
+    int in_offset = out_offset << 1;
+    int num_token = min(max_token_num - token_offset, token_per_block);
+    if(num_token <= 0) return;
+    const scalar_t *ptr_block_input = input + in_offset;
+    scalar_t* ptr_block_output = out + out_offset;
+    int output_size = num_token * d;
+    int input_size = output_size << 1;
+    
+    scalar_t* ptr_sm_buffer = (scalar_t*)sm_buffer;
+    int stride = blockDim_x * N;
+    for(int i = threadIdx.x*N; i < input_size; i += stride) {
+        *(VT*)(ptr_sm_buffer + i) = *(VT*)(ptr_block_input + i);
+    }
+    __syncthreads();
+    for(int i = threadIdx.x; i < output_size; i += blockDim_x) {
+      int token_id = i / d;
+      int x_offset = i % d;
+      scalar_t *ptr_input0 = ptr_sm_buffer + token_id * d * 2 + x_offset;
+      scalar_t *ptr_input1 = ptr_input0 + d;
+      *(ptr_block_output + i) = compute<scalar_t, ACT_FN, act_first>(*ptr_input0, ptr_input1[0]);
+    }
+}
+
+template<typename scalar_t, typename VT, int N, scalar_t (*ACT_FN)(const scalar_t&),
+        bool act_first>
+__global__ void act_and_mul_kernel_sd_fast_opt(
+  scalar_t* __restrict__ out,               // [..., d]
+  const scalar_t* __restrict__ input,       // [..., 2, d]
+  const int d,
+  const int blockDim_x,
+  const int gridDim_x,
+  const int token_per_block,
+  const int max_token_num) {
+    __shared__ int8_t sm_buffer[16384];
+    int token_offset = blockIdx.x * token_per_block;
+    int out_offset = token_offset * d;
+    int in_offset = out_offset << 1;
+    int num_token = min(max_token_num - token_offset, token_per_block);
+    if(num_token <= 0) return;
+    const scalar_t *ptr_block_input = input + in_offset;
+    scalar_t* ptr_block_output = out + out_offset;
+    int output_size = num_token * d;
+    int input_size = output_size << 1;
+    
+    scalar_t* ptr_sm_buffer = (scalar_t*)sm_buffer;
+    int stride = blockDim_x * N;
+    for(int i = threadIdx.x*N; i < input_size; i += stride) {
+        *(VT*)(ptr_sm_buffer + i) = *(VT*)(ptr_block_input + i);
+    }
+    __syncthreads();
+    for(int i = threadIdx.x*N; i < output_size; i += stride) {
+      int token_id = i / d;
+      int x_offset = i % d;
+      scalar_t *ptr_input0 = ptr_sm_buffer + token_id * d * 2 + x_offset;
+      scalar_t *ptr_input1 = ptr_input0 + d;
+      VT vdst;
+      scalar_t *ptr_dst = (scalar_t*)&vdst;
+      #pragma unroll N
+      for(int j = 0; j < N; j++) {
+        ptr_dst[j] = compute<scalar_t, ACT_FN, act_first>(ptr_input0[j], ptr_input1[j]);
+      }
+      *(VT*)(ptr_block_output + i) = vdst;
+    }
+}
+
 template <typename T>
 __device__ __forceinline__ T silu_kernel(const T& x) {
   // x * sigmoid(x)
-  return (T)(((float)x) / (1.0f + expf((float)-x)));
+  // return (T)(((float)x) / (1.0f + expf((float)-x)));
+  float x_f = (float)x;
+  return (T) ((x_f) / (1.0f + __builtin_expf(-x_f)));
 }
 
 template <typename T>
@@ -68,16 +179,68 @@ __device__ __forceinline__ T gelu_tanh_kernel(const T& x) {
 #define LAUNCH_ACTIVATION_GATE_KERNEL(KERNEL, ACT_FIRST)                 \
   int d = input.size(-1) / 2;                                            \
   int64_t num_tokens = input.numel() / input.size(-1);                   \
+  int n = 16 / input.element_size();                                                          \
+  if(((d&(n - 1)) == 0) && d >= 512 * n) {\
+    int blocksize = 512;                                                                  \
+    dim3 gridsize((d + 512*n - 1) / (512*n), num_tokens,1);                               \
+    const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                     \
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                         \
+    VLLM_DISPATCH_FLOATING_TYPES(                                                         \
+      input.scalar_type(),                                                                \
+      "act_and_mul_kernel_bd_opt",                                                               \
+      [&] {                                                                               \
+        vllm::act_and_mul_kernel_bd_opt<scalar_t, float4, 16 / sizeof(scalar_t), KERNEL<scalar_t>, ACT_FIRST><<<gridsize, blocksize, 0, stream>>>(   \
+          out.data_ptr<scalar_t>(),                                                         \
+          input.data_ptr<scalar_t>(),                                                       \
+          d, blocksize);                                                                    \
+      });                                                                                   \
+  } else if(d < 512 && (d & (n - 1)) == 0) {                                                \
+        int block_token = 16384 / input.element_size() / 2 / d;                             \
+        block_token = block_token / n * n;                                                  \
+        int blocksize = 512;                                                                \
+        int gridsize = (num_tokens + block_token - 1) / block_token;                        \
+        const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                   \
+        const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                       \
+        VLLM_DISPATCH_FLOATING_TYPES(                                                       \
+        input.scalar_type(),                                                                \
+        "act_and_mul_kernel_sd_fast_opt",                                                   \
+        [&] {                                                                               \
+        vllm::act_and_mul_kernel_sd_fast_opt<scalar_t, float4, 16 / sizeof(scalar_t), KERNEL<scalar_t>, ACT_FIRST><<<gridsize, blocksize, 0, stream>>>(   \
+        out.data_ptr<scalar_t>(),                                                       \
+        input.data_ptr<scalar_t>(),                                                     \
+        d, blocksize,gridsize,block_token,num_tokens);                                  \
+        });                                                                                 \
+  } else if(d < 512) {                                                                      \
+        int block_token = 16384 / input.element_size() / 2 / d;                             \
+        block_token = block_token / n * n;                                                  \
+        int blocksize = 512;                                                                \
+        int gridsize = (num_tokens + block_token - 1) / block_token;                        \
+        const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                   \
+        const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                       \
+        VLLM_DISPATCH_FLOATING_TYPES(                                                       \
+        input.scalar_type(),                                                                \
+        "act_and_mul_kernel_sd_opt",                                                        \
+        [&] {                                                                               \
+          vllm::act_and_mul_kernel_sd_opt<scalar_t, float4, 16 / sizeof(scalar_t), KERNEL<scalar_t>, ACT_FIRST><<<gridsize, blocksize, 0, stream>>>(   \
+            out.data_ptr<scalar_t>(),                                                       \
+            input.data_ptr<scalar_t>(),                                                     \
+            d, blocksize,gridsize,block_token,num_tokens);                                  \
+        });                                                                                 \
+  } else {                                                                                  \
   dim3 grid(num_tokens);                                                 \
   dim3 block(std::min(d, 1024));                                         \
   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));      \
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();          \
   VLLM_DISPATCH_FLOATING_TYPES(                                          \
-      input.scalar_type(), "act_and_mul_kernel", [&] {                   \
-        vllm::act_and_mul_kernel<scalar_t, KERNEL<scalar_t>, ACT_FIRST>  \
-            <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(),       \
-                                         input.data_ptr<scalar_t>(), d); \
-      });
+      input.scalar_type(),                                                                  \
+      "act_and_mul_kernel",                                                                 \
+      [&] {                                                                                 \
+        vllm::act_and_mul_kernel<scalar_t, KERNEL<scalar_t>, ACT_FIRST><<<grid, block, 0, stream>>>(   \
+          out.data_ptr<scalar_t>(),                                                         \
+          input.data_ptr<scalar_t>(),                                                       \
+          d);                                                                               \
+      });                                                                                   \
+  }
 
 void silu_and_mul(torch::Tensor& out,    // [..., d]
                   torch::Tensor& input)  // [..., 2 * d]
diff --git a/csrc/attention/attention_kernels.cuh b/csrc/attention/attention_kernels.cuh
index eb216dc8b..5033890ed 100644
--- a/csrc/attention/attention_kernels.cuh
+++ b/csrc/attention/attention_kernels.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
  * Adapted from
  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
@@ -43,6 +44,10 @@ typedef __hip_bfloat16 __nv_bfloat16;
 #define MIN(a, b) ((a) < (b) ? (a) : (b))
 #define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))
 
+typedef __NATIVE_VECTOR__(2, float) v2f;
+typedef __NATIVE_VECTOR__(2, _Float16) v2h;
+typedef __NATIVE_VECTOR__(4, float) v4f;
+
 namespace vllm {
 
 // Utility function for attention softmax.
@@ -50,7 +55,7 @@ template <int NUM_WARPS>
 inline __device__ float block_sum(float* red_smem, float sum) {
   // Decompose the thread index into warp / lane.
   int warp = threadIdx.x / WARP_SIZE;
-  int lane = threadIdx.x % WARP_SIZE;
+  int lane = threadIdx.x & (WARP_SIZE - 1);
 
   // Compute the sum per warp.
 #pragma unroll
@@ -81,6 +86,229 @@ inline __device__ float block_sum(float* red_smem, float sum) {
   return VLLM_SHFL_SYNC(sum, 0);
 }
 
+template<int NUM_WARPS>
+inline __device__ float mxblock_sum(float* red_smem, float sum) {
+  // Decompose the thread index into warp / lane.
+  int warp = threadIdx.x >> 6;
+  int lane = threadIdx.x & (MXWARP_SIZE - 1);
+
+  // Compute the sum per warp.
+#pragma unroll
+  for (int mask = MXWARP_SIZE / 2; mask >= 1; mask /= 2) {
+    sum += MXVLLM_SHFL_XOR_SYNC(sum, mask);
+  }
+
+  // Warp leaders store the data to shared memory.
+  if (lane == 0) {
+    red_smem[warp] = sum;
+  }
+
+  // Make sure the data is in shared memory.
+  __syncthreads();
+
+  // The warps compute the final sums.
+  if (lane < NUM_WARPS) {
+    sum = red_smem[lane];
+  }
+ // Parallel reduction inside the warp.
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    sum += MXVLLM_SHFL_XOR_SYNC(sum, mask);
+  }
+
+  // Broadcast to other threads.
+  return MXVLLM_SHFL_SYNC(sum, 0);
+}
+template<typename scalar_t>
+__device__  float __forceinline__ atten_mul(scalar_t *a, float b, int j) {
+  printf("not support\n");
+}
+
+template<>
+__device__ float __forceinline__ atten_mul(uint16_t *a, float b, int j) {
+    return __half2float(*((half*)a + j)) * __half2float(__float2half(b));
+}
+
+template<>
+__device__ float __forceinline__ atten_mul(__nv_bfloat16 *a, float b, int j) {
+    return __bfloat162float(*(a + j)) * __bfloat162float(__float2bfloat16(b));
+}
+
+template<typename scalar_t>
+__device__  float __forceinline__ atten_mul_opt(scalar_t *a, float b, int j) {
+  printf("not support\n");
+}
+
+template<>
+__device__ float __forceinline__ atten_mul_opt(uint16_t *a, float b, int j) {
+    return __half2float(*((half*)a + j)) * b;
+}
+
+template<>
+__device__ float __forceinline__ atten_mul_opt(__nv_bfloat16 *a, float b, int j) {
+    return __bfloat162float(*(a + j)) * b;
+}
+
+template<typename scalar_t>
+__device__  void __forceinline__ atten_mul_opt2(scalar_t *a, float b, int j, float &r0, float &r1) {
+  printf("not support\n");
+}
+template<>
+__device__ void __forceinline__ atten_mul_opt2(uint16_t *a, float b, int j, float &r0, float &r1) {
+    v2f vacc; vacc[0] = r0; vacc[1] = r1;
+    v2f vb; vb[0] = b; vb[1] = b;
+    v2h a_2h = *(v2h*)(a + j);
+    v2f va = __builtin_mxc_cvt_pk_f16tof32(a_2h);
+    vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
+    r0 = vacc[0]; r1 = vacc[1];
+}
+
+template<>
+__device__ void __forceinline__ atten_mul_opt2(__nv_bfloat16 *a, float b, int j, float &r0, float &r1) {
+    v2f vacc; vacc[0] = r0; vacc[1] = r1;
+    v2f vb; vb[0] = b; vb[1] = b;
+    v2f va; va[0] = __bfloat162float(*(a + j)); va[1] = __bfloat162float(*(a + j + 1));
+    vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
+    r0 = vacc[0]; r1 = vacc[1];
+}
+
+template<typename scalar_t, typename cache_t>
+__device__ float __forceinline__ atten_dot(scalar_t* a, cache_t *b ,int i){
+  printf("not support\n");
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(uint16_t* a, uint16_t *b ,int i){
+  return __half2float(*((half*)a + i)) * __half2float(*((half*)b + i));
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(float* a, uint16_t *b ,int i) {
+  return *(a + i) * __half2float(*((half*)b + i));
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(__nv_bfloat16* a, __nv_bfloat16 *b ,int i){
+  return __bfloat162float(a[i]) * __bfloat162float(b[i]);
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(float *a, __nv_bfloat16* b, int i) {
+  return *(a + i) * __bfloat162float(b[i]);
+}
+
+template<typename scalar_t, typename cache_t, typename T, int Vec_size>
+__device__ void __forceinline__ atten_dot(scalar_t &v1, cache_t &v2, T&qk) {
+  printf("not support\n");
+}
+
+template<>
+__device__  void __forceinline__ atten_dot<Float8_, uint4,v2f, 8>(Float8_ &v1, uint4 &v2,v2f &vdst) {
+    v2h *ptr_v2 = (v2h*)&v2;
+    v4f* ptr_v1 = (v4f*)&v1;
+    v4f reg_v1_0 = ptr_v1[0], reg_v1_1 = ptr_v1[1];
+    v2f v1_2f, v2_2f;
+    v2h v2_2h;
+    v1_2f[0] = reg_v1_0[0];                  v1_2f[1] = reg_v1_0[1];
+    v2_2h = ptr_v2[0];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = reg_v1_0[2];                  v1_2f[1] = reg_v1_0[3];
+    v2_2h = ptr_v2[1];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = reg_v1_1[0];                  v1_2f[1] = reg_v1_1[1];
+    v2_2h = ptr_v2[2];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = reg_v1_1[2];                  v1_2f[1] = reg_v1_1[3];
+    v2_2h = ptr_v2[3];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+}
+template<>
+__device__ void __forceinline__ atten_dot<Float8_, bf16_8_t, v2f, 8>(Float8_ &v1, bf16_8_t &v2, v2f &vdst) {
+    __nv_bfloat16 * ptr_v2 = (__nv_bfloat16*)&v2;
+    v2f v1_2f, v2_2f;
+    v1_2f[0] = v1.x.x;                  v1_2f[1] = v1.x.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[0]); v2_2f[1] = __bfloat162float(ptr_v2[1]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = v1.y.x;                  v1_2f[1] = v1.y.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[2]); v2_2f[1] = __bfloat162float(ptr_v2[3]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = v1.z.x;                  v1_2f[1] = v1.z.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[4]); v2_2f[1] = __bfloat162float(ptr_v2[5]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = v1.w.x;                  v1_2f[1] = v1.w.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[6]); v2_2f[1] = __bfloat162float(ptr_v2[7]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+}
+
+template<typename T, typename Vec_T0, typename Vec_T1, int Vec_size>
+__device__ __forceinline__ void convert(Vec_T0 & src , Vec_T1 &dst){
+    printf("not support\n");
+}
+
+template<>
+__device__ __forceinline__ void convert<uint16_t, uint4, Float8_, 8>(uint4 & src , Float8_ &dst) {
+  half * ptr_src = (half *)&src;
+  dst.x.x = __half2float(ptr_src[0]);
+  dst.x.y = __half2float(ptr_src[1]);
+  dst.y.x = __half2float(ptr_src[2]);
+  dst.y.y = __half2float(ptr_src[3]);
+  dst.z.x = __half2float(ptr_src[4]);
+  dst.z.y = __half2float(ptr_src[5]);
+  dst.w.x = __half2float(ptr_src[6]);
+  dst.w.y = __half2float(ptr_src[7]);
+}
+template<>
+__device__ __forceinline__ void convert<__nv_bfloat16, bf16_8_t, Float8_, 8>(bf16_8_t & src , Float8_ &dst) {
+  __nv_bfloat16 * ptr_src = (__nv_bfloat16 *)&src;
+  dst.x.x = __bfloat162float(ptr_src[0]);
+  dst.x.y = __bfloat162float(ptr_src[1]);
+  dst.y.x = __bfloat162float(ptr_src[2]);
+  dst.y.y = __bfloat162float(ptr_src[3]);
+  dst.z.x = __bfloat162float(ptr_src[4]);
+  dst.z.y = __bfloat162float(ptr_src[5]);
+  dst.w.x = __bfloat162float(ptr_src[6]);
+  dst.w.y = __bfloat162float(ptr_src[7]);
+}
+
+template<typename T>
+__device__ __forceinline__ float convert(float src){
+  printf("not support\n");
+}
+
+template<>
+__device__ __forceinline__ float convert<uint16_t>(float src) {
+   return __half2float(__float2half(src));
+}
+
+template<>
+__device__ __forceinline__ float convert<__nv_bfloat16>(float src) {
+   return __bfloat162float(__float2bfloat16(src));
+}
+
+template<typename scalar_t>
+__device__ void to_v2f(scalar_t& a, scalar_t& b, v2f& vdst){
+printf("not support\n");
+}
+
+template<>
+__device__ void to_v2f(uint16_t& a, uint16_t& b, v2f &vdst) {
+  v2h f_d;
+  _Float16 * ptr_a = (_Float16*)&f_d;
+  ptr_a[0] = *(_Float16*)&a;
+  ptr_a[1] = *(_Float16*)&b;
+  vdst = __builtin_mxc_cvt_pk_f16tof32(f_d);
+}
+
+template<>
+__device__ void to_v2f(__nv_bfloat16& a, __nv_bfloat16& b, v2f &vdst) {
+  vdst[0] = __bfloat162float(a);
+  vdst[1] = __bfloat162float(b);
+}
+
 // TODO(woosuk): Merge the last two dimensions of the grid.
 // Grid: (num_heads, num_seqs, max_num_partitions).
 template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
@@ -111,37 +339,30 @@ __device__ void paged_attention_kernel(
   const int seq_idx = blockIdx.y;
   const int partition_idx = blockIdx.z;
   const int max_num_partitions = gridDim.z;
+  const int blockDim_x = blockDim.x;
   constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
   const int seq_len = seq_lens[seq_idx];
   if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
     // No work to do. Terminate the thread block.
     return;
   }
-
   const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
-  const int num_blocks_per_partition =
-      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
+  const int num_blocks_per_partition = USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
 
   // [start_block_idx, end_block_idx) is the range of blocks to process.
-  const int start_block_idx =
-      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
-  const int end_block_idx =
-      MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
   const int num_blocks = end_block_idx - start_block_idx;
 
   // [start_token_idx, end_token_idx) is the range of tokens to process.
   const int start_token_idx = start_block_idx * BLOCK_SIZE;
-  const int end_token_idx =
-      MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
   const int num_tokens = end_token_idx - start_token_idx;
 
   constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
-  constexpr int NUM_THREAD_GROUPS =
-      NUM_THREADS / THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
-                                        // divides NUM_THREADS
+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
   assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
-  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
-      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
   const int thread_idx = threadIdx.x;
   const int warp_idx = thread_idx / WARP_SIZE;
@@ -151,18 +372,14 @@ __device__ void paged_attention_kernel(
   const int num_heads = gridDim.x;
   const int num_queries_per_kv = num_heads / num_kv_heads;
   const int kv_head_idx = head_idx / num_queries_per_kv;
-  const float alibi_slope =
-      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
-
-  // A vector type to store a part of a key or a query.
-  // The vector size is configured in such a way that the threads in a thread
-  // group fetch or compute 16 bytes at a time. For example, if the size of a
-  // thread group is 4 and the data type is half, then the vector size is 16 /
-  // (4 * sizeof(half)) == 2.
-  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
   using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
   using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
+#ifdef ENABLE_FP8_E5M2
   using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+#endif
 
   constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
   constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
@@ -172,21 +389,20 @@ __device__ void paged_attention_kernel(
 
   // Load the query to registers.
   // Each thread in a thread group has a different part of the query.
-  // For example, if the the thread group size is 4, then the first thread in
-  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
-  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
-  // q is split from a qkv tensor, it may not be contiguous.
+  // For example, if the the thread group size is 4, then the first thread in the group
+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
+  // th vectors of the query, and so on.
+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
   const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
-  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
 #pragma unroll
-  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
-       i += NUM_THREAD_GROUPS) {
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
+    Q_vec_l dst;
     const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
-    q_vecs[thread_group_offset][i] =
-        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
   }
-  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
-                    // memory wall right before we use q_vecs
+  __syncthreads(); // TODO(naed90): possible speedup if this is replaced with a memory wall right before we use q_vecs
 
   // Memory planning.
   extern __shared__ char shared_mem[];
@@ -195,9 +411,8 @@ __device__ void paged_attention_kernel(
   // Workspace for reduction.
   __shared__ float red_smem[2 * NUM_WARPS];
 
-  // x == THREAD_GROUP_SIZE * VEC_SIZE
   // Each thread group fetches x elements from the key at a time.
-  constexpr int x = 16 / sizeof(cache_t);
+  constexpr int x = 16;
   float qk_max = -FLT_MAX;
 
   // Iterate over the key blocks.
@@ -224,13 +439,96 @@ __device__ void paged_attention_kernel(
                         1;
   }
 
-  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
-       block_idx += NUM_WARPS) {
-    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
-    // int64 because int32 can lead to overflow when this variable is multiplied
-    // by large numbers (e.g., kv_block_stride).
-    // For blocksparse attention: skip computation on blocks that are not
-    // attended
+  int block_idx0 = start_block_idx + warp_idx;
+  int kv_offset0, kv_offset1;
+  K_vec load_k[NUM_VECS_PER_THREAD];
+  K_vec compute_k[NUM_VECS_PER_THREAD];
+  
+  int k_offset[NUM_VECS_PER_THREAD];
+  kv_offset0 = block_table[block_idx0];
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  #pragma unroll
+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
+    const int offset1 = vec_idx >> 4;
+    const int offset2 = vec_idx & 15;
+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
+  }
+  if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if(is_remote || is_local) {
+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+            const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+            const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                              + kv_head_idx * kv_head_stride
+                                              + physical_block_offset * x;
+            
+        #pragma unroll
+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }    
+      } 
+  } else {
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+      const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                        + kv_head_idx * kv_head_stride
+                                        + physical_block_offset * x;
+      
+  #pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+      }
+    }
+  } 
+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        compute_k[j] = load_k[j];
+      }
+      if(block_idx < end_block_idx - NUM_WARPS) {
+          kv_offset0 = kv_offset1;
+          int nblock_idx = block_idx + NUM_WARPS;
+          if(block_idx < end_block_idx - (NUM_WARPS << 1)) {
+            kv_offset1 = block_table[block_idx + (NUM_WARPS<<1)];
+          }
+          if constexpr (IS_BLOCK_SPARSE) {
+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
+            const bool is_remote =
+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+            const bool is_local =
+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+            if(is_remote || is_local) {
+              const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                      + kv_head_idx * kv_head_stride
+                                      + physical_block_offset * x;
+              #pragma unroll NUM_VECS_PER_THREAD
+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+              }  
+            }
+          } else {
+            const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                      + kv_head_idx * kv_head_stride
+                                      + physical_block_offset * x;
+            #pragma unroll NUM_VECS_PER_THREAD
+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          } 
+    }
     if constexpr (IS_BLOCK_SPARSE) {
       const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
       const bool is_remote =
@@ -254,48 +552,26 @@ __device__ void paged_attention_kernel(
         continue;
       }
     }
-    const int64_t physical_block_number =
-        static_cast<int64_t>(block_table[block_idx]);
-
-    // Load a key to registers.
-    // Each thread in a thread group has a different part of the key.
-    // For example, if the the thread group size is 4, then the first thread in
-    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
-    // has 1, 5, 9, ... th vectors of the key, and so on.
-    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-      const int physical_block_offset =
-          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
-      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-      K_vec k_vecs[NUM_VECS_PER_THREAD];
-
-#pragma unroll
-      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-        const cache_t* k_ptr =
-            k_cache + physical_block_number * kv_block_stride +
-            kv_head_idx * kv_head_stride + physical_block_offset * x;
-        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
-        const int offset1 = (vec_idx * VEC_SIZE) / x;
-        const int offset2 = (vec_idx * VEC_SIZE) % x;
-
-        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
-          k_vecs[j] = *reinterpret_cast<const K_vec*>(
-              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
-        } else {
-          // Vector conversion from Quant_vec to K_vec.
-          Quant_vec k_vec_quant = *reinterpret_cast<const Quant_vec*>(
-              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
-          k_vecs[j] = fp8::scaled_convert<K_vec, Quant_vec, KV_DTYPE>(
-              k_vec_quant, *k_scale);
-        }
-      }
 
       // Compute dot product.
       // This includes a reduction across the threads in the same thread group.
-      float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
-                             q_vecs[thread_group_offset], k_vecs);
+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
+      float qk = 0.0f;
+      v2f f2_qk = {0,0};
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+	atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
+      }
+      qk = f2_qk[0] + f2_qk[1];
+  
+      #pragma unroll
+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+        qk += VLLM_SHFL_XOR_SYNC(qk, mask);
+      }
+      qk = scale * qk;
       // Add the ALiBi bias if slopes are given.
       qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
-
+      
       if (thread_group_offset == 0) {
         // Store the partial reductions to shared memory.
         // NOTE(woosuk): It is required to zero out the masked logits.
@@ -318,7 +594,6 @@ __device__ void paged_attention_kernel(
     red_smem[warp_idx] = qk_max;
   }
   __syncthreads();
-
   // TODO(woosuk): Refactor this part.
   // Get the max qk value for the sequence.
   qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
@@ -332,14 +607,14 @@ __device__ void paged_attention_kernel(
   // Get the sum of the exp values.
   float exp_sum = 0.f;
   for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-    float val = __expf(logits[i] - qk_max);
+    float val = __builtin_expf(logits[i] - qk_max);
     logits[i] = val;
     exp_sum += val;
   }
   exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
 
   // Compute softmax.
-  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
   for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
     logits[i] *= inv_sum;
   }
@@ -347,201 +622,216 @@ __device__ void paged_attention_kernel(
 
   // If partitioning is enabled, store the max logit and exp_sum.
   if (USE_PARTITIONING && thread_idx == 0) {
-    float* max_logits_ptr = max_logits +
-                            seq_idx * num_heads * max_num_partitions +
-                            head_idx * max_num_partitions + partition_idx;
+    float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
+                                       + head_idx * max_num_partitions
+                                       + partition_idx;
     *max_logits_ptr = qk_max;
-    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
-                          head_idx * max_num_partitions + partition_idx;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
+                                   + head_idx * max_num_partitions
+                                   + partition_idx;
     *exp_sums_ptr = exp_sum;
   }
-
-  // Each thread will fetch 16 bytes from the value cache at a time.
-  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
+  constexpr int NUM_COLS_PER_ITER = MAX(WARP_SIZE / NUM_V_VECS_PER_THREAD,1);
+  constexpr int NUM_VALID_THREAD = NUM_COLS_PER_ITER * NUM_V_VECS_PER_THREAD;
+  constexpr int NUM_LGT_PER_COL = (BLOCK_SIZE + NUM_COLS_PER_ITER - 1) / NUM_COLS_PER_ITER;
+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
   using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
   using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-  using V_quant_vec = typename Vec<cache_t, V_VEC_SIZE>::Type;
   using Float_L_vec = typename FloatVec<L_vec>::Type;
-
-  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
-  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
-  constexpr int NUM_ROWS_PER_THREAD =
-      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
-
-  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
-  float accs[NUM_ROWS_PER_THREAD];
-#pragma unroll
-  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-    accs[i] = 0.f;
-  }
-
-  scalar_t zero_value;
-  zero(zero_value);
-  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
-       block_idx += NUM_WARPS) {
-    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
-    // int64 because int32 can lead to overflow when this variable is multiplied
-    // by large numbers (e.g., kv_block_stride).
-    // For blocksparse attention: skip computation on blocks that are not
-    // attended
+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
+  V_vec v_vecs[NUM_LGT_PER_COL];
+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
+  float accs[V_VEC_SIZE];
+  float reg_log[NUM_LGT_PER_COL];
+  float reg_prev_log[NUM_LGT_PER_COL];
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    accs[i] = 0.0f;
+  }
+  int token_idx, kv_stride, block_offset;
+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
+  kv_offset0 = block_table[block_idx0];
+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE; 
+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+  float *ptr_logits = logits + token_idx - start_token_idx;
+  if(lane < NUM_VALID_THREAD) {
     if constexpr (IS_BLOCK_SPARSE) {
-      int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-      if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
-          !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-        continue;
-      }
+          int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+          if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+              ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+              if(block_idx0 == num_seq_blocks - 1) {
+              #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                  if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                  }
+                }
+              } else {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                  if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                  }
+                }
+              }
+          }
     }
-    const int64_t physical_block_number =
-        static_cast<int64_t>(block_table[block_idx]);
-    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
-    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-    L_vec logits_vec;
-    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
-                                                           start_token_idx));
-
-    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
-                           kv_head_idx * kv_head_stride;
-#pragma unroll
-    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-      if (row_idx < HEAD_SIZE) {
-        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
-        V_vec v_vec;
-
-        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
-          v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+    else {
+      if(block_idx0 == num_seq_blocks - 1) {
+        #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+	    if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+          }
         } else {
-          V_quant_vec v_quant_vec =
-              *reinterpret_cast<const V_quant_vec*>(v_ptr + offset);
-          // Vector conversion from V_quant_vec to V_vec.
-          v_vec = fp8::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec,
-                                                                    *v_scale);
-        }
-        if (block_idx == num_seq_blocks - 1) {
-          // NOTE(woosuk): When v_vec contains the tokens that are out of the
-          // context, we should explicitly zero out the values since they may
-          // contain NaNs. See
-          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
-          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
-#pragma unroll
-          for (int j = 0; j < V_VEC_SIZE; j++) {
-            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;
+          #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+	    if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
           }
         }
-        accs[i] += dot(logits_vec, v_vec);
-      }
     }
-  }
-
-  // Perform reduction within each warp.
-#pragma unroll
-  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-    float acc = accs[i];
-#pragma unroll
-    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
-      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
+    
+    for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+        int next_block = block_idx + NUM_WARPS;
+        int nnext_block = next_block + NUM_WARPS;
+        for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+            v_vecs[i] = v_prev_vecs[i];
+            reg_log[i] = reg_prev_log[i];
+        }
+        if(next_block < end_block_idx) {
+            kv_offset0 = kv_offset1;
+            if(nnext_block < end_block_idx) {
+            kv_offset1 = block_table[nnext_block];
+            }
+            token_idx = next_block * BLOCK_SIZE + physical_block_offset;
+            const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+            ptr_logits = logits + token_idx - start_token_idx;
+            if constexpr (IS_BLOCK_SPARSE) {
+            int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
+            if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+                ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+                    if(next_block == num_seq_blocks - 1) {
+                    #pragma unroll
+                    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                    }
+                    }
+                } else {
+                    #pragma unroll
+                    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                    }
+                    }
+                }
+            }
+            } else {
+            if(next_block == num_seq_blocks - 1) {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+                }
+            } else {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+                }
+            }
+            }
+        }
+        
+      if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          continue;
+        }
+      }
+      
+      token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+        if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
+	  #pragma unroll
+          for(int j = 0; j < V_VEC_SIZE; j+=2) {
+            atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
+          }
+        }
+      } 
     }
-    accs[i] = acc;
   }
-
-  // NOTE(woosuk): A barrier is required because the shared memory space for
-  // logits is reused for the output.
   __syncthreads();
-
-  // Perform reduction across warps.
+  //need move
   float* out_smem = reinterpret_cast<float*>(shared_mem);
-#pragma unroll
-  for (int i = NUM_WARPS; i > 1; i /= 2) {
-    int mid = i / 2;
-    // Upper warps write to shared memory.
-    if (warp_idx >= mid && warp_idx < i) {
-      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
-#pragma unroll
-      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
-          dst[row_idx] = accs[i];
-        }
-      }
-    }
-    __syncthreads();
+  for(int i = threadIdx.x; i < NUM_WARPS * NUM_COLS_PER_ITER * HEAD_SIZE; i += blockDim_x) {
+    out_smem[i] = 0.0f;
+  }
+  __syncthreads();
 
-    // Lower warps update the output.
-    if (warp_idx < mid) {
-      const float* src = &out_smem[warp_idx * HEAD_SIZE];
-#pragma unroll
-      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
-          accs[i] += src[row_idx];
-        }
-      }
+  if(lane < NUM_VALID_THREAD) {
+    float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
+    for(int i = 0; i < V_VEC_SIZE; i++) {
+      ptr_out_smem[i] = accs[i];
     }
-    __syncthreads();
   }
-
-  // Write the final output.
-  if (warp_idx == 0) {
-    scalar_t* out_ptr =
-        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
-        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
-#pragma unroll
-    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
-        from_float(*(out_ptr + row_idx), accs[i]);
+  __syncthreads();
+  scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                        + head_idx * max_num_partitions * HEAD_SIZE
+                        + partition_idx * HEAD_SIZE;
+  if(threadIdx.x < HEAD_SIZE) {
+    int length = NUM_LANE * HEAD_SIZE;
+      float r = 0;
+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
+        r += out_smem[i];
       }
-    }
+      from_float(*(out_ptr + threadIdx.x), r);
   }
 }
 
-// Grid: (num_heads, num_seqs, 1).
-template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-          bool IS_BLOCK_SPARSE>
-__global__ void paged_attention_v1_kernel(
-    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
-    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size/x, block_size, x]
-    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size, block_size]
-    const int num_kv_heads,               // [num_heads]
-    const float scale,
-    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-    const int* __restrict__ seq_lens,      // [num_seqs]
-    const int max_num_blocks_per_seq,
-    const float* __restrict__ alibi_slopes,  // [num_heads]
-    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-    const float* k_scale, const float* v_scale, const int tp_rank,
-    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-                         KV_DTYPE, IS_BLOCK_SPARSE>(
-      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
-      v_cache, num_kv_heads, scale, block_tables, seq_lens,
-      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
-      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
-      blocksparse_vert_stride, blocksparse_block_size,
-      blocksparse_head_sliding_step);
-}
-
-// Grid: (num_heads, num_seqs, max_num_partitions).
 template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
           int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
           bool IS_BLOCK_SPARSE,
-          int PARTITION_SIZE>
-__global__ void paged_attention_v2_kernel(
+          int PARTITION_SIZE = 0>  // Zero means no partitioning.
+__device__ __forceinline__ void paged_attention_kernel_32N(
     float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
-    float* __restrict__ max_logits,       // [num_seqs, num_heads,
-                                          // max_num_partitions]
-    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
-                                          // max_num_partitions, head_size]
+    float* __restrict__ max_logits,  // [num_seqs, num_heads,
+                                     // max_num_partitions]
+    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,head_size]
     const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size/x, block_size, x]
-    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size, block_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads, head_size/x, block_size, x]->[num_blocks, num_kv_heads, head_size/16, block_size, 16]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads, head_size, block_size]->[num_blocks, num_kv_heads, block_size, head_size]
     const int num_kv_heads,               // [num_heads]
     const float scale,
     const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
@@ -549,16 +839,1186 @@ __global__ void paged_attention_v2_kernel(
     const int max_num_blocks_per_seq,
     const float* __restrict__ alibi_slopes,  // [num_heads]
     const int q_stride, const int kv_block_stride, const int kv_head_stride,
-    const float* k_scale, const float* v_scale, const int tp_rank,
-    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
-      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
-      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
-      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
-      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
-      blocksparse_head_sliding_step);
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
+  const int seq_idx = blockIdx.y;
+  const int partition_idx = blockIdx.z;
+  // const int max_num_partitions = gridDim.z;
+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
+  const int seq_len = seq_lens[seq_idx];
+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
+    // No work to do. Terminate the thread block.
+    return;
+  }
+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
+  const int num_blocks_per_partition = USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+  constexpr int THREAD_GROUP_SIZE = MAX(MXWARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
+  // assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, MXWARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS >> 6;
+  const int thread_idx = threadIdx.x;
+  const int warp_idx = thread_idx >> 6;
+  const int lane = thread_idx & 63;
+
+  const int head_idx = blockIdx.x;
+  //const int num_heads = gridDim.x;
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+  // A vector type to store a part of a key or a query.
+  // The vector size is configured in such a way that the threads in a thread group
+  // fetch or compute 16 bytes at a time.
+  // For example, if the size of a thread group is 4 and the data type is half,
+  // then the vector size is 16 / (4 * sizeof(half)) == 2.
+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
+#ifdef ENABLE_FP8_E5M2
+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+#endif
+  
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in the group
+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
+  // th vectors of the query, and so on.
+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
+    Q_vec_l dst;
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE); 
+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
+  }
+  __syncthreads(); // TODO(naed90): possible speedup if this is replaced with a memory wall right before we use q_vecs
+  // Memory planning.
+  extern __shared__ char shared_mem[];
+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
+  float* logits = reinterpret_cast<float*>(shared_mem);
+  // Workspace for reduction.
+  __shared__ float red_smem[2 * NUM_WARPS];
+
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16;
+  float qk_max = -FLT_MAX;
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
+  // blocksparse specific vars
+  int bs_block_offset;
+  int q_bs_block_id;
+  if constexpr (IS_BLOCK_SPARSE) {
+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
+    // blocksparse_block_size);
+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
+    if (blocksparse_head_sliding_step >= 0)
+      // sliding on q heads
+      bs_block_offset =
+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
+    else
+      // sliding on kv heads
+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
+                            (-blocksparse_head_sliding_step) +
+                        1;
+  }
+  int block_idx0 = start_block_idx + warp_idx;
+  int kv_offset0, kv_offset1;
+  K_vec load_k[NUM_VECS_PER_THREAD];
+  K_vec compute_k[NUM_VECS_PER_THREAD];
+  int k_offset[NUM_VECS_PER_THREAD];
+  kv_offset0 = block_table[block_idx0];
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  #pragma unroll
+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
+    const int offset1 = vec_idx >> 4;
+    const int offset2 = vec_idx & 15;
+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
+  }
+  const cache_t* ptr_k_cache = k_cache + kv_head_idx * kv_head_stride;
+  if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if(is_remote || is_local) {
+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+        #pragma unroll
+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }    
+      } 
+  } else {
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+        const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1) ;
+        const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+        const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+    #pragma unroll
+        for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+          load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+        } 
+    }
+  }
+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {  
+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        compute_k[j] = load_k[j];
+      }
+      if(block_idx < end_block_idx - NUM_WARPS) {
+          kv_offset0 = kv_offset1;
+	  int nblock_idx = block_idx + NUM_WARPS;
+          if(block_idx < end_block_idx - (NUM_WARPS << 1)) {
+            kv_offset1 = block_table[block_idx + (NUM_WARPS<<1)];
+          }
+          if constexpr (IS_BLOCK_SPARSE) {
+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
+            const bool is_remote =
+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+            const bool is_local =
+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+            if(is_remote || is_local) {
+              const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+              #pragma unroll NUM_VECS_PER_THREAD
+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+              }  
+            }
+          } else {
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+            #pragma unroll NUM_VECS_PER_THREAD
+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }
+      }
+      if constexpr (IS_BLOCK_SPARSE) {
+        const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        const bool is_remote =
+            ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+        const bool is_local =
+            (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+        if (!is_remote && !is_local) {
+            for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset =
+                (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+            const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+            if (thread_group_offset == 0) {
+                // NOTE(linxihui): assign very large number to skipped tokens to
+                // avoid contribution to the sumexp softmax normalizer. This will
+                // not be used at computing sum(softmax*v) as the blocks will be
+                // skipped.
+                logits[token_idx - start_token_idx] = -FLT_MAX;
+            }
+            }
+            continue;
+        }
+    }
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
+      float qk = 0.0f;
+      v2f f2_qk = {0,0};
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
+      }
+      qk = f2_qk[0] + f2_qk[1];
+      #pragma unroll
+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+        qk += MXVLLM_SHFL_XOR_SYNC(qk, mask);
+      }
+      qk = scale * qk;
+      // Add the ALiBi bias if slopes are given.
+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
+
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= seq_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
+      }
+    }
+  }
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+  #pragma unroll
+  for (int mask = MXWARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  __syncthreads();
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+  #pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  qk_max = MXVLLM_SHFL_SYNC(qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = __builtin_expf(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
+
+  // Compute softmax.
+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = logits[i];
+    val *= inv_sum;
+    logits[i] = convert<cache_t>(val);
+  }
+  __syncthreads();
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (USE_PARTITIONING && thread_idx == 0) {
+    float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
+                                       + head_idx * max_num_partitions
+                                       + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
+                                   + head_idx * max_num_partitions
+                                   + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
+  constexpr int NUM_COLS_PER_ITER = MAX(MXWARP_SIZE / NUM_V_VECS_PER_THREAD , 1); 
+  constexpr int NUM_LGT_PER_COL = BLOCK_SIZE / NUM_COLS_PER_ITER; 
+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
+  V_vec v_vecs[NUM_LGT_PER_COL];
+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
+  float accs[V_VEC_SIZE];
+  float reg_log[NUM_LGT_PER_COL];
+  float reg_prev_log[NUM_LGT_PER_COL];
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    accs[i] = 0.0f;
+  }
+  int token_idx, kv_stride, block_offset;
+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
+  kv_offset0 = block_table[block_idx0];
+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE; 
+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+  float *ptr_logits = logits + token_idx - start_token_idx;
+  if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+                const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+            }
+        }
+    } else {
+          #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+            if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+          }
+    }
+  for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    int next_block = block_idx + NUM_WARPS;
+    int nnext_block = next_block + NUM_WARPS;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      v_vecs[i] = v_prev_vecs[i];
+      reg_log[i] = reg_prev_log[i];
+    }
+    if(next_block < end_block_idx) {
+      kv_offset0 = kv_offset1;
+      if(nnext_block < end_block_idx) {
+        kv_offset1 = block_table[nnext_block];
+      }
+      token_idx = next_block * BLOCK_SIZE + physical_block_offset;
+      const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+      ptr_logits = logits + token_idx - start_token_idx;
+      if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+            }
+        }
+        } else  {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+        }
+    }
+    if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          continue;
+        }
+    }
+    
+    token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    float *ptr_logits = logits + token_idx - start_token_idx;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+        scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
+        #pragma unroll
+        for(int j = 0; j < V_VEC_SIZE; j+=2) {
+          atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
+        }
+      }
+    } 
+  }
+  __syncthreads();
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+  float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    ptr_out_smem[i] = accs[i];
+  }
+   __syncthreads();
+  
+  scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                        + head_idx * max_num_partitions * HEAD_SIZE
+                        + partition_idx * HEAD_SIZE;
+  if(threadIdx.x < HEAD_SIZE) {
+    int length = NUM_LANE * HEAD_SIZE;
+      float r = 0;
+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
+        r += out_smem[i];
+      }
+      from_float(*(out_ptr + threadIdx.x), r);
+  }
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+	  int PARTITION_SIZE = 512>  // Zero means no partitioning.
+__device__ __forceinline__ void paged_attention_kernel_32N_final(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,  // [num_seqs, num_heads,
+                                     // max_num_partitions]
+    int* __restrict__ block_count,          // [num_seqs, num_heads]
+    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,
+                                 // head_size]
+    scalar_t* __restrict__ final_out,      // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads, head_size/x, block_size, x]->[num_blocks, num_kv_heads, head_size/16, block_size, 16]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads, head_size, block_size]->[num_blocks, num_kv_heads, block_size, head_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step, const int max_num_partitions,
+    const int blockDim_x,
+    const int num_heads,
+    const int grid_dim_y,
+    const bool count_init_once) {
+  const int seq_idx = blockIdx.y;
+  const int partition_idx = blockIdx.z;
+  // const int max_num_partitions = gridDim.z;
+  // const int blockDim_x = blockDim.x;
+  const int head_idx = blockIdx.x;
+  // const int num_heads = gridDim.x;
+  // const int grid_dim_y = gridDim.y;
+  const int seq_len = seq_lens[seq_idx];
+
+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
+  const int num_blocks_per_partition = PARTITION_SIZE / BLOCK_SIZE;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx = partition_idx * num_blocks_per_partition;
+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+
+  constexpr int THREAD_GROUP_SIZE = MAX(MXWARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, MXWARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS >> 6;
+  const int thread_idx = threadIdx.x;
+  const int warp_idx = thread_idx >> 6;
+  const int lane = thread_idx & 63;
+
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+  int offset0 = seq_idx * num_heads;
+  int offset1 = offset0 * HEAD_SIZE;
+  int offset2 = head_idx * HEAD_SIZE;
+  int offset3 = head_idx * max_num_partitions;
+  int offset4 = offset0 * max_num_partitions;
+  int offset5 = seq_idx * max_num_blocks_per_seq;
+  int num_warps2 = NUM_WARPS << 1;
+
+  // The vector size is configured in such a way that the threads in a thread group
+  // fetch or compute 16 bytes at a time.
+  // For example, if the size of a thread group is 4 and the data type is half,
+  // then the vector size is 16 / (4 * sizeof(half)) == 2.
+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
+#ifdef ENABLE_FP8_E5M2
+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+#endif
+
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in the group
+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
+  // th vectors of the query, and so on.
+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
+  }
+  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
+                    // memory wall right before we use q_vecs
+
+  // Memory planning.
+  extern __shared__ char shared_mem[];
+  float* red_smem = reinterpret_cast<float*>(shared_mem);
+  int * block_table_smem = reinterpret_cast<int*>(red_smem + num_warps2);
+  float * logits = reinterpret_cast<float*>(block_table_smem + 512 + num_warps2);
+
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16;
+  float qk_max = -FLT_MAX;
+
+  // blocksparse specific vars
+  int bs_block_offset;
+  int q_bs_block_id;
+  if constexpr (IS_BLOCK_SPARSE) {
+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
+    // blocksparse_block_size);
+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
+    if (blocksparse_head_sliding_step >= 0)
+      // sliding on q heads
+      bs_block_offset =
+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
+    else
+      // sliding on kv heads
+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
+                            (-blocksparse_head_sliding_step) +
+                        1;
+  }
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + offset5 + start_block_idx;
+  //load block_table to share memory
+  for(int i = threadIdx.x; i < num_blocks; i += blockDim_x){
+    block_table_smem[i] = block_table[i];
+  }
+  int block_idx0 = start_block_idx + warp_idx;
+  __syncthreads();
+  int kv_offset0, kv_offset1;
+  K_vec load_k[NUM_VECS_PER_THREAD];
+  K_vec compute_k[NUM_VECS_PER_THREAD];
+  int k_offset[NUM_VECS_PER_THREAD];
+  kv_offset0 = block_table_smem[block_idx0 - start_block_idx];
+  kv_offset1 = block_table_smem[block_idx0 + NUM_WARPS - start_block_idx];
+  #pragma unroll
+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
+    const int offset1 = vec_idx >> 4;
+    const int offset2 = vec_idx & 15;
+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
+  }
+
+  const cache_t* ptr_k_cache = k_cache + kv_head_idx * kv_head_stride;
+  if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if(is_remote || is_local) {
+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+        #pragma unroll
+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }    
+      } 
+  } else {
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+      const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+    #pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+      }
+    }
+  }
+  
+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        compute_k[j] = load_k[j];
+      }
+      if(block_idx < end_block_idx - NUM_WARPS) {
+          kv_offset0 = kv_offset1;
+	  int nblock_idx = block_idx + NUM_WARPS;
+          kv_offset1 = block_table_smem[block_idx + num_warps2 - start_block_idx];
+          if constexpr (IS_BLOCK_SPARSE) {
+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
+            const bool is_remote =
+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+            const bool is_local =
+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+            if(is_remote || is_local) {
+              const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+              #pragma unroll NUM_VECS_PER_THREAD
+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+              }  
+            }
+          } else {
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+            #pragma unroll NUM_VECS_PER_THREAD
+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }
+      }
+      if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if (!is_remote && !is_local) {
+        for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+          const int physical_block_offset =
+              (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+          const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+          if (thread_group_offset == 0) {
+            // NOTE(linxihui): assign very large number to skipped tokens to
+            // avoid contribution to the sumexp softmax normalizer. This will
+            // not be used at computing sum(softmax*v) as the blocks will be
+            // skipped.
+            logits[token_idx - start_token_idx] = -FLT_MAX;
+          }
+        }
+        continue;
+      }
+    }
+
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
+      float qk = 0.0f;
+      v2f f2_qk = {0,0};
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
+      }
+      qk = f2_qk[0] + f2_qk[1];
+      #pragma unroll
+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+        qk += MXVLLM_SHFL_XOR_SYNC(qk, mask);
+      }
+
+      qk = scale * qk;
+      // Add the ALiBi bias if slopes are given.
+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
+
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= seq_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
+      }
+    }
+  }
+
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+#pragma unroll
+  for (int mask = MXWARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  __syncthreads();
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  qk_max = MXVLLM_SHFL_SYNC(qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = __builtin_expf(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
+
+  // Compute softmax.
+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = logits[i];
+    val *= inv_sum;
+    logits[i] = convert<cache_t>(val);
+  }
+  __syncthreads();
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (thread_idx == 0) {
+    float* max_logits_ptr = max_logits + offset4
+                                       + offset3
+                                       + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + offset4
+                                   + offset3
+                                   + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+
+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
+  constexpr int NUM_COLS_PER_ITER = MAX(MXWARP_SIZE / NUM_V_VECS_PER_THREAD , 1); 
+  constexpr int NUM_LGT_PER_COL = BLOCK_SIZE / NUM_COLS_PER_ITER; 
+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
+  V_vec v_vecs[NUM_LGT_PER_COL];
+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
+  float accs[V_VEC_SIZE];
+  float reg_log[NUM_LGT_PER_COL];
+  float reg_prev_log[NUM_LGT_PER_COL];
+
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    accs[i] = 0.0f;
+  }
+  int token_idx, kv_stride, block_offset;
+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
+  kv_offset0 = block_table_smem[block_idx0 - start_block_idx];
+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
+  kv_offset1 = block_table_smem[block_idx0 + NUM_WARPS - start_block_idx];
+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE;
+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+  float *ptr_logits = logits + token_idx - start_token_idx;
+
+  if constexpr (IS_BLOCK_SPARSE) {
+      int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+          ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+              if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+              }
+          }
+      }
+  } else {
+        #pragma unroll
+        for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+          if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+            const int idx = laneid * V_VEC_SIZE + i * block_offset;
+            v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+            reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+          }
+        }
+    }
+
+  for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    int next_block = block_idx + NUM_WARPS;
+    int nnext_block = next_block + NUM_WARPS;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      v_vecs[i] = v_prev_vecs[i]; reg_log[i] = reg_prev_log[i];
+    }
+    if(next_block < end_block_idx) {
+      kv_offset0 = kv_offset1;
+      kv_offset1 = block_table_smem[nnext_block - start_block_idx];
+      token_idx = next_block * BLOCK_SIZE + physical_block_offset;
+      const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+      ptr_logits = logits + token_idx - start_token_idx;
+      if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+              if(next_block == num_seq_blocks - 1) {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                    }
+                }
+              } else {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+              }
+        }
+        } else {
+          if(next_block == num_seq_blocks - 1) {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+                const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+              }
+            }
+          } else {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+          }
+        }
+    }
+    if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          continue;
+        }
+    }
+
+    token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    float *ptr_logits = logits + token_idx - start_token_idx;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+        scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
+        #pragma unroll
+        for(int j = 0; j < V_VEC_SIZE; j+=2) {
+          atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
+        }
+      }
+    } 
+  }
+
+  __syncthreads();
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+  float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    ptr_out_smem[i] = accs[i];
+  }
+   __syncthreads();
+  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);
+
+  if(partition_idx * PARTITION_SIZE < seq_len) {
+    scalar_t* out_ptr = out + (offset4 + offset3 + partition_idx) * HEAD_SIZE;
+    if(threadIdx.x < HEAD_SIZE) {
+      int length = NUM_LANE * HEAD_SIZE;
+      float r = 0;
+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
+        r += out_smem[i];
+      }
+      from_float(*(out_ptr + threadIdx.x), r);
+    }
+  }
+
+  __syncthreads();
+  bool last_block = false;
+  if(threadIdx.x == blockDim_x - 1) {
+    if(atomicAdd(block_count + head_idx * grid_dim_y + seq_idx, 1) == max_num_partitions - 1) {
+      last_block = true;
+    }
+  }
+  if (__syncthreads_or(last_block)) {
+      if(count_init_once) {
+        if(threadIdx.x == blockDim_x - 2){
+          *(block_count + head_idx * grid_dim_y + seq_idx) = 0;
+        }
+      }
+      if(num_partitions == 1) {
+        scalar_t* out_ptr = final_out + offset1 + offset2;
+        const scalar_t* tmp_out_ptr = out + (offset4 + offset3) * HEAD_SIZE;
+        V_vec* ptr_vec_out = (V_vec*)out_ptr;
+        V_vec* ptr_vec_in = (V_vec*)tmp_out_ptr;
+        int num = HEAD_SIZE / V_VEC_SIZE;
+        for (int i = threadIdx.x; i < num; i += blockDim_x) {
+          ptr_vec_out[i] = ptr_vec_in[i];
+        }
+	return;
+      }
+      // Load max logits to shared memory.
+      float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
+      float * red_smem = shared_max_logits + (num_partitions << 1 );
+      const float* max_logits_ptr = max_logits + offset4 + offset3;
+      float max_logit = -FLT_MAX;
+      for (int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        const float l = max_logits_ptr[i];
+        shared_max_logits[i] = l;
+      }
+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        max_logit = fmaxf(max_logit, shared_max_logits[i]);
+      }
+      __syncthreads();
+      // Get the global max logit.
+      // Reduce within the warp.
+      #pragma unroll
+      for (int mask = MXWARP_SIZE / 2; mask >= 1; mask /= 2) {
+        max_logit = fmaxf(max_logit, MXVLLM_SHFL_XOR_SYNC(max_logit, mask));
+      }
+      
+      if (lane == 0) {
+        red_smem[warp_idx] = max_logit;
+      }
+      __syncthreads();
+      // Reduce across warps.
+      max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+    #pragma unroll
+      for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+        max_logit = fmaxf(max_logit, MXVLLM_SHFL_XOR_SYNC(max_logit, mask));
+      }
+      // Broadcast the max value to all threads.
+      max_logit = MXVLLM_SHFL_SYNC(max_logit, 0);
+
+      // Load rescaled exp sums to shared memory.
+      float* shared_exp_sums = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+      const float* exp_sums_ptr = exp_sums + offset4 + offset3;
+
+      float global_exp_sum = 0.0f;  
+      float * out_sm_ptr = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions * 2);
+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        out_sm_ptr[i] = exp_sums_ptr[i];
+      }
+
+      for (int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        float l = shared_max_logits[i];
+        float rescaled_exp_sum = out_sm_ptr[i] * __builtin_expf(l - max_logit);
+        global_exp_sum += rescaled_exp_sum;
+        shared_exp_sums[i] = rescaled_exp_sum;
+      }
+      __syncthreads();
+
+      global_exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum);
+      const float inv_global_exp_sum = __builtin_mxc_rcpf(global_exp_sum + 1e-6f);
+      
+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+          shared_exp_sums[i] = shared_exp_sums[i] * inv_global_exp_sum;
+      }
+
+      // Aggregate tmp_out to out.
+      scalar_t* out_ptr = final_out + offset1 + offset2;
+      const scalar_t* tmp_out_ptr = out + (offset1 + offset2) * max_num_partitions;
+      scalar_t * out_sm_ptr_2 = reinterpret_cast<scalar_t*>(out_sm_ptr);
+      int buffer_size = num_partitions * HEAD_SIZE / V_VEC_SIZE;
+      for(int i = threadIdx.x; i < buffer_size; i += blockDim_x) {
+          int offset = i * V_VEC_SIZE;
+          V_vec reg = *reinterpret_cast<const V_vec*>(tmp_out_ptr + offset);
+          *(V_vec *)(out_sm_ptr_2  + offset) = reg;
+      }
+      __syncthreads();
+      
+      if(threadIdx.x < HEAD_SIZE) {
+        scalar_t * ptr_sm_out_ptr = out_sm_ptr_2 + threadIdx.x;
+        float acc = 0.0f;
+        int num_partitions2 = num_partitions >> 1 << 1;
+        int j = 0;
+        v2f vacc; vacc[0] = 0.0f; vacc[1] = 0.0f;
+        for(; j < num_partitions2; j += 2) {
+          v2f va; 
+          scalar_t a0, a1;
+          a0 = *(ptr_sm_out_ptr); ptr_sm_out_ptr += HEAD_SIZE;
+          a1 = *(ptr_sm_out_ptr); ptr_sm_out_ptr += HEAD_SIZE;
+          to_v2f(a0,a1,va);
+          v2f vb; 
+          vb[0] = shared_exp_sums[j]; vb[1] = shared_exp_sums[j + 1];
+          vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
+        }
+        acc = vacc[0] + vacc[1];
+        for (; j < num_partitions; ++j) {
+          acc += to_float(*ptr_sm_out_ptr) * shared_exp_sums[j];
+          ptr_sm_out_ptr += HEAD_SIZE;
+        }
+        from_float(out_ptr[threadIdx.x], acc);
+      }
+  }
+}
+
+// Grid: (num_heads, num_seqs, 1).
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE>
+__global__ void paged_attention_v1_kernel(
+    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank,
+    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
+    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE>(
+      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
+      v_cache, num_kv_heads, scale, block_tables, seq_lens,
+      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
+      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
+      blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step);
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE>
+__global__ void paged_attention_v1_32N_kernel(
+    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
+  paged_attention_kernel_32N<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE>(
+      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
+      v_cache, num_kv_heads, scale, block_tables, seq_lens,
+      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
+      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
+      blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step,max_num_partitions,num_heads);
+}
+
+// Grid: (num_heads, num_seqs, max_num_partitions).
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+          int PARTITION_SIZE>
+__global__ void paged_attention_v2_kernel(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank,
+    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
+    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
+      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step);
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+          int PARTITION_SIZE>
+__global__ void paged_attention_v2_32N_kernel(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
+  paged_attention_kernel_32N<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
+      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step,max_num_partitions,num_heads);
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+          int PARTITION_SIZE>
+__global__ void paged_attention_v2_kernel_final(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    int* __restrict__ block_count,          // [num_seqs, num_heads]
+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    scalar_t* __restrict__ final_out,      // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,
+    const int blockDim_x,
+    const int num_heads,
+    const int grid_dim_y, const bool count_init_once) {
+  paged_attention_kernel_32N_final<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
+      exp_sums, max_logits, block_count, tmp_out, final_out, q, k_cache, v_cache, num_kv_heads, scale,
+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step,max_num_partitions,blockDim_x, num_heads,grid_dim_y,count_init_once);
 }
 
 // Grid: (num_heads, num_seqs).
@@ -581,11 +2041,9 @@ __global__ void paged_attention_v2_reduce_kernel(
   const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);
   if (num_partitions == 1) {
     // No need to reduce. Only copy tmp_out to out.
-    scalar_t* out_ptr =
-        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
-    const scalar_t* tmp_out_ptr =
-        tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
-        head_idx * max_num_partitions * HEAD_SIZE;
+    scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+    const scalar_t* tmp_out_ptr = tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                                          + head_idx * max_num_partitions * HEAD_SIZE;
     for (int i = threadIdx.x; i < HEAD_SIZE; i += blockDim.x) {
       out_ptr[i] = tmp_out_ptr[i];
     }
@@ -604,9 +2062,8 @@ __global__ void paged_attention_v2_reduce_kernel(
 
   // Load max logits to shared memory.
   float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
-  const float* max_logits_ptr = max_logits +
-                                seq_idx * num_heads * max_num_partitions +
-                                head_idx * max_num_partitions;
+  const float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
+                                           + head_idx * max_num_partitions;
   float max_logit = -FLT_MAX;
   for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {
     const float l = max_logits_ptr[i];
@@ -635,11 +2092,9 @@ __global__ void paged_attention_v2_reduce_kernel(
   max_logit = VLLM_SHFL_SYNC(max_logit, 0);
 
   // Load rescaled exp sums to shared memory.
-  float* shared_exp_sums =
-      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
-  const float* exp_sums_ptr = exp_sums +
-                              seq_idx * num_heads * max_num_partitions +
-                              head_idx * max_num_partitions;
+  float* shared_exp_sums = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+  const float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
+                                       + head_idx * max_num_partitions;
   float global_exp_sum = 0.0f;
   for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {
     float l = shared_max_logits[i];
@@ -652,17 +2107,14 @@ __global__ void paged_attention_v2_reduce_kernel(
   const float inv_global_exp_sum = __fdividef(1.0f, global_exp_sum + 1e-6f);
 
   // Aggregate tmp_out to out.
-  const scalar_t* tmp_out_ptr =
-      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
-      head_idx * max_num_partitions * HEAD_SIZE;
-  scalar_t* out_ptr =
-      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+  const scalar_t* tmp_out_ptr = tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                                        + head_idx * max_num_partitions * HEAD_SIZE;
+  scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
 #pragma unroll
   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {
     float acc = 0.0f;
     for (int j = 0; j < num_partitions; ++j) {
-      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *
-             inv_global_exp_sum;
+      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] * inv_global_exp_sum;
     }
     from_float(out_ptr[i], acc);
   }
diff --git a/csrc/attention/attention_utils.cuh b/csrc/attention/attention_utils.cuh
index 826b0edff..b56e7f67b 100644
--- a/csrc/attention/attention_utils.cuh
+++ b/csrc/attention/attention_utils.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
  * Adapted from
  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
@@ -28,7 +29,8 @@ namespace vllm {
 
 // Q*K^T operation.
 template <int THREAD_GROUP_SIZE, typename Vec, int N>
-inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
+//inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
+__forceinline__ __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
   using A_vec = typename FloatVec<Vec>::Type;
   // Compute the parallel products for Q*K^T (treat vector lanes separately).
   A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);
@@ -49,7 +51,8 @@ inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
 template <typename T, int THREAD_GROUP_SIZE>
 struct Qk_dot {
   template <typename Vec, int N>
-  static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
+  //static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
+  static __forceinline__ __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
     return qk_dot_<THREAD_GROUP_SIZE>(q, k);
   }
 };
diff --git a/csrc/attention/dtype_float16.cuh b/csrc/attention/dtype_float16.cuh
index 3a1815f0e..bbcf457b4 100644
--- a/csrc/attention/dtype_float16.cuh
+++ b/csrc/attention/dtype_float16.cuh
@@ -1,8 +1,8 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
- * Adapted from
- * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
- * and
- * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
  * Copyright (c) 2023, The vLLM team.
  * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
  *
@@ -22,6 +22,7 @@
 
 #include "attention_generic.cuh"
 #include "dtype_float32.cuh"
+#include "cuda_fp16.h"
 
 #ifdef USE_ROCM
   #include <hip/hip_fp16.h>
@@ -32,37 +33,37 @@
 namespace vllm {
 
 // FP16 vector types for Q, K, V.
-template <>
+template<>
 struct Vec<uint16_t, 1> {
   using Type = uint16_t;
 };
-template <>
+template<>
 struct Vec<uint16_t, 2> {
   using Type = uint32_t;
 };
-template <>
+template<>
 struct Vec<uint16_t, 4> {
   using Type = uint2;
 };
-template <>
+template<>
 struct Vec<uint16_t, 8> {
   using Type = uint4;
 };
 
 // FP32 accumulator vector types corresponding to Vec.
-template <>
+template<>
 struct FloatVec<uint16_t> {
   using Type = float;
 };
-template <>
+template<>
 struct FloatVec<uint32_t> {
   using Type = float2;
 };
-template <>
+template<>
 struct FloatVec<uint2> {
   using Type = Float4_;
 };
-template <>
+template<>
 struct FloatVec<uint4> {
   using Type = Float8_;
 };
@@ -71,12 +72,16 @@ struct FloatVec<uint4> {
 inline __device__ uint32_t h0_h0(uint16_t a) {
 #ifndef USE_ROCM
   uint32_t b;
-  asm volatile("mov.b32 %0, {%1, %1};" : "=r"(b) : "h"(a));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("mov.b32 %0, {%1, %1};" : "=r"(b) : "h"(a));
+  b = a;
+  b = b << 16 | b;
   return b;
 #else
   union {
-    uint32_t u32;
-    uint16_t u16[2];
+   uint32_t u32;
+   uint16_t u16[2];
   } tmp;
   tmp.u16[0] = a;
   tmp.u16[1] = a;
@@ -87,9 +92,14 @@ inline __device__ uint32_t h0_h0(uint16_t a) {
 inline __device__ float half_to_float(uint16_t h) {
   float f;
 #ifndef USE_ROCM
-  asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
+  f = __half2float(*(__half*)&h);
 #else
-  asm volatile("v_cvt_f32_f16 %0, %1;" : "=v"(f) : "v"(h));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_cvt_f32_f16 %0, %1;" : "=v"(f) : "v"(h));
 #endif
   return f;
 }
@@ -97,7 +107,16 @@ inline __device__ float half_to_float(uint16_t h) {
 inline __device__ float2 half2_to_float2(uint32_t v) {
 #ifndef USE_ROCM
   uint16_t lo, hi;
-  asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo), "=h"(hi) : "r"(v));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo), "=h"(hi) : "r"(v));
+  union {
+    uint32_t u32;
+    uint16_t u16[2];
+  } tmp;
+  tmp.u32 = v;
+  lo = tmp.u16[0];
+  hi = tmp.u16[1];
   return make_float2(half_to_float(lo), half_to_float(hi));
 #else
   union {
@@ -118,9 +137,15 @@ inline __device__ uint16_t float_to_half(float f) {
     uint16_t u16[2];
   } tmp;
 #ifndef USE_ROCM
-  asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f));
+  __half __tmp = __float2half(f);
+  tmp.u16[0] = *(uint16_t*)&__tmp;
 #else
-  asm volatile("v_cvt_f16_f32 %0, %1;\n" : "=v"(tmp.u32) : "v"(f));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_cvt_f16_f32 %0, %1;\n" : "=v"(tmp.u32) : "v"(f));
 #endif
   return tmp.u16[0];
 }
@@ -132,12 +157,19 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
   } tmp;
 #ifndef USE_ROCM
   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
-  asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n"
-               : "=r"(tmp.u32)
-               : "f"(f.y), "f"(f.x));
+    
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n" : "=r"(tmp.u32) : "f"(f.y), "f"(f.x));
+  __half2 __tmp = __half2(__float2half(f.x), __float2half(f.y));
+  tmp.u32 = *(uint32_t*)&__tmp;
   #else
-  asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f.x));
-  asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[1]) : "f"(f.y));
+    
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f.x));
+  tmp.u16[0] = float_to_half(f.x);
+// >>>> PTX Rejuvenate Failed <<<<
+// asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[1]) : "f"(f.y));
+  tmp.u16[1] = float_to_half(f.y);
   #endif
 #else
   tmp.u16[0] = float_to_half(f.x);
@@ -150,9 +182,21 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
 inline __device__ uint16_t add(uint16_t a, uint16_t b) {
   uint16_t c;
 #ifndef USE_ROCM
-  asm volatile("add.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
+  
+// >>>> PTX Rejuvenate Success <<<<
+{
+{
+unsigned short __a=(a);
+unsigned short __b=(b);
+__half __d=__hadd(*(__half*)&__a,*(__half*)&__b);
+(c)=*(unsigned short*)&__d;
+}
+}
+
 #else
-  asm volatile("v_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
   return c;
 }
@@ -160,9 +204,21 @@ inline __device__ uint16_t add(uint16_t a, uint16_t b) {
 inline __device__ uint32_t add(uint32_t a, uint32_t b) {
   uint32_t c;
 #ifndef USE_ROCM
-  asm volatile("add.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
+  
+// >>>> PTX Rejuvenate Success <<<<
+{
+{
+unsigned int __a=(a);
+unsigned int __b=(b);
+__half2 __d=__hadd2(*(__half2*)&__a,*(__half2*)&__b);
+(c)=*(unsigned int*)&__d;
+}
+}
+
 #else
-  asm volatile("v_pk_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_pk_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
   return c;
 }
@@ -205,34 +261,58 @@ inline __device__ Float8_ add(uint4 a, Float8_ fb) {
 }
 
 // Vector multiplication.
-template <>
+template<>
 inline __device__ uint16_t mul(uint16_t a, uint16_t b) {
   uint16_t c;
 #ifndef USE_ROCM
-  asm volatile("mul.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
+  
+// >>>> PTX Rejuvenate Success <<<<
+{
+{
+unsigned short __a=(a);
+unsigned short __b=(b);
+__half __d=__hmul(*(__half*)&__a,*(__half*)&__b);
+(c)=*(unsigned short*)&__d;
+}
+}
+
 #else
-  asm volatile("v_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
   return c;
 }
 
-template <>
+template<>
 inline __device__ uint32_t mul(uint32_t a, uint32_t b) {
   uint32_t c;
 #ifndef USE_ROCM
-  asm volatile("mul.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
+  
+// >>>> PTX Rejuvenate Success <<<<
+{
+{
+unsigned int __a=(a);
+unsigned int __b=(b);
+__half2 __d=__hmul2(*(__half2*)&__a,*(__half2*)&__b);
+(c)=*(unsigned int*)&__d;
+}
+}
+
 #else
-  asm volatile("v_pk_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_pk_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
   return c;
 }
 
-template <>
+template<>
 inline __device__ uint32_t mul(uint16_t a, uint32_t b) {
   return mul<uint32_t, uint32_t, uint32_t>(h0_h0(a), b);
 }
 
-template <>
+template<>
 inline __device__ uint2 mul(uint2 a, uint2 b) {
   uint2 c;
   c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);
@@ -240,7 +320,7 @@ inline __device__ uint2 mul(uint2 a, uint2 b) {
   return c;
 }
 
-template <>
+template<>
 inline __device__ uint2 mul(uint16_t a, uint2 b) {
   uint32_t s = h0_h0(a);
   uint2 c;
@@ -249,7 +329,7 @@ inline __device__ uint2 mul(uint16_t a, uint2 b) {
   return c;
 }
 
-template <>
+template<>
 inline __device__ uint4 mul(uint4 a, uint4 b) {
   uint4 c;
   c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);
@@ -259,7 +339,7 @@ inline __device__ uint4 mul(uint4 a, uint4 b) {
   return c;
 }
 
-template <>
+template<>
 inline __device__ uint4 mul(uint16_t a, uint4 b) {
   uint32_t s = h0_h0(a);
   uint4 c;
@@ -270,26 +350,26 @@ inline __device__ uint4 mul(uint16_t a, uint4 b) {
   return c;
 }
 
-template <>
+template<>
 inline __device__ float mul(uint16_t a, uint16_t b) {
   float fa = half_to_float(a);
   float fb = half_to_float(b);
   return fa * fb;
 }
 
-template <>
+template<>
 inline __device__ float2 mul(uint32_t a, uint32_t b) {
   float2 fa = half2_to_float2(a);
   float2 fb = half2_to_float2(b);
   return mul<float2, float2, float2>(fa, fb);
 }
 
-template <>
+template<>
 inline __device__ float2 mul(uint16_t a, uint32_t b) {
   return mul<float2, uint32_t, uint32_t>(h0_h0(a), b);
 }
 
-template <>
+template<>
 inline __device__ Float4_ mul(uint2 a, uint2 b) {
   Float4_ fc;
   fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);
@@ -297,7 +377,7 @@ inline __device__ Float4_ mul(uint2 a, uint2 b) {
   return fc;
 }
 
-template <>
+template<>
 inline __device__ Float4_ mul(uint16_t a, uint2 b) {
   uint32_t s = h0_h0(a);
   Float4_ fc;
@@ -306,7 +386,7 @@ inline __device__ Float4_ mul(uint16_t a, uint2 b) {
   return fc;
 }
 
-template <>
+template<>
 inline __device__ Float8_ mul(uint4 a, uint4 b) {
   Float8_ fc;
   fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);
@@ -316,7 +396,7 @@ inline __device__ Float8_ mul(uint4 a, uint4 b) {
   return fc;
 }
 
-template <>
+template<>
 inline __device__ Float8_ mul(uint16_t a, uint4 b) {
   uint32_t s = h0_h0(a);
   Float8_ fc;
@@ -331,13 +411,22 @@ inline __device__ Float8_ mul(uint16_t a, uint4 b) {
 inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {
   uint32_t d;
 #ifndef USE_ROCM
-  asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
-               : "=r"(d)
-               : "r"(a), "r"(b), "r"(c));
+  
+// >>>> PTX Rejuvenate Success <<<<
+{
+{
+unsigned int __a=(a);
+unsigned int __b=(b);
+unsigned int __c=(c);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(d)=*(unsigned int*)&__d;
+}
+}
+
 #else
-  asm volatile("v_pk_fma_f16 %0, %1, %2, %3;\n"
-               : "=v"(d)
-               : "v"(a), "v"(b), "v"(c));
+  
+// >>>> PTX Rejuvenate Failed <<<<
+asm volatile("v_pk_fma_f16 %0, %1, %2, %3;\n" : "=v"(d) : "v"(a), "v"(b), "v"(c));
 #endif
   return d;
 }
@@ -431,24 +520,24 @@ inline __device__ Float8_ fma(uint16_t a, uint4 b, Float8_ fc) {
 }
 
 // Vector sum.
-template <>
+template<>
 inline __device__ float sum(uint16_t v) {
   return half_to_float(v);
 }
 
-template <>
+template<>
 inline __device__ float sum(uint32_t v) {
   float2 tmp = half2_to_float2(v);
   return tmp.x + tmp.y;
 }
 
-template <>
+template<>
 inline __device__ float sum(uint2 v) {
   uint32_t c = add(v.x, v.y);
   return sum(c);
 }
 
-template <>
+template<>
 inline __device__ float sum(uint4 v) {
   uint32_t c = add(v.x, v.y);
   c = add(c, v.z);
@@ -478,9 +567,13 @@ inline __device__ void from_float(uint4& dst, Float8_ src) {
 }
 
 // From float16 to float32.
-inline __device__ float to_float(uint16_t u) { return half_to_float(u); }
+inline __device__ float to_float(uint16_t u) {
+  return half_to_float(u);
+}
 
-inline __device__ float2 to_float(uint32_t u) { return half2_to_float2(u); }
+inline __device__ float2 to_float(uint32_t u) {
+  return half2_to_float2(u);
+}
 
 inline __device__ Float4_ to_float(uint2 u) {
   Float4_ tmp;
@@ -499,6 +592,8 @@ inline __device__ Float8_ to_float(uint4 u) {
 }
 
 // Zero-out a variable.
-inline __device__ void zero(uint16_t& dst) { dst = uint16_t(0); }
+inline __device__ void zero(uint16_t& dst) {
+  dst = uint16_t(0);
+}
 
-}  // namespace vllm
+} // namespace vllm
diff --git a/csrc/attention/dtype_fp8.cuh b/csrc/attention/dtype_fp8.cuh
index e714e321b..c91a1bdeb 100644
--- a/csrc/attention/dtype_fp8.cuh
+++ b/csrc/attention/dtype_fp8.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include "attention_generic.cuh"
@@ -15,6 +16,7 @@ enum class Fp8KVCacheDataType {
   kAuto = 0,
   kFp8E4M3 = 1,
   kFp8E5M2 = 2,
+  kInt8 = 3,
 };
 
 // fp8 vector types for quantization of kv cache
diff --git a/csrc/attention/paged_attention_v1.cu b/csrc/attention/paged_attention_v1.cu
index 9b3a5c4b1..36dcdc47d 100644
--- a/csrc/attention/paged_attention_v1.cu
+++ b/csrc/attention/paged_attention_v1.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
  * Adapted from
  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
@@ -41,38 +42,164 @@
           out_ptr, query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, \
           scale, block_tables_ptr, seq_lens_ptr, max_num_blocks_per_seq,    \
           alibi_slopes_ptr, q_stride, kv_block_stride, kv_head_stride,      \
-          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,      \
+          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,              \
           blocksparse_vert_stride, blocksparse_block_size,                  \
           blocksparse_head_sliding_step);
 
+#define LAUNCH_PAGED_ATTENTION_V1_32N(HEAD_SIZE)                                \
+  VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \
+      ((void*)vllm::paged_attention_v1_32N_kernel<T, CACHE_T, HEAD_SIZE,        \
+                                              BLOCK_SIZE, NUM_THREADS,      \
+                                              KV_DTYPE, IS_BLOCK_SPARSE>),  \
+      shared_mem_size);                                                     \
+  vllm::paged_attention_v1_32N_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,        \
+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE>   \
+      <<<grid, block, shared_mem_size, stream>>>(                           \
+          out_ptr, query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, \
+          scale, block_tables_ptr, seq_lens_ptr, max_num_blocks_per_seq,    \
+          alibi_slopes_ptr, q_stride, kv_block_stride, kv_head_stride,      \
+          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,                      \
+          blocksparse_vert_stride, blocksparse_block_size,                  \
+          blocksparse_head_sliding_step, 1, num_heads);
+
+template< typename scalar_t>
+__global__ void reshape_k_layout_new(scalar_t * __restrict__ k_buffer, scalar_t* k_output,int num_blocks,int num_kv_heads, int head_size,int block_size, int x,int dst_x) {
+  int k_head_stride = head_size * block_size;
+  scalar_t *ptr_k_buffer = k_buffer + blockIdx.x * k_head_stride;
+  scalar_t *ptr_output = k_output + blockIdx.x * k_head_stride;
+  for(int t = threadIdx.x; t < k_head_stride; t += blockDim.x) {
+    int heightId = t / (block_size * dst_x);
+    int remain = t % (block_size * dst_x);
+    int blockId = remain / dst_x;
+    int wId = remain % dst_x;
+    int inId = heightId * dst_x + wId;
+    int in_y = inId / x;
+    int in_x = inId % x;
+    int inIndex = in_y  * block_size * x + blockId * x + in_x;
+    ptr_output[t] = ptr_k_buffer[inIndex];
+  }
+}
+// [num_blocks, num_kv_heads, head_size, block_size] -->   [num_blocks,  num_kv_heads, block_size,head_size]
+template<typename scalar_t>
+__global__ void reshape_v_layout(scalar_t * __restrict__ v_buffer, scalar_t* v_output,int num_blocks,int num_kv_heads, int head_size,int block_size) {
+      int v_block_stride = head_size * block_size * num_kv_heads;
+      int v_head_stride = head_size * block_size;
+      scalar_t *ptr_in = v_buffer + blockIdx.x * v_block_stride;
+      scalar_t *ptr_output = v_output + blockIdx.x * v_block_stride;
+      for(int t = threadIdx.x; t < v_block_stride; t += blockDim.x) {
+        int num_kv_headIdx = t / v_head_stride;
+        int remain = t % v_head_stride;
+        int headId_H = remain / block_size;
+        remain = remain % block_size;
+        int out_idx = num_kv_headIdx * head_size * block_size + remain * head_size + headId_H;
+        ptr_output[out_idx] = ptr_in[t];
+      }
+}
+
+template<
+  typename CACHE_T,
+  int BLOCK_SIZE>
+void reshape_kv_cache(
+  torch::Tensor& key_cache,
+  torch::Tensor& value_cache,
+  torch::Tensor& key_cache_new_layer,
+  torch::Tensor& value_cache_new_layer,
+  int num_seqs,
+  int num_heads,
+  int head_size,
+  int num_kv_heads) {
+  int kv_block_stride = key_cache.stride(0); // NU ,BLC ,HEAD, HEAD_DIM
+  int kv_head_stride = key_cache.stride(1);
+
+  CACHE_T* key_cache_ptr = reinterpret_cast<CACHE_T*>(key_cache.data_ptr());
+  CACHE_T* value_cache_ptr = reinterpret_cast<CACHE_T*>(value_cache.data_ptr());
+  CACHE_T* key_cache_tmp = reinterpret_cast<CACHE_T*>(key_cache_new_layer.data_ptr());
+  CACHE_T* value_cache_tmp = reinterpret_cast<CACHE_T*>(value_cache_new_layer.data_ptr());
+
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  reshape_k_layout_new<CACHE_T><<<dim3(key_cache.size(0)*num_kv_heads,1,1),dim3(256,1,1),0,stream>>>(key_cache_ptr,key_cache_tmp,key_cache.size(0),num_kv_heads,head_size,BLOCK_SIZE,8,16);
+  reshape_v_layout<CACHE_T><<<dim3(key_cache.size(0),1,1),dim3(256,1,1),0,stream>>>(value_cache_ptr,value_cache_tmp,key_cache.size(0),num_kv_heads,head_size,BLOCK_SIZE);
+}
+#define CALL_RESHAPE_LAUNCHER(CACHE_T, BLOCK_SIZE)       \
+  reshape_kv_cache<CACHE_T, BLOCK_SIZE>( \
+    key_cache,                                                               \
+    value_cache,                                                             \
+    key_cache_new_layer,                                                     \
+    value_cache_new_layer,                                                   \
+    num_seqs,\
+    num_heads,\
+    head_size,\
+    num_kv_heads);
+
+#define CALL_RESHAPE_BLOCK_SIZE(CACHE_T) \
+  switch (block_size) {                                               \
+    case 8:                                                           \
+      CALL_RESHAPE_LAUNCHER(CACHE_T, 8);          \
+      break;                                                          \
+    case 16:                                                          \
+      CALL_RESHAPE_LAUNCHER(CACHE_T, 16);         \
+      break;                                                          \
+    case 32:                                                          \
+      CALL_RESHAPE_LAUNCHER(CACHE_T, 32);         \
+      break;                                                          \
+    default:                                                          \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size);     \
+      break;                                                          \
+  }
+void page_reshape_kv_cache(
+  torch::Tensor& key_cache,       // [num_blocks, num_heads, head_size/x, block_size, x]
+  torch::Tensor& value_cache,     // [num_blocks, num_heads, head_size, block_size]
+  torch::Tensor& key_cache_new_layer, //[num_blocks, num_heads, head_size/16, block_size, 16]
+  torch::Tensor& value_cache_new_layer,//[num_blocks, num_heads, block_size, head_size]
+  int64_t num_seqs,
+  int64_t num_heads,
+  int64_t head_size,
+  int64_t num_kv_heads,               // [num_heads]
+  int64_t block_size,
+  const std::string& kv_cache_dtype) {
+  if (kv_cache_dtype == "auto") {
+    if (sizeof(key_cache.dtype())==4) {
+      CALL_RESHAPE_BLOCK_SIZE(float);
+    } else if (sizeof(key_cache.dtype()) == 2) {
+      CALL_RESHAPE_BLOCK_SIZE(uint16_t);
+    } else {
+      TORCH_CHECK(false, "Unsupported data type: ", key_cache.dtype());
+    }
+  }  else {
+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
+  }
+}
+
+
 // TODO(woosuk): Tune NUM_THREADS.
 template <typename T, typename CACHE_T, int BLOCK_SIZE,
           vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,
-          int NUM_THREADS = 128>
+          int NUM_THREADS = 256>
 void paged_attention_v1_launcher(
     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int num_kv_heads, float scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
-    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
-    torch::Tensor& v_scale, const int tp_rank,
-    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+    const c10::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
+    torch::Tensor& v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step) {
   int num_seqs = query.size(0);
   int num_heads = query.size(1);
   int head_size = query.size(2);
   int max_num_blocks_per_seq = block_tables.size(1);
-  int q_stride = query.stride(0);
-  int kv_block_stride = key_cache.stride(0);
+  int q_stride = query.stride(0);  //num head head_dim 
+  int kv_block_stride = key_cache.stride(0);   // NU ,BLC ,HEAD, HEAD_DIM
   int kv_head_stride = key_cache.stride(1);
 
-  [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
   assert(head_size % thread_group_size == 0);
+  assert((head_size & 7) == 0);
 
   // NOTE: alibi_slopes is optional.
-  const float* alibi_slopes_ptr =
-      alibi_slopes
-          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
-          : nullptr;
+  const float* alibi_slopes_ptr = alibi_slopes ?
+    reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+    : nullptr;
 
   T* out_ptr = reinterpret_cast<T*>(out.data_ptr());
   T* query_ptr = reinterpret_cast<T*>(query.data_ptr());
@@ -84,10 +211,12 @@ void paged_attention_v1_launcher(
   const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());
 
   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
-  int padded_max_seq_len =
-      DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;
+  int padded_max_seq_len = DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;
   int logits_size = padded_max_seq_len * sizeof(float);
-  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  int V_VEC_SIZE = 16 / sizeof(CACHE_T); 
+  int NUM_V_VECS_PER_THREAD = head_size / V_VEC_SIZE; 
+  int NUM_COLS_PER_ITER = MAX(WARP_SIZE / NUM_V_VECS_PER_THREAD, 1); 
+  int outputs_size = NUM_WARPS * head_size * sizeof(float) * NUM_COLS_PER_ITER;
   // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len
   // Keep that in sync with the logic here!
   int shared_mem_size = std::max(logits_size, outputs_size);
@@ -104,7 +233,7 @@ void paged_attention_v1_launcher(
       LAUNCH_PAGED_ATTENTION_V1(32);
       break;
     case 64:
-      LAUNCH_PAGED_ATTENTION_V1(64);
+      LAUNCH_PAGED_ATTENTION_V1_32N(64);
       break;
     case 80:
       LAUNCH_PAGED_ATTENTION_V1(80);
@@ -119,13 +248,15 @@ void paged_attention_v1_launcher(
       LAUNCH_PAGED_ATTENTION_V1(120);
       break;
     case 128:
-      LAUNCH_PAGED_ATTENTION_V1(128);
+      LAUNCH_PAGED_ATTENTION_V1_32N(128);
       break;
+    case 160:
+      LAUNCH_PAGED_ATTENTION_V1(160);
     case 192:
       LAUNCH_PAGED_ATTENTION_V1(192);
       break;
     case 256:
-      LAUNCH_PAGED_ATTENTION_V1(256);
+      LAUNCH_PAGED_ATTENTION_V1_32N(256);
       break;
     default:
       TORCH_CHECK(false, "Unsupported head size: ", head_size);
@@ -178,10 +309,9 @@ void paged_attention_v1(
     torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]
     torch::Tensor& seq_lens,      // [num_seqs]
     int64_t block_size, int64_t max_seq_len,
-    const std::optional<torch::Tensor>& alibi_slopes,
-    const std::string& kv_cache_dtype, torch::Tensor& k_scale,
-    torch::Tensor& v_scale, const int64_t tp_rank,
-    const int64_t blocksparse_local_blocks,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype, torch::Tensor& k_scale, torch::Tensor& v_scale,
+    const int64_t tp_rank, const int64_t blocksparse_local_blocks,
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
     const int64_t blocksparse_head_sliding_step) {
   const bool is_block_sparse = (blocksparse_vert_stride > 1);
@@ -193,4 +323,4 @@ void paged_attention_v1(
 #undef WARP_SIZE
 #undef MAX
 #undef MIN
-#undef DIVIDE_ROUND_UP
\ No newline at end of file
+#undef DIVIDE_ROUND_UP
diff --git a/csrc/attention/paged_attention_v2.cu b/csrc/attention/paged_attention_v2.cu
index 9935359e0..ebacff6b2 100644
--- a/csrc/attention/paged_attention_v2.cu
+++ b/csrc/attention/paged_attention_v2.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
  * Adapted from
  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
@@ -37,7 +38,7 @@
           exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \
           value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \
           seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \
-          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,  \
+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,          \
           blocksparse_local_blocks, blocksparse_vert_stride,                   \
           blocksparse_block_size, blocksparse_head_sliding_step);              \
   vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \
@@ -46,18 +47,52 @@
           out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
           max_num_partitions);
 
+#define LAUNCH_PAGED_ATTENTION_V2_32N(HEAD_SIZE)                                   \
+  vllm::paged_attention_v2_32N_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \
+                                  PARTITION_SIZE>                              \
+      <<<grid, block, shared_mem_size, stream>>>(                              \
+          exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \
+          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \
+          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \
+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,                  \
+          blocksparse_local_blocks, blocksparse_vert_stride,                   \
+          blocksparse_block_size, blocksparse_head_sliding_step, max_num_partitions, num_heads);              \
+  vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \
+                                         PARTITION_SIZE>                       \
+      <<<reduce_grid, block, reduce_shared_mem_size, stream>>>(                \
+          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
+          max_num_partitions);
+
+#define LAUNCH_PAGED_ATTENTION_V2_FINAL(HEAD_SIZE)                                   \
+  vllm::paged_attention_v2_kernel_final<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \
+                                  PARTITION_SIZE>                              \
+      <<<grid, block, shared_mem_size, stream>>>(                              \
+          exp_sums_ptr, max_logits_ptr, block_count_ptr, tmp_out_ptr, out_ptr, query_ptr, key_cache_ptr, \
+          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,                   \
+          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,         \
+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,                        \
+          blocksparse_local_blocks, blocksparse_vert_stride,                        \
+          blocksparse_block_size, blocksparse_head_sliding_step,max_num_partitions, \
+          NUM_THREADS,                                                              \
+          num_heads,                                                                \
+          num_seqs,                                                                 \
+          count_init_once                                                           \
+	  );
+
 template <typename T, typename CACHE_T, int BLOCK_SIZE,
           vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,
-          int NUM_THREADS = 128, int PARTITION_SIZE = 512>
+          int NUM_THREADS = 256, int PARTITION_SIZE = 512>
 void paged_attention_v2_launcher(
     torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,
-    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
+    torch::Tensor& tmp_out, torch::Tensor& block_count, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int num_kv_heads, float scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
-    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
-    torch::Tensor& v_scale, const int tp_rank,
-    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+    const c10::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
+    torch::Tensor& v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step, const bool count_init_once) {
   int num_seqs = query.size(0);
   int num_heads = query.size(1);
   int head_size = query.size(2);
@@ -66,14 +101,14 @@ void paged_attention_v2_launcher(
   int kv_block_stride = key_cache.stride(0);
   int kv_head_stride = key_cache.stride(1);
 
-  [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
   assert(head_size % thread_group_size == 0);
+  assert((head_size & 7) == 0);
 
   // NOTE: alibi_slopes is optional.
-  const float* alibi_slopes_ptr =
-      alibi_slopes
-          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
-          : nullptr;
+  const float* alibi_slopes_ptr = alibi_slopes ?
+    reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+    : nullptr;
 
   T* out_ptr = reinterpret_cast<T*>(out.data_ptr());
   float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());
@@ -86,15 +121,20 @@ void paged_attention_v2_launcher(
   int* seq_lens_ptr = seq_lens.data_ptr<int>();
   const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());
   const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());
+  int* block_count_ptr = block_count.data_ptr<int>();
 
   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
   int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);
   int logits_size = PARTITION_SIZE * sizeof(float);
-  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  int V_VEC_SIZE = 16 / sizeof(CACHE_T);
+  int NUM_V_VECS_PER_THREAD = head_size / V_VEC_SIZE;
+  int NUM_COLS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_THREAD;
+  int outputs_size = NUM_WARPS * head_size * sizeof(float) * NUM_COLS_PER_ITER;
 
   // For paged attention v2 kernel.
   dim3 grid(num_heads, num_seqs, max_num_partitions);
-  int shared_mem_size = std::max(logits_size, outputs_size);
+  int shared_mem_size = std::max(std::max(logits_size, outputs_size) + (2 * NUM_WARPS + 512 + (NUM_WARPS<<1))*sizeof(float),
+     (2 * max_num_partitions + 2 * NUM_WARPS + max_num_partitions * head_size)*sizeof(float));
   // For paged attention v2 reduce kernel.
   dim3 reduce_grid(num_heads, num_seqs);
   int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
@@ -110,7 +150,7 @@ void paged_attention_v2_launcher(
       LAUNCH_PAGED_ATTENTION_V2(32);
       break;
     case 64:
-      LAUNCH_PAGED_ATTENTION_V2(64);
+      LAUNCH_PAGED_ATTENTION_V2_FINAL(64);
       break;
     case 80:
       LAUNCH_PAGED_ATTENTION_V2(80);
@@ -125,13 +165,15 @@ void paged_attention_v2_launcher(
       LAUNCH_PAGED_ATTENTION_V2(120);
       break;
     case 128:
-      LAUNCH_PAGED_ATTENTION_V2(128);
+      LAUNCH_PAGED_ATTENTION_V2_FINAL(128);
       break;
+    case 160:
+      LAUNCH_PAGED_ATTENTION_V2(160);
     case 192:
       LAUNCH_PAGED_ATTENTION_V2(192);
       break;
     case 256:
-      LAUNCH_PAGED_ATTENTION_V2(256);
+      LAUNCH_PAGED_ATTENTION_V2_FINAL(256);
       break;
     default:
       TORCH_CHECK(false, "Unsupported head size: ", head_size);
@@ -142,11 +184,11 @@ void paged_attention_v2_launcher(
 #define CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, KV_DTYPE, IS_BLOCK_SPARSE)   \
   paged_attention_v2_launcher<T, CACHE_T, BLOCK_SIZE, KV_DTYPE,               \
                               IS_BLOCK_SPARSE>(                               \
-      out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,      \
+      out, exp_sums, max_logits, tmp_out, block_count, query, key_cache, value_cache,      \
       num_kv_heads, scale, block_tables, seq_lens, max_seq_len, alibi_slopes, \
       k_scale, v_scale, tp_rank, blocksparse_local_blocks,                    \
       blocksparse_vert_stride, blocksparse_block_size,                        \
-      blocksparse_head_sliding_step);
+      blocksparse_head_sliding_step, count_init_once);
 
 #define CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE) \
   if (is_block_sparse) {                                                   \
@@ -179,6 +221,7 @@ void paged_attention_v2(
     torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]
     torch::Tensor&
         tmp_out,  // [num_seqs, num_heads, max_num_partitions, head_size]
+    torch::Tensor& block_count,     // [num_seqs, num_heads]
     torch::Tensor& query,  // [num_seqs, num_heads, head_size]
     torch::Tensor&
         key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]
@@ -189,18 +232,16 @@ void paged_attention_v2(
     torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]
     torch::Tensor& seq_lens,      // [num_seqs]
     int64_t block_size, int64_t max_seq_len,
-    const std::optional<torch::Tensor>& alibi_slopes,
-    const std::string& kv_cache_dtype, torch::Tensor& k_scale,
-    torch::Tensor& v_scale, const int64_t tp_rank,
-    const int64_t blocksparse_local_blocks,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype, torch::Tensor& k_scale, torch::Tensor& v_scale,
+    const int64_t tp_rank, const int64_t blocksparse_local_blocks,
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
-    const int64_t blocksparse_head_sliding_step) {
+    const int64_t blocksparse_head_sliding_step, bool count_init_once) {
   const bool is_block_sparse = (blocksparse_vert_stride > 1);
-  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,
-                             CALL_V2_LAUNCHER_BLOCK_SIZE)
+  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype, CALL_V2_LAUNCHER_BLOCK_SIZE)
 }
 
 #undef WARP_SIZE
 #undef MAX
 #undef MIN
-#undef DIVIDE_ROUND_UP
\ No newline at end of file
+#undef DIVIDE_ROUND_UP
diff --git a/csrc/cache.h b/csrc/cache.h
index cf4a65c29..7e157c0fc 100644
--- a/csrc/cache.h
+++ b/csrc/cache.h
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include <torch/all.h>
@@ -24,6 +25,16 @@ void reshape_and_cache(torch::Tensor& key, torch::Tensor& value,
                        const std::string& kv_cache_dtype,
                        torch::Tensor& k_scale, torch::Tensor& v_scale);
 
+void reshape_and_cache_new(
+		torch::Tensor& key,
+		torch::Tensor& value,
+                torch::Tensor& key_cache,
+		torch::Tensor& value_cache,
+                torch::Tensor& slot_mapping,
+                const std::string& kv_cache_dtype,
+                const double k_scale,
+		const double v_scale);
+
 void reshape_and_cache_flash(torch::Tensor& key, torch::Tensor& value,
                              torch::Tensor& key_cache,
                              torch::Tensor& value_cache,
diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu
index 0960888d1..98a5858e1 100644
--- a/csrc/cache_kernels.cu
+++ b/csrc/cache_kernels.cu
@@ -1,9 +1,11 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <torch/all.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
 
 #include "cuda_compat.h"
 #include "dispatch_utils.h"
+#include "quantization/fused_kernels/quant_conversions.cuh"
 
 #ifdef USE_ROCM
   #include "quantization/fp8/amd/quant_utils.cuh"
@@ -260,6 +262,118 @@ __global__ void reshape_and_cache_kernel(
   }
 }
 
+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
+__global__ void reshape_and_cache_kernel_layout(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,
+                                         // block_size, x]
+    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,
+                                         // block_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride, const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const float kv_scale) {
+  const int64_t token_idx = blockIdx.x;
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = threadIdx.x; i < n; i += blockDim.x) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * (head_size / x) * block_size * x +
+        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + block_offset * head_size +
+        head_offset;
+    scalar_t tgt_key = key[src_key_idx];
+    scalar_t tgt_value = value[src_value_idx];
+    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
+      key_cache[tgt_key_idx] = tgt_key;
+      value_cache[tgt_value_idx] = tgt_value;
+    } else {
+      key_cache[tgt_key_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, kv_scale);
+      value_cache[tgt_value_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, kv_scale);
+    }
+  }
+}
+
+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
+__global__ void reshape_and_cache_kernel_layout_opt(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,
+                                         // block_size, x]
+    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,
+                                         // block_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride, const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const float kv_scale) {
+  const int64_t token_idx = blockIdx.x;
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+  const int n = num_heads * head_size / 8;
+  for (int t = threadIdx.x; t < n; t += blockDim.x) {
+    int i = t << 3;
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + block_offset * head_size +
+        head_offset;
+
+    *(float4*)(key_cache + tgt_key_idx) = *(float4*)(key + src_key_idx);
+    *(float4*)(value_cache + tgt_value_idx) = *(float4*)(value + src_value_idx);
+   /*
+    scalar_t tgt_key = key[src_key_idx];
+    scalar_t tgt_value = value[src_value_idx];
+    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
+      key_cache[tgt_key_idx] = tgt_key;
+      value_cache[tgt_value_idx] = tgt_value;
+    } else {
+      key_cache[tgt_key_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, kv_scale);
+      value_cache[tgt_value_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, kv_scale);
+    }
+    */
+  }
+}
+
 template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
 __global__ void reshape_and_cache_flash_kernel(
     const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
@@ -336,6 +450,8 @@ __global__ void concat_and_cache_mla_kernel(
           block_idx * block_stride + block_offset * entry_stride + i + offset;
       if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
         dst[dst_idx] = src[src_idx];
+      } else if (kv_dt == Fp8KVCacheDataType::kInt8) {
+        dst[dst_idx] = scaled_quant_to_int8<scalar_t>(src[src_idx], scale);
       } else {
         dst[dst_idx] =
             fp8::scaled_convert<cache_t, scalar_t, kv_dt>(src[src_idx], *scale);
@@ -392,6 +508,83 @@ void reshape_and_cache(
                              CALL_RESHAPE_AND_CACHE)
 }
 
+#define CALL_RESHAPE_AND_CACHE_LAYOUT(KV_T, CACHE_T, KV_DTYPE)               \
+  vllm::reshape_and_cache_kernel_layout<KV_T, CACHE_T, KV_DTYPE>             \
+      <<<grid, block, 0, stream>>>(                                   \
+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
+          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \
+          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \
+          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \
+          num_heads, head_size, block_size, x, kv_scale);
+
+#define CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(KV_T, CACHE_T, KV_DTYPE)               \
+  vllm::reshape_and_cache_kernel_layout_opt<KV_T, CACHE_T, KV_DTYPE>             \
+      <<<grid, block, 0, stream>>>(                                   \
+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
+          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \
+          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \
+          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \
+          num_heads, head_size, block_size, x, kv_scale);
+
+void reshape_and_cache_new(
+    torch::Tensor& key,    // [num_tokens, num_heads, head_size]
+    torch::Tensor& value,  // [num_tokens, num_heads, head_size]
+    torch::Tensor&
+        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]
+    torch::Tensor&
+        value_cache,  // [num_blocks, num_heads, head_size, block_size]
+    torch::Tensor& slot_mapping,  // [num_tokens]
+    const std::string& kv_cache_dtype, const double kv_scale, const double v_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(3);
+  int x = key_cache.size(4);
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  dim3 grid(num_tokens);
+  dim3 block(std::min(num_heads * head_size, 512));
+  const at::cuda::OptionalCUDAGuard device_guard(device_of(key));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  if (kv_cache_dtype == "auto") {
+    if (key.dtype() == at::ScalarType::Float) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(float, float, vllm::Fp8KVCacheDataType::kAuto);
+    } else if (key.dtype() == at::ScalarType::Half) {
+      if((x & 7) == 0 && (head_size & 7) == 0) {
+        CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(half, half, vllm::Fp8KVCacheDataType::kAuto);
+      } else {
+        CALL_RESHAPE_AND_CACHE_LAYOUT(half, half, vllm::Fp8KVCacheDataType::kAuto);
+      }
+    }
+    else if (key.dtype() == at::ScalarType::BFloat16) {
+      if((x & 7) == 0 && (head_size & 7) == 0) {
+        CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);
+      } else {
+        CALL_RESHAPE_AND_CACHE_LAYOUT(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);
+      }
+    }
+  }
+ /*
+  else if (kv_cache_dtype == "fp8_e5m2") {
+    if (key.dtype() == at::ScalarType::Float) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(float, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
+    } else if (key.dtype() == at::ScalarType::Half) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
+    } else if (key.dtype() == at::ScalarType::BFloat16) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
+    }
+  }
+  */
+  else {
+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
+  }
+}
+
 // KV_T is the stored data type of kv-cache.
 // CACHE_T is the data type of key and value tensors.
 // KV_DTYPE is the real data type of kv-cache.
diff --git a/csrc/core/scalar_type.hpp b/csrc/core/scalar_type.hpp
index c2ae554c9..f743d3d31 100644
--- a/csrc/core/scalar_type.hpp
+++ b/csrc/core/scalar_type.hpp
@@ -1,6 +1,8 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 // For TORCH_CHECK
+#include <variant>
 #include <torch/library.h>
 
 namespace vllm {
diff --git a/csrc/cpu/attention.cpp b/csrc/cpu/attention.cpp
index b9764056e..0f75e6657 100644
--- a/csrc/cpu/attention.cpp
+++ b/csrc/cpu/attention.cpp
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include "cpu_types.hpp"
 
 namespace {
@@ -386,7 +387,7 @@ void paged_attention_v1_impl_launcher(
     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int num_kv_heads, float scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
-    const std::optional<torch::Tensor>& alibi_slopes) {
+    const c10::optional<torch::Tensor>& alibi_slopes) {
   int num_seqs = query.size(0);
   int num_heads = query.size(1);
   int head_size = query.size(2);
@@ -459,7 +460,7 @@ void paged_attention_v1(
     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
-    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
     torch::Tensor& v_scale, const int64_t tp_rank,
     const int64_t blocksparse_local_blocks,
@@ -702,7 +703,7 @@ void paged_attention_v2_impl_launcher(
     torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int num_kv_heads, float scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int block_size,
-    int max_seq_len, const std::optional<torch::Tensor>& alibi_slopes) {
+    int max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes) {
   int num_seqs = query.size(0);
   int num_heads = query.size(1);
   int head_size = query.size(2);
@@ -781,7 +782,7 @@ void paged_attention_v2(
     torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
-    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
     torch::Tensor& v_scale, const int64_t tp_rank,
     const int64_t blocksparse_local_blocks,
@@ -795,4 +796,4 @@ void paged_attention_v2(
                                  CALL_V2_KERNEL_LAUNCHER_BLOCK_SIZE(scalar_t);
                                  CPU_KERNEL_GUARD_OUT(paged_attention_v2_impl)
                                });
-}
\ No newline at end of file
+}
diff --git a/csrc/cpu/quant.cpp b/csrc/cpu/quant.cpp
index 33b163783..27f300897 100644
--- a/csrc/cpu/quant.cpp
+++ b/csrc/cpu/quant.cpp
@@ -1,3 +1,5 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+
 #include "cpu_types.hpp"
 #include "dnnl_helper.hpp"
 
@@ -561,7 +563,7 @@ void int8_scaled_mm_azp(torch::Tensor& c,        // [M, OC], row-major
 void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
                               const torch::Tensor& input,  // [..., hidden_size]
                               const torch::Tensor& scale,
-                              std::optional<torch::Tensor> const& azp) {
+                              c10::optional<torch::Tensor> const& azp) {
   CPU_KERNEL_GUARD_IN(static_scaled_int8_quant)
   TORCH_CHECK(input.is_contiguous());
   TORCH_CHECK(out.is_contiguous());
@@ -590,7 +592,7 @@ void dynamic_scaled_int8_quant(
     torch::Tensor& out,          // [..., hidden_size]
     const torch::Tensor& input,  // [..., hidden_size]
     torch::Tensor& scale,        // [..., 1]
-    std::optional<torch::Tensor> const& azp) {
+    c10::optional<torch::Tensor> const& azp) {
   CPU_KERNEL_GUARD_IN(dynamic_scaled_int8_quant)
   TORCH_CHECK(input.is_contiguous());
   TORCH_CHECK(out.is_contiguous());
diff --git a/csrc/cuda_compat.h b/csrc/cuda_compat.h
index 82e55613d..753909903 100644
--- a/csrc/cuda_compat.h
+++ b/csrc/cuda_compat.h
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #ifdef USE_ROCM
@@ -47,3 +48,17 @@
   #define VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(FUNC, VAL) \
     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)
 #endif
+
+#define MXWARP_SIZE 64
+#ifndef USE_ROCM
+  #define MXVLLM_SHFL_SYNC(var, src_lane) __shfl_sync(uint64_t(-1), var, src_lane)
+#else
+  #define MXVLLM_SHFL_SYNC(var, src_lane) __shfl(var, src_lane)
+#endif
+
+#ifndef USE_ROCM
+  #define MXVLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor_sync(uint64_t(-1), var, lane_mask)
+#else
+  #define MXVLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor(var, lane_mask)
+#endif
+
diff --git a/csrc/cumem_allocator.cpp b/csrc/cumem_allocator.cpp
index e8555d853..3297fdb36 100644
--- a/csrc/cumem_allocator.cpp
+++ b/csrc/cumem_allocator.cpp
@@ -1,10 +1,9 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 // A CUDAPluggableAllocator based on cumem* APIs.
 // Important: allocation size, CUdeviceptr and CUmemGenericAllocationHandle*
 // need to be unsigned long long
 #include <iostream>
 
-extern "C" {
-
 #define PY_SSIZE_T_CLEAN
 #include <Python.h>
 
@@ -12,6 +11,8 @@ extern "C" {
 #include <cuda_runtime_api.h>
 #include <cuda.h>
 
+extern "C" {
+
 #define CUDA_CHECK(condition)                                                  \
   do {                                                                         \
     CUresult error = condition;                                                \
diff --git a/csrc/custom_all_reduce.cuh b/csrc/custom_all_reduce.cuh
index b9df4ed16..acfe506e9 100644
--- a/csrc/custom_all_reduce.cuh
+++ b/csrc/custom_all_reduce.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include <cuda.h>
@@ -38,9 +39,10 @@ struct Signal {
   alignas(128) FlagType peer_counter[2][kMaxBlocks][8];
 };
 
-struct __align__(16) RankData {
-  const void* __restrict__ ptrs[8];
-};
+//struct __align__(16) RankData {
+//  const void* __restrict__ ptrs[8];
+//};
+struct __align__(16) RankData { const void* ptrs[8]; };
 
 struct __align__(16) RankSignals {
   Signal* signals[8];
@@ -135,6 +137,7 @@ DINLINE O downcast(array_t<float, O::size> val) {
 }
 
 static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
+#ifdef MX_MACA
 #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
   asm volatile("st.release.sys.global.u32 [%1], %0;" ::"r"(flag),
                "l"(flag_addr));
@@ -142,10 +145,12 @@ static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
   asm volatile("membar.sys; st.volatile.global.u32 [%1], %0;" ::"r"(flag),
                "l"(flag_addr));
 #endif
+#endif
 }
 
 static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
   FlagType flag;
+#ifdef MX_MACA
 #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
   asm volatile("ld.acquire.sys.global.u32 %0, [%1];"
                : "=r"(flag)
@@ -156,17 +161,22 @@ static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
                : "l"(flag_addr));
 #endif
   return flag;
+#endif
 }
 
 static DINLINE void st_flag_volatile(FlagType* flag_addr, FlagType flag) {
+#ifdef MX_MACA
   asm volatile("st.volatile.global.u32 [%1], %0;" ::"r"(flag), "l"(flag_addr));
+#endif
 }
 
 static DINLINE FlagType ld_flag_volatile(FlagType* flag_addr) {
   FlagType flag;
+#ifdef MX_MACA
   asm volatile("ld.volatile.global.u32 %0, [%1];"
                : "=r"(flag)
                : "l"(flag_addr));
+#endif
   return flag;
 }
 
@@ -281,7 +291,7 @@ __global__ void __launch_bounds__(512, 1)
 
 using IPC_KEY = std::array<uint8_t, sizeof(cudaIpcMemHandle_t)>;
 static_assert(sizeof(IPC_KEY) == sizeof(cudaIpcMemHandle_t));
-static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));
+//static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));
 
 class CustomAllreduce {
  public:
diff --git a/csrc/cutlass_extensions/common.hpp b/csrc/cutlass_extensions/common.hpp
index febc4eccd..c0a9dd531 100644
--- a/csrc/cutlass_extensions/common.hpp
+++ b/csrc/cutlass_extensions/common.hpp
@@ -1,6 +1,7 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
-#include "cutlass/cutlass.h"
+#include "mctlass/mctlass.h"
 #include <climits>
 #include "cuda_runtime.h"
 #include <iostream>
@@ -10,9 +11,9 @@
  */
 #define CUTLASS_CHECK(status)                       \
   {                                                 \
-    cutlass::Status error = status;                 \
-    TORCH_CHECK(error == cutlass::Status::kSuccess, \
-                cutlassGetStatusString(error));     \
+    mctlass::Status error = status;                 \
+    TORCH_CHECK(error == mctlass::Status::kSuccess, \
+                mctlassGetStatusString(error));     \
   }
 
 /**
@@ -43,9 +44,9 @@ int32_t get_sm_version_num();
 template <typename Kernel>
 struct enable_sm90_or_later : Kernel {
   template <typename... Args>
-  CUTLASS_DEVICE void operator()(Args&&... args) {
+  MCTLASS_DEVICE void operator()(Args&&... args) {
 #if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 900
     Kernel::operator()(std::forward<Args>(args)...);
 #endif
   }
-};
\ No newline at end of file
+};
diff --git a/csrc/mamba/causal_conv1d/causal_conv1d.cu b/csrc/mamba/causal_conv1d/causal_conv1d.cu
index f0e5533bc..beeea22a4 100644
--- a/csrc/mamba/causal_conv1d/causal_conv1d.cu
+++ b/csrc/mamba/causal_conv1d/causal_conv1d.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 // clang-format off
 // adapted from https://github.com/Dao-AILab/causal-conv1d/blob/main/csrc/causal_conv1d_fwd.cu 
 // and https://github.com/Dao-AILab/causal-conv1d/blob/main/csrc/causal_conv1d_update.cu
@@ -53,12 +54,12 @@ void set_conv_params_fwd(ConvParamsBase &params,
                          const at::Tensor x,
                          const at::Tensor weight,
                          const at::Tensor out,
-                         const std::optional<at::Tensor>& bias,
+                         const c10::optional<at::Tensor>& bias,
                          bool silu_activation,
                          int64_t pad_slot_id,
-                         const std::optional<at::Tensor>& query_start_loc = std::nullopt,
-                         const std::optional<at::Tensor>& cache_indices = std::nullopt,
-                         const std::optional<at::Tensor>& has_initial_state = std::nullopt) {
+                         const c10::optional<at::Tensor>& query_start_loc = at::nullopt,
+                         const c10::optional<at::Tensor>& cache_indices = at::nullopt,
+                         const c10::optional<at::Tensor>& has_initial_state = at::nullopt) {
 
     // Reset the parameters
     memset(&params, 0, sizeof(params));
@@ -93,11 +94,11 @@ void set_conv_params_fwd(ConvParamsBase &params,
 
 
 void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
-                  const std::optional<at::Tensor> &bias_,
-                  const std::optional<at::Tensor> &conv_states,
-                  const std::optional<at::Tensor> &query_start_loc,
-                  const std::optional<at::Tensor> &cache_indices,
-                  const std::optional<at::Tensor> &has_initial_state,
+                  const c10::optional<at::Tensor> &bias_,
+                  const c10::optional<at::Tensor> &conv_states,
+                  const c10::optional<at::Tensor> &query_start_loc,
+                  const c10::optional<at::Tensor> &cache_indices,
+                  const c10::optional<at::Tensor> &has_initial_state,
                   bool silu_activation,
                  // used to identify padding entries if cache_indices provided
                  // in case of padding, the kernel will return early
@@ -183,7 +184,7 @@ void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_fwd", [&] {
             causal_conv1d_fwd_cuda<input_t, weight_t>(params, stream);
@@ -194,10 +195,10 @@ void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
 void causal_conv1d_update(const at::Tensor &x,
                      const at::Tensor &conv_state,
                      const at::Tensor &weight,
-                     const std::optional<at::Tensor> &bias_,
+                     const c10::optional<at::Tensor> &bias_,
                      bool silu_activation,
-                     const std::optional<at::Tensor> &cache_seqlens_,
-                     const std::optional<at::Tensor> &conv_state_indices_,
+                     const c10::optional<at::Tensor> &cache_seqlens_,
+                     const c10::optional<at::Tensor> &conv_state_indices_,
                      // used to identify padding entries if cache_indices provided
                      // in case of padding, the kernel will return early
                      int64_t pad_slot_id) {
@@ -276,7 +277,7 @@ void causal_conv1d_update(const at::Tensor &x,
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_update", [&] {
             causal_conv1d_update_cuda<input_t, weight_t>(params, stream);
diff --git a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
index bd0a34119..a479369fb 100644
--- a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
+++ b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 // clang-format off
 // adapted from https://github.com/state-spaces/mamba/blob/main/csrc/selective_scan/selective_scan_fwd_kernel.cuh
 #include <torch/all.h>
@@ -332,7 +333,7 @@ void selective_scan_fwd_launch(SSMParamsBase &params, cudaStream_t stream) {
 template<typename input_t, typename weight_t>
 void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {
 
-    #ifndef USE_ROCM
+    #ifdef MX_MACA
         if (params.seqlen <= 128) {           
             selective_scan_fwd_launch<32, 4, input_t, weight_t>(params, stream);
         } else if (params.seqlen <= 256) {
@@ -402,14 +403,14 @@ void set_ssm_params_fwd(SSMParamsBase &params,
                         const torch::Tensor out,
                         const torch::Tensor z,
                         const torch::Tensor out_z,
-                        const std::optional<at::Tensor>& D,
-                        const std::optional<at::Tensor>& delta_bias,
+                        const c10::optional<at::Tensor>& D,
+                        const c10::optional<at::Tensor>& delta_bias,
                         const torch::Tensor ssm_states,
                         bool has_z, 
                         bool delta_softplus,
-                        const std::optional<at::Tensor>& query_start_loc,
-                        const std::optional<at::Tensor>& cache_indices,
-                        const std::optional<at::Tensor>& has_initial_state,
+                        const c10::optional<at::Tensor>& query_start_loc,
+                        const c10::optional<at::Tensor>& cache_indices,
+                        const c10::optional<at::Tensor>& has_initial_state,
                         bool varlen,
                         int64_t pad_slot_id) {
 
@@ -504,13 +505,13 @@ void set_ssm_params_fwd(SSMParamsBase &params,
 
 void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,
                   const torch::Tensor &A, const torch::Tensor &B, const torch::Tensor &C,
-                  const std::optional<torch::Tensor> &D_,
-                  const std::optional<torch::Tensor> &z_,
-                  const std::optional<torch::Tensor> &delta_bias_,
+                  const c10::optional<torch::Tensor> &D_,
+                  const c10::optional<torch::Tensor> &z_,
+                  const c10::optional<torch::Tensor> &delta_bias_,
                   bool delta_softplus,
-                  const std::optional<torch::Tensor> &query_start_loc,
-                  const std::optional<torch::Tensor> &cache_indices,
-                  const std::optional<torch::Tensor> &has_initial_state,
+                  const c10::optional<torch::Tensor> &query_start_loc,
+                  const c10::optional<torch::Tensor> &cache_indices,
+                  const c10::optional<torch::Tensor> &has_initial_state,
                   const torch::Tensor &ssm_states,
                   // used to identify padding entries if cache_indices provided
                   // in case of padding, the kernel will return early
@@ -649,7 +650,7 @@ void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,
     
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)u.get_device()};
+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(u.get_device())};
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(u.scalar_type(), "selective_scan_fwd", [&] {
         selective_scan_fwd_cuda<input_t, weight_t>(params, stream);
diff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu
index 01dac4044..66ef3c4bf 100644
--- a/csrc/moe/moe_align_sum_kernels.cu
+++ b/csrc/moe/moe_align_sum_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <torch/all.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
@@ -197,6 +198,153 @@ __global__ void moe_align_block_size_global_mem_kernel(
   }
 }
 
+
+__device__ __forceinline__ int32_t ScanWarp2(int32_t val) {
+  int32_t lane = threadIdx.x & 31;
+  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1, 8);
+  if (lane >= 1) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 2, 8);
+  if (lane >= 2) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 4, 8);
+  if (lane >= 4) {
+    val += tmp;
+  }
+  return val;
+}
+
+__device__ __forceinline__ int32_t ScanWarp(int32_t val) {
+  int32_t lane = threadIdx.x & 31;
+  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1);
+  if (lane >= 1) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 2);
+  if (lane >= 2) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 4);
+  if (lane >= 4) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 8);
+  if (lane >= 8) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 16);
+  if (lane >= 16) {
+    val += tmp;
+  }
+  return val;
+}
+
+
+template <typename scalar_t>
+__global__ void opt_sgl_moe_align_block_size_kernel(
+    scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids,
+    int32_t* expert_ids, int32_t* total_tokens_post_pad, int32_t num_experts,
+    int32_t block_size, size_t numel, int32_t* cumsum) {
+  __shared__ int32_t shared_counts[32][8];
+  __shared__ int32_t local_offsets[256];
+  __shared__ int32_t cum_test[257];
+  __shared__ int32_t blocksum[32];
+
+  const int warp_id = threadIdx.x / 32;
+  const int lane_id = threadIdx.x % 32;
+  const int experts_per_warp = 8;
+  const int my_expert_start = warp_id * experts_per_warp;
+
+  int32_t idx = threadIdx.x;
+  if(idx < num_experts){
+    shared_counts[idx / 8][idx % 8] = 0;
+    cum_test[0] = 0;
+  }
+  __syncthreads();
+
+  const size_t tokens_per_thread = CEILDIV(numel, blockDim.x);
+  const size_t start_idx = threadIdx.x * tokens_per_thread;
+
+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
+    int expert_id = topk_ids[i];
+    int warp_idx = expert_id / experts_per_warp;
+    int expert_offset = expert_id % experts_per_warp;
+    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
+  }
+  __syncthreads();
+
+  int val = 0;
+  if (threadIdx.x < 256) {
+    int32_t final_val = 0;
+    int row = idx / 8;
+    int line = idx % 8;
+    val = shared_counts[row][line];
+    val = CEILDIV(val, block_size) * block_size;
+  }
+  __syncthreads();
+
+  if(idx < 256) {
+    int tmp = 0;
+    val = ScanWarp(val);
+    if(lane_id == 31) {
+      blocksum[warp_id] = val;
+    }
+  }
+  __syncthreads();
+
+  if(warp_id == 0 && lane_id < 8) {
+    int res = blocksum[lane_id];
+    blocksum[lane_id] = ScanWarp2(res);
+  }
+  __syncthreads();
+
+  if(threadIdx.x < 256 && warp_id > 0){
+    val += blocksum[warp_id - 1];
+  }
+  __syncthreads();
+
+  if(idx < 256){
+    cum_test[idx + 1] = val;
+  }
+  __syncthreads();
+
+  if (threadIdx.x < num_experts) {
+    for (int i = cum_test[threadIdx.x]; i < cum_test[threadIdx.x + 1];
+         i += block_size) {
+      expert_ids[i / block_size] = threadIdx.x;
+    }
+    local_offsets[threadIdx.x] = cum_test[threadIdx.x];
+    if(threadIdx.x == 0){
+      *total_tokens_post_pad = cum_test[num_experts];
+    }
+  }
+  __syncthreads();
+
+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
+    int32_t expert_id = topk_ids[i];
+    int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);
+    sorted_token_ids[rank_post_pad] = i;
+  }
+}
+
+template <typename scalar_t, int TOPK>
+__global__ void moe_sum_kernel(
+    scalar_t* __restrict__ out,          // [..., d]
+    const scalar_t* __restrict__ input,  // [..., topk, d]
+    const int d) {
+  const int64_t token_idx = blockIdx.x;
+  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {
+    scalar_t x = 0.0;
+#pragma unroll
+    for (int k = 0; k < TOPK; ++k) {
+      x += VLLM_LDG(&input[token_idx * TOPK * d + k * d + idx]);
+    }
+    out[token_idx * d + idx] = x;
+  }
+}
+
 // taken from
 // https://github.com/sgl-project/sglang/commit/ded9fcd09a43d5e7d5bb31a2bc3e9fc21bf65d2a
 template <typename scalar_t>
@@ -263,21 +411,6 @@ __global__ void sgl_moe_align_block_size_kernel(
   }
 }
 
-template <typename scalar_t, int TOPK>
-__global__ void moe_sum_kernel(
-    scalar_t* __restrict__ out,          // [..., d]
-    const scalar_t* __restrict__ input,  // [..., topk, d]
-    const int d) {
-  const int64_t token_idx = blockIdx.x;
-  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {
-    scalar_t x = 0.0;
-#pragma unroll
-    for (int k = 0; k < TOPK; ++k) {
-      x += VLLM_LDG(&input[token_idx * TOPK * d + k * d + idx]);
-    }
-    out[token_idx * d + idx] = x;
-  }
-}
 
 }  // namespace moe
 }  // namespace vllm
@@ -388,7 +521,7 @@ void sgl_moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
         torch::Tensor cumsum_buffer =
             torch::empty({num_experts + 1}, options_int);
 
-        auto kernel = vllm::moe::sgl_moe_align_block_size_kernel<scalar_t>;
+        auto kernel = vllm::moe::opt_sgl_moe_align_block_size_kernel<scalar_t>;
         kernel<<<1, 1024, 0, stream>>>(
             topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
             experts_ids.data_ptr<int32_t>(),
diff --git a/csrc/moe/moe_ops.cpp b/csrc/moe/moe_ops.cpp
new file mode 100644
index 000000000..a4a44e075
--- /dev/null
+++ b/csrc/moe/moe_ops.cpp
@@ -0,0 +1,66 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+
+#include "moe_ops.h"
+
+#include <ATen/cuda/CUDAContext.h>
+#include <mcblas.h>
+#include <maca_fp16.h>
+
+mcblasStatus_t mcblasFusedMoe(mcStream_t stream,
+                              const void *a_ptr,
+                              const void *b_ptr,
+                              void *c_ptr,
+                              const int *sorted_token_ids_ptr,
+                              const int *expert_ids_ptr,
+                              const int *num_tokens_post_padded,
+                              int N,
+                              int K,
+                              int num_valid_tokens,
+                              int sorted_token_ids_len,
+                              int stride_am,
+                              int stride_ak,
+                              int stride_be,
+                              int stride_bk,
+                              int stride_bn,
+                              int stride_cm,
+                              int stride_cn,
+                              int top_k,
+                              bool mul_routed_weight,
+                              const float *topk_weights_ptr,
+                              macaDataType compute_type,
+                              int tileConfig = 0);
+
+void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
+                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
+                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
+                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig) {
+    
+    assert(topk_weights.stride(1) == 1);
+    assert(sorted_token_ids.stride(0) == 1);
+
+    auto stream = at::cuda::getCurrentCUDAStream();
+    macaDataType compute_type = (A.dtype() == at::ScalarType::BFloat16) ? MACA_R_16BF : MACA_R_16F;
+    mcblasFusedMoe(stream,
+                  A.data_ptr(),
+                  B.data_ptr(),
+                  C.data_ptr(),
+                  sorted_token_ids.data_ptr<int>(),
+                  expert_ids.data_ptr<int>(),
+                  num_tokens_post_padded.data_ptr<int>(),
+                  B.size(1),
+                  B.size(2),
+                  topk_ids.numel(),
+                  sorted_token_ids.size(0),
+                  A.stride(0),
+                  A.stride(1),
+                  B.stride(0),
+                  B.stride(2),
+                  B.stride(1),
+                  C.stride(1),
+                  C.stride(2),
+                  static_cast<int>(top_k),
+                  mul_routed_weight,
+                  topk_weights.data_ptr<float>(),
+                  compute_type,
+                  static_cast<int>(tileConfig));
+}
diff --git a/csrc/moe/moe_ops.h b/csrc/moe/moe_ops.h
index 66bb5f41b..702c03f5f 100644
--- a/csrc/moe/moe_ops.h
+++ b/csrc/moe/moe_ops.h
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include <torch/all.h>
@@ -18,3 +19,8 @@ void sgl_moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
                               torch::Tensor sorted_token_ids,
                               torch::Tensor experts_ids,
                               torch::Tensor num_tokens_post_pad);
+
+void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
+                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
+                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
+                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig);
diff --git a/csrc/moe/torch_bindings.cpp b/csrc/moe/torch_bindings.cpp
index 8540633dc..5d379755f 100644
--- a/csrc/moe/torch_bindings.cpp
+++ b/csrc/moe/torch_bindings.cpp
@@ -1,3 +1,5 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+
 #include "core/registration.h"
 #include "moe_ops.h"
 
@@ -43,6 +45,14 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, m) {
       " -> Tensor");
   // conditionally compiled so impl registration is in source file
 #endif
+
+// Fused moe in mcblas
+  m.def(
+      "fused_moe_kernel(Tensor! A, Tensor! B, Tensor! C,"
+      "Tensor! topk_weights, Tensor! topk_ids,"
+      "Tensor! sorted_token_ids, Tensor! expert_ids,"
+      "Tensor! num_tokens_post_padded, bool mul_routed_weight, int top_k, int tileConfig) -> ()");
+  m.impl("fused_moe_kernel", torch::kCUDA, &fused_moe_kernel);
 }
 
 REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
diff --git a/csrc/ops.h b/csrc/ops.h
index e39d4ef31..44d1d5d0b 100644
--- a/csrc/ops.h
+++ b/csrc/ops.h
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include <optional>
@@ -33,7 +34,7 @@ void paged_attention_v1(
     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
-    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
     torch::Tensor& v_scale, const int64_t tp_rank,
     const int64_t blocksparse_local_blocks,
@@ -42,15 +43,27 @@ void paged_attention_v1(
 
 void paged_attention_v2(
     torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,
-    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
+    torch::Tensor& tmp_out, torch::Tensor& block_count, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
-    int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
+    int64_t max_seq_len, const c10::optional<torch::Tensor>& alibi_slopes,
     const std::string& kv_cache_dtype, torch::Tensor& k_scale,
     torch::Tensor& v_scale, const int64_t tp_rank,
     const int64_t blocksparse_local_blocks,
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
-    const int64_t blocksparse_head_sliding_step);
+    const int64_t blocksparse_head_sliding_step, bool count_init_once);
+
+void page_reshape_kv_cache(
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& key_cache_new_layer,
+    torch::Tensor& value_cache_new_layer,
+    int64_t num_seqs,
+    int64_t num_heads,
+    int64_t head_size,
+    int64_t num_kv_heads,
+    int64_t block_size,
+    const std::string& kv_cache_dtype);
 
 void rms_norm(torch::Tensor& out, torch::Tensor& input, torch::Tensor& weight,
               double epsilon);
@@ -73,8 +86,8 @@ void rms_norm_dynamic_per_token_quant(torch::Tensor& out,
                                       torch::Tensor const& weight,
                                       torch::Tensor& scales,
                                       double const epsilon,
-                                      std::optional<torch::Tensor> scale_ub,
-                                      std::optional<torch::Tensor> residual);
+                                      c10::optional<torch::Tensor> scale_ub,
+                                      c10::optional<torch::Tensor> residual);
 
 void rotary_embedding(torch::Tensor& positions, torch::Tensor& query,
                       torch::Tensor& key, int64_t head_size,
@@ -130,17 +143,20 @@ torch::Tensor aqlm_dequant(
     const torch::Tensor& codes, const torch::Tensor& codebooks,
     const std::vector<int64_t>& codebook_partition_sizes);
 
+torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm);
+#endif
+
 torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
-                       int64_t split_k_iters);
+                       int64_t split_k_iters, torch::Tensor _temp_space, 
+                       bool dtype_bf16);
 
 torch::Tensor awq_dequantize(torch::Tensor _kernel,
                              torch::Tensor _scaling_factors,
                              torch::Tensor _zeros, int64_t split_k_iters,
                              int64_t thx, int64_t thy);
 
-torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm);
-#endif
+torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight);
 
 torch::Tensor ggml_dequantize(torch::Tensor W, int64_t type, int64_t m,
                               int64_t n);
@@ -158,15 +174,15 @@ bool cutlass_scaled_mm_supports_block_fp8(int64_t cuda_device_capability);
 void cutlass_scaled_mm(torch::Tensor& out, torch::Tensor const& a,
                        torch::Tensor const& b, torch::Tensor const& a_scales,
                        torch::Tensor const& b_scales,
-                       std::optional<torch::Tensor> const& bias);
+                       c10::optional<torch::Tensor> const& bias);
 
 void cutlass_scaled_mm_azp(torch::Tensor& out, torch::Tensor const& a,
                            torch::Tensor const& b,
                            torch::Tensor const& a_scales,
                            torch::Tensor const& b_scales,
                            torch::Tensor const& azp_adj,
-                           std::optional<torch::Tensor> const& azp,
-                           std::optional<torch::Tensor> const& bias);
+                           c10::optional<torch::Tensor> const& azp,
+                           c10::optional<torch::Tensor> const& bias);
 
 bool cutlass_sparse_scaled_mm_supported(int64_t cuda_device_capability);
 
@@ -174,7 +190,7 @@ void cutlass_scaled_sparse_mm(torch::Tensor& out, torch::Tensor const& a,
                               torch::Tensor const& b, torch::Tensor const& e,
                               torch::Tensor const& a_scales,
                               torch::Tensor const& b_scales,
-                              std::optional<torch::Tensor> const& bias);
+                              c10::optional<torch::Tensor> const& bias);
 
 bool cutlass_sparse_compress_entry(torch::Tensor& a_compressed,
                                    torch::Tensor& e, torch::Tensor const& a);
@@ -182,16 +198,18 @@ bool cutlass_sparse_compress_entry(torch::Tensor& a_compressed,
 
 void static_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
                               torch::Tensor const& scale,
-                              std::optional<torch::Tensor> const& azp);
+                              c10::optional<torch::Tensor> const& azp);
 
 void dynamic_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
                                torch::Tensor& scales,
-                               std::optional<torch::Tensor> const& azp);
+                               c10::optional<torch::Tensor> const& azp);
 
 torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
                         torch::Tensor b_gptq_qzeros,
                         torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
-                        bool use_exllama, int64_t bit);
+                        bool use_exllama, int64_t bit, int64_t group_size, 
+			torch::Tensor perm_space, torch::Tensor temp_space,
+			bool dtype_bf16);
 
 void gptq_shuffle(torch::Tensor q_weight, torch::Tensor q_perm, int64_t bit);
 
@@ -203,34 +221,34 @@ void dynamic_scaled_fp8_quant(torch::Tensor& out, torch::Tensor const& input,
 
 void dynamic_per_token_scaled_fp8_quant(
     torch::Tensor& out, torch::Tensor const& input, torch::Tensor& scale,
-    std::optional<torch::Tensor> const& scale_ub);
+    c10::optional<torch::Tensor> const& scale_ub);
 
 void selective_scan_fwd(const torch::Tensor& u, const torch::Tensor& delta,
                         const torch::Tensor& A, const torch::Tensor& B,
                         const torch::Tensor& C,
-                        const std::optional<torch::Tensor>& D_,
-                        const std::optional<torch::Tensor>& z_,
-                        const std::optional<torch::Tensor>& delta_bias_,
+                        const c10::optional<torch::Tensor>& D_,
+                        const c10::optional<torch::Tensor>& z_,
+                        const c10::optional<torch::Tensor>& delta_bias_,
                         bool delta_softplus,
-                        const std::optional<torch::Tensor>& query_start_loc,
-                        const std::optional<torch::Tensor>& cache_indices,
-                        const std::optional<torch::Tensor>& has_initial_state,
+                        const c10::optional<torch::Tensor>& query_start_loc,
+                        const c10::optional<torch::Tensor>& cache_indices,
+                        const c10::optional<torch::Tensor>& has_initial_state,
                         const torch::Tensor& ssm_states, int64_t pad_slot_id);
 
 void causal_conv1d_update(const at::Tensor& x, const at::Tensor& conv_state,
                           const at::Tensor& weight,
-                          const std::optional<at::Tensor>& bias_,
+                          const c10::optional<at::Tensor>& bias_,
                           bool silu_activation,
-                          const std::optional<at::Tensor>& cache_seqlens_,
-                          const std::optional<at::Tensor>& conv_state_indices_,
+                          const c10::optional<at::Tensor>& cache_seqlens_,
+                          const c10::optional<at::Tensor>& conv_state_indices_,
                           int64_t pad_slot_id);
 
 void causal_conv1d_fwd(const at::Tensor& x, const at::Tensor& weight,
-                       const std::optional<at::Tensor>& bias_,
-                       const std::optional<at::Tensor>& conv_states,
-                       const std::optional<at::Tensor>& query_start_loc,
-                       const std::optional<at::Tensor>& cache_indices,
-                       const std::optional<at::Tensor>& has_initial_state,
+                       const c10::optional<at::Tensor>& bias_,
+                       const c10::optional<at::Tensor>& conv_states,
+                       const c10::optional<at::Tensor>& query_start_loc,
+                       const c10::optional<at::Tensor>& cache_indices,
+                       const c10::optional<at::Tensor>& has_initial_state,
                        bool silu_activation, int64_t pad_slot_id);
 
 #ifndef USE_ROCM
diff --git a/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
new file mode 100644
index 000000000..f10aca98f
--- /dev/null
+++ b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
@@ -0,0 +1,474 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+#include "../gptq/Hgemm_common.cuh"
+#include "awq_4bits.cuh"
+#include "maca_fp16.h"
+
+#define input_type __half
+#define output_type __half
+#define scalar_type float
+#define acc_type float
+
+#define SEL0 0x01000504
+#define SEL1 0x03020706
+
+#define SWIZZLE_INDEX(index, phase) (index ^ phase)
+#define ONE_DIM_INDEX(x, y, lda) (y * lda + x)
+#define CLAMP(value, bound, align) (value < bound ? value : bound - align)
+
+#define LDG_B                                                       \
+    {                                                               \
+        ldg_b128_bsm_async(b_sts, dB[0], pred_n0, true);            \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1, true); \
+    }
+
+#define LDG_B_HEAD                                                            \
+    {                                                                         \
+        bool pred_k = rowB_swizzle < ktail;                                   \
+        ldg_b128_bsm_async(b_sts, dB[0], pred_n0 &&pred_k, true);             \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1 && pred_k, true); \
+    }
+
+#define MMA_ABC1                                                             \
+    {                                                                        \
+        for (int index_n = 0; index_n < 2; ++index_n)                        \
+        {                                                                    \
+            for (int index_m = 0; index_m < 8; ++index_m)                    \
+            {                                                                \
+                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                                cast_b64(rgB)[ONE_DIM_INDEX(0, index_n, 2)], \
+                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
+            }                                                                \
+        }                                                                    \
+    }
+
+#define MMA_ABC2                                                             \
+    {                                                                        \
+        for (int index_n = 0; index_n < 2; ++index_n)                        \
+        {                                                                    \
+            for (int index_m = 0; index_m < 8; ++index_m)                    \
+            {                                                                \
+                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                                cast_b64(rgB)[ONE_DIM_INDEX(1, index_n, 2)], \
+                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
+            }                                                                \
+        }                                                                    \
+    }
+
+#define LDS_B                                          \
+    {                                                  \
+        cast_b128(rgB)[0] = cast_b128(b_lds)[0];       \
+        cast_b128(rgB)[1] = cast_b128(b_lds)[16 * 16]; \
+    }
+
+#define PERM_C(C2perm)                             \
+    {                                              \
+        Float4VecType C_tmp[8];                    \
+        float *ptri = (float *)C2perm;             \
+        float *ptro = (float *)C_tmp;              \
+        for (int j = 0; j < 4; ++j)                \
+        {                                          \
+            for (int i = 0; i < 8; ++i)            \
+            {                                      \
+                ptro[j * 8 + i] = ptri[j + i * 4]; \
+            }                                      \
+        }                                          \
+        for (int i = 0; i < 8; ++i)                \
+        {                                          \
+            C2perm[i] = C_tmp[i];                  \
+        }                                          \
+    }
+
+#define STS_C(phase)                                            \
+    {                                                           \
+        cast_b128(c_sts[0])[0] = cast_b128(rgC)[0 + phase * 8]; \
+        cast_b128(c_sts[1])[0] = cast_b128(rgC)[1 + phase * 8]; \
+        cast_b128(c_sts[2])[0] = cast_b128(rgC)[2 + phase * 8]; \
+        cast_b128(c_sts[3])[0] = cast_b128(rgC)[3 + phase * 8]; \
+        cast_b128(c_sts[4])[0] = cast_b128(rgC)[4 + phase * 8]; \
+        cast_b128(c_sts[5])[0] = cast_b128(rgC)[5 + phase * 8]; \
+        cast_b128(c_sts[6])[0] = cast_b128(rgC)[6 + phase * 8]; \
+        cast_b128(c_sts[7])[0] = cast_b128(rgC)[7 + phase * 8]; \
+    }
+
+#define REDUCE_C(phase)                                                   \
+    {                                                                     \
+        cast_b128(rgC)[0 + phase * 8] = cast_b128(c_lds[0])[0];           \
+        cast_b128(rgC)[1 + phase * 8] = cast_b128(c_lds[1])[0];           \
+        cast_b128(rgC)[2 + phase * 8] = cast_b128(c_lds[0])[1 * 16 * 32]; \
+        cast_b128(rgC)[3 + phase * 8] = cast_b128(c_lds[1])[1 * 16 * 32]; \
+        cast_b128(rgC)[4 + phase * 8] = cast_b128(c_lds[0])[2 * 16 * 32]; \
+        cast_b128(rgC)[5 + phase * 8] = cast_b128(c_lds[1])[2 * 16 * 32]; \
+        cast_b128(rgC)[6 + phase * 8] = cast_b128(c_lds[0])[3 * 16 * 32]; \
+        cast_b128(rgC)[7 + phase * 8] = cast_b128(c_lds[1])[3 * 16 * 32]; \
+        float *reduc_c = reinterpret_cast<float *>(rgC);                  \
+        for (int loop = 0; loop < 8; ++loop)                              \
+        {                                                                 \
+            float acc = 0;                                                \
+            acc += reduc_c[loop + phase * 32 + 0 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 1 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 2 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 3 * 8];                    \
+            reduc_c[loop + phase * 8] = acc;                              \
+        }                                                                 \
+    }
+
+template <Operation_t TransA,
+          Operation_t TransB,
+          bool IsBetaZero,
+          int tileM,
+          int tileN,
+          int tileK,
+          bool splitk = false,
+          bool Swap = false>
+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_awq_4bit(int m,
+                                                                           int n,
+                                                                           int k,
+                                                                           const scalar_type alpha,
+                                                                           const scalar_type beta,
+                                                                           const quant_packed_type *dA_input,
+                                                                           int lda,
+                                                                           const input_type *dB_input,
+                                                                           int ldb,
+                                                                           output_type *dC_input,
+                                                                           output_type *dC_output,
+                                                                           int ldc,
+                                                                           quant_packed_type *d_zeros,
+                                                                           input_type *d_scales,
+                                                                           int splitk_iters = 1,
+                                                                           acc_type * d_acc_tmp=nullptr)
+{
+    __shared__ uint8_t smem_base[0x8000];
+    int bidx = Swap ? blockIdx.y : blockIdx.x;
+    int bidy = Swap ? blockIdx.x : blockIdx.y;
+    int bidz = blockIdx.z;
+    int tid = threadIdx.x;
+
+    uint64_t arowstride = TransA == OP_N ? 1 : lda;
+    uint64_t acolstride = TransA == OP_N ? lda : 1;
+    uint64_t browstride = TransB == OP_N ? 1 : ldb;
+    uint64_t bcolstride = TransB == OP_N ? ldb : 1;
+
+    const int malign = 8;
+    const int nalign = 1;
+    const int kalign = 8;
+    int align_m = (m + malign - 1) / malign * malign;
+    int align_n = (n + nalign - 1) / nalign * nalign;
+    int align_k = (k + kalign - 1) / kalign * kalign;
+
+    int k_num = (align_k + splitk_iters - 1) / splitk_iters;
+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
+    int k_begin = bidz * k_num;
+    int k_end = (bidz + 1) * k_num;
+    k_end = k_end > align_k ? align_k : k_end;
+    k_num = k_end - k_begin;
+    int ktail = k_num % tileK;
+    int kloop = k_begin;
+    if (k_begin > align_k)
+    {
+        return;
+    }    
+
+
+    int slot = tid / 64;
+    int lane = tid & 63;
+    int m64d16 = lane / 16;
+    int m64m16 = lane % 16;
+    output_type *c_base_i = dC_input;
+    output_type *c_base_o = dC_output;
+
+
+    quant_packed_type rga[8];
+    b128VecType rgb[2];
+
+    b64VecType rgA[16], rgB[4], rgCi[4];
+    Float4VecType rgC[16];
+    uint32_t rgZeros[1];
+    b128VecType rgScales[1];
+
+    quant_packed_type *dA[1];
+    input_type *dB[2];
+
+    // ldg A/B head
+
+    int rowA = m64m16 * 8;
+    int colA = m64d16 * 8 + slot * 32;
+    uint current_m = bidx * tileM + rowA;
+    bool pred_m = current_m < align_m;
+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m / PACK_RATIO) * (uint64_t)(arowstride) +
+            (uint64_t)(colA + k_begin) * (uint64_t)(acolstride / PACK_RATIO);
+
+    int colB = m64d16 + slot * 4;
+    int rowB = m64m16 * 8;
+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
+    int current_n = bidy * tileN + colB;
+    bool pred_n0 = current_n < align_n;
+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    current_n += 16;
+    bool pred_n1 = current_n < align_n;
+    dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    // ldgB to BSM need swizzle
+    input_type *b_sts = (input_type *)smem_base;
+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
+
+    // lds need swizzle
+    input_type *b_lds = (input_type *)smem_base;
+    int colB_lds = m64m16;
+    int rowB_lds = m64d16 * 8 + slot * 32;
+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
+
+    // ldg zeros and scales
+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO) * (kloop / tileK) + bidx * (tileM / PACK_RATIO) + tid;
+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + tid * 2;
+    quant_packed_type *lds_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + m64m16 * 8;
+
+    // clear C registers
+    rgC[0] = 0;
+    rgC[1] = 0;
+    rgC[2] = 0;
+    rgC[3] = 0;
+    rgC[4] = 0;
+    rgC[5] = 0;
+    rgC[6] = 0;
+    rgC[7] = 0;
+    rgC[8] = 0;
+    rgC[9] = 0;
+    rgC[10] = 0;
+    rgC[11] = 0;
+    rgC[12] = 0;
+    rgC[13] = 0;
+    rgC[14] = 0;
+    rgC[15] = 0;
+
+    int aincr = tileK * acolstride / PACK_RATIO;
+    int bincr = tileK * browstride;
+
+    if (k_num > ktail)
+    {
+        LDG_B;
+        LDG_ZEROS;
+        LDG_SCALES;
+        LDG_A;
+
+        dA[0] += aincr;
+        dB[0] += bincr;
+        dB[1] += bincr;
+
+        // lds_A, need swizzle
+        arrive_gvmcnt(8);
+        barrier();
+        LDS_B;
+        LDS_ZEROS;
+        LDS_SCALES;
+        arrive_bsmcnt(0);
+        barrier();
+
+        ldg_zeros_offset += lda / PACK_RATIO;
+        ldg_scales_offset += lda;
+
+// KLOOP
+#pragma unroll 1
+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
+        {
+            // load next B
+            LDG_B;
+            LDG_ZEROS;
+            LDG_SCALES;
+
+            // MMA_ABC1;
+            arrive_gvmcnt(4);
+            PERM_A;
+            LDG_A;
+            MMA;
+
+            // sts && lds
+            arrive_gvmcnt(8);
+            barrier();
+            LDS_B;
+            LDS_ZEROS;
+            LDS_SCALES;
+            arrive_bsmcnt(0);
+            barrier();
+
+            dA[0] += aincr;
+            dB[0] += bincr;
+            dB[1] += bincr;
+            ldg_zeros_offset += lda / PACK_RATIO;
+            ldg_scales_offset += lda;
+        }
+
+        arrive_gvmcnt(0);
+        PERM_A;
+        MMA;
+    }
+
+    // final tail kloop
+    if (ktail > 0)
+    {
+        LDG_B_HEAD;
+        LDG_ZEROS;
+        LDG_SCALES;
+        LDG_A_HEAD;
+        arrive_gvmcnt(8);
+        barrier();
+        LDS_B;
+        LDS_ZEROS;
+        LDS_SCALES;
+        arrive_bsmcnt(0);
+        barrier();
+        arrive_gvmcnt(0);
+        PERM_A;
+        MMA;
+    }
+
+    // store C registers into BSM & do reduction
+    int colC = m64m16 + slot * 16;
+    int rowC = m64d16 * 32;
+    scalar_type *c_sts[8];
+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 16, (colC % 4 * 4)), colC, 128);
+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 20, (colC % 4 * 4)), colC, 128);
+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 24, (colC % 4 * 4)), colC, 128);
+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 28, (colC % 4 * 4)), colC, 128);
+
+    colC = m64d16 + slot * 4;
+    rowC = m64m16 * 4;
+    scalar_type *c_lds[2];
+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+
+    PERM_C(rgC);
+    STS_C(0);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(0);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    PERM_C((rgC + 8))
+    STS_C(1);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(1);
+
+    arrive_bsmcnt(0);
+
+    // read C_input if beta!=0
+    colC = m64d16 + slot * 4 + bidy * tileN;
+    rowC = m64m16 * 4 + bidx * tileM;
+    int colC1 = colC + 16;
+    int rowC1 = rowC + 64;
+
+
+    if constexpr (!IsBetaZero && !splitk)
+    {
+        ldg_b64_reg_async(cast_b64(rgCi)[0],
+                          c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
+                          rowC < align_m && colC < align_n, true);
+        ldg_b64_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
+                          rowC1 < align_m && colC < align_n, true);
+        ldg_b64_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
+                          rowC < align_m && colC1 < align_n, true);
+        ldg_b64_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
+                          rowC1 < align_m && colC1 < align_n, true);
+        arrive_gvmcnt(0);
+    }
+
+    if constexpr (IsBetaZero && !splitk)
+    {
+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
+    }
+
+
+    if constexpr (!splitk)
+    {
+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC < align_m && colC < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC < align_m && colC1 < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+    }
+    else
+    {
+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC < align_m && colC < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC < align_m && colC1 < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
+        }
+    }
+}
\ No newline at end of file
diff --git a/csrc/quantization/awq/awq_4bits.cuh b/csrc/quantization/awq/awq_4bits.cuh
new file mode 100644
index 000000000..7653066cc
--- /dev/null
+++ b/csrc/quantization/awq/awq_4bits.cuh
@@ -0,0 +1,126 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#include "../gptq/Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+
+#define QBITS 4
+#define PACK_RATIO (32 / QBITS)
+
+#define LDG_A                                                                         \
+    {                                                                                 \
+        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m, true); \
+    }
+
+#define LDG_A_HEAD                                                                    \
+    {                                                                                 \
+        bool predk = colA < ktail;                                                    \
+        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m && predk, true); \
+    }
+
+// 每个block中用tileM/PACK_RATIO个线程去加载tileM个zeros，zeros为int4类型，每个32bits寄存器可以保存8个zeros
+#define LDG_ZEROS                                                                                                                              \
+    {                                                                                                                                          \
+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO && (bidx * tileM + tid * PACK_RATIO < align_m), false); \
+    }
+
+// 每个block中用tileM/(sizeof(uint32_t)/sizeof(__half))个线程去加载tileM个scales，scale为fp16类型，每个32bits寄存器可以保存2个scales
+#define LDG_SCALES                                                                                                      \
+    {                                                                                                                   \
+        ldg_b32_bsm_async(sts_scales_offset, ldg_scales_offset, tid < 64 && (bidx * tileM + tid * 2 < align_m), false); \
+    }
+
+#define LDS_ZEROS                         \
+    {                                     \
+        rgZeros[0] = lds_zeros_offset[0]; \
+    }
+
+#define LDS_SCALES                                     \
+    {                                                  \
+        rgScales[0] = cast_b128(lds_scales_offset)[0]; \
+    }
+
+// zeros中从低字节到高字节分别对应第 0 2 4 6 1 3 5 7 列， 所以index_shfl = index % 2 * 4 + index / 2
+//    提取出int4的zeros后转成int32，再把weight（矩阵A）还原成fp16，Weight_Q=Scale*(Weight_4Bit-ZeroPoint)
+#define PERM_ELEM(index)                                                                                                          \
+    {                                                                                                                             \
+        __half_raw elem;                                                                                                          \
+        if constexpr (index & 0x1)                                                                                                \
+        {                                                                                                                         \
+            elem.x = cast_b32(rgScales)[index / 2] >> 16;                                                                         \
+        }                                                                                                                         \
+        else                                                                                                                      \
+        {                                                                                                                         \
+            elem.x = cast_b32(rgScales)[index / 2] & 0xffff;                                                                      \
+        }                                                                                                                         \
+        __half scale = __half(elem);                                                                                              \
+        constexpr int index_shfl = index % 2 * 4 + index / 2;                                                                     \
+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[0], QBITS * index_shfl, QBITS);                                                \
+        cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[0], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[1], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[2], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[3], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[4], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+        cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[5], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+        cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[6], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+        cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[7], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+    }
+
+#define PERM_A       \
+    {                \
+        PERM_ELEM(0) \
+        PERM_ELEM(1) \
+        PERM_ELEM(2) \
+        PERM_ELEM(3) \
+        PERM_ELEM(4) \
+        PERM_ELEM(5) \
+        PERM_ELEM(6) \
+        PERM_ELEM(7) \
+    }
+
+#define MMA_ELEM(index_m)                                        \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);
+
+#define MMA     \
+    MMA_ELEM(0) \
+    MMA_ELEM(1) \
+    MMA_ELEM(2) \
+    MMA_ELEM(3) \
+    MMA_ELEM(4) \
+    MMA_ELEM(5) \
+    MMA_ELEM(6) \
+    MMA_ELEM(7)
diff --git a/csrc/quantization/awq/dequant.cuh b/csrc/quantization/awq/dequant.cuh
new file mode 100644
index 000000000..a4fc08831
--- /dev/null
+++ b/csrc/quantization/awq/dequant.cuh
@@ -0,0 +1,11 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#define BFE(source, bit_start, num_bits) __builtin_mxc_ubfe(source, bit_start, num_bits);
+
+template <typename outputT, typename inputT, int qbits>
+__device__ __forceinline__ outputT extract(inputT *input, size_t loc)
+{
+    constexpr int widthPerElem = sizeof(inputT) * 8 / qbits;
+    return (outputT)BFE(input[loc / widthPerElem], loc % widthPerElem, qbits);
+}
\ No newline at end of file
diff --git a/csrc/quantization/awq/dequantize.cuh b/csrc/quantization/awq/dequantize.cuh
index 5fa4b5f64..6d65f3a2b 100644
--- a/csrc/quantization/awq/dequantize.cuh
+++ b/csrc/quantization/awq/dequantize.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
 Adapted from https://github.com/mit-han-lab/llm-awq
 Modified from NVIDIA FasterTransformer:
@@ -40,6 +41,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
   // dependency if we issue immediately before required.
   const uint32_t top_i4s = i4s >> 8;
   // Extract elt_01 - (i4s & 0x000f000f) | 0x64006400
+#ifdef MX_MACA
   asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                : "=r"(h[0])
                : "r"(i4s), "n"(BOTTOM_MASK), "n"(I4s_TO_F16s_MAGIC_NUM),
@@ -59,7 +61,63 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
                : "=r"(h[3])
                : "r"(top_i4s), "n"(TOP_MASK), "n"(I4s_TO_F16s_MAGIC_NUM),
                  "n"(immLut));
+#else
+      // >>>> PTX2CPP Success <<<<
+{
+(h[0])=0;
+if((immLut)&0x01)(h[0])|=~(i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[0])|=~(i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[0])|=~(i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[0])|=~(i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[0])|= (i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[0])|= (i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[0])|= (i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[0])|= (i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+
+    // Extract elt_23 (i4s & 0x00f000f0) | 0x64006400
+
+// >>>> PTX2CPP Success <<<<
+{
+(h[1])=0;
+if((immLut)&0x01)(h[1])|=~(i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[1])|=~(i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[1])|=~(i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[1])|=~(i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[1])|= (i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[1])|= (i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[1])|= (i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[1])|= (i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+// >>>> PTX2CPP Success <<<<
+{
+(h[2])=0;
+if((immLut)&0x01)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[2])|=~(top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[2])|=~(top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[2])|= (top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[2])|= (top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[2])|= (top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[2])|= (top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+    // Extract elt_67 (top_i4s & 0x00f000f0) | 0x64006400
 
+// >>>> PTX2CPP Success <<<<
+{
+(h[3])=0;
+if((immLut)&0x01)(h[3])|=~(top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[3])|=~(top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[3])|=~(top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[3])|=~(top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[3])|= (top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[3])|= (top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[3])|= (top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[3])|= (top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+
+
+#endif
   // I use inline PTX below because I am not sure if the compiler will emit
   // float2half instructions if I use the half2 ctor. In this case, I chose
   // performance reliability over code readability.
@@ -77,6 +135,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
 
   // Finally, we construct the output numbers.
   // Convert elt_01
+#ifdef MX_MACA
   asm volatile("sub.f16x2 %0, %1, %2;\n"
                : "=r"(h[0])
                : "r"(h[0]), "r"(FP16_TOP_MAGIC_NUM));
@@ -92,7 +151,54 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                : "=r"(h[3])
                : "r"(h[3]), "r"(ONE_SIXTEENTH), "r"(NEG_64));
+#else
+    // >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[0]);
+unsigned int __b=(FP16_TOP_MAGIC_NUM);
+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
+(h[0])=*(unsigned int*)&__d;
+}
+}
+
+    // Convert elt_23
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[1]);
+unsigned int __b=(ONE_SIXTEENTH);
+unsigned int __c=(NEG_64);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(h[1])=*(unsigned int*)&__d;
+}
+}
+    // Convert elt_45
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[2]);
+unsigned int __b=(FP16_TOP_MAGIC_NUM);
+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
+(h[2])=*(unsigned int*)&__d;
+}
+}
+
+    // Convert elt_67
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[3]);
+unsigned int __b=(ONE_SIXTEENTH);
+unsigned int __c=(NEG_64);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(h[3])=*(unsigned int*)&__d;
+}
+}
 
+#endif
   return result;
 #endif
   __builtin_unreachable();  // Suppress missing return statement warning
diff --git a/csrc/quantization/awq/gemm_kernels.cu b/csrc/quantization/awq/gemm_kernels.cu
index 9da724a1b..5c495577e 100644
--- a/csrc/quantization/awq/gemm_kernels.cu
+++ b/csrc/quantization/awq/gemm_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
 Adapted from https://github.com/mit-han-lab/llm-awq
 @article{lin2023awq,
@@ -14,9 +15,21 @@ Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
 
 #include <cuda_fp16.h>
 
+#include "../gptq/hgemm_gptq.h"
+#include "../gptq/scalar_type.hpp"
+
+//#include "hgemv_nn_splitk_awq.hpp"
+//#include "hgemv_selector.hpp"
+//#include "Hgemm_nn_128x32x128_8m1n8k_awq.hpp"
+
 namespace vllm {
 namespace awq {
+#define input_type __half
+#define output_type __half
+#define quant_packed_type uint32_t
+#define QUANT_GROUP 128
 
+#if 0
 template <int N>
 __global__ void __launch_bounds__(64)
     gemm_forward_4bit_cuda_m16nXk32(int G, int split_k_iters,
@@ -136,6 +149,7 @@ __global__ void __launch_bounds__(64)
       // - zero and * scale
       // TODO (Haotian): can save 4 assembly instructions if sormulate as deq =
       // q * scale - zero * scale.
+#ifdef MX_MACA
       asm volatile("sub.f16x2 %0, %1, %2;\n"
                    : "=r"(B_loaded_fp16.x)
                    : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
@@ -160,6 +174,7 @@ __global__ void __launch_bounds__(64)
       asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                    : "=r"(B_loaded_fp16.w)
                    : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
+#endif
       /*
       if (ax0_ax1_fused_0 == 0 && blockIdx_z == 0 && blockIdx_y == 0 && k_0_0 ==
       0 && threadIdx.x == 17 && threadIdx.y == 0){ printf("[x] %X %X %X %X\n",
@@ -176,6 +191,7 @@ __global__ void __launch_bounds__(64)
     for (int k_0_1 = 0; k_0_1 < 2; ++k_0_1) {
       {
         unsigned int addr;
+#ifdef MX_MACA
         __asm__ __volatile__(
             "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
             "addr; }\n"
@@ -192,11 +208,13 @@ __global__ void __launch_bounds__(64)
               "=r"(((unsigned*)(A_shared_warp + 0))[2]),
               "=r"(((unsigned*)(A_shared_warp + 0))[3])
             : "r"(addr));
+#endif
       }
 
       for (int ax1_0 = 0; ax1_0 < N / 32; ++ax1_0) {
         {
           unsigned int addr;
+#ifdef MX_MACA
           __asm__ __volatile__(
               "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
               "addr; }\n"
@@ -214,9 +232,11 @@ __global__ void __launch_bounds__(64)
                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[2]),
                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[3])
               : "r"(addr));
+#endif
         }
       }
       for (int j_0_4 = 0; j_0_4 < N / 32; ++j_0_4) {
+#ifdef MX_MACA
   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 750
         {
           __asm__ __volatile__(
@@ -329,6 +349,7 @@ __global__ void __launch_bounds__(64)
         }
 
   #endif
+#endif
       }
     }
   }
@@ -346,6 +367,7 @@ __global__ void __launch_bounds__(64)
   }
 #endif
 }
+#endif
 
 __global__ void __launch_bounds__(64)
     dequantize_weights(int* __restrict__ B, half* __restrict__ scaling_factors,
@@ -375,6 +397,7 @@ __global__ void __launch_bounds__(64)
 
   uint32_t B_loaded = *(uint32_t*)B_ptr2;
   uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2(B_loaded);
+#ifdef MX_MACA
   asm volatile("sub.f16x2 %0, %1, %2;\n"
                : "=r"(B_loaded_fp16.x)
                : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
@@ -399,7 +422,91 @@ __global__ void __launch_bounds__(64)
   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                : "=r"(B_loaded_fp16.w)
                : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
+#else
+     // >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.x);
+unsigned int __b=(B_loaded_zero.x);
+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
+(B_loaded_fp16.x)=*(unsigned int*)&__d;
+}
+}
+
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.x);
+unsigned int __b=(B_loaded_scale.x);
+unsigned int __c=(ZERO);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(B_loaded_fp16.x)=*(unsigned int*)&__d;
+}
+}
 
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.y);
+unsigned int __b=(B_loaded_zero.y);
+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
+(B_loaded_fp16.y)=*(unsigned int*)&__d;
+}
+}
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.y);
+unsigned int __b=(B_loaded_scale.y);
+unsigned int __c=(ZERO);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(B_loaded_fp16.y)=*(unsigned int*)&__d;
+}
+}
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.z);
+unsigned int __b=(B_loaded_zero.z);
+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
+(B_loaded_fp16.z)=*(unsigned int*)&__d;
+}
+}
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.z);
+unsigned int __b=(B_loaded_scale.z);
+unsigned int __c=(ZERO);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(B_loaded_fp16.z)=*(unsigned int*)&__d;
+}
+}
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.w);
+unsigned int __b=(B_loaded_zero.w);
+__half2 __d=__hsub2(*(__half2*)&__a,*(__half2*)&__b);
+(B_loaded_fp16.w)=*(unsigned int*)&__d;
+}
+}
+
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.w);
+unsigned int __b=(B_loaded_scale.w);
+unsigned int __c=(ZERO);
+__half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+(B_loaded_fp16.w)=*(unsigned int*)&__d;
+}
+}
+
+#endif
   *(uint4*)B_shared_ptr2 = B_loaded_fp16;
 
   for (int i = 0; i < 8; ++i) {
@@ -407,9 +514,392 @@ __global__ void __launch_bounds__(64)
   }
 }
 
+#if 0
+template <typename dstT, typename srcT, typename scalarT>
+__global__ void blasMemcpy(dstT *dst, const srcT *src, size_t cnt, scalarT beta) {
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (cnt + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
+        dst[loop * threads + tid] =
+            static_cast<double>(beta) * static_cast<double>(src[loop * threads + tid]);
+    }
+}
+
+#define SWITCH_CASE_BATCH(BlockDimX, SplitK, BATCH) \
+    case BATCH: {                                   \
+        CALL_GEMM(BlockDimX, SplitK, BATCH)         \
+        break;                                      \
+    }
+
+#define APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH) \
+    switch(BATCH) {                                 \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 1)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 2)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 3)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 4)           \
+        default: {                                          \
+            launched = false;                               \
+            printf("ERROR: Unsupported BATCH %d\n", BATCH); \
+            break;                                          \
+        }                                                   \
+    }
+
+#define SWITCH_CASE_BlockDimX(BlockDimX, SplitK, BATCH) \
+    case BlockDimX: {                                   \
+        APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH)    \
+        break;                                          \
+    }
+
+#define APPLY_HGEMM(BlockDimX, SplitK, BATCH)           \
+    switch (BlockDimX) {                                \
+        SWITCH_CASE_BlockDimX(16, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(32, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(64, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(128, SplitK, BATCH)       \
+        SWITCH_CASE_BlockDimX(256, SplitK, BATCH)       \
+        default: {                                                  \
+            launched = false;                                       \
+            printf("ERROR: Unsupported BlockDimX %d\n", BlockDimX); \
+            break;                                                  \
+        }                                                           \
+    }
+
+bool call_kernel(const half *srcB,
+    const quant_packed_type *srcA,
+    quant_packed_type *zeros, half *scales,
+    half* dst_D,
+    int m, int n, int k, int srcStride, int dstStride,
+    int block_x, int split_k,
+    const int* b_perm_D = nullptr) {
+    //constexpr int PACK_RATIO = 8;
+    constexpr int ThreadBlock = 256;
+    const dim3 threadBlock = {static_cast<unsigned int>(ThreadBlock)};
+    const dim3 gridBlock = {static_cast<unsigned int>((m + 8*block_x-1) / 8 / block_x), static_cast<unsigned int>(split_k)};
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    if (split_k * QUANT_GROUP > k || k % QUANT_GROUP != 0) return false;
+    if (block_x < 16 || n > 4) return false;
+    bool launched = true;
+    #define CALL_GEMM(BX, SK, N) \
+        if (SK * 128 == k) {   \
+            hgemv_nn_splitk_awq_kb128<BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+        } else { \
+            hgemv_nn_splitk_awq<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+        }
+    APPLY_HGEMM(block_x, split_k, n);
+    return launched;
+}
+
+void launch_gemm_awq(Operation_t trans_a,
+                      Operation_t trans_b,
+                      int m,
+                      int n,
+                      int k,
+                      const float alpha,
+                      const float beta,
+                      const uint32_t* dA,
+                      int lda,
+                      const half* dB,
+                      int ldb,
+                      half* dC,
+                      int ldc,
+                      uint32_t* d_zeros,
+                      half* d_scales,
+                      float* space_mid,
+                      cudaStream_t stream,
+                      int splitk_iters = 1) {
+    if (n <= 4) {
+            constexpr int thread_block = 256;
+            constexpr int m_per_thread = 8;
+            auto kernel_testing = [&](int bx, int sk) -> bool {
+                return call_kernel(dB, dA, d_zeros, d_scales, dC, m, n, k, m, m, bx, sk);
+            };
+            //Select parameters when warmup
+            auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,8,m_per_thread>::selector(m, n, k, true);
+            if (sl_warmup.valid()) {
+                sl_warmup.run(kernel_testing);
+            }
+    }
+    else {
+            const int threads_n = 256;
+            const int tileM = 128;
+            const int tileN = 32;
+            const int tileK = 128;
+
+            bool isSplitk = splitk_iters > 1;
+
+            uint32_t gridx = (m - 1) / tileM + 1;
+            uint32_t gridy = (n - 1) / tileN + 1;
+            uint32_t gridz = splitk_iters;
+
+            dim3 dimBlock(threads_n, 1, 1);
+            dim3 dimGrid(gridx, gridy, gridz);
+            bool isBetaZero = (beta == 0.0);
+
+            if (trans_a == OP_N && trans_b == OP_N && m % 8 == 0 && k % 8 == 0) {
+                if (!isSplitk) {
+                    if (isBetaZero)
+                        Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, true, tileM, tileN, tileK>
+                            <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC,
+                                                               dC, ldc, d_zeros, d_scales);
+                    else
+                        Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, false, tileM, tileN, tileK>
+                            <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC,
+                                                               dC, ldc, d_zeros, d_scales);
+                } else {
+                    if (!isBetaZero)
+                        blasMemcpy<<<104, 512, 0, stream>>>(space_mid, dC, m * n, beta);
+                    Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true>
+                        <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC, dC,
+                                                           ldc, d_zeros, d_scales, splitk_iters, space_mid);
+                    blasMemcpy<<<104, 512, 0, stream>>>(dC, space_mid, m * n, 1);
+                }
+            } else {
+                printf("Parameters not supported!\n");
+                return;
+            }
+    }
+}
+#endif
+
+template <int BLOCK_SIZE>
+__global__ void awq_to_gptq_4bit(uint32_t *output, const uint32_t *input, int k, int n) {
+    constexpr int COMPACT_FACTOR = 8;
+    constexpr int QBIT = 4;
+    int tid = threadIdx.x;
+    int tile_idx = blockIdx.x * BLOCK_SIZE + tid;
+    int N_COMPACT = (n + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
+    int K_COMPACT = (k + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
+    int tile_n_idx = tile_idx / K_COMPACT;
+    int tile_k_idx = tile_idx % K_COMPACT;
+
+    uint32_t awq_data[COMPACT_FACTOR];
+    uint32_t temp_data[COMPACT_FACTOR];
+    uint32_t gptq_data[COMPACT_FACTOR];
+
+    int gptq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
+    int awq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
+
+    // load k8xn8
+    #pragma unroll
+    for (int i = 0; i < COMPACT_FACTOR; i++) {
+        int gvm_addr_offset = (tile_k_idx * COMPACT_FACTOR + i) * N_COMPACT + tile_n_idx;
+        int pred_k = tile_k_idx * COMPACT_FACTOR + i < k;
+        int pred_n = tile_n_idx * COMPACT_FACTOR < n;
+        if (pred_k && pred_n) {
+            awq_data[i] = *(input + gvm_addr_offset);
+        }
+    }
+
+    // decompress awq_data and recompress to gptq_data
+    #pragma unroll
+    for (int i = 0; i < COMPACT_FACTOR; i++) {
+        #pragma unroll
+        for(int j = 0; j < COMPACT_FACTOR; j++) {
+            temp_data[j] = ((awq_data[j] >> (awq_shift[i] * QBIT)) & 0xf);
+        }
+        #pragma unroll
+        for(int j = 0; j < COMPACT_FACTOR; j++) {
+            gptq_data[i] &= (~(0xf << (gptq_shift[j] * QBIT)));
+            gptq_data[i] |= temp_data[j] << (gptq_shift[j] * QBIT);
+
+        }
+    }
+
+    // store k8xn8
+    #pragma unroll
+    for (int i = 0; i < COMPACT_FACTOR; i++) {
+        int gvm_addr_offset = tile_k_idx * n + tile_n_idx * COMPACT_FACTOR + i;
+        int pred_k = tile_k_idx * COMPACT_FACTOR < k;
+        int pred_n = tile_n_idx * COMPACT_FACTOR + i < n;
+        if (pred_k && pred_n) {
+            *(output + gvm_addr_offset) = gptq_data[i];
+        } else {
+            *(output + gvm_addr_offset) = 0x00000000;
+        }
+    }
+}
+
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm_gptq(int m,
+                      int n,
+                      int k,
+                      int quant_group,
+                      const input_tp *dA,
+                      int lda,
+                      const quant_packed_tp *dB,
+                      int ldb,
+                      output_tp *dC,
+		      float *dC_temp,
+                      int ldc,
+                      quant_packed_tp *d_zeros,
+                      input_tp *d_scales,
+                      const cudaStream_t stream,
+                      int chunks = 1) {
+    using namespace hgemm_marlin_gptq;
+    if(n % 16 != 0) {
+        printf("n %% 16 != 0, n = %d\n", n);
+        return false;
+    }
+    if(k % 32 != 0) {
+        printf("k %% 32 != 0, k = %d\n", k);
+        return false;
+    }
+    //const vllm::ScalarTypeId w_type_id = vllm::kU4B8.id();
+    const int THREADS = 256;
+    int BLOCKS_M = div_ceil(m, SLICE_M);
+    if(BLOCKS_M >= MAX_BLOCKS_M && BLOCKS_M % MAX_BLOCKS_M != 0) {
+        printf("Error: input m is error, m = %d, blocks_m = %d\n", m, BLOCKS_M);
+        return false;
+    }
+    if (BLOCKS_M > MAX_BLOCKS_M) BLOCKS_M = MAX_BLOCKS_M;
+    int BLOCKS_N = 8;
+    //It is better let TILE_K = quant_group
+    //But if quant_group is too large, a quant_group can be divided into two parts
+    int BLOCKS_K = quant_group / SLICE_K;
+    if (quant_group > 128) BLOCKS_K = 128 / SLICE_K;
+    //if (BLOCKS_M == 1 || BLOCKS_M == 2) {
+    //    BLOCKS_N = 16;
+    //}
+    const bool HAS_ACT_ORDER = false;
+    const bool HAS_ZP = (w_type_id == vllm::kU4.id()) || (w_type_id == vllm::kU8.id());
+    int *g_idx = nullptr;
+    bool HAS_NK_PRED = true;
+    bool HAS_M_PRED = true;
+    if (n % TILE_N == 0 && k % TILE_K == 0) {
+        HAS_NK_PRED = false;
+    }
+    if (m % TILE_M == 0) {
+        HAS_M_PRED = false;
+    }
+
+#define LAUNCH_AWQ(threads, bm, bn, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    else if (THREADS == threads && BLOCKS_M == bm && BLOCKS_N == bn \
+        && BLOCKS_K == bk  && HAS_ACT_ORDER == has_act_order \
+        && HAS_ZP == has_zp \
+        && HAS_M_PRED == has_m_pred && HAS_NK_PRED == has_nk_pred) { \
+            launch_gemm_gptq_kernel<input_tp, w_type_id, \
+                    threads, bm, bn, bk, has_act_order, has_zp, has_m_pred, has_nk_pred>( \
+                    (const PackTypeInt4*)dA, \
+                    (const PackTypeInt4*)dB, \
+                    (PackTypeInt4*)dC, \
+                    (PackTypeInt4*)dC_temp, \
+                    (const PackTypeInt4*)d_scales, \
+                    (const PackTypeInt4*)d_zeros, \
+                    nullptr, m, n, k, quant_group, chunks,\
+                    stream); \
+    }
+
+#define LAUNCH_AWQ_K(bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 1, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 2, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 3, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 4, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 1, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 2, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred)
+
+
+#define LAUNCH_AWQ_ZP(has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(1, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(2, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(4, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(8, false, has_zp, has_nk_pred, has_m_pred)
+
+#define LAUNCH_AWQ_PRED(has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_ZP(false, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_ZP(true, has_nk_pred, has_m_pred)
+
+    if (false) {
+
+    }
+    LAUNCH_AWQ_PRED(true, true)
+    LAUNCH_AWQ_PRED(true, false)
+    LAUNCH_AWQ_PRED(false, true)
+    LAUNCH_AWQ_PRED(false, false)
+    else {
+        printf("BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_k=%d, THREADS=%d, HAS_ACT_ORDER=%d, HAS_ZP=%d, quant_group=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d is not supported\n",
+        BLOCKS_M, BLOCKS_N, BLOCKS_K, THREADS, HAS_ACT_ORDER, HAS_ZP, quant_group, HAS_M_PRED, HAS_NK_PRED);
+        return false;
+    }
+
+    return true;
+}
+
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm(int m,
+                int n,
+                int k,
+                const input_tp *dA,
+                int lda,
+                const quant_packed_tp *dB,
+                int ldb,
+                output_tp *dC,
+		float* dC_temp,
+                int ldc,
+                quant_packed_tp *d_zeros,
+                input_tp *d_scales,
+		const cudaStream_t stream) {
+    using namespace hgemm_marlin_gptq;
+    //constexpr int max_blocks_m = 4;
+    int total_m_blocks = div_ceil(m, SLICE_M);
+    int chunks = total_m_blocks / MAX_BLOCKS_M;
+    int rest_blocks_m = total_m_blocks % MAX_BLOCKS_M;
+    // printf("m=%d,n=%d,k=%d,lda=%d,ldb=%d,ldc=%d,total_m_blocks=%d,chunks=%d,rest_blocks_m=%d\n",
+    //     m, n, k, lda, ldb, ldc, total_m_blocks, chunks, rest_blocks_m
+    // );
+    //const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    const int quant_group = 128;
+    bool ret = true;
+    if (chunks > 0) {
+        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
+        ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(real_m, n, k, quant_group, dA, lda, dB, ldb, dC, dC_temp, ldc, d_zeros, d_scales, stream, chunks);
+    }
+    if (rest_blocks_m > 0) {
+        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
+        ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(m - m_offset, n, k, quant_group, dA + lda * m_offset, lda, dB, ldb, dC + ldc * m_offset, dC_temp + ldc * m_offset, ldc, d_zeros, d_scales, stream, 1);
+    }
+
+    return ret;
+}
+
+
+
 }  // namespace awq
 }  // namespace vllm
 
+torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight) {
+
+  const at::cuda::OptionalCUDAGuard device_guard(device_of(qweight));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  const uint32_t* qweight_ptr = reinterpret_cast<const uint32_t*>(qweight.data_ptr<int>());
+
+  int num_in_channels = qweight.size(0);
+  int num_out_channels = qweight.size(1) * 8;
+
+  int compact_n = (num_out_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;
+  int compact_output_k = (num_in_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;;
+
+  int block_size = 256;
+  int tile_all_num = compact_n * compact_output_k;
+  int grid_size = (tile_all_num + 255) / 256;
+
+  auto options = torch::TensorOptions()
+                     .dtype(qweight.dtype())
+                     .device(qweight.device());
+
+  torch::Tensor out = torch::zeros({num_out_channels,  compact_output_k}, options);
+  uint32_t* out_ptr = reinterpret_cast<uint32_t*>(out.data_ptr<int>());
+
+  vllm::awq::awq_to_gptq_4bit<256><<<grid_size, block_size, 0, stream>>>((uint32_t*)out_ptr, (const uint32_t*)qweight_ptr, num_in_channels, num_out_channels);
+
+  return out;
+}
+
 torch::Tensor awq_dequantize(torch::Tensor _kernel,
                              torch::Tensor _scaling_factors,
                              torch::Tensor _zeros, int64_t split_k_iters,
@@ -468,59 +958,62 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
 
 torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
-                       int64_t split_k_iters) {
+                       int64_t split_k_iters,
+                       torch::Tensor _temp_space,
+                       bool dtype_bf16) {
   int num_in_feats = _in_feats.size(0);
   int num_in_channels = _in_feats.size(1);
   const at::cuda::OptionalCUDAGuard device_guard(device_of(_in_feats));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 
   auto options = torch::TensorOptions()
                      .dtype(_in_feats.dtype())
                      .device(_in_feats.device());
+                     
+  // int num_out_channels = _kernel.size(1) * 8;
+  int num_out_channels = _kernel.size(0); 
   at::Tensor _out_feats =
-      torch::empty({split_k_iters, num_in_feats, _kernel.size(1) * 8}, options);
-  int num_out_feats = _out_feats.size(-2);
-  int num_out_channels = _out_feats.size(-1);
+      torch::zeros({num_in_feats, num_out_channels}, options);
 
-  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
+  //auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
   auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
-  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
-  auto scaling_factors =
-      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+  //auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
+  //auto scaling_factors =
+  //    reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
   auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
+  auto temp_space = reinterpret_cast<float*>(_temp_space.data_ptr<float>());
   int group_size = num_in_channels / _scaling_factors.size(0);
 
-  if (num_out_channels % 64 != 0)
-    throw std::invalid_argument("OC is not multiple of cta_N = 64");
-  if (num_out_channels % 8 != 0)
-    throw std::invalid_argument("OC is not multiple of pack_num = 8");
-  if (group_size % 32 != 0)
-    throw std::invalid_argument("Group size should be a multiple of 32");
-  if (num_out_channels % group_size != 0)
-    throw std::invalid_argument("OC is not multiple of Group size");
+#if 0
+  int lda = num_out_channels;
+  int ldb = num_in_channels;
+  int ldc = num_out_channels;
 
+  float alpha = 1.0, beta = 0.0;
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-  if (num_out_channels % 128 == 0) {
-    int j_factors1 = num_out_channels / 128 / 1;
-    dim3 num_blocks((num_out_feats + 16 - 1) / 16 * j_factors1 * split_k_iters);
-    // threadIdx.x: 32
-    // threadIdx.y: i_factors[2] * j_factors[2]
-    dim3 threads_per_block(32, 2);
-    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<128>
-        <<<num_blocks, threads_per_block, 0, stream>>>(
-            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
-            num_in_feats, num_in_channels, num_out_channels, out_feats);
-  } else if (num_out_channels % 64 == 0) {
-    int j_factors1 = num_out_channels / 64 / 1;
-    dim3 num_blocks(1 * (num_out_feats + 16 - 1) / 16 * j_factors1 *
-                    split_k_iters);
-
-    // threadIdx.x: 32
-    // threadIdx.y: i_factors[2] * j_factors[2]
-    dim3 threads_per_block(32, 2);
-    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<64>
-        <<<num_blocks, threads_per_block, 0, stream>>>(
-            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
-            num_in_feats, num_in_channels, num_out_channels, out_feats);
+  vllm::awq::launch_gemm_awq(Operation_t(0), Operation_t(0), num_out_channels, num_in_feats, num_in_channels, alpha, beta, (const uint32_t*)kernel, lda,
+                             (const half*)in_feats, ldb,
+                             (half*)out_feats, ldc, (uint32_t*)zeros, (half*)scaling_factors, space_mid, stream, 3);
+#endif
+
+  int lda = num_in_channels;
+  int ldb = num_out_channels;
+  int ldc = num_out_channels;
+
+  if (dtype_bf16) {
+	  using scalar_t = __maca_bfloat16;
+	  vllm::awq::launch_gemm<scalar_t, vllm::kU4.id(), scalar_t, quant_packed_type>(num_in_feats, num_out_channels, num_in_channels,
+                                     (const scalar_t*)_in_feats.data_ptr(), lda, (const uint32_t*)kernel, ldb, (scalar_t*)_out_feats.data_ptr(), temp_space, ldc,
+                                     (uint32_t*)zeros, (scalar_t*)_scaling_factors.data_ptr(), stream);
+  } else {
+	  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
+	  auto scaling_factors = reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+	  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
+	  vllm::awq::launch_gemm<input_type, vllm::kU4.id(), output_type, quant_packed_type>(num_in_feats, num_out_channels, num_in_channels,
+                                     (const half*)in_feats, lda, (const uint32_t*)kernel, ldb, (half*)out_feats, nullptr, ldc,
+                                     (uint32_t*)zeros, (half*)scaling_factors, stream);
   }
-  return _out_feats.sum(0);
+
+
+  return _out_feats;
 }
diff --git a/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
new file mode 100644
index 000000000..efc366c76
--- /dev/null
+++ b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
@@ -0,0 +1,521 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+/**
+ * @file hgemv_nn_splitk.hpp
+ * @author Jiawen Yang (jiawen.yang@metax-tech.com)
+ * @brief *
+ * @version 0.1
+ * @date 2024-03-05
+ * @copyright Copyright (c) 2024
+ *
+ *   fp16 gemv kernel template for some gemv cases
+ *   Note:
+ *    1. BlockDimX * BlockDimY = 64, and BlockDimX should be 8/16/32/64
+ *    2. LoopNum % 2 == 0, so Load_B can use ldg_b32 or ldg_b64
+ *    3. m % (BlockDimX * 8) == 0
+ *    4. k % (ThreadBlock / BlockDimX * LoopNum * SplitKNum) = 0
+ *
+ *    A load layout:
+ *
+ *       **************************** Wave_0 ******************* | Wave_1  ...
+ *       ********* Repeat LoopNum *********                      |
+ *       tid_0(ldg_b128)   tid_0 ... tid_0 | tid_(BlockDimX) ... |
+ *       tid_1                             |                     |
+ *       tid_2                             |                     |
+ *       ……                                |                     |
+ *       tid_(BlockDimX-1)                 |                     |
+ *
+ */
+#pragma once
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include "../gptq/Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+typedef __NATIVE_VECTOR__(2, float) v2f;
+
+template<int N, int m_per_thread>
+__device__ __forceinline__ void dequant_fma_awq_int4(
+                const quant_packed_type& a,
+                const v2f (&scale)[m_per_thread/2],
+                const v2f (&zero)[m_per_thread/2],
+                const v2f (&b)[N],
+                v2f (&out)[N][m_per_thread/2]) {
+    uint32_t p0 = a & 0x0f0f0f0f;
+    float o1,o3;
+    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+    v2f a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[0], zero[0]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][0] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][0]);
+    }
+
+    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+    a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[2], zero[2]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][2] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][2]);
+    }
+
+    uint32_t p1 = (a >> 4) & 0x0f0f0f0f;
+    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p1));
+    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p1));
+    a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[1], zero[1]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][1] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][1]);
+    }
+
+    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p1));
+    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p1));
+    a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[3], zero[3]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][3] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][3]);
+    }
+};
+
+template <int ThreadBlock, int BlockDimX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    if (splitKOffset + k_block > k) k = k - splitKOffset;
+    else k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = PACK_RATIO;
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+
+    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
+    half *bsm_scales_ptr;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread/2];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
+    int m_index = tidRow * m_per_thread;
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        int loading_count = this_group_elements / loading_pack;
+        //Load needed zeros, scales
+        const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            int packed_group_offset = (x >> 2) << 3;
+            int packed_index = (x << 1) % 8;
+            int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
+            int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
+            //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
+            temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[dest_offset_0] = temp_scales[0];
+            bsm_scales_ptr[dest_offset_1] = temp_scales[1];
+        }
+
+        int loop_index = 0;
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        if (m_index < this_group_elements) {
+            v2f local_scales[m_per_thread/2];
+            for (int c = 0; c < m_per_thread / 2; c++) {
+                float s0 = (float)bsm_scales_ptr[m_index + c*2];
+                float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
+                local_scales[c] = {s0, s1};
+            }
+            v2f local_zeros[m_per_thread/2];
+            for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
+
+    #define DEQUANT_FMA(a, b) \
+            dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
+
+            quant_packed_type A[4];
+            const int packed_a_stride = srcAStride / PACK_RATIO;
+            int src_a_offset = (loop_index + i + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
+            A[0] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[1] = srcA[src_a_offset];
+
+            v2f local_b[4][N];
+            //#pragma unroll LoopNum / 4 - 1
+            for (; loop_index < LoopNum - 4; loop_index += 4) {
+                //Load A
+                src_a_offset += packed_a_stride;
+                A[2] = srcA[src_a_offset];
+                src_a_offset += packed_a_stride;
+                A[3] = srcA[src_a_offset];
+
+                for (int y = 0; y < N; y++) {
+                    float s[4];
+                    *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+                    local_b[0][y] = {s[0], s[0]};
+                    local_b[1][y] = {s[1], s[1]};
+                    local_b[2][y] = {s[2], s[2]};
+                    local_b[3][y] = {s[3], s[3]};
+                }
+                DEQUANT_FMA(A[0], local_b[0])
+                DEQUANT_FMA(A[1], local_b[1])
+                src_a_offset += packed_a_stride;
+                A[0] = srcA[src_a_offset];
+                src_a_offset += packed_a_stride;
+                A[1] = srcA[src_a_offset];
+                DEQUANT_FMA(A[2], local_b[2])
+                DEQUANT_FMA(A[3], local_b[3])
+            }
+            src_a_offset += packed_a_stride;
+            A[2] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[3] = srcA[src_a_offset];
+            for (int y = 0; y < N; y++) {
+                float s[4];
+                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+                local_b[0][y] = {s[0], s[0]};
+                local_b[1][y] = {s[1], s[1]};
+                local_b[2][y] = {s[2], s[2]};
+                local_b[3][y] = {s[3], s[3]};
+            }
+            DEQUANT_FMA(A[0], local_b[0])
+            DEQUANT_FMA(A[1], local_b[1])
+            DEQUANT_FMA(A[2], local_b[2])
+            DEQUANT_FMA(A[3], local_b[3])
+        }
+        __syncthreads();
+    }
+#undef DEQUANT_FMA
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        if (m_index < this_group_elements) {
+            for (int i = 0; i < m_per_thread/2; i++) {
+                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
+                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
+            }
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
+
+template <int BX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq_kb128(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int ThreadBlock = 256;
+    constexpr int BlockDimX = BX;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = 128;
+    const int splitKOffset = blockIdx.y * k_block;
+
+    k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = PACK_RATIO;
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
+    half *bsm_scales_ptr;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread/2];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
+
+    constexpr int quant_group = 0;
+    constexpr int loading_pack = 2;
+    int loading_count = this_group_elements / loading_pack;
+    //Load needed zeros, scales
+    const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
+    for (int x = tid; x < loading_count; x += ThreadBlock) {
+        uint8_t temp_zeros = 0;
+        temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+        half temp_scales[2];
+        int packed_group_offset = (x >> 2) << 3;
+        int packed_index = (x << 1) % 8;
+        int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
+        int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
+        //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+        temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
+        temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
+        uint32_t z = temp_zeros;
+        uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
+        uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
+        float s1 = (float)(temp_scales[0]);
+        float s2 = (float)(temp_scales[1]);
+        //Store to shared memory
+        bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
+        bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
+        bsm_scales_ptr[dest_offset_0] = temp_scales[0];
+        bsm_scales_ptr[dest_offset_1] = temp_scales[1];
+    }
+
+    int loop_index = 0;
+
+    //Load B and transform to float
+    if (b_perm != nullptr) {
+        for (int y = 0; y < N; y++) {
+            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + tidCol * LoopNum + x] + y * k_stride];
+            }
+        }
+    } else {
+        for (int y = 0; y < N; y++) {
+            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + tidCol * LoopNum + x + y * k_stride];
+            }
+        }
+    }
+
+    __syncthreads();
+
+    //Load zero and scale from bsm
+    int m_index = tidRow * m_per_thread;
+    if (m_index < this_group_elements) {
+        v2f local_scales[m_per_thread/2];
+        for (int c = 0; c < m_per_thread / 2; c++) {
+            float s0 = (float)bsm_scales_ptr[m_index + c*2];
+            float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
+            local_scales[c] = {s0, s1};
+        }
+        v2f local_zeros[m_per_thread/2];
+        for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
+
+#define DEQUANT_FMA(a, b) \
+        dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
+
+        quant_packed_type A[4];
+        const int packed_a_stride = srcAStride / PACK_RATIO;
+        int src_a_offset = (loop_index + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
+        A[0] = srcA[src_a_offset];
+        src_a_offset += packed_a_stride;
+        A[1] = srcA[src_a_offset];
+
+        v2f local_b[4][N];
+        #pragma unroll LoopNum / 4 - 1
+        for (; loop_index < LoopNum - 4; loop_index += 4) {
+            //Load A
+            src_a_offset += packed_a_stride;
+            A[2] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[3] = srcA[src_a_offset];
+
+            for (int y = 0; y < N; y++) {
+                float s[4];
+                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+                local_b[0][y] = {s[0], s[0]};
+                local_b[1][y] = {s[1], s[1]};
+                local_b[2][y] = {s[2], s[2]};
+                local_b[3][y] = {s[3], s[3]};
+            }
+            DEQUANT_FMA(A[0], local_b[0])
+            DEQUANT_FMA(A[1], local_b[1])
+            src_a_offset += packed_a_stride;
+            A[0] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[1] = srcA[src_a_offset];
+            DEQUANT_FMA(A[2], local_b[2])
+            DEQUANT_FMA(A[3], local_b[3])
+        }
+        src_a_offset += packed_a_stride;
+        A[2] = srcA[src_a_offset];
+        src_a_offset += packed_a_stride;
+        A[3] = srcA[src_a_offset];
+        for (int y = 0; y < N; y++) {
+            float s[4];
+            *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+            local_b[0][y] = {s[0], s[0]};
+            local_b[1][y] = {s[1], s[1]};
+            local_b[2][y] = {s[2], s[2]};
+            local_b[3][y] = {s[3], s[3]};
+        }
+        DEQUANT_FMA(A[0], local_b[0])
+        DEQUANT_FMA(A[1], local_b[1])
+        DEQUANT_FMA(A[2], local_b[2])
+        DEQUANT_FMA(A[3], local_b[3])
+    }
+    __syncthreads();
+
+#undef DEQUANT_FMA
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        if (m_index < this_group_elements) {
+            for (int i = 0; i < m_per_thread/2; i++) {
+                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
+                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
+            }
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
diff --git a/csrc/quantization/awq/hgemv_selector.hpp b/csrc/quantization/awq/hgemv_selector.hpp
new file mode 100644
index 000000000..2a7fef782
--- /dev/null
+++ b/csrc/quantization/awq/hgemv_selector.hpp
@@ -0,0 +1,288 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#include <memory>
+#include <vector>
+#include <algorithm>
+#include "mc_runtime.h"
+#include "maca_fp16.h"
+
+struct KernelEventRecorder {
+    mcEvent_t _start;
+    mcEvent_t _stop;
+    float _eventMs = -1.f;
+
+    KernelEventRecorder() {
+        mcEventCreate(&_start);
+        mcEventCreate(&_stop);
+    }
+
+    ~KernelEventRecorder() {
+        mcEventDestroy(_start);
+        mcEventDestroy(_stop);
+    }
+
+    void start() {
+        mcEventRecord(_start, NULL);
+    }
+
+    float stop() {
+        mcEventRecord(_stop, NULL);
+        mcEventSynchronize(_stop);
+        mcEventElapsedTime(&_eventMs, _start, _stop);
+        return _eventMs;
+    }
+};
+namespace hgemv_selector {
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+struct GemvParamAutoSelector {
+    int m;
+    int k;
+    int best_block_x = 0;
+    int best_split_k = 0;
+
+    std::pair<int,int> block_x_range;
+    std::pair<int,int> split_k_range;
+    bool _valid = false;
+
+private:
+    std::vector<std::pair<int, int>> param_candidates;
+    int warmup_iters = 0;
+    int current_block_x = 0;
+    int current_split_k = 0;
+    int current_perf_iter = 0;
+    std::vector<float> perf_times;
+    float kernel_best_time_ms_ave = 99999999.0f;
+    float best_band_width;
+    float data_size_gb;
+    std::shared_ptr<KernelEventRecorder> _r;
+    bool _selected = false;
+    const static int MAX_PERF_COUNT = 20;
+
+public:
+    GemvParamAutoSelector(int _m, int _n, int _k, bool allow_imcomplete_bx = false) : m(_m), k(_k)
+    {
+        if (!prepare_parameters(_m, _n, _k, block_x_range.second, split_k_range.second, allow_imcomplete_bx)) return;
+        block_x_range.first = block_x_range.second;
+        split_k_range.first = split_k_range.second;
+        for (int i = 0; i < 3; i++) if (block_x_range.first / 2 >= 16) block_x_range.first /= 2;
+        for (int i = 0; i < 3; i++) if (split_k_range.first % 2 == 0 && split_k_range.first / 2 > 0) split_k_range.first /= 2;
+        for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+            for (int j = split_k_range.first; j <= split_k_range.second; j *= 2) {
+                param_candidates.emplace_back(i, j);
+            }
+        }
+        if (split_k_range.second * quant_group != k) {
+            int max_split_k = k / quant_group;
+            if (max_split_k < 256) {
+                for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+                    param_candidates.emplace_back(i, max_split_k);
+                }
+            }
+        }
+
+        current_block_x = block_x_range.second;
+        current_split_k = split_k_range.second;
+        uint64_t data_size_in_bytes = _n * _k * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_n * _m * sizeof(half);
+        data_size_gb = float(data_size_in_bytes) / 1024 / 1024 / 1024;
+        warmup_iters = 4;
+        _valid = true;
+    }
+
+    void select_in_warmup(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (_selected) {
+            f(best_block_x, best_split_k);
+            return;
+        };
+        //Warmup
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            for (int i = 0; i < 5; i++) {
+                auto &p = *iter;
+                f(p.first, p.second);
+            }
+        }
+        _r.reset(new KernelEventRecorder());
+        kernel_best_time_ms_ave = 9999999.0f;
+        mcDeviceSynchronize();
+
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            auto &p = *iter;
+            auto &bx = p.first;
+            auto &sk = p.second;
+            mcDeviceSynchronize();
+            _r->start();
+            bool launched = false;
+            for (int i = 0; i < MAX_PERF_COUNT; i++) {
+                launched = f(bx, sk);
+            }
+            auto ms = _r->stop();
+            if (launched && ms / MAX_PERF_COUNT < kernel_best_time_ms_ave) {
+                best_block_x = bx;
+                best_split_k = sk;
+                kernel_best_time_ms_ave = ms / MAX_PERF_COUNT;
+            }
+        }
+
+        _r.reset();
+        _selected = true;
+        warmup_iters = 0;
+    }
+
+    void run(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (warmup_iters > 0) {
+            f(current_block_x, current_split_k);
+            //printf("Calling warmup current_block_x = %d, current_split_k = %d\n", current_block_x, current_split_k);
+            if (current_block_x > block_x_range.first) current_block_x /= 2;
+            else if (current_split_k > split_k_range.first) current_split_k /= 2;
+            if (current_block_x == block_x_range.first && current_split_k >= split_k_range.first) {
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+            }
+            warmup_iters--;
+            mcDeviceSynchronize();
+            return;
+        }
+
+        if (_selected) {
+            f(best_block_x, best_split_k);
+        } else {
+            if (!_r) {
+                _r = std::shared_ptr<KernelEventRecorder>(new KernelEventRecorder());
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+                current_perf_iter = MAX_PERF_COUNT;
+            }
+            _r->start();
+            auto launched = f(current_block_x, current_split_k);
+            auto ms = _r->stop();
+            //printf("Launched with bx=%d,sk=%d,iter=%d,ms=%fms\n", current_block_x, current_split_k, current_perf_iter, ms);
+            if (!launched) {
+                printf("ERROR: Cannot run gemv with bx = %d, sk = %d\n", current_block_x, current_split_k);
+                return;
+            }
+            if (current_perf_iter-- > 0) {
+                perf_times.emplace_back(ms);
+                return;
+            }
+
+            std::sort(perf_times.begin(), perf_times.end());
+            float total_tm = 0;
+            for (size_t i = 1; i < perf_times.size() - 1; i++) {
+                total_tm += perf_times[i];
+            }
+
+            ms = total_tm /= (MAX_PERF_COUNT - 2);
+            perf_times.clear();
+            current_perf_iter = MAX_PERF_COUNT;
+            //printf("get ave time %fms\n", ms);
+
+            if (ms < kernel_best_time_ms_ave) {
+                best_block_x = current_block_x;
+                best_split_k = current_split_k;
+                kernel_best_time_ms_ave = ms;
+            }
+
+            if (current_split_k > split_k_range.first) {
+                current_split_k /= 2;
+            } else if (current_block_x > block_x_range.first){
+                current_split_k = split_k_range.second;
+                current_block_x /= 2;
+            } else if (current_block_x == block_x_range.first && current_split_k == split_k_range.first) {
+                _selected = true;
+                _r.reset();
+            } else {
+                printf("Error: all parameters are tried! current_block_x = %d, current_split_k = %d, block_x_range = %d,%d, split_k_range = %d,%d, best_block_x = %d, best_split_k = %d\n",
+                    current_block_x, current_split_k,
+                    block_x_range.first, block_x_range.second,
+                    split_k_range.first, split_k_range.second,
+                    best_block_x, best_split_k
+                );
+            }
+        }
+    }
+
+    bool valid() const { return _valid; }
+    bool selected() const {return _selected; }
+
+    float gemv_ave_time_us_cost() {
+        return kernel_best_time_ms_ave;
+    }
+
+    float gemv_bandwidth() {
+        return data_size_gb / (kernel_best_time_ms_ave / 1000.0f);
+    }
+
+    static bool prepare_parameters(int m, int n, int k, int &bx, int &sk, bool allow_imcomplete_bx = false) {
+        if (n > 4) return false;
+        if (k % quant_group != 0) return false;
+        if (m < 16 * m_per_thread) return false;
+        int max_split_k = k / quant_group;
+        int proper_splitk = 1;
+        while (proper_splitk*2 <= 256 && proper_splitk*2 <= max_split_k) proper_splitk *= 2;
+
+        int thread_block = 256;  //A thread block of 512 will wait for more warps when doing syncthreads
+
+        int proper_bx = 16;
+        if (m % (proper_bx * m_per_thread) != 0) return false;
+        //proper_bx * m_per_threads corresponds to group_elements, this value should not be larger than 2048 as limit of shared memory
+        while (proper_bx * 2 * m_per_thread && (m % (proper_bx*2*m_per_thread) == 0) && proper_bx * 2 <= 256 && proper_bx * 2 * m_per_thread <= 2048) proper_bx*=2;
+        if (allow_imcomplete_bx) {
+            int may_proper_bx = proper_bx * 2;
+            if (may_proper_bx <= 256 && may_proper_bx * m_per_thread <= 2048 && (m % (may_proper_bx*m_per_thread)) > may_proper_bx * m_per_thread / 4) {
+                proper_bx = may_proper_bx;
+            }
+        }
+
+        bx = proper_bx;
+        sk = proper_splitk;
+        //printf("m=%d,n=%d,k=%d,get bx=%d,sk=%d,allow_imcomplete_bx=%d\n", m, n, k, bx, sk, allow_imcomplete_bx);
+        return true;
+    }
+};
+
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+class GemvSelectorHolder {
+private:
+    std::vector<GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>> _selectors;
+    static std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> _holder;
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> _invalid_selector;
+
+public:
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& selector(int m, int n, int k, bool allow_imcomplete_bx = false) {
+        if (!GemvSelectorHolder::_holder) {
+            GemvSelectorHolder::_holder.reset(new GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>());
+        }
+        int bx, sk;
+        if (!GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>::prepare_parameters(m, n, k, bx, sk, allow_imcomplete_bx)) {
+            return _invalid_selector;
+        }
+        auto iter = std::find_if(_holder->_selectors.begin(), _holder->_selectors.end(),
+            [&](const GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& p) -> bool {
+                return p.m == m && p.k == k;
+            });
+        if (iter != _holder->_selectors.end()) return *iter;
+        GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> sl(m, n, k, allow_imcomplete_bx);
+        if (!sl.valid()) {
+            return _invalid_selector;
+        }
+        _holder->_selectors.emplace_back(sl);
+        return _holder->_selectors.back();
+    }
+};
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_holder;
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_invalid_selector(0,8,0);
+}
\ No newline at end of file
diff --git a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
index e79785827..9bf453e3b 100644
--- a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
+++ b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <ATen/cuda/CUDAContext.h>
 #include <torch/all.h>
 #include <cmath>
@@ -30,8 +31,13 @@ static inline __device__ int8_t float_to_int8_rn(float x) {
   return static_cast<int8_t>(dst);
 #else
   // CUDA path
-  uint32_t dst;
-  asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
+  //uint32_t dst;
+  //asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
+  //return reinterpret_cast<const int8_t&>(dst);
+  constexpr float c = 0.5;
+  int32_t dst;
+  dst = (int32_t)(x > 0 ? x + c: x - c);
+  dst = (char)(dst > 127 ? 127 : (dst < -128 ? -128 : dst));
   return reinterpret_cast<const int8_t&>(dst);
 #endif
 }
@@ -66,7 +72,9 @@ static inline __device__ int32_t float_to_int32_rn(float x) {
 #else
   // CUDA path
   uint32_t dst;
+#ifdef MX_MACA
   asm volatile("cvt.rni.sat.s32.f32 %0, %1;" : "=r"(dst) : "f"(x));
+#endif
   return reinterpret_cast<const int32_t&>(dst);
 #endif
 }
@@ -84,7 +92,9 @@ static inline __device__ int8_t int32_to_int8(int32_t x) {
 #else
   // CUDA path
   uint32_t dst;
+#ifdef MX_MACA
   asm volatile("cvt.sat.s8.s32 %0, %1;" : "=r"(dst) : "r"(x));
+#endif
   return reinterpret_cast<const int8_t&>(dst);
 #endif
 }
@@ -96,15 +106,12 @@ __global__ void static_scaled_int8_quant_kernel(
     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
     scale_type const* scale_ptr, const int hidden_size) {
   int const tid = threadIdx.x;
-  int64_t const token_idx = blockIdx.x;
+  int const token_idx = blockIdx.x;
   scale_type const scale = *scale_ptr;
 
-  // Must be performed using 64-bit math to avoid integer overflow.
-  out += token_idx * hidden_size;
-  input += token_idx * hidden_size;
-
   for (int i = tid; i < hidden_size; i += blockDim.x) {
-    out[i] = float_to_int8_rn(static_cast<float>(input[i]) / scale);
+    out[token_idx * hidden_size + i] = float_to_int8_rn(
+        static_cast<float>(input[token_idx * hidden_size + i]) / scale);
   }
 }
 
@@ -114,18 +121,14 @@ __global__ void static_scaled_int8_azp_quant_kernel(
     scale_type const* scale_ptr, azp_type const* azp_ptr,
     const int hidden_size) {
   int const tid = threadIdx.x;
-  int64_t const token_idx = blockIdx.x;
+  int const token_idx = blockIdx.x;
   scale_type const scale = *scale_ptr;
   azp_type const azp = *azp_ptr;
 
-  // Must be performed using 64-bit math to avoid integer overflow.
-  out += token_idx * hidden_size;
-  input += token_idx * hidden_size;
-
   for (int i = tid; i < hidden_size; i += blockDim.x) {
-    auto const val = static_cast<float>(input[i]);
+    auto const val = static_cast<float>(input[token_idx * hidden_size + i]);
     auto const quant_val = int32_to_int8(float_to_int32_rn(val / scale) + azp);
-    out[i] = quant_val;
+    out[token_idx * hidden_size + i] = quant_val;
   }
 }
 
@@ -134,16 +137,12 @@ __global__ void dynamic_scaled_int8_quant_kernel(
     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
     scale_type* scale, const int hidden_size) {
   int const tid = threadIdx.x;
-  int64_t const token_idx = blockIdx.x;
+  int const token_idx = blockIdx.x;
   float absmax_val = 0.0f;
   float const zero = 0.0f;
 
-  // Must be performed using 64-bit math to avoid integer overflow.
-  out += token_idx * hidden_size;
-  input += token_idx * hidden_size;
-
   for (int i = tid; i < hidden_size; i += blockDim.x) {
-    float val = static_cast<float>(input[i]);
+    float val = static_cast<float>(input[token_idx * hidden_size + i]);
     val = val > zero ? val : -val;
     absmax_val = val > absmax_val ? val : absmax_val;
   }
@@ -161,7 +160,213 @@ __global__ void dynamic_scaled_int8_quant_kernel(
 
   float const tmp_scale = 127.0f / block_absmax_val;
   for (int i = tid; i < hidden_size; i += blockDim.x) {
-    out[i] = float_to_int8_rn(static_cast<float>(input[i]) * tmp_scale);
+    out[token_idx * hidden_size + i] = float_to_int8_rn(
+        static_cast<float>(input[token_idx * hidden_size + i]) * tmp_scale);
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__global__ void dynamic_scaled_int8_quant_kernel_sreg_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, int blockDim_x) {
+  int const tid = threadIdx.x;
+  int const token_idx = blockIdx.x;
+  scalar_t absmax_val = static_cast<scalar_t>(0.0f);
+  float const zero = 0.0f;
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  scalar_t reg_src0[N];
+  scalar_t const* ptr_input = input + token_idx * hidden_size;
+  int reg_length = blockDim_x * N;
+  int length = min(hidden_size, reg_length);
+  int index = tid * N;
+  if(index < length) {
+    *(VT*)reg_src0 = *(VT*)(ptr_input + index);
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      scalar_t val = abs(reg_src0[i]);
+      absmax_val = max(absmax_val, val);
+    }
+  }
+
+  using BlockReduce = cub::BlockReduce<scalar_t, 512>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  float const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
+  __shared__ scale_type block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
+    scale[token_idx] = static_cast<scale_type>(block_absmax_val / 127.0f);
+  }
+  __syncthreads();
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  if(index < length) {
+    VT1 vdst;
+    int8_t* ptr_reg = (int8_t*)&vdst;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      ptr_reg[i] = float_to_int8_rn(
+            static_cast<float>(reg_src0[i]) * tmp_scale);
+    }
+    *(VT1*)(ptr_output + index) = vdst;
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__global__ void dynamic_scaled_int8_quant_kernel_reg_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, int blockDim_x) {
+  int const tid = threadIdx.x;
+  int const token_idx = blockIdx.x;
+  scalar_t absmax_val = static_cast<scalar_t>(0.0f);
+  float const zero = 0.0f;
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  scalar_t reg_src0[N];
+  scalar_t reg_src1[N];
+  scalar_t const* ptr_input = input + token_idx * hidden_size;
+  int reg_length = 2 * blockDim_x * N;
+  int length = min(hidden_size, reg_length);
+  int index = 2 * tid * N;
+  if(index < length) {
+    *(VT*)reg_src0 = *(VT*)(ptr_input + index);
+    *(VT*)reg_src1 = *(VT*)(ptr_input + index + N);
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      scalar_t val = abs(reg_src0[i]);
+      absmax_val =  max(val, absmax_val);
+      val = abs(reg_src1[i]);
+      absmax_val = max(val, absmax_val);
+    }
+  }
+
+  using BlockReduce = cub::BlockReduce<scalar_t, 512>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  scalar_t const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
+  __shared__ scale_type block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
+    scale[token_idx] = block_absmax_val / 127.0f;
+  }
+  __syncthreads();
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  if(index < length) {
+    VT1 vdst;
+    int8_t* ptr_reg = (int8_t*)&vdst;
+    constexpr int ON = 2 * N;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      ptr_reg[i] = float_to_int8_rn(
+             static_cast<float>(reg_src0[i]) * tmp_scale);
+    }
+    ptr_reg = ptr_reg + N;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      ptr_reg[i] = float_to_int8_rn(
+            static_cast<float>(reg_src1[i]) * tmp_scale);
+    }
+    *(VT1*)(ptr_output + index) = vdst;
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__global__ void dynamic_scaled_int8_quant_kernel_sm_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, int blockDim_x) {
+  int const tid = threadIdx.x;
+  int const token_idx = blockIdx.x;
+  float absmax_val = 0.0f;
+  float const zero = 0.0f;
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  int stride = blockDim_x * N;
+  __shared__ float sm_buffer[8064];
+  scalar_t const* ptr_input = input + token_idx * hidden_size;
+  for(int i = tid * N; i < hidden_size; i += stride) {
+    VT vsrc = *(VT*)(ptr_input + i);
+    scalar_t *ptr_src = (scalar_t*)&vsrc;
+    float* ptr_sm_buffer = sm_buffer + i;
+    #pragma unroll N
+    for(int j = 0; j < N; j++) {
+        float val = static_cast<float>(ptr_src[j]);
+        ptr_sm_buffer[j] = val;
+        val = val > zero ? val : -val;
+        absmax_val = val > absmax_val ? val : absmax_val;
+    }
+  }
+  using BlockReduce = cub::BlockReduce<float, 512>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  float const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
+  __shared__ float block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = block_absmax_val_maybe;
+    scale[token_idx] = block_absmax_val / 127.0f;
+  }
+  
+  __syncthreads();
+
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  for(int i = tid * N; i < hidden_size; i += stride) {
+    VT1 vdst;
+    int8_t* ptr_reg = (int8_t*)&vdst;
+    float* ptr_sm_buffer = sm_buffer + i;
+    #pragma unroll N
+    for(int j = 0; j < N; j++) {
+        ptr_reg[j] = float_to_int8_rn(
+            ptr_sm_buffer[j] * tmp_scale);
+    }
+    *(VT1*)(ptr_output + i) = vdst;
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__launch_bounds__(1024) __global__ void dynamic_scaled_int8_quant_kernel_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, const int blockDim_x) {
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  int const tid = threadIdx.x * N;
+  int const token_idx = blockIdx.x;
+  float absmax_val = 0.0f;
+  int stride = blockDim_x * N;
+  const scalar_t * ptr_input = input + token_idx * hidden_size;
+
+  for (int i = tid ; i < hidden_size; i += stride) {
+    VT vsrc = *(VT*)(ptr_input + i);
+    scalar_t *ptr_src = (scalar_t*)&vsrc;
+    #pragma unroll N
+    for(int j = 0; j < N; j++) {
+        float val = static_cast<float>(ptr_src[j]);
+        val = val > 0 ? val : -val;
+        absmax_val = val > absmax_val ? val : absmax_val;
+    }
+  }
+
+    using BlockReduce = cub::BlockReduce<float, 1024>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  float const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
+  __shared__ float block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = block_absmax_val_maybe;
+    scale[token_idx] = block_absmax_val / 127.0f;
+  }
+  __syncthreads();
+
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  for (int i = tid; i < hidden_size; i += stride) {
+    VT vsrc = *(VT*)(ptr_input + i);
+    VT1 vdst;
+    scalar_t *ptr_src = (scalar_t*)&vsrc;
+    int8_t* ptr_dst = (int8_t*)&vdst;
+    #pragma unroll N
+    for(int j = 0; j < N; ++j) {
+        ptr_dst[j] = float_to_int8_rn(
+        static_cast<float>(ptr_src[j]) * tmp_scale);
+    }
+    *(VT1*)(ptr_output + i) = vdst;
   }
 }
 
@@ -169,17 +374,13 @@ template <typename scalar_t, typename scale_type, typename azp_type>
 __global__ void dynamic_scaled_int8_azp_quant_kernel(
     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
     scale_type* scale, azp_type* azp, const int hidden_size) {
-  int64_t const token_idx = blockIdx.x;
-
-  // Must be performed using 64-bit math to avoid integer overflow.
-  out += token_idx * hidden_size;
-  input += token_idx * hidden_size;
+  int const token_idx = blockIdx.x;
 
   // Scan for the min and max value for this token
   float max_val = std::numeric_limits<float>::min();
   float min_val = std::numeric_limits<float>::max();
   for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-    auto val = static_cast<float>(input[i]);
+    auto val = static_cast<float>(input[token_idx * hidden_size + i]);
     max_val = std::max(max_val, val);
     min_val = std::min(min_val, val);
   }
@@ -214,10 +415,10 @@ __global__ void dynamic_scaled_int8_azp_quant_kernel(
 
   // Quantize the values
   for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-    auto const val = static_cast<float>(input[i]);
+    auto const val = static_cast<float>(input[token_idx * hidden_size + i]);
     auto const quant_val =
         int32_to_int8(float_to_int32_rn(val / scale_val) + azp_val);
-    out[i] = quant_val;
+    out[token_idx * hidden_size + i] = quant_val;
   }
 }
 
@@ -226,7 +427,7 @@ __global__ void dynamic_scaled_int8_azp_quant_kernel(
 void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
                               torch::Tensor const& input,  // [..., hidden_size]
                               torch::Tensor const& scale,
-                              std::optional<torch::Tensor> const& azp) {
+                              c10::optional<torch::Tensor> const& azp) {
   TORCH_CHECK(input.is_contiguous());
   TORCH_CHECK(out.is_contiguous());
   TORCH_CHECK(scale.numel() == 1);
@@ -257,7 +458,7 @@ void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]
 void dynamic_scaled_int8_quant(
     torch::Tensor& out,          // [..., hidden_size]
     torch::Tensor const& input,  // [..., hidden_size]
-    torch::Tensor& scales, std::optional<torch::Tensor> const& azp) {
+    torch::Tensor& scales, c10::optional<torch::Tensor> const& azp) {
   TORCH_CHECK(input.is_contiguous());
   TORCH_CHECK(out.is_contiguous());
   TORCH_CHECK(scales.is_contiguous());
@@ -271,10 +472,30 @@ void dynamic_scaled_int8_quant(
   VLLM_DISPATCH_FLOATING_TYPES(
       input.scalar_type(), "dynamic_scaled_int8_quant_kernel", [&] {
         if (!azp) {
-          vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>
-              <<<grid, block, 0, stream>>>(
-                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
-                  scales.data_ptr<float>(), hidden_size);
+          int n = 16 / sizeof(scalar_t);
+          if(hidden_size <= 4096 && ((hidden_size & (n - 1)) == 0) && n == 8) {
+            int gridsize = num_tokens;
+            int blocksize = 512;
+            vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
+          } else if(hidden_size > 4096 &&hidden_size <= 8192 && ((hidden_size & (2*n - 1)) == 0) && n == 8) {
+            int gridsize = num_tokens; int blocksize = 512;
+            vllm::dynamic_scaled_int8_quant_kernel_reg_opt<scalar_t, float, float4, float4><<<gridsize, blocksize, 0, stream>>>(input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
+          } else if(hidden_size <= 8064 && (hidden_size & (n - 1)) == 0 && n == 8) {
+            int gridsize = num_tokens;
+            int blocksize = 512;
+            vllm::dynamic_scaled_int8_quant_kernel_sm_opt<scalar_t, float, float4, float2><<<gridsize, blocksize, 0, stream>>>(
+              input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
+          } else if (hidden_size > 8064 && ((hidden_size & (n - 1)) == 0 && n == 8)) {
+            int blocksize = 1024;
+            vllm::dynamic_scaled_int8_quant_kernel_opt<scalar_t, float,float4,float2>
+                    <<<grid, blocksize, 0, stream>>>(
+                        input.data_ptr<scalar_t>(),out.data_ptr<int8_t>(),scales.data_ptr<float>(),hidden_size,blocksize);
+          } else {
+              vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>
+                  <<<grid, block, 0, stream>>>(
+                      input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+                      scales.data_ptr<float>(), hidden_size);
+          }
         } else {
           vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t>
               <<<grid, block, 0, stream>>>(
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
index 865fef5ae..835f319b3 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
@@ -1,14 +1,10 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <stddef.h>
 #include <torch/all.h>
-#include "cutlass/cutlass.h"
+#include "mctlass/mctlass.h"
 
 #include "scaled_mm_c2x.cuh"
 #include "scaled_mm_c2x_sm75_dispatch.cuh"
-#include "scaled_mm_c2x_sm80_dispatch.cuh"
-#include "scaled_mm_c2x_sm89_fp8_dispatch.cuh"
-#include "scaled_mm_c2x_sm89_int8_dispatch.cuh"
-
-#include "cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp"
 
 using namespace vllm;
 
@@ -22,6 +18,7 @@ template <template <typename, typename> typename Epilogue,
 void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
                                      torch::Tensor const& b,
                                      EpilogueArgs&&... epilogue_args) {
+#if 0
   TORCH_CHECK(a.dtype() == torch::kInt8);
   TORCH_CHECK(b.dtype() == torch::kInt8);
 
@@ -33,26 +30,196 @@ void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
     return cutlass_gemm_sm75_dispatch<int8_t, cutlass::half_t, Epilogue>(
         out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);
   }
+#endif
 }
 
 void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
                             torch::Tensor const& b,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
-                            std::optional<torch::Tensor> const& bias) {
-  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
-  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
-  if (bias) {
-    TORCH_CHECK(bias->dtype() == out.dtype(),
-                "currently bias dtype must match output dtype ", out.dtype());
-    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBias>(
-        out, a, b, a_scales, b_scales, *bias);
-  } else {
-    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogue>(
-        out, a, b, a_scales, b_scales);
-  }
+                            c10::optional<torch::Tensor> const& bias) {
+    int32_t m = a.size(0);
+    int32_t n = b.size(1);
+    int32_t k = a.size(1);
+
+    using ArchTag = mctlass::arch::Sm80;
+    using ElementA = int8_t;
+    using ElementB = int8_t;
+    using ElementC = mctlass::half_t;
+    using ElementCompute = float;
+    using LayoutA = mctlass::layout::RowMajor;
+    //using LayoutB = mctlass::layout::RowMajor;
+    using LayoutB = mctlass::layout::ColumnMajor;
+    using LayoutC = mctlass::layout::RowMajor;
+
+    if (out.dtype() == torch::kBFloat16)
+    {
+    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
+    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
+    auto c_ptr = static_cast<maca_bfloat16*>(out.data_ptr());
+    auto scale_a = a_scales.data_ptr<float>();
+    auto scale_b = b_scales.data_ptr<float>();
+    auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
+    if (bias) {
+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+        mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
+        using mctlassGemmScaleOp = mctlassGemmScale<
+          ElementA,
+          LayoutA,
+          ElementB,
+          LayoutB,
+          maca_bfloat16,
+          LayoutC,
+          ElementCompute,
+          ArchTag,
+          scale_type
+        >;
+        maca_bfloat16 *bias_t;
+        bias_t = static_cast<maca_bfloat16 *>(bias.value().data_ptr());
+        mctlassGemmScaleOp mctlass_op;
+        mctlass::gemm::GemmCoord problem_size(m, n, k);
+        typename mctlassGemmScaleOp::Arguments arguments{
+            mctlass::gemm::GemmUniversalMode::kGemm,
+            problem_size,
+            1,//batch_count
+            {scale_a, scale_b, bias_t},
+            a_ptr,
+            b_ptr,
+            c_ptr,
+            c_ptr,
+            m * k,
+            n * k,
+            m * n,
+            m * n,
+            k,
+            n,
+            n,
+            n
+        };
+        mctlass_op(arguments, NULL, stream);
+    }
+    else{
+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+        mctlass::epilogue::thread::ScaleType::ScaleAvBv;
+        using mctlassGemmScaleOp = mctlassGemmScale<
+          ElementA,
+          LayoutA,
+          ElementB,
+          LayoutB,
+          maca_bfloat16,
+          LayoutC,
+          ElementCompute,
+          ArchTag,
+          scale_type
+        >;
+        mctlassGemmScaleOp mctlass_op;
+        mctlass::gemm::GemmCoord problem_size(m, n, k);
+        typename mctlassGemmScaleOp::Arguments arguments{
+            mctlass::gemm::GemmUniversalMode::kGemm,
+            problem_size,
+            1,//batch_count
+            {scale_a, scale_b, nullptr},
+            a_ptr,
+            b_ptr,
+            c_ptr,
+            c_ptr,
+            m * k,
+            n * k,
+            m * n,
+            m * n,
+            k,
+            n,
+            n,
+            n
+        };
+        mctlass_op(arguments, NULL, stream);
+    }
+    }
+    else{
+    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
+    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
+    auto c_ptr = static_cast<ElementC*>(out.data_ptr());
+    auto scale_a = a_scales.data_ptr<float>();
+    auto scale_b = b_scales.data_ptr<float>();
+    auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
+    if (bias) {
+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+        mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
+        using mctlassGemmScaleOp = mctlassGemmScale<
+          ElementA,
+          LayoutA,
+          ElementB,
+          LayoutB,
+          ElementC,
+          LayoutC,
+          ElementCompute,
+          ArchTag,
+          scale_type
+        >;
+        ElementC *bias_t;
+        bias_t = static_cast<ElementC *>(bias.value().data_ptr());
+        mctlassGemmScaleOp mctlass_op;
+        mctlass::gemm::GemmCoord problem_size(m, n, k);
+        typename mctlassGemmScaleOp::Arguments arguments{
+            mctlass::gemm::GemmUniversalMode::kGemm,
+            problem_size,
+            1,//batch_count
+            {scale_a, scale_b, bias_t},
+            a_ptr,
+            b_ptr,
+            c_ptr,
+            c_ptr,
+            m * k,
+            n * k,
+            m * n,
+            m * n,
+            k,
+            n,
+            n,
+            n
+        };
+        mctlass_op(arguments, NULL, stream);
+    }
+    else{
+        mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+        mctlass::epilogue::thread::ScaleType::ScaleAvBv;
+        using mctlassGemmScaleOp = mctlassGemmScale<
+          ElementA,
+          LayoutA,
+          ElementB,
+          LayoutB,
+          ElementC,
+          LayoutC,
+          ElementCompute,
+          ArchTag,
+          scale_type
+        >;
+        mctlassGemmScaleOp mctlass_op;
+        mctlass::gemm::GemmCoord problem_size(m, n, k);
+        typename mctlassGemmScaleOp::Arguments arguments{
+            mctlass::gemm::GemmUniversalMode::kGemm,
+            problem_size,
+            1,//batch_count
+            {scale_a, scale_b, nullptr},
+            a_ptr,
+            b_ptr,
+            c_ptr,
+            c_ptr,
+            m * k,
+            n * k,
+            m * n,
+            m * n,
+            k,
+            n,
+            n,
+            n
+        };
+        mctlass_op(arguments, NULL, stream);
+    }
+    }
 }
 
+#if 0
 void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
                                 torch::Tensor const& b,
                                 torch::Tensor const& a_scales,
@@ -165,7 +332,7 @@ void cutlass_scaled_mm_sm89(torch::Tensor& out, torch::Tensor const& a,
                             torch::Tensor const& b,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
-                            std::optional<torch::Tensor> const& bias) {
+                            c10::optional<torch::Tensor> const& bias) {
   TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
   TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
   if (bias) {
@@ -178,14 +345,15 @@ void cutlass_scaled_mm_sm89(torch::Tensor& out, torch::Tensor const& a,
         out, a, b, a_scales, b_scales);
   }
 }
-
+#endif
 void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,
                                 torch::Tensor const& b,
                                 torch::Tensor const& a_scales,
                                 torch::Tensor const& b_scales,
                                 torch::Tensor const& azp_adj,
-                                std::optional<torch::Tensor> const& azp,
-                                std::optional<torch::Tensor> const& bias) {
+                                c10::optional<torch::Tensor> const& azp,
+                                c10::optional<torch::Tensor> const& bias) {
+#if 0
   TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
   TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
 
@@ -196,4 +364,5 @@ void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,
     return cutlass_scaled_mm_sm89_epilogue<c2x::ScaledEpilogueBiasAzp>(
         out, a, b, a_scales, b_scales, azp_adj, bias);
   }
+#endif
 }
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
index f2fae4b66..3af219185 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 #include <stddef.h>
 #include <torch/all.h>
@@ -6,6 +7,7 @@
 
 // clang-format will break include orders
 // clang-format off
+#if 0
 #include "cute/tensor.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cutlass/numeric_types.h"
@@ -20,6 +22,13 @@
 
 #include "cutlass/epilogue/threadblock/fusion/visitors.hpp"
 #include "cutlass/gemm/kernel/default_gemm_universal_with_visitor.h"
+#endif
+
+#include "mctlass/mctlass_ex.h"
+#include "mctlass/half.h"
+#include "mctlass/layout/matrix.h"
+#include "mctlass/epilogue/thread/scale_type.h"
+#include "mctlass/util/command_line.h"
 
 #include "core/math.hpp"
 #include "cutlass_extensions/common.hpp"
@@ -42,6 +51,7 @@ namespace vllm {
 // reduce the size of the compiled binary.
 // __CUDA_ARCH__ is not defined in host code, so this lets us smuggle the ifdef
 // into code that will be executed on the device where it is defined.
+#if 0
 template <typename Kernel>
 struct enable_sm75_to_sm80 : Kernel {
   template <typename... Args>
@@ -71,12 +81,15 @@ struct enable_sm89_to_sm90 : Kernel {
 #endif
   }
 };
+#endif
+
 template <typename Arch, template <typename> typename ArchGuard,
           typename ElementAB_, typename ElementD_,
           template <typename, typename> typename Epilogue_, typename TileShape,
           typename WarpShape, typename InstructionShape, int32_t MainLoopStages,
-          typename FP8MathOperator = cutlass::arch::OpMultiplyAdd>
+          typename FP8MathOperator = mctlass::arch::OpMultiplyAdd>
 struct cutlass_2x_gemm {
+#if 0
   using ElementAB = ElementAB_;
   using ElementD = ElementD_;
 
@@ -122,12 +135,14 @@ struct cutlass_2x_gemm {
   // clang-format on
 
   using Op = cutlass::gemm::device::GemmUniversalAdapter<KernelType>;
+#endif
 };
 
 template <typename Gemm, typename... EpilogueArgs>
 inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
                                 torch::Tensor const& b,
                                 EpilogueArgs&&... epilogue_params) {
+#if 0
   using ElementAB = typename Gemm::ElementAB;
   using ElementD = typename Gemm::ElementD;
 
@@ -188,6 +203,7 @@ inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
   CUTLASS_CHECK(gemm_op.can_implement(args));
   cutlass::Status status = gemm_op(args, workspace.data_ptr(), stream);
   CUTLASS_CHECK(status);
+#endif
 }
 
 template <typename Gemm, typename FallbackGemm, typename... EpilogueArgs>
@@ -195,6 +211,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
                                          torch::Tensor const& a,
                                          torch::Tensor const& b,
                                          EpilogueArgs&&... args) {
+#if 0
   // In some cases, the GPU isn't able to accommodate the
   // shared memory requirements of the Gemm. In such cases, use
   // the FallbackGemm instead.
@@ -215,6 +232,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
     return cutlass_gemm_caller<FallbackGemm>(
         out, a, b, std::forward<EpilogueArgs>(args)...);
   }
+#endif
 }
 
 }  // namespace vllm
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
index a562fd896..dcafcf0f7 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include "scaled_mm_c2x.cuh"
@@ -9,6 +10,7 @@
 
 namespace vllm {
 
+#if 0
 template <typename InType, typename OutType,
           template <typename, typename> typename Epilogue>
 struct sm75_config_default {
@@ -66,6 +68,7 @@ struct sm75_config_M32 {
       cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,
                       Epilogue, TileShape, WarpShape, InstructionShape, 2>;
 };
+#endif
 
 template <typename InType, typename OutType,
           template <typename, typename> typename Epilogue,
@@ -74,6 +77,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
                                        torch::Tensor const& a,
                                        torch::Tensor const& b,
                                        EpilogueArgs&&... args) {
+#if 0
   static_assert(std::is_same<InType, int8_t>());
   TORCH_CHECK(a.dtype() == torch::kInt8);
   TORCH_CHECK(b.dtype() == torch::kInt8);
@@ -118,6 +122,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
     return fallback_cutlass_gemm_caller<Cutlass2xGemmDefault, FallbackGemm>(
         out, a, b, std::forward<EpilogueArgs>(args)...);
   }
+#endif
 }
 
 }  // namespace vllm
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
index 6bef55088..87bd5506d 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <cudaTypedefs.h>
 
 #include <c10/cuda/CUDAGuard.h>
@@ -9,26 +10,26 @@ void cutlass_scaled_mm_sm75(torch::Tensor& c, torch::Tensor const& a,
                             torch::Tensor const& b,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
-                            std::optional<torch::Tensor> const& bias);
+                            c10::optional<torch::Tensor> const& bias);
 
 void cutlass_scaled_mm_sm80(torch::Tensor& c, torch::Tensor const& a,
                             torch::Tensor const& b,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
-                            std::optional<torch::Tensor> const& bias);
+                            c10::optional<torch::Tensor> const& bias);
 
 void cutlass_scaled_mm_sm89(torch::Tensor& c, torch::Tensor const& a,
                             torch::Tensor const& b,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
-                            std::optional<torch::Tensor> const& bias);
+                            c10::optional<torch::Tensor> const& bias);
 
 #if defined ENABLE_SCALED_MM_C3X && ENABLE_SCALED_MM_C3X
 void cutlass_scaled_mm_sm90(torch::Tensor& c, torch::Tensor const& a,
                             torch::Tensor const& b,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
-                            std::optional<torch::Tensor> const& bias);
+                            c10::optional<torch::Tensor> const& bias);
 #endif
 
 void cutlass_scaled_mm_azp_sm75(torch::Tensor& c, torch::Tensor const& a,
@@ -36,24 +37,24 @@ void cutlass_scaled_mm_azp_sm75(torch::Tensor& c, torch::Tensor const& a,
                                 torch::Tensor const& a_scales,
                                 torch::Tensor const& b_scales,
                                 torch::Tensor const& azp_adj,
-                                std::optional<torch::Tensor> const& azp,
-                                std::optional<torch::Tensor> const& bias);
+                                c10::optional<torch::Tensor> const& azp,
+                                c10::optional<torch::Tensor> const& bias);
 
 void cutlass_scaled_mm_azp_sm80(torch::Tensor& c, torch::Tensor const& a,
                                 torch::Tensor const& b,
                                 torch::Tensor const& a_scales,
                                 torch::Tensor const& b_scales,
                                 torch::Tensor const& azp_adj,
-                                std::optional<torch::Tensor> const& azp,
-                                std::optional<torch::Tensor> const& bias);
+                                c10::optional<torch::Tensor> const& azp,
+                                c10::optional<torch::Tensor> const& bias);
 
 void cutlass_scaled_mm_azp_sm89(torch::Tensor& c, torch::Tensor const& a,
                                 torch::Tensor const& b,
                                 torch::Tensor const& a_scales,
                                 torch::Tensor const& b_scales,
                                 torch::Tensor const& azp_adj,
-                                std::optional<torch::Tensor> const& azp,
-                                std::optional<torch::Tensor> const& bias);
+                                c10::optional<torch::Tensor> const& azp,
+                                c10::optional<torch::Tensor> const& bias);
 
 #if defined CUDA_VERSION && CUDA_VERSION >= 12000
 void cutlass_scaled_mm_azp_sm90(torch::Tensor& c, torch::Tensor const& a,
@@ -61,8 +62,8 @@ void cutlass_scaled_mm_azp_sm90(torch::Tensor& c, torch::Tensor const& a,
                                 torch::Tensor const& a_scales,
                                 torch::Tensor const& b_scales,
                                 torch::Tensor const& azp_adj,
-                                std::optional<torch::Tensor> const& azp,
-                                std::optional<torch::Tensor> const& bias);
+                                c10::optional<torch::Tensor> const& azp,
+                                c10::optional<torch::Tensor> const& bias);
 #endif
 
 bool cutlass_scaled_mm_supports_fp8(int64_t cuda_device_capability) {
@@ -97,60 +98,10 @@ bool cutlass_scaled_mm_supports_block_fp8(int64_t cuda_device_capability) {
 void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
                        torch::Tensor const& b, torch::Tensor const& a_scales,
                        torch::Tensor const& b_scales,
-                       std::optional<torch::Tensor> const& bias) {
-  // Checks for conformality
-  TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);
-  TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&
-              b.size(1) == c.size(1));
+                       c10::optional<torch::Tensor> const& bias) {
 
-  // Check for strides and alignment
-  TORCH_CHECK(a.stride(1) == 1 && c.stride(1) == 1);  // Row-major
-  TORCH_CHECK(b.stride(0) == 1);                      // Column-major
-  TORCH_CHECK(c.stride(0) % 16 == 0 &&
-              b.stride(1) % 16 == 0);  // 16 Byte Alignment
-
-  if (bias) {
-    TORCH_CHECK(bias->numel() == b.size(1) && bias->is_contiguous() &&
-                bias->dim() == 1);
-  }
-
-  at::cuda::OptionalCUDAGuard const device_guard(device_of(a));
-  int32_t version_num = get_sm_version_num();
-  // Hopper
-
-  // Guard against compilation issues for sm90 kernels
-#if defined ENABLE_SCALED_MM_C3X && ENABLE_SCALED_MM_C3X
-  if (version_num >= 90) {
-    cutlass_scaled_mm_sm90(c, a, b, a_scales, b_scales, bias);
-    return;
-  }
-#endif
+  cutlass_scaled_mm_sm75(c, a, b, a_scales, b_scales, bias);
 
-#if defined ENABLE_SCALED_MM_C2X && ENABLE_SCALED_MM_C2X
-  if (version_num == 89) {
-    // Ada Lovelace
-    cutlass_scaled_mm_sm89(c, a, b, a_scales, b_scales, bias);
-    return;
-  }
-
-  if (version_num >= 80) {
-    // Ampere
-    cutlass_scaled_mm_sm80(c, a, b, a_scales, b_scales, bias);
-    return;
-  }
-
-  if (version_num >= 75) {
-    // Turing
-    cutlass_scaled_mm_sm75(c, a, b, a_scales, b_scales, bias);
-    return;
-  }
-#endif
-
-  TORCH_CHECK_NOT_IMPLEMENTED(
-      false,
-      "No compiled cutlass_scaled_mm for a compute capability less than "
-      "CUDA device capability: ",
-      version_num);
 }
 
 void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
@@ -158,8 +109,9 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
                            torch::Tensor const& a_scales,
                            torch::Tensor const& b_scales,
                            torch::Tensor const& azp_adj,
-                           std::optional<torch::Tensor> const& azp,
-                           std::optional<torch::Tensor> const& bias) {
+                           c10::optional<torch::Tensor> const& azp,
+                           c10::optional<torch::Tensor> const& bias) {
+#if 0
   // Checks for conformality
   TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);
   TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&
@@ -225,4 +177,5 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
       "No compiled cutlass_scaled_mm_azp for a compute capability less than "
       "CUDA device capability: ",
       version_num);
+#endif
 }
diff --git a/csrc/quantization/fp8/common.cu b/csrc/quantization/fp8/common.cu
index e4f6615ed..8feafca25 100644
--- a/csrc/quantization/fp8/common.cu
+++ b/csrc/quantization/fp8/common.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include "common.cuh"
 #include "dispatch_utils.h"
 
@@ -126,7 +127,7 @@ void dynamic_scaled_fp8_quant(torch::Tensor& out,          // [..., d]
 void dynamic_per_token_scaled_fp8_quant(
     torch::Tensor& out,          // [..., d]
     torch::Tensor const& input,  // [..., d]
-    torch::Tensor& scales, std::optional<at::Tensor> const& scale_ub) {
+    torch::Tensor& scales, c10::optional<at::Tensor> const& scale_ub) {
   TORCH_CHECK(input.is_contiguous());
   TORCH_CHECK(out.is_contiguous());
 
diff --git a/csrc/quantization/fp8/nvidia/quant_utils.cuh b/csrc/quantization/fp8/nvidia/quant_utils.cuh
index f8cd1dcba..f04db47d4 100644
--- a/csrc/quantization/fp8/nvidia/quant_utils.cuh
+++ b/csrc/quantization/fp8/nvidia/quant_utils.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 #include "../../../attention/attention_dtypes.h"
@@ -563,6 +564,54 @@ __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
           TORCH_CHECK(false,                                                   \
                       "Unsupported input type of kv cache: ", SRC_DTYPE);      \
         }                                                                      \
+      } else if (KV_DTYPE == "int8") {                                         \
+        if (SRC_DTYPE == at::ScalarType::Half) {                               \
+          FN(uint16_t, int8_t, vllm::Fp8KVCacheDataType::kInt8);               \
+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
+          FN(__nv_bfloat16, int8_t, vllm::Fp8KVCacheDataType::kInt8);          \
+        } else {                                                               \
+          TORCH_CHECK(false,                                                   \
+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
+        }                                                                      \    
+      } else {                                                                 \
+        TORCH_CHECK(false, "Unsupported data type of kv cache: ", KV_DTYPE);   \
+      }                                                                        \
+    }
+
+  #define DISPATCH_BY_KV_CACHE_V2_DTYPE(SRC_DTYPE, KV_DTYPE, COUNT_INIT_ONCE, FN)                  \
+    if (KV_DTYPE == "auto") {                                                  \
+      if (SRC_DTYPE == at::ScalarType::Float) {                                \
+        FN(float, float, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);                     \
+      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \
+        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);               \
+      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \
+        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);     \
+      } else {                                                                 \
+        TORCH_CHECK(false, "Unsupported input type of kv cache: ", SRC_DTYPE); \
+      }                                                                        \
+    } else {                                                                   \
+      if (KV_DTYPE == "fp8" || KV_DTYPE == "fp8_e4m3") {                       \
+        if (SRC_DTYPE == at::ScalarType::Float) {                              \
+          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);              \
+        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \
+          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);           \
+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
+          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);      \
+        } else {                                                               \
+          TORCH_CHECK(false,                                                   \
+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
+        }                                                                      \
+      } else if (KV_DTYPE == "fp8_e5m2") {                                     \
+        if (SRC_DTYPE == at::ScalarType::Float) {                              \
+          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);              \
+        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \
+          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);           \
+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
+          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);      \
+        } else {                                                               \
+          TORCH_CHECK(false,                                                   \
+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
+        }                                                                      \
       } else {                                                                 \
         TORCH_CHECK(false, "Unsupported data type of kv cache: ", KV_DTYPE);   \
       }                                                                        \
diff --git a/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu b/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
index 3c4f183bf..b1b6ab8eb 100644
--- a/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
+++ b/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
@@ -95,8 +96,8 @@ void rms_norm_dynamic_per_token_quant_dispatch(
     torch::Tensor const& weight,  // [hidden_size]
     torch::Tensor& scales,        // [num_tokens]
     double const var_epsilon,     // Variance epsilon used in norm calculation
-    std::optional<at::Tensor> const& scale_ub,
-    std::optional<at::Tensor>& residual) {
+    c10::optional<at::Tensor> const& scale_ub,
+    c10::optional<at::Tensor>& residual) {
   int32_t hidden_size = input.size(-1);
   int32_t num_tokens = input.numel() / hidden_size;
 
@@ -143,7 +144,7 @@ void rms_norm_dynamic_per_token_quant(
     torch::Tensor const& weight,  // [hidden_size]
     torch::Tensor& scales,        // [num_tokens]
     double const var_epsilon,     // Variance epsilon used in norm calculation
-    std::optional<at::Tensor> scale_ub, std::optional<at::Tensor> residual) {
+    c10::optional<at::Tensor> scale_ub, c10::optional<at::Tensor> residual) {
   TORCH_CHECK(out.dtype() == kFp8Type || out.dtype() == torch::kInt8);
   TORCH_CHECK(out.is_contiguous() && input.is_contiguous());
 
diff --git a/csrc/quantization/fused_kernels/quant_conversions.cuh b/csrc/quantization/fused_kernels/quant_conversions.cuh
index f8a987222..0a15bee98 100644
--- a/csrc/quantization/fused_kernels/quant_conversions.cuh
+++ b/csrc/quantization/fused_kernels/quant_conversions.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 
 /**
@@ -7,6 +8,7 @@
 #include "quantization/vectorization.cuh"
 // TODO(luka/varun):refactor common.cuh to use this file instead
 #include "quantization/fp8/common.cuh"
+#include "attention/dtype_float16.cuh"
 
 namespace vllm {
 
@@ -25,12 +27,43 @@ static __device__ __forceinline__ int8_t float_to_int8_rn(float const x) {
   return static_cast<int8_t>(dst);
 #else
   // CUDA path
-  uint32_t dst;
-  asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
+  // uint32_t dst;
+  // asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "r"(x));
+  // return reinterpret_cast<const int8_t&>(dst);
+
+  // int32_t dst;
+  // dst = (int32_t)(x > 0? x + 0.5: x - 0.5);
+  // dst = (char)(dst > 127 ? 127 : (dst < -128 ? -128 : dst));
+  // return reinterpret_cast<const int8_t&>(dst);
+  
+  int32_t dst;
+  dst = __float2int_rn(x);
+  dst = min(dst, 127);
+  dst = max(dst, -127);
   return reinterpret_cast<const int8_t&>(dst);
+
+  // return static_cast<int8_t>(fmaxf(-127.0f, fminf(roundf(x), 127.0f)));
 #endif
 }
 
+template <typename Tin>
+__inline__ __device__ int8_t scaled_quant_to_int8(
+    const Tin& x, const float *scale);
+
+// half -> int8
+template <>
+__inline__ __device__ int8_t scaled_quant_to_int8<uint16_t>(
+    const uint16_t& x, const float *scale) {
+  return float_to_int8_rn(half_to_float(x) / *scale);
+}
+
+// bf16 -> int8
+template <>
+__inline__ __device__ int8_t scaled_quant_to_int8<__nv_bfloat16>(
+    const __nv_bfloat16& x, const float *scale) {
+  return float_to_int8_rn(__bfloat162float(x) / *scale);
+}
+
 static __device__ __forceinline__ FP8_TYPE float_to_fp8(float const x) {
   float const r = fmax(-FP8_E4M3_MAX, fmin(x, FP8_E4M3_MAX));
   return static_cast<FP8_TYPE>(r);
diff --git a/csrc/quantization/gguf/ggml-common.h b/csrc/quantization/gguf/ggml-common.h
index d42205a65..4be54baa7 100644
--- a/csrc/quantization/gguf/ggml-common.h
+++ b/csrc/quantization/gguf/ggml-common.h
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 // copied from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-common.h
 #define QK_K 256
 #define K_QUANTS_PER_ITERATION 2
@@ -1075,13 +1076,14 @@ typedef float (*vec_dot_q_mul_mat_cuda_t)(
 
 // Utility function
 
-#if defined(USE_ROCM)
-
-#ifndef __has_builtin
-    #define __has_builtin(x) 0
-#endif
+//#if defined(USE_ROCM)
+#if 1
+//#ifndef __has_builtin
+//    #define __has_builtin(x) 0
+//#endif
 
 typedef int8_t int8x4_t __attribute__((ext_vector_type(4)));
+#if 0
 static __device__ __forceinline__ int __vsubss4(const int a, const int b) {
     const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);
     const int8x4_t vb = reinterpret_cast<const int8x4_t&>(b);
@@ -1101,9 +1103,11 @@ static __device__ __forceinline__ int __vsubss4(const int a, const int b) {
     return reinterpret_cast<int &>(c);
 #endif // __has_builtin(__builtin_elementwise_sub_sat)
 }
+#endif
 
 static __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {
-#if __has_builtin(__builtin_amdgcn_sdot4)
+//#if __has_builtin(__builtin_amdgcn_sdot4)
+#if 0
     c = __builtin_amdgcn_sdot4(a, b, c, false);
 #else
     const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);
@@ -1113,6 +1117,7 @@ static __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {
     return c;
 }
 
+#if 0
 static __device__ __forceinline__ uint32_t __vcmpeq4(const uint32_t a, const uint32_t b) {
     uint32_t neq = a^b;
     return !(neq & 0xff000000) * 0xff000000 |
@@ -1127,4 +1132,5 @@ static __device__ __forceinline__ uint32_t __vsub4(const uint32_t a, const uint3
            (static_cast<uint8_t>(((a & 0x0000ff00) >>  8) - ((b & 0x0000ff00) >>  8)) <<  8) +
            (static_cast<uint8_t>(((a & 0x000000ff) >>  0) - ((b & 0x000000ff) >>  0)) <<  0);
 }
+#endif
 #endif // defined(USE_ROCM)
diff --git a/csrc/quantization/gptq/Hgemm_common.cuh b/csrc/quantization/gptq/Hgemm_common.cuh
new file mode 100644
index 000000000..d49b4be54
--- /dev/null
+++ b/csrc/quantization/gptq/Hgemm_common.cuh
@@ -0,0 +1,91 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+#include "maca_fp16.h"
+
+using b32VecType = uint32_t;
+using b64VecType = __NATIVE_VECTOR__(2, uint32_t);
+using b128VecType = __NATIVE_VECTOR__(4, uint32_t);
+using b128VecType_i = __NATIVE_VECTOR__(4, int32_t);
+using Float4VecType = __NATIVE_VECTOR__(4, float);
+typedef enum
+{
+    OP_N = 0,
+    OP_T = 1
+} Operation_t;
+
+#define cast_half(ptr) reinterpret_cast<__half *>(ptr)
+#define cast_b16(ptr) reinterpret_cast<uint16_t *>(ptr)
+#define cast_b32(ptr) reinterpret_cast<uint32_t *>(ptr)
+#define cast_b64(ptr) reinterpret_cast<b64VecType *>(ptr)
+#define cast_u64(ptr) reinterpret_cast<uint64_t *>(ptr)
+#define cast_b128(ptr) reinterpret_cast<b128VecType *>(ptr)
+#define cast_b128_i(ptr) reinterpret_cast<b128VecType_i *>(ptr)
+
+#define ldg_b32_reg_noasync(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b32_predicator(cast_b32(base), 0, ret0_en, true, false, false, pred, 1, \
+                                           MACA_ICMP_EQ)[0];
+#define ldg_b64_reg_noasync(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b64_predicator(cast_b64(base), 0, ret0_en, true, false, false, pred, 1, \
+                                           MACA_ICMP_EQ);
+#define ldg_b128_reg_noasync(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b128_predicator(cast_b128(base), 0, ret0_en, true, false, false, pred, 1, \
+                                           MACA_ICMP_EQ);
+
+#define ldg_b32_reg_async(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b32_predicator(cast_b32(base), 0, ret0_en, true, false, true, pred, 1, \
+                                           MACA_ICMP_EQ)[0];
+#define ldg_b64_reg_async(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b64_predicator(cast_b64(base), 0, ret0_en, true, false, true, pred, 1, \
+                                           MACA_ICMP_EQ);
+#define ldg_b128_reg_async(dst, base, pred, ret0_en)                                                 \
+    dst = __builtin_mxc_ldg_b128_predicator(cast_b128(base), 0, ret0_en, true, false, true, pred, 1, \
+                                            MACA_ICMP_EQ);
+#define ldg_b64_v4h_reg_async(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b64_predicator(cast_b64(base), 0, ret0_en, true, false, true, pred, 1, \
+                                           MACA_ICMP_EQ);
+
+#define ldg_b32_bsm_noasync(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b32_bsm_predicator(cast_b32(saddr), cast_b32(base), 0, ret0_en, true, false, false, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b64_bsm_noasync(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b64_bsm_predicator(cast_b64(saddr), cast_b64(base), 0, ret0_en, true, false, false, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b128_bsm_noasync(saddr, base, pred, ret0_en)                                                \
+    __builtin_mxc_ldg_b128_bsm_predicator(cast_b128(saddr), cast_b128(base), 0, ret0_en, true, false, \
+                                          false, pred, 1, MACA_ICMP_EQ);
+
+#define ldg_b32_bsm_async(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b32_bsm_predicator(cast_b32(saddr), cast_b32(base), 0, ret0_en, true, false, true, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b64_bsm_async(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b64_bsm_predicator(cast_b64(saddr), cast_b64(base), 0, ret0_en, true, false, true, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b128_bsm_async(saddr, base, pred, ret0_en)                                                \
+    __builtin_mxc_ldg_b128_bsm_predicator(cast_b128(saddr), cast_b128(base), 0, ret0_en, true, false, \
+                                          true, pred, 1, MACA_ICMP_EQ);
+
+#define stg_b32_async(data, base, pred)                                                   \
+    __builtin_mxc_stg_b32_predicator(cast_b32(base), 0, data, true, false, true, pred, 1, \
+                                     MACA_ICMP_EQ);
+#define stg_b64_async(data, base, pred)                                                   \
+    __builtin_mxc_stg_b64_predicator(cast_b64(base), 0, data, true, false, true, pred, 1, \
+                                     MACA_ICMP_EQ);
+#define stg_b128_async(data, base, pred)                                                    \
+    __builtin_mxc_stg_b128_predicator(cast_b128(base), 0, data, true, false, true, pred, 1, \
+                                      MACA_ICMP_EQ);
+
+#define perm_b32(dst, reg1, reg2, selector) dst = __builtin_mxc_byte_perm(reg1, reg2, selector)
+#define mma_16x16x16f16(a_reg, b_reg, c_reg) \
+    c_reg = __builtin_mxc_mma_16x16x16f16(a_reg, b_reg, c_reg)
+
+#define mma_16x16x16bf16(a_reg, b_reg, c_reg) \
+    c_reg = __builtin_mxc_mma_16x16x16bf16(a_reg, b_reg, c_reg)
+
+#define FENCE__ asm volatile(";")
+#define arrive_gvmcnt(num) __builtin_mxc_arrive(64 + num)
+#define arrive_bsmcnt(num) __builtin_mxc_arrive(4096 + 128 * num)
+#define arrive_gvm_bsmcnt(gvm, bsm) __builtin_mxc_arrive(4096 | (128 * bsm) | 64 | gvm)
+#define barrier __builtin_mxc_barrier_inst
+#define barrier_all __builtin_mxc_barrier_ex(0)
+#define barrier_bsm __builtin_mxc_barrier_ex(1)
+#define barrier_inst __builtin_mxc_barrier_ex(2)
diff --git a/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
new file mode 100644
index 000000000..9a866c81a
--- /dev/null
+++ b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
@@ -0,0 +1,427 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+#include "Hgemm_common.cuh"
+#include "gptq.cuh"
+#include "maca_fp16.h"
+
+template <Operation_t TransA,
+          Operation_t TransB,
+          bool IsBetaZero,
+          int tileM,
+          int tileN,
+          int tileK,
+          bool doShuffle = true,
+          bool splitk = false,
+          bool Swap = false>
+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_gptq_4bit(int m,
+                                                                            int n,
+                                                                            int k,
+                                                                            const scalar_type alpha,    // alpha is 1.0f for gptq
+                                                                            const scalar_type beta,     // beta is 0.0f for gptq
+                                                                            const quant_packed_type *dA_input,
+                                                                            int lda,
+                                                                            const input_type *dB_input,
+                                                                            int ldb,
+                                                                            output_type *dC_input,
+                                                                            output_type *dC_output,
+                                                                            int ldc,
+                                                                            quant_packed_type *d_zeros,
+                                                                            input_type *d_scales,
+                                                                            int splitk_iters = 1,
+                                                                            acc_type * d_acc_tmp=nullptr)
+{
+    __shared__ uint8_t smem_base[0x8000];
+    int bidx = Swap ? blockIdx.y : blockIdx.x;
+    int bidy = Swap ? blockIdx.x : blockIdx.y;
+    int bidz = blockIdx.z;
+    int tid = threadIdx.x;
+
+    // this kernel only support NN trans mode
+    uint64_t arowstride = 1;
+    uint64_t acolstride = lda;
+    uint64_t browstride = 1;
+    uint64_t bcolstride = ldb;
+
+    const int malign = 8;
+    const int nalign = 1;
+    const int kalign = 8;
+    int align_m = (m + malign - 1) / malign * malign;
+    int align_n = (n + nalign - 1) / nalign * nalign;
+    int align_k = (k + kalign - 1) / kalign * kalign;
+
+    int k_num = (align_k + splitk_iters -1) / splitk_iters;
+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
+    int k_begin = bidz * k_num;
+    int k_end = (bidz + 1) * k_num;
+    k_end = k_end > align_k ? align_k : k_end;
+    k_num = k_end - k_begin;
+    int ktail = k_num % tileK;
+    int kloop = k_begin;
+
+    // if k_begin > align_k(we force k_num to be aligned to 8, so it is possible), return immediately
+    if (k_begin >= align_k) 
+    {
+        return;
+    }
+
+    int slot = tid / 64;
+    int lane = tid & 63;
+    int m64d16 = lane / 16;
+    int m64m16 = lane % 16;
+    output_type *c_base_i = dC_input;
+    output_type *c_base_o = dC_output;
+
+
+    quant_packed_type rga[8], rgzeros[1];
+    b128VecType rgb[2];
+    input_type rgscales[2];
+
+    b64VecType rgA[16], rgB[4], rgCi[4];
+    Float4VecType rgC[16];
+    uint32_t rgZeros[1];
+    b64VecType rgScales[2];
+
+    quant_packed_type *dA[2];
+    input_type *dB[2];
+
+    // ldg A/B head
+    bool predm[2], predn[2];
+    int rowA = m64m16 * 4;
+    int colA = (m64d16 * 8 + slot * 32) / PACK_RATIO_4BITS;
+    int current_m = bidx * tileM + rowA;
+    predm[0] = current_m < align_m;
+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+            (uint64_t)(colA + k_begin / PACK_RATIO_4BITS) * (uint64_t)(acolstride);
+    current_m += 64;
+    predm[1] = current_m < align_m;
+    // dA[1] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+    //         (uint64_t)(colA + k_begin / PACK_RATIO_4BITS) * (uint64_t)(acolstride);
+    dA[1] = dA[0] + 64 * (uint64_t)(arowstride);
+
+    int colB = m64d16 + slot * 4;
+    int rowB = m64m16 * 8;
+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
+    int current_n = bidy * tileN + colB;
+    predn[0] = current_n < align_n;
+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    current_n += 16;
+    predn[1] = current_n < align_n;
+    // dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+    //         (uint64_t)(current_n) * (uint64_t)bcolstride;
+    dB[1] = dB[0] + 16 * (uint64_t)bcolstride;
+
+    // ldgB to BSM need swizzle
+    input_type *b_sts = (input_type *)smem_base;
+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
+
+    input_type *b_lds = (input_type *)smem_base;
+    int colB_lds = m64m16;
+    int rowB_lds = m64d16 * 8 + slot * 32;
+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
+
+    // ldg zeros and scales
+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO_4BITS) * (kloop / tileK) + bidx * (tileM / PACK_RATIO_4BITS) + tid;
+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_4BITS * sizeof(quant_packed_type)) + tid * 2;
+    uint16_t *lds_zeros_offset = (uint16_t *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_4BITS * sizeof(quant_packed_type)) + m64m16 * 4;
+
+    // clear C registers
+    rgC[0] = 0;
+    rgC[1] = 0;
+    rgC[2] = 0;
+    rgC[3] = 0;
+    rgC[4] = 0;
+    rgC[5] = 0;
+    rgC[6] = 0;
+    rgC[7] = 0;
+    rgC[8] = 0;
+    rgC[9] = 0;
+    rgC[10] = 0;
+    rgC[11] = 0;
+    rgC[12] = 0;
+    rgC[13] = 0;
+    rgC[14] = 0;
+    rgC[15] = 0;
+
+    int aincr = tileK * acolstride / PACK_RATIO_4BITS;
+    int bincr = tileK * browstride;
+
+    if (k_num > ktail)
+    {
+        LDG_B;      // 2
+        LDG_ZEROS_4BITS;  // 3
+        LDG_SCALES_4BITS; // 4
+        LDG_A1_4BITS;     // 5
+        LDG_A2_4BITS;     // 6
+
+        dA[0] += aincr;
+        dA[1] += aincr;
+        dB[0] += bincr;
+        dB[1] += bincr;
+
+        // lds_A, need swizzle
+        arrive_gvmcnt(2);
+        barrier();
+
+        LDS_B;
+        LDS_ZEROS_4BITS;
+        LDS_SCALES_4BITS;
+        arrive_bsmcnt(0);
+        barrier();
+        ldg_zeros_offset += lda / PACK_RATIO_4BITS;
+        ldg_scales_offset += lda;
+
+// KLOOP
+#pragma unroll 1
+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
+        {
+            // load next B
+            LDG_B;      // 4
+            LDG_ZEROS_4BITS;  // 5
+            LDG_SCALES_4BITS; // 6
+
+            arrive_gvmcnt(5);
+            PERM_A1_4BITS;
+            LDG_A1_4BITS;
+            MMA1;
+
+            arrive_gvmcnt(5);
+            PERM_A2_4BITS;
+            LDG_A2_4BITS; // 6
+            MMA2;
+
+            // sts && lds
+            arrive_gvmcnt(2);
+            barrier();
+            LDS_B;
+            LDS_ZEROS_4BITS;
+            LDS_SCALES_4BITS;
+            arrive_bsmcnt(0);
+            barrier();
+
+            dA[0] += aincr;
+            dA[1] += aincr;
+            dB[0] += bincr;
+            dB[1] += bincr;
+            ldg_zeros_offset += lda / PACK_RATIO_4BITS;
+            ldg_scales_offset += lda;
+        }
+
+        arrive_gvmcnt(1);
+        PERM_A1_4BITS;
+        MMA1;
+
+        arrive_gvmcnt(0);
+        PERM_A2_4BITS;
+        MMA2;
+    }
+
+    // final tail kloop
+    if (ktail > 0)
+    {
+        LDG_B_HEAD;
+        LDG_ZEROS_4BITS;
+        LDG_SCALES_4BITS;
+        LDG_A1_HEAD_4BITS;
+        LDG_A2_HEAD_4BITS;
+        arrive_gvmcnt(2);
+        barrier();
+        LDS_B;
+        LDS_ZEROS_4BITS;
+        LDS_SCALES_4BITS;
+        arrive_bsmcnt(0);
+        barrier();
+
+        arrive_gvmcnt(1);
+        PERM_A1_4BITS;
+        MMA1;
+        arrive_gvmcnt(0);
+        PERM_A2_4BITS;
+        MMA2;
+    }
+
+    // store C registers into BSM & do reduction
+    int colC = m64m16 + slot * 16;
+    int rowC = m64d16 * 16;
+    scalar_type *c_sts[8];
+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 68, (colC % 4 * 4)), colC, 128);
+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 72, (colC % 4 * 4)), colC, 128);
+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 76, (colC % 4 * 4)), colC, 128);
+
+    colC = m64d16 + slot * 4;
+    rowC = m64m16 * 4;
+    scalar_type *c_lds[2];
+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+
+    PERM_C(rgC);
+    STS_C(0);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(0);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    PERM_C((rgC + 8));
+    STS_C(1);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(1);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    // read C_input if beta!=0
+    colC = m64d16 + slot * 4 + bidy * tileN;
+    rowC = m64m16 * 4 + bidx * tileM;
+    int colC1 = colC + 16;
+    int rowC1 = rowC + 64;
+
+
+    if constexpr (!IsBetaZero && !splitk)
+    {
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[0],
+                              c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC < align_m && colC1 < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC1 < align_n, true);
+        arrive_gvmcnt(0);
+    }
+    
+    if constexpr (IsBetaZero && !splitk)
+    {
+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
+    }
+
+    if constexpr (!splitk)
+    {
+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
+        output_type result[4];
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[0][0]);
+            result[1] = static_cast<output_type>(rgC[0][1]);
+            result[2] = static_cast<output_type>(rgC[0][2]);
+            result[3] = static_cast<output_type>(rgC[0][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[1][0]);
+            result[1] = static_cast<output_type>(rgC[1][1]);
+            result[2] = static_cast<output_type>(rgC[1][2]);
+            result[3] = static_cast<output_type>(rgC[1][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[2][0]);
+            result[1] = static_cast<output_type>(rgC[2][1]);
+            result[2] = static_cast<output_type>(rgC[2][2]);
+            result[3] = static_cast<output_type>(rgC[2][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[3][0]);
+            result[1] = static_cast<output_type>(rgC[3][1]);
+            result[2] = static_cast<output_type>(rgC[3][2]);
+            result[3] = static_cast<output_type>(rgC[3][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+    }
+    else
+    {
+        // for acc_type is float
+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[0][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[0][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[0][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[0][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[1][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[1][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[1][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[1][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[2][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[2][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[2][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[2][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[3][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[3][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[3][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[3][3]));
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
new file mode 100644
index 000000000..ac25b323f
--- /dev/null
+++ b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
@@ -0,0 +1,441 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+#include "Hgemm_common.cuh"
+#include "gptq.cuh"
+#include "maca_fp16.h"
+
+template <Operation_t TransA,
+          Operation_t TransB,
+          bool IsBetaZero,
+          int tileM,
+          int tileN,
+          int tileK,
+          bool doShuffle = true,
+          bool splitk = false,
+          bool Swap = false>
+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_gptq_8bit(int m,
+                                                                            int n,
+                                                                            int k,
+                                                                            const scalar_type alpha,    // alpha is 1.0f for gptq
+                                                                            const scalar_type beta,     // beta is 0.0f for gptq
+                                                                            const quant_packed_type *dA_input,
+                                                                            int lda,
+                                                                            const input_type *dB_input,
+                                                                            int ldb,
+                                                                            output_type *dC_input,
+                                                                            output_type *dC_output,
+                                                                            int ldc,
+                                                                            quant_packed_type *d_zeros,
+                                                                            input_type *d_scales,
+                                                                            int splitk_iters = 1,
+                                                                            acc_type * d_acc_tmp=nullptr,
+                                                                            half* dequanted = nullptr)
+{
+    __shared__ uint8_t smem_base[0x8000];
+    int bidx = Swap ? blockIdx.y : blockIdx.x;
+    int bidy = Swap ? blockIdx.x : blockIdx.y;
+    int bidz = blockIdx.z;
+    int tid = threadIdx.x;
+
+    // this kernel only support NN trans mode
+    uint64_t arowstride = 1;
+    uint64_t acolstride = lda;
+    uint64_t browstride = 1;
+    uint64_t bcolstride = ldb;
+
+    const int malign = 8;
+    const int nalign = 1;
+    const int kalign = 8;
+    int align_m = (m + malign - 1) / malign * malign;
+    int align_n = (n + nalign - 1) / nalign * nalign;
+    int align_k = (k + kalign - 1) / kalign * kalign;
+
+    int k_num = (align_k + splitk_iters -1) / splitk_iters;
+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
+    int k_begin = bidz * k_num;
+    int k_end = (bidz + 1) * k_num;
+    k_end = k_end > align_k ? align_k : k_end;
+    k_num = k_end - k_begin;
+    int ktail = k_num % tileK;
+    int kloop = k_begin;
+
+    // if k_begin > align_k(we force k_num to be aligned to 8, so it is possible), return immediately
+    if (k_begin >= align_k) 
+    {
+        return;
+    }
+
+    int slot = tid / 64;
+    int lane = tid & 63;
+    int m64d16 = lane / 16;
+    int m64m16 = lane % 16;
+    output_type *c_base_i = dC_input;
+    output_type *c_base_o = dC_output;
+
+
+    quant_packed_type rga[16], rgzeros[2];
+    b128VecType rgb[2];
+    input_type rgscales[2];
+
+    b64VecType rgA[16], rgB[4], rgCi[4];
+    Float4VecType rgC[16];
+    uint32_t rgZeros[2];
+    b64VecType rgScales[2];
+
+    quant_packed_type *dA[4];
+    input_type *dB[2];
+
+    // ldg A/B head
+    bool predm[2], predn[2];
+    int rowA = m64m16 * 4;
+    int colA = (m64d16 * 8 + slot * 32) / PACK_RATIO_8BITS;
+    int current_m = bidx * tileM + rowA;
+    predm[0] = current_m < align_m;
+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+            (uint64_t)(colA + k_begin / PACK_RATIO_8BITS) * (uint64_t)(acolstride);
+    dA[2] = dA[0] + align_m;
+    current_m += 64;
+    predm[1] = current_m < align_m;
+    // dA[1] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+    //         (uint64_t)(colA + k_begin / PACK_RATIO_8BITS) * (uint64_t)(acolstride);
+    dA[1] = dA[0] + 64 * (uint64_t)(arowstride);
+    dA[3] = dA[1] + align_m;
+
+    int colB = m64d16 + slot * 4;
+    int rowB = m64m16 * 8;
+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
+    int current_n = bidy * tileN + colB;
+    predn[0] = current_n < align_n;
+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    current_n += 16;
+    predn[1] = current_n < align_n;
+    // dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+    //         (uint64_t)(current_n) * (uint64_t)bcolstride;
+    dB[1] = dB[0] + 16 * (uint64_t)bcolstride;
+
+    // ldgB to BSM need swizzle
+    input_type *b_sts = (input_type *)smem_base;
+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
+
+    input_type *b_lds = (input_type *)smem_base;
+    int colB_lds = m64m16;
+    int rowB_lds = m64d16 * 8 + slot * 32;
+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
+
+    // ldg zeros and scales
+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO_8BITS) * (kloop / tileK) + bidx * (tileM / PACK_RATIO_8BITS) + tid;
+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_8BITS * sizeof(quant_packed_type)) + tid * 2;
+    quant_packed_type *lds_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_8BITS * sizeof(quant_packed_type)) + m64m16 * 4;
+
+    // clear C registers
+    rgC[0] = 0;
+    rgC[1] = 0;
+    rgC[2] = 0;
+    rgC[3] = 0;
+    rgC[4] = 0;
+    rgC[5] = 0;
+    rgC[6] = 0;
+    rgC[7] = 0;
+    rgC[8] = 0;
+    rgC[9] = 0;
+    rgC[10] = 0;
+    rgC[11] = 0;
+    rgC[12] = 0;
+    rgC[13] = 0;
+    rgC[14] = 0;
+    rgC[15] = 0;
+
+    int aincr = tileK * acolstride / PACK_RATIO_8BITS;
+    int bincr = tileK * browstride;
+
+    if (k_num > ktail)
+    {
+        LDG_B;      // 2
+        LDG_ZEROS_8BITS;  // 3
+        LDG_SCALES_8BITS; // 4
+        LDG_A1_8BITS;     // 5
+        LDG_A3_8BITS;     // 6
+        LDG_A2_8BITS;     // 7
+        LDG_A4_8BITS;     // 8
+
+        dA[0] += aincr;
+        dA[1] += aincr;
+        dA[2] += aincr;
+        dA[3] += aincr;
+        dB[0] += bincr;
+        dB[1] += bincr;
+
+        // lds_A, need swizzle
+        arrive_gvmcnt(4);
+        barrier();
+
+        LDS_B;
+        LDS_ZEROS_8BITS;
+        LDS_SCALES_8BITS;
+        arrive_bsmcnt(0);
+        barrier();
+        ldg_zeros_offset += lda / PACK_RATIO_8BITS;
+        ldg_scales_offset += lda;
+        constexpr bool loading_filter = false;//blockIdx.x == 0 && threadIdx.x == 0;
+
+// KLOOP
+#pragma unroll 1
+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
+        {
+            // load next B
+            LDG_B;      // 6
+            LDG_ZEROS_8BITS;  // 7
+            LDG_SCALES_8BITS; // 8
+
+            arrive_gvmcnt(6);
+            PERM_A1A3_8BITS;
+            LDG_A1_8BITS;
+            LDG_A3_8BITS;
+            MMA1;
+
+            arrive_gvmcnt(6);
+            PERM_A2A4_8BITS;
+            LDG_A2_8BITS; // 6
+            LDG_A4_8BITS;
+            MMA2;
+
+            // sts && lds
+            arrive_gvmcnt(4);
+            barrier();
+            LDS_B;
+            LDS_ZEROS_8BITS;
+            LDS_SCALES_8BITS;
+            arrive_bsmcnt(0);
+            barrier();
+
+            dA[0] += aincr;
+            dA[1] += aincr;
+            dA[2] += aincr;
+            dA[3] += aincr;
+            dB[0] += bincr;
+            dB[1] += bincr;
+            ldg_zeros_offset += lda / PACK_RATIO_8BITS;
+            ldg_scales_offset += lda;
+        }
+
+        arrive_gvmcnt(2);
+        PERM_A1A3_8BITS;
+        MMA1;
+
+        arrive_gvmcnt(0);
+        PERM_A2A4_8BITS;
+        MMA2;
+    }
+
+    // final tail kloop
+    if (ktail > 0)
+    {
+        LDG_B_HEAD;
+        LDG_ZEROS_8BITS;
+        LDG_SCALES_8BITS;
+        LDG_A1_HEAD_8BITS;
+        LDG_A3_HEAD_8BITS;
+        LDG_A2_HEAD_8BITS;
+        LDG_A4_HEAD_8BITS;
+        arrive_gvmcnt(4);
+        barrier();
+        LDS_B;
+        LDS_ZEROS_8BITS;
+        LDS_SCALES_8BITS;
+        arrive_bsmcnt(0);
+        barrier();
+
+        arrive_gvmcnt(2);
+        PERM_A1A3_8BITS;
+        MMA1;
+        arrive_gvmcnt(0);
+        PERM_A2A4_8BITS;
+        MMA2;
+    }
+
+    // store C registers into BSM & do reduction
+    int colC = m64m16 + slot * 16;
+    int rowC = m64d16 * 16;
+    scalar_type *c_sts[8];
+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 68, (colC % 4 * 4)), colC, 128);
+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 72, (colC % 4 * 4)), colC, 128);
+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 76, (colC % 4 * 4)), colC, 128);
+
+    colC = m64d16 + slot * 4;
+    rowC = m64m16 * 4;
+    scalar_type *c_lds[2];
+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+
+    PERM_C(rgC);
+    STS_C(0);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(0);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    PERM_C((rgC + 8));
+    STS_C(1);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(1);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    // read C_input if beta!=0
+    colC = m64d16 + slot * 4 + bidy * tileN;
+    rowC = m64m16 * 4 + bidx * tileM;
+    int colC1 = colC + 16;
+    int rowC1 = rowC + 64;
+
+
+    if constexpr (!IsBetaZero && !splitk)
+    {
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[0],
+                              c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC < align_m && colC1 < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC1 < align_n, true);
+        arrive_gvmcnt(0);
+    }
+    
+    if constexpr (IsBetaZero && !splitk)
+    {
+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
+    }
+
+    if constexpr (!splitk)
+    {
+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
+        output_type result[4];
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[0][0]);
+            result[1] = static_cast<output_type>(rgC[0][1]);
+            result[2] = static_cast<output_type>(rgC[0][2]);
+            result[3] = static_cast<output_type>(rgC[0][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[1][0]);
+            result[1] = static_cast<output_type>(rgC[1][1]);
+            result[2] = static_cast<output_type>(rgC[1][2]);
+            result[3] = static_cast<output_type>(rgC[1][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[2][0]);
+            result[1] = static_cast<output_type>(rgC[2][1]);
+            result[2] = static_cast<output_type>(rgC[2][2]);
+            result[3] = static_cast<output_type>(rgC[2][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[3][0]);
+            result[1] = static_cast<output_type>(rgC[3][1]);
+            result[2] = static_cast<output_type>(rgC[3][2]);
+            result[3] = static_cast<output_type>(rgC[3][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+    }
+    else
+    {
+        // for acc_type is float
+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[0][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[0][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[0][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[0][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[1][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[1][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[1][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[1][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[2][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[2][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[2][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[2][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[3][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[3][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[3][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[3][3]));
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/dequant.cuh b/csrc/quantization/gptq/dequant.cuh
new file mode 100644
index 000000000..2cfe33bab
--- /dev/null
+++ b/csrc/quantization/gptq/dequant.cuh
@@ -0,0 +1,12 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#define BFE(source, bit_start, num_bits) __builtin_mxc_ubfe(source, bit_start, num_bits);
+
+template <typename outputT, typename inputT, int qbits>
+__device__ __forceinline__ outputT extract(inputT *input, size_t loc)
+{
+    constexpr int widthPerElem = sizeof(inputT) * 8 / qbits;
+    return (outputT)BFE(input[loc / widthPerElem], loc % widthPerElem, qbits);
+}
\ No newline at end of file
diff --git a/csrc/quantization/gptq/gptq.cuh b/csrc/quantization/gptq/gptq.cuh
new file mode 100644
index 000000000..b676c5776
--- /dev/null
+++ b/csrc/quantization/gptq/gptq.cuh
@@ -0,0 +1,366 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#include "Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+
+#define Q4BITS 4
+#define PACK_RATIO_4BITS (32 / Q4BITS)
+
+#define Q8BITS 8
+#define PACK_RATIO_8BITS (32 / Q8BITS)
+
+#define input_type __half
+#define output_type __half
+#define scalar_type float
+//#define acc_type float
+#define acc_type __half
+
+#define SEL0 0x01000504
+#define SEL1 0x03020706
+
+#define SWIZZLE_INDEX(index, phase) (index ^ phase)
+#define ONE_DIM_INDEX(x, y, lda) (y * lda + x)
+#define CLAMP(value, bound, align) (value < bound ? value : bound - align)
+
+#define LDG_A1                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[0], dA[0], predm[0], true); \
+    }
+
+#define LDG_A2                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[1], dA[1], predm[1], true); \
+    }
+
+#define LDG_A3                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[2], dA[2], predm[0], true); \
+    }
+
+#define LDG_A4                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[3], dA[3], predm[1], true); \
+    }
+
+#define LDG_A1_4BITS LDG_A1
+#define LDG_A2_4BITS LDG_A2
+#define LDG_A1_8BITS LDG_A1
+#define LDG_A2_8BITS LDG_A2
+#define LDG_A3_8BITS LDG_A3
+#define LDG_A4_8BITS LDG_A4
+
+#define LDG_A1_HEAD_4BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_4BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[0], dA[0], predm[0] && predk, true); \
+    }
+
+#define LDG_A2_HEAD_4BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_4BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[1], dA[1], predm[1] && predk, true); \
+    }
+
+#define LDG_A1_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[0], dA[0], predm[0] && predk, true); \
+    }
+
+#define LDG_A2_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[1], dA[1], predm[1] && predk, true); \
+    }
+
+#define LDG_A3_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[2], dA[2], predm[0] && predk, true); \
+    }
+
+#define LDG_A4_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[3], dA[3], predm[1] && predk, true); \
+    }
+
+#define LDG_ZEROS_4BITS                                                                                                                        \
+    {                                                                                                                                          \
+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO_4BITS && (bidx * tileM + tid * PACK_RATIO_4BITS < align_m), false); \
+    }
+
+#define LDG_ZEROS_8BITS                                                                                                                        \
+    {                                                                                                                                          \
+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO_8BITS && (bidx * tileM + tid * PACK_RATIO_8BITS < align_m), false); \
+    }
+
+#define LDG_SCALES                                                                                                      \
+    {                                                                                                                   \
+        ldg_b32_bsm_async(sts_scales_offset, ldg_scales_offset, tid < 64 && (bidx * tileM + tid * 2 < align_m), false); \
+    }
+
+#define LDG_SCALES_4BITS LDG_SCALES
+#define LDG_SCALES_8BITS LDG_SCALES
+
+#define LDS_ZEROS_4BITS                              \
+    {                                                \
+        cast_b16(rgZeros)[0] = lds_zeros_offset[0];  \
+        cast_b16(rgZeros)[1] = lds_zeros_offset[16]; \
+    }
+
+#define LDS_ZEROS_8BITS                              \
+    {                                                \
+        cast_b32(rgZeros)[0] = lds_zeros_offset[0];  \
+        cast_b32(rgZeros)[1] = lds_zeros_offset[16]; \
+    }
+
+#define LDS_SCALES                                               \
+    {                                                            \
+        cast_b64(rgScales)[0] = cast_b64(lds_scales_offset)[0];  \
+        cast_b64(rgScales)[1] = cast_b64(lds_scales_offset)[16]; \
+    }
+
+#define LDS_SCALES_4BITS LDS_SCALES
+#define LDS_SCALES_8BITS LDS_SCALES
+
+#define PERM_ELEM_4BITS(index)                                                                                                          \
+    {                                                                                                                                   \
+        __half_raw elem;                                                                                                                \
+        if constexpr (index & 0x1)                                                                                                      \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] >> 16;                                                                          \
+        }                                                                                                                               \
+        else                                                                                                                            \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] & 0xffff;                                                                       \
+        }                                                                                                                               \
+        __half scale = __half(elem);                                                                                                    \
+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[0], Q4BITS * index, Q4BITS) + 1;                                                     \
+        if constexpr (doShuffle)                                                                                                        \
+        {                                                                                                                               \
+            cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 0, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 4, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 1, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 5, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 2, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 6, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 3, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 7, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+        }                                                                                                                               \
+        else                                                                                                                            \
+        {                                                                                                                               \
+            cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 0, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 1, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 2, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 3, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 4, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 5, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 6, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 7, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+        }                                                                                                                               \
+    }
+
+#define PERM_A1_4BITS       \
+    {                       \
+        PERM_ELEM_4BITS(0)  \
+        PERM_ELEM_4BITS(1)  \
+        PERM_ELEM_4BITS(2)  \
+        PERM_ELEM_4BITS(3)  \
+    }
+
+#define PERM_A2_4BITS       \
+    {                       \
+        PERM_ELEM_4BITS(4)  \
+        PERM_ELEM_4BITS(5)  \
+        PERM_ELEM_4BITS(6)  \
+        PERM_ELEM_4BITS(7)  \
+    }
+
+#define PERM_ELEM_8BITS(index)                                                                                                          \
+    {                                                                                                                                   \
+        __half_raw elem;                                                                                                                \
+        if constexpr (index & 0x1)                                                                                                      \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] >> 16;                                                                          \
+        }                                                                                                                               \
+        else                                                                                                                            \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] & 0xffff;                                                                       \
+        }                                                                                                                               \
+        __half scale = __half(elem);                                                                                                    \
+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[index/4], Q8BITS * index, Q8BITS) + 1;                                               \
+                                                                                                                                        \
+        cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 0, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 1, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 2, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 3, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 0, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 1, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 2, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 3, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+    }
+
+#define PERM_A1A3_8BITS     \
+    {                       \
+        PERM_ELEM_8BITS(0)  \
+        PERM_ELEM_8BITS(1)  \
+        PERM_ELEM_8BITS(2)  \
+        PERM_ELEM_8BITS(3)  \
+    }
+
+#define PERM_A2A4_8BITS     \
+    {                       \
+        PERM_ELEM_8BITS(4)  \
+        PERM_ELEM_8BITS(5)  \
+        PERM_ELEM_8BITS(6)  \
+        PERM_ELEM_8BITS(7)  \
+    }
+
+#define MMA_ELEM(index_m)                                        \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);
+
+#define MMA1    \
+    MMA_ELEM(0) \
+    MMA_ELEM(1) \
+    MMA_ELEM(2) \
+    MMA_ELEM(3)
+
+#define MMA2    \
+    MMA_ELEM(4) \
+    MMA_ELEM(5) \
+    MMA_ELEM(6) \
+    MMA_ELEM(7)
+
+#define LDG_B                                                        \
+    {                                                                \
+        ldg_b128_bsm_async(b_sts, dB[0], predn[0], true);            \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], predn[1], true); \
+    }
+
+#define LDG_B_HEAD                                                            \
+    {                                                                         \
+        bool predk = rowB_swizzle < ktail;                                    \
+        ldg_b128_bsm_async(b_sts, dB[0], predn[0] && predk, true);            \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], predn[1] && predk, true); \
+    }
+
+#define LDS_B                                          \
+    {                                                  \
+        cast_b128(rgB)[0] = cast_b128(b_lds)[0];       \
+        cast_b128(rgB)[1] = cast_b128(b_lds)[16 * 16]; \
+    }
+
+#define PERM_C(C2perm)                                       \
+    {                                                        \
+        Float4VecType C_tmp[8];                              \
+        float *ptri = (float *)C2perm;                       \
+        float *ptro = (float *)C_tmp;                        \
+        for (int j = 0; j < 4; ++j)                          \
+        {                                                    \
+            for (int i = 0; i < 4; ++i)                      \
+            {                                                \
+                ptro[j * 4 + i] = ptri[j + i * 4];           \
+                ptro[j * 4 + i + 16] = ptri[j + i * 4 + 16]; \
+            }                                                \
+        }                                                    \
+        for (int i = 0; i < 8; ++i)                          \
+        {                                                    \
+            C2perm[i] = C_tmp[i];                            \
+        }                                                    \
+    }
+
+#define STS_C(phase)                                            \
+    {                                                           \
+        cast_b128(c_sts[0])[0] = cast_b128(rgC)[0 + phase * 8]; \
+        cast_b128(c_sts[1])[0] = cast_b128(rgC)[1 + phase * 8]; \
+        cast_b128(c_sts[2])[0] = cast_b128(rgC)[2 + phase * 8]; \
+        cast_b128(c_sts[3])[0] = cast_b128(rgC)[3 + phase * 8]; \
+        cast_b128(c_sts[4])[0] = cast_b128(rgC)[4 + phase * 8]; \
+        cast_b128(c_sts[5])[0] = cast_b128(rgC)[5 + phase * 8]; \
+        cast_b128(c_sts[6])[0] = cast_b128(rgC)[6 + phase * 8]; \
+        cast_b128(c_sts[7])[0] = cast_b128(rgC)[7 + phase * 8]; \
+    }
+
+#define REDUCE_C(phase)                                                   \
+    {                                                                     \
+        cast_b128(rgC)[0 + phase * 8] = cast_b128(c_lds[0])[0];           \
+        cast_b128(rgC)[1 + phase * 8] = cast_b128(c_lds[1])[0];           \
+        cast_b128(rgC)[2 + phase * 8] = cast_b128(c_lds[0])[1 * 16 * 32]; \
+        cast_b128(rgC)[3 + phase * 8] = cast_b128(c_lds[1])[1 * 16 * 32]; \
+        cast_b128(rgC)[4 + phase * 8] = cast_b128(c_lds[0])[2 * 16 * 32]; \
+        cast_b128(rgC)[5 + phase * 8] = cast_b128(c_lds[1])[2 * 16 * 32]; \
+        cast_b128(rgC)[6 + phase * 8] = cast_b128(c_lds[0])[3 * 16 * 32]; \
+        cast_b128(rgC)[7 + phase * 8] = cast_b128(c_lds[1])[3 * 16 * 32]; \
+        float *reduc_c = reinterpret_cast<float *>(rgC);                  \
+        for (int loop = 0; loop < 8; ++loop)                              \
+        {                                                                 \
+            float acc = 0;                                                \
+            acc += reduc_c[loop + phase * 32 + 0 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 1 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 2 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 3 * 8];                    \
+            reduc_c[loop + phase * 8] = acc;                              \
+        }                                                                 \
+    }
diff --git a/csrc/quantization/gptq/hgemm_gptq.h b/csrc/quantization/gptq/hgemm_gptq.h
new file mode 100644
index 000000000..c646e6198
--- /dev/null
+++ b/csrc/quantization/gptq/hgemm_gptq.h
@@ -0,0 +1,1936 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+/*
+hgemm gptq 4bits
+
+hgemm inputs:
+half/tf16 a[m*k];
+uint32_t b[k*n/pack_ratio]; //pack_ratio = sizeof(uint32_t) / 4;
+uint32_t zp[k*n] with kU4 or zp[0] with kU4B8;
+half scales[m*k/quant_group];
+
+MMA accepts a m16xk16 b k16xn16
+A tile should be m16n64k128 or m32n64k128
+When m>=32, we should load a when compute, implement this stragedge later?
+
+We will make a block size 256, which means 4 waves, we need all these 4 waves execute on one PEU
+So the maximum shared memory used by a block use not larger than 16K, and MTRegisters should be
+less than 128.
+
+A tile will be divied into serveral sub_tiles, a minimum sub_tile is m16n16k32, so we can have
+m_blocks*k_blocks when loading a and k_blocks*n_blocks when loading b.
+
+We will have n*k/TILE_N/TILE_K tiles in b, and these TILES are diveded into ITERS where ITERS*PEUS>=TILES
+for a proper shape.
+Of course some small shapes will nerver have chance to use all the PEUS
+
+Parallism is not suitable for C500 as parallism will dequant b more than once, that is not acceptable
+*/
+#include <iostream>
+#include <algorithm>
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include <maca_bfloat16.h>
+
+#include "Hgemm_common.cuh"
+#include "scalar_type.hpp"
+
+using cudaStream_t = mcStream_t;
+
+#define WAVES_PER_BLOCK (THREADS/WAVE)
+#define TILE_M (BLOCKS_M*SLICE_M)
+#define TILE_N (BLOCKS_N*SLICE_N)
+#define TILE_K (BLOCKS_K*SLICE_K)
+#define N_ITERS (TILE_N / (WAVES_PER_BLOCK*SLOT))
+#define LOADING_A_LOOP SLICE_K * TILE_M / (sizeof(PackType) / sizeof(scalar_t)) / THREADS
+#define AS_PTR_B128(x) ((PackTypeInt4*)x)
+#define AS_PTR_B64(x) ((PackTypeInt2*)x)
+#define AS_PTR_B32(x) ((float*)x)
+#define AS_PTR_B16(x) ((half*)x)
+#define AS_PTR_B8(x) ((uint8_t*)x)
+
+#define BF16_HIGH_PRECISION
+
+#define div_ceil(x, y) (x + y - 1) / (y)
+
+//Although quant_group can be any positive value, but it is not a good idea
+//to set quant_group to values that cannot fit the SLICE_K as we will get a
+//very low performance, and we are not ready to support these values
+//Here we annouce that we support quant_group = 32, 64, 128, but actually
+//quant_group = 2^n where n >= 5 is also supported, for very large k.
+static int get_power2(uint32_t v) {
+    uint32_t power = 0;
+    uint32_t mask = 0x00000001;
+    while (power < 32) {
+        if ((v & mask) > 0) break;
+        power++;
+        mask <<= 1;
+    }
+    if ((1 << power) != v) return -1;
+    return static_cast<int>(power);
+}
+
+
+
+namespace hgemm_marlin_gptq {
+
+constexpr static int clean_kernel_thread_num = 512;
+constexpr static int clean_kernel_pack_num = 4;
+constexpr static int reduce_kernel_thread_num = 512;
+constexpr static int reduce_kernel_pack_num = 4;
+
+//#define DEBUG
+using PackTypeInt4 = b128VecType;
+using PackTypeInt2 = b64VecType;
+using PackType = uint32_t;
+
+template<class scalar_t>
+__device__ __forceinline__ void mma_16x16x16(PackTypeInt2& a, PackTypeInt2& b, PackTypeInt4& c) {
+}
+
+template<>
+__device__ __forceinline__ void mma_16x16x16<half>(PackTypeInt2& a, PackTypeInt2& b, PackTypeInt4& c) {
+    mma_16x16x16f16(a, b, c);
+}
+
+template<>
+__device__ __forceinline__ void mma_16x16x16<__maca_bfloat16>(PackTypeInt2& a, PackTypeInt2& b, PackTypeInt4& c) {
+    mma_16x16x16bf16(a, b, c);
+}
+
+#if 0
+#ifdef BF16_HIGH_PRECISION
+__global__ void vectorized_elementwise_fp32tobf16(float* input, __maca_bfloat16* output, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        // printf("tid = %d, input = %f, output = %f\n", tid, input[tid], (float)(__maca_bfloat16)input[tid]);
+        *(__maca_bfloat16*)(output+tid) = (__maca_bfloat16)input[tid];
+    }
+}
+#else
+__global__ void vectorized_elementwise_fp16tobf16(__maca_bfloat16* input, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        input[tid] = (float)(*(half*)(input+tid));
+    }
+}
+#endif
+#endif
+
+template<
+    const int THREADS, // number of threads in a threadblock
+    const int PACK_NUM
+    >
+__global__ void clean_zero(float* data, size_t num_elem) {
+    int bidx = blockIdx.x;
+    int tidx = threadIdx.x;
+    int idx = (bidx * THREADS + tidx) * PACK_NUM;
+    float zeros[4] = {0.0, 0.0, 0.0, 0.0};
+    if (idx < num_elem) {
+        *((b128VecType*)(&data[idx])) = *((b128VecType*)zeros);
+    }
+}
+
+template<
+    const int THREADS, // number of threads in a threadblock
+    const int PACK_NUM,
+    const bool USE_C = false
+    >
+__global__ void all_reduce(float* in_data, maca_bfloat16* out_data, size_t num_elem) {
+    int bidx = blockIdx.x;
+    int tidx = threadIdx.x;
+    int idx = (bidx * THREADS + tidx) * PACK_NUM;
+
+    if constexpr(PACK_NUM == 4) {
+        if constexpr(USE_C == true) {
+            float temp_in_fp32[PACK_NUM];
+            float temp_out_fp32[PACK_NUM];
+            maca_bfloat16 temp_out_bf16[PACK_NUM];
+
+            bool pred = idx < num_elem;
+            ldg_b64_reg_noasync(*((b64VecType*)temp_out_bf16), ((b64VecType*)(out_data + idx)), pred, true);
+            ldg_b128_reg_noasync(*((b128VecType*)temp_in_fp32), ((b128VecType*)(in_data + idx)), pred, true);
+
+            #pragma unroll
+            for(int i = 0; i < PACK_NUM; i++) {
+                temp_out_fp32[i] = __bfloat162float(temp_out_bf16[i]);
+                temp_out_fp32[i] += temp_in_fp32[i];
+                temp_out_bf16[i] = __float2bfloat16(temp_out_fp32[i]);
+            }
+
+            if (pred) {
+                *((b64VecType*)(out_data + idx)) = *((b64VecType*)temp_out_bf16);
+            }
+        } else {
+            float temp_in_fp32[PACK_NUM];
+            maca_bfloat16 temp_out_bf16[PACK_NUM];
+
+            bool pred = idx < num_elem;
+            ldg_b128_reg_noasync(*((b128VecType*)temp_in_fp32), ((b128VecType*)(in_data + idx)), pred, true);
+
+            #pragma unroll
+            for(int i = 0; i < PACK_NUM; i++) {
+                temp_out_bf16[i] = __float2bfloat16(temp_in_fp32[i]);
+            }
+
+            if (pred) {
+                *((b64VecType*)(out_data + idx)) = *((b64VecType*)temp_out_bf16);
+            }
+        }
+    }
+}
+
+typedef __NATIVE_VECTOR__(2, float) v2f;
+using PackTypeFloat2 = v2f;
+constexpr static int Q4BITS = 4;
+constexpr static int Q8BITS = 8;
+constexpr static int PACK_RATIO_4BITS = sizeof(PackType) * 8 / Q4BITS;
+constexpr static int PACK_RATIO_8BITS = sizeof(PackType) * 8 / Q8BITS;
+constexpr static int SLICE_M = 16;
+constexpr static int SLICE_N = 16;
+constexpr static int SLICE_K = 32;
+constexpr static int PAD_SLICE_K = 40;
+constexpr static int SLOT    = 16;
+constexpr static int WAVE    = 64;
+constexpr static int WAVE_SLOTS = 4;
+constexpr static int PEUS = 13*8*4; //For C500, There are 8 DPC and each DPC have 13 APs, each AP have 4 PEUs
+constexpr static int MAX_BLOCKS_M = 4;
+constexpr static uint32_t seil = 0x03020706u;
+
+__device__ __forceinline__ void f32x2_cvt_bf16x2(uint32_t& dst, float src[2]) {
+    uint32_t tmp[2];
+    tmp[0] = __builtin_mxc_ubfe(*(reinterpret_cast<uint32_t*>(src)), 16, 1);
+    tmp[0] = tmp[0] + *reinterpret_cast<uint32_t*>(src);
+    tmp[0] = (uint32_t)0x7fff + tmp[0];
+    tmp[1] = __builtin_mxc_ubfe(*(reinterpret_cast<uint32_t*>(src + 1)), 16, 1);
+    tmp[1] = tmp[1] + *(reinterpret_cast<uint32_t*>(src + 1));
+    tmp[1] = (uint32_t)0x7fff + tmp[1];
+    dst = __builtin_mxc_byte_perm(tmp[0], tmp[1], seil);
+}
+
+
+// #define CVT_B0TOF32(q, out) asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(out):"r"(q));
+// #define CVT_B1TOF32(q, out) asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(out):"r"(q));
+// #define CVT_B2TOF32(q, out) asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(out):"r"(q));
+// #define CVT_B3TOF32(q, out) asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(out):"r"(q));
+
+#define CVT_B0TOF32(q, out) out = __builtin_mxc_b0_cast_to_f32(q);
+#define CVT_B1TOF32(q, out) out = __builtin_mxc_b1_cast_to_f32(q);
+#define CVT_B2TOF32(q, out) out = __builtin_mxc_b2_cast_to_f32(q);
+#define CVT_B3TOF32(q, out) out = __builtin_mxc_b3_cast_to_f32(q);
+
+//FIXME: We'd rather a quant group will not divided into serveral blocks
+template<int BLOCKS_M, int BLOCKS_N, int BLOCKS_K>
+struct TileManager {
+    int tile_start_row;
+    int tile_start_col;
+    int tiles_k;
+    int my_iters = 0;
+    bool global_pred = true;
+
+    __device__ __forceinline__ void init(int m, int n, int k, int bidx, int iters) {
+        //Calculate tile start row and cols so we can calculate the offset address of a b and c
+        int tile_idx = iters * bidx;
+        int tiles_n = div_ceil(n, TILE_N);
+        tiles_k = div_ceil(k, TILE_K);
+        //if (tile_idx >= tiles_n*tiles_k) return false;
+        global_pred = tile_idx < tiles_n * tiles_k;
+        int tile_col = tile_idx / tiles_k;
+        int tile_row = tile_idx - tile_col * tiles_k;
+        tile_start_col = tile_col;
+        tile_start_row = tile_row;
+        my_iters = tile_idx + iters >= tiles_n*tiles_k ? tiles_n*tiles_k - tile_idx : iters;
+        my_iters = global_pred ? my_iters : 0;
+    }
+
+    __device__ __forceinline__ void next_tile() {
+        tile_start_col = tile_start_row + 1 == tiles_k ? tile_start_col + 1 : tile_start_col;
+        tile_start_row = tile_start_row + 1 == tiles_k ? 0 : tile_start_row + 1;
+        --my_iters;
+        global_pred = my_iters > 0;
+    }
+
+    __device__ __host__ __forceinline__ bool need_save_data() {
+        if (global_pred && my_iters == 1) return true;
+        if (global_pred && tile_start_row + 1 == tiles_k) return true;
+        return false;
+    }
+
+    //support for preloading next tile in current tile calculation
+    //The point is when all quanted values are dequanted and all a are stored to bsm already
+    //Then the registers for scales, zeros, and a_caches are free to use, bsm for scales are already
+    //free to use.
+    //The main problem we will face is that no more registers can be used to cache tile information
+    //when we want to preload next tile data
+    bool flag_save_data = false;
+    int tile_start_col_cache; //Kept for saving result n calculation
+
+    __device__ __forceinline__ void next_tile_pre() {
+        flag_save_data = need_save_data();
+        tile_start_col_cache = tile_start_col;
+        next_tile();
+    }
+
+    __device__ __forceinline__ bool need_save_data_pre() {
+        return flag_save_data;
+    }
+};
+
+struct ThreadView {
+    int tid;
+    int wave_idx;
+    int wave_tid;
+    int slot_idx;
+    int slot_tid;
+
+    __device__ __forceinline__ void init() {
+        tid = threadIdx.x;
+        wave_idx = tid / WAVE;
+        wave_tid = tid % WAVE;
+        slot_idx = wave_tid / SLOT;
+        slot_tid = wave_tid % SLOT;
+    }
+};
+
+/*
+Currently, we develop a version that TILE size is m64n128k64
+So every TILE we load 64x64x2=8192bytes A, 128x64/8x4=4096bytes B, 128x4=512 scales, 128x4=512 zeros if needed
+
+If more batches is needed, I think it is better to run dequant again. We do not count on situation that m>128 now.
+
+Memory View:
+There are 1/2/4 waves according to BLOCKS_N, wave_count = BLOCKS_N, mostly, BLOCKS_N=4, and THREADS=256
+Each wave will process a m16n16k32 sub tile with two mma instructions, we should re-order data in bsm so
+we can load data from shared memory only with tid or wave_tid, where we can reduce the instructions of 
+calculate address offsets of each data:
+
+BSM A order(assume that BLOCKS_M=4):
+|---------------------------------8x4x16 half value ------------------------------------------------------------------------|
+|-use by wave  slotidx 0 slottid 0------|-use by wave  slotidx 0 slottid 1------|...|-use by wave  slotidx 0 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k00~07|m16k00~07|m32k00~07|m48k00~07|m01k00~07|m17k00~07|m33k00~07|m49k00~07|...|m15k00~07|m31k00~07|m47k00~07|m63k00~07|
+
+|-use by wave  slotidx 1 slottid 0------|-use by wave  slotidx 1 slottid 1------|...|-use by wave  slotidx 1 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k08~15|m16k08~15|m32k08~15|m48k08~15|m01k08~15|m17k08~15|m33k08~15|m49k08~15|...|m15k08~15|m31k08~15|m47k08~15|m63k08~15|
+
+...
+
+|-use by wave  slotidx 3 slottid 0------|-use by wave  slotidx 3 slottid 1------|...|-use by wave  slotidx 3 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k24~31|m16k24~31|m32k24~31|m48k24~31|m01k24~31|m17k24~31|m33k24~31|m49k24~31|...|m15k24~31|m31k24~31|m47k24~31|m63k24~31|
+
+---- next k ----
+|---------------------------------8x4x16 half value ------------------------------------------------------------------------|
+|-use by wave  slotidx 0 slottid 0------|-use by wave  slotidx 0 slottid 1------|...|-use by wave  slotidx 0 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k32~39|m16k32~39|m32k32~39|m48k32~39|m01k32~39|m17k32~39|m33k32~39|m49k32~39|...|m15k32~39|m31k32~39|m47k32~39|m63k32~39|
+
+...
+
+|---------------------------------8x4x16 half value ------------------------------------------------------------------------|
+|-use by wave  slotidx 3 slottid 0------|-use by wave  slotidx 3 slottid 1------|...|-use by wave  slotidx 3 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k56~63|m16k56~63|m32k56~63|m48k56~63|m01k56~63|m17k56~63|m33k56~63|m49k56~63|...|m15k56~63|m31k56~63|m47k56~63|m63k56~63|
+
+When loading a from bsm, each thread only read 4x8 half value with offset wave_tid:
+b128 out[4];
+b128 *bsm_a_ptr;
+out[0] = bsm_a_ptr[wave_tid*BLOCKS_M];
+out[1] = bsm_a_ptr[wave_tid*BLOCKS_M+1];
+out[2] = bsm_a_ptr[wave_tid*BLOCKS_M+2];
+out[3] = bsm_a_ptr[wave_tid*BLOCKS_M+3];
+
+
+BSM B order: here k means the quanted k, which means each k represents 8 dequanted k data
+//Loop = BLOCKS_N*SLICE_N/16/WAVES
+Loop 0:
+|---------16 uint32_t loaded by wave0-|
+k00n00 k00n01 k00n02 k00n03 ... k00n15
+k01n00 k01n01 k01n02 k01n03 ... k01n15
+k02n00 k02n01 k02n02 k02n03 ... k02n15
+k03n00 k03n01 k03n02 k03n03 ... k03n15
+|---------16 uint32_t loaded by wave1-|
+k00n16 k00n17 k00n18 k00n19 ... k00n31
+...
+k03n16 k03n17 k03n18 k03n19 ... k03n31
+|---------16 uint32_t loaded by wave2-|
+k00n32 k00n33 k00n34 k00n35 ... k00n47
+...
+k03n32 k03n33 k03n34 k03n35 ... k03n47
+|---------16 uint32_t loaded by wave3-|
+k00n48 k00n49 k00n50 k00n51 ... k00n63
+...
+k03n48 k03n49 k03n50 k03n51 ... k03n63
+*/
+
+
+//deqaunt a uint32_t
+template<class scalar_t>
+__device__ __forceinline__ void dequant_gptq_4bits(const PackType& p, scalar_t (&out)[8], const v2f& scale, const v2f& scale_zero) {
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        v2f a0;
+        float tmp[2];
+        int p0 = p & 0x0f0f0f0f;
+
+        // CVT_B0TOF32(p0, a0.x);
+        // CVT_B2TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[0] = __float2bfloat16(a0.x);
+        // out[1] = __float2bfloat16(a0.y);
+
+        CVT_B0TOF32(p0, tmp[0]);
+        CVT_B2TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)out), tmp);
+
+        // CVT_B1TOF32(p0, a0.x);
+        // CVT_B3TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[4] = __float2bfloat16(a0.x);
+        // out[5] = __float2bfloat16(a0.y);
+
+        CVT_B1TOF32(p0, tmp[0]);
+        CVT_B3TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 4)), tmp);
+
+        p0 = (p >> 4) & 0x0f0f0f0f;
+
+        // CVT_B0TOF32(p0, a0.x);
+        // CVT_B2TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[2] = __float2bfloat16(a0.x);
+        // out[3] = __float2bfloat16(a0.y);
+
+        CVT_B0TOF32(p0, tmp[0]);
+        CVT_B2TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 2)), tmp);
+
+        // CVT_B1TOF32(p0, a0.x);
+        // CVT_B3TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[6] = __float2bfloat16(a0.x);
+        // out[7] = __float2bfloat16(a0.y);
+
+        CVT_B1TOF32(p0, tmp[0]);
+        CVT_B3TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 6)), tmp);
+    } else {
+        v2f a0;
+        int p0 = p & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, a0.x);
+        CVT_B2TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[0] = (scalar_t)a0.x;
+        out[1] = (scalar_t)a0.y;
+
+        CVT_B1TOF32(p0, a0.x);
+        CVT_B3TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[4] = (scalar_t)a0.x;
+        out[5] = (scalar_t)a0.y;
+
+        p0 = (p >> 4) & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, a0.x);
+        CVT_B2TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[2] = (scalar_t)a0.x;
+        out[3] = (scalar_t)a0.y;
+
+        CVT_B1TOF32(p0, a0.x);
+        CVT_B3TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[6] = (scalar_t)a0.x;
+        out[7] = (scalar_t)a0.y;
+    }
+}
+
+template<class scalar_t>
+__device__ __forceinline__ void dequant_gptq_8bits(const PackType& p, scalar_t (&out)[4], const v2f& scale, const v2f& scale_zero) {
+    v2f a0;
+    CVT_B0TOF32(p, a0.x);
+    CVT_B1TOF32(p, a0.y);
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+    out[0] = (scalar_t)a0.x;
+    out[1] = (scalar_t)a0.y;
+
+    CVT_B2TOF32(p, a0.x);
+    CVT_B3TOF32(p, a0.y);
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+    out[2] = (scalar_t)a0.x;
+    out[3] = (scalar_t)a0.y;
+}
+
+// decompress zero
+__device__ __forceinline__ void decompress_zero_4bits(const PackType& zp, float (&out)[8]) {
+    v2f a0;
+    int p0 = zp & 0x0f0f0f0f;
+    CVT_B0TOF32(p0, a0.x);
+    CVT_B2TOF32(p0, a0.y);
+    out[0] = -a0.x;
+    out[1] = -a0.y;
+
+    CVT_B1TOF32(p0, a0.x);
+    CVT_B3TOF32(p0, a0.y);
+    out[4] = -a0.x;
+    out[5] = -a0.y;
+
+    p0 = (zp >> 4) & 0x0f0f0f0f;
+    CVT_B0TOF32(p0, a0.x);
+    CVT_B2TOF32(p0, a0.y);
+    out[2] = -a0.x;
+    out[3] = -a0.y;
+
+    CVT_B1TOF32(p0, a0.x);
+    CVT_B3TOF32(p0, a0.y);
+    out[6] = -a0.x;
+    out[7] = -a0.y;
+}
+
+namespace __hgemm_singular_blocks_k {
+template<typename scalar_t, const vllm::ScalarTypeId w_type_id, int THREADS, int BLOCKS_M, int BLOCKS_N, int BLOCKS_K, bool HAS_ACT, bool HAS_ZP, bool HAS_M_PRED, bool HAS_NK_PRED>
+struct LoadingManager {
+    constexpr static int FragACount = 2;
+    using FragA = PackType;
+    constexpr static int FragBCount = 1;
+    using FragB = PackType;
+    constexpr static int FragCCount = 4;
+    #ifdef BF16_HIGH_PRECISION
+    using FragC = scalar_t;
+    #else
+    //Directly use half as the final atomic type:
+    //1. Half precision and data range satisfies need of deepseek gemm
+    //2. C500 has no atomic instructions for bfloat16, we cannot atomic a bfloat16 memory
+    //3. The perfect precision type of atomic should be fp32, but the cost is too high to allocate a temp memory for float atomic
+    using atomic_type = half;
+    using FragC = atomic_type;
+    #endif
+    const FragA* A;
+    const FragA* A_loading;
+    const FragB* B;
+    const FragB* B_loading;
+    FragC* C;
+    float* C_temp;
+    using FragScaleLoading = half2;
+    using FragZeroLoading = uint32_t;
+    const FragScaleLoading* scales;
+    const FragScaleLoading* scales_loading;
+    const FragZeroLoading* zeros;
+    const FragZeroLoading* zeros_loading;
+    
+    int m;
+    int n;
+    int k;
+    int quant_group_power2;
+    uint8_t* smem_base;
+    int bidx;
+
+    PackTypeInt4* bsm_a_ptr;
+    scalar_t* bsm_scales_ptr;
+    float* bsm_zeros_ptr;
+    float* remaining_bsm_ptr;
+
+    PackTypeInt2 local_a[BLOCKS_M][2];
+    PackType local_b[N_ITERS];
+    PackType local_b_cache[N_ITERS];
+    scalar_t local_dequanted_b_8bits[N_ITERS][2][PACK_RATIO_8BITS];
+    scalar_t local_dequanted_b[N_ITERS][PACK_RATIO_4BITS];
+    v2f local_scales[N_ITERS];
+    v2f local_zeros[N_ITERS];
+    FragScaleLoading temp_scales;
+    PackType temp_zeros;
+    float output[BLOCKS_M][N_ITERS][4];
+    FragA temp_a[LOADING_A_LOOP];
+
+    TileManager<BLOCKS_M, BLOCKS_N, BLOCKS_K> tile_manager;
+    ThreadView tv;
+
+    __device__ __forceinline__ void set_address(const PackTypeInt4* a,
+        const PackTypeInt4* b,
+        PackTypeInt4* c,
+        PackTypeInt4* c_temp,
+        const PackTypeInt4* scale_ptr,
+        const PackTypeInt4* zp_ptr = nullptr) {
+            A = (const FragA*)a;
+            B = (const FragB*)b;
+            C = (FragC*)c;
+            C_temp = (float*)c_temp;
+            scales = (const FragScaleLoading*)scale_ptr;
+            if constexpr(w_type_id == vllm::kU4.id()) {
+                zeros = (const FragZeroLoading*)zp_ptr;
+            }
+    }
+
+    __device__ __forceinline__ bool debug() {
+        #ifdef DEBUG
+        bool do_print = tv.wave_idx == 1 && tv.slot_idx == 0 && tv.slot_tid == 0;
+        return do_print;
+        #else
+        return false;
+        #endif
+    }
+
+    __device__ __forceinline__ void next_k() {
+        //Update only bsm_a_ptr
+        bsm_a_ptr = (PackTypeInt4*)smem_base;
+        bsm_a_ptr += tv.slot_tid * (PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+    }
+
+    __device__ __forceinline__ void next_k_pre() {
+        A_loading += SLICE_K / FragACount;
+        //B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading += SLICE_K / PACK_RATIO_8BITS * n;
+        }
+    }
+
+    __device__ __forceinline__ void ldg_a(int k_idx) {
+        //32x64/2/256 = 16 / 4 = 4
+        int t = tv.tid;
+        int k_broad = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (SLICE_K / FragACount);
+            int reading_k = t % (SLICE_K / FragACount);
+            int gvm_offset = reading_m * k / FragACount + reading_k;
+            FragA* gvm_addr = (FragA*)A_loading + gvm_offset;
+            //FIXME: we cannot do slice k pad as ldg_b32_bsm_async seems does not support padding
+            if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                bool pred = reading_m < m;
+                bool pred_k = k_broad + reading_k * FragACount < k;
+                pred = pred && pred_k && tile_manager.global_pred;
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_M_PRED) {
+                bool pred = reading_m < m && tile_manager.global_pred;
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_NK_PRED) {
+                bool pred_k = k_broad + reading_k * FragACount < k && tile_manager.global_pred;
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, pred_k, true);
+            } else {
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, tile_manager.global_pred, true);
+            }
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void sts_a() {
+        FragA* to_bsm_a_ptr = (FragA*)smem_base;
+        int t = tv.tid;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (SLICE_K / FragACount);
+            int reading_k = t % (SLICE_K / FragACount);
+            int bsm_offset = reading_m * (PAD_SLICE_K / FragACount) + reading_k;
+            *(to_bsm_a_ptr + bsm_offset) = temp_a[i];
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void lds_a(int midx) {
+        *((PackTypeInt4*)local_a[midx]) = *bsm_a_ptr;
+        bsm_a_ptr += SLOT * (PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t)));
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    __device__ __forceinline__ void ldg_b(int k_idx, int korder = 0) {
+        if constexpr(HAS_NK_PRED) {
+            bool pred_k = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K + tv.slot_idx * PACK_RATIO_4BITS + korder < k;
+            bool pred_n = tile_manager.tile_start_col * TILE_N + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS < n;
+            bool pred = pred_n && pred_k && tile_manager.global_pred;
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), pred, true);
+            }
+        } else {
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), tile_manager.global_pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), tile_manager.global_pred, true);
+            }
+        }
+    }
+
+    __device__ __forceinline__ void swap_b_cache(int i) {
+        local_b[i] = local_b_cache[i];
+    }
+
+    __device__ __forceinline__ void ldg_scales() {
+        bool pred = tv.tid < TILE_N / (sizeof(FragScaleLoading) / sizeof(scalar_t)) && tile_manager.global_pred;
+        if constexpr(HAS_NK_PRED) {
+            pred = pred && tv.tid < (n - tile_manager.tile_start_col * TILE_N) / (sizeof(FragScaleLoading) / sizeof(scalar_t));
+        }
+        //FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x2000) + tv.tid;
+        FragScaleLoading *gvm_addr = (FragScaleLoading*)scales_loading + tv.tid;
+        //ldg_b32_bsm_async(scale_bsm, gvm_addr, pred, false);
+        ldg_b32_reg_noasync(*((PackType*)&temp_scales), gvm_addr, pred, true);
+    }
+
+    __device__ __forceinline__ void ldg_zp() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if constexpr(HAS_NK_PRED) {
+                pred = pred && tv.tid < ((n - tile_manager.tile_start_col * TILE_N) / PACK_RATIO_4BITS);
+            }
+            FragZeroLoading *gvm_addr = (FragZeroLoading*)zeros_loading + tv.tid;
+            ldg_b32_reg_noasync(*((PackType*)&temp_zeros), gvm_addr, pred, true);
+        }
+    }
+
+    __device__ __forceinline__ void sts_scales() {
+        FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x2000) + tv.tid;
+        *scale_bsm = temp_scales;
+    }
+
+    __device__ __forceinline__ void sts_zeros() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if (pred) {
+                float temp[PACK_RATIO_4BITS];
+                decompress_zero_4bits(temp_zeros, temp);
+                float *scale_bsm = (float*)(smem_base + 0x3000) + tv.tid * PACK_RATIO_4BITS;
+                for (int i = 0; i < PACK_RATIO_4BITS; i++) {
+                    *(scale_bsm + i) = temp[i];
+                }
+            }
+        }
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    __device__ __forceinline__ void lds_scales() {
+        if constexpr(N_ITERS==2) {
+            *((half2*)local_dequanted_b[0]) = *((half2*)bsm_scales_ptr);
+        } else if constexpr(N_ITERS==4) {
+            *((PackTypeInt2*)local_dequanted_b[0]) = *((PackTypeInt2*)bsm_scales_ptr);
+        }
+    }
+
+    __device__ __forceinline__ void pack_scales() {
+        if constexpr(w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -8 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU4.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s;
+                if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+                    s = __bfloat162float(local_dequanted_b[0][i]);
+                } else {
+                    s = local_dequanted_b[0][i];
+                }
+                float z = *(bsm_zeros_ptr + i);
+                z = z * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -128 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8.id()) {
+            // should apply zeros
+        }
+    }
+
+    __device__ __forceinline__ void dequant(int kdx, int korder = 0) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            dequant_gptq_4bits<scalar_t>(local_b[kdx], local_dequanted_b[kdx], local_scales[kdx], local_zeros[kdx]);
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            dequant_gptq_8bits<scalar_t>(local_b[kdx], local_dequanted_b_8bits[kdx][korder], local_scales[kdx], local_zeros[kdx]);
+        }
+    }
+
+    __device__ __forceinline__ void matmul(int mdx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b[i]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b[i] + 1), *((PackTypeInt4*)output[mdx][i]));
+	    }
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b_8bits[i][0]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b_8bits[i][1]), *((PackTypeInt4*)output[mdx][i]));
+            }
+        }
+    }
+
+    __device__ __forceinline__ void clear_c() {
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    output[miter][niter][miter2] = 0;
+                }
+            }
+        }
+    }
+
+    //functions for preloading next tile data
+    __device__ __forceinline__ void init_address_pre(int _m, int _n, int _k, int _quant_group_power2, int _bidx, int _iters, uint8_t *_smem_base) {
+        tv.init();
+        m = _m;
+        n = _n;
+        k = _k;
+        quant_group_power2 = _quant_group_power2;
+        bidx = _bidx;
+        smem_base = _smem_base;
+        tile_manager.init(m, n, k, bidx, _iters);
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+    __device__ __forceinline__ void init_tile_pre(int col, int row) {
+        //Initialize start slice address and set them to A_loading and B_loading
+        int offset_n = col * TILE_N;
+        int offset_k = row * TILE_K;
+        //A_loading address will always be valid
+        A_loading = A + offset_k / (FragACount);
+        //B_loading = B + (offset_k / PACK_RATIO_4BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_4BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_8BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n * 2 + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        }
+        scales_loading = scales + ((offset_k >> quant_group_power2) * n + offset_n) / (sizeof(FragScaleLoading)/sizeof(scalar_t));
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            zeros_loading = zeros + ((offset_k >> quant_group_power2) * n + offset_n) / PACK_RATIO_4BITS;
+        }
+    }
+
+    __device__ __forceinline__ void next_tile_pre() {
+        tile_manager.next_tile_pre();
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+    __device__ __forceinline__ void init_bsm_addr() {
+        bsm_a_ptr = (PackTypeInt4*)smem_base;           //use 8k bytes, will load at most 32x128*sizeof(half), either m32k128 or m128k32
+        remaining_bsm_ptr = (float*)(smem_base + 0x2000 + 0x1000); //3k bytes
+        bsm_a_ptr += tv.slot_tid * (PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+        bsm_scales_ptr = (scalar_t*)(smem_base + 0x2000);      //use 128xsizeof(float)*2 = 1k
+        bsm_scales_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bsm_zeros_ptr = (float*)(smem_base + 0x3000);
+            bsm_zeros_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        }
+    }
+
+    __device__ __forceinline__ void write_c(int offset, const float& v) {
+        if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+            #ifdef BF16_HIGH_PRECISION
+            atomicAdd(C_temp + offset, v);
+            #else
+            atomicAdd(C+offset, (atomic_type)v);
+            #endif
+        } else {
+            atomicAdd(C + offset, (scalar_t)v);
+        }
+    }
+
+    //atomic write to c
+    __device__ __forceinline__ void write_c_pre() {
+        int k_broad = tv.slot_idx * 4;
+        int n_broad = (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS + tile_manager.tile_start_col_cache * TILE_N;
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                int store_n = n_broad + niter;
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    int store_m = k_broad + miter * SLICE_M + miter2;
+                    if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                        if (store_m < m && store_n < n) {
+                            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+			            }
+                    } else if constexpr(HAS_M_PRED) {
+                        if (store_m < m) {
+                            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+			            }
+                    } else if constexpr(HAS_NK_PRED) {
+                        if (store_n < n) {
+                            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+			            }
+                    } else {
+                        write_c(store_m * n + store_n, output[miter][niter][miter2]);
+		            }
+                }
+            }
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters1(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            ldg_b(k_idx, 1);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            dequant(0);
+            swap_b_cache(0);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0); //dequant b0
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters2(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(1);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(1);   //dequant b64
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            ldg_b(k_idx, 1);
+            dequant(0); //dequant b0
+            dequant(1);
+            swap_b_cache(0);
+            swap_b_cache(1);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1);
+            dequant(1, 1);   //dequant b64
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters3(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(1);
+            dequant(1);
+            swap_b_cache(2);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(2); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            ldg_b(k_idx, 1);
+            dequant(0); //dequant b0
+            dequant(1); //dequant b0
+            dequant(2); //dequant b0
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+            dequant(1, 1); //dequant b0
+            dequant(2, 1); //dequant b0
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters4(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(1);
+            dequant(1); //dequant b1
+            swap_b_cache(2);
+            swap_b_cache(3);
+            dequant(2); //dequant b2
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(3); //dequant b3
+        } else {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            ldg_b(k_idx, 1);
+            dequant(1); //dequant b0
+            dequant(2); //dequant b0
+            dequant(3); //dequant b0
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b3
+            dequant(1, 1); //dequant b3
+            dequant(2, 1); //dequant b3
+            dequant(3, 1); //dequant b3
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant(int kdx) {
+        if constexpr(N_ITERS == 1) on_dequant_niters1<KTAIL>(kdx);
+        else if constexpr(N_ITERS == 2) on_dequant_niters2<KTAIL>(kdx);
+        else if constexpr(N_ITERS == 3) on_dequant_niters3<KTAIL>(kdx);
+        else if constexpr(N_ITERS == 4) on_dequant_niters4<KTAIL>(kdx);
+    }
+};
+
+template<typename scalar_t,
+    const vllm::ScalarTypeId w_type_id,
+    const int THREADS,          // number of threads in a threadblock
+    const int BLOCKS_M,         // number of 16x16 blocks in the m
+                                // dimension (batchsize) of the
+                                // threadblock
+    const int BLOCKS_N,         // same for n dimension (output)
+    const int BLOCKS_K,         // same for k dimension (reduction)
+    const bool HAS_ACT_ORDER,   // whether act_order is enabled
+    const bool HAS_ZP,          // whether zero-points are enabled
+    const bool HAS_M_PRED = true,  //If we should use predictors to load m from gvm
+    const bool HAS_NK_PRED = true  //If we should use predictors to load nk from gvm
+    >
+__global__ void hgemm_gptq(
+    const PackTypeInt4* __restrict__ A,  // fp16 input matrix of shape mxk
+    const PackTypeInt4* __restrict__ B,  // 4bit quantized weight matrix of shape kxn
+    PackTypeInt4* __restrict__ C,        // fp16 output buffer of shape mxn
+    PackTypeInt4* __restrict__ C_tmp,    // fp32 tmp output buffer (for reduce)
+    const PackTypeInt4* __restrict__ scales_ptr,  // fp16 quantization scales of shape
+                                          // (k/groupsize)xn
+    const PackTypeInt4* __restrict__ zp_ptr,      // 4bit packed zero-points of shape
+                                          // (k/groupsize)x(n/pack_factor)
+    const int* __restrict__ g_idx,        // int32 group indices of shape k
+    int prob_m,           // batch dimension m
+    int prob_n,           // output dimension n
+    int prob_k,           // reduction dimension k
+    int quant_group_power2, // quant group means how many quanted values share the same scale and zero, this value restricts to 2^x where x >= 5
+    int max_iters,        // max tile iterations for one block
+    int* locks,           // extra global storage for barrier synchronization
+    bool use_fp32_reduce  // whether to use fp32 global reduce
+) {
+    int bidx = blockIdx.x;
+    __shared__ uint8_t smem_base[0x4000]; //4x16x256 = 16Kbytes
+    using LoadingManagerType = LoadingManager<scalar_t, w_type_id, THREADS, BLOCKS_M, BLOCKS_N, BLOCKS_K, HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED>;
+    LoadingManagerType loading_manager;
+    A += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_k / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    C += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    #ifdef BF16_HIGH_PRECISION
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        C_tmp += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(float));
+    }
+    #endif
+    loading_manager.set_address(A, B, C, C_tmp, scales_ptr, zp_ptr);
+    //loading_manager.init_address(prob_m, prob_n, prob_k, bidx, max_iters, smem_base);
+    loading_manager.init_address_pre(std::min<int>(MAX_BLOCKS_M*SLICE_M, prob_m - blockIdx.y * (MAX_BLOCKS_M * SLICE_M)), prob_n, prob_k, quant_group_power2, bidx, max_iters, smem_base);
+    loading_manager.clear_c();
+
+    while (max_iters > 0) {
+        loading_manager.init_bsm_addr(); //reset all bsm address for current tile
+        loading_manager.ldg_scales(); //Load all scales to bsm
+        loading_manager.ldg_zp();
+        loading_manager.ldg_b(0);    //load b0 and b64, two gvm
+        loading_manager.ldg_a(0);    //Load first k0~31 and all m, one ldg_b128, heavy load
+        loading_manager.sts_scales();
+        loading_manager.sts_zeros();
+        barrier_bsm;
+        loading_manager.lds_scales(); //load scale0 and scale64
+        loading_manager.pack_scales(); //pack scales into two v2f structure
+
+        int k_idx = 0;
+        if constexpr(BLOCKS_K > 1) {
+            #pragma unroll BLOCKS_K - 1
+            for (; k_idx < BLOCKS_K - 1; k_idx++) {
+                int m_idx = 0;
+                loading_manager.template on_dequant<false>(k_idx);
+                //Loop for 3 times so that we can add some loading instructions before matmul
+                if constexpr(BLOCKS_M > 1) {
+                    #pragma unroll BLOCKS_M - 1
+                    for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                        loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                        loading_manager.matmul(m_idx); //do matmul
+                    }
+                }
+                barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+                loading_manager.next_k(); //modify gvm/bsm address of a and b
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        int m_idx = 0;
+        loading_manager.template on_dequant<true>(k_idx);
+        if constexpr(BLOCKS_M > 1) {
+            #pragma unroll BLOCKS_M - 1
+            for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        loading_manager.next_tile_pre();
+
+        loading_manager.matmul(m_idx); //do matmul
+        max_iters--;
+
+        if (loading_manager.tile_manager.need_save_data_pre()) {
+            loading_manager.write_c_pre(); // reduce and write back
+            loading_manager.clear_c();
+        }
+
+        barrier_bsm;
+    }
+}
+
+} //end of namespace __hgemm_singular_blocks_k
+
+namespace __hgemm_even_blocks_k {
+template<typename scalar_t, const vllm::ScalarTypeId w_type_id, int THREADS, int BLOCKS_M, int BLOCKS_N, int BLOCKS_K, bool HAS_ACT, bool HAS_ZP, bool HAS_M_PRED, bool HAS_NK_PRED>
+struct LoadingManager {
+    constexpr static int FragACount = 4;
+    using FragA = PackTypeInt2;
+    constexpr static int FragBCount = 1;
+    using FragB = PackType;
+    constexpr static int FragCCount = 4;
+    #ifdef BF16_HIGH_PRECISION
+    using FragC = scalar_t;
+    #else
+    //Directly use half as the final atomic type:
+    //1. Half precision and data range satisfies need of deepseek gemm
+    //2. C500 has no atomic instructions for bfloat16, we cannot atomic a bfloat16 memory
+    //3. The perfect precision type of atomic should be fp32, but the cost is too high to allocate a temp memory for float atomic
+    using atomic_type = half;
+    using FragC = atomic_type;
+    #endif
+    const FragA* A;
+    const FragA* A_loading;
+    const FragB* B;
+    const FragB* B_loading;
+    FragC* C;
+    float* C_temp;
+    using FragScaleLoading = half2;
+    using FragZeroLoading = uint32_t;
+    const FragScaleLoading* scales;
+    const FragScaleLoading* scales_loading;
+    const FragZeroLoading* zeros;
+    const FragZeroLoading* zeros_loading;
+
+    constexpr static int DOUBLE_SLICE_K = SLICE_K * 2;
+    constexpr static int DOUBLE_PAD_SLICE_K = SLICE_K * 2 + sizeof(PackTypeInt4) / sizeof(scalar_t);
+
+    int m;
+    int n;
+    int k;
+    int quant_group_power2;
+    uint8_t* smem_base;
+    int bidx;
+
+    PackTypeInt4* bsm_a_ptr;
+    scalar_t* bsm_scales_ptr;
+    float* bsm_zeros_ptr;
+    //float* remaining_bsm_ptr;
+
+    PackTypeInt2 local_a[BLOCKS_M][2];
+    PackType local_b[N_ITERS];
+    PackType local_b_cache[N_ITERS];
+    scalar_t local_dequanted_b[N_ITERS][PACK_RATIO_4BITS];
+    scalar_t local_dequanted_b_8bits[N_ITERS][2][PACK_RATIO_8BITS];
+    v2f local_scales[N_ITERS];
+    v2f local_zeros[N_ITERS];
+    FragScaleLoading temp_scales;
+    PackType temp_zeros;
+    float output[BLOCKS_M][N_ITERS][4];
+    FragA temp_a[LOADING_A_LOOP];
+
+    TileManager<BLOCKS_M, BLOCKS_N, BLOCKS_K> tile_manager;
+    ThreadView tv;
+
+    __device__ __forceinline__ void set_address(const PackTypeInt4* a,
+        const PackTypeInt4* b,
+        PackTypeInt4* c,
+        PackTypeInt4* c_temp,
+        const PackTypeInt4* scale_ptr,
+        const PackTypeInt4* zp_ptr = nullptr) {
+            A = (const FragA*)a;
+            B = (const FragB*)b;
+            C = (FragC*)c;
+            C_temp = (float*)c_temp;
+            scales = (const FragScaleLoading*)scale_ptr;
+            if constexpr(w_type_id == vllm::kU4.id()) {
+                zeros = (const FragZeroLoading*)zp_ptr;
+            }
+    }
+
+    __device__ __forceinline__ bool debug() {
+        #ifdef DEBUG
+        bool do_print = tv.wave_idx == 1 && tv.slot_idx == 0 && tv.slot_tid == 0;
+        return do_print;
+        #else
+        return false;
+        #endif
+    }
+
+    __device__ __forceinline__ void next_k0() {
+        //reset bsm a to base
+        bsm_a_ptr = (PackTypeInt4*)smem_base;
+        bsm_a_ptr += tv.slot_tid * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+    }
+
+    __device__ __forceinline__ void next_k1() {
+        //Update only bsm_a_ptr
+        bsm_a_ptr = (PackTypeInt4*)smem_base;
+        bsm_a_ptr += tv.slot_tid * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx + WAVE_SLOTS;
+        //load k32~k63
+        //bsm_a_ptr += 4;
+    }
+
+    __device__ __forceinline__ void next_k0_pre() {
+        //A_loading += SLICE_K / FragACount;
+        //B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading += SLICE_K / PACK_RATIO_8BITS * n;
+        }
+    }
+
+    __device__ __forceinline__ void next_k1_pre() {
+        A_loading += DOUBLE_SLICE_K / FragACount;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading += SLICE_K / PACK_RATIO_8BITS * n;
+        }
+    }
+
+    __device__ __forceinline__ void ldg_a(int k_idx) {
+        //32x64/2/256 = 16 / 4 = 4
+        int t = tv.tid;
+        int k_broad = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (DOUBLE_SLICE_K / FragACount);
+            int reading_k = t % (DOUBLE_SLICE_K / FragACount);
+            int gvm_offset = reading_m * k / FragACount + reading_k;
+            FragA* gvm_addr = (FragA*)A_loading + gvm_offset;
+            //FIXME: we cannot do slice k pad as ldg_b32_bsm_async seems does not support padding
+            if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                bool pred = reading_m < m;
+                bool pred_k = k_broad + reading_k * FragACount < k;
+                pred = pred && pred_k && tile_manager.global_pred;
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_M_PRED) {
+                bool pred = reading_m < m && tile_manager.global_pred;
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_NK_PRED) {
+                bool pred_k = k_broad + reading_k * FragACount < k && tile_manager.global_pred;
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, pred_k, true);
+            } else {
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, tile_manager.global_pred, true);
+            }
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void sts_a() {
+        FragA* to_bsm_a_ptr = (FragA*)smem_base;
+        int t = tv.tid;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (DOUBLE_SLICE_K / FragACount);
+            int reading_k = t % (DOUBLE_SLICE_K / FragACount);
+            int bsm_offset = reading_m * (DOUBLE_PAD_SLICE_K / FragACount) + reading_k;
+            *(to_bsm_a_ptr + bsm_offset) = temp_a[i];
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void lds_a(int midx) {
+        *((PackTypeInt4*)local_a[midx]) = *bsm_a_ptr;
+        bsm_a_ptr += SLOT * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t)));
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    //korder used gptq_8bits, ldg_b will load two times in one SLICE_K
+    //For example, t0 loads packed_k0, packed_k1, and packed_k0 represents a packed 4 ks in first line of B,
+    //and packed_k1 represents a packed 4 ks in second line of B
+    __device__ __forceinline__ void ldg_b(int k_idx, int korder = 0) {
+        if constexpr(HAS_NK_PRED) {
+            bool pred_k = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K + tv.slot_idx * PACK_RATIO_4BITS + korder < k;
+            bool pred_n = tile_manager.tile_start_col * TILE_N + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS < n;
+            bool pred = pred_n && pred_k && tile_manager.global_pred;
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), pred, true);
+            }
+        } else {
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), tile_manager.global_pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), tile_manager.global_pred, true);
+            }
+        }
+    }
+
+    __device__ __forceinline__ void swap_b_cache(int i) {
+        local_b[i] = local_b_cache[i];
+    }
+
+    __device__ __forceinline__ void ldg_scales() {
+        bool pred = tv.tid < TILE_N / (sizeof(FragScaleLoading) / sizeof(scalar_t)) && tile_manager.global_pred;
+        if constexpr(HAS_NK_PRED) {
+            pred = pred && tv.tid < (n - tile_manager.tile_start_col * TILE_N) / (sizeof(FragScaleLoading) / sizeof(scalar_t));
+        }
+        //FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x2000) + tv.tid;
+        FragScaleLoading *gvm_addr = (FragScaleLoading*)scales_loading + tv.tid;
+        //ldg_b32_bsm_async(scale_bsm, gvm_addr, pred, false);
+        ldg_b32_reg_noasync(*((PackType*)&temp_scales), gvm_addr, pred, true);
+    }
+
+    __device__ __forceinline__ void ldg_zp() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if constexpr(HAS_NK_PRED) {
+                pred = pred && tv.tid < ((n - tile_manager.tile_start_col * TILE_N) / PACK_RATIO_4BITS);
+            }
+            FragZeroLoading *gvm_addr = (FragZeroLoading*)zeros_loading + tv.tid;
+            ldg_b32_reg_noasync(*((PackType*)&temp_zeros), gvm_addr, pred, true);
+        }
+    }
+
+    __device__ __forceinline__ void sts_scales() {
+        FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x3000) + tv.tid;
+        *scale_bsm = temp_scales;
+    }
+
+    __device__ __forceinline__ void sts_zeros() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if (pred) {
+                float temp[PACK_RATIO_4BITS];
+                decompress_zero_4bits(temp_zeros, temp);
+                float *zeros_bsm = (float*)(smem_base + 0x3400) + tv.tid * PACK_RATIO_4BITS;
+                for (int i = 0; i < PACK_RATIO_4BITS; i++) {
+                    *(zeros_bsm + i) = temp[i];
+                }
+            }
+        }
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    __device__ __forceinline__ void lds_scales() {
+        if constexpr(N_ITERS==2) {
+            *((half2*)local_dequanted_b[0]) = *((half2*)bsm_scales_ptr);
+        } else if constexpr(N_ITERS==4) {
+            *((PackTypeInt2*)local_dequanted_b[0]) = *((PackTypeInt2*)bsm_scales_ptr);
+        }
+    }
+
+    __device__ __forceinline__ void pack_scales() {
+        if constexpr(w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -8 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU4.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s;
+                if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+                    s = __bfloat162float(local_dequanted_b[0][i]);
+                } else {
+                    s = local_dequanted_b[0][i];
+                }
+                float z = *(bsm_zeros_ptr + i);
+                z = z * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -128 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8.id()) {
+            // should apply zeros
+        }
+    }
+
+    __device__ __forceinline__ void dequant(int kdx, int korder = 0) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            dequant_gptq_4bits<scalar_t>(local_b[kdx], local_dequanted_b[kdx], local_scales[kdx], local_zeros[kdx]);
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            dequant_gptq_8bits<scalar_t>(local_b[kdx], local_dequanted_b_8bits[kdx][korder], local_scales[kdx], local_zeros[kdx]);
+        }
+    }
+
+    __device__ __forceinline__ void matmul(int mdx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b[i]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b[i] + 1), *((PackTypeInt4*)output[mdx][i]));
+            }
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b_8bits[i][0]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b_8bits[i][1]), *((PackTypeInt4*)output[mdx][i]));
+            }
+        }
+    }
+
+    __device__ __forceinline__ void clear_c() {
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    output[miter][niter][miter2] = 0;
+                }
+            }
+        }
+    }
+
+    //functions for preloading next tile data
+    __device__ __forceinline__ void init_address_pre(int _m, int _n, int _k, int _quant_group_power2, int _bidx, int _iters, uint8_t *_smem_base) {
+        tv.init();
+        m = _m;
+        n = _n;
+        k = _k;
+        quant_group_power2 = _quant_group_power2;
+        bidx = _bidx;
+        smem_base = _smem_base;
+        tile_manager.init(m, n, k, bidx, _iters);
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+     __device__ __forceinline__ void init_tile_pre(int col, int row) {
+        //Initialize start slice address and set them to A_loading and B_loading
+        int offset_n = col * TILE_N;
+        int offset_k = row * TILE_K;
+        //A_loading address will always be valid
+        A_loading = A + offset_k / (FragACount);
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_4BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_8BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n * 2 + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        }
+        scales_loading = scales + ((offset_k >> quant_group_power2) * n + offset_n) / (sizeof(FragScaleLoading)/sizeof(scalar_t));
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            zeros_loading = zeros + ((offset_k >> quant_group_power2) * n + offset_n) / PACK_RATIO_4BITS;
+        }
+    }
+
+    __device__ __forceinline__ void next_tile_pre() {
+        tile_manager.next_tile_pre();
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+    __device__ __forceinline__ void init_bsm_addr() {
+        bsm_a_ptr = (PackTypeInt4*)smem_base;           //use 8k bytes, will load at most 32x128*sizeof(half), either m32k128 or m128k32
+        //remaining_bsm_ptr = (float*)(smem_base + 0x2000 + 0x1000); //3k bytes
+        bsm_a_ptr += tv.slot_tid * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+        bsm_scales_ptr = (scalar_t*)(smem_base + 0x3000);      //use 128xsizeof(float)*2 = 1k
+        bsm_scales_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bsm_zeros_ptr = (float*)(smem_base + 0x3400);      //use 128xsizeof(float)*2 = 1k
+            bsm_zeros_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        }
+    }
+
+    __device__ __forceinline__ void write_c(int offset, const float& v) {
+        if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+            #ifdef BF16_HIGH_PRECISION
+	        atomicAdd(C_temp + offset, v);
+            #else
+            atomicAdd(C + offset, (atomic_type)v);
+            #endif
+        } else {
+            atomicAdd(C + offset, (scalar_t)v);
+        }
+    }
+
+    //atomic write to c
+    __device__ __forceinline__ void write_c_pre() {
+        int k_broad = tv.slot_idx * 4;
+        int n_broad = (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS + tile_manager.tile_start_col_cache * TILE_N;
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                int store_n = n_broad + niter;
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    int store_m = k_broad + miter * SLICE_M + miter2;
+                    if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                        if (store_m < m && store_n < n) {
+			                write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                        }
+                    } else if constexpr(HAS_M_PRED) {
+                        if (store_m < m) {
+			                write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                        }
+                    } else if constexpr(HAS_NK_PRED) {
+                        if (store_n < n) {
+			                write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                        }
+                    } else {
+			            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                    }
+                }
+            }
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_sts_a() {
+        if constexpr(K == 0) {
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_preload(int k_idx) {
+        if constexpr(!KTAIL) {
+            if constexpr(K == 0) {
+                next_k0_pre(); // preload gvm a/b
+            } else {
+                next_k1_pre(); // preload gvm a/b
+            }
+            ldg_b(k_idx + K + 1); //preload b for next k
+            if constexpr(K == 1) {
+                ldg_a(k_idx + K + 1); //preload a for next k
+            }
+        } else {
+            next_tile_pre();
+            ldg_scales();
+            ldg_zp();
+            ldg_b(0);
+            ldg_a(0);
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters1(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            on_sts_a<K,KTAIL>(); // reorder data_a from reg(gvm resouce) to bsm
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            ldg_b(0, 1);
+            dequant(0, 0);
+            on_sts_a<K,KTAIL>(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(0);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters2(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(1);   //dequant b64
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            ldg_b(0, 1);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            dequant(1);
+            swap_b_cache(0);
+            swap_b_cache(1);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1);   //dequant b64
+            dequant(1, 1);   //dequant b64
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters3(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            dequant(1);
+            swap_b_cache(2);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(2); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            ldg_b(0, 1);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            dequant(1);
+            dequant(2);
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+            dequant(1, 1); //dequant b0
+            dequant(2, 1); //dequant b0
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters4(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            dequant(1); //dequant b1
+            swap_b_cache(2);
+            swap_b_cache(3);
+            dequant(2); //dequant b2
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(3); //dequant b3
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            ldg_b(0, 1);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            dequant(1); //dequant b1
+            dequant(2); //dequant b2
+            dequant(3); //dequant b2
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+            dequant(1, 1); //dequant b1
+            dequant(2, 1); //dequant b2
+            dequant(3, 1); //dequant b3
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant(int kdx) {
+        if constexpr(N_ITERS == 1) on_dequant_niters1<K,KTAIL>(kdx);
+        else if constexpr(N_ITERS == 2) on_dequant_niters2<K,KTAIL>(kdx);
+        else if constexpr(N_ITERS == 3) on_dequant_niters3<K,KTAIL>(kdx);
+        else if constexpr(N_ITERS == 4) on_dequant_niters4<K,KTAIL>(kdx);
+    }
+};
+
+template<typename scalar_t,
+    const vllm::ScalarTypeId w_type_id,
+    const int THREADS,          // number of threads in a threadblock
+    const int BLOCKS_M,         // number of 16x16 blocks in the m
+                                // dimension (batchsize) of the
+                                // threadblock
+    const int BLOCKS_N,         // same for n dimension (output)
+    const int BLOCKS_K,         // same for k dimension (reduction)
+    const bool HAS_ACT_ORDER,   // whether act_order is enabled
+    const bool HAS_ZP,          // whether zero-points are enabled
+    const bool HAS_M_PRED = true,  //If we should use predictors to load m from gvm
+    const bool HAS_NK_PRED = true  //If we should use predictors to load nk from gvm
+    >
+__global__ void hgemm_gptq(
+    const PackTypeInt4* __restrict__ A,  // fp16 input matrix of shape mxk
+    const PackTypeInt4* __restrict__ B,  // 4bit quantized weight matrix of shape kxn
+    PackTypeInt4* __restrict__ C,        // fp16 output buffer of shape mxn
+    PackTypeInt4* __restrict__ C_tmp,    // fp32 tmp output buffer (for reduce)
+    const PackTypeInt4* __restrict__ scales_ptr,  // fp16 quantization scales of shape
+                                          // (k/groupsize)xn
+    const PackTypeInt4* __restrict__ zp_ptr,      // 4bit packed zero-points of shape
+                                          // (k/groupsize)x(n/pack_factor)
+    const int* __restrict__ g_idx,        // int32 group indices of shape k
+    int prob_m,           // batch dimension m
+    int prob_n,           // output dimension n
+    int prob_k,           // reduction dimension k
+    int quant_group_power2,
+    int max_iters,        // max tile iterations for one block
+    int* locks,           // extra global storage for barrier synchronization
+    bool use_fp32_reduce  // whether to use fp32 global reduce
+) {
+    int bidx = blockIdx.x;
+    __shared__ uint8_t smem_base[0x4000]; //4x16x256 = 16Kbytes
+    using LoadingManagerType = LoadingManager<scalar_t, w_type_id, THREADS, BLOCKS_M, BLOCKS_N, BLOCKS_K, HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED>;
+    LoadingManagerType loading_manager;
+    A += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_k / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    C += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    #ifdef BF16_HIGH_PRECISION
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        C_tmp += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(float));
+    }
+    #endif
+    loading_manager.set_address(A, B, C, C_tmp, scales_ptr, zp_ptr);
+    //loading_manager.init_address(prob_m, prob_n, prob_k, bidx, max_iters, smem_base);
+    loading_manager.init_address_pre(std::min<int>(MAX_BLOCKS_M*SLICE_M, prob_m - blockIdx.y * (MAX_BLOCKS_M * SLICE_M)), prob_n, prob_k, quant_group_power2, bidx, max_iters, smem_base);
+
+    loading_manager.ldg_scales(); //Load all scales to bsm
+    loading_manager.ldg_zp();
+    loading_manager.ldg_b(0);    //load b in k0~31
+    loading_manager.ldg_a(0);    //Load first k0~63 and all m
+    loading_manager.clear_c();
+
+    while (max_iters > 0) {
+        loading_manager.init_bsm_addr(); //reset all bsm address for current tile
+        loading_manager.sts_scales();
+        loading_manager.sts_zeros();
+        barrier_bsm;
+        loading_manager.lds_scales(); //load scale0 and scale64
+        loading_manager.pack_scales(); //pack scales into two v2f structure
+
+        int k_idx = 0;
+        if constexpr(BLOCKS_K / 2 - 1 > 0) {
+            #pragma unroll BLOCKS_K / 2 - 1
+            for (int kloop = 0; kloop < BLOCKS_K / 2 - 1; kloop++) {
+                int m_idx = 0;
+                loading_manager.template on_dequant<0, false>(k_idx);
+                if constexpr(BLOCKS_M > 1) {
+                    #pragma unroll BLOCKS_M - 1
+                    for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                        loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                        loading_manager.matmul(m_idx); //do matmul
+                    }
+                }
+                barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+                loading_manager.next_k1(); //modify gvm/bsm address of a and b
+                loading_manager.matmul(m_idx); //do matmul
+                m_idx = 0;
+                loading_manager.template on_dequant<1, false>(k_idx);
+                if constexpr(BLOCKS_M > 1) {
+                    #pragma unroll BLOCKS_M - 1
+                    for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                        loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                        loading_manager.matmul(m_idx); //do matmul
+                    }
+                }
+                barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+                loading_manager.next_k0(); //modify gvm/bsm address of a and b
+                loading_manager.matmul(m_idx); //do matmul
+                k_idx += 2;
+            }
+        }
+
+        int m_idx = 0;
+        loading_manager.template on_dequant<0, false>(k_idx);
+        if constexpr(BLOCKS_M > 1) {
+            #pragma unroll BLOCKS_M - 1
+            for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+        loading_manager.next_k1(); //modify gvm/bsm address of a and b
+        loading_manager.matmul(m_idx); //do matmul
+        m_idx = 0;
+        loading_manager.template on_dequant<1, true>(k_idx);
+        if constexpr(BLOCKS_M > 1) {
+            #pragma unroll BLOCKS_M - 1
+            for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+        //loading_manager.next_tile_pre(); should move into on_dequant ?
+        loading_manager.matmul(m_idx); //do matmul
+
+        max_iters--;
+
+        if (loading_manager.tile_manager.need_save_data_pre()) {
+            loading_manager.write_c_pre(); // reduce and write back
+            loading_manager.clear_c();
+        }
+
+        barrier_bsm;
+    }
+}
+} //end of namespace __hgemm_even_blocks_k
+
+template<typename scalar_t,
+    const vllm::ScalarTypeId w_type_id,
+    int THREADS,
+    int BLOCKS_M,
+    int BLOCKS_N,
+    int BLOCKS_K,
+    bool HAS_ACT_ORDER,
+    bool HAS_ZP,
+    bool HAS_M_PRED,
+    bool HAS_NK_PRED>
+bool launch_gemm_gptq_kernel(const PackTypeInt4* A,
+    const PackTypeInt4* B,
+    PackTypeInt4* C,
+    PackTypeInt4* C_temp,
+    const PackTypeInt4* scales,
+    const PackTypeInt4* zeros,
+    int* g_idx, int m, int n, int k, int quant_group, int chunks, cudaStream_t stream = nullptr) {
+    int tiles_m = div_ceil(m, TILE_M);
+    int tiles_n = div_ceil(n, TILE_N);
+    int tiles_k = div_ceil(k, TILE_K);
+    if (TILE_K > quant_group && TILE_K % quant_group != 0) {
+        printf("Invalid TILE_K %d that can not be dived by QUANT_GROUP %d\n", TILE_K, quant_group);
+        return false;
+    }
+
+    int total_tiles = tiles_n * tiles_k;
+    int blocks = PEUS;
+    int iters = div_ceil(total_tiles, PEUS);
+    if (total_tiles < PEUS) {
+        if (TILE_K < quant_group) {
+            iters = quant_group / TILE_K;
+            blocks = div_ceil(total_tiles, iters);
+        } else {
+            iters = 1;
+            blocks = total_tiles;
+        }
+    } else {
+        if (TILE_K < quant_group) {
+            iters = div_ceil(iters, quant_group / TILE_K) * quant_group / TILE_K;
+            blocks = div_ceil(total_tiles, iters);
+        }
+    }
+    while (iters * blocks - total_tiles >= iters) {
+        blocks -= 1;
+    }
+
+    if (total_tiles < blocks) {
+        printf("total slice %d < blocks %d, Invalid configure\n", total_tiles, blocks);
+        return false;
+    }
+    // printf("Launching hgemm_gptq_4bits THREADS=%d, BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_K=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d, tiles_n=%d, tiles_k = %d, total_tiles = %d, iters = %d, blocks = %d, chunks = %d, TILE=m%dn%dk%d\n",
+    // THREADS, BLOCKS_M, BLOCKS_N, BLOCKS_K, HAS_M_PRED, HAS_NK_PRED, tiles_n, tiles_k, total_tiles, iters, blocks, chunks, BLOCKS_M*SLICE_M, BLOCKS_N*SLICE_N, BLOCKS_K*SLICE_K
+    // );
+
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        size_t num_elem = size_t(m) * size_t(n);
+        size_t clean_blocks = std::max(size_t(1), num_elem / (clean_kernel_thread_num * clean_kernel_pack_num));
+        clean_zero<clean_kernel_thread_num, clean_kernel_pack_num><<<clean_blocks, clean_kernel_thread_num>>>((float*)C_temp, num_elem);
+    }
+
+
+    //It is better to do perm before launch kernel
+    if constexpr(BLOCKS_K % 2 == 1) {
+        __hgemm_singular_blocks_k::hgemm_gptq<scalar_t,
+            w_type_id,
+            THREADS,
+            BLOCKS_M, BLOCKS_N, BLOCKS_K,
+            HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED
+            ><<<dim3(std::max(blocks,1), std::max(chunks,1), 1), THREADS, 0, stream>>>(A, B, C, C_temp, scales, zeros, g_idx, m, n, k, get_power2(quant_group), iters, nullptr, false);
+    } else {
+        __hgemm_even_blocks_k::hgemm_gptq<scalar_t,
+            w_type_id,
+            THREADS,
+            BLOCKS_M, BLOCKS_N, BLOCKS_K,
+            HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED
+            ><<<dim3(std::max(blocks,1), std::max(chunks,1), 1), THREADS, 0, stream>>>(A, B, C, C_temp, scales, zeros, g_idx, m, n, k, get_power2(quant_group), iters, nullptr, false);
+    }
+
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        size_t num_elem = size_t(m) * size_t(n);
+        size_t reduce_blocks = std::max(size_t(1), num_elem / (reduce_kernel_thread_num * reduce_kernel_pack_num));
+        all_reduce<reduce_kernel_thread_num, reduce_kernel_pack_num, false><<<reduce_blocks, reduce_kernel_thread_num>>>((float*)C_temp, (maca_bfloat16*)C, num_elem);
+    }
+
+    return true;
+}
+
+}
+
diff --git a/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp b/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
new file mode 100644
index 000000000..398071852
--- /dev/null
+++ b/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
@@ -0,0 +1,634 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include "Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+typedef __NATIVE_VECTOR__(2, float) v2f;
+template <int ThreadBlock, int BlockDimX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    if (splitKOffset + k_block > k) k = k - splitKOffset;
+    else k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements * 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr, *bsm_scales_ptr, *smem;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += BlockDimX) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            *((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS) + 1;
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS) + 1;
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[x*2] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[x*2+1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[x*2] = s1;
+            bsm_scales_ptr[x*2+1] = s2;
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        v2f local_scales_d16[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = bsm_scales_ptr[m_index + c];
+            float s_d16 = s / 16;
+            local_scales[c] = {s, s};
+            local_scales_d16[c] = {s_d16, s_d16};
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c],bsm_zeros_ptr[m_index + c]};
+
+        const int shuffled_dequant_index[PACK_RATIO] = {0, 4, 1, 5, 2, 6, 3, 7};
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            if constexpr (N <= 4) {
+                v2f local_b[PACK_RATIO/2*N];
+                for (int y = 0; y < N; y++) {
+                    for (int x = 0; x < PACK_RATIO / 2; x++) {
+                        local_b[x+PACK_RATIO/2*y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                        local_b[x+PACK_RATIO/2*y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                    }
+                }
+                #pragma unroll m_per_thread
+                for (int c = 0; c < m_per_thread; c++) {
+                    uint32_t p0 = A[c] & 0x0f0f0f0f;
+                    uint32_t p1 = A[c] & 0xf0f0f0f0;
+                    float o1,o2,o3,o4,o5,o6,o7,o8;
+                    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+                    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(p0));
+                    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+                    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(p0));
+                    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o5):"r"(p1));
+                    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o6):"r"(p1));
+                    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o7):"r"(p1));
+                    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o8):"r"(p1));
+                    v2f a0 = {o1, o3};
+                    v2f a1 = {o5, o7};
+                    v2f a2 = {o2, o4};
+                    v2f a3 = {o6, o8};
+
+                    a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+                    a1 = __builtin_mxc_pk_fma_f32(a1, local_scales_d16[c], local_zeros[c]);
+                    a2 = __builtin_mxc_pk_fma_f32(a2, local_scales[c], local_zeros[c]);
+                    a3 = __builtin_mxc_pk_fma_f32(a3, local_scales_d16[c], local_zeros[c]);
+
+                    #pragma unroll N
+                    for (int y = 0; y < N; y++) {
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[0+PACK_RATIO/2*y], c_splited[y][c]);
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[1+PACK_RATIO/2*y], c_splited[y][c]);
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a2, local_b[2+PACK_RATIO/2*y], c_splited[y][c]);
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a3, local_b[3+PACK_RATIO/2*y], c_splited[y][c]);
+                    }
+                }
+            } else {
+                v2f local_b;
+                for (int x = 0; x < PACK_RATIO / 2; x++) {
+                    for (int y = 0; y < N; y++) {
+                        local_b.x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                        local_b.y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                        for (int c = 0; c < m_per_thread; c++) {
+                            float a1 = (float)__builtin_mxc_ubfe(A[c], QBITS * shuffled_dequant_index[x*2], QBITS);
+                            float a2 = (float)__builtin_mxc_ubfe(A[c], QBITS * shuffled_dequant_index[x*2+1], QBITS);
+                            v2f a = {a1, a2};
+                            a = __builtin_mxc_pk_fma_f32(a, local_scales[c], local_zeros[c]);
+                            c_splited[y][c] = __builtin_mxc_pk_fma_f32(a, local_b, c_splited[y][c]);
+                        }
+                    }
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        for (int i = 0; i < m_per_thread; i++) {
+            smem[tidCol + (tidRow * m_per_thread + i) * ThreadBlock / BlockDimX] = c_splited[y][i].x + c_splited[y][i].y;
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < m_per_thread * BlockDimX; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
+
+
+template<int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq_tb256_bx256_kb128(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int ThreadBlock = 256;
+    constexpr int BlockDimX = 256;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    constexpr int k_block = QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr;
+    half *bsm_scales_ptr;
+
+    __shared__ float bsm_ptr[2048];  //128*N+256*4+256*4/2 = 最大8K，每个block 256线程，占据半个PEU，一个AP可以跑满8个block
+    bsm_b_ptr = bsm_ptr;
+    bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+    bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+    //smem = bsm_ptr;
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            *((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS) + 1;
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS) + 1;
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[x*2] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[x*2+1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[x*2] = temp_scales[0];
+            bsm_scales_ptr[x*2+1] = temp_scales[1];
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        v2f local_scales_d16[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = (float)bsm_scales_ptr[m_index + c];
+            float s_d16 = s / 16;
+            local_scales[c] = {s, s};
+            local_scales_d16[c] = {s_d16, s_d16};
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c],bsm_zeros_ptr[m_index + c]};
+
+        const int shuffled_dequant_index[PACK_RATIO] = {0, 4, 1, 5, 2, 6, 3, 7};
+
+        #pragma unroll 16
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            v2f local_b[PACK_RATIO/2*N];
+            for (int y = 0; y < N; y++) {
+                for (int x = 0; x < PACK_RATIO / 2; x++) {
+                    local_b[x+PACK_RATIO/2*y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                    local_b[x+PACK_RATIO/2*y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                }
+            }
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                uint32_t p0 = A[c] & 0x0f0f0f0f;
+                uint32_t p1 = A[c] & 0xf0f0f0f0;
+                float o1,o2,o3,o4,o5,o6,o7,o8;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(p0));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(p0));
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o5):"r"(p1));
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o6):"r"(p1));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o7):"r"(p1));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o8):"r"(p1));
+                v2f a0 = {o1, o3};
+                v2f a1 = {o5, o7};
+                v2f a2 = {o2, o4};
+                v2f a3 = {o6, o8};
+
+                a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+                a1 = __builtin_mxc_pk_fma_f32(a1, local_scales_d16[c], local_zeros[c]);
+                a2 = __builtin_mxc_pk_fma_f32(a2, local_scales[c], local_zeros[c]);
+                a3 = __builtin_mxc_pk_fma_f32(a3, local_scales_d16[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[0+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[1+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a2, local_b[2+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a3, local_b[3+PACK_RATIO/2*y], c_splited[y][c]);
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    // directly do atomic add, may cause partial write?
+    for (int y = 0; y < N; y++) {
+        for (int c = 0; c < m_per_thread; c++) {
+            atomicAdd(dst + tidRow * m_per_thread + c + y * dstStride, (half)(c_splited[y][c].x + c_splited[y][c].y));
+        }
+    }
+}
+
+
+template<>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq_tb256_bx256_kb128<4>(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int ThreadBlock = 256;
+    constexpr int BlockDimX = 256;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = 4;
+    const int k_stride = k;
+    constexpr int k_block = QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr;
+    half *bsm_scales_ptr;
+
+    __shared__ float bsm_ptr[2048];  //128*N+256*4+256*4/2 = 最大8K，每个block 256线程，占据半个PEU，一个AP可以跑满8个block
+    bsm_b_ptr = bsm_ptr;
+    bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+    bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+    //smem = bsm_ptr;
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            *((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS) + 1;
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS) + 1;
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[x*2] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[x*2+1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[x*2] = temp_scales[0];
+            bsm_scales_ptr[x*2+1] = temp_scales[1];
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = (float)bsm_scales_ptr[m_index + c];
+            local_scales[c] = {s,s};
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c], bsm_zeros_ptr[m_index + c]};
+
+        //const int shuffled_dequant_index[PACK_RATIO] = {0, 4, 1, 5, 2, 6, 3, 7};
+
+        #pragma unroll 16
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            //Split dequant and w*b into 4 parts so we can reduce registers usage from 114 to 76, and each peu will run 6 waves
+            v2f local_b[N];
+            uint32_t Aq[m_per_thread];
+            for (int y = 0; y < N; y++) {
+                int x = 0;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                Aq[c] = A[c] & 0x0f0f0f0f;
+                float o1,o3;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(Aq[c]));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(Aq[c]));
+                v2f a0 = {o1, o3};
+
+                a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            for (int y = 0; y < N; y++) {
+                int x = 2;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                float o2,o4;
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(Aq[c]));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(Aq[c]));
+                v2f a2 = {o2, o4};
+
+                a2 = __builtin_mxc_pk_fma_f32(a2, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a2, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            for (int y = 0; y < N; y++) {
+                int x = 1;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+
+            for (int c = 0; c < m_per_thread; c++) {
+                Aq[c] = (A[c] >> 4) & 0x0f0f0f0f;
+                float o5,o7;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o5):"r"(Aq[c]));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o7):"r"(Aq[c]));
+                v2f a1 = {o5, o7};
+                a1 = __builtin_mxc_pk_fma_f32(a1, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            for (int y = 0; y < N; y++) {
+                int x = 3;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+
+            for (int c = 0; c < m_per_thread; c++) {
+                float o6,o8;
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o6):"r"(Aq[c]));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o8):"r"(Aq[c]));
+                v2f a3 = {o6, o8};
+                a3 = __builtin_mxc_pk_fma_f32(a3, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a3, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    // directly do atomic add, may cause partial write?
+    for (int y = 0; y < N; y++) {
+        for (int c = 0; c < m_per_thread; c++) {
+            atomicAdd(dst + tidRow * m_per_thread + c + y * dstStride, (half)(c_splited[y][c].x + c_splited[y][c].y));
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp b/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
new file mode 100644
index 000000000..8a04b9035
--- /dev/null
+++ b/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
@@ -0,0 +1,216 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include "Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+typedef __NATIVE_VECTOR__(2, float) v2f;
+template <int ThreadBlock, int BlockDimX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq_int8(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 8;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    if (splitKOffset + k_block > k) k = k - splitKOffset;
+    else k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements * 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr, *bsm_scales_ptr, *smem;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+    
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 1;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales;
+            temp_scales = (scales + quant_group * scales_stride + m_offset_scales)[x];
+            uint32_t z = temp_zeros;
+            float z0;
+            asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(z0):"r"(z));
+            float s1 = (float)(temp_scales);
+            //Store to shared memory
+            //bsm_zeros_ptr[x] = (float)z0 * s1 * -1.0f;    //modify 11.19
+	    bsm_zeros_ptr[x] = (float)(z0+1) * s1 * -1.0f;
+            bsm_scales_ptr[x] = s1;
+            // if (i == 0 && blockIdx.x == 0) {
+            //     printf("tid %d, x = %d, temp_zero=%u, temp_scale=%f,  z0 = %f, s1 = %f\n", tid, x, (uint32_t)temp_zeros, (float)temp_scales, z0, s1);
+            // }
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = bsm_scales_ptr[m_index + c];
+            local_scales[c] = {s, s};
+            // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+            //     printf("get scale %f from index %d\n", s, m_index+c);
+            // }
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            local_zeros[c] = {bsm_zeros_ptr[m_index + c],bsm_zeros_ptr[m_index + c]};
+            // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+            //     printf("get zero %f from index %d\n", local_zeros[c].x, m_index+c);
+            // }
+        }
+
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            v2f local_b[PACK_RATIO/2*N];
+            for (int y = 0; y < N; y++) {
+                for (int x = 0; x < PACK_RATIO / 2; x++) {
+                    local_b[x+PACK_RATIO/2*y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                    local_b[x+PACK_RATIO/2*y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                }
+            }
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                float o1,o2,o3,o4;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(A[c]));
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(A[c]));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(A[c]));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(A[c]));
+                v2f a0 = {o1, o2};
+                v2f a1 = {o3, o4};
+
+                // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+                //     printf("GPU int8 a0=%f,%f, a1=%f,%f,scale=%f,zero=%f\n",
+                //     a0.x, a0.y, a1.x, a1.y, local_scales[c].x, local_zeros[c].x);
+                // }
+
+                a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+                a1 = __builtin_mxc_pk_fma_f32(a1, local_scales[c], local_zeros[c]);
+
+                // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+                //     printf("GPU a=%x, a0=%f,%f,a1=%f,%f,b=%f,%f,%f,%f\n",
+                //         A[c], a0.x, a0.y, a1.x, a1.y, local_b[0].x, local_b[0].y, local_b[1].x, local_b[1].y
+                //     );
+                // }
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[0+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[1+PACK_RATIO/2*y], c_splited[y][c]);
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        for (int i = 0; i < m_per_thread; i++) {
+            smem[tidCol + (tidRow * m_per_thread + i) * ThreadBlock / BlockDimX] = c_splited[y][i].x + c_splited[y][i].y;
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < m_per_thread * BlockDimX; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/hgemv_selector.hpp b/csrc/quantization/gptq/hgemv_selector.hpp
new file mode 100644
index 000000000..2a7fef782
--- /dev/null
+++ b/csrc/quantization/gptq/hgemv_selector.hpp
@@ -0,0 +1,288 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#include <memory>
+#include <vector>
+#include <algorithm>
+#include "mc_runtime.h"
+#include "maca_fp16.h"
+
+struct KernelEventRecorder {
+    mcEvent_t _start;
+    mcEvent_t _stop;
+    float _eventMs = -1.f;
+
+    KernelEventRecorder() {
+        mcEventCreate(&_start);
+        mcEventCreate(&_stop);
+    }
+
+    ~KernelEventRecorder() {
+        mcEventDestroy(_start);
+        mcEventDestroy(_stop);
+    }
+
+    void start() {
+        mcEventRecord(_start, NULL);
+    }
+
+    float stop() {
+        mcEventRecord(_stop, NULL);
+        mcEventSynchronize(_stop);
+        mcEventElapsedTime(&_eventMs, _start, _stop);
+        return _eventMs;
+    }
+};
+namespace hgemv_selector {
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+struct GemvParamAutoSelector {
+    int m;
+    int k;
+    int best_block_x = 0;
+    int best_split_k = 0;
+
+    std::pair<int,int> block_x_range;
+    std::pair<int,int> split_k_range;
+    bool _valid = false;
+
+private:
+    std::vector<std::pair<int, int>> param_candidates;
+    int warmup_iters = 0;
+    int current_block_x = 0;
+    int current_split_k = 0;
+    int current_perf_iter = 0;
+    std::vector<float> perf_times;
+    float kernel_best_time_ms_ave = 99999999.0f;
+    float best_band_width;
+    float data_size_gb;
+    std::shared_ptr<KernelEventRecorder> _r;
+    bool _selected = false;
+    const static int MAX_PERF_COUNT = 20;
+
+public:
+    GemvParamAutoSelector(int _m, int _n, int _k, bool allow_imcomplete_bx = false) : m(_m), k(_k)
+    {
+        if (!prepare_parameters(_m, _n, _k, block_x_range.second, split_k_range.second, allow_imcomplete_bx)) return;
+        block_x_range.first = block_x_range.second;
+        split_k_range.first = split_k_range.second;
+        for (int i = 0; i < 3; i++) if (block_x_range.first / 2 >= 16) block_x_range.first /= 2;
+        for (int i = 0; i < 3; i++) if (split_k_range.first % 2 == 0 && split_k_range.first / 2 > 0) split_k_range.first /= 2;
+        for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+            for (int j = split_k_range.first; j <= split_k_range.second; j *= 2) {
+                param_candidates.emplace_back(i, j);
+            }
+        }
+        if (split_k_range.second * quant_group != k) {
+            int max_split_k = k / quant_group;
+            if (max_split_k < 256) {
+                for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+                    param_candidates.emplace_back(i, max_split_k);
+                }
+            }
+        }
+
+        current_block_x = block_x_range.second;
+        current_split_k = split_k_range.second;
+        uint64_t data_size_in_bytes = _n * _k * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_n * _m * sizeof(half);
+        data_size_gb = float(data_size_in_bytes) / 1024 / 1024 / 1024;
+        warmup_iters = 4;
+        _valid = true;
+    }
+
+    void select_in_warmup(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (_selected) {
+            f(best_block_x, best_split_k);
+            return;
+        };
+        //Warmup
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            for (int i = 0; i < 5; i++) {
+                auto &p = *iter;
+                f(p.first, p.second);
+            }
+        }
+        _r.reset(new KernelEventRecorder());
+        kernel_best_time_ms_ave = 9999999.0f;
+        mcDeviceSynchronize();
+
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            auto &p = *iter;
+            auto &bx = p.first;
+            auto &sk = p.second;
+            mcDeviceSynchronize();
+            _r->start();
+            bool launched = false;
+            for (int i = 0; i < MAX_PERF_COUNT; i++) {
+                launched = f(bx, sk);
+            }
+            auto ms = _r->stop();
+            if (launched && ms / MAX_PERF_COUNT < kernel_best_time_ms_ave) {
+                best_block_x = bx;
+                best_split_k = sk;
+                kernel_best_time_ms_ave = ms / MAX_PERF_COUNT;
+            }
+        }
+
+        _r.reset();
+        _selected = true;
+        warmup_iters = 0;
+    }
+
+    void run(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (warmup_iters > 0) {
+            f(current_block_x, current_split_k);
+            //printf("Calling warmup current_block_x = %d, current_split_k = %d\n", current_block_x, current_split_k);
+            if (current_block_x > block_x_range.first) current_block_x /= 2;
+            else if (current_split_k > split_k_range.first) current_split_k /= 2;
+            if (current_block_x == block_x_range.first && current_split_k >= split_k_range.first) {
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+            }
+            warmup_iters--;
+            mcDeviceSynchronize();
+            return;
+        }
+
+        if (_selected) {
+            f(best_block_x, best_split_k);
+        } else {
+            if (!_r) {
+                _r = std::shared_ptr<KernelEventRecorder>(new KernelEventRecorder());
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+                current_perf_iter = MAX_PERF_COUNT;
+            }
+            _r->start();
+            auto launched = f(current_block_x, current_split_k);
+            auto ms = _r->stop();
+            //printf("Launched with bx=%d,sk=%d,iter=%d,ms=%fms\n", current_block_x, current_split_k, current_perf_iter, ms);
+            if (!launched) {
+                printf("ERROR: Cannot run gemv with bx = %d, sk = %d\n", current_block_x, current_split_k);
+                return;
+            }
+            if (current_perf_iter-- > 0) {
+                perf_times.emplace_back(ms);
+                return;
+            }
+
+            std::sort(perf_times.begin(), perf_times.end());
+            float total_tm = 0;
+            for (size_t i = 1; i < perf_times.size() - 1; i++) {
+                total_tm += perf_times[i];
+            }
+
+            ms = total_tm /= (MAX_PERF_COUNT - 2);
+            perf_times.clear();
+            current_perf_iter = MAX_PERF_COUNT;
+            //printf("get ave time %fms\n", ms);
+
+            if (ms < kernel_best_time_ms_ave) {
+                best_block_x = current_block_x;
+                best_split_k = current_split_k;
+                kernel_best_time_ms_ave = ms;
+            }
+
+            if (current_split_k > split_k_range.first) {
+                current_split_k /= 2;
+            } else if (current_block_x > block_x_range.first){
+                current_split_k = split_k_range.second;
+                current_block_x /= 2;
+            } else if (current_block_x == block_x_range.first && current_split_k == split_k_range.first) {
+                _selected = true;
+                _r.reset();
+            } else {
+                printf("Error: all parameters are tried! current_block_x = %d, current_split_k = %d, block_x_range = %d,%d, split_k_range = %d,%d, best_block_x = %d, best_split_k = %d\n",
+                    current_block_x, current_split_k,
+                    block_x_range.first, block_x_range.second,
+                    split_k_range.first, split_k_range.second,
+                    best_block_x, best_split_k
+                );
+            }
+        }
+    }
+
+    bool valid() const { return _valid; }
+    bool selected() const {return _selected; }
+
+    float gemv_ave_time_us_cost() {
+        return kernel_best_time_ms_ave;
+    }
+
+    float gemv_bandwidth() {
+        return data_size_gb / (kernel_best_time_ms_ave / 1000.0f);
+    }
+
+    static bool prepare_parameters(int m, int n, int k, int &bx, int &sk, bool allow_imcomplete_bx = false) {
+        if (n > 4) return false;
+        if (k % quant_group != 0) return false;
+        if (m < 16 * m_per_thread) return false;
+        int max_split_k = k / quant_group;
+        int proper_splitk = 1;
+        while (proper_splitk*2 <= 256 && proper_splitk*2 <= max_split_k) proper_splitk *= 2;
+
+        int thread_block = 256;  //A thread block of 512 will wait for more warps when doing syncthreads
+
+        int proper_bx = 16;
+        if (m % (proper_bx * m_per_thread) != 0) return false;
+        //proper_bx * m_per_threads corresponds to group_elements, this value should not be larger than 2048 as limit of shared memory
+        while (proper_bx * 2 * m_per_thread && (m % (proper_bx*2*m_per_thread) == 0) && proper_bx * 2 <= 256 && proper_bx * 2 * m_per_thread <= 2048) proper_bx*=2;
+        if (allow_imcomplete_bx) {
+            int may_proper_bx = proper_bx * 2;
+            if (may_proper_bx <= 256 && may_proper_bx * m_per_thread <= 2048 && (m % (may_proper_bx*m_per_thread)) > may_proper_bx * m_per_thread / 4) {
+                proper_bx = may_proper_bx;
+            }
+        }
+
+        bx = proper_bx;
+        sk = proper_splitk;
+        //printf("m=%d,n=%d,k=%d,get bx=%d,sk=%d,allow_imcomplete_bx=%d\n", m, n, k, bx, sk, allow_imcomplete_bx);
+        return true;
+    }
+};
+
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+class GemvSelectorHolder {
+private:
+    std::vector<GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>> _selectors;
+    static std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> _holder;
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> _invalid_selector;
+
+public:
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& selector(int m, int n, int k, bool allow_imcomplete_bx = false) {
+        if (!GemvSelectorHolder::_holder) {
+            GemvSelectorHolder::_holder.reset(new GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>());
+        }
+        int bx, sk;
+        if (!GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>::prepare_parameters(m, n, k, bx, sk, allow_imcomplete_bx)) {
+            return _invalid_selector;
+        }
+        auto iter = std::find_if(_holder->_selectors.begin(), _holder->_selectors.end(),
+            [&](const GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& p) -> bool {
+                return p.m == m && p.k == k;
+            });
+        if (iter != _holder->_selectors.end()) return *iter;
+        GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> sl(m, n, k, allow_imcomplete_bx);
+        if (!sl.valid()) {
+            return _invalid_selector;
+        }
+        _holder->_selectors.emplace_back(sl);
+        return _holder->_selectors.back();
+    }
+};
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_holder;
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_invalid_selector(0,8,0);
+}
\ No newline at end of file
diff --git a/csrc/quantization/gptq/q_gemm.cu b/csrc/quantization/gptq/q_gemm.cu
index 785f1a09c..5f9a2c6d2 100644
--- a/csrc/quantization/gptq/q_gemm.cu
+++ b/csrc/quantization/gptq/q_gemm.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
 Adapted from https://github.com/turboderp/exllamav2 and
 https://github.com/qwopqwop200/GPTQ-for-LLaMa
@@ -19,6 +20,15 @@ https://github.com/qwopqwop200/GPTQ-for-LLaMa
 #include "qdq_4.cuh"
 #include "qdq_8.cuh"
 
+#include "hgemm_gptq.h"
+#include "scalar_type.hpp"
+
+#include "hgemv_nn_splitk_gptq.hpp"
+#include "hgemv_nn_splitk_gptq_int8.hpp"
+#include "hgemv_selector.hpp"
+#include "Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp"
+#include "Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp"
+
 namespace vllm {
 namespace gptq {
 
@@ -31,6 +41,8 @@ namespace gptq {
 #define THREADS_X 32
 #define THREADS_Y 32
 #define DIVIDE(x, size) (((x) + (size) - 1) / (size))
+#define QUANT_GROUP 128
+#define BF16_HIGH_PRECISION
 
 #if defined(USE_ROCM)
   #include <hipblas/hipblas.h>
@@ -734,26 +746,309 @@ fp_gemm_half_q_half_gptq_kernel pick_gemm_half_q_half_gptq_kernel(
   return NULL;
 }
 
+template <typename T>
+__global__ void blasMemset(T *data, size_t cnt, T init) {
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (cnt + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
+        data[loop * threads + tid] = init;
+    }
+}
+
+template <typename dstT, typename srcT, typename scalarT>
+__global__ void blasMemcpy(dstT *dst, const srcT *src, size_t cnt, scalarT beta) {
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (cnt + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
+        dst[loop * threads + tid] =
+            static_cast<double>(beta) * static_cast<double>(src[loop * threads + tid]);
+    }
+}
+
+template <typename reducT, typename outputT, typename scalarT>
+__global__ void blasReduc(outputT *dC_out, outputT *dC_in, reducT *d_acc, int count, int segs, scalarT beta)
+{
+    using accT = float;
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (count + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && (loop * threads + tid) < count; ++loop) {
+        accT acc = static_cast<accT>(beta) * static_cast<accT>(dC_in[loop * threads + tid]);
+        for (size_t SEG=0; SEG < segs; ++SEG)
+        {
+            acc += static_cast<accT>(d_acc[SEG * count + loop * threads + tid]);
+        }
+        dC_out[loop * threads + tid] = static_cast<outputT>(acc);
+    }
+}
+
+template<typename T_ACC, typename T_ACC_PACK, typename T, typename T_PACK>
+__global__ void split_reduce(const T_ACC* src, const int row, const int splitk, T* dest) {
+    constexpr int ELEMS = sizeof(T_ACC_PACK)/sizeof(T_ACC);
+    for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < row*sizeof(T_ACC)/sizeof(T_ACC_PACK); i += blockDim.x*gridDim.x) {
+        T_ACC_PACK p0 = ((T_ACC_PACK*)src)[i];
+        T_ACC p0_a[ELEMS];
+        for (int j = 0; j < ELEMS; j++) p0_a[j] = ((T_ACC*)&p0)[j];
+        for (int k = 1; k < splitk; k++) {
+            p0 = ((T_ACC_PACK*)src)[i + row / ELEMS * k];
+            for (int j = 0; j < ELEMS; j++) {
+                p0_a[j] += ((T_ACC*)&p0)[j];
+            }
+        }
+        T dest_pack[ELEMS];
+        for (int j = 0; j < ELEMS; j++) dest_pack[j] = p0_a[j];
+        ((T_PACK*)dest)[i] = *(T_PACK*)dest_pack;
+    }
+}
+
+template <int tileK, int tileN>
+__global__ void perm_b(half *output, const half *input, const int *idx, int k, int n, int ldb) {
+    int tid = threadIdx.x;
+    int row = blockIdx.x * tileK + tid;
+    if (row < k) {
+        int index = idx[row];
+        int col_offset = blockIdx.y * tileN;
+#pragma unroll 1
+        for (int i = 0; (i < tileN) && ((col_offset + i) < n); ++i) {
+            int col = col_offset + i;
+            output[row + ldb * col] = input[index + ldb * col];
+        }
+    }
+}
+
+#define SWITCH_CASE_BATCH(BlockDimX, SplitK, BATCH) \
+    case BATCH: {                                   \
+        CALL_GEMM(BlockDimX, SplitK, BATCH)         \
+        break;                                      \
+    }
+
+#define APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH) \
+    switch(BATCH) {                                 \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 1)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 2)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 3)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 4)           \
+        default: {                                          \
+            launched = false;                               \
+            printf("ERROR: Unsupported BATCH %d\n", BATCH); \
+            break;                                          \
+        }                                                   \
+    }
+
+#define SWITCH_CASE_BlockDimX(BlockDimX, SplitK, BATCH) \
+    case BlockDimX: {                                   \
+        APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH)    \
+        break;                                          \
+    }
+
+#define APPLY_HGEMM(BlockDimX, SplitK, BATCH)           \
+    switch (BlockDimX) {                                \
+        SWITCH_CASE_BlockDimX(16, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(32, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(64, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(128, SplitK, BATCH)       \
+        SWITCH_CASE_BlockDimX(256, SplitK, BATCH)       \
+        default: {                                                  \
+            launched = false;                                       \
+            printf("ERROR: Unsupported BlockDimX %d\n", BlockDimX); \
+            break;                                                  \
+        }                                                           \
+    }
+
+bool call_kernel(const half *srcB,
+    const quant_packed_type *srcA,
+    quant_packed_type *zeros, half *scales,
+    half* dst_D,
+    int m, int n, int k, int srcStride, int dstStride,
+    int block_x, int split_k, int bit,
+    const int* b_perm_D = nullptr) {
+    constexpr int ThreadBlock = 256;
+    const dim3 threadBlock = {static_cast<unsigned int>(ThreadBlock)};
+    const dim3 gridBlock = {static_cast<unsigned int>(m / (block_x * sizeof(float4) / sizeof(quant_packed_type))), static_cast<unsigned int>(split_k)};
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    if (split_k * QUANT_GROUP > k || k % QUANT_GROUP != 0) return false;
+    if (block_x < 16 || n > 4) return false;
+    bool launched = true;
+    #define CALL_GEMM(BX, SK, N) \
+    if (bit == 4) { \
+        if (QUANT_GROUP*SK == k && BX == 256) { \
+            hgemv_nn_splitk_gptq_tb256_bx256_kb128<N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+        } else {    \
+            hgemv_nn_splitk_gptq<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D);  \
+        } \
+    } \
+    if (bit == 8) { \
+        hgemv_nn_splitk_gptq_int8<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+            srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+    }
+    APPLY_HGEMM(block_x, split_k, n);
+    return launched;
+}
+
 void gemm_half_q_half_cuda_part(const half* a, const uint32_t* b_q_weight,
                                 const uint32_t* b_gptq_qzeros,
                                 const half* b_gptq_scales, const int* b_q_perm,
                                 half* c, int size_m, int size_n, int size_k,
-                                int m_count, int groups, int bit) {
-  dim3 blockDim, gridDim;
-  blockDim.x = BLOCK_KN_SIZE;
-  blockDim.y = 1;
-  blockDim.z = 1;
-  gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
-  gridDim.y = DIVIDE(size_m, m_count);
-  gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
-
-  fp_gemm_half_q_half_gptq_kernel kernel =
-      pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
-
-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-  kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
-                                           b_gptq_scales, c, size_m, size_n,
-                                           size_k, groups, b_q_perm);
+                                int m_count, int groups, int bit, bool m_sign, bool v_sign) {
+  if ((bit == 4 || bit == 8) && m_sign && !v_sign){
+        const int threads_n = 256;
+        const int tileM = 128;
+        const int tileN = 32;
+        const int tileK = 128;
+        int lda = size_n;
+        int ldb = size_k;
+        int ldc = size_n;
+
+        int splitk_iters = 3;
+        bool isSplitk = splitk_iters > 1;
+
+        uint32_t gridx = (size_n - 1) / tileM + 1;
+        uint32_t gridy = (size_m - 1) / tileN + 1;
+        uint32_t gridz = splitk_iters;
+
+        uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+        half* scales = const_cast<half*>(b_gptq_scales);
+
+        dim3 dimBlock(threads_n, 1, 1);
+        dim3 dimGrid(gridx, gridy, gridz);
+        float alpha = 1.0, beta = 0.0;
+        const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+        bool isBetaZero = (beta == 0.0);
+        Operation_t trans_a = Operation_t(0);
+        Operation_t trans_b = Operation_t(0);
+
+        if (trans_a == OP_N && trans_b == OP_N && size_n % 8 == 0 && size_k % 8 == 0) {
+             half *dB_perm;
+             if (b_q_perm != nullptr) {
+                 mcMallocAsync((void **)&dB_perm, ldb * size_m * sizeof(input_type), stream);
+                 const int threads_n1 = 128;
+                 const int tileK1 = 128;
+                 const int tileN1 = 8;
+                 uint32_t gridx1 = (size_k - 1) / tileK1 + 1;
+                 uint32_t gridy1 = (size_m - 1) / tileN1 + 1;
+                 dim3 dimBlock1(threads_n1, 1, 1);
+                 dim3 dimGrid1(gridx1, gridy1, 1);
+                 perm_b<tileK1, tileN1><<<dimGrid1, dimBlock1, 0, stream>>>(dB_perm, a, b_q_perm, size_k, size_m, ldb);
+             }
+             const half *dB_actual = (b_q_perm != nullptr ? dB_perm : a);
+	     if (bit == 4) {
+                 if (!isSplitk) {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales);
+                     } else {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, false, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales);
+                     }
+                 } else {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, c);
+                     }
+                     else {
+                         acc_type *d_acc;
+                         mcMalloc(reinterpret_cast<void **>(&d_acc), size_n * size_m * sizeof(acc_type));
+                         blasMemcpy<<<104, 512, 0, stream>>>(d_acc, c, size_n * size_m, beta);
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, false, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, d_acc);
+                         blasMemcpy<<<104, 512, 0, stream>>>(c, d_acc, size_n * size_m, 1);
+                         mcFree(d_acc);
+                     }
+                 }
+             }
+	     else if (bit == 8){
+                 if (!isSplitk) {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, true, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales, 1, nullptr, nullptr);
+                     } else {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, false, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales);
+                     }
+                 } else {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, true, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, c);
+                     } else {
+                         acc_type *d_acc;
+                         mcMallocAsync(reinterpret_cast<void **>(&d_acc), size_n * size_m * sizeof(acc_type), stream);
+                         blasMemcpy<<<104, 512, 0, stream>>>(d_acc, c, size_n * size_m, beta);
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, false, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, d_acc);
+                         blasMemcpy<<<104, 512, 0, stream>>>(c, d_acc, size_n * size_m, 1);
+                         mcFreeAsync(d_acc, stream);
+                     }
+                 }
+             }
+             if (b_q_perm != nullptr) {
+                 mcFreeAsync(dB_perm, stream);
+             }
+        } else {
+            printf("Parameters not supported!\n");
+            return;
+        }
+  }
+  else if((bit == 4 || bit == 8) && v_sign){
+         constexpr int m_per_thread = 4;
+         uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+         half* scales = const_cast<half*>(b_gptq_scales);
+         auto kernel_testing = [&](int bx, int sk) -> bool {
+             return call_kernel(a, b_q_weight, zeros, scales, c, size_n, size_m, size_k, size_n, size_n, bx, sk, bit, b_q_perm);
+         };
+         if (bit == 4){
+             auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,8,m_per_thread>::selector(size_n, size_m, size_k);
+             if (sl_warmup.valid()) {
+                 if (!sl_warmup.selected()) {
+                     sl_warmup.select_in_warmup(kernel_testing);
+                     mcMemset(c, 0, size_n * size_m * sizeof(half));
+                     sl_warmup.run(kernel_testing);
+                 } else {
+                     sl_warmup.run(kernel_testing);
+                 }
+             }
+         }
+         else {
+             auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,4,m_per_thread>::selector(size_n, size_m, size_k);
+             if (sl_warmup.valid()) {
+                 if (!sl_warmup.selected()) {
+                     sl_warmup.select_in_warmup(kernel_testing);
+                     mcMemset(c, 0, size_n * size_m * sizeof(half));
+                     sl_warmup.run(kernel_testing);
+                 } else {
+                     sl_warmup.run(kernel_testing);
+                 }
+             }
+         }
+  }
+  else {
+	 dim3 blockDim, gridDim;
+	 blockDim.x = BLOCK_KN_SIZE;
+	 blockDim.y = 1;
+	 blockDim.z = 1;
+	 gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
+	 gridDim.y = DIVIDE(size_m, m_count);
+	 gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
+
+	 fp_gemm_half_q_half_gptq_kernel kernel =
+	     pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
+
+	 const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+	 kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
+						  b_gptq_scales, c, size_m, size_n,
+						  size_k, groups, b_q_perm);
+  }
 }
 
 __global__ void reconstruct_exllama_8bit_kernel(
@@ -1487,56 +1782,329 @@ void reconstruct_gptq(const uint32_t* b_q_weight, const uint32_t* b_gptq_qzeros,
                                            width, groups, out);
 }
 
+template <int tileK, int tileM, typename dtype>
+__global__ void perm_a(dtype *output, const dtype *input, const int *idx, int k, int m, int lda) {
+    int tid = threadIdx.x;
+    int row = blockIdx.x * tileK + tid;
+    int col_st = blockIdx.y * tileM;
+    if (row < k) {
+        int index = idx[row];
+        #pragma unroll tileM
+        for (int i = 0; i < tileM; ++i) {
+            int col = col_st + i;
+            if (col < m) {
+                output[row + lda * col] = input[index + lda * col];
+            }
+        }
+    }
+}
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm_gptq(int m,
+                      int n,
+                      int k,
+                      int quant_group,
+                      const input_tp *dA,
+                      int lda,
+                      const quant_packed_tp *dB,
+                      int ldb,
+                      output_tp *dC,
+		      float *dC_temp,
+                      int ldc,
+                      quant_packed_tp *d_zeros,
+                      input_tp *d_scales,
+                      const cudaStream_t stream,
+                      int chunks = 1) {
+    using namespace hgemm_marlin_gptq;
+    if(n % 16 != 0) {
+        printf("n %% 16 != 0, n = %d\n", n);
+        return false;
+    }
+    if(k % 32 != 0) {
+        printf("k %% 32 != 0, k = %d\n", k);
+        return false;
+    }
+    //const vllm::ScalarTypeId w_type_id = vllm::kU4B8.id();
+    const int THREADS = 256;
+    int BLOCKS_M = div_ceil(m, SLICE_M);
+    if(BLOCKS_M >= MAX_BLOCKS_M && BLOCKS_M % MAX_BLOCKS_M != 0) {
+        printf("Error: input m is error, m = %d, blocks_m = %d\n", m, BLOCKS_M);
+        return false;
+    }
+    if (BLOCKS_M > MAX_BLOCKS_M) BLOCKS_M = MAX_BLOCKS_M;
+    int BLOCKS_N = 8;
+    //int BLOCKS_K = 4;
+    //It is better let TILE_K = quant_group
+    //But if quant_group is too large, a quant_group can be divided into two parts
+    int BLOCKS_K = quant_group / SLICE_K;
+    if (quant_group > 128) BLOCKS_K = 128 / SLICE_K;
+
+    if (BLOCKS_M == 1 || BLOCKS_M == 2) {
+        BLOCKS_N = 16;
+    }
+    const bool HAS_ACT_ORDER = false;
+    //const bool HAS_ZP = false;
+    const bool HAS_ZP = (w_type_id == vllm::kU4.id()) || (w_type_id == vllm::kU8.id());
+    int *g_idx = nullptr;
+    bool HAS_NK_PRED = true;
+    bool HAS_M_PRED = true;
+    //int TILE_N = BLOCKS_N * SLICE_N;
+    //int TILE_K = BLOCKS_K * SLICE_K;
+    //int TILE_M = BLOCKS_M * SLICE_M;
+    if (n % TILE_N == 0 && k % TILE_K == 0) {
+        HAS_NK_PRED = false;
+    }
+    if (m % TILE_M == 0) {
+        HAS_M_PRED = false;
+    }
+
+#define LAUNCH_GPTQ(threads, bm, bn, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    else if (THREADS == threads && BLOCKS_M == bm && BLOCKS_N == bn \
+        && BLOCKS_K == bk  && HAS_ACT_ORDER == has_act_order \
+        && HAS_ZP == has_zp \
+        && HAS_M_PRED == has_m_pred && HAS_NK_PRED == has_nk_pred) { \
+            launch_gemm_gptq_kernel<input_tp, w_type_id, \
+                    threads, bm, bn, bk, has_act_order, has_zp, has_m_pred, has_nk_pred>( \
+                    (const PackTypeInt4*)dA, \
+                    (const PackTypeInt4*)dB, \
+                    (PackTypeInt4*)dC, \
+                    (PackTypeInt4*)dC_temp, \
+                    (const PackTypeInt4*)d_scales, \
+                    (const PackTypeInt4*)d_zeros, \
+                    nullptr, m, n, k, quant_group, chunks,\
+                    stream); \
+    }
+
+#define LAUNCH_GPTQ_K(bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 1, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 2, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 3, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 4, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred)
+
+#define LAUNCH_GPTQ_ZP(has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(1, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(2, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(4, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(8, false, has_zp, has_nk_pred, has_m_pred)
+
+#define LAUNCH_GPTQ_PRED(has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_ZP(false, has_nk_pred, has_m_pred) 
+    //LAUNCH_GPTQ_ZP(true, has_nk_pred, has_m_pred)
+
+    if (false) {
+
+    }
+    LAUNCH_GPTQ_PRED(true, true)
+    LAUNCH_GPTQ_PRED(true, false)
+    LAUNCH_GPTQ_PRED(false, true)
+    LAUNCH_GPTQ_PRED(false, false)
+    else {
+        printf("BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_k=%d, THREADS=%d, HAS_ACT_ORDER=%d, HAS_ZP=%d, quant_group=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d is not supported\n",
+        BLOCKS_M, BLOCKS_N, BLOCKS_K, THREADS, HAS_ACT_ORDER, HAS_ZP, quant_group, HAS_M_PRED, HAS_NK_PRED);
+        return false;
+    }
+
+    return true;
+}
+
+#ifdef BF16_HIGH_PRECISION
+__global__ void vectorized_elementwise_fp32tobf16(float* input, __maca_bfloat16* output, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        // printf("tid = %d, input = %f, output = %f\n", tid, input[tid], (float)(__maca_bfloat16)input[tid]);
+        *(__maca_bfloat16*)(output+tid) = (__maca_bfloat16)input[tid];
+    }
+}
+#else
+__global__ void vectorized_elementwise_fp16tobf16(__maca_bfloat16* input, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        input[tid] = (float)(*(half*)(input+tid));
+    }
+}
+#endif
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm(int m,
+                int n,
+                int k,
+                int quant_group,
+                const input_tp *dA,
+                int lda,
+                const quant_packed_tp *dB,
+                int ldb,
+                output_tp *dC,
+		float *dC_temp,
+                int ldc,
+                quant_packed_tp *d_zeros,
+                input_tp *d_scales,
+                const int* g_idx,
+                input_tp *perm_space,
+                bool is_gptq = true) {
+    using namespace hgemm_marlin_gptq;
+    //constexpr int max_blocks_m = 4;
+    int total_m_blocks = div_ceil(m, SLICE_M);
+    int chunks = total_m_blocks / MAX_BLOCKS_M;
+    int rest_blocks_m = total_m_blocks % MAX_BLOCKS_M;
+    // printf("m=%d,n=%d,k=%d,lda=%d,ldb=%d,ldc=%d,total_m_blocks=%d,chunks=%d,rest_blocks_m=%d\n",
+    //     m, n, k, lda, ldb, ldc, total_m_blocks, chunks, rest_blocks_m
+    // );
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    //input_tp *dA_perm;
+    if (g_idx != nullptr) {
+        //mcMalloc(reinterpret_cast<void **>(&dA_perm), k * m * sizeof(input_tp));
+        //mcMallocAsync(reinterpret_cast<void **>(&dA_perm), k * m * sizeof(input_tp), stream);
+        const int threads = 256;
+        const int tileK1 = 256;
+        const int tileM1 = 16;
+        uint32_t gridx1 = (k + tileK1 - 1) / tileK1;
+        uint32_t gridy1 = (m + tileM1 - 1) / tileM1;
+        dim3 dimBlock1(threads, 1, 1);
+        dim3 dimGrid1(gridx1, gridy1, 1);
+        perm_a<tileK1, tileM1, input_tp><<<dimGrid1, dimBlock1, 0, stream>>>(perm_space, dA, g_idx, k, m, k);
+    }
+    const input_tp *dA_actual = (g_idx != nullptr ? perm_space : dA);
+    bool ret = true;
+    if (chunks > 0) {
+        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
+        if (is_gptq) {
+            ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(real_m, n, k, quant_group, dA_actual, lda, dB, ldb, dC, dC_temp, ldc, d_zeros, d_scales, stream, chunks);
+        }
+    }
+    if (rest_blocks_m > 0) {
+        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
+        if (is_gptq) {
+            ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(m - m_offset, n, k, quant_group, dA_actual + lda * m_offset, lda, dB, ldb, dC + ldc * m_offset, dC_temp + ldc * m_offset, ldc, d_zeros, d_scales, stream, 1);
+        }
+    }
+
+//#if 0
+    if constexpr(std::is_same_v<input_tp, __maca_bfloat16>) {
+        uint64_t size = m * n;
+        uint64_t block = 512;
+        uint64_t grid = div_ceil(size, block);
+	    vectorized_elementwise_fp32tobf16<<<grid, block, 0, stream>>>((float*)dC_temp, (input_tp*)dC, size);
+    }
+#if 0
+    #ifdef BF16_HIGH_PRECISION
+	  vectorized_elementwise_fp32tobf16<<<grid, block, 0, stream>>>((float*)dC_temp, (input_tp*)dC, size);
+    #else
+          vectorized_elementwise_fp16tobf16<<<grid, block, 0, stream>>>((input_tp*)dC, size);
+    #endif
+#endif
+
+
+    return ret;
+}
+
+void gemm_bf16_q_bf16_cuda(const __maca_bfloat16* a,
+		           const uint32_t* b_q_weight,
+			   const uint32_t* b_gptq_qzeros,
+			   const __maca_bfloat16* b_gptq_scales, const int* b_g_idx,
+			   __maca_bfloat16* c, float* temp_space, int size_m, int size_n, int size_k,
+			   int bit, int group_size, __maca_bfloat16* perm_space) {
+  bool opt = ((group_size == 128) || (group_size == 64));
+  using scalar_t = __maca_bfloat16;
+  if ((bit == 4) && opt) {
+	  uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+	  scalar_t* scales = const_cast<scalar_t*>(b_gptq_scales);
+	  launch_gemm<scalar_t, vllm::kU4B8.id(), scalar_t, quant_packed_type>(size_m, size_n, size_k,
+			  group_size, a, size_k, b_q_weight, size_n, c, temp_space, size_n, zeros, scales,
+			  b_g_idx, perm_space, true);
+  } else if ((bit == 8) && opt) {
+          uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+          scalar_t* scales = const_cast<scalar_t*>(b_gptq_scales);
+          launch_gemm<scalar_t, vllm::kU8B128.id(), scalar_t, quant_packed_type>(size_m, size_n, size_k,
+                          group_size, a, size_k, b_q_weight, size_n, c, temp_space, size_n, zeros, scales,
+                          b_g_idx, perm_space, true);
+  } else {
+	  printf("Only supported bit-4 , bit-8 of block_size 128 or 64 now!\n");
+  }
+
+}
+
 void gemm_half_q_half_cuda(cublasHandle_t cublas_handle, const half* a,
                            const uint32_t* b_q_weight,
                            const uint32_t* b_gptq_qzeros,
                            const half* b_gptq_scales, const int* b_g_idx,
                            half* c, half* temp_dq, int size_m, int size_n,
-                           int size_k, int groups, bool use_exllama, int bit) {
+                           int size_k, int groups, bool use_exllama, int bit,
+			   int group_size, half* perm_space) {
   bool use_reconstruct;
-  if (use_exllama) {
-    use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
-                       (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
-  } else {
-    // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
-    // we disabled them for now.
-    use_reconstruct = (bit < 4 || size_m > MAX_ALT_GEMM_ROWS);
-  }
-  if (use_reconstruct) {
-    // Reconstruct FP16 matrix, then cuBLAS
-    if (use_exllama) {
-      reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                          temp_dq, size_k, size_n, groups, bit);
-    } else {
-      reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                       temp_dq, size_k, size_n, groups, bit);
-    }
-
-    const half alpha = __float2half(1.0f);
-    const half beta = __float2half(0.0f);
-    cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
-                &alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);
-  } else if (use_exllama) {
-    // Quantized matmul
-    int max_chunks = size_m / BLOCK_M_SIZE_MAX;
-    int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
-    int last_chunk_size = size_m - last_chunk;
-
-    if (max_chunks) {
-      gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
-                                 b_g_idx, c, last_chunk, size_n, size_k,
-                                 BLOCK_M_SIZE_MAX, groups, bit);
-    }
-
-    if (last_chunk_size) {
-      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
-                                 b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                                 c + last_chunk * size_n, last_chunk_size,
-                                 size_n, size_k, last_chunk_size, groups, bit);
-    }
+  bool opt = ((group_size == 128) || (group_size == 64));
+  if ((bit == 4) && opt) {
+          if ((size_m <= 2) && (group_size == 128)) {
+                  gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
+                                             b_g_idx, c, size_m, size_n, size_k,
+                                             BLOCK_M_SIZE_MAX, groups, bit, true, true);
+          } else {
+                  uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+                  half* scales = const_cast<half*>(b_gptq_scales);
+                  launch_gemm<input_type, vllm::kU4B8.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
+                                  group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
+                                  b_g_idx, perm_space, true);
+          }
+  } else if ((bit == 8) && opt) {
+          uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+          half* scales = const_cast<half*>(b_gptq_scales);
+          launch_gemm<input_type, vllm::kU8B128.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
+                          group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
+                          b_g_idx, perm_space, true);
   } else {
-    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                         c, size_m, size_n, size_k, bit);
+	  if (use_exllama) {
+	    use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
+			       (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
+	  } else {
+	    // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
+	    // we disabled them for now.
+	    use_reconstruct = (bit < 4 || size_m > 0);
+	  }
+	  if (use_reconstruct) {
+	    // Reconstruct FP16 matrix, then cuBLAS
+	    if (use_exllama) {
+	      reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+				  temp_dq, size_k, size_n, groups, bit);
+	    } else {
+	      reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+			       temp_dq, size_k, size_n, groups, bit);
+	    }
+
+	    const half alpha = __float2half(1.0f);
+	    const half beta = __float2half(0.0f);
+	    cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
+			&alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);
+	  } else if (use_exllama) {
+	    // Quantized matmul
+	    int max_chunks = size_m / BLOCK_M_SIZE_MAX;
+	    int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
+	    int last_chunk_size = size_m - last_chunk;
+
+	    bool m_sign;
+            bool v_sign;
+            if (group_size == 128) {
+                    m_sign = size_m <= 50;
+                    v_sign = size_m <= 4;
+            } else {
+                    m_sign = false;
+                    v_sign = false;
+            }
+
+	    if (max_chunks) {
+	      gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
+					 b_g_idx, c, last_chunk, size_n, size_k,
+					 BLOCK_M_SIZE_MAX, groups, bit, m_sign, v_sign);
+	    }
+
+	    if (last_chunk_size) {
+	      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
+					 b_gptq_qzeros, b_gptq_scales, b_g_idx,
+					 c + last_chunk * size_n, last_chunk_size,
+					 size_n, size_k, last_chunk_size, groups, bit, m_sign, v_sign);
+	    }
+	  } else {
+	    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+				 c, size_m, size_n, size_k, bit);
+	  }
   }
 }
 
@@ -1823,25 +2391,45 @@ void shuffle_exllama_weight(uint32_t* q_weight, int* q_perm, int height,
 torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
                         torch::Tensor b_gptq_qzeros,
                         torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
-                        bool use_exllama, int64_t bit) {
+                        bool use_exllama, int64_t bit, int64_t group_size, 
+			torch::Tensor perm_space, torch::Tensor temp_space,
+			bool dtype_bf16) {
   const at::cuda::OptionalCUDAGuard device_guard(device_of(a));
   auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());
-  at::Tensor c = torch::empty({a.size(0), b_q_weight.size(1)}, options);
+  at::Tensor c = torch::zeros({a.size(0), b_q_weight.size(1)}, options);
   at::Tensor temp_dq = torch::empty(
       {b_q_weight.size(0) * 32 / bit, b_q_weight.size(1)}, options);
 
-  vllm::gptq::gemm_half_q_half_cuda(
-      at::cuda::getCurrentCUDABlasHandle(), (const half*)a.data_ptr(),
-      (const uint32_t*)b_q_weight.data_ptr(),
-      (const uint32_t*)b_gptq_qzeros.data_ptr(),
-      (const half*)b_gptq_scales.data_ptr(),
-      b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),
-      (half*)c.data_ptr(), (half*)temp_dq.data_ptr(),
-      c.size(0),              // m
-      c.size(1),              // n
-      a.size(1),              // k
-      b_gptq_qzeros.size(0),  // group number
-      use_exllama, bit);
+  if (dtype_bf16) {
+      vllm::gptq::gemm_bf16_q_bf16_cuda(
+        (const __maca_bfloat16*)a.data_ptr(),
+        (const uint32_t*)b_q_weight.data_ptr(),
+        (const uint32_t*)b_gptq_qzeros.data_ptr(),
+        (const __maca_bfloat16*)b_gptq_scales.data_ptr(),
+        b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),
+        (__maca_bfloat16*)c.data_ptr(),
+	(float*)temp_space.data_ptr(),
+        c.size(0),              // m
+        c.size(1),              // n
+        a.size(1),              // k
+        bit, group_size,
+        (__maca_bfloat16*)perm_space.data_ptr());
+  } else {
+      vllm::gptq::gemm_half_q_half_cuda(
+        at::cuda::getCurrentCUDABlasHandle(), (const half*)a.data_ptr(),
+        (const uint32_t*)b_q_weight.data_ptr(),
+        (const uint32_t*)b_gptq_qzeros.data_ptr(),
+        (const half*)b_gptq_scales.data_ptr(),
+        b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),
+        (half*)c.data_ptr(), (half*)temp_dq.data_ptr(),
+        c.size(0),              // m
+        c.size(1),              // n
+        a.size(1),              // k
+        b_gptq_qzeros.size(0),  // group number
+        use_exllama, bit, group_size,
+        (half*)perm_space.data_ptr());
+  }
+
   return c;
 }
 
diff --git a/csrc/quantization/gptq/qdq_4.cuh b/csrc/quantization/gptq/qdq_4.cuh
index 7f65d2d28..34714f9a5 100644
--- a/csrc/quantization/gptq/qdq_4.cuh
+++ b/csrc/quantization/gptq/qdq_4.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
 Copied from https://github.com/turboderp/exllamav2
 */
@@ -86,6 +87,7 @@ __forceinline__ __device__ void dequant_4bit_8_prep_zero(const uint32_t zero,
   y1y16[1] = __half2half2(y16);
 }
 
+typedef __NATIVE_VECTOR__(2, float) v2f;
 __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
                                                     half2 (&dq)[4],
                                                     half2 (&z1z16)[2],
@@ -112,12 +114,50 @@ __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
     dq[2] = __hfma2(q2.as_half2, y1y16[0], z1z16[0]);
     dq[3] = __hfma2(q3.as_half2, y1y16[1], z1z16[1]);
   } else {
+#if 0
     dq[0] = __hadd2(q0.as_half2, z1z16[0]);  // half2( q[0] - z, q[1] - z )
     dq[1] = __hfma2(q1.as_half2, y1y16[1],
                     z1z16[1]);               // half2( q[2] - z, q[3] - z )
     dq[2] = __hadd2(q2.as_half2, z1z16[0]);  // half2( q[4] - z, q[5] - z )
     dq[3] = __hfma2(q3.as_half2, y1y16[1],
                     z1z16[1]);  // half2( q[6] - z, q[7] - z )
+#endif
+#if 1
+    dq[0] = __hadd2(q0.as_half2, z1z16[0]);
+    dq[2] = __hadd2(q2.as_half2, z1z16[0]);
+    v2f q1_float2, q3_float2, y1y16_1_float2, z1z16_1_float2;
+    q1_float2[0] = __half2float(__low2half(q1.as_half2));
+    q1_float2[1] = __half2float(__high2half(q1.as_half2));
+
+    q3_float2[0] = __half2float(__low2half(q3.as_half2));
+    q3_float2[1] = __half2float(__high2half(q3.as_half2));
+
+    y1y16_1_float2[0] = __half2float(__low2half(y1y16[1]));
+    y1y16_1_float2[1] = __half2float(__high2half(y1y16[1]));
+
+    z1z16_1_float2[0] = __half2float(__low2half(z1z16[1]));
+    z1z16_1_float2[1] = __half2float(__high2half(z1z16[1]));
+    v2f result1, result3;
+    result1 = __builtin_mxc_pk_fma_f32(q1_float2, y1y16_1_float2, z1z16_1_float2);
+    result3 = __builtin_mxc_pk_fma_f32(q3_float2, y1y16_1_float2, z1z16_1_float2);
+    dq[1] = __floats2half2_rn(result1[0], result1[1]);
+    dq[3] = __floats2half2_rn(result3[0], result3[1]);
+#else
+    dq[0] = __hadd2(q0.as_half2, z1z16[0]);
+    dq[2] = __hadd2(q2.as_half2, z1z16[0]);
+
+    v2f q1_float2, q3_float2, y1y16_1_float2, z1z16_1_float2;
+    q1_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&q1.as_half2));
+    q3_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&q3.as_half2));
+    y1y16_1_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&y1y16[1]));
+    z1z16_1_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&z1z16[1]));
+
+    v2f result1 = __builtin_mxc_pk_fma_f32(q1_float2, y1y16_1_float2, z1z16_1_float2);
+    v2f result3 = __builtin_mxc_pk_fma_f32(q3_float2, y1y16_1_float2, z1z16_1_float2);
+
+    dq[1] = __floats2half2_rn(result1[0], result1[1]);
+    dq[3] = __floats2half2_rn(result3[0], result3[1]);
+#endif
   }
 }
 }  // namespace gptq
diff --git a/csrc/quantization/gptq/scalar_type.hpp b/csrc/quantization/gptq/scalar_type.hpp
new file mode 100644
index 000000000..d17dea475
--- /dev/null
+++ b/csrc/quantization/gptq/scalar_type.hpp
@@ -0,0 +1,552 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#include <variant>
+#include <tuple>
+//#include <torch/custom_class.h>
+
+namespace vllm {
+
+//
+//  ScalarType can represent a wide range of floating point and integer types,
+//  in particular it can be used to represent sub-byte data types (something
+//  that torch.dtype currently does not support).
+//
+//  ScalarTypeTorch is a subclass of ScalarType that is compatible with
+//  TORCH_LIBRARY, making it accessible from Python as well meaning this class
+//  can be used as a argument for custom operators, helping to simplify these
+//  interfaces.
+//
+//  The type definitions on the Python side can be found in: vllm/_core_ext.pyi
+//  these type definitions should be kept up to date with any Python API changes
+//  here.
+//
+class ScalarType {
+ public:
+  enum NanRepr : uint8_t {
+    NAN_NONE = 0,                // nans are not supported
+    NAN_IEEE_754 = 1,            // nans are: exp all 1s, mantissa not all 0s
+    NAN_EXTD_RANGE_MAX_MIN = 2,  // nans are: exp all 1s, mantissa all 1s
+
+    NAN_REPR_ID_MAX
+  };
+
+  constexpr ScalarType(uint8_t exponent, uint8_t mantissa, bool signed_,
+                       int32_t bias, bool finite_values_only = false,
+                       NanRepr nan_repr = NAN_IEEE_754)
+      : exponent(exponent),
+        mantissa(mantissa),
+        signed_(signed_),
+        bias(bias),
+        finite_values_only(finite_values_only),
+        nan_repr(nan_repr){};
+
+  static constexpr ScalarType int_(uint8_t size_bits, int32_t bias = 0) {
+    return ScalarType(0, size_bits - 1, true, bias);
+  }
+
+  static constexpr ScalarType uint(uint8_t size_bits, int32_t bias = 0) {
+    return ScalarType(0, size_bits, false, bias);
+  }
+
+  // IEEE 754 compliant floating point type
+  static constexpr ScalarType float_IEEE754(uint8_t exponent,
+                                            uint8_t mantissa) {
+    //TORCH_CHECK(mantissa > 0 && exponent > 0);
+    return ScalarType(exponent, mantissa, true, 0, false, NAN_IEEE_754);
+  }
+
+  // IEEE 754 non-compliant floating point type
+  static constexpr ScalarType float_(uint8_t exponent, uint8_t mantissa,
+                                     bool finite_values_only,
+                                     NanRepr nan_repr) {
+    //TORCH_CHECK(nan_repr < NAN_REPR_ID_MAX, "Invalid NanRepr");
+    //TORCH_CHECK(mantissa > 0 && exponent > 0);
+    //TORCH_CHECK(nan_repr != NAN_IEEE_754,
+    //            "use `float_IEEE754` constructor for floating point types that "
+    //            "follow IEEE 754 conventions");
+    return ScalarType(exponent, mantissa, true, 0, finite_values_only,
+                      nan_repr);
+  }
+
+  uint8_t const exponent;  // size of the exponent field (0 for integer types)
+  uint8_t const mantissa;  // size of the mantissa field (size of the integer
+                           // excluding the sign bit for integer types)
+  bool const signed_;  // flag if the type supports negative numbers (i.e. has a
+                       // sign bit)
+  int32_t const bias;  // stored values equal value + bias,
+                       // used for quantized type
+
+  // Extra Floating point info
+  bool const finite_values_only;  // i.e. no +/-inf if true
+  NanRepr const nan_repr;         // how NaNs are represented
+                                  // (not applicable for integer types)
+
+  using Id = int64_t;
+
+ private:
+  // Field size in id
+  template <typename T_>
+  static constexpr size_t member_id_field_width() {
+    using T = std::decay_t<T_>;
+    return std::is_same_v<T, bool> ? 1 : sizeof(T) * 8;
+  }
+
+  template <typename Fn, typename Init, typename Member, typename... Rest>
+  static constexpr auto reduce_members_helper(Fn f, Init val, Member member,
+                                              Rest... rest) {
+    auto new_val = f(val, member);
+    if constexpr (sizeof...(rest) > 0) {
+      return reduce_members_helper(f, new_val, rest...);
+    } else {
+      return new_val;
+    };
+  }
+
+  template <typename Fn, typename Init>
+  constexpr auto reduce_members(Fn f, Init init) const {
+    // Should be in constructor order for `from_id`
+    return reduce_members_helper(f, init, exponent, mantissa, signed_, bias,
+                                 finite_values_only, nan_repr);
+  };
+
+  template <typename Fn, typename Init>
+  static constexpr auto reduce_member_types(Fn f, Init init) {
+    constexpr auto dummy_type = ScalarType(0, 0, false, 0, false, NAN_NONE);
+    return dummy_type.reduce_members(f, init);
+  };
+
+  static constexpr auto id_size_bits() {
+    return reduce_member_types(
+        [](int acc, auto member) -> int {
+          return acc + member_id_field_width<decltype(member)>();
+        },
+        0);
+  }
+
+ public:
+  // unique id for this scalar type that can be computed at compile time for
+  //  c++17 template specialization this is not needed once we migrate to
+  //  c++20 and can pass literal classes as template parameters
+  constexpr Id id() const {
+    static_assert(id_size_bits() <= sizeof(Id) * 8,
+                  "ScalarType id is too large to be stored");
+
+    auto or_and_advance = [](std::pair<Id, uint32_t> result,
+                             auto member) -> std::pair<Id, uint32_t> {
+      auto [id, bit_offset] = result;
+      auto constexpr bits = member_id_field_width<decltype(member)>();
+      return {id | (int64_t(member) & ((uint64_t(1) << bits) - 1))
+                       << bit_offset,
+              bit_offset + bits};
+    };
+    return reduce_members(or_and_advance, std::pair<Id, uint32_t>{}).first;
+  }
+
+  // create a ScalarType from an id, for c++17 template specialization,
+  //  this is not needed once we migrate to c++20 and can pass literal
+  //  classes as template parameters
+  static constexpr ScalarType from_id(Id id) {
+    auto extract_and_advance = [id](auto result, auto member) {
+      using T = decltype(member);
+      auto [tuple, bit_offset] = result;
+      auto constexpr bits = member_id_field_width<T>();
+      auto extracted_val = static_cast<T>((int64_t(id) >> bit_offset) &
+                                          ((uint64_t(1) << bits) - 1));
+      auto new_tuple = std::tuple_cat(tuple, std::make_tuple(extracted_val));
+      return std::pair<decltype(new_tuple), int>{new_tuple, bit_offset + bits};
+    };
+
+    auto [tuple_args, _] = reduce_member_types(extract_and_advance,
+                                               std::pair<std::tuple<>, int>{});
+    return std::apply([](auto... args) { return ScalarType(args...); },
+                      tuple_args);
+  }
+
+  constexpr int64_t size_bits() const {
+    return mantissa + exponent + is_signed();
+  }
+  constexpr bool is_signed() const { return signed_; }
+  constexpr bool is_integer() const { return exponent == 0; }
+  constexpr bool is_floating_point() const { return exponent > 0; }
+  constexpr bool is_ieee_754() const {
+    return is_floating_point() && finite_values_only == false &&
+           nan_repr == NAN_IEEE_754;
+  }
+  constexpr bool has_nans() const {
+    return is_floating_point() && nan_repr != NAN_NONE;
+  }
+  constexpr bool has_infs() const {
+    return is_floating_point() && finite_values_only == false;
+  }
+  constexpr bool has_bias() const { return bias != 0; }
+
+ private:
+  double _floating_point_max() const {
+    //TORCH_CHECK(mantissa <= 52 && exponent <= 11,
+    //            "Cannot represent max/min as a double for type ", str());
+
+    uint64_t max_mantissa = (uint64_t(1) << mantissa) - 1;
+    if (nan_repr == NAN_EXTD_RANGE_MAX_MIN) {
+      max_mantissa -= 1;
+    }
+
+    uint64_t max_exponent = (uint64_t(1) << exponent) - 2;
+    if (nan_repr == NAN_EXTD_RANGE_MAX_MIN || nan_repr == NAN_NONE) {
+      //TORCH_CHECK(exponent < 11,
+      //            "Cannot represent max/min as a double for type ", str());
+      max_exponent += 1;
+    }
+
+    // adjust the exponent to match that of a double
+    //  for now we assume the exponent bias is the standard 2^(e-1) -1, (where e
+    //  is the exponent bits), there is some precedent for non-standard biases,
+    //  example `float8_e4m3b11fnuz` here: https://github.com/jax-ml/ml_dtypes
+    //  but to avoid premature over complication we are just assuming the
+    //  standard exponent bias until there is a need to support non-standard
+    //  biases
+    uint64_t exponent_bias = (uint64_t(1) << (exponent - 1)) - 1;
+    uint64_t exponent_bias_double = (uint64_t(1) << 10) - 1;  // double e = 11
+
+    uint64_t max_exponent_double =
+        max_exponent - exponent_bias + exponent_bias_double;
+
+    // shift the mantissa into the position for a double and
+    // the exponent
+    uint64_t double_raw =
+        (max_mantissa << (52 - mantissa)) | (max_exponent_double << 52);
+
+    return *reinterpret_cast<double*>(&double_raw);
+  }
+
+  constexpr std::variant<int64_t, double> _raw_max() const {
+    if (is_floating_point()) {
+      return {_floating_point_max()};
+    } else {
+      //TORCH_CHECK(size_bits() < 64 || size_bits() == 64 && is_signed(),
+      //            "Cannot represent max as a int64_t");
+      return {(int64_t(1) << mantissa) - 1};
+    }
+  }
+
+  constexpr std::variant<int64_t, double> _raw_min() const {
+    if (is_floating_point()) {
+      //TORCH_CHECK(is_signed(),
+      //            "We currently assume all floating point types are signed");
+      constexpr uint64_t sign_bit_double = (uint64_t(1) << 63);
+
+      double max = _floating_point_max();
+      uint64_t max_raw = *reinterpret_cast<uint64_t*>(&max);
+      uint64_t min_raw = max_raw | sign_bit_double;
+      return {*reinterpret_cast<double*>(&min_raw)};
+    } else {
+      //TORCH_CHECK(!is_signed() || size_bits() <= 64,
+      //            "Cannot represent min as a int64_t");
+      if (is_signed()) {
+        // set the top bit to 1 (i.e. INT64_MIN) and the rest to 0
+        // then perform an arithmetic shift right to set all the bits above
+        // (size_bits() - 1) to 1
+        return {INT64_MIN >> (64 - size_bits())};
+      } else {
+        return {int64_t(0)};
+      }
+    }
+  }
+
+ public:
+  // Max representable value for this scalar type.
+  // (accounting for bias if there is one)
+  constexpr std::variant<int64_t, double> max() const {
+    return std::visit(
+        [this](auto x) -> std::variant<int64_t, double> { return {x - bias}; },
+        _raw_max());
+  }
+
+  // Min representable value for this scalar type.
+  // (accounting for bias if there is one)
+  constexpr std::variant<int64_t, double> min() const {
+    return std::visit(
+        [this](auto x) -> std::variant<int64_t, double> { return {x - bias}; },
+        _raw_min());
+  }
+
+  std::string str() const {
+    /* naming generally follows: https://github.com/jax-ml/ml_dtypes
+     * for floating point types (leading f) the scheme is:
+     *  `float<size_bits>_e<exponent_bits>m<mantissa_bits>[flags]`
+     *  flags:
+     *  - no-flags: means it follows IEEE 754 conventions
+     *  - f: means finite values only (no infinities)
+     *  - n: means nans are supported (non-standard encoding)
+     * for integer types the scheme is:
+     *  `[u]int<size_bits>[b<bias>]`
+     *  - if bias is not present it means its zero
+     */
+    if (is_floating_point()) {
+      auto ret = "float" + std::to_string(size_bits()) + "_e" +
+                 std::to_string(exponent) + "m" + std::to_string(mantissa);
+      if (!is_ieee_754()) {
+        if (finite_values_only) {
+          ret += "f";
+        }
+        if (nan_repr != NAN_NONE) {
+          ret += "n";
+        }
+      }
+      return ret;
+    } else {
+      auto ret = ((is_signed()) ? "int" : "uint") + std::to_string(size_bits());
+      if (has_bias()) {
+        ret += "b" + std::to_string(bias);
+      }
+      return ret;
+    }
+  }
+
+  constexpr bool operator==(ScalarType const& other) const {
+    return mantissa == other.mantissa && exponent == other.exponent &&
+           bias == other.bias && signed_ == other.signed_ &&
+           finite_values_only == other.finite_values_only &&
+           nan_repr == other.nan_repr;
+  }
+};
+#if 0
+// Create a TORCH_LIBRARY compatible version of ScalarType (i.e. inherit from
+//  torch::CustomClassHolder), we use multiple inheritance here since we cannot
+//  have ScalarType inherit from torch::CustomClassHolder and have a constexpr
+//  constructor at the same time (torch::CustomClassHolder does not have a
+//  constexpr destructor)
+// See also:
+// https://docs.google.com/document/d/18fBMPuOJ0fY5ZQ6YyrHUppw9FA332CpNtgB6SOIgyuA
+class ScalarTypeTorch : public torch::CustomClassHolder, public ScalarType {
+ public:
+  ScalarTypeTorch(int64_t exponent, int64_t mantissa, int64_t bias,
+                  bool _signed)
+      : ScalarType(exponent, mantissa, bias, _signed){};
+
+  ScalarTypeTorch(ScalarType type) : ScalarType(type){};
+
+  using Base = ScalarType;
+  using Self = ScalarTypeTorch;
+  using SelfPtr = c10::intrusive_ptr<Self>;
+
+  static void check_size_bits(int64_t size_bits, bool signed_) {
+    TORCH_CHECK(
+        size_bits <=
+            std::numeric_limits<decltype(std::declval<Self>().mantissa)>::max(),
+        "size_bits bit width is too large to be represented");
+  }
+
+  static void check_bias(int64_t bias) {
+    using Bias = decltype(std::declval<Self>().bias);
+    TORCH_CHECK(bias <= std::numeric_limits<Bias>::max() &&
+                    bias >= std::numeric_limits<Bias>::min(),
+                "bias too large or small to be represented");
+  }
+
+  static void check_exponent(int64_t exponent) {
+    TORCH_CHECK(
+        exponent <=
+            std::numeric_limits<decltype(std::declval<Self>().exponent)>::max(),
+        "exponent bit width is too large to be represented");
+  }
+
+  static void check_mantissa(int64_t mantissa) {
+    TORCH_CHECK(
+        mantissa <=
+            std::numeric_limits<decltype(std::declval<Self>().mantissa)>::max(),
+        "mantissa bit width is too large to be represented");
+  }
+
+  static SelfPtr int_(int64_t size_bits, c10::optional<int64_t> bias) {
+    check_size_bits(size_bits, true);
+    check_bias(bias.value_or(0));
+    return c10::make_intrusive<Self>(
+        ScalarType::int_(size_bits, bias.value_or(0)));
+  }
+
+  static SelfPtr uint(int64_t size_bits, c10::optional<int64_t> bias) {
+    check_size_bits(size_bits, true);
+    check_bias(bias.value_or(0));
+    return c10::make_intrusive<Self>(
+        ScalarType::uint(size_bits, bias.value_or(0)));
+  }
+
+  static SelfPtr float_IEEE754(int64_t exponent, int64_t mantissa) {
+    check_mantissa(mantissa);
+    check_exponent(exponent);
+    return c10::make_intrusive<Self>(
+        ScalarType::float_IEEE754(exponent, mantissa));
+  }
+
+  static SelfPtr float_(int64_t exponent, int64_t mantissa,
+                        bool finite_values_only, int64_t nan_repr) {
+    check_mantissa(mantissa);
+    check_exponent(exponent);
+    return c10::make_intrusive<Self>(ScalarType::float_(
+        exponent, mantissa, finite_values_only, NanRepr(nan_repr)));
+  }
+
+  // This needs to be implemented and throw a TypeError in order for
+  // PyTorch's opcheck to work on ops that use ScalarTypes.
+  int64_t len() const {
+    throw c10::TypeError({__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
+                         "__len__ not implemented");
+    return 0;
+  }
+
+  // Serialize a ScalarType into a tuple of pairs.  Where each pair
+  // is a (fieldname, value).
+  // For simplicity, we are just going to convert to a ScalarTypeId.
+  std::tuple<std::tuple<std::string, int64_t>> obj_flatten() const {
+    return {{"ScalarType", id()}};
+  }
+
+  // Deserialize a scalar type that has been serialized by obj_flatten,
+  // ostensibly from a tuple of (member name, value) pairs, but in reality
+  // just a ScalarTypeId.
+  static SelfPtr obj_unflatten(
+      std::tuple<std::tuple<std::string, int64_t>> const& flat_type) {
+    return c10::make_intrusive<Self>(
+        from_id(std::get<1>(std::get<0>(flat_type))));
+  }
+
+  template <typename T>
+  static void bind_readonly_property(torch::class_<Self>& cls,
+                                     std::string const& name, T Base::*field) {
+    auto getter_func_helper = [field = std::move(field)](SelfPtr const& self) {
+      if constexpr (std::is_member_function_pointer_v<decltype(field)>) {
+        return (self.get()->*field)();
+      } else {
+        return self.get()->*field;
+      }
+    };
+
+    auto getter_func = [field = std::move(field),
+                        getter_func_helper = std::move(getter_func_helper)](
+                           SelfPtr const& self) {
+      auto val = getter_func_helper(self);
+      // upconvert uint8_t, int32_t etc. to int64_t for python
+      if constexpr (std::is_integral_v<T>) {
+        return static_cast<int64_t>(val);
+      } else {
+        return val;
+      }
+    };
+
+    cls.def_property(name, getter_func);
+  }
+
+  template <typename MemberFunc, typename Cls>
+  static void bind_function(torch::class_<Self>& cls, const std::string& name,
+                            MemberFunc Cls::*member) {
+    cls.def(name, [member = std::move(member)](SelfPtr const& self) {
+      return (self.get()->*member)();
+    });
+  }
+
+  template <typename Func>
+  static void bind_function(torch::class_<Self>& cls, const std::string& name,
+                            Func func) {
+    cls.def(name, func);
+  }
+
+  template <typename Func>
+  static void bind_static_function(torch::class_<Self>& cls,
+                                   const std::string& name, Func func) {
+    cls.def_static(name, func);
+  }
+
+  static void bind_class(torch::Library& lib) {
+    auto cls = lib.class_<ScalarTypeTorch>("ScalarType")
+                   .def(torch::init<int64_t, int64_t, int64_t, bool>());
+
+    // Bind Properties
+    bind_readonly_property(cls, "mantissa", &Base::mantissa);
+    bind_readonly_property(cls, "exponent", &Base::exponent);
+    bind_readonly_property(cls, "bias", &Base::bias);
+    bind_readonly_property(cls, "signed", &Base::is_signed);
+    bind_readonly_property(cls, "size_bits", &Base::size_bits);
+
+    // Bind member functions
+    bind_function(cls, "is_signed", &Base::is_signed);
+    bind_function(cls, "is_integer", &Base::is_integer);
+    bind_function(cls, "is_floating_point", &Base::is_floating_point);
+    bind_function(cls, "is_ieee_754", &Base::is_ieee_754);
+    bind_function(cls, "has_nans", &Base::has_nans);
+    bind_function(cls, "has_infs", &Base::has_infs);
+    bind_function(cls, "has_bias", &Base::has_bias);
+
+    bind_function(cls, "max", [](SelfPtr const& self) {
+      return std::visit([](auto arg) { return c10::IValue(arg); },
+                        self.get()->max());
+    });
+    bind_function(cls, "min", [](SelfPtr const& self) {
+      return std::visit([](auto arg) { return c10::IValue(arg); },
+                        self.get()->min());
+    });
+
+    bind_function(cls, "__len__", &ScalarTypeTorch::len);
+    bind_function(cls, "__str__", &Base::str);
+    bind_function(cls, "__eq__", [](SelfPtr const& self, SelfPtr const& other) {
+      return *self == *other;
+    });
+    bind_function(cls, "__repr__", [](SelfPtr const& self) {
+      return "ScalarType." + self.get()->str();
+    });
+
+    bind_function(cls, "__obj_flatten__", &ScalarTypeTorch::obj_flatten);
+    bind_static_function(cls, "__obj_unflatten__",
+                         &ScalarTypeTorch::obj_unflatten);
+
+    // Bind static functions (convenience constructors)
+    bind_static_function(cls, "int_", &ScalarTypeTorch::int_);
+    bind_static_function(cls, "uint", &ScalarTypeTorch::uint);
+    bind_static_function(cls, "float_IEEE754", &ScalarTypeTorch::float_IEEE754);
+    bind_static_function(cls, "float_", &ScalarTypeTorch::float_);
+  }
+};
+#endif
+using ScalarTypeId = int64_t;
+//using ScalarTypeTorchPtr = c10::intrusive_ptr<ScalarTypeTorch>;
+
+// "rust style" names generally following:
+//   https://github.com/pytorch/pytorch/blob/6d9f74f0af54751311f0dd71f7e5c01a93260ab3/torch/csrc/api/include/torch/types.h#L60-L70
+static inline constexpr auto kS4 = ScalarType::int_(4);
+static inline constexpr auto kU4 = ScalarType::uint(4);
+static inline constexpr auto kU4B8 = ScalarType::uint(4, 8);
+static inline constexpr auto kS8 = ScalarType::int_(8);
+static inline constexpr auto kU8 = ScalarType::uint(8);
+static inline constexpr auto kU8B128 = ScalarType::uint(8, 128);
+
+static inline constexpr auto kFE3M2f =
+    ScalarType::float_(3, 2, true, ScalarType::NAN_NONE);
+static inline constexpr auto kFE4M3fn =
+    ScalarType::float_(4, 3, true, ScalarType::NAN_EXTD_RANGE_MAX_MIN);
+static inline constexpr auto kFE5M2 = ScalarType::float_IEEE754(5, 2);
+static inline constexpr auto kFE8M7 = ScalarType::float_IEEE754(8, 7);
+static inline constexpr auto kFE5M10 = ScalarType::float_IEEE754(5, 10);
+#if 0
+// Fixed width style names, generally following:
+//  https://github.com/pytorch/pytorch/blob/6d9f74f0af54751311f0dd71f7e5c01a93260ab3/torch/csrc/api/include/torch/types.h#L47-L57
+static inline constexpr auto kInt4 = kS4;
+static inline constexpr auto kUint4 = kU4;
+static inline constexpr auto kUint4b8 = kU4B8;
+static inline constexpr auto kInt8 = kS8;
+static inline constexpr auto kUint8 = kU8;
+static inline constexpr auto kUint8b128 = kU8B128;
+
+static inline constexpr auto kFloat6_e3m2f = kFE3M2f;
+static inline constexpr auto kFloat8_e4m3fn = kFE4M3fn;
+static inline constexpr auto kFloat8_e5m2 = kFE5M2;
+static inline constexpr auto kFloat16_e8m7 = kFE8M7;
+static inline constexpr auto kFloat16_e5m10 = kFE5M10;
+
+// colloquial names
+static inline constexpr auto kHalf = kFE5M10;
+static inline constexpr auto kFloat16 = kHalf;
+static inline constexpr auto kBFloat16 = kFE8M7;
+
+static inline constexpr auto kFloat16Id = kFloat16.id();
+#endif
+};  // namespace vllm
diff --git a/csrc/quantization/vectorization.cuh b/csrc/quantization/vectorization.cuh
index 44c999130..2bf2c884a 100644
--- a/csrc/quantization/vectorization.cuh
+++ b/csrc/quantization/vectorization.cuh
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #pragma once
 /**
  * __device__ datatypes vectorized by 4
@@ -5,8 +6,8 @@
 
 // Include both AMD and NVIDIA fp8 types to avoid circular import
 // TODO(luka/varun) use FP8_TYPE instead after refactoring
-#include <c10/util/Float8_e4m3fnuz.h>
-#include <c10/util/Float8_e4m3fn.h>
+//#include <c10/util/Float8_e4m3fnuz.h>
+//#include <c10/util/Float8_e4m3fn.h>
 
 namespace vllm {
 
@@ -21,9 +22,9 @@ struct __align__(8) vec4_t {
 
 template <typename quant_type_t>
 struct __align__(4) q8x4_t {
-  static_assert(std::is_same_v<quant_type_t, int8_t> ||
-                std::is_same_v<quant_type_t, c10::Float8_e4m3fn> ||
-                std::is_same_v<quant_type_t, c10::Float8_e4m3fnuz>);
+  //static_assert(std::is_same_v<quant_type_t, int8_t> ||
+  //              std::is_same_v<quant_type_t, c10::Float8_e4m3fn> ||
+  //              std::is_same_v<quant_type_t, c10::Float8_e4m3fnuz>);
   quant_type_t x;
   quant_type_t y;
   quant_type_t z;
diff --git a/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu b/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
index 371de0950..a871383fa 100644
--- a/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
+++ b/csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <cudaTypedefs.h>
 
 #include <c10/cuda/CUDAGuard.h>
@@ -30,7 +31,7 @@ void cutlass_scaled_sparse_mm(torch::Tensor& c, torch::Tensor const& a,
                               torch::Tensor const& bt_meta,
                               torch::Tensor const& a_scales,
                               torch::Tensor const& b_scales,
-                              std::optional<torch::Tensor> const& bias) {
+                              c10::optional<torch::Tensor> const& bias) {
   // Checks for conformality
   TORCH_CHECK(a.dim() == 2 && bt_nzs.dim() == 2 && c.dim() == 2);
   TORCH_CHECK(c.size(1) == bt_nzs.size(0) && bt_nzs.size(1) * 2 == a.size(1) &&
diff --git a/csrc/torch_bindings.cpp b/csrc/torch_bindings.cpp
index c03806f43..175426438 100644
--- a/csrc/torch_bindings.cpp
+++ b/csrc/torch_bindings.cpp
@@ -1,3 +1,5 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+
 #include "cache.h"
 #include "cuda_utils.h"
 #include "ops.h"
@@ -40,16 +42,25 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   ops.def(
       "paged_attention_v2("
       "    Tensor! out, Tensor! exp_sums, Tensor! max_logits,"
-      "    Tensor! tmp_out, Tensor query, Tensor key_cache,"
+      "    Tensor tmp_out, Tensor block_count, Tensor query, Tensor key_cache,"
       "    Tensor value_cache, int num_kv_heads, float scale,"
       "    Tensor block_tables, Tensor seq_lens, int block_size,"
       "    int max_seq_len, Tensor? alibi_slopes,"
       "    str kv_cache_dtype, Tensor k_scale, Tensor v_scale,"
       "    int tp_rank, int blocksparse_local_blocks,"
       "    int blocksparse_vert_stride, int blocksparse_block_size,"
-      "    int blocksparse_head_sliding_step) -> ()");
+      "    int blocksparse_head_sliding_step, bool count_init_once) -> ()");
   ops.impl("paged_attention_v2", torch::kCUDA, &paged_attention_v2);
 
+  ops.def(
+      "page_reshape_kv_cache("
+      "    Tensor! key_cache, Tensor! value_cache,"
+      "    Tensor! key_cache_new_layer, Tensor! value_cache_new_layer,"
+      "    int num_seqs, int num_heads, int head_size, int num_kv_heads, int block_size,"
+      "           str! kv_cache_dtype) -> ()");
+  //ops.def("page_reshape_kv_cache", &page_reshape_kv_cache);
+  ops.impl("page_reshape_kv_cache", torch::kCUDA, &page_reshape_kv_cache);
+
   // Activation ops
   // Activation function used in SwiGLU.
   ops.def("silu_and_mul(Tensor! out, Tensor input) -> ()");
@@ -114,6 +125,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "float epsilon) -> ()");
   ops.impl("fused_add_rms_norm", torch::kCUDA, &fused_add_rms_norm);
 
+// #ifdef MX_MACA
   // Layernorm-quant
   // Apply Root Mean Square (RMS) Normalization to the input tensor.
   ops.def(
@@ -138,6 +150,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "Tensor? scale_ub, Tensor!? residual) -> ()");
   ops.impl("rms_norm_dynamic_per_token_quant", torch::kCUDA,
            &rms_norm_dynamic_per_token_quant);
+// #endif
 
   // Rotary embedding
   // Apply GPT-NeoX or GPT-J style rotary embedding to query and key.
@@ -158,6 +171,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   ops.impl("batched_rotary_embedding", torch::kCUDA, &batched_rotary_embedding);
 
   // Quantization ops
+#ifdef MX_MACA
 #ifndef USE_ROCM
   // Quantized GEMM for AQLM.
   ops.def(
@@ -172,18 +186,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "int[] codebook_partition_sizes) -> Tensor");
   ops.impl("aqlm_dequant", torch::kCUDA, &aqlm_dequant);
 
-  // Quantized GEMM for AWQ.
-  ops.def(
-      "awq_gemm(Tensor _in_feats, Tensor _kernel, Tensor _scaling_factors, "
-      "Tensor _zeros, SymInt split_k_iters) -> Tensor");
-  ops.impl("awq_gemm", torch::kCUDA, &awq_gemm);
-
-  // Dequantization for AWQ.
-  ops.def(
-      "awq_dequantize(Tensor _kernel, Tensor _scaling_factors, "
-      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor");
-  ops.impl("awq_dequantize", torch::kCUDA, &awq_dequantize);
-
   // Note about marlin kernel 'workspace' arguments:
   // Technically these should be mutable since they are modified by the kernel.
   // But since they are set back to zero once the kernel is finished we can
@@ -213,41 +215,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "SymInt size_m, SymInt size_n, SymInt size_k) -> Tensor");
   //  conditionally compiled so impl in source file
 
-  // Machete (Dense) Optimized Mixed Precision GEMM for Hopper.
-  ops.def(
-      "machete_supported_schedules("
-      "   ScalarType a_type,"
-      "   int b_type,"
-      "   ScalarType? maybe_group_scales_type,"
-      "   ScalarType? maybe_group_zeros_type,"
-      "   ScalarType? maybe_channel_scales_type,"
-      "   ScalarType? maybe_token_scales_type,"
-      "   ScalarType? maybe_out_type"
-      ") -> str[]");
-  ops.def(
-      "machete_mm("
-      "   Tensor A,"
-      "   Tensor B,"
-      "   int b_type,"
-      "   ScalarType? out_type,"
-      "   Tensor? group_scales,"
-      "   Tensor? group_zeros,"
-      "   int?    group_size,"
-      "   Tensor? channel_scales,"
-      "   Tensor? token_scales,"
-      "   str?    schedule"
-      ") -> Tensor");
-  ops.def(
-      "machete_prepack_B("
-      "   Tensor B,"
-      "   ScalarType a_type,"
-      "   int b_type,"
-      "   ScalarType? group_scales_type"
-      ") -> Tensor");
-  // conditionally compiled so impl registration is in source file
 
-  ops.def("permute_cols(Tensor A, Tensor perm) -> Tensor");
-  ops.impl("permute_cols", torch::kCUDA, &permute_cols);
 
   // gptq_marlin Optimized Quantized GEMM for GPTQ.
   ops.def(
@@ -271,21 +239,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   // conditionally compiled so impl registrations are in source file
 #endif
 
-  // Dequantization for GGML.
-  ops.def("ggml_dequantize(Tensor W, int type, SymInt m, SymInt n) -> Tensor");
-  ops.impl("ggml_dequantize", torch::kCUDA, &ggml_dequantize);
-
-  // mmvq kernel for GGML.
-  ops.def(
-      "ggml_mul_mat_vec_a8(Tensor W, Tensor X, int type, SymInt row) "
-      "-> Tensor");
-  ops.impl("ggml_mul_mat_vec_a8", torch::kCUDA, &ggml_mul_mat_vec_a8);
-
-  // mmq kernel for GGML.
-  ops.def(
-      "ggml_mul_mat_a8(Tensor W, Tensor X, int type, SymInt row) -> Tensor");
-  ops.impl("ggml_mul_mat_a8", torch::kCUDA, &ggml_mul_mat_a8);
-
 #ifndef USE_ROCM
   // fp8_marlin Optimized Quantized GEMM for FP8 weight-only.
   ops.def(
@@ -302,23 +255,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "SymInt size_k) -> Tensor");
   // conditionally compiled so impl registration is in source file
 
-  // CUTLASS w8a8 GEMM, supporting symmetric per-tensor or per-row/column
-  // quantization, as well as bias
-  ops.def(
-      "cutlass_scaled_mm(Tensor! out, Tensor a,"
-      "                  Tensor b, Tensor a_scales,"
-      "                  Tensor b_scales, Tensor? bias) -> ()");
-  ops.impl("cutlass_scaled_mm", torch::kCUDA, &cutlass_scaled_mm);
-
-  // CUTLASS w8a8 GEMM, supporting asymmetric per-tensor or per-row/column
-  // quantization.
-  ops.def(
-      "cutlass_scaled_mm_azp(Tensor! out, Tensor a,"
-      "                  Tensor b, Tensor a_scales,"
-      "                  Tensor b_scales, Tensor azp_adj,"
-      "                  Tensor? azp, Tensor? bias) -> ()");
-  ops.impl("cutlass_scaled_mm_azp", torch::kCUDA, &cutlass_scaled_mm_azp);
-
   // Check if cutlass scaled_mm is supported for CUDA devices of the given
   // capability
   ops.def("cutlass_scaled_mm_supports_fp8(int cuda_device_capability) -> bool");
@@ -353,6 +289,46 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "                              Tensor a) -> bool");
   ops.impl("cutlass_sparse_compress_entry", &cutlass_sparse_compress_entry);
 
+
+#endif
+#endif
+
+ops.def("permute_cols(Tensor A, Tensor perm) -> Tensor");
+ops.impl("permute_cols", torch::kCUDA, &permute_cols);
+
+  // Machete (Dense) Optimized Mixed Precision GEMM for Hopper.
+  ops.def(
+      "machete_supported_schedules("
+      "   ScalarType a_type,"
+      "   int b_type,"
+      "   ScalarType? maybe_group_scales_type,"
+      "   ScalarType? maybe_group_zeros_type,"
+      "   ScalarType? maybe_channel_scales_type,"
+      "   ScalarType? maybe_token_scales_type,"
+      "   ScalarType? maybe_out_type"
+      ") -> str[]");
+  ops.def(
+      "machete_mm("
+      "   Tensor A,"
+      "   Tensor B,"
+      "   int b_type,"
+      "   ScalarType? out_type,"
+      "   Tensor? group_scales,"
+      "   Tensor? group_zeros,"
+      "   int?    group_size,"
+      "   Tensor? channel_scales,"
+      "   Tensor? token_scales,"
+      "   str?    schedule"
+      ") -> Tensor");
+  ops.def(
+      "machete_prepack_B("
+      "   Tensor B,"
+      "   ScalarType a_type,"
+      "   int b_type,"
+      "   ScalarType? group_scales_type"
+      ") -> Tensor");
+  // conditionally compiled so impl registration is in source file
+
   // Mamba selective scan kernel
   ops.def(
       "selective_scan_fwd(Tensor! u, Tensor! delta,"
@@ -387,15 +363,64 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "bool silu_activation,"
       "int pad_slot_id) -> ()");
   ops.impl("causal_conv1d_fwd", torch::kCUDA, &causal_conv1d_fwd);
-#endif
+
+  // CUTLASS w8a8 GEMM, supporting symmetric per-tensor or per-row/column
+  // quantization, as well as bias
+  ops.def(
+      "cutlass_scaled_mm(Tensor! out, Tensor a,"
+      "                  Tensor b, Tensor a_scales,"
+      "                  Tensor b_scales, Tensor? bias) -> ()");
+  ops.impl("cutlass_scaled_mm", torch::kCUDA, &cutlass_scaled_mm);
+
+  // CUTLASS w8a8 GEMM, supporting asymmetric per-tensor or per-row/column
+  // quantization.
+  ops.def(
+      "cutlass_scaled_mm_azp(Tensor! out, Tensor a,"
+      "                  Tensor b, Tensor a_scales,"
+      "                  Tensor b_scales, Tensor azp_adj,"
+      "                  Tensor? azp, Tensor? bias) -> ()");
+  ops.impl("cutlass_scaled_mm_azp", torch::kCUDA, &cutlass_scaled_mm_azp);
+
+  // Dequantization for GGML.
+  ops.def("ggml_dequantize(Tensor W, int type, SymInt m, SymInt n) -> Tensor");
+  ops.impl("ggml_dequantize", torch::kCUDA, &ggml_dequantize);
+
+  // mmvq kernel for GGML.
+  ops.def(
+      "ggml_mul_mat_vec_a8(Tensor W, Tensor X, int type, SymInt row) "
+      "-> Tensor");
+  ops.impl("ggml_mul_mat_vec_a8", torch::kCUDA, &ggml_mul_mat_vec_a8);
+
+  // mmq kernel for GGML.
+  ops.def(
+      "ggml_mul_mat_a8(Tensor W, Tensor X, int type, SymInt row) -> Tensor");
+  ops.impl("ggml_mul_mat_a8", torch::kCUDA, &ggml_mul_mat_a8);
+
+
+  // Quantized GEMM for AWQ.
+  ops.def(
+      "awq_gemm(Tensor _in_feats, Tensor _kernel, Tensor _scaling_factors, "
+      "Tensor _zeros, SymInt split_k_iters, Tensor _temp_space, bool dtype_bf16) -> Tensor");
+  ops.impl("awq_gemm", torch::kCUDA, &awq_gemm);
+
+  // Dequantization for AWQ.
+  ops.def(
+      "awq_dequantize(Tensor _kernel, Tensor _scaling_factors, "
+      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor");
+  ops.impl("awq_dequantize", torch::kCUDA, &awq_dequantize);
+  
+  // Convert AWQ to GPTQ
+  ops.def(
+      "awq_to_gptq_4bit(Tensor qweight) -> Tensor");
+  ops.impl("awq_to_gptq_4bit", torch::kCUDA, &awq_to_gptq_4bit);
 
   // Quantized GEMM for GPTQ.
   // Note: even though the C++ inferred schema is correct for this op, it seems
   // to prevent the meta function registry.
   ops.def(
       "gptq_gemm(Tensor a, Tensor b_q_weight, Tensor b_gptq_qzeros, "
-      "Tensor b_gptq_scales, Tensor b_g_idx, bool use_exllama, int bit) "
-      "-> Tensor");
+      "Tensor b_gptq_scales, Tensor b_g_idx, bool use_exllama, int bit, int group_size, Tensor perm_space, "
+      "Tensor temp_space, bool dtype_bf16)-> Tensor");
   ops.impl("gptq_gemm", torch::kCUDA, &gptq_gemm);
 
   // Post processing for GPTQ.
@@ -415,6 +440,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "()");
   ops.impl("dynamic_scaled_fp8_quant", torch::kCUDA, &dynamic_scaled_fp8_quant);
 
+// #ifdef MX_MACA
   // Compute dynamic-per-token FP8 quantized tensor and scaling factor.
   ops.def(
       "dynamic_per_token_scaled_fp8_quant(Tensor! result, Tensor input, "
@@ -423,6 +449,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   ops.impl("dynamic_per_token_scaled_fp8_quant", torch::kCUDA,
            &dynamic_per_token_scaled_fp8_quant);
 
+// #endif
   // Compute int8 quantized tensor for given scaling factor.
   ops.def(
       "static_scaled_int8_quant(Tensor! result, Tensor input, Tensor scale,"
@@ -463,6 +490,15 @@ TORCH_LIBRARY_EXPAND(CONCAT(TORCH_EXTENSION_NAME, _cache_ops), cache_ops) {
       "                  Tensor k_scale, Tensor v_scale) -> ()");
   cache_ops.impl("reshape_and_cache", torch::kCUDA, &reshape_and_cache);
 
+  cache_ops.def(
+      "reshape_and_cache_new(Tensor key, Tensor value,"
+      "                  Tensor! key_cache, Tensor! value_cache,"
+      "                  Tensor slot_mapping,"
+      "                  str kv_cache_dtype,"
+      "                  float kv_scale,"
+      "                  float v_scale) -> ()");
+  cache_ops.impl("reshape_and_cache_new", torch::kCUDA, &reshape_and_cache_new);
+
   // Reshape the key and value tensors and cache them.
   cache_ops.def(
       "reshape_and_cache_flash(Tensor key, Tensor value,"
diff --git a/docs/source/models/supported_models.md b/docs/source/models/supported_models.md
index 32f3e9def..acb387594 100644
--- a/docs/source/models/supported_models.md
+++ b/docs/source/models/supported_models.md
@@ -1,971 +1,981 @@
-(supported-models)=
-
-# List of Supported Models
-
-vLLM supports generative and pooling models across various tasks.
-If a model supports more than one task, you can set the task via the `--task` argument.
-
-For each task, we list the model architectures that have been implemented in vLLM.
-Alongside each architecture, we include some popular models that use it.
-
-## Loading a Model
-
-### HuggingFace Hub
-
-By default, vLLM loads models from [HuggingFace (HF) Hub](https://huggingface.co/models).
-
-To determine whether a given model is supported, you can check the `config.json` file inside the HF repository.
-If the `"architectures"` field contains a model architecture listed below, then it should be supported in theory.
-
-:::{tip}
-The easiest way to check if your model is really supported at runtime is to run the program below:
-
-```python
-from vllm import LLM
-
-# For generative models (task=generate) only
-llm = LLM(model=..., task="generate")  # Name or path of your model
-output = llm.generate("Hello, my name is")
-print(output)
-
-# For pooling models (task={embed,classify,reward,score}) only
-llm = LLM(model=..., task="embed")  # Name or path of your model
-output = llm.encode("Hello, my name is")
-print(output)
-```
-
-If vLLM successfully returns text (for generative models) or hidden states (for pooling models), it indicates that your model is supported.
-:::
-
-Otherwise, please refer to [Adding a New Model](#new-model) for instructions on how to implement your model in vLLM.
-Alternatively, you can [open an issue on GitHub](https://github.com/vllm-project/vllm/issues/new/choose) to request vLLM support.
-
-### Transformers fallback
-
-After the merge of <gh-pr:11330>, `vllm` can fallback to models that are available in `transformers`. This does not work for all models for now, but most decoder language models are supported, and vision language model support is planned!
-
-To check if the backend is `transformers`, you can simply do this:
-
-```python 
-from vllm import LLM
-llm = LLM(model=..., task="generate")  # Name or path of your model
-llm.apply_model(lambda model: print(model.__class__))
-```
-
-If it is `TransformersModel` then it means it's based on `transformers`!
-
-#### Supported features
-
-##### LORA and quantization
-
-Both are not supported yet! Make sure to open an issue and we'll work on this together with the `transformers` team!
-
-Usually `transformers` model load weights via the `load_adapters` API, that depends on PEFT. We need to work a bit to either use this api (for now this would result in some weights not being marked as loaded) or replace modules accordingly.
-
-Hints as to how this would look like:
-
-```python
-class TransformersModel(nn.Module, SupportsLoRA):
-  def __init__(*):
-    ...
-    self.model.load_adapter(vllm_config.load_config.model_loader_extra_config["qlora_adapter_name_or_path"])
-```
-
-Blocker is that you need to specify supported lora layers, when we would ideally want to load whatever is inside the checkpoint!
-
-##### Remote code
-
-This fallback also means that any model on the hub that can be used in `transformers` with `trust_remote_code=True` that correctly implements attention can be used in production!
-
-```python 
-from vllm import LLM
-llm = LLM(model=..., task="generate", trust_remote_code=True)  # Name or path of your model
-llm.apply_model(lambda model: print(model.__class__))
-```
-
-A model just needs the following two things:
-
-```python
-from transformers import PreTrainedModel
-from torch import nn
-
-class MyAttention(nn.Module):
-
-  def forward(self, hidden_states, **kwargs): # <- kwargs are required
-
-    ...
-    attention_interface = attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
-    attn_output, attn_weights = attention_interface(
-      self,
-      query_states,
-      key_states,
-      value_states,
-      **kwargs,
-    )
-    ...
-
-class MyModel(PreTrainedModel):
-  _supports_attention_backend = True
-```
-
-Here is what happens in the background:
-
-1. The config is loaded
-2. `MyModel` python class is loaded from the `auto_map`, and we check that the model `_supports_attention_backend`.
-3. The `TransformersModel` backend is used. See `/model_executors/models/transformers`, which leverage `self.config._attn_implementation = "vllm"`, thus the need to use `ALL_ATTENTION_FUNCTION`.
-
-That's it!
-
-### ModelScope
-
-To use models from [ModelScope](https://www.modelscope.cn) instead of HuggingFace Hub, set an environment variable:
-
-```shell
-export VLLM_USE_MODELSCOPE=True
-```
-
-And use with `trust_remote_code=True`.
-
-```python
-from vllm import LLM
-
-llm = LLM(model=..., revision=..., task=..., trust_remote_code=True)
-
-# For generative models (task=generate) only
-output = llm.generate("Hello, my name is")
-print(output)
-
-# For pooling models (task={embed,classify,reward,score}) only
-output = llm.encode("Hello, my name is")
-print(output)
-```
-
-## List of Text-only Language Models
-
-### Generative Models
-
-See [this page](#generative-models) for more information on how to use generative models.
-
-#### Text Generation (`--task generate`)
-
-:::{list-table}
-:widths: 25 25 50 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-- * `AquilaForCausalLM`
-  * Aquila, Aquila2
-  * `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.
-  * ✅︎
-  * ✅︎
-- * `ArcticForCausalLM`
-  * Arctic
-  * `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc.
-  *
-  * ✅︎
-- * `BaiChuanForCausalLM`
-  * Baichuan2, Baichuan
-  * `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.
-  * ✅︎
-  * ✅︎
-- * `BloomForCausalLM`
-  * BLOOM, BLOOMZ, BLOOMChat
-  * `bigscience/bloom`, `bigscience/bloomz`, etc.
-  *
-  * ✅︎
-- * `BartForConditionalGeneration`
-  * BART
-  * `facebook/bart-base`, `facebook/bart-large-cnn`, etc.
-  *
-  *
-- * `ChatGLMModel`
-  * ChatGLM
-  * `THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.
-  * ✅︎
-  * ✅︎
-- * `CohereForCausalLM`, `Cohere2ForCausalLM`
-  * Command-R
-  * `CohereForAI/c4ai-command-r-v01`, `CohereForAI/c4ai-command-r7b-12-2024`, etc.
-  * ✅︎
-  * ✅︎
-- * `DbrxForCausalLM`
-  * DBRX
-  * `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc.
-  *
-  * ✅︎
-- * `DeciLMForCausalLM`
-  * DeciLM
-  * `Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc.
-  *
-  * ✅︎
-- * `DeepseekForCausalLM`
-  * DeepSeek
-  * `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat` etc.
-  *
-  * ✅︎
-- * `DeepseekV2ForCausalLM`
-  * DeepSeek-V2
-  * `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat` etc.
-  *
-  * ✅︎
-- * `DeepseekV3ForCausalLM`
-  * DeepSeek-V3
-  * `deepseek-ai/DeepSeek-V3-Base`, `deepseek-ai/DeepSeek-V3` etc.
-  *
-  * ✅︎
-- * `ExaoneForCausalLM`
-  * EXAONE-3
-  * `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `FalconForCausalLM`
-  * Falcon
-  * `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.
-  *
-  * ✅︎
-- * `FalconMambaForCausalLM`
-  * FalconMamba
-  * `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `GemmaForCausalLM`
-  * Gemma
-  * `google/gemma-2b`, `google/gemma-7b`, etc.
-  * ✅︎
-  * ✅︎
-- * `Gemma2ForCausalLM`
-  * Gemma2
-  * `google/gemma-2-9b`, `google/gemma-2-27b`, etc.
-  * ✅︎
-  * ✅︎
-- * `GlmForCausalLM`
-  * GLM-4
-  * `THUDM/glm-4-9b-chat-hf`, etc.
-  * ✅︎
-  * ✅︎
-- * `GPT2LMHeadModel`
-  * GPT-2
-  * `gpt2`, `gpt2-xl`, etc.
-  *
-  * ✅︎
-- * `GPTBigCodeForCausalLM`
-  * StarCoder, SantaCoder, WizardCoder
-  * `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc.
-  * ✅︎
-  * ✅︎
-- * `GPTJForCausalLM`
-  * GPT-J
-  * `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.
-  *
-  * ✅︎
-- * `GPTNeoXForCausalLM`
-  * GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM
-  * `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.
-  *
-  * ✅︎
-- * `GraniteForCausalLM`
-  * Granite 3.0, Granite 3.1, PowerLM
-  * `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc.
-  * ✅︎
-  * ✅︎
-- * `GraniteMoeForCausalLM`
-  * Granite 3.0 MoE, PowerMoE
-  * `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc.
-  * ✅︎
-  * ✅︎
-- * `GritLM`
-  * GritLM
-  * `parasail-ai/GritLM-7B-vllm`.
-  * ✅︎
-  * ✅︎
-- * `InternLMForCausalLM`
-  * InternLM
-  * `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.
-  * ✅︎
-  * ✅︎
-- * `InternLM2ForCausalLM`
-  * InternLM2
-  * `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc.
-  * ✅︎
-  * ✅︎
-- * `InternLM3ForCausalLM`
-  * InternLM3
-  * `internlm/internlm3-8b-instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `JAISLMHeadModel`
-  * Jais
-  * `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc.
-  *
-  * ✅︎
-- * `JambaForCausalLM`
-  * Jamba
-  * `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc.
-  * ✅︎
-  * ✅︎
-- * `LlamaForCausalLM`
-  * Llama 3.1, Llama 3, Llama 2, LLaMA, Yi
-  * `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc.
-  * ✅︎
-  * ✅︎
-- * `MambaForCausalLM`
-  * Mamba
-  * `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc.
-  *
-  * ✅︎
-- * `MiniCPMForCausalLM`
-  * MiniCPM
-  * `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc.
-  * ✅︎
-  * ✅︎
-- * `MiniCPM3ForCausalLM`
-  * MiniCPM3
-  * `openbmb/MiniCPM3-4B`, etc.
-  * ✅︎
-  * ✅︎
-- * `MistralForCausalLM`
-  * Mistral, Mistral-Instruct
-  * `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.
-  * ✅︎
-  * ✅︎
-- * `MixtralForCausalLM`
-  * Mixtral-8x7B, Mixtral-8x7B-Instruct
-  * `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc.
-  * ✅︎
-  * ✅︎
-- * `MPTForCausalLM`
-  * MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter
-  * `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc.
-  *
-  * ✅︎
-- * `NemotronForCausalLM`
-  * Nemotron-3, Nemotron-4, Minitron
-  * `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc.
-  * ✅︎
-  * ✅︎
-- * `OLMoForCausalLM`
-  * OLMo
-  * `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc.
-  *
-  * ✅︎
-- * `OLMo2ForCausalLM`
-  * OLMo2
-  * `allenai/OLMo2-7B-1124`, etc.
-  *
-  * ✅︎
-- * `OLMoEForCausalLM`
-  * OLMoE
-  * `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `OPTForCausalLM`
-  * OPT, OPT-IML
-  * `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.
-  *
-  * ✅︎
-- * `OrionForCausalLM`
-  * Orion
-  * `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc.
-  *
-  * ✅︎
-- * `PhiForCausalLM`
-  * Phi
-  * `microsoft/phi-1_5`, `microsoft/phi-2`, etc.
-  * ✅︎
-  * ✅︎
-- * `Phi3ForCausalLM`
-  * Phi-4, Phi-3
-  * `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `Phi3SmallForCausalLM`
-  * Phi-3-Small
-  * `microsoft/Phi-3-small-8k-instruct`, `microsoft/Phi-3-small-128k-instruct`, etc.
-  *
-  * ✅︎
-- * `PhiMoEForCausalLM`
-  * Phi-3.5-MoE
-  * `microsoft/Phi-3.5-MoE-instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `PersimmonForCausalLM`
-  * Persimmon
-  * `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc.
-  *
-  * ✅︎
-- * `QWenLMHeadModel`
-  * Qwen
-  * `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.
-  * ✅︎
-  * ✅︎
-- * `Qwen2ForCausalLM`
-  * QwQ, Qwen2
-  * `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc.
-  * ✅︎
-  * ✅︎
-- * `Qwen2MoeForCausalLM`
-  * Qwen2MoE
-  * `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.
-  *
-  * ✅︎
-- * `StableLmForCausalLM`
-  * StableLM
-  * `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.
-  *
-  * ✅︎
-- * `Starcoder2ForCausalLM`
-  * Starcoder2
-  * `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc.
-  *
-  * ✅︎
-- * `SolarForCausalLM`
-  * Solar Pro
-  * `upstage/solar-pro-preview-instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `TeleChat2ForCausalLM`
-  * TeleChat2
-  * `TeleAI/TeleChat2-3B`, `TeleAI/TeleChat2-7B`, `TeleAI/TeleChat2-35B`, etc.
-  * ✅︎
-  * ✅︎
-- * `XverseForCausalLM`
-  * XVERSE
-  * `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc.
-  * ✅︎
-  * ✅︎
-:::
-
-:::{note}
-Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.
-:::
-
-### Pooling Models
-
-See [this page](pooling-models) for more information on how to use pooling models.
-
-:::{important}
-Since some model architectures support both generative and pooling tasks,
-you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
-:::
-
-#### Text Embedding (`--task embed`)
-
-:::{list-table}
-:widths: 25 25 50 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-- * `BertModel`
-  * BERT-based
-  * `BAAI/bge-base-en-v1.5`, etc.
-  *
-  *
-- * `Gemma2Model`
-  * Gemma2-based
-  * `BAAI/bge-multilingual-gemma2`, etc.
-  *
-  * ✅︎
-- * `GritLM`
-  * GritLM
-  * `parasail-ai/GritLM-7B-vllm`.
-  * ✅︎
-  * ✅︎
-- * `LlamaModel`, `LlamaForCausalLM`, `MistralModel`, etc.
-  * Llama-based
-  * `intfloat/e5-mistral-7b-instruct`, etc.
-  * ✅︎
-  * ✅︎
-- * `Qwen2Model`, `Qwen2ForCausalLM`
-  * Qwen2-based
-  * `ssmits/Qwen2-7B-Instruct-embed-base` (see note), `Alibaba-NLP/gte-Qwen2-7B-instruct` (see note), etc.
-  * ✅︎
-  * ✅︎
-- * `RobertaModel`, `RobertaForMaskedLM`
-  * RoBERTa-based
-  * `sentence-transformers/all-roberta-large-v1`, `sentence-transformers/all-roberta-large-v1`, etc.
-  *
-  *
-- * `XLMRobertaModel`
-  * XLM-RoBERTa-based
-  * `intfloat/multilingual-e5-large`, etc.
-  *
-  *
-:::
-
-:::{note}
-`ssmits/Qwen2-7B-Instruct-embed-base` has an improperly defined Sentence Transformers config.
-You should manually set mean pooling by passing `--override-pooler-config '{"pooling_type": "MEAN"}'`.
-:::
-
-:::{note}
-Unlike base Qwen2, `Alibaba-NLP/gte-Qwen2-7B-instruct` uses bi-directional attention.
-You can set `--hf-overrides '{"is_causal": false}'` to change the attention mask accordingly.
-
-On the other hand, its 1.5B variant (`Alibaba-NLP/gte-Qwen2-1.5B-instruct`) uses causal attention
-despite being described otherwise on its model card.
-
-Regardless of the variant, you need to enable `--trust-remote-code` for the correct tokenizer to be
-loaded. See [relevant issue on HF Transformers](https://github.com/huggingface/transformers/issues/34882).
-:::
-
-If your model is not in the above list, we will try to automatically convert the model using
-{func}`~vllm.model_executor.models.adapters.as_embedding_model`. By default, the embeddings
-of the whole prompt are extracted from the normalized hidden state corresponding to the last token.
-
-#### Reward Modeling (`--task reward`)
-
-:::{list-table}
-:widths: 25 25 50 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-- * `InternLM2ForRewardModel`
-  * InternLM2-based
-  * `internlm/internlm2-1_8b-reward`, `internlm/internlm2-7b-reward`, etc.
-  * ✅︎
-  * ✅︎
-- * `LlamaForCausalLM`
-  * Llama-based
-  * `peiyi9979/math-shepherd-mistral-7b-prm`, etc.
-  * ✅︎
-  * ✅︎
-- * `Qwen2ForRewardModel`
-  * Qwen2-based
-  * `Qwen/Qwen2.5-Math-RM-72B`, etc.
-  * ✅︎
-  * ✅︎
-- * `Qwen2ForProcessRewardModel`
-  * Qwen2-based
-  * `Qwen/Qwen2.5-Math-PRM-7B`, `Qwen/Qwen2.5-Math-PRM-72B`, etc.
-  * ✅︎
-  * ✅︎
-:::
-
-If your model is not in the above list, we will try to automatically convert the model using
-{func}`~vllm.model_executor.models.adapters.as_reward_model`. By default, we return the hidden states of each token directly.
-
-:::{important}
-For process-supervised reward models such as `peiyi9979/math-shepherd-mistral-7b-prm`, the pooling config should be set explicitly,
-e.g.: `--override-pooler-config '{"pooling_type": "STEP", "step_tag_id": 123, "returned_token_ids": [456, 789]}'`.
-:::
-
-#### Classification (`--task classify`)
-
-:::{list-table}
-:widths: 25 25 50 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-- * `JambaForSequenceClassification`
-  * Jamba
-  * `ai21labs/Jamba-tiny-reward-dev`, etc.
-  * ✅︎
-  * ✅︎
-- * `Qwen2ForSequenceClassification`
-  * Qwen2-based
-  * `jason9693/Qwen2.5-1.5B-apeach`, etc.
-  * ✅︎
-  * ✅︎
-:::
-
-If your model is not in the above list, we will try to automatically convert the model using
-{func}`~vllm.model_executor.models.adapters.as_classification_model`. By default, the class probabilities are extracted from the softmaxed hidden state corresponding to the last token.
-
-#### Sentence Pair Scoring (`--task score`)
-
-:::{list-table}
-:widths: 25 25 50 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-- * `BertForSequenceClassification`
-  * BERT-based
-  * `cross-encoder/ms-marco-MiniLM-L-6-v2`, etc.
-  *
-  *
-- * `RobertaForSequenceClassification`
-  * RoBERTa-based
-  * `cross-encoder/quora-roberta-base`, etc.
-  *
-  *
-- * `XLMRobertaForSequenceClassification`
-  * XLM-RoBERTa-based
-  * `BAAI/bge-reranker-v2-m3`, etc.
-  *
-  *
-:::
-
-(supported-mm-models)=
-
-## List of Multimodal Language Models
-
-The following modalities are supported depending on the model:
-
-- **T**ext
-- **I**mage
-- **V**ideo
-- **A**udio
-
-Any combination of modalities joined by `+` are supported.
-
-- e.g.: `T + I` means that the model supports text-only, image-only, and text-with-image inputs.
-
-On the other hand, modalities separated by `/` are mutually exclusive.
-
-- e.g.: `T / I` means that the model supports text-only and image-only inputs, but not text-with-image inputs.
-
-See [this page](#multimodal-inputs) on how to pass multi-modal inputs to the model.
-
-:::{important}
-To enable multiple multi-modal items per text prompt, you have to set `limit_mm_per_prompt` (offline inference)
-or `--limit-mm-per-prompt` (online serving). For example, to enable passing up to 4 images per text prompt:
-
-Offline inference:
-
-```python
-llm = LLM(
-    model="Qwen/Qwen2-VL-7B-Instruct",
-    limit_mm_per_prompt={"image": 4},
-)
-```
-
-Online serving:
-
-```bash
-vllm serve Qwen/Qwen2-VL-7B-Instruct --limit-mm-per-prompt image=4
-```
-
-:::
-
-:::{note}
-vLLM currently only supports adding LoRA to the language backbone of multimodal models.
-:::
-
-### Generative Models
-
-See [this page](#generative-models) for more information on how to use generative models.
-
-#### Text Generation (`--task generate`)
-
-:::{list-table}
-:widths: 25 25 15 20 5 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Inputs
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-  * [V1](gh-issue:8779)
-- * `AriaForConditionalGeneration`
-  * Aria
-  * T + I<sup>+</sup>
-  * `rhymes-ai/Aria`
-  *
-  * ✅︎
-  * ✅︎
-- * `Blip2ForConditionalGeneration`
-  * BLIP-2
-  * T + I<sup>E</sup>
-  * `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `ChameleonForConditionalGeneration`
-  * Chameleon
-  * T + I
-  * `facebook/chameleon-7b` etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `DeepseekVLV2ForCausalLM`
-  * DeepSeek-VL2
-  * T + I<sup>+</sup>
-  * `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2` etc. (see note)
-  *
-  * ✅︎
-  * ✅︎
-- * `FuyuForCausalLM`
-  * Fuyu
-  * T + I
-  * `adept/fuyu-8b` etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `ChatGLMModel`
-  * GLM-4V
-  * T + I
-  * `THUDM/glm-4v-9b` etc.
-  * ✅︎
-  * ✅︎
-  *
-- * `H2OVLChatModel`
-  * H2OVL
-  * T + I<sup>E+</sup>
-  * `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc.
-  *
-  * ✅︎
-  * \*
-- * `Idefics3ForConditionalGeneration`
-  * Idefics3
-  * T + I
-  * `HuggingFaceM4/Idefics3-8B-Llama3` etc.
-  * ✅︎
-  *
-  * ✅︎
-- * `InternVLChatModel`
-  * InternVL 2.5, Mono-InternVL, InternVL 2.0
-  * T + I<sup>E+</sup>
-  * `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `LlavaForConditionalGeneration`
-  * LLaVA-1.5
-  * T + I<sup>E+</sup>
-  * `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `LlavaNextForConditionalGeneration`
-  * LLaVA-NeXT
-  * T + I<sup>E+</sup>
-  * `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `LlavaNextVideoForConditionalGeneration`
-  * LLaVA-NeXT-Video
-  * T + V
-  * `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `LlavaOnevisionForConditionalGeneration`
-  * LLaVA-Onevision
-  * T + I<sup>+</sup> + V<sup>+</sup>
-  * `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `MiniCPMO`
-  * MiniCPM-O
-  * T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup>
-  * `openbmb/MiniCPM-o-2_6`, etc.
-  * ✅︎
-  * ✅︎
-  *
-- * `MiniCPMV`
-  * MiniCPM-V
-  * T + I<sup>E+</sup> + V<sup>E+</sup>
-  * `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc.
-  * ✅︎
-  * ✅︎
-  *
-- * `MllamaForConditionalGeneration`
-  * Llama 3.2
-  * T + I<sup>+</sup>
-  * `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc.
-  *
-  *
-  *
-- * `MolmoForCausalLM`
-  * Molmo
-  * T + I
-  * `allenai/Molmo-7B-D-0924`, `allenai/Molmo-72B-0924`, etc.
-  * ✅︎
-  * ✅︎
-  * ✅︎
-- * `NVLM_D_Model`
-  * NVLM-D 1.0
-  * T + I<sup>+</sup>
-  * `nvidia/NVLM-D-72B`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `PaliGemmaForConditionalGeneration`
-  * PaliGemma, PaliGemma 2
-  * T + I<sup>E</sup>
-  * `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc.
-  *
-  * ✅︎
-  *
-- * `Phi3VForCausalLM`
-  * Phi-3-Vision, Phi-3.5-Vision
-  * T + I<sup>E+</sup>
-  * `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `PixtralForConditionalGeneration`
-  * Pixtral
-  * T + I<sup>+</sup>
-  * `mistralai/Pixtral-12B-2409`, `mistral-community/pixtral-12b` (see note), etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `QWenLMHeadModel`
-  * Qwen-VL
-  * T + I<sup>E+</sup>
-  * `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc.
-  * ✅︎
-  * ✅︎
-  * ✅︎
-- * `Qwen2AudioForConditionalGeneration`
-  * Qwen2-Audio
-  * T + A<sup>+</sup>
-  * `Qwen/Qwen2-Audio-7B-Instruct`
-  *
-  * ✅︎
-  * ✅︎
-- * `Qwen2VLForConditionalGeneration`
-  * QVQ, Qwen2-VL
-  * T + I<sup>E+</sup> + V<sup>E+</sup>
-  * `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc.
-  * ✅︎
-  * ✅︎
-  * ✅︎
-- * `Qwen2_5_VLForConditionalGeneration`
-  * Qwen2.5-VL
-  * T + I<sup>E+</sup> + V<sup>E+</sup>
-  * `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc.
-  *
-  * ✅︎
-  * ✅︎
-- * `UltravoxModel`
-  * Ultravox
-  * T + A<sup>E+</sup>
-  * `fixie-ai/ultravox-v0_3`
-  * ✅︎
-  * ✅︎
-  * ✅︎
-:::
-
-<sup>E</sup> Pre-computed embeddings can be inputted for this modality.  
-<sup>+</sup> Multiple items can be inputted per text prompt for this modality.
-
-:::{note}
-To use DeepSeek-VL2 series models, you have to pass `--hf_overrides '{"architectures": ["DeepseekVLV2ForCausalLM"]}'` when running vLLM.
-:::
-
-:::{note}
-H2O-VL series models will be available in V1 once we support backends other than FlashAttention.
-:::
-
-:::{note}
-To use `TIGER-Lab/Mantis-8B-siglip-llama3`, you have to pass `--hf_overrides '{"architectures": ["MantisForConditionalGeneration"]}'` when running vLLM.
-:::
-
-:::{note}
-The official `openbmb/MiniCPM-V-2` doesn't work yet, so we need to use a fork (`HwwwH/MiniCPM-V-2`) for now.
-For more details, please see: <gh-pr:4087#issuecomment-2250397630>
-:::
-
-:::{note}
-`mistral-community/pixtral-12b` does not support V1 yet.
-:::
-
-:::{note}
-To use Qwen2.5-VL series models, you have to install Huggingface `transformers` library from source via `pip install git+https://github.com/huggingface/transformers`.
-:::
-
-### Pooling Models
-
-See [this page](pooling-models) for more information on how to use pooling models.
-
-:::{important}
-Since some model architectures support both generative and pooling tasks,
-you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
-:::
-
-#### Text Embedding (`--task embed`)
-
-Any text generation model can be converted into an embedding model by passing `--task embed`.
-
-:::{note}
-To get the best results, you should use pooling models that are specifically trained as such.
-:::
-
-The following table lists those that are tested in vLLM.
-
-:::{list-table}
-:widths: 25 25 15 25 5 5
-:header-rows: 1
-
-- * Architecture
-  * Models
-  * Inputs
-  * Example HF Models
-  * [LoRA](#lora-adapter)
-  * [PP](#distributed-serving)
-- * `LlavaNextForConditionalGeneration`
-  * LLaVA-NeXT-based
-  * T / I
-  * `royokong/e5-v`
-  *
-  * ✅︎
-- * `Phi3VForCausalLM`
-  * Phi-3-Vision-based
-  * T + I
-  * `TIGER-Lab/VLM2Vec-Full`
-  * 🚧
-  * ✅︎
-- * `Qwen2VLForConditionalGeneration`
-  * Qwen2-VL-based
-  * T + I
-  * `MrLight/dse-qwen2-2b-mrl-v1`
-  *
-  * ✅︎
-:::
-
-_________________
-
-## Model Support Policy
-
-At vLLM, we are committed to facilitating the integration and support of third-party models within our ecosystem. Our approach is designed to balance the need for robustness and the practical limitations of supporting a wide range of models. Here’s how we manage third-party model support:
-
-1. **Community-Driven Support**: We encourage community contributions for adding new models. When a user requests support for a new model, we welcome pull requests (PRs) from the community. These contributions are evaluated primarily on the sensibility of the output they generate, rather than strict consistency with existing implementations such as those in transformers. **Call for contribution:** PRs coming directly from model vendors are greatly appreciated!
-
-2. **Best-Effort Consistency**: While we aim to maintain a level of consistency between the models implemented in vLLM and other frameworks like transformers, complete alignment is not always feasible. Factors like acceleration techniques and the use of low-precision computations can introduce discrepancies. Our commitment is to ensure that the implemented models are functional and produce sensible results.
-
-    :::{tip}
-    When comparing the output of `model.generate` from HuggingFace Transformers with the output of `llm.generate` from vLLM, note that the former reads the model's generation config file (i.e., [generation_config.json](https://github.com/huggingface/transformers/blob/19dabe96362803fb0a9ae7073d03533966598b17/src/transformers/generation/utils.py#L1945)) and applies the default parameters for generation, while the latter only uses the parameters passed to the function. Ensure all sampling parameters are identical when comparing outputs.
-    :::
-
-3. **Issue Resolution and Model Updates**: Users are encouraged to report any bugs or issues they encounter with third-party models. Proposed fixes should be submitted via PRs, with a clear explanation of the problem and the rationale behind the proposed solution. If a fix for one model impacts another, we rely on the community to highlight and address these cross-model dependencies. Note: for bugfix PRs, it is good etiquette to inform the original author to seek their feedback.
-
-4. **Monitoring and Updates**: Users interested in specific models should monitor the commit history for those models (e.g., by tracking changes in the main/vllm/model_executor/models directory). This proactive approach helps users stay informed about updates and changes that may affect the models they use.
-
-5. **Selective Focus**: Our resources are primarily directed towards models with significant user interest and impact. Models that are less frequently used may receive less attention, and we rely on the community to play a more active role in their upkeep and improvement.
-
-Through this approach, vLLM fosters a collaborative environment where both the core development team and the broader community contribute to the robustness and diversity of the third-party models supported in our ecosystem.
-
-Note that, as an inference engine, vLLM does not introduce new models. Therefore, all models supported by vLLM are third-party models in this regard.
-
-We have the following levels of testing for models:
-
-1. **Strict Consistency**: We compare the output of the model with the output of the model in the HuggingFace Transformers library under greedy decoding. This is the most stringent test. Please refer to [models tests](https://github.com/vllm-project/vllm/blob/main/tests/models) for the models that have passed this test.
-2. **Output Sensibility**: We check if the output of the model is sensible and coherent, by measuring the perplexity of the output and checking for any obvious errors. This is a less stringent test.
-3. **Runtime Functionality**: We check if the model can be loaded and run without errors. This is the least stringent test. Please refer to [functionality tests](gh-dir:tests) and [examples](gh-dir:main/examples) for the models that have passed this test.
-4. **Community Feedback**: We rely on the community to provide feedback on the models. If a model is broken or not working as expected, we encourage users to raise issues to report it or open pull requests to fix it. The rest of the models fall under this category.
+(supported-models)=
+
+# List of Supported Models
+
+vLLM supports generative and pooling models across various tasks.
+If a model supports more than one task, you can set the task via the `--task` argument.
+
+For each task, we list the model architectures that have been implemented in vLLM.
+Alongside each architecture, we include some popular models that use it.
+
+## Loading a Model
+
+### HuggingFace Hub
+
+By default, vLLM loads models from [HuggingFace (HF) Hub](https://huggingface.co/models).
+
+To determine whether a given model is supported, you can check the `config.json` file inside the HF repository.
+If the `"architectures"` field contains a model architecture listed below, then it should be supported in theory.
+
+:::{tip}
+The easiest way to check if your model is really supported at runtime is to run the program below:
+
+```python
+from vllm import LLM
+
+# For generative models (task=generate) only
+llm = LLM(model=..., task="generate")  # Name or path of your model
+output = llm.generate("Hello, my name is")
+print(output)
+
+# For pooling models (task={embed,classify,reward,score}) only
+llm = LLM(model=..., task="embed")  # Name or path of your model
+output = llm.encode("Hello, my name is")
+print(output)
+```
+
+If vLLM successfully returns text (for generative models) or hidden states (for pooling models), it indicates that your model is supported.
+:::
+
+Otherwise, please refer to [Adding a New Model](#new-model) for instructions on how to implement your model in vLLM.
+Alternatively, you can [open an issue on GitHub](https://github.com/vllm-project/vllm/issues/new/choose) to request vLLM support.
+
+### Transformers fallback
+
+After the merge of <gh-pr:11330>, `vllm` can fallback to models that are available in `transformers`. This does not work for all models for now, but most decoder language models are supported, and vision language model support is planned!
+
+To check if the backend is `transformers`, you can simply do this:
+
+```python 
+from vllm import LLM
+llm = LLM(model=..., task="generate")  # Name or path of your model
+llm.apply_model(lambda model: print(model.__class__))
+```
+
+If it is `TransformersModel` then it means it's based on `transformers`!
+
+#### Supported features
+
+##### LORA and quantization
+
+Both are not supported yet! Make sure to open an issue and we'll work on this together with the `transformers` team!
+
+Usually `transformers` model load weights via the `load_adapters` API, that depends on PEFT. We need to work a bit to either use this api (for now this would result in some weights not being marked as loaded) or replace modules accordingly.
+
+Hints as to how this would look like:
+
+```python
+class TransformersModel(nn.Module, SupportsLoRA):
+  def __init__(*):
+    ...
+    self.model.load_adapter(vllm_config.load_config.model_loader_extra_config["qlora_adapter_name_or_path"])
+```
+
+Blocker is that you need to specify supported lora layers, when we would ideally want to load whatever is inside the checkpoint!
+
+##### Remote code
+
+This fallback also means that any model on the hub that can be used in `transformers` with `trust_remote_code=True` that correctly implements attention can be used in production!
+
+```python 
+from vllm import LLM
+llm = LLM(model=..., task="generate", trust_remote_code=True)  # Name or path of your model
+llm.apply_model(lambda model: print(model.__class__))
+```
+
+A model just needs the following two things:
+
+```python
+from transformers import PreTrainedModel
+from torch import nn
+
+class MyAttention(nn.Module):
+
+  def forward(self, hidden_states, **kwargs): # <- kwargs are required
+
+    ...
+    attention_interface = attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
+    attn_output, attn_weights = attention_interface(
+      self,
+      query_states,
+      key_states,
+      value_states,
+      **kwargs,
+    )
+    ...
+
+class MyModel(PreTrainedModel):
+  _supports_attention_backend = True
+```
+
+Here is what happens in the background:
+
+1. The config is loaded
+2. `MyModel` python class is loaded from the `auto_map`, and we check that the model `_supports_attention_backend`.
+3. The `TransformersModel` backend is used. See `/model_executors/models/transformers`, which leverage `self.config._attn_implementation = "vllm"`, thus the need to use `ALL_ATTENTION_FUNCTION`.
+
+That's it!
+
+### ModelScope
+
+To use models from [ModelScope](https://www.modelscope.cn) instead of HuggingFace Hub, set an environment variable:
+
+```shell
+export VLLM_USE_MODELSCOPE=True
+```
+
+And use with `trust_remote_code=True`.
+
+```python
+from vllm import LLM
+
+llm = LLM(model=..., revision=..., task=..., trust_remote_code=True)
+
+# For generative models (task=generate) only
+output = llm.generate("Hello, my name is")
+print(output)
+
+# For pooling models (task={embed,classify,reward,score}) only
+output = llm.encode("Hello, my name is")
+print(output)
+```
+
+## List of Text-only Language Models
+
+### Generative Models
+
+See [this page](#generative-models) for more information on how to use generative models.
+
+#### Text Generation (`--task generate`)
+
+:::{list-table}
+:widths: 25 25 50 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+- * `AquilaForCausalLM`
+  * Aquila, Aquila2
+  * `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.
+  * ✅︎
+  * ✅︎
+- * `ArcticForCausalLM`
+  * Arctic
+  * `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc.
+  *
+  * ✅︎
+- * `BaiChuanForCausalLM`
+  * Baichuan2, Baichuan
+  * `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.
+  * ✅︎
+  * ✅︎
+- * `BloomForCausalLM`
+  * BLOOM, BLOOMZ, BLOOMChat
+  * `bigscience/bloom`, `bigscience/bloomz`, etc.
+  *
+  * ✅︎
+- * `BartForConditionalGeneration`
+  * BART
+  * `facebook/bart-base`, `facebook/bart-large-cnn`, etc.
+  *
+  *
+- * `ChatGLMModel`
+  * ChatGLM
+  * `THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.
+  * ✅︎
+  * ✅︎
+- * `CohereForCausalLM`, `Cohere2ForCausalLM`
+  * Command-R
+  * `CohereForAI/c4ai-command-r-v01`, `CohereForAI/c4ai-command-r7b-12-2024`, etc.
+  * ✅︎
+  * ✅︎
+- * `DbrxForCausalLM`
+  * DBRX
+  * `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc.
+  *
+  * ✅︎
+- * `DeciLMForCausalLM`
+  * DeciLM
+  * `Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc.
+  *
+  * ✅︎
+- * `DeepseekForCausalLM`
+  * DeepSeek
+  * `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat` etc.
+  *
+  * ✅︎
+- * `DeepseekV2ForCausalLM`
+  * DeepSeek-V2
+  * `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat` etc.
+  *
+  * ✅︎
+- * `DeepseekV3ForCausalLM`
+  * DeepSeek-V3
+  * `deepseek-ai/DeepSeek-V3-Base`, `deepseek-ai/DeepSeek-V3` etc.
+  *
+  * ✅︎
+- * `ExaoneForCausalLM`
+  * EXAONE-3
+  * `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `FalconForCausalLM`
+  * Falcon
+  * `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.
+  *
+  * ✅︎
+- * `FalconMambaForCausalLM`
+  * FalconMamba
+  * `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `GemmaForCausalLM`
+  * Gemma
+  * `google/gemma-2b`, `google/gemma-7b`, etc.
+  * ✅︎
+  * ✅︎
+- * `Gemma2ForCausalLM`
+  * Gemma2
+  * `google/gemma-2-9b`, `google/gemma-2-27b`, etc.
+  * ✅︎
+  * ✅︎
+- * `GlmForCausalLM`
+  * GLM-4
+  * `THUDM/glm-4-9b-chat-hf`, etc.
+  * ✅︎
+  * ✅︎
+- * `GPT2LMHeadModel`
+  * GPT-2
+  * `gpt2`, `gpt2-xl`, etc.
+  *
+  * ✅︎
+- * `GPTBigCodeForCausalLM`
+  * StarCoder, SantaCoder, WizardCoder
+  * `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc.
+  * ✅︎
+  * ✅︎
+- * `GPTJForCausalLM`
+  * GPT-J
+  * `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.
+  *
+  * ✅︎
+- * `GPTNeoXForCausalLM`
+  * GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM
+  * `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.
+  *
+  * ✅︎
+- * `GraniteForCausalLM`
+  * Granite 3.0, Granite 3.1, PowerLM
+  * `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc.
+  * ✅︎
+  * ✅︎
+- * `GraniteMoeForCausalLM`
+  * Granite 3.0 MoE, PowerMoE
+  * `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc.
+  * ✅︎
+  * ✅︎
+- * `GritLM`
+  * GritLM
+  * `parasail-ai/GritLM-7B-vllm`.
+  * ✅︎
+  * ✅︎
+- * `InternLMForCausalLM`
+  * InternLM
+  * `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.
+  * ✅︎
+  * ✅︎
+- * `InternLM2ForCausalLM`
+  * InternLM2
+  * `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc.
+  * ✅︎
+  * ✅︎
+- * `InternLM3ForCausalLM`
+  * InternLM3
+  * `internlm/internlm3-8b-instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `JAISLMHeadModel`
+  * Jais
+  * `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc.
+  *
+  * ✅︎
+- * `JambaForCausalLM`
+  * Jamba
+  * `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc.
+  * ✅︎
+  * ✅︎
+- * `LlamaForCausalLM`
+  * Llama 3.1, Llama 3, Llama 2, LLaMA, Yi
+  * `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc.
+  * ✅︎
+  * ✅︎
+- * `MambaForCausalLM`
+  * Mamba
+  * `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc.
+  *
+  * ✅︎
+- * `MiniCPMForCausalLM`
+  * MiniCPM
+  * `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc.
+  * ✅︎
+  * ✅︎
+- * `MiniCPM3ForCausalLM`
+  * MiniCPM3
+  * `openbmb/MiniCPM3-4B`, etc.
+  * ✅︎
+  * ✅︎
+- * `MistralForCausalLM`
+  * Mistral, Mistral-Instruct
+  * `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.
+  * ✅︎
+  * ✅︎
+- * `MixtralForCausalLM`
+  * Mixtral-8x7B, Mixtral-8x7B-Instruct
+  * `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc.
+  * ✅︎
+  * ✅︎
+- * `MPTForCausalLM`
+  * MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter
+  * `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc.
+  *
+  * ✅︎
+- * `NemotronForCausalLM`
+  * Nemotron-3, Nemotron-4, Minitron
+  * `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc.
+  * ✅︎
+  * ✅︎
+- * `OLMoForCausalLM`
+  * OLMo
+  * `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc.
+  *
+  * ✅︎
+- * `OLMo2ForCausalLM`
+  * OLMo2
+  * `allenai/OLMo2-7B-1124`, etc.
+  *
+  * ✅︎
+- * `OLMoEForCausalLM`
+  * OLMoE
+  * `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `OPTForCausalLM`
+  * OPT, OPT-IML
+  * `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.
+  *
+  * ✅︎
+- * `OrionForCausalLM`
+  * Orion
+  * `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc.
+  *
+  * ✅︎
+- * `PhiForCausalLM`
+  * Phi
+  * `microsoft/phi-1_5`, `microsoft/phi-2`, etc.
+  * ✅︎
+  * ✅︎
+- * `Phi3ForCausalLM`
+  * Phi-4, Phi-3
+  * `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `Phi3SmallForCausalLM`
+  * Phi-3-Small
+  * `microsoft/Phi-3-small-8k-instruct`, `microsoft/Phi-3-small-128k-instruct`, etc.
+  *
+  * ✅︎
+- * `PhiMoEForCausalLM`
+  * Phi-3.5-MoE
+  * `microsoft/Phi-3.5-MoE-instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `PersimmonForCausalLM`
+  * Persimmon
+  * `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc.
+  *
+  * ✅︎
+- * `QWenLMHeadModel`
+  * Qwen
+  * `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen2ForCausalLM`
+  * QwQ, Qwen2
+  * `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen2MoeForCausalLM`
+  * Qwen2MoE
+  * `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.
+  *
+  * ✅︎
+  - * `Qwen3ForCausalLM`
+  * Qwen3
+  * `Qwen/Qwen3-8B`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen3MoeForCausalLM`
+  * Qwen3MoE
+  * `Qwen/Qwen3-30B-A3B`, etc.
+  * ✅︎
+  * ✅︎
+- * `StableLmForCausalLM`
+  * StableLM
+  * `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.
+  *
+  * ✅︎
+- * `Starcoder2ForCausalLM`
+  * Starcoder2
+  * `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc.
+  *
+  * ✅︎
+- * `SolarForCausalLM`
+  * Solar Pro
+  * `upstage/solar-pro-preview-instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `TeleChat2ForCausalLM`
+  * TeleChat2
+  * `TeleAI/TeleChat2-3B`, `TeleAI/TeleChat2-7B`, `TeleAI/TeleChat2-35B`, etc.
+  * ✅︎
+  * ✅︎
+- * `XverseForCausalLM`
+  * XVERSE
+  * `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc.
+  * ✅︎
+  * ✅︎
+:::
+
+:::{note}
+Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.
+:::
+
+### Pooling Models
+
+See [this page](pooling-models) for more information on how to use pooling models.
+
+:::{important}
+Since some model architectures support both generative and pooling tasks,
+you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
+:::
+
+#### Text Embedding (`--task embed`)
+
+:::{list-table}
+:widths: 25 25 50 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+- * `BertModel`
+  * BERT-based
+  * `BAAI/bge-base-en-v1.5`, etc.
+  *
+  *
+- * `Gemma2Model`
+  * Gemma2-based
+  * `BAAI/bge-multilingual-gemma2`, etc.
+  *
+  * ✅︎
+- * `GritLM`
+  * GritLM
+  * `parasail-ai/GritLM-7B-vllm`.
+  * ✅︎
+  * ✅︎
+- * `LlamaModel`, `LlamaForCausalLM`, `MistralModel`, etc.
+  * Llama-based
+  * `intfloat/e5-mistral-7b-instruct`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen2Model`, `Qwen2ForCausalLM`
+  * Qwen2-based
+  * `ssmits/Qwen2-7B-Instruct-embed-base` (see note), `Alibaba-NLP/gte-Qwen2-7B-instruct` (see note), etc.
+  * ✅︎
+  * ✅︎
+- * `RobertaModel`, `RobertaForMaskedLM`
+  * RoBERTa-based
+  * `sentence-transformers/all-roberta-large-v1`, `sentence-transformers/all-roberta-large-v1`, etc.
+  *
+  *
+- * `XLMRobertaModel`
+  * XLM-RoBERTa-based
+  * `intfloat/multilingual-e5-large`, etc.
+  *
+  *
+:::
+
+:::{note}
+`ssmits/Qwen2-7B-Instruct-embed-base` has an improperly defined Sentence Transformers config.
+You should manually set mean pooling by passing `--override-pooler-config '{"pooling_type": "MEAN"}'`.
+:::
+
+:::{note}
+Unlike base Qwen2, `Alibaba-NLP/gte-Qwen2-7B-instruct` uses bi-directional attention.
+You can set `--hf-overrides '{"is_causal": false}'` to change the attention mask accordingly.
+
+On the other hand, its 1.5B variant (`Alibaba-NLP/gte-Qwen2-1.5B-instruct`) uses causal attention
+despite being described otherwise on its model card.
+
+Regardless of the variant, you need to enable `--trust-remote-code` for the correct tokenizer to be
+loaded. See [relevant issue on HF Transformers](https://github.com/huggingface/transformers/issues/34882).
+:::
+
+If your model is not in the above list, we will try to automatically convert the model using
+{func}`~vllm.model_executor.models.adapters.as_embedding_model`. By default, the embeddings
+of the whole prompt are extracted from the normalized hidden state corresponding to the last token.
+
+#### Reward Modeling (`--task reward`)
+
+:::{list-table}
+:widths: 25 25 50 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+- * `InternLM2ForRewardModel`
+  * InternLM2-based
+  * `internlm/internlm2-1_8b-reward`, `internlm/internlm2-7b-reward`, etc.
+  * ✅︎
+  * ✅︎
+- * `LlamaForCausalLM`
+  * Llama-based
+  * `peiyi9979/math-shepherd-mistral-7b-prm`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen2ForRewardModel`
+  * Qwen2-based
+  * `Qwen/Qwen2.5-Math-RM-72B`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen2ForProcessRewardModel`
+  * Qwen2-based
+  * `Qwen/Qwen2.5-Math-PRM-7B`, `Qwen/Qwen2.5-Math-PRM-72B`, etc.
+  * ✅︎
+  * ✅︎
+:::
+
+If your model is not in the above list, we will try to automatically convert the model using
+{func}`~vllm.model_executor.models.adapters.as_reward_model`. By default, we return the hidden states of each token directly.
+
+:::{important}
+For process-supervised reward models such as `peiyi9979/math-shepherd-mistral-7b-prm`, the pooling config should be set explicitly,
+e.g.: `--override-pooler-config '{"pooling_type": "STEP", "step_tag_id": 123, "returned_token_ids": [456, 789]}'`.
+:::
+
+#### Classification (`--task classify`)
+
+:::{list-table}
+:widths: 25 25 50 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+- * `JambaForSequenceClassification`
+  * Jamba
+  * `ai21labs/Jamba-tiny-reward-dev`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen2ForSequenceClassification`
+  * Qwen2-based
+  * `jason9693/Qwen2.5-1.5B-apeach`, etc.
+  * ✅︎
+  * ✅︎
+:::
+
+If your model is not in the above list, we will try to automatically convert the model using
+{func}`~vllm.model_executor.models.adapters.as_classification_model`. By default, the class probabilities are extracted from the softmaxed hidden state corresponding to the last token.
+
+#### Sentence Pair Scoring (`--task score`)
+
+:::{list-table}
+:widths: 25 25 50 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+- * `BertForSequenceClassification`
+  * BERT-based
+  * `cross-encoder/ms-marco-MiniLM-L-6-v2`, etc.
+  *
+  *
+- * `RobertaForSequenceClassification`
+  * RoBERTa-based
+  * `cross-encoder/quora-roberta-base`, etc.
+  *
+  *
+- * `XLMRobertaForSequenceClassification`
+  * XLM-RoBERTa-based
+  * `BAAI/bge-reranker-v2-m3`, etc.
+  *
+  *
+:::
+
+(supported-mm-models)=
+
+## List of Multimodal Language Models
+
+The following modalities are supported depending on the model:
+
+- **T**ext
+- **I**mage
+- **V**ideo
+- **A**udio
+
+Any combination of modalities joined by `+` are supported.
+
+- e.g.: `T + I` means that the model supports text-only, image-only, and text-with-image inputs.
+
+On the other hand, modalities separated by `/` are mutually exclusive.
+
+- e.g.: `T / I` means that the model supports text-only and image-only inputs, but not text-with-image inputs.
+
+See [this page](#multimodal-inputs) on how to pass multi-modal inputs to the model.
+
+:::{important}
+To enable multiple multi-modal items per text prompt, you have to set `limit_mm_per_prompt` (offline inference)
+or `--limit-mm-per-prompt` (online serving). For example, to enable passing up to 4 images per text prompt:
+
+Offline inference:
+
+```python
+llm = LLM(
+    model="Qwen/Qwen2-VL-7B-Instruct",
+    limit_mm_per_prompt={"image": 4},
+)
+```
+
+Online serving:
+
+```bash
+vllm serve Qwen/Qwen2-VL-7B-Instruct --limit-mm-per-prompt image=4
+```
+
+:::
+
+:::{note}
+vLLM currently only supports adding LoRA to the language backbone of multimodal models.
+:::
+
+### Generative Models
+
+See [this page](#generative-models) for more information on how to use generative models.
+
+#### Text Generation (`--task generate`)
+
+:::{list-table}
+:widths: 25 25 15 20 5 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Inputs
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+  * [V1](gh-issue:8779)
+- * `AriaForConditionalGeneration`
+  * Aria
+  * T + I<sup>+</sup>
+  * `rhymes-ai/Aria`
+  *
+  * ✅︎
+  * ✅︎
+- * `Blip2ForConditionalGeneration`
+  * BLIP-2
+  * T + I<sup>E</sup>
+  * `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `ChameleonForConditionalGeneration`
+  * Chameleon
+  * T + I
+  * `facebook/chameleon-7b` etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `DeepseekVLV2ForCausalLM`
+  * DeepSeek-VL2
+  * T + I<sup>+</sup>
+  * `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2` etc. (see note)
+  *
+  * ✅︎
+  * ✅︎
+- * `FuyuForCausalLM`
+  * Fuyu
+  * T + I
+  * `adept/fuyu-8b` etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `ChatGLMModel`
+  * GLM-4V
+  * T + I
+  * `THUDM/glm-4v-9b` etc.
+  * ✅︎
+  * ✅︎
+  *
+- * `H2OVLChatModel`
+  * H2OVL
+  * T + I<sup>E+</sup>
+  * `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc.
+  *
+  * ✅︎
+  * \*
+- * `Idefics3ForConditionalGeneration`
+  * Idefics3
+  * T + I
+  * `HuggingFaceM4/Idefics3-8B-Llama3` etc.
+  * ✅︎
+  *
+  * ✅︎
+- * `InternVLChatModel`
+  * InternVL 2.5, Mono-InternVL, InternVL 2.0
+  * T + I<sup>E+</sup>
+  * `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `LlavaForConditionalGeneration`
+  * LLaVA-1.5
+  * T + I<sup>E+</sup>
+  * `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `LlavaNextForConditionalGeneration`
+  * LLaVA-NeXT
+  * T + I<sup>E+</sup>
+  * `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `LlavaNextVideoForConditionalGeneration`
+  * LLaVA-NeXT-Video
+  * T + V
+  * `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `LlavaOnevisionForConditionalGeneration`
+  * LLaVA-Onevision
+  * T + I<sup>+</sup> + V<sup>+</sup>
+  * `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `MiniCPMO`
+  * MiniCPM-O
+  * T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup>
+  * `openbmb/MiniCPM-o-2_6`, etc.
+  * ✅︎
+  * ✅︎
+  *
+- * `MiniCPMV`
+  * MiniCPM-V
+  * T + I<sup>E+</sup> + V<sup>E+</sup>
+  * `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc.
+  * ✅︎
+  * ✅︎
+  *
+- * `MllamaForConditionalGeneration`
+  * Llama 3.2
+  * T + I<sup>+</sup>
+  * `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc.
+  *
+  *
+  *
+- * `MolmoForCausalLM`
+  * Molmo
+  * T + I
+  * `allenai/Molmo-7B-D-0924`, `allenai/Molmo-72B-0924`, etc.
+  * ✅︎
+  * ✅︎
+  * ✅︎
+- * `NVLM_D_Model`
+  * NVLM-D 1.0
+  * T + I<sup>+</sup>
+  * `nvidia/NVLM-D-72B`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `PaliGemmaForConditionalGeneration`
+  * PaliGemma, PaliGemma 2
+  * T + I<sup>E</sup>
+  * `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc.
+  *
+  * ✅︎
+  *
+- * `Phi3VForCausalLM`
+  * Phi-3-Vision, Phi-3.5-Vision
+  * T + I<sup>E+</sup>
+  * `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `PixtralForConditionalGeneration`
+  * Pixtral
+  * T + I<sup>+</sup>
+  * `mistralai/Pixtral-12B-2409`, `mistral-community/pixtral-12b` (see note), etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `QWenLMHeadModel`
+  * Qwen-VL
+  * T + I<sup>E+</sup>
+  * `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc.
+  * ✅︎
+  * ✅︎
+  * ✅︎
+- * `Qwen2AudioForConditionalGeneration`
+  * Qwen2-Audio
+  * T + A<sup>+</sup>
+  * `Qwen/Qwen2-Audio-7B-Instruct`
+  *
+  * ✅︎
+  * ✅︎
+- * `Qwen2VLForConditionalGeneration`
+  * QVQ, Qwen2-VL
+  * T + I<sup>E+</sup> + V<sup>E+</sup>
+  * `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc.
+  * ✅︎
+  * ✅︎
+  * ✅︎
+- * `Qwen2_5_VLForConditionalGeneration`
+  * Qwen2.5-VL
+  * T + I<sup>E+</sup> + V<sup>E+</sup>
+  * `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc.
+  *
+  * ✅︎
+  * ✅︎
+- * `UltravoxModel`
+  * Ultravox
+  * T + A<sup>E+</sup>
+  * `fixie-ai/ultravox-v0_3`
+  * ✅︎
+  * ✅︎
+  * ✅︎
+:::
+
+<sup>E</sup> Pre-computed embeddings can be inputted for this modality.  
+<sup>+</sup> Multiple items can be inputted per text prompt for this modality.
+
+:::{note}
+To use DeepSeek-VL2 series models, you have to pass `--hf_overrides '{"architectures": ["DeepseekVLV2ForCausalLM"]}'` when running vLLM.
+:::
+
+:::{note}
+H2O-VL series models will be available in V1 once we support backends other than FlashAttention.
+:::
+
+:::{note}
+To use `TIGER-Lab/Mantis-8B-siglip-llama3`, you have to pass `--hf_overrides '{"architectures": ["MantisForConditionalGeneration"]}'` when running vLLM.
+:::
+
+:::{note}
+The official `openbmb/MiniCPM-V-2` doesn't work yet, so we need to use a fork (`HwwwH/MiniCPM-V-2`) for now.
+For more details, please see: <gh-pr:4087#issuecomment-2250397630>
+:::
+
+:::{note}
+`mistral-community/pixtral-12b` does not support V1 yet.
+:::
+
+:::{note}
+To use Qwen2.5-VL series models, you have to install Huggingface `transformers` library from source via `pip install git+https://github.com/huggingface/transformers`.
+:::
+
+### Pooling Models
+
+See [this page](pooling-models) for more information on how to use pooling models.
+
+:::{important}
+Since some model architectures support both generative and pooling tasks,
+you should explicitly specify the task type to ensure that the model is used in pooling mode instead of generative mode.
+:::
+
+#### Text Embedding (`--task embed`)
+
+Any text generation model can be converted into an embedding model by passing `--task embed`.
+
+:::{note}
+To get the best results, you should use pooling models that are specifically trained as such.
+:::
+
+The following table lists those that are tested in vLLM.
+
+:::{list-table}
+:widths: 25 25 15 25 5 5
+:header-rows: 1
+
+- * Architecture
+  * Models
+  * Inputs
+  * Example HF Models
+  * [LoRA](#lora-adapter)
+  * [PP](#distributed-serving)
+- * `LlavaNextForConditionalGeneration`
+  * LLaVA-NeXT-based
+  * T / I
+  * `royokong/e5-v`
+  *
+  * ✅︎
+- * `Phi3VForCausalLM`
+  * Phi-3-Vision-based
+  * T + I
+  * `TIGER-Lab/VLM2Vec-Full`
+  * 🚧
+  * ✅︎
+- * `Qwen2VLForConditionalGeneration`
+  * Qwen2-VL-based
+  * T + I
+  * `MrLight/dse-qwen2-2b-mrl-v1`
+  *
+  * ✅︎
+:::
+
+_________________
+
+## Model Support Policy
+
+At vLLM, we are committed to facilitating the integration and support of third-party models within our ecosystem. Our approach is designed to balance the need for robustness and the practical limitations of supporting a wide range of models. Here’s how we manage third-party model support:
+
+1. **Community-Driven Support**: We encourage community contributions for adding new models. When a user requests support for a new model, we welcome pull requests (PRs) from the community. These contributions are evaluated primarily on the sensibility of the output they generate, rather than strict consistency with existing implementations such as those in transformers. **Call for contribution:** PRs coming directly from model vendors are greatly appreciated!
+
+2. **Best-Effort Consistency**: While we aim to maintain a level of consistency between the models implemented in vLLM and other frameworks like transformers, complete alignment is not always feasible. Factors like acceleration techniques and the use of low-precision computations can introduce discrepancies. Our commitment is to ensure that the implemented models are functional and produce sensible results.
+
+    :::{tip}
+    When comparing the output of `model.generate` from HuggingFace Transformers with the output of `llm.generate` from vLLM, note that the former reads the model's generation config file (i.e., [generation_config.json](https://github.com/huggingface/transformers/blob/19dabe96362803fb0a9ae7073d03533966598b17/src/transformers/generation/utils.py#L1945)) and applies the default parameters for generation, while the latter only uses the parameters passed to the function. Ensure all sampling parameters are identical when comparing outputs.
+    :::
+
+3. **Issue Resolution and Model Updates**: Users are encouraged to report any bugs or issues they encounter with third-party models. Proposed fixes should be submitted via PRs, with a clear explanation of the problem and the rationale behind the proposed solution. If a fix for one model impacts another, we rely on the community to highlight and address these cross-model dependencies. Note: for bugfix PRs, it is good etiquette to inform the original author to seek their feedback.
+
+4. **Monitoring and Updates**: Users interested in specific models should monitor the commit history for those models (e.g., by tracking changes in the main/vllm/model_executor/models directory). This proactive approach helps users stay informed about updates and changes that may affect the models they use.
+
+5. **Selective Focus**: Our resources are primarily directed towards models with significant user interest and impact. Models that are less frequently used may receive less attention, and we rely on the community to play a more active role in their upkeep and improvement.
+
+Through this approach, vLLM fosters a collaborative environment where both the core development team and the broader community contribute to the robustness and diversity of the third-party models supported in our ecosystem.
+
+Note that, as an inference engine, vLLM does not introduce new models. Therefore, all models supported by vLLM are third-party models in this regard.
+
+We have the following levels of testing for models:
+
+1. **Strict Consistency**: We compare the output of the model with the output of the model in the HuggingFace Transformers library under greedy decoding. This is the most stringent test. Please refer to [models tests](https://github.com/vllm-project/vllm/blob/main/tests/models) for the models that have passed this test.
+2. **Output Sensibility**: We check if the output of the model is sensible and coherent, by measuring the perplexity of the output and checking for any obvious errors. This is a less stringent test.
+3. **Runtime Functionality**: We check if the model can be loaded and run without errors. This is the least stringent test. Please refer to [functionality tests](gh-dir:tests) and [examples](gh-dir:main/examples) for the models that have passed this test.
+4. **Community Feedback**: We rely on the community to provide feedback on the models. If a model is broken or not working as expected, we encourage users to raise issues to report it or open pull requests to fix it. The rest of the models fall under this category.
diff --git a/env.sh b/env.sh
new file mode 100644
index 000000000..e95190521
--- /dev/null
+++ b/env.sh
@@ -0,0 +1,10 @@
+DEFAULT_DIR="/opt/maca"
+export MACA_PATH=${1:-$DEFAULT_DIR}
+export CUDA_PATH=/usr/local/cuda
+export CUCC_PATH=${MACA_PATH}/tools/cu-bridge
+export PATH=${CUDA_PATH}/bin:${MACA_PATH}/mxgpu_llvm/bin:${MACA_PATH}/bin:${CUCC_PATH}/tools:${CUCC_PATH}/bin:${PATH}
+export LD_LIBRARY_PATH=${MACA_PATH}/lib:${MACA_PATH}/ompi/lib:${MACA_PATH}/mxgpu_llvm/lib:${LD_LIBRARY_PATH}
+
+export VLLM_INSTALL_PUNICA_KERNELS=1
+
+echo "MACA PATH: ${MACA_PATH} Compile Code"
diff --git a/examples/llm_engine_example_bytemlperf.py b/examples/llm_engine_example_bytemlperf.py
new file mode 100644
index 000000000..c4766ca9e
--- /dev/null
+++ b/examples/llm_engine_example_bytemlperf.py
@@ -0,0 +1,61 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+import argparse
+from typing import List, Tuple
+
+from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams
+from vllm.utils import FlexibleArgumentParser
+
+
+def create_test_prompts() -> List[Tuple[str, SamplingParams]]:
+    """Create a list of test prompts with their sampling parameters."""
+    return [
+        ("A robot may not injure a human being",
+         SamplingParams(temperature=0.8, top_k=1, presence_penalty=0.2)),
+        ("To be or not to be,",
+         SamplingParams(temperature=0.8, top_k=1, presence_penalty=0.2)),
+        # ("What is the meaning of life?",
+        #  SamplingParams(n=2,
+        #                 best_of=5,
+        #                 temperature=0.8,
+        #                 top_p=0.95,
+        #                 frequency_penalty=0.1)),
+    ]
+
+def process_requests(engine: LLMEngine,
+                     test_prompts: List[Tuple[str, SamplingParams]]):
+    """Continuously process a list of prompts and handle the outputs."""
+    request_id = 0
+
+    while test_prompts or engine.has_unfinished_requests():
+        if test_prompts:
+            prompt, sampling_params = test_prompts.pop(0)
+            engine.add_request(str(request_id), prompt, sampling_params)
+            request_id += 1
+
+        request_outputs, hidden_states = engine.step_new()
+        print(f"hidden_states: {hidden_states.shape}")
+
+        for request_output in request_outputs:
+            if request_output.finished:
+                print(request_output)
+
+
+def initialize_engine(args: argparse.Namespace) -> LLMEngine:
+    """Initialize the LLMEngine from the command line arguments."""
+    engine_args = EngineArgs.from_cli_args(args)
+    return LLMEngine.from_engine_args(engine_args)
+
+
+def main(args: argparse.Namespace):
+    """Main function that sets up and runs the prompt processing."""
+    engine = initialize_engine(args)
+    test_prompts = create_test_prompts()
+    process_requests(engine, test_prompts)
+
+
+if __name__ == '__main__':
+    parser = FlexibleArgumentParser(
+        description='Demo on using the LLMEngine class directly')
+    parser = EngineArgs.add_cli_args(parser)
+    args = parser.parse_args()
+    main(args)
diff --git a/requirements-cuda.txt b/requirements-cuda.txt
index 78fa360f2..4b2a6945d 100644
--- a/requirements-cuda.txt
+++ b/requirements-cuda.txt
@@ -3,9 +3,9 @@
 
 # Dependencies for NVIDIA GPUs
 ray[default] >= 2.9
-nvidia-ml-py >= 12.560.30 # for pynvml package
-torch == 2.5.1
-torchaudio==2.5.1
+#nvidia-ml-py >= 12.560.30 # for pynvml package
+#torch == 2.1.2
+#torchaudio==2.5.1
 # These must be updated alongside torch
-torchvision == 0.20.1 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
-xformers == 0.0.28.post3; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.5.1
+#torchvision == 0.20.1 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
+#xformers == 0.0.28.post3; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.5.1
diff --git a/setup.py b/setup.py
index a4043c43a..e63951689 100755
--- a/setup.py
+++ b/setup.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import ctypes
@@ -15,7 +16,7 @@ import torch
 from packaging.version import Version, parse
 from setuptools import Extension, find_packages, setup
 from setuptools.command.build_ext import build_ext
-from setuptools_scm import get_version
+#from setuptools_scm import get_version
 from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME
 
 
@@ -125,8 +126,10 @@ class cmake_build_ext(build_ext):
         # Note: optimization level + debug info are set by the build type
         default_cfg = "Debug" if self.debug else "RelWithDebInfo"
         cfg = envs.CMAKE_BUILD_TYPE or default_cfg
-
+ 
+        use_maca = "ON"
         cmake_args = [
+            '-DUSE_MACA={}'.format(use_maca),
             '-DCMAKE_BUILD_TYPE={}'.format(cfg),
             '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
         ]
@@ -184,13 +187,13 @@ class cmake_build_ext(build_ext):
             # Default build tool to whatever cmake picks.
             build_tool = []
         subprocess.check_call(
-            ['cmake', ext.cmake_lists_dir, *build_tool, *cmake_args],
+            ['cmake_maca', ext.cmake_lists_dir, *build_tool, *cmake_args],
             cwd=self.build_temp)
 
     def build_extensions(self) -> None:
         # Ensure that CMake is present and working
         try:
-            subprocess.check_output(['cmake', '--version'])
+            subprocess.check_output(['cmake_maca', '--version'])
         except OSError as e:
             raise RuntimeError('Cannot find CMake executable') from e
 
@@ -217,7 +220,7 @@ class cmake_build_ext(build_ext):
             *[f"--target={name}" for name in targets],
         ]
 
-        subprocess.check_call(["cmake", *build_args], cwd=self.build_temp)
+        subprocess.check_call(["cmake_maca", *build_args], cwd=self.build_temp)
 
         # Install the libraries
         for ext in self.extensions:
@@ -239,7 +242,7 @@ class cmake_build_ext(build_ext):
 
             # prefix here should actually be the same for all components
             install_args = [
-                "cmake", "--install", ".", "--prefix", prefix, "--component",
+                "cmake_maca", "--install", ".", "--prefix", prefix, "--component",
                 target_name(ext.name)
             ]
             subprocess.check_call(install_args, cwd=self.build_temp)
@@ -252,7 +255,7 @@ class cmake_build_ext(build_ext):
         # directory so that they can be included in the editable build
         import glob
         files = glob.glob(
-            os.path.join(self.build_lib, "vllm", "vllm_flash_attn", "*.py"))
+            os.path.join(self.build_lib, "vllm", "*.py"))
         for file in files:
             dst_file = os.path.join("vllm/vllm_flash_attn",
                                     os.path.basename(file))
@@ -303,8 +306,8 @@ class repackage_wheel(build_ext):
             files_to_copy = [
                 "vllm/_C.abi3.so",
                 "vllm/_moe_C.abi3.so",
-                "vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
-                "vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
+                #"vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
+                #"vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
                 "vllm/vllm_flash_attn/flash_attn_interface.py",
                 "vllm/vllm_flash_attn/__init__.py",
                 "vllm/cumem_allocator.abi3.so",
@@ -477,11 +480,45 @@ def get_gaudi_sw_version():
             " ", "").split(":")[1][:-1].split("-")[0]
     return "0.0.0"  # when hl-smi is not available
 
+def get_maca_version():
+    maca_path = os.getenv('MACA_PATH')
+    if not os.path.exists(maca_path):
+        return None
+    file_full_path = os.path.join(maca_path, 'Version.txt')
+    if not os.path.isfile(file_full_path):
+        return None
+    
+    with open(file_full_path, 'r', encoding='utf-8') as file:
+        first_line = file.readline().strip()
+    return first_line.split(":")[-1]
+
+def get_git_commit():
+    curdir = os.path.dirname(__file__)
+    default_gitdir = os.path.normpath(os.path.join(curdir, ".git"))
+    print(default_gitdir)
+    try:
+        subprocess.check_output(["git", "--git-dir", default_gitdir, "config", "--global", "--add", "safe.directory", '*'])
+        commit_id = subprocess.check_output(["git", "--git-dir", default_gitdir, "rev-parse", "HEAD"]).decode("utf-8").strip()
+        return commit_id
+    except Exception as e:
+        print(f"Error: {e}")
+        return "git error"
+
+def write_to_file(file_path, content):
+    try:
+        with open(file_path, "w") as file:
+            file.write(content)
+        print(f"Content written to {file_path} successfully.")
+    except Exception as e:
+        print(f"Error writing to file: {e}")
 
 def get_vllm_version() -> str:
-    version = get_version(
-        write_to="vllm/_version.py",  # TODO: move this to pyproject.toml
-    )
+    #version = get_version(
+    #    write_to="vllm/_version.py",  # TODO: move this to pyproject.toml
+    #)
+    commit_id = get_git_commit()
+    write_to_file("vllm/_release_info.txt", commit_id)
+    version = "0.7.2"
 
     sep = "+" if "+" not in version else "."  # dev versions might contain +
 
@@ -492,12 +529,18 @@ def get_vllm_version() -> str:
         if envs.VLLM_USE_PRECOMPILED:
             version += f"{sep}precompiled"
         else:
-            cuda_version = str(get_nvcc_cuda_version())
-            if cuda_version != MAIN_CUDA_VERSION:
-                cuda_version_str = cuda_version.replace(".", "")[:3]
-                # skip this for source tarball, required for pypi
-                if "sdist" not in sys.argv:
-                    version += f"{sep}cu{cuda_version_str}"
+            maca_version_str = get_maca_version()
+            if maca_version_str is None:
+                cuda_version = str(get_nvcc_cuda_version())
+                if cuda_version != MAIN_CUDA_VERSION:
+                    cuda_version_str = cuda_version.replace(".", "")[:3]
+                    # skip this for source tarball, required for pypi
+                    if "sdist" not in sys.argv:
+                        version += f"{sep}cu{cuda_version_str}"
+            else:
+                torch_version = torch.__version__
+                major_minor_version = ".".join(torch_version.split(".")[:2])
+                version += f"{sep}maca{maca_version_str}torch{major_minor_version}"
     elif _is_hip():
         # Get the Rocm Version
         rocm_version = get_rocm_version() or torch.version.hip
@@ -598,7 +641,7 @@ if _is_cuda() or _is_hip():
 if _is_hip():
     ext_modules.append(CMakeExtension(name="vllm._rocm_C"))
 
-if _is_cuda():
+if _is_cuda() and False:
     ext_modules.append(CMakeExtension(name="vllm.vllm_flash_attn._vllm_fa2_C"))
     if envs.VLLM_USE_PRECOMPILED or get_nvcc_cuda_version() >= Version("12.0"):
         # FA3 requires CUDA 12.0 or later
@@ -614,6 +657,8 @@ package_data = {
         "py.typed",
         "model_executor/layers/fused_moe/configs/*.json",
         "model_executor/layers/quantization/utils/configs/*.json",
+        "attention/backends/configs/*.json",
+        "_release_info.txt",
     ]
 }
 
diff --git a/tests/distributed/test_pynccl.py b/tests/distributed/test_pynccl.py
index 4c42a0ed8..fb57f8b96 100644
--- a/tests/distributed/test_pynccl.py
+++ b/tests/distributed/test_pynccl.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import multiprocessing
diff --git a/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py b/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
index f7b81be48..da9a898e9 100644
--- a/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
+++ b/tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import List
@@ -15,32 +16,62 @@ start_token = "<think>"
 end_token = "</think>"
 
 SIMPLE_REASONING = {
-    "output": "<think>This is a reasoning section</think>This is the rest",
+    "output": "This is a reasoning section</think>This is the rest",
     "reasoning_content": "This is a reasoning section",
     "content": "This is the rest",
 }
 COMPLETE_REASONING = {
-    "output": "<think>This is a reasoning section</think>",
+    "output": "This is a reasoning section</think>",
     "reasoning_content": "This is a reasoning section",
     "content": None,
 }
-NO_REASONING = {
+NO_CONTENT = {
+    "output": "This is content",
+    "reasoning_content": "This is content",
+    "content": None,
+}
+NO_REASONING_STREAMING = {
     "output": "This is a reasoning section",
-    "reasoning_content": None,
-    "content": "This is a reasoning section",
+    "reasoning_content": "This is a reasoning section",
+    "content": None,
 }
 MULTIPLE_LINES = {
-    "output": "<think>This\nThat</think>This is the rest\nThat",
+    "output": "This\nThat</think>This is the rest\nThat",
     "reasoning_content": "This\nThat",
     "content": "This is the rest\nThat",
 }
 SHORTEST_REASONING_NO_STREAMING = {
-    "output": "<think></think>This is the rest",
+    "output": "</think>This is the rest",
     "reasoning_content": "",
     "content": "This is the rest",
 }
 SHORTEST_REASONING = {
-    "output": "<think></think>This is the rest",
+    "output": "</think>This is the rest",
+    "reasoning_content": None,
+    "content": "This is the rest",
+}
+REASONING_WITH_THINK = {
+    "output": "<think>This is a reasoning section</think>This is the rest",
+    "reasoning_content": "This is a reasoning section",
+    "content": "This is the rest",
+}
+COMPLETE_REASONING_WITH_THINK = {
+    "output": "<think>This is a reasoning section</think>",
+    "reasoning_content": "This is a reasoning section",
+    "content": None,
+}
+MULTIPLE_LINES_WITH_THINK = {
+    "output": "<think>This\nThat</think>This is the rest\nThat",
+    "reasoning_content": "This\nThat",
+    "content": "This is the rest\nThat",
+}
+SHORTEST_REASONING_NO_STREAMING_WITH_THINK = {
+    "output": "</think>This is the rest",
+    "reasoning_content": "",
+    "content": "This is the rest",
+}
+SHORTEST_REASONING_WITH_THINK = {
+    "output": "</think>This is the rest",
     "reasoning_content": None,
     "content": "This is the rest",
 }
@@ -49,37 +80,37 @@ TEST_CASES = [
     pytest.param(
         False,
         SIMPLE_REASONING,
-        id="simple_streaming",
+        id="simple_reasoning",
     ),
     pytest.param(
         True,
         SIMPLE_REASONING,
-        id="simple_streaming",
+        id="simple_reasoning_streaming",
     ),
     pytest.param(
         False,
         COMPLETE_REASONING,
-        id="complete_streaming",
+        id="complete_reasoning",
     ),
     pytest.param(
         True,
         COMPLETE_REASONING,
-        id="complete_streaming",
+        id="complete_reasoning_streaming",
     ),
     pytest.param(
         False,
-        NO_REASONING,
-        id="no_streaming",
+        NO_CONTENT,
+        id="no_content_token",
     ),
     pytest.param(
         True,
-        NO_REASONING,
-        id="no_streaming",
+        NO_REASONING_STREAMING,
+        id="no_reasoning_token_streaming",
     ),
     pytest.param(
         False,
         MULTIPLE_LINES,
-        id="multiple_lines_streaming",
+        id="multiple_lines",
     ),
     pytest.param(
         True,
@@ -89,23 +120,65 @@ TEST_CASES = [
     pytest.param(
         True,
         SHORTEST_REASONING,
-        id="shortest_streaming",
+        id="shortest",
     ),
     pytest.param(
         False,
         SHORTEST_REASONING_NO_STREAMING,
         id="shortest_streaming",
     ),
+    pytest.param(
+        False,
+        REASONING_WITH_THINK,
+        id="reasoning_with_think",
+    ),
+    pytest.param(
+        True,
+        REASONING_WITH_THINK,
+        id="reasoning_with_think_streaming",
+    ),
+    pytest.param(
+        False,
+        COMPLETE_REASONING_WITH_THINK,
+        id="complete_reasoning_with_think",
+    ),
+    pytest.param(
+        True,
+        COMPLETE_REASONING_WITH_THINK,
+        id="complete_reasoning_with_think_streaming",
+    ),
+    pytest.param(
+        False,
+        MULTIPLE_LINES_WITH_THINK,
+        id="multiple_lines_with_think",
+    ),
+    pytest.param(
+        True,
+        MULTIPLE_LINES_WITH_THINK,
+        id="multiple_lines_with_think_streaming",
+    ),
+    pytest.param(
+        False,
+        SHORTEST_REASONING_NO_STREAMING_WITH_THINK,
+        id="shortest_with_think",
+    ),
+    pytest.param(
+        True,
+        SHORTEST_REASONING_WITH_THINK,
+        id="shortest_with_think_streaming",
+    ),
 ]
 
+# Global tokenizer initialization to avoid repeated loading
+tokenizer = AutoTokenizer.from_pretrained("/pde_ai/models/llm/OPT/opt-125/")
+tokenizer.add_tokens([start_token, end_token])
+
 
 @pytest.mark.parametrize("streaming, param_dict", TEST_CASES)
 def test_reasoning(
     streaming: bool,
     param_dict: dict,
 ):
-    tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
-    tokenizer.add_tokens([start_token, end_token])
     output = tokenizer.tokenize(param_dict["output"])
     # decode everything to tokens
     output_tokens: List[str] = [
@@ -119,4 +192,4 @@ def test_reasoning(
                                                   streaming=streaming)
 
     assert reasoning == param_dict["reasoning_content"]
-    assert content == param_dict["content"]
+    assert content == param_dict["content"]
\ No newline at end of file
diff --git a/tests/kernels/test_awq.py b/tests/kernels/test_awq.py
index ace75a336..cb9c3b023 100644
--- a/tests/kernels/test_awq.py
+++ b/tests/kernels/test_awq.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import os
@@ -5,7 +6,7 @@ import os
 import pytest
 import torch
 
-from tests.kernels.utils import opcheck
+# from tests.kernels.utils import opcheck
 from vllm import _custom_ops as ops  # noqa: F401
 
 
@@ -22,17 +23,17 @@ def test_awq_dequantize_opcheck():
     split_k_iters = 0
     thx = 0
     thy = 0
-    opcheck(torch.ops._C.awq_dequantize,
-            (qweight, scales, zeros, split_k_iters, thx, thy))
+    torch.ops._C.awq_dequantize(qweight, scales, zeros, split_k_iters, thx, thy)
 
 
 @pytest.mark.skipif(not hasattr(torch.ops._C, "awq_gemm"),
                     reason="AWQ is not supported on this GPU type.")
-def test_awq_gemm_opcheck():
+@pytest.mark.parametrize("dtype_bf16", [True, False])
+def test_awq_gemm_opcheck(dtype_bf16: bool):
     os.environ["VLLM_USE_TRITON_AWQ"] = "0"
-    input = torch.rand((2, 8192), device='cuda', dtype=torch.float16)
+    input = torch.rand((2, 8192), device='cuda', dtype=torch.float16 if not dtype_bf16 else torch.bfloat16)
     qweight = torch.randint(-2000000000,
-                            2000000000, (8192, 256),
+                            2000000000, (256* 8, (input.shape[1] + 8 - 1) // 8),
                             device='cuda',
                             dtype=torch.int32)
     scales = torch.randint(-2000000000,
@@ -41,5 +42,9 @@ def test_awq_gemm_opcheck():
                            dtype=torch.int32)
     qzeros = torch.empty((64, 2048), device='cuda', dtype=torch.float16)
     split_k_iters = 8
-    opcheck(torch.ops._C.awq_gemm,
-            (input, qweight, qzeros, scales, split_k_iters))
+
+    temp_space = torch.empty(0)
+    if input.dtype == torch.bfloat16:
+            temp_space = torch.zeros(input.shape[0], qweight.shape[0],
+                                        dtype=torch.float32, device="cuda")
+    torch.ops._C.awq_gemm(input, qweight, qzeros, scales, split_k_iters, temp_space, dtype_bf16)
diff --git a/tests/kernels/test_causal_conv1d.py b/tests/kernels/test_causal_conv1d.py
index 93064e23d..69cd0208c 100644
--- a/tests/kernels/test_causal_conv1d.py
+++ b/tests/kernels/test_causal_conv1d.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import Optional
diff --git a/tests/kernels/test_cutlass.py b/tests/kernels/test_cutlass.py
index 49fd8ed63..e225b10e5 100644
--- a/tests/kernels/test_cutlass.py
+++ b/tests/kernels/test_cutlass.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Tests for cutlass kernels
 
@@ -8,7 +9,7 @@ from typing import Type
 import pytest
 import torch
 
-from tests.kernels.utils import opcheck
+#from tests.kernels.utils import opcheck
 from vllm import _custom_ops as ops
 from vllm.platforms import current_platform
 from vllm.utils import cdiv
@@ -99,8 +100,8 @@ def cutlass_fp8_gemm_helper(m: int,
 
     torch.testing.assert_close(out, baseline, rtol=1e-2, atol=5e-2)
 
-    opcheck(torch.ops._C.cutlass_scaled_mm,
-            (out, a, b, scale_a, scale_b, bias))
+    #opcheck(torch.ops._C.cutlass_scaled_mm,
+    #        (out, a, b, scale_a, scale_b, bias))
 
 
 def cutlass_int8_gemm_helper(m: int,
@@ -132,8 +133,8 @@ def cutlass_int8_gemm_helper(m: int,
 
     torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)
 
-    opcheck(torch.ops._C.cutlass_scaled_mm,
-            (out, a, b, scale_a, scale_b, bias))
+    #opcheck(torch.ops._C.cutlass_scaled_mm,
+    #        (out, a, b, scale_a, scale_b, bias))
 
 
 @pytest.mark.parametrize("m,n,k", MNK_FACTORS)
@@ -420,7 +421,8 @@ def test_cutlass_int8_azp(m: int, n: int, k: int, out_dtype: torch.dtype,
     atol = 1e-3
     torch.testing.assert_close(out, baseline_dq, rtol=rtol, atol=atol)
     torch.testing.assert_close(out, baseline_q, rtol=rtol, atol=atol)
-
+    
+    """
     if azp_per_token:
         opcheck(torch.ops._C.cutlass_scaled_mm_azp,
                 (out, aq_i8, bq_i8, scale_a, scale_b, azp_adj_i32, azp_i32,
@@ -429,7 +431,7 @@ def test_cutlass_int8_azp(m: int, n: int, k: int, out_dtype: torch.dtype,
         opcheck(torch.ops._C.cutlass_scaled_mm_azp,
                 (out, aq_i8, bq_i8, scale_a, scale_b, azp_with_adj_i32, None,
                  func_bias))
-
+    """
 
 # Test working with a subset of A and B
 def test_cutlass_subset():
@@ -505,6 +507,7 @@ def test_cutlass_cuda_graph(per_act_token: bool, per_out_ch: bool):
                         scale_b * b.to(dtype=torch.float32)).to(torch.bfloat16)
     torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)
 
-
+"""
 def test_cutlass_support_opcheck():
     opcheck(torch.ops._C.cutlass_scaled_mm_supports_fp8, (capability, ))
+"""
diff --git a/tests/kernels/test_fused_quant_layernorm.py b/tests/kernels/test_fused_quant_layernorm.py
index d4b674b23..797184682 100644
--- a/tests/kernels/test_fused_quant_layernorm.py
+++ b/tests/kernels/test_fused_quant_layernorm.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import Optional, Tuple, Union
diff --git a/tests/kernels/test_int8_quant.py b/tests/kernels/test_int8_quant.py
index 25dcb587e..ca9892055 100644
--- a/tests/kernels/test_int8_quant.py
+++ b/tests/kernels/test_int8_quant.py
@@ -1,10 +1,11 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import pytest
 import torch
 
 from tests.kernels.quant_utils import ref_dynamic_per_token_quant
-from tests.kernels.utils import opcheck
+#from tests.kernels.utils import opcheck
 from vllm._custom_ops import scaled_int8_quant
 from vllm.platforms import current_platform
 
@@ -14,7 +15,7 @@ NUM_TOKENS = [1, 7, 83, 4096]  # Arbitrary values for testing
 SEEDS = [0]
 SCALE = [0.1, 2.1]
 
-
+"""
 def opcheck_int8_quant_static(output, input, scale, azp=None):
     if azp is None:
         opcheck(torch.ops._C.static_scaled_int8_quant,
@@ -37,7 +38,7 @@ def opcheck_int8_quant_dynamic(output, input, symmetric=True):
                           dtype=torch.int32)
         opcheck(torch.ops._C.dynamic_scaled_int8_quant,
                 (output, input, scale, azp))
-
+"""
 
 @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
 @pytest.mark.parametrize("hidden_size", HIDDEN_SIZES)
@@ -59,7 +60,7 @@ def test_dynamic_scaled_int8_quant(num_tokens: int, hidden_size: int,
     # big atol to account for rounding errors
     torch.testing.assert_close(ops_out, ref_out, atol=1, rtol=0.0)
 
-    opcheck_int8_quant_dynamic(ops_out, x)
+    #opcheck_int8_quant_dynamic(ops_out, x)
 
 
 @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
@@ -98,7 +99,7 @@ def test_dynamic_scaled_int8_azp_quant(num_tokens: int, hidden_size: int,
     # if AZP is off by 1, after rounding-to-even, the output may be off by 2
     torch.testing.assert_close(ops_out, torch_out, atol=2, rtol=0.0)
 
-    opcheck_int8_quant_dynamic(ops_out, x, False)
+    #opcheck_int8_quant_dynamic(ops_out, x, False)
 
 
 @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
@@ -124,7 +125,7 @@ def test_static_scaled_int8_quant(num_tokens: int, hidden_size: int,
     # big atol to account for rounding errors
     torch.testing.assert_close(out1, out2, atol=1, rtol=0.0)
 
-    opcheck_int8_quant_static(out2, x, scale_arg)
+    #opcheck_int8_quant_static(out2, x, scale_arg)
 
 
 @pytest.mark.parametrize("num_tokens", NUM_TOKENS)
@@ -158,7 +159,7 @@ def test_static_scaled_int8_azp_quant(num_tokens: int, hidden_size: int,
     # big atol to account for rounding errors
     torch.testing.assert_close(out1, out2, atol=1, rtol=0.0)
 
-    opcheck_int8_quant_static(out2, x, scale_arg, azp_arg)
+    #opcheck_int8_quant_static(out2, x, scale_arg, azp_arg)
 
 
 @pytest.mark.parametrize("is_max", [True, False])
diff --git a/tests/kernels/test_mamba_ssm.py b/tests/kernels/test_mamba_ssm.py
index 84d4c347e..048961262 100644
--- a/tests/kernels/test_mamba_ssm.py
+++ b/tests/kernels/test_mamba_ssm.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import pytest
diff --git a/tests/kernels/test_marlin_gemm.py b/tests/kernels/test_marlin_gemm.py
index b96aca06c..75d032a4e 100644
--- a/tests/kernels/test_marlin_gemm.py
+++ b/tests/kernels/test_marlin_gemm.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Tests for the marlin kernel.
 
diff --git a/tests/kernels/utils.py b/tests/kernels/utils.py
index 5be111d71..b7c88f22d 100644
--- a/tests/kernels/utils.py
+++ b/tests/kernels/utils.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Kernel test utils"""
 
diff --git a/tests/models/registry.py b/tests/models/registry.py
index 20787fe00..f635cbf06 100644
--- a/tests/models/registry.py
+++ b/tests/models/registry.py
@@ -1,324 +1,326 @@
-# SPDX-License-Identifier: Apache-2.0
-
-from dataclasses import dataclass, field
-from typing import AbstractSet, Any, Literal, Mapping, Optional
-
-import pytest
-from packaging.version import Version
-from transformers import __version__ as TRANSFORMERS_VERSION
-
-
-@dataclass(frozen=True)
-class _HfExamplesInfo:
-    default: str
-    """The default model to use for testing this architecture."""
-
-    extras: Mapping[str, str] = field(default_factory=dict)
-    """Extra models to use for testing this architecture."""
-
-    tokenizer: Optional[str] = None
-    """Set the tokenizer to load for this architecture."""
-
-    tokenizer_mode: str = "auto"
-    """Set the tokenizer type for this architecture."""
-
-    speculative_model: Optional[str] = None
-    """
-    The default model to use for testing this architecture, which is only used
-    for speculative decoding.
-    """
-
-    min_transformers_version: Optional[str] = None
-    """
-    The minimum version of HF Transformers that is required to run this model.
-    """
-
-    is_available_online: bool = True
-    """
-    Set this to ``False`` if the name of this architecture no longer exists on
-    the HF repo. To maintain backwards compatibility, we have not removed them
-    from the main model registry, so without this flag the registry tests will
-    fail.
-    """
-
-    trust_remote_code: bool = False
-    """The ``trust_remote_code`` level required to load the model."""
-
-    hf_overrides: dict[str, Any] = field(default_factory=dict)
-    """The ``hf_overrides`` required to load the model."""
-
-    def check_transformers_version(
-        self,
-        *,
-        on_fail: Literal["error", "skip"],
-    ) -> None:
-        """
-        If the installed transformers version does not meet the requirements,
-        perform the given action.
-        """
-        if self.min_transformers_version is None:
-            return
-
-        current_version = TRANSFORMERS_VERSION
-        required_version = self.min_transformers_version
-        if Version(current_version) < Version(required_version):
-            msg = (
-                f"You have `transformers=={current_version}` installed, but "
-                f"`transformers>={required_version}` is required to run this "
-                "model")
-
-            if on_fail == "error":
-                raise RuntimeError(msg)
-            else:
-                pytest.skip(msg)
-
-    def check_available_online(
-        self,
-        *,
-        on_fail: Literal["error", "skip"],
-    ) -> None:
-        """
-        If the model is not available online, perform the given action.
-        """
-        if not self.is_available_online:
-            msg = "Model is not available online"
-
-            if on_fail == "error":
-                raise RuntimeError(msg)
-            else:
-                pytest.skip(msg)
-
-
-# yapf: disable
-_TEXT_GENERATION_EXAMPLE_MODELS = {
-    # [Decoder-only]
-    "AquilaModel": _HfExamplesInfo("BAAI/AquilaChat-7B",
-                                   trust_remote_code=True),
-    "AquilaForCausalLM": _HfExamplesInfo("BAAI/AquilaChat2-7B",
-                                         trust_remote_code=True),
-    "ArcticForCausalLM": _HfExamplesInfo("Snowflake/snowflake-arctic-instruct",
-                                         trust_remote_code=True),
-    "BaiChuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan-7B",
-                                         trust_remote_code=True),
-    "BaichuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan2-7B-chat",
-                                         trust_remote_code=True),
-    "BloomForCausalLM": _HfExamplesInfo("bigscience/bloomz-1b1"),
-    # ChatGLMModel supports multimodal
-    "CohereForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r-v01",
-                                         trust_remote_code=True),
-    "Cohere2ForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r7b-12-2024", # noqa: E501
-                                         trust_remote_code=True),
-    "DbrxForCausalLM": _HfExamplesInfo("databricks/dbrx-instruct"),
-    "DeciLMForCausalLM": _HfExamplesInfo("Deci/DeciLM-7B-instruct",
-                                         trust_remote_code=True),
-    "DeepseekForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-llm-7b-chat"),
-    "DeepseekV2ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V2-Lite-Chat",  # noqa: E501
-                                         trust_remote_code=True),
-    "DeepseekV3ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V3",  # noqa: E501
-                                         trust_remote_code=True),
-    "ExaoneForCausalLM": _HfExamplesInfo("LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"),  # noqa: E501
-    "Fairseq2LlamaForCausalLM": _HfExamplesInfo("mgleize/fairseq2-dummy-Llama-3.2-1B"),  # noqa: E501
-    "FalconForCausalLM": _HfExamplesInfo("tiiuae/falcon-7b"),
-    "GemmaForCausalLM": _HfExamplesInfo("google/gemma-2b"),
-    "Gemma2ForCausalLM": _HfExamplesInfo("google/gemma-2-9b"),
-    "GlmForCausalLM": _HfExamplesInfo("THUDM/glm-4-9b-chat-hf"),
-    "GPT2LMHeadModel": _HfExamplesInfo("gpt2"),
-    "GPTBigCodeForCausalLM": _HfExamplesInfo("bigcode/starcoder"),
-    "GPTJForCausalLM": _HfExamplesInfo("EleutherAI/gpt-j-6b"),
-    "GPTNeoXForCausalLM": _HfExamplesInfo("EleutherAI/pythia-160m"),
-    "GraniteForCausalLM": _HfExamplesInfo("ibm/PowerLM-3b"),
-    "GraniteMoeForCausalLM": _HfExamplesInfo("ibm/PowerMoE-3b"),
-    "InternLMForCausalLM": _HfExamplesInfo("internlm/internlm-chat-7b",
-                                           trust_remote_code=True),
-    "InternLM2ForCausalLM": _HfExamplesInfo("internlm/internlm2-chat-7b",
-                                            trust_remote_code=True),
-    "InternLM2VEForCausalLM": _HfExamplesInfo("OpenGVLab/Mono-InternVL-2B",
-                                              trust_remote_code=True),
-    "InternLM3ForCausalLM": _HfExamplesInfo("internlm/internlm3-8b-instruct",
-                                            trust_remote_code=True),
-    "JAISLMHeadModel": _HfExamplesInfo("inceptionai/jais-13b-chat"),
-    "JambaForCausalLM": _HfExamplesInfo("ai21labs/AI21-Jamba-1.5-Mini"),
-    "LlamaForCausalLM": _HfExamplesInfo("meta-llama/Meta-Llama-3-8B"),
-    "LLaMAForCausalLM": _HfExamplesInfo("decapoda-research/llama-7b-hf",
-                                        is_available_online=False),
-    "MambaForCausalLM": _HfExamplesInfo("state-spaces/mamba-130m-hf"),
-    "FalconMambaForCausalLM": _HfExamplesInfo("tiiuae/falcon-mamba-7b-instruct"),  # noqa: E501
-    "MiniCPMForCausalLM": _HfExamplesInfo("openbmb/MiniCPM-2B-sft-bf16",
-                                         trust_remote_code=True),
-    "MiniCPM3ForCausalLM": _HfExamplesInfo("openbmb/MiniCPM3-4B",
-                                         trust_remote_code=True),
-    "MistralForCausalLM": _HfExamplesInfo("mistralai/Mistral-7B-Instruct-v0.1"),
-    "MixtralForCausalLM": _HfExamplesInfo("mistralai/Mixtral-8x7B-Instruct-v0.1"),  # noqa: E501
-    "QuantMixtralForCausalLM": _HfExamplesInfo("mistral-community/Mixtral-8x22B-v0.1-AWQ"),  # noqa: E501
-    "MptForCausalLM": _HfExamplesInfo("mpt", is_available_online=False),
-    "MPTForCausalLM": _HfExamplesInfo("mosaicml/mpt-7b"),
-    "NemotronForCausalLM": _HfExamplesInfo("nvidia/Minitron-8B-Base"),
-    "OlmoForCausalLM": _HfExamplesInfo("allenai/OLMo-1B-hf"),
-    "Olmo2ForCausalLM": _HfExamplesInfo("shanearora/OLMo-7B-1124-hf"),
-    "OlmoeForCausalLM": _HfExamplesInfo("allenai/OLMoE-1B-7B-0924-Instruct"),
-    "OPTForCausalLM": _HfExamplesInfo("facebook/opt-iml-max-1.3b"),
-    "OrionForCausalLM": _HfExamplesInfo("OrionStarAI/Orion-14B-Chat",
-                                        trust_remote_code=True),
-    "PersimmonForCausalLM": _HfExamplesInfo("adept/persimmon-8b-chat"),
-    "PhiForCausalLM": _HfExamplesInfo("microsoft/phi-2"),
-    "Phi3ForCausalLM": _HfExamplesInfo("microsoft/Phi-3-mini-4k-instruct"),
-    "Phi3SmallForCausalLM": _HfExamplesInfo("microsoft/Phi-3-small-8k-instruct",
-                                            trust_remote_code=True),
-    "PhiMoEForCausalLM": _HfExamplesInfo("microsoft/Phi-3.5-MoE-instruct",
-                                         trust_remote_code=True),
-    # QWenLMHeadModel supports multimodal
-    "Qwen2ForCausalLM": _HfExamplesInfo("Qwen/Qwen2-7B-Instruct"),
-    "Qwen2MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen1.5-MoE-A2.7B-Chat"),
-    "RWForCausalLM": _HfExamplesInfo("tiiuae/falcon-40b",
-                                     is_available_online=False),
-    "StableLMEpochForCausalLM": _HfExamplesInfo("stabilityai/stablelm-zephyr-3b",  # noqa: E501
-                                                is_available_online=False),
-    "StableLmForCausalLM": _HfExamplesInfo("stabilityai/stablelm-3b-4e1t"),
-    "Starcoder2ForCausalLM": _HfExamplesInfo("bigcode/starcoder2-3b"),
-    "SolarForCausalLM": _HfExamplesInfo("upstage/solar-pro-preview-instruct"),
-    "TeleChat2ForCausalLM": _HfExamplesInfo("Tele-AI/TeleChat2-3B",
-                                            trust_remote_code=True),
-    "XverseForCausalLM": _HfExamplesInfo("xverse/XVERSE-7B-Chat",
-                                         is_available_online=False,
-                                         trust_remote_code=True),
-    # [Encoder-decoder]
-    "BartModel": _HfExamplesInfo("facebook/bart-base"),
-    "BartForConditionalGeneration": _HfExamplesInfo("facebook/bart-large-cnn"),
-    # Florence-2 uses BartFastTokenizer which can't be loaded from AutoTokenizer
-    # Therefore, we borrow the BartTokenizer from the original Bart model
-    "Florence2ForConditionalGeneration": _HfExamplesInfo("microsoft/Florence-2-base",  # noqa: E501
-                                                         tokenizer="facebook/bart-base",
-                                                         trust_remote_code=True),  # noqa: E501
-}
-
-_EMBEDDING_EXAMPLE_MODELS = {
-    # [Text-only]
-    "BertModel": _HfExamplesInfo("BAAI/bge-base-en-v1.5"),
-    "Gemma2Model": _HfExamplesInfo("BAAI/bge-multilingual-gemma2"),
-    "GritLM": _HfExamplesInfo("parasail-ai/GritLM-7B-vllm"),
-    "InternLM2ForRewardModel": _HfExamplesInfo("internlm/internlm2-1_8b-reward",
-                                               trust_remote_code=True),
-    "JambaForSequenceClassification": _HfExamplesInfo("ai21labs/Jamba-tiny-reward-dev"),  # noqa: E501
-    "LlamaModel": _HfExamplesInfo("llama", is_available_online=False),
-    "MistralModel": _HfExamplesInfo("intfloat/e5-mistral-7b-instruct"),
-    "Qwen2Model": _HfExamplesInfo("ssmits/Qwen2-7B-Instruct-embed-base"),
-    "Qwen2ForRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-RM-72B"),
-    "Qwen2ForProcessRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-PRM-7B"),
-    "Qwen2ForSequenceClassification": _HfExamplesInfo("jason9693/Qwen2.5-1.5B-apeach"),  # noqa: E501
-    "RobertaModel": _HfExamplesInfo("sentence-transformers/stsb-roberta-base-v2"),  # noqa: E501
-    "RobertaForMaskedLM": _HfExamplesInfo("sentence-transformers/all-roberta-large-v1"),  # noqa: E501
-    "XLMRobertaModel": _HfExamplesInfo("intfloat/multilingual-e5-large"),
-    # [Multimodal]
-    "LlavaNextForConditionalGeneration": _HfExamplesInfo("royokong/e5-v"),
-    "Phi3VForCausalLM": _HfExamplesInfo("TIGER-Lab/VLM2Vec-Full",
-                                         trust_remote_code=True),
-    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("MrLight/dse-qwen2-2b-mrl-v1"), # noqa: E501
-}
-
-_CROSS_ENCODER_EXAMPLE_MODELS = {
-    # [Text-only]
-    "BertForSequenceClassification": _HfExamplesInfo("cross-encoder/ms-marco-MiniLM-L-6-v2"),  # noqa: E501
-    "RobertaForSequenceClassification": _HfExamplesInfo("cross-encoder/quora-roberta-base"),  # noqa: E501
-    "XLMRobertaForSequenceClassification": _HfExamplesInfo("BAAI/bge-reranker-v2-m3"),  # noqa: E501
-}
-
-_MULTIMODAL_EXAMPLE_MODELS = {
-    # [Decoder-only]
-    "AriaForConditionalGeneration": _HfExamplesInfo("rhymes-ai/Aria"),
-    "Blip2ForConditionalGeneration": _HfExamplesInfo("Salesforce/blip2-opt-2.7b"),  # noqa: E501
-    "ChameleonForConditionalGeneration": _HfExamplesInfo("facebook/chameleon-7b"),  # noqa: E501
-    "ChatGLMModel": _HfExamplesInfo("THUDM/glm-4v-9b",
-                                    extras={"text_only": "THUDM/chatglm3-6b"},
-                                    trust_remote_code=True),
-    "ChatGLMForConditionalGeneration": _HfExamplesInfo("chatglm2-6b",
-                                                       is_available_online=False),
-    "DeepseekVLV2ForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-vl2-tiny",  # noqa: E501
-                                               hf_overrides={"architectures": ["DeepseekVLV2ForCausalLM"]}),  # noqa: E501
-    "FuyuForCausalLM": _HfExamplesInfo("adept/fuyu-8b"),
-    "H2OVLChatModel": _HfExamplesInfo("h2oai/h2ovl-mississippi-800m"),
-    "InternVLChatModel": _HfExamplesInfo("OpenGVLab/InternVL2-1B",
-                                         trust_remote_code=True),
-    "Idefics3ForConditionalGeneration": _HfExamplesInfo("HuggingFaceM4/Idefics3-8B-Llama3"),  # noqa: E501
-    "LlavaForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-1.5-7b-hf",
-                                                     extras={"mistral": "mistral-community/pixtral-12b"}),  # noqa: E501
-    "LlavaNextForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-v1.6-mistral-7b-hf"),  # noqa: E501
-    "LlavaNextVideoForConditionalGeneration": _HfExamplesInfo("llava-hf/LLaVA-NeXT-Video-7B-hf"),  # noqa: E501
-    "LlavaOnevisionForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-onevision-qwen2-0.5b-ov-hf"),  # noqa: E501
-    "MantisForConditionalGeneration": _HfExamplesInfo("TIGER-Lab/Mantis-8B-siglip-llama3",  # noqa: E501
-                                                      hf_overrides={"architectures": ["MantisForConditionalGeneration"]}),  # noqa: E501
-    "MiniCPMO": _HfExamplesInfo("openbmb/MiniCPM-o-2_6",
-                                trust_remote_code=True),
-    "MiniCPMV": _HfExamplesInfo("openbmb/MiniCPM-V-2_6",
-                                trust_remote_code=True),
-    "MolmoForCausalLM": _HfExamplesInfo("allenai/Molmo-7B-D-0924",
-                                        trust_remote_code=True),
-    "NVLM_D": _HfExamplesInfo("nvidia/NVLM-D-72B",
-                              trust_remote_code=True),
-    "PaliGemmaForConditionalGeneration": _HfExamplesInfo("google/paligemma-3b-pt-224"),  # noqa: E501
-    "Phi3VForCausalLM": _HfExamplesInfo("microsoft/Phi-3-vision-128k-instruct",
-                                        trust_remote_code=True),
-    "PixtralForConditionalGeneration": _HfExamplesInfo("mistralai/Pixtral-12B-2409",  # noqa: E501
-                                                       tokenizer_mode="mistral"),
-    "QWenLMHeadModel": _HfExamplesInfo("Qwen/Qwen-VL-Chat",
-                                       extras={"text_only": "Qwen/Qwen-7B-Chat"},  # noqa: E501
-                                       trust_remote_code=True),
-    "Qwen2AudioForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-Audio-7B-Instruct"),  # noqa: E501
-    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-VL-2B-Instruct"),  # noqa: E501
-    "Qwen2_5_VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2.5-VL-3B-Instruct",  # noqa: E501
-                                                          min_transformers_version="4.49"),  # noqa: E501
-    "UltravoxModel": _HfExamplesInfo("fixie-ai/ultravox-v0_3",
-                                     trust_remote_code=True),
-    # [Encoder-decoder]
-    "MllamaForConditionalGeneration": _HfExamplesInfo("meta-llama/Llama-3.2-11B-Vision-Instruct"),  # noqa: E501
-    "WhisperForConditionalGeneration": _HfExamplesInfo("openai/whisper-large-v3"),  # noqa: E501
-}
-
-_SPECULATIVE_DECODING_EXAMPLE_MODELS = {
-    "EAGLEModel": _HfExamplesInfo("JackFram/llama-68m",
-                                  speculative_model="abhigoyal/vllm-eagle-llama-68m-random"),  # noqa: E501
-    "MedusaModel": _HfExamplesInfo("JackFram/llama-68m",
-                                   speculative_model="abhigoyal/vllm-medusa-llama-68m-random"),  # noqa: E501
-    "MLPSpeculatorPreTrainedModel": _HfExamplesInfo("JackFram/llama-160m",
-                                                    speculative_model="ibm-ai-platform/llama-160m-accelerator"),  # noqa: E501
-}
-
-_FALLBACK_MODEL = {
-    "TransformersModel": _HfExamplesInfo("ArthurZ/Ilama-3.2-1B", trust_remote_code=True),  # noqa: E501
-}
-
-_EXAMPLE_MODELS = {
-    **_TEXT_GENERATION_EXAMPLE_MODELS,
-    **_EMBEDDING_EXAMPLE_MODELS,
-    **_CROSS_ENCODER_EXAMPLE_MODELS,
-    **_MULTIMODAL_EXAMPLE_MODELS,
-    **_SPECULATIVE_DECODING_EXAMPLE_MODELS,
-    **_FALLBACK_MODEL,
-}
-
-
-class HfExampleModels:
-    def __init__(self, hf_models: Mapping[str, _HfExamplesInfo]) -> None:
-        super().__init__()
-
-        self.hf_models = hf_models
-
-    def get_supported_archs(self) -> AbstractSet[str]:
-        return self.hf_models.keys()
-
-    def get_hf_info(self, model_arch: str) -> _HfExamplesInfo:
-        return self.hf_models[model_arch]
-
-    def find_hf_info(self, model_id: str) -> _HfExamplesInfo:
-        for info in self.hf_models.values():
-            if info.default == model_id:
-                return info
-
-        # Fallback to extras
-        for info in self.hf_models.values():
-            if any(extra == model_id for extra in info.extras.values()):
-                return info
-
-        raise ValueError(f"No example model defined for {model_id}")
-
-
-HF_EXAMPLE_MODELS = HfExampleModels(_EXAMPLE_MODELS)
+# SPDX-License-Identifier: Apache-2.0
+
+from dataclasses import dataclass, field
+from typing import AbstractSet, Any, Literal, Mapping, Optional
+
+import pytest
+from packaging.version import Version
+from transformers import __version__ as TRANSFORMERS_VERSION
+
+
+@dataclass(frozen=True)
+class _HfExamplesInfo:
+    default: str
+    """The default model to use for testing this architecture."""
+
+    extras: Mapping[str, str] = field(default_factory=dict)
+    """Extra models to use for testing this architecture."""
+
+    tokenizer: Optional[str] = None
+    """Set the tokenizer to load for this architecture."""
+
+    tokenizer_mode: str = "auto"
+    """Set the tokenizer type for this architecture."""
+
+    speculative_model: Optional[str] = None
+    """
+    The default model to use for testing this architecture, which is only used
+    for speculative decoding.
+    """
+
+    min_transformers_version: Optional[str] = None
+    """
+    The minimum version of HF Transformers that is required to run this model.
+    """
+
+    is_available_online: bool = True
+    """
+    Set this to ``False`` if the name of this architecture no longer exists on
+    the HF repo. To maintain backwards compatibility, we have not removed them
+    from the main model registry, so without this flag the registry tests will
+    fail.
+    """
+
+    trust_remote_code: bool = False
+    """The ``trust_remote_code`` level required to load the model."""
+
+    hf_overrides: dict[str, Any] = field(default_factory=dict)
+    """The ``hf_overrides`` required to load the model."""
+
+    def check_transformers_version(
+        self,
+        *,
+        on_fail: Literal["error", "skip"],
+    ) -> None:
+        """
+        If the installed transformers version does not meet the requirements,
+        perform the given action.
+        """
+        if self.min_transformers_version is None:
+            return
+
+        current_version = TRANSFORMERS_VERSION
+        required_version = self.min_transformers_version
+        if Version(current_version) < Version(required_version):
+            msg = (
+                f"You have `transformers=={current_version}` installed, but "
+                f"`transformers>={required_version}` is required to run this "
+                "model")
+
+            if on_fail == "error":
+                raise RuntimeError(msg)
+            else:
+                pytest.skip(msg)
+
+    def check_available_online(
+        self,
+        *,
+        on_fail: Literal["error", "skip"],
+    ) -> None:
+        """
+        If the model is not available online, perform the given action.
+        """
+        if not self.is_available_online:
+            msg = "Model is not available online"
+
+            if on_fail == "error":
+                raise RuntimeError(msg)
+            else:
+                pytest.skip(msg)
+
+
+# yapf: disable
+_TEXT_GENERATION_EXAMPLE_MODELS = {
+    # [Decoder-only]
+    "AquilaModel": _HfExamplesInfo("BAAI/AquilaChat-7B",
+                                   trust_remote_code=True),
+    "AquilaForCausalLM": _HfExamplesInfo("BAAI/AquilaChat2-7B",
+                                         trust_remote_code=True),
+    "ArcticForCausalLM": _HfExamplesInfo("Snowflake/snowflake-arctic-instruct",
+                                         trust_remote_code=True),
+    "BaiChuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan-7B",
+                                         trust_remote_code=True),
+    "BaichuanForCausalLM": _HfExamplesInfo("baichuan-inc/Baichuan2-7B-chat",
+                                         trust_remote_code=True),
+    "BloomForCausalLM": _HfExamplesInfo("bigscience/bloomz-1b1"),
+    # ChatGLMModel supports multimodal
+    "CohereForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r-v01",
+                                         trust_remote_code=True),
+    "Cohere2ForCausalLM": _HfExamplesInfo("CohereForAI/c4ai-command-r7b-12-2024", # noqa: E501
+                                         trust_remote_code=True),
+    "DbrxForCausalLM": _HfExamplesInfo("databricks/dbrx-instruct"),
+    "DeciLMForCausalLM": _HfExamplesInfo("Deci/DeciLM-7B-instruct",
+                                         trust_remote_code=True),
+    "DeepseekForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-llm-7b-chat"),
+    "DeepseekV2ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V2-Lite-Chat",  # noqa: E501
+                                         trust_remote_code=True),
+    "DeepseekV3ForCausalLM": _HfExamplesInfo("deepseek-ai/DeepSeek-V3",  # noqa: E501
+                                         trust_remote_code=True),
+    "ExaoneForCausalLM": _HfExamplesInfo("LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"),  # noqa: E501
+    "Fairseq2LlamaForCausalLM": _HfExamplesInfo("mgleize/fairseq2-dummy-Llama-3.2-1B"),  # noqa: E501
+    "FalconForCausalLM": _HfExamplesInfo("tiiuae/falcon-7b"),
+    "GemmaForCausalLM": _HfExamplesInfo("google/gemma-2b"),
+    "Gemma2ForCausalLM": _HfExamplesInfo("google/gemma-2-9b"),
+    "GlmForCausalLM": _HfExamplesInfo("THUDM/glm-4-9b-chat-hf"),
+    "GPT2LMHeadModel": _HfExamplesInfo("gpt2"),
+    "GPTBigCodeForCausalLM": _HfExamplesInfo("bigcode/starcoder"),
+    "GPTJForCausalLM": _HfExamplesInfo("EleutherAI/gpt-j-6b"),
+    "GPTNeoXForCausalLM": _HfExamplesInfo("EleutherAI/pythia-160m"),
+    "GraniteForCausalLM": _HfExamplesInfo("ibm/PowerLM-3b"),
+    "GraniteMoeForCausalLM": _HfExamplesInfo("ibm/PowerMoE-3b"),
+    "InternLMForCausalLM": _HfExamplesInfo("internlm/internlm-chat-7b",
+                                           trust_remote_code=True),
+    "InternLM2ForCausalLM": _HfExamplesInfo("internlm/internlm2-chat-7b",
+                                            trust_remote_code=True),
+    "InternLM2VEForCausalLM": _HfExamplesInfo("OpenGVLab/Mono-InternVL-2B",
+                                              trust_remote_code=True),
+    "InternLM3ForCausalLM": _HfExamplesInfo("internlm/internlm3-8b-instruct",
+                                            trust_remote_code=True),
+    "JAISLMHeadModel": _HfExamplesInfo("inceptionai/jais-13b-chat"),
+    "JambaForCausalLM": _HfExamplesInfo("ai21labs/AI21-Jamba-1.5-Mini"),
+    "LlamaForCausalLM": _HfExamplesInfo("meta-llama/Meta-Llama-3-8B"),
+    "LLaMAForCausalLM": _HfExamplesInfo("decapoda-research/llama-7b-hf",
+                                        is_available_online=False),
+    "MambaForCausalLM": _HfExamplesInfo("state-spaces/mamba-130m-hf"),
+    "FalconMambaForCausalLM": _HfExamplesInfo("tiiuae/falcon-mamba-7b-instruct"),  # noqa: E501
+    "MiniCPMForCausalLM": _HfExamplesInfo("openbmb/MiniCPM-2B-sft-bf16",
+                                         trust_remote_code=True),
+    "MiniCPM3ForCausalLM": _HfExamplesInfo("openbmb/MiniCPM3-4B",
+                                         trust_remote_code=True),
+    "MistralForCausalLM": _HfExamplesInfo("mistralai/Mistral-7B-Instruct-v0.1"),
+    "MixtralForCausalLM": _HfExamplesInfo("mistralai/Mixtral-8x7B-Instruct-v0.1"),  # noqa: E501
+    "QuantMixtralForCausalLM": _HfExamplesInfo("mistral-community/Mixtral-8x22B-v0.1-AWQ"),  # noqa: E501
+    "MptForCausalLM": _HfExamplesInfo("mpt", is_available_online=False),
+    "MPTForCausalLM": _HfExamplesInfo("mosaicml/mpt-7b"),
+    "NemotronForCausalLM": _HfExamplesInfo("nvidia/Minitron-8B-Base"),
+    "OlmoForCausalLM": _HfExamplesInfo("allenai/OLMo-1B-hf"),
+    "Olmo2ForCausalLM": _HfExamplesInfo("shanearora/OLMo-7B-1124-hf"),
+    "OlmoeForCausalLM": _HfExamplesInfo("allenai/OLMoE-1B-7B-0924-Instruct"),
+    "OPTForCausalLM": _HfExamplesInfo("facebook/opt-iml-max-1.3b"),
+    "OrionForCausalLM": _HfExamplesInfo("OrionStarAI/Orion-14B-Chat",
+                                        trust_remote_code=True),
+    "PersimmonForCausalLM": _HfExamplesInfo("adept/persimmon-8b-chat"),
+    "PhiForCausalLM": _HfExamplesInfo("microsoft/phi-2"),
+    "Phi3ForCausalLM": _HfExamplesInfo("microsoft/Phi-3-mini-4k-instruct"),
+    "Phi3SmallForCausalLM": _HfExamplesInfo("microsoft/Phi-3-small-8k-instruct",
+                                            trust_remote_code=True),
+    "PhiMoEForCausalLM": _HfExamplesInfo("microsoft/Phi-3.5-MoE-instruct",
+                                         trust_remote_code=True),
+    # QWenLMHeadModel supports multimodal
+    "Qwen2ForCausalLM": _HfExamplesInfo("Qwen/Qwen2-7B-Instruct"),
+    "Qwen2MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen1.5-MoE-A2.7B-Chat"),
+    "Qwen3ForCausalLM": _HfExamplesInfo("Qwen/Qwen3-8B"),
+    "Qwen3MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen3-30B-A3B"),
+    "RWForCausalLM": _HfExamplesInfo("tiiuae/falcon-40b",
+                                     is_available_online=False),
+    "StableLMEpochForCausalLM": _HfExamplesInfo("stabilityai/stablelm-zephyr-3b",  # noqa: E501
+                                                is_available_online=False),
+    "StableLmForCausalLM": _HfExamplesInfo("stabilityai/stablelm-3b-4e1t"),
+    "Starcoder2ForCausalLM": _HfExamplesInfo("bigcode/starcoder2-3b"),
+    "SolarForCausalLM": _HfExamplesInfo("upstage/solar-pro-preview-instruct"),
+    "TeleChat2ForCausalLM": _HfExamplesInfo("Tele-AI/TeleChat2-3B",
+                                            trust_remote_code=True),
+    "XverseForCausalLM": _HfExamplesInfo("xverse/XVERSE-7B-Chat",
+                                         is_available_online=False,
+                                         trust_remote_code=True),
+    # [Encoder-decoder]
+    "BartModel": _HfExamplesInfo("facebook/bart-base"),
+    "BartForConditionalGeneration": _HfExamplesInfo("facebook/bart-large-cnn"),
+    # Florence-2 uses BartFastTokenizer which can't be loaded from AutoTokenizer
+    # Therefore, we borrow the BartTokenizer from the original Bart model
+    "Florence2ForConditionalGeneration": _HfExamplesInfo("microsoft/Florence-2-base",  # noqa: E501
+                                                         tokenizer="facebook/bart-base",
+                                                         trust_remote_code=True),  # noqa: E501
+}
+
+_EMBEDDING_EXAMPLE_MODELS = {
+    # [Text-only]
+    "BertModel": _HfExamplesInfo("BAAI/bge-base-en-v1.5"),
+    "Gemma2Model": _HfExamplesInfo("BAAI/bge-multilingual-gemma2"),
+    "GritLM": _HfExamplesInfo("parasail-ai/GritLM-7B-vllm"),
+    "InternLM2ForRewardModel": _HfExamplesInfo("internlm/internlm2-1_8b-reward",
+                                               trust_remote_code=True),
+    "JambaForSequenceClassification": _HfExamplesInfo("ai21labs/Jamba-tiny-reward-dev"),  # noqa: E501
+    "LlamaModel": _HfExamplesInfo("llama", is_available_online=False),
+    "MistralModel": _HfExamplesInfo("intfloat/e5-mistral-7b-instruct"),
+    "Qwen2Model": _HfExamplesInfo("ssmits/Qwen2-7B-Instruct-embed-base"),
+    "Qwen2ForRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-RM-72B"),
+    "Qwen2ForProcessRewardModel": _HfExamplesInfo("Qwen/Qwen2.5-Math-PRM-7B"),
+    "Qwen2ForSequenceClassification": _HfExamplesInfo("jason9693/Qwen2.5-1.5B-apeach"),  # noqa: E501
+    "RobertaModel": _HfExamplesInfo("sentence-transformers/stsb-roberta-base-v2"),  # noqa: E501
+    "RobertaForMaskedLM": _HfExamplesInfo("sentence-transformers/all-roberta-large-v1"),  # noqa: E501
+    "XLMRobertaModel": _HfExamplesInfo("intfloat/multilingual-e5-large"),
+    # [Multimodal]
+    "LlavaNextForConditionalGeneration": _HfExamplesInfo("royokong/e5-v"),
+    "Phi3VForCausalLM": _HfExamplesInfo("TIGER-Lab/VLM2Vec-Full",
+                                         trust_remote_code=True),
+    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("MrLight/dse-qwen2-2b-mrl-v1"), # noqa: E501
+}
+
+_CROSS_ENCODER_EXAMPLE_MODELS = {
+    # [Text-only]
+    "BertForSequenceClassification": _HfExamplesInfo("cross-encoder/ms-marco-MiniLM-L-6-v2"),  # noqa: E501
+    "RobertaForSequenceClassification": _HfExamplesInfo("cross-encoder/quora-roberta-base"),  # noqa: E501
+    "XLMRobertaForSequenceClassification": _HfExamplesInfo("BAAI/bge-reranker-v2-m3"),  # noqa: E501
+}
+
+_MULTIMODAL_EXAMPLE_MODELS = {
+    # [Decoder-only]
+    "AriaForConditionalGeneration": _HfExamplesInfo("rhymes-ai/Aria"),
+    "Blip2ForConditionalGeneration": _HfExamplesInfo("Salesforce/blip2-opt-2.7b"),  # noqa: E501
+    "ChameleonForConditionalGeneration": _HfExamplesInfo("facebook/chameleon-7b"),  # noqa: E501
+    "ChatGLMModel": _HfExamplesInfo("THUDM/glm-4v-9b",
+                                    extras={"text_only": "THUDM/chatglm3-6b"},
+                                    trust_remote_code=True),
+    "ChatGLMForConditionalGeneration": _HfExamplesInfo("chatglm2-6b",
+                                                       is_available_online=False),
+    "DeepseekVLV2ForCausalLM": _HfExamplesInfo("deepseek-ai/deepseek-vl2-tiny",  # noqa: E501
+                                               hf_overrides={"architectures": ["DeepseekVLV2ForCausalLM"]}),  # noqa: E501
+    "FuyuForCausalLM": _HfExamplesInfo("adept/fuyu-8b"),
+    "H2OVLChatModel": _HfExamplesInfo("h2oai/h2ovl-mississippi-800m"),
+    "InternVLChatModel": _HfExamplesInfo("OpenGVLab/InternVL2-1B",
+                                         trust_remote_code=True),
+    "Idefics3ForConditionalGeneration": _HfExamplesInfo("HuggingFaceM4/Idefics3-8B-Llama3"),  # noqa: E501
+    "LlavaForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-1.5-7b-hf",
+                                                     extras={"mistral": "mistral-community/pixtral-12b"}),  # noqa: E501
+    "LlavaNextForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-v1.6-mistral-7b-hf"),  # noqa: E501
+    "LlavaNextVideoForConditionalGeneration": _HfExamplesInfo("llava-hf/LLaVA-NeXT-Video-7B-hf"),  # noqa: E501
+    "LlavaOnevisionForConditionalGeneration": _HfExamplesInfo("llava-hf/llava-onevision-qwen2-0.5b-ov-hf"),  # noqa: E501
+    "MantisForConditionalGeneration": _HfExamplesInfo("TIGER-Lab/Mantis-8B-siglip-llama3",  # noqa: E501
+                                                      hf_overrides={"architectures": ["MantisForConditionalGeneration"]}),  # noqa: E501
+    "MiniCPMO": _HfExamplesInfo("openbmb/MiniCPM-o-2_6",
+                                trust_remote_code=True),
+    "MiniCPMV": _HfExamplesInfo("openbmb/MiniCPM-V-2_6",
+                                trust_remote_code=True),
+    "MolmoForCausalLM": _HfExamplesInfo("allenai/Molmo-7B-D-0924",
+                                        trust_remote_code=True),
+    "NVLM_D": _HfExamplesInfo("nvidia/NVLM-D-72B",
+                              trust_remote_code=True),
+    "PaliGemmaForConditionalGeneration": _HfExamplesInfo("google/paligemma-3b-pt-224"),  # noqa: E501
+    "Phi3VForCausalLM": _HfExamplesInfo("microsoft/Phi-3-vision-128k-instruct",
+                                        trust_remote_code=True),
+    "PixtralForConditionalGeneration": _HfExamplesInfo("mistralai/Pixtral-12B-2409",  # noqa: E501
+                                                       tokenizer_mode="mistral"),
+    "QWenLMHeadModel": _HfExamplesInfo("Qwen/Qwen-VL-Chat",
+                                       extras={"text_only": "Qwen/Qwen-7B-Chat"},  # noqa: E501
+                                       trust_remote_code=True),
+    "Qwen2AudioForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-Audio-7B-Instruct"),  # noqa: E501
+    "Qwen2VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2-VL-2B-Instruct"),  # noqa: E501
+    "Qwen2_5_VLForConditionalGeneration": _HfExamplesInfo("Qwen/Qwen2.5-VL-3B-Instruct",  # noqa: E501
+                                                          min_transformers_version="4.49"),  # noqa: E501
+    "UltravoxModel": _HfExamplesInfo("fixie-ai/ultravox-v0_3",
+                                     trust_remote_code=True),
+    # [Encoder-decoder]
+    "MllamaForConditionalGeneration": _HfExamplesInfo("meta-llama/Llama-3.2-11B-Vision-Instruct"),  # noqa: E501
+    "WhisperForConditionalGeneration": _HfExamplesInfo("openai/whisper-large-v3"),  # noqa: E501
+}
+
+_SPECULATIVE_DECODING_EXAMPLE_MODELS = {
+    "EAGLEModel": _HfExamplesInfo("JackFram/llama-68m",
+                                  speculative_model="abhigoyal/vllm-eagle-llama-68m-random"),  # noqa: E501
+    "MedusaModel": _HfExamplesInfo("JackFram/llama-68m",
+                                   speculative_model="abhigoyal/vllm-medusa-llama-68m-random"),  # noqa: E501
+    "MLPSpeculatorPreTrainedModel": _HfExamplesInfo("JackFram/llama-160m",
+                                                    speculative_model="ibm-ai-platform/llama-160m-accelerator"),  # noqa: E501
+}
+
+_FALLBACK_MODEL = {
+    "TransformersModel": _HfExamplesInfo("ArthurZ/Ilama-3.2-1B", trust_remote_code=True),  # noqa: E501
+}
+
+_EXAMPLE_MODELS = {
+    **_TEXT_GENERATION_EXAMPLE_MODELS,
+    **_EMBEDDING_EXAMPLE_MODELS,
+    **_CROSS_ENCODER_EXAMPLE_MODELS,
+    **_MULTIMODAL_EXAMPLE_MODELS,
+    **_SPECULATIVE_DECODING_EXAMPLE_MODELS,
+    **_FALLBACK_MODEL,
+}
+
+
+class HfExampleModels:
+    def __init__(self, hf_models: Mapping[str, _HfExamplesInfo]) -> None:
+        super().__init__()
+
+        self.hf_models = hf_models
+
+    def get_supported_archs(self) -> AbstractSet[str]:
+        return self.hf_models.keys()
+
+    def get_hf_info(self, model_arch: str) -> _HfExamplesInfo:
+        return self.hf_models[model_arch]
+
+    def find_hf_info(self, model_id: str) -> _HfExamplesInfo:
+        for info in self.hf_models.values():
+            if info.default == model_id:
+                return info
+
+        # Fallback to extras
+        for info in self.hf_models.values():
+            if any(extra == model_id for extra in info.extras.values()):
+                return info
+
+        raise ValueError(f"No example model defined for {model_id}")
+
+
+HF_EXAMPLE_MODELS = HfExampleModels(_EXAMPLE_MODELS)
diff --git a/vllm/__init__.py b/vllm/__init__.py
index 566c5116d..0bebaad83 100644
--- a/vllm/__init__.py
+++ b/vllm/__init__.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """vLLM: a high-throughput and memory-efficient inference engine for LLMs"""
 import os
@@ -32,7 +33,7 @@ os.environ['NCCL_CUMEM_ENABLE'] = '0'
 # see https://github.com/vllm-project/vllm/issues/10480
 os.environ['TORCHINDUCTOR_COMPILE_THREADS'] = '1'
 # see https://github.com/vllm-project/vllm/issues/10619
-torch._inductor.config.compile_threads = 1
+#torch._inductor.config.compile_threads = 1
 
 __all__ = [
     "__version__",
diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index a68235016..b6008ec40 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import contextlib
@@ -24,7 +25,7 @@ supports_moe_ops = False
 with contextlib.suppress(ImportError):
     import vllm._moe_C  # noqa: F401
     supports_moe_ops = True
-
+"""
 if TYPE_CHECKING:
 
     def register_fake(fn):
@@ -34,7 +35,7 @@ else:
         from torch.library import register_fake
     except ImportError:
         from torch.library import impl_abstract as register_fake
-
+"""
 
 # page attention ops
 def paged_attention_v1(
@@ -71,6 +72,7 @@ def paged_attention_v2(
     exp_sum: torch.Tensor,
     max_logits: torch.Tensor,
     tmp_out: torch.Tensor,
+    block_count: torch.Tensor,
     query: torch.Tensor,
     key_cache: torch.Tensor,
     value_cache: torch.Tensor,
@@ -89,14 +91,14 @@ def paged_attention_v2(
     blocksparse_vert_stride: int = 0,
     blocksparse_block_size: int = 64,
     blocksparse_head_sliding_step: int = 0,
+    count_init_once: bool = False,
 ) -> None:
     torch.ops._C.paged_attention_v2(
-        out, exp_sum, max_logits, tmp_out, query, key_cache, value_cache,
+        out, exp_sum, max_logits, tmp_out, block_count, query, key_cache, value_cache,
         num_kv_heads, scale, block_tables, seq_lens, block_size, max_seq_len,
         alibi_slopes, kv_cache_dtype, k_scale, v_scale, tp_rank,
         blocksparse_local_blocks, blocksparse_vert_stride,
-        blocksparse_block_size, blocksparse_head_sliding_step)
-
+        blocksparse_block_size, blocksparse_head_sliding_step, count_init_once)
 
 def paged_attention_rocm(
     out: torch.Tensor,
@@ -146,6 +148,20 @@ def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
                                           cos_sin_cache, is_neox, rot_dim,
                                           cos_sin_cache_offsets)
 
+def page_reshape_kv_cache(
+    key_cache: torch.Tensor,
+    value_cache: torch.Tensor,
+    key_cache_new_layer: torch.Tensor,
+    value_cache_new_layer: torch.Tensor,
+    num_seqs: int,
+    num_heads: int,
+    head_size: int,
+    num_kv_heads: int,
+    block_size: int,
+    kv_cache_dtype: str,
+  )->None:
+    torch.ops._C.page_reshape_kv_cache(key_cache, value_cache, key_cache_new_layer,
+            value_cache_new_layer, num_seqs, num_heads, head_size, num_kv_heads, block_size, kv_cache_dtype)
 
 # layer norm ops
 def rms_norm(out: torch.Tensor, input: torch.Tensor, weight: torch.Tensor,
@@ -224,23 +240,32 @@ def awq_dequantize(qweight: torch.Tensor, scales: torch.Tensor,
 
 
 def awq_gemm(input: torch.Tensor, qweight: torch.Tensor, qzeros: torch.Tensor,
-             scales: torch.Tensor, split_k_iters: int) -> torch.Tensor:
+        scales: torch.Tensor, split_k_iters: int, temp_space: torch.Tensor, 
+        dtype_bf16: bool) -> torch.Tensor:
     if envs.VLLM_USE_TRITON_AWQ:
         from vllm.model_executor.layers.quantization.awq_triton import (
             awq_gemm_triton)
-        return awq_gemm_triton(input, qweight, qzeros, scales, split_k_iters)
-    return torch.ops._C.awq_gemm(input, qweight, qzeros, scales, split_k_iters)
+        return awq_gemm_triton(input, qweight, scales, qzeros, split_k_iters)
+    return torch.ops._C.awq_gemm(input, qweight, scales, qzeros, split_k_iters,
+                                 temp_space, dtype_bf16)
 
+def awq_to_gptq_4bit(qweight: torch.Tensor) -> torch.Tensor:
+    if envs.VLLM_USE_TRITON_AWQ:
+        return qweight
+    return torch.ops._C.awq_to_gptq_4bit(qweight)
+    
 
 # gptq
 def gptq_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
               b_gptq_qzeros: torch.Tensor, b_gptq_scales: torch.Tensor,
               b_g_idx: torch.Tensor, use_exllama: bool,
-              bit: int) -> torch.Tensor:
+              bit: int, group_size: int, perm_space: torch.Tensor,
+              temp_space: torch.Tensor, dtype_bf16: bool) -> torch.Tensor:
     return torch.ops._C.gptq_gemm(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
-                                  b_g_idx, use_exllama, bit)
-
+                                  b_g_idx, use_exllama, bit, group_size, 
+                                  perm_space, temp_space, dtype_bf16)
 
+"""
 if hasattr(torch.ops._C, "gptq_gemm"):
 
     @register_fake("_C::gptq_gemm")
@@ -251,7 +276,7 @@ if hasattr(torch.ops._C, "gptq_gemm"):
         return torch.empty((a.size(0), b_q_weight.size(1)),
                            dtype=a.dtype,
                            device=a.device)
-
+"""
 
 def gptq_shuffle(q_weight: torch.Tensor, q_perm: torch.Tensor,
                  bit: int) -> None:
@@ -275,7 +300,7 @@ def gptq_marlin_24_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
                                             workspace, b_q_type.id, size_m,
                                             size_n, size_k)
 
-
+"""
 if hasattr(torch.ops._C, "gptq_marlin_24_gemm"):
 
     @register_fake("_C::gptq_marlin_24_gemm")
@@ -430,16 +455,18 @@ if hasattr(torch.ops._C, "ggml_dequantize"):
     ) -> torch.Tensor:
         batch = X.size(0)
         return torch.empty((batch, row), dtype=torch.float16, device=W.device)
-
+"""
 
 # cutlass
 def cutlass_scaled_mm_supports_fp8(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
+    #return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
+    return 
 
 
 def cutlass_scaled_mm_supports_block_fp8(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
-        cuda_device_capability)
+    #return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
+    #    cuda_device_capability)
+    return 
 
 
 def cutlass_scaled_mm(a: torch.Tensor,
@@ -752,14 +779,14 @@ def machete_prepack_B(
     return torch.ops._C.machete_prepack_B(b_q_weight, a_type, b_type.id,
                                           group_scales_type)
 
-
+"""
 if hasattr(torch.ops._C, "permute_cols"):
 
     @register_fake("_C::permute_cols")
     def _permute_cols_fake(a: torch.Tensor,
                            perm: torch.Tensor) -> torch.Tensor:
         return torch.empty_like(a)
-
+"""
 
 def permute_cols(a: torch.Tensor, perm: torch.Tensor) -> torch.Tensor:
     return torch.ops._C.permute_cols(a, perm)
@@ -967,7 +994,16 @@ def topk_softmax(topk_weights: torch.Tensor, topk_ids: torch.Tensor,
     torch.ops._moe_C.topk_softmax(topk_weights, topk_ids,
                                   token_expert_indicies, gating_output)
 
+def fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor,
+                    topk_weights: torch.Tensor, topk_ids: torch.Tensor,
+                    sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor,
+                    num_tokens_post_padded: torch.Tensor, mul_routed_weight: bool, top_k: int, tileConfig: int) -> None:
+    torch.ops._moe_C.fused_moe_kernel(A, B, C,
+                    topk_weights, topk_ids,
+                    sorted_token_ids, expert_ids,
+                    num_tokens_post_padded, mul_routed_weight, top_k, tileConfig)
 
+"""
 if supports_moe_ops and hasattr(torch.ops._moe_C, "marlin_gemm_moe"):
 
     @register_fake("_moe_C::marlin_gemm_moe")
@@ -985,7 +1021,7 @@ if supports_moe_ops and hasattr(torch.ops._moe_C, "marlin_gemm_moe"):
         return torch.empty((size_m, topk, size_n),
                            dtype=a.dtype,
                            device=a.device)
-
+"""
 
 def reshape_and_cache(
     key: torch.Tensor,
@@ -1001,6 +1037,19 @@ def reshape_and_cache(
                                              value_cache, slot_mapping,
                                              kv_cache_dtype, k_scale, v_scale)
 
+def reshape_and_cache_new(
+    key: torch.Tensor,
+    value: torch.Tensor,
+    key_cache: torch.Tensor,
+    value_cache: torch.Tensor,
+    slot_mapping: torch.Tensor,
+    kv_cache_dtype: str,
+    kv_scale: float,
+    v_scale: float,
+) -> None:
+    torch.ops._C_cache_ops.reshape_and_cache_new(key, value, key_cache,
+                                             value_cache, slot_mapping,
+                                             kv_cache_dtype, kv_scale, v_scale)
 
 def reshape_and_cache_flash(
     key: torch.Tensor,
diff --git a/vllm/_release_info.txt b/vllm/_release_info.txt
new file mode 100644
index 000000000..32a80d8ac
--- /dev/null
+++ b/vllm/_release_info.txt
@@ -0,0 +1 @@
+git error
\ No newline at end of file
diff --git a/vllm/attention/backends/configs/tp8_merge.json b/vllm/attention/backends/configs/tp8_merge.json
new file mode 100644
index 000000000..773051b85
--- /dev/null
+++ b/vllm/attention/backends/configs/tp8_merge.json
@@ -0,0 +1,986 @@
+[
+    {
+        "BS": 1,
+        "L": 2,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 4,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 8,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 16,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 32,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 64,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 128,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 256,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 512,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 1024,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 2048,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 4096,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 8192,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 16384,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 32768,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 65536,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 2,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 4,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 8,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 16,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 32,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 64,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 128,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 256,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 512,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 1024,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 2048,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 4096,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 8192,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 16384,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 32768,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 65536,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 2,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 4,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 8,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 16,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 32,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 64,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 128,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 256,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 512,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 1024,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 2048,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 4096,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 8192,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 16384,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 32768,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 2,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 8,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 16,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 32,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 64,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 128,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 256,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 512,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 1024,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 2048,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 4096,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 8192,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 16384,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 32,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 64,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 128,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 256,
+        "num_kv_splits": 6,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 512,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 1024,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 2048,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 4096,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 8192,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 128,
+        "num_kv_splits": 3,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 256,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 512,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 1024,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 2048,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 4096,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 2,
+        "num_kv_splits": 4,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 128,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 256,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 512,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 1024,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 2048,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 2048,
+        "num_kv_splits": 8,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 256,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 512,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 1024,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 256,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 512,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 1024,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 256,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 512,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 256,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    }
+]
\ No newline at end of file
diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py
old mode 100755
new mode 100644
index 6a82127ac..2aac995e1
--- a/vllm/attention/backends/flash_attn.py
+++ b/vllm/attention/backends/flash_attn.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Attention layer with FlashAttention."""
 from collections import defaultdict
@@ -23,10 +24,12 @@ from vllm.logger import init_logger
 from vllm.multimodal import MultiModalPlaceholderMap
 from vllm.platforms import current_platform
 from vllm.utils import async_tensor_h2d, make_tensor_with_pad
-from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
-                                  flash_attn_varlen_func,
-                                  flash_attn_with_kvcache,
-                                  is_fa_version_supported)
+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
+#                                  flash_attn_varlen_func,
+#                                  flash_attn_with_kvcache,
+#                                  is_fa_version_supported)
+from flash_attn import (flash_attn_varlen_func,
+                        flash_attn_with_kvcache)
 
 if TYPE_CHECKING:
     from vllm.worker.model_runner import (ModelInputForGPUBuilder,
@@ -41,7 +44,7 @@ class FlashAttentionBackend(AttentionBackend):
 
     @staticmethod
     def get_supported_head_sizes() -> List[int]:
-        return [32, 64, 96, 128, 160, 192, 224, 256]
+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
 
     @staticmethod
     def get_name() -> str:
@@ -647,21 +650,21 @@ class FlashAttentionImpl(AttentionImpl):
         # if hopper default to FA3, otherwise stick to FA2 for now
         # TODO(lucas): profile FA3 on ampere to see if it makes sense to
         #  use FA3 as default for both
-        if current_platform.get_device_capability()[0] >= 9:
-            self.fa_version = 3 if is_fa_version_supported(3) else 2
-        else:
-            self.fa_version = 2
+        #if current_platform.get_device_capability()[0] >= 9:
+        #    self.fa_version = 3 if is_fa_version_supported(3) else 2
+        #else:
+        #    self.fa_version = 2
 
-        if VLLM_FLASH_ATTN_VERSION is not None:
-            assert VLLM_FLASH_ATTN_VERSION in [2, 3]
-            self.fa_version = VLLM_FLASH_ATTN_VERSION
+        #if VLLM_FLASH_ATTN_VERSION is not None:
+        #    assert VLLM_FLASH_ATTN_VERSION in [2, 3]
+        #    self.fa_version = VLLM_FLASH_ATTN_VERSION
 
-        if not is_fa_version_supported(self.fa_version):
-            logger.error("Cannot use FA version %d is not supported due to %s",
-                         self.fa_version,
-                         fa_version_unsupported_reason(self.fa_version))
+        #if not is_fa_version_supported(self.fa_version):
+        #    logger.error("Cannot use FA version %d is not supported due to %s",
+        #                 self.fa_version,
+        #                 fa_version_unsupported_reason(self.fa_version))
 
-        assert is_fa_version_supported(self.fa_version)
+        #assert is_fa_version_supported(self.fa_version)
 
     def forward(
         self,
@@ -767,7 +770,8 @@ class FlashAttentionImpl(AttentionImpl):
                 key = key[:num_prefill_kv_tokens]
                 value = value[:num_prefill_kv_tokens]
 
-                flash_attn_varlen_func(
+                #flash_attn_varlen_func(
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
                     q=query,
                     k=key,
                     v=value,
@@ -780,8 +784,8 @@ class FlashAttentionImpl(AttentionImpl):
                     window_size=window_size,
                     alibi_slopes=alibi_slopes,
                     softcap=logits_soft_cap,
-                    out=prefill_output,
-                    fa_version=self.fa_version,
+                    #out=prefill_output,
+                    #fa_version=self.fa_version,
                 )
             else:
                 # prefix-enabled attention
@@ -789,13 +793,15 @@ class FlashAttentionImpl(AttentionImpl):
                     "Only decoder-only models support prefix caching")
                 assert prefill_meta.seq_lens is not None
                 max_seq_len = max(prefill_meta.seq_lens)
-                flash_attn_varlen_func(  # noqa
+                #flash_attn_varlen_func(  # noqa
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
                     q=query,
                     k=key_cache,
                     v=value_cache,
                     cu_seqlens_q=prefill_meta.query_start_loc,
                     max_seqlen_q=prefill_meta.max_query_len,
-                    seqused_k=prefill_meta.seq_lens_tensor,
+                    #seqused_k=prefill_meta.seq_lens_tensor,
+                    cu_seqlens_k=prefill_meta.seq_start_loc,
                     max_seqlen_k=max_seq_len,
                     softmax_scale=softmax_scale,
                     causal=True,
@@ -803,8 +809,8 @@ class FlashAttentionImpl(AttentionImpl):
                     alibi_slopes=alibi_slopes,
                     block_table=prefill_meta.block_tables,
                     softcap=logits_soft_cap,
-                    out=prefill_output,
-                    fa_version=self.fa_version,
+                    #out=prefill_output,
+                    #fa_version=self.fa_version,
                 )
 
         if decode_meta := attn_metadata.decode_metadata:
@@ -818,13 +824,14 @@ class FlashAttentionImpl(AttentionImpl):
                 assert attn_type == AttentionType.DECODER, (
                     "Only decoder-only models support max_decode_query_len > 1"
                 )
-                flash_attn_varlen_func(
+                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
                     q=decode_query,
                     k=key_cache,
                     v=value_cache,
                     cu_seqlens_q=decode_meta.query_start_loc,
                     max_seqlen_q=decode_meta.max_decode_query_len,
-                    seqused_k=decode_meta.seq_lens_tensor,
+                    # seqused_k=decode_meta.seq_lens_tensor,
+                    cu_seqlens_k=decode_meta.seq_start_loc,
                     max_seqlen_k=decode_meta.max_decode_seq_len,
                     softmax_scale=softmax_scale,
                     causal=True,
@@ -832,8 +839,8 @@ class FlashAttentionImpl(AttentionImpl):
                     alibi_slopes=alibi_slopes,
                     softcap=logits_soft_cap,
                     block_table=decode_meta.block_tables,
-                    out=decode_output,
-                    fa_version=self.fa_version,
+                    #out=decode_output,
+                    #fa_version=self.fa_version,
                 )
             else:
                 # Use flash_attn_with_kvcache for normal decoding.
@@ -842,7 +849,7 @@ class FlashAttentionImpl(AttentionImpl):
                     _,
                     block_tables_arg,
                 ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
-                flash_attn_with_kvcache(
+                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
                     q=decode_query.unsqueeze(1),
                     k_cache=key_cache,
                     v_cache=value_cache,
@@ -853,9 +860,9 @@ class FlashAttentionImpl(AttentionImpl):
                     window_size=window_size,
                     alibi_slopes=alibi_slopes,
                     softcap=logits_soft_cap,
-                    out=decode_output.unsqueeze(1),
-                    fa_version=self.fa_version,
-                )
+                    #out=decode_output.unsqueeze(1),
+                    #fa_version=self.fa_version,
+                ).squeeze(1)
         return output
 
 
diff --git a/vllm/attention/backends/flash_attn_pg.py b/vllm/attention/backends/flash_attn_pg.py
new file mode 100755
index 000000000..8435f9baa
--- /dev/null
+++ b/vllm/attention/backends/flash_attn_pg.py
@@ -0,0 +1,1028 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# SPDX-License-Identifier: Apache-2.0
+"""Attention layer with FlashAttention."""
+from collections import defaultdict
+from dataclasses import dataclass
+from itertools import accumulate
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type
+
+import torch
+
+from vllm import _custom_ops as ops
+from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+                                              AttentionLayer,
+                                              AttentionMetadata,
+                                              AttentionMetadataBuilder,
+                                              AttentionType)
+from vllm.attention.backends.utils import (
+    PAD_SLOT_ID, CommonAttentionState, compute_slot_mapping,
+    compute_slot_mapping_start_idx, get_num_prefill_decode_query_kv_tokens,
+    get_seq_len_block_table_args, is_all_cross_attn_metadata_set,
+    is_all_encoder_attn_metadata_set, is_block_tables_empty)
+from vllm.envs import VLLM_FLASH_ATTN_VERSION
+from vllm.logger import init_logger
+from vllm.multimodal import MultiModalPlaceholderMap
+from vllm.platforms import current_platform
+from vllm.utils import async_tensor_h2d, make_tensor_with_pad
+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
+#                                  flash_attn_varlen_func,
+#                                  flash_attn_with_kvcache,
+#                                  is_fa_version_supported)
+from vllm.attention.ops.paged_attn import (PagedAttention,
+                                           PagedAttentionMetadata)
+
+from flash_attn import (flash_attn_varlen_func,
+                        flash_attn_with_kvcache)
+
+if TYPE_CHECKING:
+    from vllm.worker.model_runner import (ModelInputForGPUBuilder,
+                                          ModelInputForGPUWithSamplingMetadata)
+
+logger = init_logger(__name__)
+
+
+class FlashAttentionBackend(AttentionBackend):
+
+    accept_output_buffer: bool = True
+
+    @staticmethod
+    def get_supported_head_sizes() -> List[int]:
+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
+
+    @staticmethod
+    def get_name() -> str:
+        return "FLASH_ATTN"
+
+    @staticmethod
+    def get_impl_cls() -> Type["FlashAttentionImpl"]:
+        return FlashAttentionImpl
+
+    @staticmethod
+    def get_metadata_cls() -> Type["AttentionMetadata"]:
+        return FlashAttentionMetadata
+
+    @staticmethod
+    def get_builder_cls() -> Type["FlashAttentionMetadataBuilder"]:
+        return FlashAttentionMetadataBuilder
+
+    @staticmethod
+    def get_state_cls() -> Type["CommonAttentionState"]:
+        return CommonAttentionState
+
+    @staticmethod
+    def get_kv_cache_shape(
+        num_blocks: int,
+        block_size: int,
+        num_kv_heads: int,
+        head_size: int,
+    ) -> Tuple[int, ...]:
+        """
+        if block_size % 16 != 0:
+            raise ValueError("Block size must be a multiple of 16.")
+        return (2, num_blocks, block_size, num_kv_heads, head_size)
+        """
+        return PagedAttention.get_kv_cache_shape(num_blocks, block_size,
+                                                 num_kv_heads, head_size)
+
+    @staticmethod
+    def swap_blocks(
+        src_kv_cache: torch.Tensor,
+        dst_kv_cache: torch.Tensor,
+        src_to_dst: torch.Tensor,
+    ) -> None:
+        """
+        src_key_cache = src_kv_cache[0]
+        dst_key_cache = dst_kv_cache[0]
+        ops.swap_blocks(src_key_cache, dst_key_cache, src_to_dst)
+        src_value_cache = src_kv_cache[1]
+        dst_value_cache = dst_kv_cache[1]
+        ops.swap_blocks(src_value_cache, dst_value_cache, src_to_dst)
+        """
+        PagedAttention.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)
+
+    @staticmethod
+    def copy_blocks(
+        kv_caches: List[torch.Tensor],
+        src_to_dists: torch.Tensor,
+    ) -> None:
+        """
+        key_caches = [kv_cache[0] for kv_cache in kv_caches]
+        value_caches = [kv_cache[1] for kv_cache in kv_caches]
+
+        ops.copy_blocks(key_caches, value_caches, src_to_dists)
+        """
+        PagedAttention.copy_blocks(kv_caches, src_to_dists)
+
+
+@dataclass
+#class FlashAttentionMetadata(AttentionMetadata):
+class FlashAttentionMetadata(AttentionMetadata, PagedAttentionMetadata):
+    """Metadata for FlashAttentionBackend.
+
+    NOTE: Any python object stored here is not updated when it is
+    cuda-graph replayed. If you have values that need to be changed
+    dynamically, it should be stored in tensor. The tensor has to be
+    updated from `CUDAGraphRunner.forward` API.
+    """
+    # (batch_size,). The sequence length per sequence. Sequence length means
+    # the computed tokens + new tokens None if it is a decoding.
+    seq_lens: Optional[List[int]]
+    # seq_lens stored as a tensor.
+    seq_lens_tensor: Optional[torch.Tensor]
+
+    # NOTE(sang): Definition of context_len, query_len, and seq_len.
+    # |---------- N-1 iteration --------|
+    # |---------------- N iteration ---------------------|
+    # |- tokenA -|......................|-- newTokens ---|
+    # |---------- context_len ----------|
+    # |-------------------- seq_len ---------------------|
+    #                                   |-- query_len ---|
+
+    # Maximum sequence length among prefill batch. 0 if there are decoding
+    # requests only.
+    max_prefill_seq_len: int
+    # Maximum sequence length among decode batch. 0 if there are prefill
+    # requests only.
+    max_decode_seq_len: int
+    # (batch_size,) A tensor of context lengths (tokens that are computed
+    # so far).
+    context_lens_tensor: Optional[torch.Tensor]
+
+    # (batch_size, max_blocks_per_seq).
+    # Block addresses per sequence. (Seq id -> list of physical block)
+    # E.g., [0, 1, 2] means tokens are stored in 0th, 1st, and 2nd blocks
+    # in the kv cache. Each block can contain up to block_size tokens.
+    # 2nd dimensions are padded up to max_blocks_per_seq if it is cuda-graph
+    # captured.
+    block_tables: Optional[torch.Tensor]
+
+    # Whether or not if cuda graph is enabled.
+    # Cuda-graph is currently enabled for decoding only.
+    # TODO(woosuk): Move `use_cuda_graph` out since it's unrelated to attention.
+
+    use_cuda_graph: bool
+
+    # Maximum query length in the batch.
+    max_query_len: Optional[int] = None
+
+    # Max number of query tokens among request in the batch.
+    max_decode_query_len: Optional[int] = None
+
+    # (batch_size + 1,). The cumulative subquery lengths of the sequences in
+    # the batch, used to index into subquery. E.g., if the subquery length
+    # is [4, 6], it is [0, 4, 10].
+    query_start_loc: Optional[torch.Tensor] = None
+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
+    # the batch, used to index into sequence. E.g., if the sequence length is
+    # [4, 6], it is [0, 4, 10].
+    seq_start_loc: Optional[torch.Tensor] = None
+
+    _cached_prefill_metadata: Optional["FlashAttentionMetadata"] = None
+    _cached_decode_metadata: Optional["FlashAttentionMetadata"] = None
+
+    # Begin encoder attn & enc/dec cross-attn fields...
+
+    # Encoder sequence lengths representation
+    encoder_seq_lens: Optional[List[int]] = None
+    encoder_seq_lens_tensor: Optional[torch.Tensor] = None
+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
+    # the batch, used to index into sequence. E.g., if the sequence length is
+    # [4, 6], it is [0, 4, 10].
+    encoder_seq_start_loc: Optional[torch.Tensor] = None
+    # Maximum sequence length among encoder sequences
+    max_encoder_seq_len: Optional[int] = None
+    # Number of tokens input to encoder
+    num_encoder_tokens: Optional[int] = None
+
+    # Cross-attention memory-mapping data structures: slot mapping
+    # and block tables
+    cross_slot_mapping: Optional[torch.Tensor] = None
+    cross_block_tables: Optional[torch.Tensor] = None
+
+    @property
+    def is_all_encoder_attn_metadata_set(self):
+        '''
+        All attention metadata required for encoder attention is set.
+        '''
+        return is_all_encoder_attn_metadata_set(self)
+
+    @property
+    def is_all_cross_attn_metadata_set(self):
+        '''
+        All attention metadata required for enc/dec cross-attention is set.
+
+        Superset of encoder attention required metadata.
+        '''
+        return is_all_cross_attn_metadata_set(self)
+
+    @property
+    def prefill_metadata(self) -> Optional["FlashAttentionMetadata"]:
+        if self.num_prefills == 0:
+            return None
+
+        if self._cached_prefill_metadata is not None:
+            return self._cached_prefill_metadata
+
+        assert ((self.seq_lens is not None)
+                or (self.encoder_seq_lens is not None))
+        assert ((self.seq_lens_tensor is not None)
+                or (self.encoder_seq_lens_tensor is not None))
+
+        # Compute some attn_metadata fields which default to None
+        query_start_loc = (None if self.query_start_loc is None else
+                           self.query_start_loc[:self.num_prefills + 1])
+        slot_mapping = (None if self.slot_mapping is None else
+                        self.slot_mapping[:self.num_prefill_tokens])
+        seq_lens = (None if self.seq_lens is None else
+                    self.seq_lens[:self.num_prefills])
+        seq_lens_tensor = (None if self.seq_lens_tensor is None else
+                           self.seq_lens_tensor[:self.num_prefills])
+        seq_start_loc = (None if self.seq_start_loc is None else
+                         self.seq_start_loc[:self.num_prefills + 1])
+        context_lens_tensor = (None if self.context_lens_tensor is None else
+                               self.context_lens_tensor[:self.num_prefills])
+        block_tables = (None if self.block_tables is None else
+                        self.block_tables[:self.num_prefills])
+
+        self._cached_prefill_metadata = FlashAttentionMetadata(
+            num_prefills=self.num_prefills,
+            num_prefill_tokens=self.num_prefill_tokens,
+            num_decode_tokens=0,
+            slot_mapping=slot_mapping,
+            multi_modal_placeholder_index_maps=self.
+            multi_modal_placeholder_index_maps,
+            enable_kv_scales_calculation=self.enable_kv_scales_calculation,
+            seq_lens=seq_lens,
+            seq_lens_tensor=seq_lens_tensor,
+            max_query_len=self.max_query_len,
+            max_prefill_seq_len=self.max_prefill_seq_len,
+            max_decode_query_len=0,
+            max_decode_seq_len=0,
+            query_start_loc=query_start_loc,
+            seq_start_loc=seq_start_loc,
+            context_lens_tensor=context_lens_tensor,
+            block_tables=block_tables,
+            use_cuda_graph=False,
+            # Begin encoder & cross attn fields below...
+            encoder_seq_lens=self.encoder_seq_lens,
+            encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
+            encoder_seq_start_loc=self.encoder_seq_start_loc,
+            max_encoder_seq_len=self.max_encoder_seq_len,
+            cross_slot_mapping=self.cross_slot_mapping,
+            cross_block_tables=self.cross_block_tables)
+        return self._cached_prefill_metadata
+
+    @property
+    def decode_metadata(self) -> Optional["FlashAttentionMetadata"]:
+        if self.num_decode_tokens == 0:
+            return None
+
+        if self._cached_decode_metadata is not None:
+            return self._cached_decode_metadata
+        assert ((self.seq_lens_tensor is not None)
+                or (self.encoder_seq_lens_tensor is not None))
+
+        # Compute some attn_metadata fields which default to None
+        slot_mapping = (None if self.slot_mapping is None else
+                        self.slot_mapping[self.num_prefill_tokens:])
+        seq_lens_tensor = (None if self.seq_lens_tensor is None else
+                           self.seq_lens_tensor[self.num_prefills:])
+        block_tables = (None if self.block_tables is None else
+                        self.block_tables[self.num_prefills:])
+
+        self._cached_decode_metadata = FlashAttentionMetadata(
+            num_prefills=0,
+            num_prefill_tokens=0,
+            num_decode_tokens=self.num_decode_tokens,
+            slot_mapping=slot_mapping,
+            multi_modal_placeholder_index_maps=None,
+            enable_kv_scales_calculation=True,
+            seq_lens=None,
+            seq_lens_tensor=seq_lens_tensor,
+            max_decode_query_len=self.max_decode_query_len,
+            max_query_len=self.max_query_len,
+            max_prefill_seq_len=0,
+            max_decode_seq_len=self.max_decode_seq_len,
+            # Batch may be composed of prefill|decodes, adjust query start
+            # indices to refer to the start of decodes. E.g.
+            # in tokens:[3 prefills|6 decodes], query_start_loc=[3,9] => [0,6].
+            query_start_loc=(self.query_start_loc[self.num_prefills:] -
+                             self.query_start_loc[self.num_prefills])
+            if self.query_start_loc is not None else None,
+            seq_start_loc=self.seq_start_loc[self.num_prefills:]
+            if self.seq_start_loc is not None else None,
+            context_lens_tensor=None,
+            block_tables=block_tables,
+            use_cuda_graph=self.use_cuda_graph,
+            # Begin encoder & cross attn fields below...
+            encoder_seq_lens=self.encoder_seq_lens,
+            encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
+            encoder_seq_start_loc=self.encoder_seq_start_loc,
+            max_encoder_seq_len=self.max_encoder_seq_len,
+            cross_slot_mapping=self.cross_slot_mapping,
+            cross_block_tables=self.cross_block_tables)
+        return self._cached_decode_metadata
+
+    def advance_step(self,
+                     model_input: "ModelInputForGPUWithSamplingMetadata",
+                     sampled_token_ids: Optional[torch.Tensor],
+                     block_size: int,
+                     num_seqs: int,
+                     num_queries: int,
+                     turn_prefills_into_decodes: bool = False):
+        """
+        Update metadata in-place to advance one decode step.
+        """
+        # When using cudagraph, the num_seqs is padded to the next captured
+        # batch sized, but num_queries tracks the actual number of requests in
+        # the batch. For --enforce-eager mode, num_seqs == num_queries
+        if num_seqs != num_queries:
+            assert num_seqs > num_queries
+            assert self.use_cuda_graph
+
+        if turn_prefills_into_decodes:
+            # When Mutli-Step is enabled with Chunked-Prefill, prefills and
+            # decodes are scheduled together. In the first step, all the
+            # prefills turn into decodes. This update reflects that
+            # conversion.
+            assert self.num_decode_tokens + self.num_prefills == num_seqs
+            self.num_decode_tokens += self.num_prefills
+            self.num_prefills = 0
+            self.num_prefill_tokens = 0
+            self.max_prefill_seq_len = 0
+            self.max_query_len = 1
+
+            self.slot_mapping = self.slot_mapping[:num_seqs]
+        else:
+            assert self.seq_lens is not None
+            assert self.max_decode_seq_len == max(self.seq_lens)
+
+        assert self.num_prefills == 0
+        assert self.num_prefill_tokens == 0
+        assert self.num_decode_tokens == num_seqs
+        assert self.slot_mapping.shape == (num_seqs, )
+
+        assert self.seq_lens is not None
+        assert len(self.seq_lens) == num_seqs
+        assert self.seq_lens_tensor is not None
+        assert self.seq_lens_tensor.shape == (num_seqs, )
+        assert self.max_query_len == 1
+        assert self.max_prefill_seq_len == 0
+
+        assert self.query_start_loc is not None
+        assert self.query_start_loc.shape == (num_queries + 1, )
+        assert self.seq_start_loc is not None
+        assert self.seq_start_loc.shape == (num_seqs + 1, )
+
+        assert self.context_lens_tensor is not None
+        assert self.context_lens_tensor.shape == (num_queries, )
+
+        assert self.block_tables is not None
+        assert self.block_tables.shape[0] == num_seqs
+
+        # Update query lengths. Note that we update only queries and not seqs,
+        # since tensors may be padded due to captured cuda graph batch size
+        for i in range(num_queries):
+            self.seq_lens[i] += 1
+        self.max_decode_seq_len = max(self.seq_lens)
+
+        ops.advance_step_flashattn(num_seqs=num_seqs,
+                                   num_queries=num_queries,
+                                   block_size=block_size,
+                                   input_tokens=model_input.input_tokens,
+                                   sampled_token_ids=sampled_token_ids,
+                                   input_positions=model_input.input_positions,
+                                   seq_lens=self.seq_lens_tensor,
+                                   slot_mapping=self.slot_mapping,
+                                   block_tables=self.block_tables)
+
+
+class FlashAttentionMetadataBuilder(
+        AttentionMetadataBuilder[FlashAttentionMetadata]):
+
+    def __init__(self, input_builder: "ModelInputForGPUBuilder"):
+        self.input_builder = input_builder
+        self.runner = input_builder.runner
+        self.sliding_window = input_builder.sliding_window
+        self.block_size = input_builder.block_size
+
+    def prepare(self):
+        self.slot_mapping: List[int] = []
+        self.prefill_seq_lens: List[int] = []
+        self.context_lens: List[int] = []
+        self.block_tables: List[List[int]] = []
+        self.curr_seq_lens: List[int] = []
+        self.multimodal_placeholder_maps: Dict[
+            str,
+            MultiModalPlaceholderMap] = defaultdict(MultiModalPlaceholderMap)
+        self.num_prefills = 0
+        self.num_prefill_tokens = 0
+        self.num_decode_tokens = 0
+        self.has_prefix_cache_hit = False
+
+    def _add_seq_group(
+            self, inter_data: "ModelInputForGPUBuilder.InterDataForSeqGroup",
+            chunked_prefill_enabled: bool, prefix_cache_hit: bool):
+        """Add a sequence group to the metadata. Specifically update/append
+        1. context length.
+        2. block table.
+        3. slot mapping.
+        """
+        is_prompt = inter_data.is_prompt
+        block_tables = inter_data.block_tables
+
+        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,
+             curr_sliding_window_block) in zip(
+                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],
+                 inter_data.orig_seq_lens, inter_data.seq_lens,
+                 inter_data.query_lens, inter_data.context_lens,
+                 inter_data.curr_sliding_window_blocks):
+            self.context_lens.append(context_len)
+
+            if is_prompt:
+                mm_maps = inter_data.multi_modal_placeholder_maps
+                if mm_maps:
+                    for modality, placeholders in mm_maps.items():
+                        self.multimodal_placeholder_maps[modality].extend(
+                            placeholders)
+
+                self.num_prefills += 1
+                self.num_prefill_tokens += token_len
+                self.prefill_seq_lens.append(seq_len)
+            else:
+                self.num_decode_tokens += query_len
+                self.curr_seq_lens.append(curr_seq_len)
+
+            # Compute block table.
+            # TODO(sang): Combine chunked prefill and prefix caching by
+            # only allowing multiple of block_size chunk size.
+            # NOTE: This only works for oooooooxxx style attention.
+            block_table = []
+            if prefix_cache_hit:
+                # NOTE(woosuk): For flash-attn, the block table should
+                # include the entries for the incoming prefill tokens.
+                block_table = block_tables[seq_id]
+            elif ((chunked_prefill_enabled or not is_prompt)
+                  and block_tables is not None):
+                if curr_sliding_window_block == 0:
+                    block_table = block_tables[seq_id]
+                else:
+                    block_table = block_tables[seq_id][
+                        -curr_sliding_window_block:]
+            self.block_tables.append(block_table)
+
+            # Compute slot mapping.
+            is_profile_run = is_block_tables_empty(block_tables)
+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,
+                                                       context_len,
+                                                       self.sliding_window)
+            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,
+                                 seq_len, context_len, start_idx,
+                                 self.block_size, inter_data.block_tables)
+
+    def _get_graph_runner_block_tables(
+            self, num_seqs: int,
+            block_tables: List[List[int]]) -> torch.Tensor:
+        # The shape of graph_block_tables is
+        # [max batch size, max context len // block size].
+        max_batch_size, max_blocks = self.runner.graph_block_tables.shape
+        assert max_batch_size >= num_seqs
+
+        graph_block_tables = self.runner.graph_block_tables[:num_seqs]
+        for i, block_table in enumerate(block_tables):
+            if block_table:
+                num_blocks = len(block_table)
+                if num_blocks <= max_blocks:
+                    graph_block_tables[i, :num_blocks] = block_table
+                else:
+                    # It may be possible to have more blocks allocated due
+                    # to lookahead slots of multi-step, however, they are
+                    # not used anyway, so can be safely ignored.
+                    graph_block_tables[
+                        i, :max_blocks] = block_table[:max_blocks]
+
+        return torch.from_numpy(graph_block_tables).to(
+            device=self.runner.device, non_blocking=True)
+
+    def build(self, seq_lens: List[int], query_lens: List[int],
+              cuda_graph_pad_size: int, batch_size: int):
+        """Build attention metadata with on-device tensors.
+
+        Args:
+            seq_lens: The maybe padded sequence lengths of the input sequences.
+            query_lens: The query lengths of the input sequences.
+            cuda_graph_pad_size: The padding size for cuda graph.
+                                 -1 if cuda graph is not used.
+            batch_size: The maybe padded batch size.
+        """
+        prefix_cache_hit = any([
+            inter_data.prefix_cache_hit
+            for inter_data in self.input_builder.inter_data_list
+        ])
+        for inter_data in self.input_builder.inter_data_list:
+            self._add_seq_group(inter_data,
+                                self.input_builder.chunked_prefill_enabled,
+                                prefix_cache_hit)
+
+        device = self.runner.device
+        use_captured_graph = cuda_graph_pad_size != -1
+
+        max_query_len = max(query_lens)
+        decode_query_lens = query_lens[self.num_prefills:]
+        if len(decode_query_lens) > 0:
+            max_decode_query_len = max(decode_query_lens)
+        else:
+            max_decode_query_len = 1
+        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)
+        max_decode_seq_len = max(self.curr_seq_lens, default=0)
+        num_decode_tokens = self.num_decode_tokens
+        query_start_loc = list(accumulate(query_lens, initial=0))
+        seq_start_loc = list(accumulate(seq_lens, initial=0))
+
+        num_seqs = len(seq_lens)
+        if use_captured_graph:
+            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)
+            self.block_tables.extend([] * cuda_graph_pad_size)
+            num_decode_tokens = batch_size - self.num_prefill_tokens
+            block_tables = self._get_graph_runner_block_tables(
+                num_seqs, self.block_tables)
+        else:
+            block_tables = make_tensor_with_pad(
+                self.block_tables,
+                pad=0,
+                dtype=torch.int,
+                device=device,
+            )
+        assert max_query_len > 0, ("query_lens: {}".format(query_lens))
+
+        assert device is not None
+        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,
+                                               device, self.runner.pin_memory)
+        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,
+                                           self.runner.pin_memory)
+        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
+                                               device, self.runner.pin_memory)
+        query_start_loc_tensor = async_tensor_h2d(query_start_loc, torch.int32,
+                                                  device,
+                                                  self.runner.pin_memory)
+        seq_start_loc_tensor = async_tensor_h2d(seq_start_loc, torch.int32,
+                                                device, self.runner.pin_memory)
+        placeholder_index_maps = {
+            modality: placeholder_map.index_map()
+            for modality, placeholder_map in
+            self.multimodal_placeholder_maps.items()
+        }
+
+        return FlashAttentionMetadata(
+            num_prefills=self.num_prefills,
+            slot_mapping=slot_mapping_tensor,
+            num_prefill_tokens=self.num_prefill_tokens,
+            num_decode_tokens=num_decode_tokens,
+            seq_lens=seq_lens,
+            multi_modal_placeholder_index_maps=placeholder_index_maps,
+            enable_kv_scales_calculation=True,
+            seq_lens_tensor=seq_lens_tensor,
+            max_query_len=max_query_len,
+            max_decode_query_len=max_decode_query_len,
+            max_prefill_seq_len=max_prefill_seq_len,
+            max_decode_seq_len=max_decode_seq_len,
+            query_start_loc=query_start_loc_tensor,
+            seq_start_loc=seq_start_loc_tensor,
+            context_lens_tensor=context_lens_tensor,
+            block_tables=block_tables,
+            use_cuda_graph=use_captured_graph,
+        )
+
+
+class FlashAttentionImpl(AttentionImpl):
+    """
+    If the input tensors contain prompt tokens, the layout is as follows:
+    |<--------------- num_prefill_tokens ----------------->|	
+    |<--prefill_0-->|<--prefill_1-->|...|<--prefill_N-1--->|
+
+    Otherwise, the layout is as follows:	
+    |<----------------- num_decode_tokens ------------------>|	
+    |<--decode_0-->|..........|<--decode_M-1-->|<--padding-->|
+
+    Generation tokens can contain padding when cuda-graph is used.
+    Currently, prompt tokens don't contain any padding.
+
+    The prompts might have different lengths, while the generation tokens
+    always have length 1.
+
+    If chunked prefill is enabled, prefill tokens and decode tokens can be
+    batched together in a flattened 1D query.
+
+    |<----- num_prefill_tokens ---->|<------- num_decode_tokens --------->|
+    |<-prefill_0->|...|<-prefill_N-1->|<--decode_0-->|...|<--decode_M-1-->|
+
+    Currently, cuda graph is disabled for chunked prefill, meaning there's no
+    padding between prefill and decode tokens.
+    """
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: int,
+        alibi_slopes: Optional[List[float]],
+        sliding_window: Optional[int],
+        kv_cache_dtype: str,
+        blocksparse_params: Optional[Dict[str, Any]] = None,
+        logits_soft_cap: Optional[float] = None,
+        attn_type: str = AttentionType.DECODER,
+    ) -> None:
+        if blocksparse_params is not None:
+            raise ValueError(
+                "FlashAttention does not support block-sparse attention.")
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = float(scale)
+        self.num_kv_heads = num_kv_heads
+        if alibi_slopes is not None:
+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)
+        self.alibi_slopes = alibi_slopes
+        self.sliding_window = ((sliding_window - 1,
+                                0) if sliding_window is not None else (-1, -1))
+        self.kv_cache_dtype = kv_cache_dtype
+        if logits_soft_cap is None:
+            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.
+            logits_soft_cap = 0
+        self.logits_soft_cap = logits_soft_cap
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+        support_head_sizes = FlashAttentionBackend.get_supported_head_sizes()
+        if head_size not in support_head_sizes:
+            raise ValueError(
+                f"Head size {head_size} is not supported by FlashAttention. "
+                f"Supported head sizes are: {support_head_sizes}.")
+        self.attn_type = attn_type
+
+        # if hopper default to FA3, otherwise stick to FA2 for now
+        # TODO(lucas): profile FA3 on ampere to see if it makes sense to
+        #  use FA3 as default for both
+        #if current_platform.get_device_capability()[0] >= 9:
+        #    self.fa_version = 3 if is_fa_version_supported(3) else 2
+        #else:
+        #    self.fa_version = 2
+
+        #if VLLM_FLASH_ATTN_VERSION is not None:
+        #    assert VLLM_FLASH_ATTN_VERSION in [2, 3]
+        #    self.fa_version = VLLM_FLASH_ATTN_VERSION
+
+        #if not is_fa_version_supported(self.fa_version):
+        #    logger.error("Cannot use FA version %d is not supported due to %s",
+        #                 self.fa_version,
+        #                 fa_version_unsupported_reason(self.fa_version))
+
+        #assert is_fa_version_supported(self.fa_version)
+
+    def forward(
+        self,
+        layer: AttentionLayer,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: FlashAttentionMetadata,
+        output: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """Forward pass with FlashAttention.
+
+        Args:
+            query: shape = [num_tokens, num_heads, head_size]
+            key: shape = [num_tokens, num_kv_heads, head_size]
+            value: shape = [num_tokens, num_kv_heads, head_size]
+            output: shape = [num_tokens, num_heads, head_size]
+            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
+                NOTE: kv_cache will be an empty tensor with shape [0]
+                for profiling run.
+            attn_metadata: Metadata for attention.
+        NOTE: It in-place updates the output tensor.
+        """
+        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.
+        assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0, (
+            "key/v_scale is not supported in FlashAttention.")
+
+        assert output is not None, "Output tensor must be provided."
+        k_scale = layer._k_scale
+        v_scale = layer._v_scale
+
+        attn_type = self.attn_type
+        if (attn_type == AttentionType.ENCODER
+                and (not attn_metadata.is_all_encoder_attn_metadata_set)):
+            raise AttributeError("Encoder attention requires setting "
+                                 "encoder metadata attributes.")
+        elif (attn_type == AttentionType.ENCODER_DECODER
+              and (not attn_metadata.is_all_cross_attn_metadata_set)):
+            raise AttributeError("Encoder/decoder cross-attention "
+                                 "requires setting cross-attention "
+                                 "metadata attributes.")
+
+        kv_cache_dtype: str = self.kv_cache_dtype
+        softmax_scale: float = self.scale
+        window_size = self.sliding_window
+        alibi_slopes: Optional[torch.Tensor] = self.alibi_slopes
+        logits_soft_cap: Optional[float] = self.logits_soft_cap
+
+        if kv_cache.numel() > 0:
+            #key_cache = kv_cache[0]
+            #value_cache = kv_cache[1]
+            key_cache, value_cache = PagedAttention.split_kv_cache(
+                kv_cache, self.num_kv_heads, self.head_size)
+            # We skip updating the KV cache under two conditions:
+            #  a. When the Attention Type is ENCODER. In this phase, we compute
+            #     only the encoder attention without updating the cache.
+            #  b. When both Key and Value are None. This occurs during
+            #     cross-attention computation in the decoding phase, where the
+            #     KV cache is already populated with the cross-attention
+            #     tensor. Thus, we skip cache updates during this time.
+            if (attn_type != AttentionType.ENCODER) and (key is not None) and (
+                    value is not None):
+                if attn_type == AttentionType.ENCODER_DECODER:
+                    # Update cross-attention KV cache (prefill-only)
+                    updated_slot_mapping = attn_metadata.cross_slot_mapping
+                else:
+                    # Update self-attention KV cache (prefill/decode)
+                    updated_slot_mapping = attn_metadata.slot_mapping
+
+                # Reshape the input keys and values and store them in the cache.
+                # If kv_cache is not provided, the new key and value tensors are
+                # not cached. This happens during the initial memory
+                # profiling run.
+                #torch.ops._C_cache_ops.reshape_and_cache_flash(
+                #    key,
+                #    value,
+                #    kv_cache[0],
+                #    kv_cache[1],
+                #    updated_slot_mapping.flatten(),  # type: ignore[union-attr]
+                #    kv_cache_dtype,
+                #    layer._k_scale,
+                #    layer._v_scale,
+                #)
+                PagedAttention.write_to_paged_cache(key, value, key_cache,
+                                                value_cache,
+                                                attn_metadata.slot_mapping,
+                                                self.kv_cache_dtype, k_scale, v_scale)
+
+        (num_prefill_query_tokens, num_prefill_kv_tokens,
+        num_decode_query_tokens) = \
+            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)
+        decode_query = query[num_prefill_query_tokens:]
+        decode_output = output[num_prefill_query_tokens:]
+        # QKV for prefill.
+        query = query[:num_prefill_query_tokens]
+        prefill_output = output[:num_prefill_query_tokens]
+        assert query.shape[0] == num_prefill_query_tokens
+        assert decode_query.shape[0] == num_decode_query_tokens
+
+        if prefill_meta := attn_metadata.prefill_metadata:
+            # Prompt run.
+            if (kv_cache.numel() == 0 or prefill_meta.block_tables is None
+                    or prefill_meta.block_tables.numel() == 0):
+                # normal attention
+                # When block_tables are not filled, it means q and k are the
+                # prompt, and they have the same length.
+                q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len = \
+                    _get_query_key_seq_metadata(prefill_meta, True, attn_type)
+
+                key = key[:num_prefill_kv_tokens]
+                value = value[:num_prefill_kv_tokens]
+
+                #flash_attn_varlen_func(
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
+                    q=query,
+                    k=key,
+                    v=value,
+                    cu_seqlens_q=q_seq_start_loc,
+                    cu_seqlens_k=k_seq_start_loc,
+                    max_seqlen_q=q_seq_len,
+                    max_seqlen_k=k_seq_len,
+                    softmax_scale=softmax_scale,
+                    causal=_get_causal_option(attn_type),
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    softcap=logits_soft_cap,
+                    #out=prefill_output,
+                    #fa_version=self.fa_version,
+                )
+            else:
+                # prefix-enabled attention
+                assert attn_type == AttentionType.DECODER, (
+                    "Only decoder-only models support prefix caching")
+                assert prefill_meta.seq_lens is not None
+                max_seq_len = max(prefill_meta.seq_lens)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads, self.head_size//(32 // kv_cache.element_size()), -1, 2, 16 // kv_cache.element_size())
+                key_cache = key_cache.permute(0,1,2,4,3,5)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads,self.head_size//(16 // kv_cache.element_size()),-1,16 // kv_cache.element_size())
+                value_cache =  value_cache.permute(0,1,3,2)
+                #flash_attn_varlen_func(  # noqa
+                """
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
+                    q=query,
+                    k=key_cache,
+                    v=value_cache,
+                    cu_seqlens_q=prefill_meta.query_start_loc,
+                    max_seqlen_q=prefill_meta.max_query_len,
+                    seqused_k=prefill_meta.seq_lens_tensor,
+                    max_seqlen_k=max_seq_len,
+                    softmax_scale=softmax_scale,
+                    causal=True,
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    block_table=prefill_meta.block_tables,
+                    softcap=logits_soft_cap,
+                    #out=prefill_output,
+                    #fa_version=self.fa_version,
+                )
+                """
+                output[:num_prefill_query_tokens] = PagedAttention.forward_prefix(
+                    query,
+                    key,
+                    value,
+                    self.kv_cache_dtype,
+                    key_cache,
+                    value_cache,
+                    attn_metadata.block_tables,
+                    attn_metadata.query_start_loc,
+                    attn_metadata.seq_lens_tensor,
+                    attn_metadata.context_lens_tensor,
+                    attn_metadata.max_query_len,
+                    self.alibi_slopes,
+                    self.logits_soft_cap,
+                    k_scale,
+                    v_scale,
+                )
+                value_cache =  value_cache.permute(0,1,3,2)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads,self.head_size//(32 // kv_cache.element_size()),2,-1,16 // kv_cache.element_size())
+                key_cache = key_cache.permute(0,1,2,4,3,5)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads, self.head_size//(32 // kv_cache.element_size()), -1, 32 // kv_cache.element_size())
+
+        if decode_meta := attn_metadata.decode_metadata:
+            # Decoding run.
+            # Use flash_attn_varlen_func kernel for speculative decoding
+            # because different queries might have different lengths.
+
+            assert decode_meta.max_decode_query_len is not None
+            # use only for actual varlen decoding
+            if decode_meta.max_decode_query_len > 1:
+                assert attn_type == AttentionType.DECODER, (
+                    "Only decoder-only models support max_decode_query_len > 1"
+                )
+                """
+                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
+                    q=decode_query,
+                    k=key_cache,
+                    v=value_cache,
+                    cu_seqlens_q=decode_meta.query_start_loc,
+                    max_seqlen_q=decode_meta.max_decode_query_len,
+                    seqused_k=decode_meta.seq_lens_tensor,
+                    max_seqlen_k=decode_meta.max_decode_seq_len,
+                    softmax_scale=softmax_scale,
+                    causal=True,
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    softcap=logits_soft_cap,
+                    block_table=decode_meta.block_tables,
+                    #out=decode_output,
+                    #fa_version=self.fa_version,
+                )
+                """
+                output[num_prefill_query_tokens:] = PagedAttention.forward_decode(
+                    decode_query,
+                    key_cache,
+                    value_cache,
+                    block_tables=attn_metadata.block_tables,
+                    seq_lens=attn_metadata.seq_lens_tensor,    #attn_metadata.context_lens
+                    max_seq_len=attn_metadata.max_decode_seq_len,  #attn_metadata.max_context_len
+                    kv_cache_dtype='auto',
+                    num_kv_heads=self.num_kv_heads,
+                    scale=self.scale,
+                    alibi_slopes=self.alibi_slopes,
+                    k_scale=k_scale,
+                    v_scale=v_scale,
+                )
+            else:
+                # Use flash_attn_with_kvcache for normal decoding.
+                (
+                    seq_lens_arg,
+                    _,
+                    block_tables_arg,
+                ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
+                """
+                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
+                    q=decode_query.unsqueeze(1),
+                    k_cache=key_cache,
+                    v_cache=value_cache,
+                    block_table=block_tables_arg,
+                    cache_seqlens=seq_lens_arg,
+                    softmax_scale=softmax_scale,
+                    causal=True,
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    softcap=logits_soft_cap,
+                    #out=decode_output.unsqueeze(1),
+                    #fa_version=self.fa_version,
+                ).squeeze(1)
+                """
+                output[num_prefill_query_tokens:] = PagedAttention.forward_decode(
+                    decode_query,
+                    key_cache,
+                    value_cache,
+                    block_tables=attn_metadata.block_tables,
+                    seq_lens=attn_metadata.seq_lens_tensor,    #attn_metadata.context_lens
+                    max_seq_len=attn_metadata.max_decode_seq_len,  #attn_metadata.max_context_len
+                    kv_cache_dtype='auto',
+                    num_kv_heads=self.num_kv_heads,
+                    scale=self.scale,
+                    alibi_slopes=self.alibi_slopes,
+                    k_scale=k_scale,
+                    v_scale=v_scale,
+                )
+        return output
+
+
+def _get_query_key_seq_metadata(
+    attn_metadata,
+    is_prompt: bool,
+    attn_type: str,
+) -> tuple:
+    """
+    Returns sequence metadata for key and query based on the specified 
+    attention type and whether input is a prompt.
+
+    This function computes the starting locations and maximum sequence lengths 
+    for key and query sequences for different attention types.
+
+    Args:
+        attn_metadata: The attention metadata object
+        is_prompt (bool): A flag indicating if the input is a prompt
+        attn_type (AttentionType): The type of attention being used.
+
+    Returns:
+        tuple: A tuple containing four integers:
+            - Starting location for the query sequence.
+            - Maximum sequence length for the query sequence.
+            - Starting location for the key sequence.
+            - Maximum sequence length for the key sequence.
+
+    Raises:
+        AttributeError: If an invalid attention type is provided.
+    """
+    if attn_type == AttentionType.DECODER:
+        # Decoder self-attention
+        # Choose max_seq_len based on whether we are in prompt_run
+        if is_prompt:
+            max_seq_len = attn_metadata.max_prefill_seq_len
+        else:
+            max_seq_len = attn_metadata.max_decode_seq_len
+        return (attn_metadata.seq_start_loc, max_seq_len,
+                attn_metadata.seq_start_loc, max_seq_len)
+
+    elif attn_type == AttentionType.ENCODER_DECODER:
+        # This is cross attention between the where the key
+        # is the precomputed encoder attention and query
+        # is the input sequence.
+        # Choose query max length based on whether it is prompt
+        # or not.
+        if is_prompt:
+            max_seq_len = attn_metadata.max_prefill_seq_len
+        else:
+            max_seq_len = attn_metadata.max_decode_seq_len
+        return (attn_metadata.seq_start_loc, max_seq_len,
+                attn_metadata.encoder_seq_start_loc,
+                attn_metadata.max_encoder_seq_len)
+    elif attn_type == AttentionType.ENCODER:
+        # For encoder attention both the query and the key are same i.e the
+        # encoder sequence.
+        return (attn_metadata.encoder_seq_start_loc,
+                attn_metadata.max_encoder_seq_len,
+                attn_metadata.encoder_seq_start_loc,
+                attn_metadata.max_encoder_seq_len)
+    elif attn_type == AttentionType.ENCODER_ONLY:
+        assert is_prompt, "Should not have decode for encoder only model."
+        return (attn_metadata.seq_start_loc, attn_metadata.max_prefill_seq_len,
+                attn_metadata.seq_start_loc, attn_metadata.max_prefill_seq_len)
+    else:
+        raise AttributeError(f"Invalid attention type {str(attn_type)}")
+
+
+def _get_causal_option(attn_type: str) -> bool:
+    """
+    Determine whether the given attention type is suitable for causal 
+    attention mechanisms.
+
+    Args:
+        attn_type (AttentionType): The type of attention being evaluated
+
+    Returns:
+        bool: Returns `True` if the attention type is suitable for causal 
+        attention (i.e., not encoder, encoder-only, or encoder-decoder), 
+        otherwise returns `False`.
+    """
+    return not (attn_type == AttentionType.ENCODER
+                or attn_type == AttentionType.ENCODER_ONLY
+                or attn_type == AttentionType.ENCODER_DECODER)
diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py
index cd8c08e5a..5d0fe19d3 100644
--- a/vllm/attention/backends/mla/utils.py
+++ b/vllm/attention/backends/mla/utils.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from abc import abstractmethod
@@ -218,16 +219,6 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
                 .view(-1, self.num_heads, self.kv_lora_rank)
 
     def process_weights_after_loading(self, act_dtype: torch.dtype):
-
-        def is_layer_fp8(layer: LinearBase) -> bool:
-            return isinstance(layer.quant_method, Fp8LinearMethod) or\
-                (isinstance(layer.quant_method, CompressedTensorsLinearMethod)\
-                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8))
-
-        def quantization_scheme_supported(layer: LinearBase) -> bool:
-            return isinstance(layer.quant_method, UnquantizedLinearMethod) or \
-                is_layer_fp8(layer)
-
         # TODO(lucas) This is very gross, we need a more wide scale refactor of
         # all the FP8 code with a more standard way of
         # defining schemes/group-shapes, we should also potentially force
@@ -237,7 +228,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
         def get_scale_group_shapes_for_fp8(layer: LinearBase) -> \
             Tuple[Tuple[int, int], Tuple[int, int]]:
             if isinstance(layer.quant_method, Fp8LinearMethod):
-                if layer.quant_method.block_quant is not None:
+                if layer.quant_method.block_quant:
                     weight_block_size = \
                         layer.quant_method.quant_config.weight_block_size
                     # per-token-group (1, X), block-quantized (X, Y)
@@ -265,41 +256,34 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
                     f"{layer.quant_method}, please run with VLLM_MLA_DISABLE=1"
                 )
 
-        def get_scales(layer: LinearBase) -> torch.Tensor:
-            if hasattr(layer, "weight_scale_inv"):
-                return layer.weight_scale_inv
-            return layer.weight_scale
+        def get_layer_weight(layer):
+            WEIGHT_NAMES = ("weight", "qweight", "weight_packed")
+            for attr in WEIGHT_NAMES:
+                if hasattr(layer, attr):
+                    return getattr(layer, attr)
+            raise AttributeError(
+                f"Layer '{layer}' has no recognized weight attribute:"
+                f" {WEIGHT_NAMES}.")
 
         def get_and_maybe_dequant_weights(layer: LinearBase):
-            if is_layer_fp8(layer):
-                if isinstance(layer.quant_method, \
-                    CompressedTensorsLinearMethod) and \
-                    isinstance(layer.scheme, CompressedTensorsW8A8Fp8):
-                    # NOTE(lucas): note sure why but `CompressedTensorsW8A8Fp8`
-                    # seems to store weights as (input, output) instead of
-                    # (output, input) so we need to transpose
-                    weight = layer.weight.T  # standardize to (output, input)
-                else:
-                    weight = layer.weight
-                _, weight_scale_group_shape = \
-                    get_scale_group_shapes_for_fp8(layer)
-                scales = get_scales(layer)
-
-                return scaled_dequantize(weight, scales,
-                                         weight_scale_group_shape)
-            else:
-                return layer.weight
-
-        if not (quantization_scheme_supported(self.kv_b_proj) and\
-            quantization_scheme_supported(self.q_proj) and\
-                quantization_scheme_supported(self.o_proj)):
-            raise NotImplementedError(
-                "Only FP8 and UnquantizedLinearMethod are supported for MLA"
-                ", please run with VLLM_MLA_DISABLE=1")
-
-        weight_dtype = self.kv_b_proj.weight.dtype
-        assert self.o_proj.weight.dtype == weight_dtype
-        assert self.q_proj.weight.dtype == weight_dtype
+            if not isinstance(layer.quant_method, UnquantizedLinearMethod):
+                # NOTE: This should only be used offline, since it's O(N^3)
+                eye = torch.eye(layer.input_size_per_partition,
+                                dtype=act_dtype,
+                                device=get_layer_weight(layer).device)
+                dequant_weights = layer.quant_method.apply(layer,
+                                                           eye,
+                                                           bias=None)
+                del eye
+                # standardize to (output, input)
+                return dequant_weights.T
+
+            return layer.weight.T
+            #return layer.weight
+
+        weight_dtype = get_layer_weight(self.kv_b_proj).dtype
+        assert get_layer_weight(self.o_proj).dtype == weight_dtype
+        assert get_layer_weight(self.q_proj).dtype == weight_dtype
 
         kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T
         assert kv_b_proj_weight.shape == (
@@ -419,28 +403,11 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
         q_nope: torch.Tensor,
         q_pe: torch.Tensor,
         kv_cache: torch.Tensor,
+        _k_scale_float: torch.Tensor,
         attn_metadata: T,
     ) -> torch.Tensor:
         raise NotImplementedError
 
-    def apply_pure_rope(
-        self,
-        input_positions: torch.Tensor,
-        q_pe: torch.Tensor,
-        k_pe: torch.Tensor,
-    ) -> tuple[torch.Tensor, torch.Tensor]:
-        seq_len = input_positions.size(0)
-        ori_q_pe_shape, ori_k_pe_shape = q_pe.shape, k_pe.shape
-
-        q_pe, k_pe = self.rotary_emb(
-            input_positions,
-            q_pe.reshape(seq_len, -1),
-            k_pe.reshape(seq_len, -1),
-        )
-        q_pe, k_pe = q_pe.view(ori_q_pe_shape), k_pe.view(ori_k_pe_shape)
-
-        return q_pe, k_pe
-
     def forward(
         self,
         layer: AttentionLayer,
@@ -465,14 +432,13 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
         # Restore head dim (for rotary embedding)
         k_pe = k_pe.unsqueeze(1)
         assert hasattr(attn_metadata, "input_positions")
-        rope_fn = (self.rotary_emb
-                   if self.use_yarn_rope else self.apply_pure_rope)
 
         if is_decode:
             q_nope = self._q_proj_and_k_up_proj(hidden_states_or_q_c)
             q_pe = torch.matmul(hidden_states_or_q_c, self.W_QR)\
                 .view(-1, self.num_heads, self.qk_rope_head_dim)
-            q_pe, k_pe = rope_fn(attn_metadata.input_positions, q_pe, k_pe)
+            q_pe, k_pe = self.rotary_emb(attn_metadata.input_positions, q_pe,
+                                         k_pe)
         else:
             assert is_prefill
             q = self.q_proj(hidden_states_or_q_c)[0]\
@@ -480,7 +446,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
 
             # TODO(lucas): there must be a nicer way to write this line
             q[..., self.qk_nope_head_dim:], k_pe = \
-                rope_fn(
+                self.rotary_emb(
                     attn_metadata.input_positions,
                     q[..., self.qk_nope_head_dim:], k_pe)
 
@@ -499,7 +465,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
             return self._forward_prefill(q, k_c_normed, k_pe, attn_metadata)
 
         if attn_metadata.decode_metadata is not None:
-            return self._forward_decode(q_nope, q_pe, kv_cache, attn_metadata)
+            return self._forward_decode(q_nope, q_pe, kv_cache, layer._k_scale, attn_metadata)
 
     # Optional common flash-attn based prefill
     def _forward_prefill_flash(
diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
index 9a1984a93..1ade587f5 100644
--- a/vllm/attention/backends/triton_mla.py
+++ b/vllm/attention/backends/triton_mla.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from collections import defaultdict
@@ -33,6 +34,42 @@ if TYPE_CHECKING:
     from vllm.worker.model_runner import (ModelInputForGPUBuilder,
                                           ModelInputForGPUWithSamplingMetadata)
 
+import json
+import os
+
+# TODO: Configure environment variables temporarily. New versions do not need to be configured
+os.environ['TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP'] = '1'
+os.environ['TRITON_ENABLE_MACA_CHAIN_DOT_OPT'] = '1'
+
+def load_config():
+    # Load JSON data from the file
+    json_path = config_file_path = os.path.join(
+        os.path.dirname(os.path.realpath(__file__)), "configs", "tp8_merge.json")
+    with open(json_path, 'r') as file:
+        data = json.load(file)
+    return data
+
+JSON_DATA = load_config()
+
+def find_best_mla_para(json_data, batch_size, input_len, tp_size):
+    best_match = None
+    best_batch_size_diff = float('inf')
+    best_input_len_diff = float('inf')
+    
+    for entry in json_data:
+        if entry["BS"] == batch_size and entry["L"] == input_len:
+            return entry["num_kv_splits"], entry['num_stages']
+        batch_size_diff = abs(entry["BS"] - batch_size)
+        input_len_diff = abs(entry["L"] - input_len)
+        
+        # Check if this is a better match than the current best match
+        if batch_size_diff < best_batch_size_diff or (batch_size_diff == best_batch_size_diff and input_len_diff < best_input_len_diff):
+            best_match = entry
+            best_batch_size_diff = batch_size_diff
+            best_input_len_diff = input_len_diff
+    
+    # If a match was found, return the best_kv_splits, otherwise return None
+    return best_match["num_kv_splits"],best_match["num_stages"]
 
 class TritonMLABackend(AttentionBackend):
 
@@ -256,15 +293,17 @@ class TritonMLAMetadata(MLACommonMetadata):
 
     num_prefill_tokens: int
 
-    num_kv_splits: int = 4  # TODO(lucas) add heuristic
+    num_kv_splits: int = 16  # TODO(lucas) add heuristic
     attn_logits: Optional[torch.Tensor] = None
     req_idx: Optional[torch.Tensor] = None
 
     # The dimension of the attention heads
     head_dim: Optional[int] = None
+    num_stages: int = 1
 
     def __post_init__(self):
         supported_head_sizes = TritonMLABackend.get_supported_head_sizes()
+
         if self.head_dim is not None and self.head_dim \
                 not in supported_head_sizes:
             raise ValueError(
@@ -342,6 +381,14 @@ class TritonMLAMetadata(MLACommonMetadata):
         input_positions = (None if self.input_positions is None else
                            self.input_positions[self.num_prefill_tokens:])
 
+        if (seq_lens_tensor is not None):
+            batch = seq_lens_tensor.shape[0]
+            max_seq_len = int(seq_lens_tensor.max())
+            num_kv_splits, num_stages = find_best_mla_para(JSON_DATA, batch, max_seq_len, 8)
+        else:
+            num_kv_splits = 4
+            num_stages = 1
+
         self._cached_decode_metadata = TritonMLAMetadata(
             num_prefills=0,
             num_prefill_tokens=0,
@@ -367,7 +414,9 @@ class TritonMLAMetadata(MLACommonMetadata):
             block_tables=block_tables,
             use_cuda_graph=self.use_cuda_graph,
             input_positions=input_positions,
-            head_dim=self.head_dim)
+            head_dim=self.head_dim,
+            num_kv_splits=num_kv_splits,
+            num_stages=num_stages)
         return self._cached_decode_metadata
 
     def advance_step(self,
@@ -438,7 +487,7 @@ class TritonMLAMetadata(MLACommonMetadata):
                                    block_size=block_size,
                                    input_tokens=model_input.input_tokens,
                                    sampled_token_ids=sampled_token_ids,
-                                   input_positions=model_input.input_positions,
+                                   input_positions=self.input_positions,
                                    seq_lens=self.seq_lens_tensor,
                                    slot_mapping=self.slot_mapping,
                                    block_tables=self.block_tables)
@@ -608,7 +657,13 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):
                                                device, self.runner.pin_memory)
         seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,
                                            self.runner.pin_memory)
-        input_positions = async_tensor_h2d(self.input_positions, torch.long,
+
+        # aligned according to batch_size for advance_step_flashattn used
+        input_positions_list = self.input_positions
+        for _ in range(len(self.input_positions), batch_size):
+            input_positions_list.append(0)
+        
+        input_positions = async_tensor_h2d(input_positions_list, torch.long,
                                            device, self.runner.pin_memory)
         slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
                                                device, self.runner.pin_memory)
@@ -623,6 +678,9 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):
             self.multimodal_placeholder_maps.items()
         }
 
+        # TODO(m01124) simple set by jira:C500-30310, need tune
+        num_kv_splits = 16 if max(seq_lens) > 256 else 2
+        
         return TritonMLAMetadata(
             num_prefills=self.num_prefills,
             slot_mapping=slot_mapping_tensor,
@@ -642,7 +700,7 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):
             context_lens_tensor=context_lens_tensor,
             block_tables=block_tables,
             use_cuda_graph=use_captured_graph,
-            num_kv_splits=4,  # TODO(lucas) add heuristic
+            num_kv_splits=num_kv_splits, 
             head_dim=self.runner.model_config.get_head_size(),
         )
 
@@ -700,6 +758,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
         q_nope: torch.Tensor,
         q_pe: torch.Tensor,
         kv_c_and_k_pe_cache: torch.Tensor,
+        kv_scale: torch.Tensor,
         attn_metadata: TritonMLAMetadata,
     ) -> torch.Tensor:
         assert kv_c_and_k_pe_cache.numel() > 0
@@ -740,7 +799,6 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
         decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,
                              decode_meta.block_tables,
                              decode_meta.seq_lens_tensor, attn_logits,
-                             attn_metadata.num_kv_splits, self.scale,
-                             PAGE_SIZE)
-
+                             attn_metadata.num_kv_splits, attn_metadata.num_stages, self.scale,
+                             kv_scale, PAGE_SIZE)
         return self._v_up_proj_and_o_proj(o)
diff --git a/vllm/attention/layer.py b/vllm/attention/layer.py
index e4df7ffc5..1abb1d1ed 100644
--- a/vllm/attention/layer.py
+++ b/vllm/attention/layer.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Attention layer."""
 from typing import Any, Dict, List, Optional
@@ -175,7 +176,7 @@ class Attention(nn.Module):
                 key = key.view(-1, self.num_kv_heads, self.head_size)
             if value is not None:
                 value = value.view(-1, self.num_kv_heads, self.head_size)
-            if self.use_direct_call:
+            if self.use_direct_call or True:
                 forward_context: ForwardContext = get_forward_context()
                 ctx_attn_metadata = forward_context.attn_metadata
                 self_kv_cache = self.kv_cache[forward_context.virtual_engine]
@@ -198,7 +199,7 @@ class Attention(nn.Module):
                 return self.impl.forward(self, query, key, value,
                                          self_kv_cache, ctx_attn_metadata)
             else:
-                return torch.ops.vllm.unified_attention(
+                return unified_attention(
                     query, key, value, self.layer_name)
 
     def calc_kv_scales(self, key, value):
diff --git a/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py b/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
index 71caf3cba..57da5ee6d 100644
--- a/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
+++ b/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import torch
@@ -117,7 +118,7 @@ def blocksparse_flash_attn_varlen_fwd(
                          q_block_size),  # smaller for decoding
         EVEN_D=block_d == head_size,
         num_warps=1 if decoding_only else 4,
-        num_stages=3)
+        num_stages=1)
 
     return out
 
diff --git a/vllm/attention/ops/blocksparse_attention/interface.py b/vllm/attention/ops/blocksparse_attention/interface.py
index 6ab69ea5b..83f16e63a 100644
--- a/vllm/attention/ops/blocksparse_attention/interface.py
+++ b/vllm/attention/ops/blocksparse_attention/interface.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import math
@@ -11,8 +12,9 @@ from .utils import (dense_to_crow_col, get_head_sliding_step,
 
 IS_COMPUTE_8_OR_ABOVE = current_platform.has_device_capability(80)
 
-if IS_COMPUTE_8_OR_ABOVE:
-    from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
+#if IS_COMPUTE_8_OR_ABOVE:
+#    from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
+from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
 
 
 class LocalStridedBlockSparseAttn(torch.nn.Module):
@@ -34,8 +36,7 @@ class LocalStridedBlockSparseAttn(torch.nn.Module):
         super().__init__()
         if use_spda is None:
             use_spda = current_platform.is_rocm() or \
-                        current_platform.is_cpu() or not \
-                        IS_COMPUTE_8_OR_ABOVE
+                        current_platform.is_cpu()
         device = device or (torch.cuda.current_device()
                             if current_platform.is_cuda_alike() else "cpu")
         device = torch.device(device)
@@ -123,10 +124,10 @@ class LocalStridedBlockSparseAttn(torch.nn.Module):
 
         return: tensor of shape as q.
         """
-        assert (
-            IS_COMPUTE_8_OR_ABOVE
-        ), "Requires compute capability of 8 or above (Ampere or newer) to use \
-            Triton kernel."
+        #assert (
+        #    IS_COMPUTE_8_OR_ABOVE
+        #), "Requires compute capability of 8 or above (Ampere or newer) to use \
+        #    Triton kernel."
 
         sm_scale = sm_scale or 1.0 / math.sqrt(q.size(-1))
 
diff --git a/vllm/attention/ops/paged_attn.py b/vllm/attention/ops/paged_attn.py
index 2c60bd0c3..5ffc5198a 100644
--- a/vllm/attention/ops/paged_attn.py
+++ b/vllm/attention/ops/paged_attn.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from dataclasses import dataclass
@@ -53,14 +54,14 @@ class PagedAttention:
         num_kv_heads: int,
         head_size: int,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        x = 16 // kv_cache.element_size()
+        x = 32 // kv_cache.element_size()
         num_blocks = kv_cache.shape[1]
 
         key_cache = kv_cache[0]
         key_cache = key_cache.view(num_blocks, num_kv_heads, head_size // x,
                                    -1, x)
         value_cache = kv_cache[1]
-        value_cache = value_cache.view(num_blocks, num_kv_heads, head_size, -1)
+        value_cache = value_cache.view(num_blocks, num_kv_heads, -1, head_size)
         return key_cache, value_cache
 
     @staticmethod
@@ -74,7 +75,7 @@ class PagedAttention:
         k_scale: torch.Tensor,
         v_scale: torch.Tensor,
     ) -> None:
-        ops.reshape_and_cache(
+        ops.reshape_and_cache_new(
             key,
             value,
             key_cache,
@@ -107,14 +108,14 @@ class PagedAttention:
     ) -> torch.Tensor:
         if blocksparse_vert_stride is not None and blocksparse_vert_stride > 1:
             # use blocksparse paged attention
-            block_size = value_cache.size(-1)
+            block_size = value_cache.shape[2]
             assert (blocksparse_block_size > 0 and
                     blocksparse_block_size % block_size == 0), \
                 (f"{blocksparse_block_size=} needs to be a multiple of"
                  f"{block_size=} used in block_tables.")
 
         output = torch.empty_like(query)
-        block_size = value_cache.shape[3]
+        block_size = value_cache.shape[2]
         num_seqs, num_heads, head_size = query.shape
         max_num_partitions = ((max_seq_len + _PARTITION_SIZE - 1) //
                               _PARTITION_SIZE)
@@ -159,17 +160,24 @@ class PagedAttention:
                 dtype=output.dtype,
                 device=output.device,
             )
+            block_count = torch.zeros(
+                size=(num_seqs, num_heads),
+                dtype=torch.int,
+                device=output.device,
+            )
             exp_sums = torch.empty(
                 size=(num_seqs, num_heads, max_num_partitions),
                 dtype=torch.float32,
                 device=output.device,
             )
             max_logits = torch.empty_like(exp_sums)
+            block_count_init_once = False
             ops.paged_attention_v2(
                 output,
                 exp_sums,
                 max_logits,
                 tmp_output,
+                block_count,
                 query,
                 key_cache,
                 value_cache,
@@ -188,6 +196,7 @@ class PagedAttention:
                 blocksparse_vert_stride,
                 blocksparse_block_size,
                 blocksparse_head_sliding_step,
+                block_count_init_once
             )
         return output
 
diff --git a/vllm/attention/ops/triton_decode_attention.py b/vllm/attention/ops/triton_decode_attention.py
index 057fccb5e..4b31a6247 100644
--- a/vllm/attention/ops/triton_decode_attention.py
+++ b/vllm/attention/ops/triton_decode_attention.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Adapted from
@@ -30,12 +31,13 @@ It supports page size >= 1.
 
 import logging
 
+import torch
 import triton
 import triton.language as tl
 
-from vllm.platforms import current_platform
+# from vllm.platforms import current_platform
 
-is_hip_ = current_platform.is_rocm()
+# is_hip_ = current_platform.is_rocm()
 
 logger = logging.getLogger(__name__)
 
@@ -45,7 +47,6 @@ logger.warning(
     "The following error message 'operation scheduled before its operands' "
     "can be ignored.")
 
-
 @triton.jit
 def tanh(x):
     # Tanh is just a scaled sigmoid
@@ -225,6 +226,161 @@ def _decode_att_m_fwd(
     )
 
 
+@triton.jit
+def _fwd_grouped_kernel_stage1_kvint8(
+    Q,
+    K_Buffer,
+    V_Buffer,
+    KV_Scale,
+    sm_scale,
+    Req_to_tokens,
+    B_Seqlen,
+    Att_Out,
+    stride_req_to_tokens_b,
+    stride_qbs,
+    stride_qh,
+    stride_buf_kbs,
+    stride_buf_kh,
+    stride_buf_vbs,
+    stride_buf_vh,
+    stride_mid_ob,
+    stride_mid_oh,
+    stride_mid_os,
+    kv_group_num: tl.constexpr,
+    q_head_num: tl.constexpr,
+    BLOCK_DMODEL: tl.constexpr,
+    BLOCK_DPE: tl.constexpr,
+    BLOCK_DV: tl.constexpr,
+    BLOCK_N: tl.constexpr,
+    BLOCK_H: tl.constexpr,
+    NUM_KV_SPLITS: tl.constexpr,
+    PAGE_SIZE: tl.constexpr,
+    logit_cap: tl.constexpr,
+    Lk: tl.constexpr,
+    Lv: tl.constexpr,
+):
+    kv_scale = tl.load(KV_Scale + 0)
+
+    cur_batch = tl.program_id(0)
+    cur_head_id = tl.program_id(1)
+    cur_kv_head = cur_head_id // tl.cdiv(kv_group_num, BLOCK_H)
+    split_kv_id = tl.program_id(2)
+
+    if kv_group_num > BLOCK_H:
+        VALID_BLOCK_H: tl.constexpr = BLOCK_H
+    else:
+        VALID_BLOCK_H: tl.constexpr = kv_group_num
+    cur_head = cur_head_id * VALID_BLOCK_H + tl.arange(0, BLOCK_H)
+    mask_h = cur_head < (cur_head_id + 1) * VALID_BLOCK_H
+    mask_h = mask_h & (cur_head < q_head_num)
+
+    offs_d = tl.arange(0, BLOCK_DMODEL)
+    offs_dv = tl.arange(0, BLOCK_DV)
+    mask_d = offs_d < Lk
+    mask_dv = offs_dv < Lv
+    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)
+    cur_batch_req_idx = cur_batch
+
+    offs_q = cur_batch * stride_qbs + cur_head[:, None] * stride_qh + offs_d[
+        None, :]
+    q = tl.load(Q + offs_q,
+                mask=(mask_h[:, None]) & (mask_d[None, :]),
+                other=0.0)
+
+    if BLOCK_DPE > 0:
+        offs_dpe = BLOCK_DMODEL + tl.arange(0, BLOCK_DPE)
+        mask_dpe = offs_dpe < Lk
+        off_qpe = (cur_batch * stride_qbs + cur_head[:, None] * stride_qh +
+                   offs_dpe[None, :])
+        qpe = tl.load(Q + off_qpe,
+                      mask=(mask_h[:, None]) & (mask_dpe[None, :]),
+                      other=0.0)
+
+    kv_len_per_split = tl.cdiv(cur_batch_seq_len, NUM_KV_SPLITS)
+    split_kv_start = kv_len_per_split * split_kv_id
+    split_kv_end = tl.minimum(split_kv_start + kv_len_per_split,
+                              cur_batch_seq_len)
+
+    e_max = tl.zeros([BLOCK_H], dtype=tl.float32) - float("inf")
+    e_sum = tl.zeros([BLOCK_H], dtype=tl.float32)
+    acc = tl.zeros([BLOCK_H, BLOCK_DV], dtype=tl.float32)
+
+    if split_kv_end > split_kv_start:
+        for start_n in range(split_kv_start, split_kv_end, BLOCK_N):
+            offs_n = start_n + tl.arange(0, BLOCK_N)
+            kv_page_number = tl.load(
+                Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx +
+                offs_n // PAGE_SIZE,
+                mask=offs_n < split_kv_end,
+                other=0,
+            )
+            kv_loc = kv_page_number * PAGE_SIZE + offs_n % PAGE_SIZE
+            offs_buf_k = (kv_loc[None, :] * stride_buf_kbs +
+                          cur_kv_head * stride_buf_kh + offs_d[:, None])
+            k = tl.load(
+                K_Buffer + offs_buf_k,
+                mask=(offs_n[None, :] < split_kv_end) & (mask_d[:, None]),
+                other=0,
+            )          
+            k = k * kv_scale
+            qk = tl.dot(q, k.to(q.dtype))
+            if BLOCK_DPE > 0:
+                offs_buf_kpe = (kv_loc[None, :] * stride_buf_kbs +
+                                cur_kv_head * stride_buf_kh +
+                                offs_dpe[:, None])
+                kpe = tl.load(
+                    K_Buffer + offs_buf_kpe,
+                    mask=(offs_n[None, :] < split_kv_end) &
+                    (mask_dpe[:, None]),
+                    other=0,
+                )
+                kpe = kpe * kv_scale
+                qk += tl.dot(qpe, kpe.to(qpe.dtype))
+            qk *= sm_scale
+
+            if logit_cap > 0:
+                qk = logit_cap * tanh(qk / logit_cap)
+
+            qk = tl.where(mask_h[:, None] & (offs_n[None, :] < split_kv_end),
+                          qk, float("-inf"))
+
+            offs_buf_v = (kv_loc[:, None] * stride_buf_vbs +
+                          cur_kv_head * stride_buf_vh + offs_dv[None, :])
+            v = tl.load(
+                V_Buffer + offs_buf_v,
+                mask=(offs_n[:, None] < split_kv_end) & (mask_dv[None, :]),
+                other=0,
+            )
+            v = (v * kv_scale).to(q.dtype)
+
+            n_e_max = tl.maximum(tl.max(qk, 1), e_max)
+            re_scale = tl.exp(e_max - n_e_max)
+            p = tl.exp(qk - n_e_max[:, None])
+            acc *= re_scale[:, None]
+            acc += tl.dot(p.to(v.dtype), v)
+
+            e_sum = e_sum * re_scale + tl.sum(p, 1)
+            e_max = n_e_max
+
+        offs_mid_o = (cur_batch * stride_mid_ob +
+                      cur_head[:, None] * stride_mid_oh +
+                      split_kv_id * stride_mid_os + offs_dv[None, :])
+
+        tl.store(
+            Att_Out + offs_mid_o,
+            acc / e_sum[:, None],
+            mask=(mask_h[:, None]) & (mask_dv[None, :]),
+        )
+
+        offs_mid_o_1 = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +
+                        split_kv_id * stride_mid_os + Lv)
+
+        tl.store(
+            Att_Out + offs_mid_o_1,
+            e_max + tl.log(e_sum),
+            mask=mask_h,
+        )
+
 @triton.jit
 def _fwd_grouped_kernel_stage1(
     Q,
@@ -383,17 +539,19 @@ def _decode_grouped_att_m_fwd(
     Req_to_tokens,
     B_Seqlen,
     num_kv_splits,
+    num_stages,
     sm_scale,
+    kv_scale,
     page_size,
     logit_cap,
 ):
-    BLOCK = 32
+    BLOCK = 16
     Lk = k_buffer.shape[-1]
     Lv = v_buffer.shape[-1]
 
     # [TODO] work around shmem limit on MI3xx
-    if is_hip_ and Lk >= 576:
-        BLOCK = 16
+    # if is_hip_ and Lk >= 576:
+    #     BLOCK = 16
 
     if Lk == 576:
         BLOCK_DMODEL = 512
@@ -410,57 +568,100 @@ def _decode_grouped_att_m_fwd(
     kv_group_num = q.shape[1] // k_buffer.shape[-2]
 
     BLOCK_H = 16
+
     NUM_KV_SPLITS = num_kv_splits
+
     grid = (
         batch,
         triton.cdiv(head_num, min(BLOCK_H, kv_group_num)),
         NUM_KV_SPLITS,
     )
-
-    extra_kargs = {}
-    if is_hip_:
-        # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
-        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
-        extra_kargs = {
-            "waves_per_eu": 4,
-            "matrix_instr_nonkdim": 16,
-            "kpack": 2
-        }
-
-    _fwd_grouped_kernel_stage1[grid](
-        q,
-        k_buffer,
-        v_buffer,
-        sm_scale,
-        Req_to_tokens,
-        B_Seqlen,
-        att_out,
-        Req_to_tokens.stride(0),
-        q.stride(0),
-        q.stride(1),
-        k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-        k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-        v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-        v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
-        att_out.stride(0),
-        att_out.stride(1),
-        att_out.stride(2),
-        kv_group_num=kv_group_num,
-        q_head_num=head_num,
-        BLOCK_DMODEL=BLOCK_DMODEL,
-        BLOCK_DPE=BLOCK_DPE,
-        BLOCK_DV=BLOCK_DV,
-        BLOCK_N=BLOCK,
-        BLOCK_H=BLOCK_H,
-        NUM_KV_SPLITS=NUM_KV_SPLITS,
-        PAGE_SIZE=page_size,
-        logit_cap=logit_cap,
-        num_warps=4,
-        num_stages=2,
-        Lk=Lk,
-        Lv=Lv,
-        **extra_kargs,
-    )
+    if num_stages == 1:
+        extra_kargs = {"scenario":"mla"}
+    elif num_stages == 2:
+        extra_kargs = {"scenario" : "mla", "pipeline" : "cpasync"}
+    else:
+        KeyError("num_stages should be 1 or 2")
+    # num_stages=1
+    # extra_kargs = {"scenario":"mla"}
+    # if is_hip_:
+    #     # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
+    #     # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
+    #     extra_kargs = {
+    #         "waves_per_eu": 4,
+    #         "matrix_instr_nonkdim": 16,
+    #         "kpack": 2
+    #     }
+    if k_buffer.dtype == torch.int8:
+        _fwd_grouped_kernel_stage1_kvint8[grid](
+            q,
+            k_buffer,
+            v_buffer,
+            kv_scale,
+            sm_scale,
+            Req_to_tokens,
+            B_Seqlen,
+            att_out,
+            Req_to_tokens.stride(0),
+            q.stride(0),
+            q.stride(1),
+            k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            att_out.stride(0),
+            att_out.stride(1),
+            att_out.stride(2),
+            kv_group_num=kv_group_num,
+            q_head_num=head_num,
+            BLOCK_DMODEL=BLOCK_DMODEL,
+            BLOCK_DPE=BLOCK_DPE,
+            BLOCK_DV=BLOCK_DV,
+            BLOCK_N=BLOCK,
+            BLOCK_H=BLOCK_H,
+            NUM_KV_SPLITS=NUM_KV_SPLITS,
+            PAGE_SIZE=page_size,
+            logit_cap=logit_cap,
+            num_stages=num_stages,
+            Lk=Lk,
+            Lv=Lv,
+            **extra_kargs,
+        )
+    else:
+        _fwd_grouped_kernel_stage1[grid](
+            q,
+            k_buffer,
+            v_buffer,
+            sm_scale,
+            Req_to_tokens,
+            B_Seqlen,
+            att_out,
+            Req_to_tokens.stride(0),
+            q.stride(0),
+            q.stride(1),
+            k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)
+            att_out.stride(0),
+            att_out.stride(1),
+            att_out.stride(2),
+            kv_group_num=kv_group_num,
+            q_head_num=head_num,
+            BLOCK_DMODEL=BLOCK_DMODEL,
+            BLOCK_DPE=BLOCK_DPE,
+            BLOCK_DV=BLOCK_DV,
+            BLOCK_N=BLOCK,
+            BLOCK_H=BLOCK_H,
+            NUM_KV_SPLITS=NUM_KV_SPLITS,
+            PAGE_SIZE=page_size,
+            logit_cap=logit_cap,
+            num_warps=4,
+            num_stages=num_stages,
+            Lk=Lk,
+            Lv=Lv,
+            **extra_kargs,
+        )
 
 
 @triton.jit
@@ -535,14 +736,14 @@ def _decode_softmax_reducev_fwd(
     NUM_KV_SPLITS = num_kv_splits
 
     extra_kargs = {}
-    if is_hip_:
-        # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
-        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
-        extra_kargs = {
-            "waves_per_eu": 4,
-            "matrix_instr_nonkdim": 16,
-            "kpack": 2
-        }
+    # if is_hip_:
+    #     # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
+    #     # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
+    #     extra_kargs = {
+    #         "waves_per_eu": 4,
+    #         "matrix_instr_nonkdim": 16,
+    #         "kpack": 2
+    #     }
 
     grid = (batch, head_num)
     _fwd_kernel_stage2[grid](
@@ -601,7 +802,9 @@ def decode_attention_fwd_grouped(
     b_seq_len,
     attn_logits,
     num_kv_splits,
+    num_stages,
     sm_scale,
+    kv_scale,
     page_size,
     logit_cap=0.0,
 ):
@@ -613,7 +816,9 @@ def decode_attention_fwd_grouped(
         req_to_token,
         b_seq_len,
         num_kv_splits,
+        num_stages,
         sm_scale,
+        kv_scale,
         page_size,
         logit_cap,
     )
@@ -630,7 +835,9 @@ def decode_attention_fwd(
     b_seq_len,
     attn_logits,
     num_kv_splits,
+    num_stages,
     sm_scale,
+    kv_scale,
     page_size=1,
     logit_cap=0.0,
 ):
@@ -663,7 +870,9 @@ def decode_attention_fwd(
             b_seq_len,
             attn_logits,
             num_kv_splits,
+            num_stages,
             sm_scale,
+            kv_scale,
             page_size,
             logit_cap,
         )
diff --git a/vllm/config.py b/vllm/config.py
index 9ba497576..dd6276160 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import ast
@@ -595,6 +596,8 @@ class ModelConfig:
                 method = get_quantization_config(name)
                 quantization_override = method.override_quantization_method(
                     quant_cfg, self.quantization)
+                if quantization_override !=name:
+                    quantization_override =None
                 if quantization_override:
                     quant_method = quantization_override
                     self.quantization = quantization_override
@@ -985,37 +988,7 @@ class ModelConfig:
 
     @property
     def use_mla(self) -> bool:
-        if not self.is_deepseek_mla or envs.VLLM_MLA_DISABLE:
-            return False
-
-        if self.quantization is not None and self.quantization not in [\
-            "fp8", "compressed-tensors"]:
-            logger.warning(
-                "MLA is not supported with %s quantization. "
-                "Disabling MLA.", self.quantization)
-            return False
-
-        # If using a "compressed-tensors" checkpoint, check that all groups
-        # have fp8 for both weights and activations.
-        if self.quantization == "compressed-tensors":
-            quant_config = self._parse_quant_hf_config()
-            for group_name, cfg in quant_config.get("config_groups", {
-                    "": {}
-            }).items():
-                act_cfg = cfg.get("input_activations", {})
-                act_type = None if act_cfg is None else act_cfg.get("type", "")
-                w_cfg = cfg.get("weights", {})
-                w_type = None if w_cfg is None else w_cfg.get("type", "")
-                if act_type != "fp8" or w_type != "fp8":
-                    logger.warning(
-                        "compressed-tensors MLA support requires fp8 "
-                        "activations and weights in group '%s', but got "
-                        "activations type '%s' and weights type '%s'.\n "
-                        "Full config: %s", group_name, act_type, w_type,
-                        quant_config)
-                    return False
-
-        return True
+        return self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE
 
     @property
     def supported_runner_types(self) -> Set[RunnerType]:
@@ -1111,7 +1084,7 @@ class CacheConfig:
     def _verify_cache_dtype(self) -> None:
         if self.cache_dtype == "auto":
             pass
-        elif self.cache_dtype in ("fp8", "fp8_e4m3", "fp8_e5m2"):
+        elif self.cache_dtype in ("fp8", "fp8_e4m3", "fp8_e5m2", "int8"):
             logger.info(
                 "Using fp8 data type to store kv cache. It reduces the GPU "
                 "memory footprint and boosts the performance. "
@@ -2706,6 +2679,12 @@ class KVTransferConfig(BaseModel):
         return self.kv_connector is not None and \
             self.kv_role in ["kv_consumer", "kv_both"]
 
+    @property
+    def is_layerwise_kv_transfer(self) -> bool:
+        # so far, only LayerwisePyNcclConnector supports layerwise kv transfer
+        return self.kv_connector is not None and self.kv_connector in [
+            "LayerwisePyNcclConnector"
+        ]
 
 class CompilationLevel:
     # constants for the levels of the compilation process
@@ -3170,12 +3149,12 @@ class VllmConfig:
 
             if capability_tuple is not None:
                 capability = capability_tuple.to_int()
-                if capability < quant_config.get_min_capability():
-                    raise ValueError(
-                        f"The quantization method {model_config.quantization} "
-                        "is not supported for the current GPU. Minimum "
-                        f"capability: {quant_config.get_min_capability()}. "
-                        f"Current capability: {capability}.")
+                #if capability < quant_config.get_min_capability():
+                #    raise ValueError(
+                #        f"The quantization method {model_config.quantization} "
+                #        "is not supported for the current GPU. Minimum "
+                #        f"capability: {quant_config.get_min_capability()}. "
+                #        f"Current capability: {capability}.")
             supported_dtypes = quant_config.get_supported_act_dtypes()
             if model_config.dtype not in supported_dtypes:
                 raise ValueError(
diff --git a/vllm/distributed/device_communicators/pynccl_wrapper.py b/vllm/distributed/device_communicators/pynccl_wrapper.py
index 03c3b0be7..13681390b 100644
--- a/vllm/distributed/device_communicators/pynccl_wrapper.py
+++ b/vllm/distributed/device_communicators/pynccl_wrapper.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # This file is a pure Python wrapper for the NCCL library.
@@ -127,18 +128,18 @@ class Function:
 class NCCLLibrary:
     exported_functions = [
         # const char* ncclGetErrorString(ncclResult_t result)
-        Function("ncclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
+        Function("mcclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
         # ncclResult_t  ncclGetVersion(int *version);
-        Function("ncclGetVersion", ncclResult_t,
+        Function("mcclGetVersion", ncclResult_t,
                  [ctypes.POINTER(ctypes.c_int)]),
         # ncclResult_t ncclGetUniqueId(ncclUniqueId* uniqueId);
-        Function("ncclGetUniqueId", ncclResult_t,
+        Function("mcclGetUniqueId", ncclResult_t,
                  [ctypes.POINTER(ncclUniqueId)]),
         # ncclResult_t  ncclCommInitRank(
         #   ncclComm_t* comm, int nranks, ncclUniqueId commId, int rank);
         # note that ncclComm_t is a pointer type, so the first argument
         # is a pointer to a pointer
-        Function("ncclCommInitRank", ncclResult_t, [
+        Function("mcclCommInitRank", ncclResult_t, [
             ctypes.POINTER(ncclComm_t), ctypes.c_int, ncclUniqueId,
             ctypes.c_int
         ]),
@@ -148,7 +149,7 @@ class NCCLLibrary:
         #   cudaStream_t stream);
         # note that cudaStream_t is a pointer type, so the last argument
         # is a pointer
-        Function("ncclAllReduce", ncclResult_t, [
+        Function("mcclAllReduce", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ncclRedOp_t, ncclComm_t, cudaStream_t
         ]),
@@ -159,7 +160,7 @@ class NCCLLibrary:
         #   cudaStream_t stream);
         # note that cudaStream_t is a pointer type, so the last argument
         # is a pointer
-        Function("ncclAllGather", ncclResult_t, [
+        Function("mcclAllGather", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ncclComm_t, cudaStream_t
         ]),
@@ -170,7 +171,7 @@ class NCCLLibrary:
         #   cudaStream_t stream);
         # note that cudaStream_t is a pointer type, so the last argument
         # is a pointer
-        Function("ncclReduceScatter", ncclResult_t, [
+        Function("mcclReduceScatter", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ncclRedOp_t, ncclComm_t, cudaStream_t
         ]),
@@ -178,7 +179,7 @@ class NCCLLibrary:
         # ncclResult_t  ncclSend(
         #   const void* sendbuff, size_t count, ncclDataType_t datatype,
         #   int dest, ncclComm_t comm, cudaStream_t stream);
-        Function("ncclSend", ncclResult_t, [
+        Function("mcclSend", ncclResult_t, [
             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
             ncclComm_t, cudaStream_t
         ]),
@@ -186,7 +187,7 @@ class NCCLLibrary:
         # ncclResult_t  ncclRecv(
         #   void* recvbuff, size_t count, ncclDataType_t datatype,
         #   int src, ncclComm_t comm, cudaStream_t stream);
-        Function("ncclRecv", ncclResult_t, [
+        Function("mcclRecv", ncclResult_t, [
             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
             ncclComm_t, cudaStream_t
         ]),
@@ -195,7 +196,7 @@ class NCCLLibrary:
         #   const void* sendbuff, void* recvbuff, size_t count,
         #   ncclDataType_t datatype, int root, ncclComm_t comm,
         #   cudaStream_t stream);
-        Function("ncclBroadcast", ncclResult_t, [
+        Function("mcclBroadcast", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ctypes.c_int, ncclComm_t, cudaStream_t
         ]),
@@ -205,7 +206,7 @@ class NCCLLibrary:
         # because Python object destruction can happen in random order,
         # it is better not to call it at all.
         # ncclResult_t  ncclCommDestroy(ncclComm_t comm);
-        Function("ncclCommDestroy", ncclResult_t, [ncclComm_t]),
+        Function("mcclCommDestroy", ncclResult_t, [ncclComm_t]),
     ]
 
     # class attribute to store the mapping from the path to the library
@@ -248,7 +249,7 @@ class NCCLLibrary:
         self._funcs = NCCLLibrary.path_to_dict_mapping[so_file]
 
     def ncclGetErrorString(self, result: ncclResult_t) -> str:
-        return self._funcs["ncclGetErrorString"](result).decode("utf-8")
+        return self._funcs["mcclGetErrorString"](result).decode("utf-8")
 
     def NCCL_CHECK(self, result: ncclResult_t) -> None:
         if result != 0:
@@ -257,7 +258,7 @@ class NCCLLibrary:
 
     def ncclGetVersion(self) -> str:
         version = ctypes.c_int()
-        self.NCCL_CHECK(self._funcs["ncclGetVersion"](ctypes.byref(version)))
+        self.NCCL_CHECK(self._funcs["mcclGetVersion"](ctypes.byref(version)))
         version_str = str(version.value)
         # something like 21903 --> "2.19.3"
         major = version_str[0].lstrip("0")
@@ -267,14 +268,14 @@ class NCCLLibrary:
 
     def ncclGetUniqueId(self) -> ncclUniqueId:
         unique_id = ncclUniqueId()
-        self.NCCL_CHECK(self._funcs["ncclGetUniqueId"](
+        self.NCCL_CHECK(self._funcs["mcclGetUniqueId"](
             ctypes.byref(unique_id)))
         return unique_id
 
     def ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId,
                          rank: int) -> ncclComm_t:
         comm = ncclComm_t()
-        self.NCCL_CHECK(self._funcs["ncclCommInitRank"](ctypes.byref(comm),
+        self.NCCL_CHECK(self._funcs["mcclCommInitRank"](ctypes.byref(comm),
                                                         world_size, unique_id,
                                                         rank))
         return comm
@@ -287,7 +288,7 @@ class NCCLLibrary:
         # both are aliases of `ctypes.c_int`
         # when we pass int to a function, it will be converted to `ctypes.c_int`
         # by ctypes automatically
-        self.NCCL_CHECK(self._funcs["ncclAllReduce"](sendbuff, recvbuff, count,
+        self.NCCL_CHECK(self._funcs["mcclAllReduce"](sendbuff, recvbuff, count,
                                                      datatype, op, comm,
                                                      stream))
 
@@ -299,7 +300,7 @@ class NCCLLibrary:
         # both are aliases of `ctypes.c_int`
         # when we pass int to a function, it will be converted to `ctypes.c_int`
         # by ctypes automatically
-        self.NCCL_CHECK(self._funcs["ncclReduceScatter"](sendbuff, recvbuff,
+        self.NCCL_CHECK(self._funcs["mcclReduceScatter"](sendbuff, recvbuff,
                                                          count, datatype, op,
                                                          comm, stream))
 
@@ -310,28 +311,28 @@ class NCCLLibrary:
         # which is an aliases of `ctypes.c_int`
         # when we pass int to a function, it will be converted to `ctypes.c_int`
         # by ctypes automatically
-        self.NCCL_CHECK(self._funcs["ncclAllGather"](sendbuff, recvbuff, count,
+        self.NCCL_CHECK(self._funcs["mcclAllGather"](sendbuff, recvbuff, count,
                                                      datatype, comm, stream))
 
     def ncclSend(self, sendbuff: buffer_type, count: int, datatype: int,
                  dest: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclSend"](sendbuff, count, datatype,
+        self.NCCL_CHECK(self._funcs["mcclSend"](sendbuff, count, datatype,
                                                 dest, comm, stream))
 
     def ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int,
                  src: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclRecv"](recvbuff, count, datatype, src,
+        self.NCCL_CHECK(self._funcs["mcclRecv"](recvbuff, count, datatype, src,
                                                 comm, stream))
 
     def ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type,
                       count: int, datatype: int, root: int, comm: ncclComm_t,
                       stream: cudaStream_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclBroadcast"](sendbuff, recvbuff, count,
+        self.NCCL_CHECK(self._funcs["mcclBroadcast"](sendbuff, recvbuff, count,
                                                      datatype, root, comm,
                                                      stream))
 
     def ncclCommDestroy(self, comm: ncclComm_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclCommDestroy"](comm))
+        self.NCCL_CHECK(self._funcs["mcclCommDestroy"](comm))
 
 
 __all__ = [
diff --git a/vllm/distributed/kv_transfer/kv_connector/base.py b/vllm/distributed/kv_transfer/kv_connector/base.py
index 57c764b48..b10d934d1 100644
--- a/vllm/distributed/kv_transfer/kv_connector/base.py
+++ b/vllm/distributed/kv_transfer/kv_connector/base.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """
 KVConnectorBase Class for Distributed KV Cache & Hidden State communication
@@ -7,6 +8,7 @@ The class provides two primary abstract methods:
 2. recv_kv_caches_and_hidden_states(): Recv KV caches and hidden states
 """
 
+import inspect
 from abc import ABC, abstractmethod
 from typing import TYPE_CHECKING, List, Tuple, Union
 
@@ -15,6 +17,7 @@ import torch
 from vllm.sequence import IntermediateTensors
 
 if TYPE_CHECKING:
+    from vllm.attention import AttentionMetadata
     from vllm.config import VllmConfig
     from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
 
@@ -28,6 +31,55 @@ class KVConnectorBase(ABC):
     2. recv_kv_caches_and_hidden_states(): Recv KV caches and hidden states
     """
 
+    def __init_subclass__(cls, **kwargs):
+        super().__init_subclass__(**kwargs)
+
+        mode_methods_dict = {
+            "bulk_transfer": [
+                "send_kv_caches_and_hidden_states",
+                "recv_kv_caches_and_hidden_states",
+            ],
+            "layerwise_transfer": [
+                "send_one_layer_kv_cache",
+                "send_hidden_states",
+                "recv_kv_caches_and_hidden_states",
+            ],
+        }
+
+        supported_groups = [
+            group_name for group_name, methods in mode_methods_dict.items()
+            if all(method in cls.__dict__ for method in methods)
+        ]
+
+        if not supported_groups:
+            error_msg = (
+                f"{cls.__name__} must implement at least one of the groups:\n"
+                + "\n".join(
+                    f"- {group_name}: {', '.join(methods)}"
+                    for group_name, methods in mode_methods_dict.items()))
+            raise TypeError(error_msg)
+
+        # Validate method signatures for all methods in supported groups
+        signature_errors = []
+        for group_name in supported_groups:
+            for method in mode_methods_dict[group_name]:
+                # Get base class method and subclass method
+                base_method = getattr(__class__, method)
+                subclass_method = getattr(cls, method)
+
+                # Compare signatures
+                base_sig = inspect.signature(base_method)
+                subclass_sig = inspect.signature(subclass_method)
+                if base_sig != subclass_sig:
+                    signature_errors.append(
+                        f"Signature mismatch in group '{group_name}': "
+                        f"Method '{method}' expects {base_sig}, "
+                        f"got {subclass_sig}")
+
+        # Raise all signature errors at once
+        if signature_errors:
+            raise TypeError("\n".join(signature_errors))
+
     @abstractmethod
     def __init__(
         self,
@@ -49,7 +101,6 @@ class KVConnectorBase(ABC):
         """
         raise NotImplementedError
 
-    @abstractmethod
     def send_kv_caches_and_hidden_states(
         self,
         model_executable: torch.nn.Module,
@@ -83,11 +134,12 @@ class KVConnectorBase(ABC):
 
         raise NotImplementedError
 
-    @abstractmethod
     def recv_kv_caches_and_hidden_states(
-        self, model_executable: torch.nn.Module,
+        self,
+        model_executable: torch.nn.Module,
         model_input: "ModelInputForGPUWithSamplingMetadata",
-        kv_caches: List[torch.Tensor]
+        kv_caches: List[torch.Tensor],
+        **kwargs,
     ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
                "ModelInputForGPUWithSamplingMetadata"]:
         """
@@ -111,7 +163,7 @@ class KVConnectorBase(ABC):
             IntermediateTensors): 
                 Concatenated hidden states if all required data is retrieved, 
                 otherwise `None`.
-            - bypass_model_exec (bool): 
+            - bypass_model_exec (List[bool]): 
                 Indicates whether the model execution can be skipped (True) or 
                 needs to be redone (False).
             - model_input (ModelInputForGPUWithSamplingMetadata): 
@@ -121,3 +173,49 @@ class KVConnectorBase(ABC):
         """
 
         raise NotImplementedError
+
+    def send_one_layer_kv_cache(self, layer_id: int,
+                                input_token_hash: List[str],
+                                kv_cache: torch.Tensor,
+                                attn_metadata: "AttentionMetadata",
+                                block_size: int) -> None:
+        """
+        Sends the KV cache of a single layer to the connector.
+        This method transmits the KV cache for a specific transformer layer,
+        along with metadata, allowing the connector to store or utilize it
+        for future requests. The transmission is layer-specific.
+        Args:
+            layer_id (int): 
+                The id of the transformer layer being sent.
+            input_token_hash (List[str]): 
+                Hashes of the input tokens associated with this KV cache.
+            kv_cache (torch.Tensor): 
+                The KV cache tensor for the specified layer.
+            attn_metadata (AttentionMetadata): 
+                Attention metadata for the current batch.
+            block_size (int): 
+                The number of tokens in one page of KV cache.
+        Returns:
+            None: This method does not return a value.
+        """
+        raise NotImplementedError
+
+    def send_hidden_states(self, input_token_hash: List[str],
+                           hidden_states: torch.Tensor,
+                           attn_metadata: "AttentionMetadata") -> None:
+        """
+        Sends hidden states to the connector.
+        Transmits computed hidden states along with attention metadata,
+        enabling the connector to bypass the full model execution 
+        using these cached states.
+        Args:
+            input_token_hash (List[str]): 
+                Hash values of the input tokens.
+            hidden_states (torch.Tensor): 
+                The hidden states tensor computed by the model.
+            attn_metadata (AttentionMetadata): 
+                Attention metadata associated with these hidden states.
+        Returns:
+            None: This method does not return a value.
+        """
+        raise NotImplementedError
\ No newline at end of file
diff --git a/vllm/distributed/kv_transfer/kv_connector/factory.py b/vllm/distributed/kv_transfer/kv_connector/factory.py
index fe4805334..5aeac29d8 100644
--- a/vllm/distributed/kv_transfer/kv_connector/factory.py
+++ b/vllm/distributed/kv_transfer/kv_connector/factory.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import importlib
@@ -48,3 +49,8 @@ KVConnectorFactory.register_connector(
     "MooncakeConnector",
     "vllm.distributed.kv_transfer.kv_connector.simple_connector",
     "SimpleConnector")
+
+KVConnectorFactory.register_connector(
+    "LayerwisePyNcclConnector",
+    "vllm.distributed.kv_transfer.kv_connector.layerwise_connector",
+    "LayerwiseConnector")
\ No newline at end of file
diff --git a/vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py b/vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
new file mode 100755
index 000000000..4250db4b7
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
@@ -0,0 +1,269 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# SPDX-License-Identifier: Apache-2.0
+"""
+Simple KV Cache Connector for Distributed Machine Learning Inference
+
+LayerwiseConnector transfers KV caches between prefill vLLM worker (KV cache 
+producer) and decode vLLM worker (KV cache consumer) using PyNcclPipe, 
+layer bylayer.
+
+The logic can be extended to support other pipe and lookup buffer.
+"""
+from typing import TYPE_CHECKING, List, Optional, Tuple, Union
+
+import torch
+
+from vllm.config import VllmConfig
+from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
+from vllm.distributed.kv_transfer.kv_lookup_buffer.simple_dict_buffer import (
+    SimpleDictBuffer)
+from vllm.distributed.kv_transfer.kv_transfer_utils import (
+    get_tensor_stable_hash)
+from vllm.logger import init_logger
+from vllm.sequence import IntermediateTensors
+
+if TYPE_CHECKING:
+    from vllm.attention import AttentionMetadata
+    from vllm.config import VllmConfig
+    from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
+
+logger = init_logger(__name__)
+
+
+class LayerwiseConnector(KVConnectorBase):
+
+    def __init__(
+        self,
+        rank: int,
+        local_rank: int,
+        config: VllmConfig,
+    ):
+
+        self.config = config.kv_transfer_config
+
+        if self.config.kv_connector != "LayerwisePyNcclConnector":
+            raise ValueError(
+                "LayerwiseConnector only supports LayerwisePyNcclConnector")
+
+        from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import PyNcclPipe
+        logger.info("Initializing PyNcclConfig under kv_transfer_config %s",
+                    self.config)
+
+        self.lookup_buffer_size = self.config.kv_buffer_size
+
+        self.producer_buffer: Optional[SimpleDictBuffer] = None
+        self.consumer_buffer: Optional[SimpleDictBuffer] = None
+
+        self.producer_data_pipe: PyNcclPipe
+        self.consumer_data_pipe: PyNcclPipe
+        self.producer_signal_pipe: PyNcclPipe
+        self.consumer_signal_pipe: PyNcclPipe
+
+        # 2 pipes for every rank in the world
+        port_offset_base = 2 * rank
+
+        # In disaggregated prefill, the prefill vLLM only uses send pipe
+        # and the decode vLLM only uses recv pipe
+        if self.config.is_kv_producer:
+
+            self.producer_data_pipe = PyNcclPipe(
+                local_rank=local_rank,
+                config=self.config,
+                port_offset=port_offset_base,
+            )
+            self.producer_signal_pipe = PyNcclPipe(
+                local_rank=local_rank,
+                config=self.config,
+                port_offset=port_offset_base + 1,
+                device="cpu",
+            )
+
+            self.producer_buffer = SimpleDictBuffer(self.producer_signal_pipe,
+                                                    self.producer_data_pipe,
+                                                    self.config.kv_buffer_size)
+
+        else:
+
+            # the current vLLM instance is KV consumer, so it needs to connect
+            # its recv pipe to the send pipe of KV producder
+
+            self.consumer_data_pipe = PyNcclPipe(
+                local_rank=local_rank,
+                config=self.config,
+                port_offset=port_offset_base,
+            )
+            self.consumer_signal_pipe = PyNcclPipe(
+                local_rank=local_rank,
+                config=self.config,
+                port_offset=port_offset_base + 1,
+                device="cpu",
+            )
+
+            self.consumer_buffer = SimpleDictBuffer(
+                self.consumer_signal_pipe,
+                self.consumer_data_pipe,
+                self.config.kv_buffer_size,
+            )
+
+    def select(self, key: List[str]) -> Optional[List[torch.Tensor]]:
+
+        assert self.consumer_buffer is not None, "Please initialize the "\
+            "consumer buffer before calling select."
+        return self.consumer_buffer.drop_select(key)
+
+    def insert(self, key: str, value: torch.Tensor) -> None:
+
+        assert self.producer_buffer is not None, "Please initialize the "\
+            "producer buffer before calling insert."
+
+        self.producer_buffer.insert(key, value)
+
+    def _get_kv_cache_key(self, input_tokens_hash: str, layer: int) -> str:
+        return f"{input_tokens_hash}_layer_{layer}"
+
+    def _get_hs_cache_key(self, input_tokens_hash: str) -> str:
+        return f"{input_tokens_hash}_hs"
+
+    def send_one_layer_kv_cache(self, layer_id: int,
+                                input_token_hash: List[str],
+                                kv_cache: torch.Tensor,
+                                attn_metadata: "AttentionMetadata",
+                                block_size: int) -> None:
+        seq_lens = attn_metadata.seq_lens
+        slot_mapping_flat = attn_metadata.slot_mapping.flatten()
+
+        assert len(input_token_hash) == len(seq_lens)
+
+        for idx, slen in enumerate(seq_lens):
+            kv_cache_key = self._get_kv_cache_key(input_token_hash[idx],
+                                                  layer_id)
+
+            start_pos = sum(seq_lens[:idx])
+            end_pos = start_pos + slen
+            current_slot_mapping = slot_mapping_flat[start_pos:end_pos]
+
+            page_index = current_slot_mapping[::block_size].div(
+                block_size, rounding_mode='floor').long()
+
+            paged_kv_cache = kv_cache[:, page_index, ...]
+
+            self.insert(kv_cache_key, paged_kv_cache)
+
+    def send_hidden_states(self, input_token_hash: List[str],
+                           hidden_states: torch.Tensor,
+                           attn_metadata: "AttentionMetadata") -> None:
+        seq_lens = attn_metadata.seq_lens
+        assert len(input_token_hash) == len(seq_lens)
+
+        for idx, slen in enumerate(seq_lens):
+            hs_cache_key = self._get_hs_cache_key(input_token_hash[idx])
+
+            start_pos = sum(seq_lens[:idx])
+            end_pos = start_pos + slen
+
+            hs_cache = hidden_states[start_pos:end_pos]
+
+            self.insert(hs_cache_key, hs_cache)
+
+    def recv_kv_caches_and_hidden_states(
+        self,
+        model_executable: torch.nn.Module,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        kv_caches: List[torch.Tensor],
+        **kwargs,
+    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
+               "ModelInputForGPUWithSamplingMetadata"]:
+
+        assert 'block_size' in kwargs, "block_size is required in kwargs"
+        block_size = kwargs['block_size']
+
+        # When bypass_model_exec is set to False, it means that at least for one
+        # request its corresponding KV cache or hidden state is missing.
+        # In this case we need to do prefilling to recompute missing KV cache
+        # and hidden states.
+        bypass_model_exec = True
+
+        input_tokens_tensor = model_input.input_tokens
+        seq_lens = model_input.attn_metadata.seq_lens
+        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
+
+        hidden_or_intermediate_states_for_one_req = []
+
+        try:
+
+            for idx, slen in enumerate(seq_lens):
+
+                start_pos = sum(seq_lens[:idx])
+                end_pos = start_pos + slen
+                current_tokens = input_tokens_tensor[start_pos:end_pos]
+                current_tokens_hash = get_tensor_stable_hash(current_tokens)
+                current_slot_mapping = slot_mapping[start_pos:end_pos]
+                num_tokens = slen
+
+                page_index = current_slot_mapping[::block_size].div(
+                    block_size, rounding_mode='floor').long()
+
+                keys = [
+                    self._get_kv_cache_key(current_tokens_hash, i)
+                    for i in range(model_executable.model.start_layer,
+                                   model_executable.model.end_layer)
+                ]
+
+                recv_kv_cache = self.select(keys)
+
+                assert model_executable.model.end_layer - \
+                    model_executable.model.start_layer == len(recv_kv_cache)
+
+                for i in range(model_executable.model.start_layer,
+                               model_executable.model.end_layer):
+
+                    kv_cache = kv_caches[i -
+                                         model_executable.model.start_layer]
+
+                    kv_cache[:, page_index, ...] = recv_kv_cache[i]
+
+                hs_cache_key = self._get_hs_cache_key(current_tokens_hash)
+                hidden_states = self.select([hs_cache_key])
+
+                hidden_or_intermediate_states_for_one_req.append(
+                    hidden_states[0])
+
+                assert num_tokens == hidden_states[0].shape[0]
+
+        except Exception as e:
+            import traceback
+            traceback.print_stack()
+            logger.error(
+                "[rank%d]: Failed to receive all KVs and hidden states. "
+                "Error: %s", torch.distributed.get_rank(), e)
+            bypass_model_exec = False
+
+        if not bypass_model_exec:
+            # Some of the KV cache is not retrieved
+            # Here we will fall back to normal model forwarding
+            # But optionally you can adjust model_input so that you only do
+            # prefilling on those tokens that are missing KV caches.
+            logger.debug(
+                "[rank%d]: Failed to receive all KVs and hidden "
+                "states, redo model forwarding.", torch.distributed.get_rank())
+            hidden_or_intermediate_states = None
+
+        else:
+            logger.debug(
+                "[rank%d]: Successfully received all KVs and hidden "
+                "states, skip model forwarding.", torch.distributed.get_rank())
+            hidden_or_intermediate_states = torch.cat(
+                hidden_or_intermediate_states_for_one_req, dim=0)
+
+        return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+    def close(self):
+        self.producer_data_pipe.close()
+        self.consumer_data_pipe.close()
+        if self.config.kv_connector == "PyNcclConnector":
+            self.producer_signal_pipe.close()
+            self.consumer_signal_pipe.close()
+        elif self.config.kv_connector == "MooncakeConnector":
+            # MooncakePipe reuses data_pipe for signal_pipe, so we only have to
+            # close the data_pipe.
+            pass
\ No newline at end of file
diff --git a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
index 2033e9762..8b5c83c17 100644
--- a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
+++ b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """
 Simple KV Cache Connector for Distributed Machine Learning Inference
@@ -8,11 +9,14 @@ MooncakePipe.
 
 But the logic can be extended to support other pipe and lookup buffer.
 """
+from copy import deepcopy
 from typing import TYPE_CHECKING, List, Optional, Tuple, Union
+from torch.nn.utils.rnn import pad_sequence
 
 import torch
 
 from vllm import _custom_ops as ops
+from vllm.attention.backends.flash_attn import FlashAttentionMetadata
 from vllm.config import VllmConfig
 from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
 from vllm.distributed.kv_transfer.kv_lookup_buffer.simple_buffer import (
@@ -37,6 +41,8 @@ class SimpleConnector(KVConnectorBase):
 
         self.config = config.kv_transfer_config
         self.tp_size = config.parallel_config.tensor_parallel_size
+        # The following config is needed to rebuild the model input
+        self.cache_config = config.cache_config
 
         if self.config.kv_connector == "PyNcclConnector":
             from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import (
@@ -148,12 +154,10 @@ class SimpleConnector(KVConnectorBase):
         self.producer_buffer.insert(input_tokens, roi, key, value, hidden)
 
     def send_kv_caches_and_hidden_states(
-        self,
-        model_executable: torch.nn.Module,
+        self, model_executable: torch.nn.Module,
         model_input: "ModelInputForGPUWithSamplingMetadata",
         kv_caches: List[torch.Tensor],
-        hidden_or_intermediate_states: Union[torch.Tensor,
-                                             IntermediateTensors],
+        hidden_or_intermediate_states: Union[torch.Tensor, IntermediateTensors]
     ) -> None:
 
         input_tokens_tensor = model_input.input_tokens
@@ -200,18 +204,14 @@ class SimpleConnector(KVConnectorBase):
         logger.debug("[rank%d]: KV send DONE.", torch.distributed.get_rank())
 
     def recv_kv_caches_and_hidden_states(
-        self, model_executable: torch.nn.Module,
+        self,
+        model_executable: torch.nn.Module,
         model_input: "ModelInputForGPUWithSamplingMetadata",
-        kv_caches: List[torch.Tensor]
+        kv_caches: List[torch.Tensor],
+        **kwargs,
     ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
                "ModelInputForGPUWithSamplingMetadata"]:
 
-        # When bypass_model_exec is set to False, it means that at least for one
-        # request its corresponding KV cache or hidden state is missing.
-        # In this case we need to do prefilling to recompute missing KV cache
-        # and hidden states.
-        bypass_model_exec = True
-
         input_tokens_tensor = model_input.input_tokens
         seq_lens = model_input.attn_metadata.seq_lens
         slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
@@ -222,6 +222,12 @@ class SimpleConnector(KVConnectorBase):
         num_computed_tokens_list = []
         start_pos_list = []
 
+        # When bypass_model_exec[i] is set to False, it means that for
+        # request[i] its corresponding KV cache or hidden state is missing.
+        # In this case we need to do prefilling to recompute missing KV cache
+        # and hidden states of request[i].
+        bypass_model_exec = [True] * len(seq_lens)
+
         # enumerate different requests
         # FIXME(Kuntai): This impl assumes that all requests are prefill.
         for idx, slen in enumerate(seq_lens):
@@ -239,7 +245,7 @@ class SimpleConnector(KVConnectorBase):
                               torch.ones_like(current_tokens, dtype=bool))
             if ret[0] is None:
                 # didn't find any match.
-                bypass_model_exec = False
+                bypass_model_exec[idx] = False
                 num_computed_tokens_list.append(0)
                 continue
 
@@ -255,11 +261,17 @@ class SimpleConnector(KVConnectorBase):
             # If not, need to redo the forwarding to compute missing states
             if not all([(num_computed_tokens == num_tokens), hidden is not None
                         ]):
-                bypass_model_exec = False
+                bypass_model_exec[idx] = False
+                continue
 
             # update the end position based on how many tokens are cached.
             end_pos = start_pos + num_computed_tokens
 
+            # Avoid error when prefix is exactly the same as the retrieved
+            if num_computed_tokens == num_tokens:
+                num_computed_tokens -= 1
+            num_computed_tokens_list.append(num_computed_tokens)
+
             # put received KV caches into paged memory
             for i in range(model_executable.model.start_layer,
                            model_executable.model.end_layer):
@@ -283,23 +295,35 @@ class SimpleConnector(KVConnectorBase):
 
             hidden_or_intermediate_states_for_one_req.append(hidden)
 
-        if not bypass_model_exec:
-            # Some of the KV cache is not retrieved
-            # Here we will fall back to normal model forwarding
-            # But optionally you can adjust model_input so that you only do
-            # prefilling on those tokens that are missing KV caches.
-            logger.debug(
-                "[rank%d]: Failed to receive all KVs and hidden "
-                "states, redo model forwarding.", torch.distributed.get_rank())
-            hidden_or_intermediate_states = None
-
-        else:
+        all_bypass_flag = True
+        for idx, bypass_flag in enumerate(bypass_model_exec):
+            if not bypass_flag:
+                # Some of the KV cache of this request is not retrieved
+                # Here we will fall back to normal model forwarding
+                logger.error(
+                    "[rank%d]: Failed to receive request %d's"
+                    " KVs and hidden states, "
+                    "redo model forwarding.", torch.distributed.get_rank(),
+                    idx)
+
+                hidden_or_intermediate_states = torch.cat(
+                    hidden_or_intermediate_states_for_one_req, dim=0)
+                all_bypass_flag = False
+        if all_bypass_flag:
             logger.debug(
                 "[rank%d]: Successfully received all KVs and hidden "
                 "states, skip model forwarding.", torch.distributed.get_rank())
             hidden_or_intermediate_states = torch.cat(
                 hidden_or_intermediate_states_for_one_req, dim=0)
 
+        if not all(bypass_model_exec):
+            rebuilt_model_input = self.build_partial_prefill_input(
+                model_input, input_tokens_list, num_computed_tokens_list,
+                start_pos_list, slot_mapping, kv_caches[0][0].device)
+            logger.error("Rebuilt the input!")
+            return (hidden_or_intermediate_states, bypass_model_exec,
+                    rebuilt_model_input)
+
         return hidden_or_intermediate_states, bypass_model_exec, model_input
 
     def close(self):
@@ -312,3 +336,115 @@ class SimpleConnector(KVConnectorBase):
             # MooncakePipe reuses data_pipe for signal_pipe, so we only have to
             # close the data_pipe.
             pass
+
+    def build_partial_prefill_input(
+            self, model_input: "ModelInputForGPUWithSamplingMetadata",
+            full_tokens_list: List[torch.Tensor],
+            num_computed_tokens_list: List[int], start_pos_list: List[int],
+            slot_mapping_flat: torch.Tensor,
+            device: torch.device) -> "ModelInputForGPUWithSamplingMetadata":
+        """Helper function to rebuild the model input for the current request.
+        """
+        assert model_input.attn_metadata is not None
+        assert isinstance(model_input.attn_metadata, FlashAttentionMetadata), \
+            "Only FlashAttention backend is supported for now."
+        assert model_input.attn_metadata.context_lens_tensor is not None
+        assert model_input.attn_metadata.block_tables is not None
+        assert model_input.attn_metadata.query_start_loc is not None
+        assert model_input.input_positions is not None
+
+        rebuilt_input_tokens = []
+        rebuilt_input_positions = []
+        rebuilt_num_prefills = 0
+        rebuilt_num_prefill_tokens = 0
+        rebuilt_slot_mapping = []
+        rebuilt_max_query_len = 0
+
+        rebuilt_block_tables = []
+
+        rebuilt_query_start_loc = [0]
+        rebuilt_context_lens_tensor = []
+
+        last_query_start_loc = 0
+
+        # recounting query and context lengths
+        for idx in range(len(full_tokens_list)):
+            token_tensor = full_tokens_list[idx]
+            num_token = len(token_tensor)
+            num_computed_token = num_computed_tokens_list[idx]
+            start_pos = start_pos_list[idx]
+            q_len = num_token - num_computed_token
+
+            rebuilt_input_tokens.append(token_tensor[num_computed_token:])
+
+            assert q_len > 0
+            start_input_pos_idx = start_pos + num_computed_token
+            end_input_pos_idx = start_input_pos_idx + q_len
+            rebuilt_input_positions.append(
+                model_input.
+                input_positions[start_input_pos_idx:end_input_pos_idx])
+
+            # Attn metadata-related
+            rebuilt_num_prefills += 1
+            rebuilt_num_prefill_tokens += q_len
+            start_slot_idx = start_pos + num_computed_token
+            end_slot_idx = start_slot_idx + q_len
+            new_slot_mapping = slot_mapping_flat[start_slot_idx:end_slot_idx]
+            rebuilt_slot_mapping.append(new_slot_mapping)
+            rebuilt_max_query_len = max(q_len, rebuilt_max_query_len)
+            last_query_start_loc += q_len
+            rebuilt_query_start_loc.append(last_query_start_loc)
+            rebuilt_context_lens_tensor.append(num_computed_token)
+
+            # recover `block_table`
+            if len(model_input.attn_metadata.block_tables[idx]) > 0:
+                rebuilt_block_tables.append(
+                    model_input.attn_metadata.block_tables[idx])
+            else:
+                slot_mapping_req = slot_mapping_flat[start_pos:end_slot_idx]
+                vllm_block_size = self.cache_config.block_size
+                rebuilt_block_table = slot_mapping_req[::16].to(torch.int32) \
+                    // vllm_block_size
+                rebuilt_block_tables.append(rebuilt_block_table)
+
+        # rebuilt attn_metadata
+        rebuilt_attn_metadata = deepcopy(model_input.attn_metadata)
+        rebuilt_attn_metadata.num_prefills = rebuilt_num_prefills
+        rebuilt_attn_metadata.num_prefill_tokens = rebuilt_num_prefill_tokens
+        rebuilt_attn_metadata.slot_mapping = torch.cat(
+            rebuilt_slot_mapping).to(device)
+        rebuilt_attn_metadata.max_query_len = rebuilt_max_query_len
+        rebuilt_attn_metadata.block_tables = pad_sequence(
+            rebuilt_block_tables, batch_first=True).to(device)
+        rebuilt_attn_metadata.query_start_loc = torch.tensor(
+            rebuilt_query_start_loc,
+            dtype=model_input.attn_metadata.query_start_loc.dtype).to(device)
+        rebuilt_attn_metadata.context_lens_tensor = torch.tensor(
+            rebuilt_context_lens_tensor,
+            dtype=model_input.attn_metadata.context_lens_tensor.dtype,
+        ).to(device)
+        rebuilt_attn_metadata._cached_prefill_metadata = None
+
+        # import here to avoid circular import.
+        from vllm.worker.model_runner import (
+            ModelInputForGPUWithSamplingMetadata)
+        rebuilt_model_input = ModelInputForGPUWithSamplingMetadata(
+            input_tokens=torch.cat(rebuilt_input_tokens).to(device),
+            input_positions=torch.cat(rebuilt_input_positions).to(device),
+            seq_lens=model_input.seq_lens,
+            query_lens=model_input.query_lens,
+            lora_mapping=model_input.lora_mapping,
+            lora_requests=model_input.lora_requests,
+            attn_metadata=rebuilt_attn_metadata,
+            prompt_adapter_mapping=model_input.prompt_adapter_mapping,
+            prompt_adapter_requests=model_input.prompt_adapter_requests,
+            multi_modal_kwargs=model_input.multi_modal_kwargs,
+            request_ids_to_seq_ids=model_input.request_ids_to_seq_ids,
+            finished_requests_ids=model_input.finished_requests_ids,
+            virtual_engine=model_input.virtual_engine,
+            sampling_metadata=model_input.sampling_metadata,
+            is_prompt=model_input.is_prompt,
+            async_callback=model_input.async_callback,
+        )
+
+        return rebuilt_model_input
\ No newline at end of file
diff --git a/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py b/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
new file mode 100755
index 000000000..8d732678d
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
@@ -0,0 +1,151 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# SPDX-License-Identifier: Apache-2.0
+"""
+    Implements a distributed key-value (KV) cache transfer mechanism.
+    Key Features:
+    - Distributed KV cache transmission using PyNccl pipes.
+    - Non-blocking `insert`, blocking `drop_select`.
+    - Use CPU signal pipe to avoid racing condition
+    - Handles buffer size constraints and provide backpressure mechanism to 
+      stop the prefill instance when the decode instance is slow.
+"""
+import threading
+from typing import Dict, List, Optional
+
+import torch
+
+from vllm.distributed.kv_transfer.kv_lookup_buffer.base import (
+    KVLookupBufferBase)
+from vllm.distributed.kv_transfer.kv_pipe.base import KVPipeBase
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
+
+def _string_to_byte_tensor(s: str) -> torch.Tensor:
+    byte_data = s.encode('utf-8')
+    byte_list = list(byte_data)
+    byte_tensor = torch.tensor(byte_list, dtype=torch.uint8, device='cpu')
+    return byte_tensor
+
+
+def _byte_tensor_to_string(t: torch.Tensor) -> str:
+    byte_list = t.tolist()
+    byte_data = bytes(byte_list)
+    return byte_data.decode('utf-8')
+
+
+class SimpleDictBuffer(KVLookupBufferBase):
+
+    def __init__(self, signal_pipe: KVPipeBase, data_pipe: KVPipeBase,
+                 buffer_size_thresh: float):
+        """
+        signal_pipe: on CPU 
+        
+        NOTE: on-device recv will block all threads in the process, making the 
+        KV cache producer unable to listen to new request while transmitting 
+        KV cache. Luckily CPU recv only blocks the current thread so we use 
+        CPU recv to listen to new request.
+        
+        data_pipe: on device (e.g. GPU)
+        """
+
+        self.buffer: Dict[str, List[torch.Tensor]] = {}
+
+        self.buffer_size = 0
+        self.buffer_size_threshold = buffer_size_thresh
+        self.buffer_cv = threading.Condition()
+        self.signal_pipe = signal_pipe
+        self.data_pipe = data_pipe
+        self.request_handling_thread: Optional[threading.Thread] = None
+
+        self.normal_signal = torch.tensor([0], device="cpu")
+        self.end_signal = None
+
+    def _is_end_signal(self, signal):
+        return signal is None
+
+    def drop_select_handler(self):
+
+        try:
+
+            while True:
+                signal = self.signal_pipe.recv_tensor()
+                if self._is_end_signal(signal):
+                    logger.info("Received end signal!")
+                    break
+
+                key_bytes = self.data_pipe.recv_tensor()
+                key_hashes = _byte_tensor_to_string(key_bytes).split(";")
+
+                with self.buffer_cv:
+                    for key_hash in key_hashes:
+                        if key_hash not in self.buffer:
+                            self.data_pipe.send_tensor(None)
+                            raise RuntimeError(
+                                f"Key {key_hash} not found in buffer")
+
+                    for key_hash in key_hashes:
+                        self.data_pipe.send_tensor(self.buffer[key_hash])
+                        self.buffer_cv.notify()
+                        self.buffer_size -= self.buffer[key_hash].element_size() * self.buffer[key_hash].numel()
+                        del self.buffer[key_hash]
+
+        except RuntimeError as e:
+            if 'Connection closed by peer' not in str(e):
+                raise e
+
+        logger.debug("Closing drop_select_handler")
+
+    def drop_select(self, keys: List[str]) -> List[Optional[torch.Tensor]]:
+
+        assert self.request_handling_thread is None, \
+            "drop_select should be called by the KV cache consumer "\
+            "(e.g. the decode vLLM instance)"
+
+        self.signal_pipe.send_tensor(self.normal_signal)
+
+        self.data_pipe.send_tensor(_string_to_byte_tensor(";".join(keys)))
+
+        value = []
+        for _ in range(len(keys)):
+            value.append(self.data_pipe.recv_tensor())
+
+        return value
+
+    def insert(self, key: str, value: List[torch.Tensor]) -> None:
+
+        if key in self.buffer:
+            return
+
+        tensor_size = sum(t.element_size() * t.numel() for t in value)
+
+        with self.buffer_cv:
+            if self.buffer_size + tensor_size > self.buffer_size_threshold:
+                # log outside the while loop to avoid this message being logged
+                # repeatedly.
+                logger.error("KV transfer buffer is full. Handling...")
+                while self.buffer_size + tensor_size > self.buffer_size_threshold:
+                    self.buffer_cv.wait()
+
+            self.buffer_size += tensor_size
+            self.buffer[key] = value
+            self.buffer_cv.notify()
+
+        # when calling the insert, the current process is a sender
+        # need to launch the request handler and start listening to request.
+        if self.request_handling_thread is None:
+            self.request_handling_thread = threading.Thread(
+                target=self.drop_select_handler)
+            self.request_handling_thread.start()
+
+    def close(self):
+
+        if hasattr(self, "request_handling_thread"
+                   ) and self.request_handling_thread is not None:
+            self.request_handling_thread.join()
+
+        else:
+            # TODO: have a explicit close signal and have a explicit way to
+            # check if it's requester
+            self.signal_pipe.send_tensor(self.end_signal)
\ No newline at end of file
diff --git a/vllm/distributed/kv_transfer/kv_transfer_agent.py b/vllm/distributed/kv_transfer/kv_transfer_agent.py
index 1e80e0bd7..75a7d697a 100644
--- a/vllm/distributed/kv_transfer/kv_transfer_agent.py
+++ b/vllm/distributed/kv_transfer/kv_transfer_agent.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """A centralized entrypoint to perform distributed KV cache transfer.
 
@@ -18,6 +19,9 @@ from vllm.distributed.kv_transfer.kv_connector.factory import (
 from vllm.logger import init_logger
 from vllm.sequence import IntermediateTensors
 
+if TYPE_CHECKING:
+    from vllm.attention import AttentionMetadata
+
 logger = init_logger(__name__)
 
 
@@ -57,20 +61,36 @@ class KVTransferAgent:
         hidden_or_intermediate_states: Union[torch.Tensor,
                                              IntermediateTensors],
     ) -> None:
-
+        print("model_executable",model_executable)
         self.connector.send_kv_caches_and_hidden_states(
             model_executable, model_input, kv_caches,
             hidden_or_intermediate_states)
 
+    def send_one_layer_kv_cache(self, layer_id: int,
+                                input_token_hash: List[str],
+                                kv_cache: torch.Tensor,
+                                attn_metadata: "AttentionMetadata",
+                                block_size: int) -> None:
+        self.connector.send_one_layer_kv_cache(layer_id, input_token_hash,
+                                               kv_cache, attn_metadata,
+                                               block_size)
+
+    def send_hidden_states(self, input_token_hash: List[str],
+                           hidden_states: torch.Tensor, attn_metadata) -> None:
+        self.connector.send_hidden_states(input_token_hash, hidden_states,
+                                          attn_metadata)
+
     def close(self) -> None:
         self.connector.close()
 
     def recv_kv_caches_and_hidden_states(
-        self, model_executable: torch.nn.Module,
+        self,
+        model_executable: torch.nn.Module,
         model_input: "ModelInputForGPUWithSamplingMetadata",
-        kv_caches: List[torch.Tensor]
-    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
+        kv_caches: List[torch.Tensor],
+        **kwargs,
+    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], List[bool],
                "ModelInputForGPUWithSamplingMetadata"]:
 
         return self.connector.recv_kv_caches_and_hidden_states(
-            model_executable, model_input, kv_caches)
+            model_executable, model_input, kv_caches, **kwargs)
diff --git a/vllm/distributed/kv_transfer/kv_transfer_utils.py b/vllm/distributed/kv_transfer/kv_transfer_utils.py
new file mode 100644
index 000000000..2661431a1
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_transfer_utils.py
@@ -0,0 +1,150 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+import hashlib
+import threading
+
+import torch
+
+from vllm.config import VllmConfig
+from vllm.distributed import get_kv_transfer_group
+
+# Thread-local storage for context management
+_hook_context = threading.local()
+
+
+# the built-in hash function is not deterministic
+# due to the design of hash randomization.
+# so we need a way to hash the tensor in a deterministic way
+# between the Prefill & Decode processes.
+def get_tensor_stable_hash(tensor):
+    data = tensor.cpu().numpy().tobytes()
+    return hashlib.md5(data).hexdigest()
+
+
+def set_context_value(value):
+    """Set a value in the thread-local context."""
+    _hook_context.value = value
+
+
+def get_context_value():
+    """Get the value from the thread-local context."""
+    return getattr(_hook_context, "value", None)
+
+
+def maybe_register_PD_disagg_hooks(model: torch.nn.Module,
+                                   vllm_config: VllmConfig):
+    if vllm_config.kv_transfer_config is None:
+        return
+
+    if (vllm_config.kv_transfer_config.is_kv_producer
+            and vllm_config.kv_transfer_config.is_layerwise_kv_transfer):
+        register_model_PD_disagg_hooks(model, vllm_config)
+        register_decoder_layer_PD_disagg_hooks(model)
+
+
+def register_model_PD_disagg_hooks(model: torch.nn.Module,
+                                   vllm_config: VllmConfig):
+    """
+    Registers hooks:
+    - A pre-forward hook to save context data.
+    - A post-forward hook to send the hidden states.
+    """
+
+    # Pre-forward hook for the top-level model
+    def pre_forward_hook(module, args, kwargs):
+        if 'input_ids' not in kwargs:
+            raise ValueError("No input_ids tensor found in kwargs.")
+        if 'attn_metadata' not in kwargs:
+            raise ValueError("No attn_metadata tensor found in kwargs.")
+
+        input_ids = kwargs['input_ids']
+        attn_metadata = kwargs['attn_metadata']
+
+        # skip if it is a profile run
+        if input_ids.view(-1)[0].item() == 0:
+            return
+
+        input_id_hashes = []
+        start_pos = 0
+        for seq_length in attn_metadata.seq_lens:
+            end_pos = start_pos + seq_length
+            input_id_hashes.append(
+                get_tensor_stable_hash(input_ids[start_pos:end_pos]))
+            start_pos = end_pos
+
+        context_dict = {
+            'input_id_hashes': input_id_hashes,
+            'block_size': vllm_config.cache_config.block_size
+        }
+        set_context_value(context_dict)
+
+    def post_forward_hook(module, args, kwargs, output):
+
+        # in case of PP, the output might be of IntermediateTensors type
+        if not isinstance(output, torch.Tensor):
+            return output
+
+        context = get_context_value()
+        if context is None or 'input_id_hashes' not in context:
+            return output
+
+        input_id_hashes = get_context_value()['input_id_hashes']
+
+        if 'attn_metadata' not in kwargs:
+            raise ValueError("No attn_metadata tensor found in kwargs.")
+
+        attn_metadata = kwargs['attn_metadata']
+
+        hidden_states = output
+
+        get_kv_transfer_group().send_hidden_states(input_id_hashes,
+                                                   hidden_states,
+                                                   attn_metadata)
+
+        return output
+
+    # Register pre-forward and post-forward hooks to the top-level model
+    model.register_forward_pre_hook(pre_forward_hook, with_kwargs=True)
+    model.register_forward_hook(post_forward_hook, with_kwargs=True)
+
+
+def register_decoder_layer_PD_disagg_hooks(module: torch.nn.Module,
+                                           suffix="DecoderLayer"):
+    """
+    Find the modules of decoder layers and register forward hooks to send
+    kv cache of one layer.
+    """
+
+    def create_decoderlayer_hook(idx):
+
+        def decoderlayer_hook(module, args, kwargs, output):
+            # do nothing if is it is profile run
+            context = get_context_value()
+            if context is None:
+                return output
+            if any(key not in context
+                   for key in ('input_id_hashes', 'block_size')):
+                return output
+
+            input_id_hashes = context['input_id_hashes']
+            block_size = context['block_size']
+
+            kv_cache, attn_metadata = args[2], args[3]
+            get_kv_transfer_group().send_one_layer_kv_cache(
+                idx, input_id_hashes, kv_cache, attn_metadata, block_size)
+
+            return output
+
+        return decoderlayer_hook
+
+    if hasattr(module, "layers") and isinstance(
+            module.layers,
+        (list, torch.nn.ModuleList
+         )) and module.layers[0].__class__.__name__.endswith(suffix):
+        for idx, child_module in enumerate(module.layers):
+            child_module.register_forward_hook(create_decoderlayer_hook(idx),
+                                               with_kwargs=True)
+
+    # Recurse over standard named_children as well, in case nested modules
+    # also contain relevant children or their own 'layers' attribute.
+    for child_name, child_module in module.named_children():
+        register_decoder_layer_PD_disagg_hooks(child_module, suffix)
\ No newline at end of file
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index 321902d11..71916bd59 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Copyright 2023 The vLLM team.
@@ -354,7 +355,8 @@ class GroupCoordinator:
                 not self.xpu_communicator.disabled:
             return self.xpu_communicator.all_reduce(input_)
 
-        return torch.ops.vllm.all_reduce(input_, group_name=self.unique_name)
+        return all_reduce(input_, group_name=self.unique_name)
+        #return torch.ops.vllm.all_reduce(input_, group_name=self.unique_name)
 
     def _all_reduce_out_place(self, input_: torch.Tensor) -> torch.Tensor:
         # always try custom allreduce first,
@@ -1193,12 +1195,13 @@ def cleanup_dist_env_and_memory(shutdown_ray: bool = False):
     from vllm.platforms import current_platform
     if not current_platform.is_cpu():
         torch.cuda.empty_cache()
+    """
     try:
         torch._C._host_emptyCache()
     except AttributeError:
         logger.warning(
             "torch._C._host_emptyCache() only available in Pytorch >=2.5")
-
+    """
 
 def in_the_same_node_as(pg: Union[ProcessGroup, StatelessProcessGroup],
                         source_rank: int = 0) -> List[bool]:
diff --git a/vllm/distributed/utils.py b/vllm/distributed/utils.py
index 84f8c0a8e..a615bd3ab 100644
--- a/vllm/distributed/utils.py
+++ b/vllm/distributed/utils.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Copyright 2023 The vLLM team.
@@ -64,7 +65,14 @@ def get_pp_indices(num_hidden_layers: int, pp_rank: int,
                    pp_size: int) -> Tuple[int, int]:
     """Try to evenly distribute layers across partitions.
     If the number of layers is not divisible by the number of partitions,
-    the last partition will have the remaining layers.
+    the remaining layers are evenly distributed across all but the last
+    partition. The last partition is excluded because it often contains an
+    additional norm layer and we are attempting to balance compute.
+    If `pp_size > 2` and the number of remaining layers is
+    `0 < x <= pp_size - 2` then the remaining layers are evenly distributed
+    across the middle partitions. The first and last partitions are excluded
+    because they contain the input and output embeddings respectively and we
+    are attempting to reduce maximum memory consumption across partitions.
     """
     partition_list_str = envs.VLLM_PP_LAYER_PARTITION
     if partition_list_str is not None:
@@ -80,15 +88,20 @@ def get_pp_indices(num_hidden_layers: int, pp_rank: int,
         if sum(partitions) != num_hidden_layers:
             raise ValueError(
                 f"{sum(partitions)=} does not match {num_hidden_layers=}.")
-        start_layer = sum(partitions[:pp_rank])
-        end_layer = start_layer + partitions[pp_rank]
     else:
         layers_per_partition = num_hidden_layers // pp_size
-        start_layer = pp_rank * layers_per_partition
-        end_layer = start_layer + layers_per_partition
-
-        if pp_rank == pp_size - 1:
-            end_layer = num_hidden_layers
+        partitions = [layers_per_partition for _ in range(pp_size)]
+
+        if remaining_layers := num_hidden_layers % pp_size:
+            for i in range(2, remaining_layers + 2):
+                partitions[-i] += 1
+            logger.info("Hidden layers were unevenly partitioned: %s",
+                        ",".join(str(p) for p in partitions))
+            logger.info("This can be manually overridden using the "
+                        "VLLM_PP_LAYER_PARTITION environment variable")
+
+    start_layer = sum(partitions[:pp_rank])
+    end_layer = start_layer + partitions[pp_rank]
 
     return (start_layer, end_layer)
 
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 40c6fb456..ecc80d284 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import argparse
@@ -349,7 +350,7 @@ class EngineArgs:
         parser.add_argument(
             '--kv-cache-dtype',
             type=str,
-            choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],
+            choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3', 'int8'],
             default=EngineArgs.kv_cache_dtype,
             help='Data type for kv cache storage. If "auto", will use model '
             'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '
@@ -1273,6 +1274,12 @@ class EngineArgs:
             or "all" in detailed_trace_modules,
         )
 
+        if (self.kv_transfer_config is not None and \
+                self.kv_transfer_config.is_layerwise_kv_transfer and \
+                self.kv_transfer_config.is_kv_producer and \
+                not model_config.enforce_eager):
+            raise ValueError("layerwise KV producer only supports eager mode")
+
         config = VllmConfig(
             model_config=model_config,
             cache_config=cache_config,
diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index d82d9ad9d..88da08990 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import copy
@@ -230,6 +231,7 @@ class LLMEngine:
         self.prompt_adapter_config = vllm_config.prompt_adapter_config  # noqa
         self.observability_config = vllm_config.observability_config or ObservabilityConfig(  # noqa
         )
+        self.parallel_config.disable_custom_all_reduce = True
 
         logger.info(
             "Initializing a V0 LLM engine (v%s) with config: %s, "
@@ -1459,6 +1461,211 @@ class LLMEngine:
 
         return ctx.request_outputs
 
+    def step_new(self):
+        """Performs one decoding iteration and returns newly generated results.
+
+        .. figure:: https://i.imgur.com/sv2HssD.png
+            :alt: Overview of the step function
+            :align: center
+
+            Overview of the step function.
+
+        Details:
+            - Step 1: Schedules the sequences to be executed in the next
+              iteration and the token blocks to be swapped in/out/copy.
+
+                - Depending on the scheduling policy,
+                  sequences may be `preempted/reordered`.
+                - A Sequence Group (SG) refer to a group of sequences
+                  that are generated from the same prompt.
+
+            - Step 2: Calls the distributed executor to execute the model.
+            - Step 3: Processes the model output. This mainly includes:
+
+                - Decodes the relevant outputs.
+                - Updates the scheduled sequence groups with model outputs
+                  based on its `sampling parameters` (`use_beam_search` or not).
+                - Frees the finished sequence groups.
+
+            - Finally, it creates and returns the newly generated results.
+
+        Example:
+            >>> # Please see the example/ folder for more detailed examples.
+            >>>
+            >>> # initialize engine and request arguments
+            >>> engine = LLMEngine.from_engine_args(engine_args)
+            >>> example_inputs = [(0, "What is LLM?",
+            >>>    SamplingParams(temperature=0.0))]
+            >>>
+            >>> # Start the engine with an event loop
+            >>> while True:
+            >>>     if example_inputs:
+            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)
+            >>>         engine.add_request(str(req_id),prompt,sampling_params)
+            >>>
+            >>>     # continue the request processing
+            >>>     request_outputs = engine.step()
+            >>>     for request_output in request_outputs:
+            >>>         if request_output.finished:
+            >>>             # return or show the request output
+            >>>
+            >>>     if not (engine.has_unfinished_requests() or example_inputs):
+            >>>         break
+        """
+        if self.parallel_config.pipeline_parallel_size > 1:
+            raise NotImplementedError(
+                "Pipeline parallelism is only supported through AsyncLLMEngine "
+                "as performance will be severely degraded otherwise.")
+
+        # For llm_engine, there is no pipeline parallel support, so the engine
+        # used is always 0.
+        virtual_engine = 0
+
+        # These are cached outputs from previous iterations. None if on first
+        # iteration
+        cached_outputs = self.cached_scheduler_outputs[virtual_engine]
+        seq_group_metadata_list = cached_outputs.seq_group_metadata_list
+        scheduler_outputs = cached_outputs.scheduler_outputs
+        allow_async_output_proc = cached_outputs.allow_async_output_proc
+
+        ctx = self.scheduler_contexts[virtual_engine]
+
+        # Clear outputs for each new scheduler iteration
+        ctx.request_outputs.clear()
+
+        # Skip the scheduler if there are any remaining steps in the seq groups.
+        # This ensures that the scheduler is only called again when the current
+        # batch has completed.
+        if not self._has_remaining_steps(seq_group_metadata_list):
+            # Schedule iteration
+            (seq_group_metadata_list, scheduler_outputs,
+             allow_async_output_proc
+             ) = self.scheduler[virtual_engine].schedule()
+
+            ctx.seq_group_metadata_list = seq_group_metadata_list
+            ctx.scheduler_outputs = scheduler_outputs
+
+            finished_requests_ids = self.scheduler[
+                virtual_engine].get_and_reset_finished_requests_ids()
+
+            # Maybe switch from async mode to sync mode
+            if not allow_async_output_proc and len(ctx.output_queue) > 0:
+                self._process_model_outputs(ctx=ctx)
+
+            if (self.scheduler_config.is_multi_step
+                    and scheduler_outputs.num_lookahead_slots > 0):
+                # cache the scheduler outputs for the next iteration if we have
+                # lookahead slots
+                self._cache_scheduler_outputs_for_multi_step(
+                    virtual_engine, seq_group_metadata_list, scheduler_outputs,
+                    allow_async_output_proc)
+        else:
+            finished_requests_ids = list()
+
+        assert seq_group_metadata_list is not None
+        assert scheduler_outputs is not None
+
+        if not scheduler_outputs.is_empty():
+
+            # Check if we have a cached last_output from the previous iteration.
+            # For supporting PP this is probably the best way to pass the
+            # sampled_token_ids, as a separate broadcast over all the PP stages
+            # will cause one virtual engine's microbatch to block the pipeline.
+            last_sampled_token_ids = \
+                self._get_last_sampled_token_ids(virtual_engine)
+
+            execute_model_req = ExecuteModelRequest(
+                seq_group_metadata_list=seq_group_metadata_list,
+                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,
+                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,
+                blocks_to_copy=scheduler_outputs.blocks_to_copy,
+                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,
+                running_queue_size=scheduler_outputs.running_queue_size,
+                finished_requests_ids=finished_requests_ids,
+                # We use ExecuteModelRequest to pass the last sampled_token_ids
+                # to each of the non-last PP stages for in-place prepare_input.
+                last_sampled_token_ids=last_sampled_token_ids)
+
+            if allow_async_output_proc:
+                execute_model_req.async_callback = self.async_callbacks[
+                    virtual_engine]
+
+            outputs = self.model_executor.execute_model_return_hidden(
+                execute_model_req=execute_model_req)
+
+            # We need to do this here so that last step's sampled_token_ids can
+            # be passed to the next iteration for PP.
+            if self.scheduler_config.is_multi_step:
+                self._update_cached_scheduler_output(virtual_engine, outputs)
+        else:
+            # Nothing scheduled => If there is pending async postprocessor,
+            # then finish it here.
+            if len(ctx.output_queue) > 0:
+                self._process_model_outputs(ctx=ctx)
+            # No outputs in this case
+            outputs = []
+
+        # Finish the current step for all the sequence groups.
+        if self.scheduler_config.is_multi_step:
+            for seq_group in seq_group_metadata_list:
+                seq_group.finish_step()
+
+        if not self._has_remaining_steps(seq_group_metadata_list):
+            # clear the cache if we have finished all the steps.
+            if self.scheduler_config.is_multi_step:
+                self.cached_scheduler_outputs[0] = SchedulerOutputState()
+
+            # is_first_step_output is True only when the num_steps of all
+            # the sequences are 1. When the num_steps > 1,
+            # multi_step_model_runner does the first-step output append.
+            is_first_step_output: bool = False if not seq_group_metadata_list \
+                else seq_group_metadata_list[0].state.num_steps == 1
+
+            # Add results to the output_queue
+            ctx.append_output(outputs=outputs,
+                              seq_group_metadata_list=seq_group_metadata_list,
+                              scheduler_outputs=scheduler_outputs,
+                              is_async=allow_async_output_proc,
+                              is_last_step=True,
+                              is_first_step_output=is_first_step_output)
+
+            if outputs and allow_async_output_proc:
+                assert len(outputs) == 1, (
+                    "Async postprocessor expects only a single output set")
+
+                self._advance_to_next_step(
+                    outputs[0], seq_group_metadata_list,
+                    scheduler_outputs.scheduled_seq_groups)
+
+            # Check if need to run the usual non-async path
+            if not allow_async_output_proc:
+                self._process_model_outputs(ctx=ctx)
+
+                # Log stats.
+                self.do_log_stats(scheduler_outputs, outputs)
+
+                # Tracing
+                self.do_tracing(scheduler_outputs)
+        else:
+            # Multi-step case # do not support step new return logits
+            return ctx.request_outputs, None
+
+        if not self.has_unfinished_requests():
+            # Drain async postprocessor (if exists)
+            if len(ctx.output_queue) > 0:
+                self._process_model_outputs(ctx=ctx)
+            assert len(ctx.output_queue) == 0
+
+            # Stop the execute model loop in parallel workers until there are
+            # more requests to process. This avoids waiting indefinitely in
+            # torch.distributed ops which may otherwise timeout, and unblocks
+            # the RPC thread in the workers so that they can process any other
+            # queued control plane messages, such as add/remove lora adapters.
+            logger.debug("Stopping remote worker execution loop.")
+            self.model_executor.stop_remote_worker_execution_loop()
+
+        return ctx.request_outputs, outputs[0].hidden_states
+
     def _has_remaining_steps(
         self, seq_group_metadata_list: Optional[List[SequenceGroupMetadata]]
     ) -> bool:
diff --git a/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py b/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
index 5c19888d4..354d4cc84 100644
--- a/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
+++ b/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import re
@@ -67,6 +68,8 @@ class DeepSeekR1ReasoningParser(ReasoningParser):
         ]):
             return None
 
+        # Check if <think> is present in previous or delta.
+        # Keep compatibility with models that don't generate <think> tokens.
         if self.think_start_token_id in previous_token_ids:
             if self.think_end_token_id in delta_token_ids:
                 # <think> in previous, </think> in delta,
@@ -85,7 +88,6 @@ class DeepSeekR1ReasoningParser(ReasoningParser):
                 # reasoning content continues
                 return DeltaMessage(reasoning_content=delta_text)
         elif self.think_start_token_id in delta_token_ids:
-            logger.info(delta_text)
             if self.think_end_token_id in delta_token_ids:
                 # <think> in delta, </think> in delta, extract reasoning content
                 start_index = delta_text.find(self.think_start_token)
@@ -101,35 +103,46 @@ class DeepSeekR1ReasoningParser(ReasoningParser):
                 # reasoning content continues
                 return DeltaMessage(reasoning_content=delta_text)
         else:
-            # No <think> in previous or delta, reasoning content continues.
-            return DeltaMessage(content=delta_text)
+            # No <think> in previous or delta, also need to check for </think>.
+            # Because the model may have generated </think> without <think>
+            # Ref https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/8a58a132790c9935686eb97f042afa8013451c9f
+            if self.think_end_token_id in delta_token_ids:
+                # </think> in delta with more tokens,
+                # extract reasoning content and content
+                end_index = delta_text.find(self.think_end_token)
+                reasoning_content = delta_text[:end_index]
+                content = delta_text[end_index + len(self.think_end_token):]
+                return DeltaMessage(reasoning_content=reasoning_content,
+                                    content=content if content else None)
+            elif self.think_end_token_id in previous_token_ids:
+                # </think> in previous, thinking content ends
+                return DeltaMessage(content=delta_text)
+            else:
+                # no </think> in previous or delta, reasoning content continues
+                return DeltaMessage(reasoning_content=delta_text)
 
     def extract_reasoning_content(
             self, model_output: str, request: ChatCompletionRequest
     ) -> Tuple[Optional[str], Optional[str]]:
 
-        # Check if the model output contains the <think> tokens.
-        if (self.think_start_token not in model_output
-                or self.think_end_token not in model_output):
-            return None, model_output
+        # DeepSeek R1 doesn't generate <think> now.
+        # Thus we assume the reasoning content is always at the start.
+        # Ref https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/8a58a132790c9935686eb97f042afa8013451c9f
+        if self.think_end_token not in model_output:
+            return model_output, None
         else:
+            # Add a start token if it's missing to keep compatibility.
+            if self.think_start_token not in model_output:
+                model_output = f"{self.think_start_token}{model_output}"
             # Use a regex to find the reasoning content
             reasoning_content = self.reasoning_regex.findall(model_output)[0]
 
-            # Remove the reasoning content from the model output
-            # Although deepseek's <think> token is always at the
-            # beginning of the line, we cannot guarantee that the
-            # other models will follow this convention.
-            # Therefore, we need to add :start_index.
-            start_index = model_output.find(self.think_start_token)
-            if start_index != -1:
-                end_index = start_index + len(
-                    f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
-                )
-                model_output = model_output[:start_index] + \
-                                model_output[end_index:]
-
-                if len(model_output) == 0:
-                    return reasoning_content, None
-
-            return reasoning_content, model_output
+            end_index = len(
+                f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
+            )
+            final_output = model_output[end_index:]
+
+            if len(final_output) == 0:
+                return reasoning_content, None
+
+            return reasoning_content, final_output
\ No newline at end of file
diff --git a/vllm/envs.py b/vllm/envs.py
index 745b068b7..6c1e3ad90 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import os
@@ -76,8 +77,8 @@ if TYPE_CHECKING:
     VLLM_ENABLE_V1_MULTIPROCESSING: bool = True
     VLLM_LOG_BATCHSIZE_INTERVAL: float = -1
     VLLM_DISABLE_COMPILE_CACHE: bool = False
-    K_SCALE_CONSTANT: int = 200
-    V_SCALE_CONSTANT: int = 100
+    K_SCALE_CONSTANT: int = 127
+    V_SCALE_CONSTANT: int = 127
     VLLM_SERVER_DEV_MODE: bool = False
     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128
     VLLM_MLA_DISABLE: bool = False
@@ -87,6 +88,8 @@ if TYPE_CHECKING:
     VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False
     VLLM_RAY_PER_WORKER_GPUS: float = 1.0
     VLLM_RAY_BUNDLE_INDICES: str = ""
+    TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP: bool = True
+    TRITON_ENABLE_MACA_CHAIN_DOT_OPT: bool = True
 
 
 def get_default_cache_root():
@@ -496,11 +499,11 @@ environment_variables: Dict[str, Callable[[], Any]] = {
 
     # Divisor for dynamic key scale factor calculation for FP8 KV Cache
     "K_SCALE_CONSTANT":
-    lambda: int(os.getenv("K_SCALE_CONSTANT", "200")),
+    lambda: int(os.getenv("K_SCALE_CONSTANT", "127")),
 
     # Divisor for dynamic value scale factor calculation for FP8 KV Cache
     "V_SCALE_CONSTANT":
-    lambda: int(os.getenv("V_SCALE_CONSTANT", "100")),
+    lambda: int(os.getenv("V_SCALE_CONSTANT", "127")),
     # If set, enable multiprocessing in LLM for the V1 code path.
     "VLLM_ENABLE_V1_MULTIPROCESSING":
     lambda: bool(int(os.getenv("VLLM_ENABLE_V1_MULTIPROCESSING", "1"))),
@@ -546,6 +549,10 @@ environment_variables: Dict[str, Callable[[], Any]] = {
     "VLLM_MLA_DISABLE_REQUANTIZATION":
     lambda: bool(int(os.getenv("VLLM_MLA_DISABLE_REQUANTIZATION", "0"))),
 
+    # Trion kernel `decode_attention_fwd` maca opt
+    "TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP": True,
+    "TRITON_ENABLE_MACA_CHAIN_DOT_OPT": True,
+
     # If set, vLLM will use the Triton implementation of moe_align_block_size,
     # i.e. moe_align_block_size_triton in fused_moe.py.
     "VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON":
@@ -571,7 +578,7 @@ environment_variables: Dict[str, Callable[[], Any]] = {
     # for all allocations. Currently this primarily affects MLA, for most other
     # models the alignment is already naturally aligned to 256 bytes.
     "VLLM_CUDA_MEM_ALIGN_KV_CACHE":
-    lambda: bool(int(os.getenv("VLLM_CUDA_MEM_ALIGN_KV_CACHE", "1"))),
+    lambda: bool(int(os.getenv("VLLM_CUDA_MEM_ALIGN_KV_CACHE", "0"))),
 }
 
 # end-env-vars-definition
diff --git a/vllm/executor/executor_base.py b/vllm/executor/executor_base.py
index fb76276bb..117f8978e 100644
--- a/vllm/executor/executor_base.py
+++ b/vllm/executor/executor_base.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import asyncio
@@ -139,6 +140,13 @@ class ExecutorBase(ABC):
                                      args=(execute_model_req, ))
         return output[0]
 
+    def execute_model_return_hidden(
+        self, execute_model_req: ExecuteModelRequest
+    ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:
+        output = self.collective_rpc("execute_model_return_hidden",
+                                    args=(execute_model_req, ))
+        return output[0]
+
     def stop_remote_worker_execution_loop(self) -> None:
         """Releases parallel workers from model loop."""
         return
@@ -275,6 +283,21 @@ class DistributedExecutorBase(ExecutorBase):
         driver_outputs = self._driver_execute_model(execute_model_req)
         assert driver_outputs is not None
         return driver_outputs
+    
+    def execute_model_return_hidden(
+        self,
+        execute_model_req: ExecuteModelRequest,
+    ) -> List[SamplerOutput]:
+        if self.parallel_worker_tasks is None:
+            self.parallel_worker_tasks = self._run_workers(
+                "start_worker_execution_loop_return_hidden",
+                async_run_tensor_parallel_workers_only=True,
+                **self.extra_execute_model_run_workers_kwargs)
+
+        # Only the driver worker returns the sampling results.
+        driver_outputs = self._driver_execute_model_return_hidden(execute_model_req)
+        assert driver_outputs is not None
+        return driver_outputs
 
     def stop_remote_worker_execution_loop(self) -> None:
         if self.parallel_worker_tasks is None:
@@ -286,6 +309,17 @@ class DistributedExecutorBase(ExecutorBase):
         # Ensure that workers exit model loop cleanly
         # (this will raise otherwise)
         self._wait_for_tasks_completion(parallel_worker_tasks)
+    
+    def stop_remote_worker_execution_loop_for_hidden(self) -> None:
+        if self.parallel_worker_tasks is None:
+            return
+
+        self._driver_execute_model_return_hidden(execute_model_req=None)
+        parallel_worker_tasks = self.parallel_worker_tasks
+        self.parallel_worker_tasks = None
+        # Ensure that workers exit model loop cleanly
+        # (this will raise otherwise)
+        self._wait_for_tasks_completion(parallel_worker_tasks)
 
     @abstractmethod
     def _driver_execute_model(
@@ -299,6 +333,18 @@ class DistributedExecutorBase(ExecutorBase):
         """
         raise NotImplementedError
 
+    @abstractmethod
+    def _driver_execute_model_return_hidden(
+        self, execute_model_req: Optional[ExecuteModelRequest]
+    ) -> Optional[List[SamplerOutput]]:
+        """Run execute_model in the driver worker.
+
+        Passing None will cause the driver to stop the model execution loop
+        running in each of the remote workers. In this case, this method
+        returns None. Otherwise, this method returns the model output.
+        """
+        raise NotImplementedError
+
     def collective_rpc(self,
                        method: Union[str, Callable],
                        timeout: Optional[float] = None,
diff --git a/vllm/executor/mp_distributed_executor.py b/vllm/executor/mp_distributed_executor.py
index d1f8c36fb..a97b5a451 100644
--- a/vllm/executor/mp_distributed_executor.py
+++ b/vllm/executor/mp_distributed_executor.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import asyncio
@@ -47,6 +48,12 @@ class MultiprocessingDistributedExecutor(DistributedExecutorBase):
                 f"please ensure that world_size ({world_size}) "
                 f"is less than than max local gpu count ({cuda_device_count})")
 
+        if "MACA_VISIBLE_DEVICES" in os.environ:
+            logger.info("update CUDA_VISIBLE_DEVICES with the value of MACA_VISIBLE_DEVICES")
+            update_environment_variables: ({
+                "CUDA_VISIBLE_DEVICES": ("".join(os.environ["MACA_VISIBLE_DEVICES"]))
+            })
+
         # Set CUDA_VISIBLE_DEVICES for the driver, inherited by workers
         if "CUDA_VISIBLE_DEVICES" not in os.environ:
             update_environment_variables({
@@ -142,6 +149,16 @@ class MultiprocessingDistributedExecutor(DistributedExecutorBase):
         loop running in each of the remote workers.
         """
         return self.driver_worker.execute_model(execute_model_req)
+    
+    def _driver_execute_model_return_hidden(
+        self, execute_model_req: Optional[ExecuteModelRequest]
+    ) -> Optional[List[SamplerOutput]]:
+        """Run execute_model in the driver worker.
+
+        Passing None will cause the driver to stop the model execution
+        loop running in each of the remote workers.
+        """
+        return self.driver_worker.execute_model_return_hidden(execute_model_req)
 
     def _run_workers(
         self,
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index 6a25a4d50..528b9c5a6 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import asyncio
@@ -400,6 +401,20 @@ class RayDistributedExecutor(DistributedExecutorBase):
             "driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1")
         return self.driver_worker.execute_method("execute_model",
                                                  execute_model_req)
+    
+    def _driver_execute_model_return_hidden(
+        self, execute_model_req: Optional[ExecuteModelRequest]
+    ) -> Optional[List[SamplerOutput]]:
+        """Run execute_model in the driver worker.
+
+        Passing None will cause the driver to stop the model execution
+        loop running in each of the remote workers.
+        """
+        assert not self.use_ray_spmd_worker, (
+            "driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1")
+        return self.driver_worker.execute_method("execute_model_return_hidden",
+                                                 execute_model_req)
+
 
     def execute_model(
             self,
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index f14200e02..1fd914b19 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Fused MoE kernel."""
 import functools
@@ -16,6 +17,7 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (
     per_token_group_quant_fp8)
 from vllm.platforms import current_platform
 from vllm.utils import direct_register_custom_op
+import math
 
 logger = init_logger(__name__)
 
@@ -61,6 +63,8 @@ def fused_moe_kernel_gptq_awq(
         BLOCK_SIZE_N: tl.constexpr,
         BLOCK_SIZE_K: tl.constexpr,
         GROUP_SIZE_M: tl.constexpr,
+        SPLIT_K: tl.constexpr,
+        ACCF32: tl.constexpr,
         MUL_ROUTED_WEIGHT: tl.constexpr,
         top_k: tl.constexpr,
         compute_type: tl.constexpr,
@@ -220,6 +224,21 @@ def fused_moe_kernel_gptq_awq(
     tl.store(c_ptrs, accumulator, mask=c_mask)
 
 
+@triton.heuristics(
+    {
+        "UPGRADE": lambda args: math.ceil((args["EM"] * args["N"]) / (args["BLOCK_SIZE_M"] * args["BLOCK_SIZE_N"])).bit_length() > 32,
+    }
+)
+@triton.heuristics(
+    {
+        "UPGRADE_A_OFFS": lambda args: (args["num_valid_tokens"] // args["top_k"] * args["stride_am"] + args["BLOCK_SIZE_K"] * args["stride_ak"]).bit_length() > 32,
+    }
+)
+@triton.heuristics(
+    {
+        "UPGRADE_B_OFFS": lambda args: (args["experts_num"] * args["stride_be"] + args["BLOCK_SIZE_K"] * args["stride_bk"] + args["N"] * args["stride_bn"]).bit_length() > 32,
+    }
+)
 @triton.jit
 def fused_moe_kernel(
         # Pointers to matrices
@@ -261,11 +280,18 @@ def fused_moe_kernel(
         BLOCK_SIZE_N: tl.constexpr,
         BLOCK_SIZE_K: tl.constexpr,
         GROUP_SIZE_M: tl.constexpr,
+        SPLIT_K: tl.constexpr,
+        ACCF32: tl.constexpr,
         MUL_ROUTED_WEIGHT: tl.constexpr,
         top_k: tl.constexpr,
+        experts_num: tl.constexpr,
         compute_type: tl.constexpr,
         use_fp8_w8a8: tl.constexpr,
-        use_int8_w8a16: tl.constexpr):
+        use_int8_w8a8: tl.constexpr,
+        use_int8_w8a16: tl.constexpr,
+        UPGRADE: tl.constexpr,
+        UPGRADE_A_OFFS: tl.constexpr,
+        UPGRADE_B_OFFS: tl.constexpr):
     """
     Implements the fused computation for a Mixture of Experts (MOE) using
     token and expert matrices.
@@ -295,7 +321,13 @@ def fused_moe_kernel(
     # -----------------------------------------------------------
     # Map program ids `pid` to the block of C it should compute.
     # This is done in a grouped ordering to promote L2 data reuse.
-    pid = tl.program_id(axis=0)
+    # pid = tl.program_id(axis=0)
+    if UPGRADE:
+        pid = tl.program_id(axis=0).to(tl.int64)
+        pid_z = tl.program_id(axis=1).to(tl.int64)
+    else:
+        pid = tl.program_id(axis=0)
+        pid_z = tl.program_id(axis=1)
     num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)
     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
     num_pid_in_group = GROUP_SIZE_M * num_pid_n
@@ -318,14 +350,23 @@ def fused_moe_kernel(
         tl.int64)
     offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)
     token_mask = offs_token < num_valid_tokens
+    if UPGRADE_A_OFFS:
+        offs_token = offs_token.to(tl.int64)
 
-    offs_bn = (pid_n * BLOCK_SIZE_N +
-               tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N
-    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    if UPGRADE_B_OFFS:
+        offs_bn = (pid_n * BLOCK_SIZE_N +
+                   tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N
+    else:
+        offs_bn = (pid_n * BLOCK_SIZE_N +
+                   tl.arange(0, BLOCK_SIZE_N)) % N
+    offs_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)
     a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +
                       offs_k[None, :] * stride_ak)
 
-    off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
+    if UPGRADE_B_OFFS:
+        off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
+    else:
+        off_experts = tl.load(expert_ids_ptr + pid_m)
     b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +
                                                 offs_bn[None, :] * stride_bn)
     if use_int8_w8a16:
@@ -333,6 +374,12 @@ def fused_moe_kernel(
             None, :] * stride_bsn
         b_scale = tl.load(b_scale_ptrs)
 
+    if use_int8_w8a8:  
+        a_scale = tl.load(a_scale_ptr+(offs_token[:, None] // top_k * stride_asm),mask=token_mask[:, None],other=0.0)
+        b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + offs_bn[
+            None, :] * stride_bsn
+        b_scale = tl.load(b_scale_ptrs)
+
     if use_fp8_w8a8:
         if group_k > 0 and group_n > 0:
             a_scale_ptrs = a_scale_ptr + (offs_token // top_k) * stride_asm
@@ -348,24 +395,28 @@ def fused_moe_kernel(
     # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
     # of fp32 values for higher accuracy.
     # `accumulator` will be converted back to fp16 after the loop.
-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+    # accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32 if use_int8_w8a8 else tl.float32)
 
-    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):
         # Load the next block of A and B, generate a mask by checking the
         # K dimension.
         a = tl.load(a_ptrs,
                     mask=token_mask[:, None] &
-                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),
+                   (offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K),
                     other=0.0)
         b = tl.load(b_ptrs,
-                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,
+                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K,
                     other=0.0)
         # We accumulate along the K dimension.
         if use_int8_w8a16:
             accumulator = tl.dot(a, b.to(compute_type), acc=accumulator)
+        elif use_int8_w8a8:
+            a = a.to(tl.int8)
+            accumulator += tl.dot(a, b,out_dtype=accumulator.dtype)
         elif use_fp8_w8a8:
             if group_k > 0 and group_n > 0:
-                k_start = k * BLOCK_SIZE_K
+                k_start = k * BLOCK_SIZE_K * SPLIT_K
                 offs_ks = k_start // group_k
                 a_scale = tl.load(a_scale_ptrs + offs_ks * stride_ask,
                                   mask=token_mask,
@@ -379,8 +430,8 @@ def fused_moe_kernel(
         else:
             accumulator += tl.dot(a, b)
         # Advance the ptrs to the next K block.
-        a_ptrs += BLOCK_SIZE_K * stride_ak
-        b_ptrs += BLOCK_SIZE_K * stride_bk
+        a_ptrs += BLOCK_SIZE_K * stride_ak * SPLIT_K
+        b_ptrs += BLOCK_SIZE_K * stride_bk* SPLIT_K
 
     if MUL_ROUTED_WEIGHT:
         moe_weight = tl.load(topk_weights_ptr + offs_token,
@@ -389,6 +440,11 @@ def fused_moe_kernel(
         accumulator = accumulator * moe_weight[:, None]
     if use_int8_w8a16:
         accumulator = (accumulator * b_scale).to(compute_type)
+    elif use_int8_w8a8:
+        accumulator = accumulator.to(tl.float32)
+        accumulator = (accumulator * a_scale * b_scale)
+        if not ACCF32:
+            accumulator = accumulator.to(compute_type)
     elif use_fp8_w8a8:
         if group_k > 0 and group_n > 0:
             accumulator = accumulator.to(compute_type)
@@ -402,7 +458,10 @@ def fused_moe_kernel(
     c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[
         None, :]
     c_mask = token_mask[:, None] & (offs_cn[None, :] < N)
-    tl.store(c_ptrs, accumulator, mask=c_mask)
+    if SPLIT_K == 1:
+        tl.store(c_ptrs, accumulator, mask=c_mask)
+    else:
+        tl.atomic_add(c_ptrs, accumulator, mask=c_mask)
 
 
 def ceil_div(a, b):
@@ -636,8 +695,10 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
                             config: Dict[str, Any],
                             compute_type: tl.dtype,
                             use_fp8_w8a8: bool,
+                            use_int8_w8a8: bool, 
                             use_int8_w8a16: bool,
                             use_int4_w4a16: bool,
+                            orig_acc_dtype: torch.dtype,
                             block_shape: Optional[List[int]] = None) -> None:
     assert topk_weights.stride(1) == 1
     assert sorted_token_ids.stride(0) == 1
@@ -653,6 +714,9 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             assert triton.cdiv(A.shape[-1], block_k) == A_scale.shape[-1]
             assert triton.cdiv(B.shape[-2], block_n) == B_scale.shape[-2]
             assert triton.cdiv(B.shape[-1], block_k) == B_scale.shape[-1]
+    elif use_int8_w8a8:
+        A, A_scale,_ = ops.scaled_int8_quant(A, A_scale)
+        assert B_scale is not None
     elif use_int8_w8a16 or use_int4_w4a16:
         assert B_scale is not None
         assert block_shape is None or block_shape[0] == 0
@@ -669,7 +733,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
         EM = min(sorted_token_ids.shape[0],
                  A.shape[0] * top_k * config['BLOCK_SIZE_M'])
     grid = lambda META: (triton.cdiv(EM, META['BLOCK_SIZE_M']) * triton.cdiv(
-        B.shape[1], META['BLOCK_SIZE_N']), )
+        B.shape[1], META['BLOCK_SIZE_N']), META['SPLIT_K'] )
 
     if (use_int8_w8a16 or use_int4_w4a16) and \
             block_shape is not None and block_shape[1] > 0:
@@ -703,7 +767,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             B_zp.stride(0) if B_zp is not None else 0,
             B_zp.stride(2) if B_zp is not None else 0,
             B_zp.stride(1) if B_zp is not None else 0,
-            block_k_diviable=A.shape[1] % config["BLOCK_SIZE_K"] == 0,
+            block_k_diviable=A.shape[1] % config["BLOCK_SIZE_K"] * config["SPLIT_K"] == 0,
             group_size=block_shape[1],
             MUL_ROUTED_WEIGHT=mul_routed_weight,
             top_k=top_k,
@@ -750,11 +814,16 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             0 if block_shape is None else block_shape[1],
             MUL_ROUTED_WEIGHT=mul_routed_weight,
             top_k=top_k,
+            experts_num=expert_ids.shape[0],
             compute_type=compute_type,
             use_fp8_w8a8=use_fp8_w8a8,
+            use_int8_w8a8=use_int8_w8a8,
             use_int8_w8a16=use_int8_w8a16,
             **config,
         )
+    if config["ACCF32"]:
+       C = C.to(orig_acc_dtype)
+    return C
 
 
 # Adapted from: https://github.com/sgl-project/sglang/pull/2628
@@ -763,6 +832,7 @@ def get_config_file_name(E: int,
                          dtype: Optional[str],
                          block_shape: Optional[List[int]] = None) -> str:
     device_name = current_platform.get_device_name().replace(" ", "_")
+    device_name = "Device_4000"
     dtype_selector = "" if not dtype else f",dtype={dtype}"
     block_shape_selector = ("" if not block_shape or not all(block_shape) else
                             f",block_shape={block_shape}").replace(" ", "")
@@ -974,14 +1044,17 @@ def grouped_topk(hidden_states: torch.Tensor,
 
 def get_config_dtype_str(dtype: torch.dtype,
                          use_int4_w4a16: Optional[bool] = False,
+                         use_int8_w8a8: Optional[bool] = False,
                          use_int8_w8a16: Optional[bool] = False,
                          use_fp8_w8a8: Optional[bool] = False):
     if use_fp8_w8a8:
         return "fp8_w8a8"
     elif use_int8_w8a16:
         return "int8_w8a16"
+    elif use_int8_w8a8:
+        return "int8_w8a8"
     elif use_int4_w4a16:
-        return "int4_w8a16"
+        return "int4_w4a16"
     elif dtype == torch.float:
         # avoiding cases where kernel fails when float32 MoE
         # use fp16/bfloat16 configs
@@ -995,6 +1068,7 @@ def inplace_fused_experts(hidden_states: torch.Tensor,
                           topk_weights: torch.Tensor,
                           topk_ids: torch.Tensor,
                           use_fp8_w8a8: bool = False,
+                          use_int8_w8a8: bool = False, 
                           use_int8_w8a16: bool = False,
                           use_int4_w4a16: bool = False,
                           w1_scale: Optional[torch.Tensor] = None,
@@ -1005,7 +1079,7 @@ def inplace_fused_experts(hidden_states: torch.Tensor,
                           a2_scale: Optional[torch.Tensor] = None,
                           block_shape: Optional[List[int]] = None) -> None:
     fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,
-                       use_fp8_w8a8, use_int8_w8a16, use_int4_w4a16, w1_scale,
+                       use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, w1_scale,
                        w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape)
 
 
@@ -1043,6 +1117,7 @@ def outplace_fused_experts(
         topk_weights: torch.Tensor,
         topk_ids: torch.Tensor,
         use_fp8_w8a8: bool = False,
+        use_int8_w8a8: bool = False,
         use_int8_w8a16: bool = False,
         use_int4_w4a16: bool = False,
         w1_scale: Optional[torch.Tensor] = None,
@@ -1053,7 +1128,7 @@ def outplace_fused_experts(
         a2_scale: Optional[torch.Tensor] = None,
         block_shape: Optional[List[int]] = None) -> torch.Tensor:
     return fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids,
-                              False, use_fp8_w8a8, use_int8_w8a16,
+                              False, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16,
                               use_int4_w4a16, w1_scale, w2_scale, w1_zp, w2_zp,
                               a1_scale, a2_scale, block_shape)
 
@@ -1092,6 +1167,7 @@ def fused_experts(hidden_states: torch.Tensor,
                   topk_ids: torch.Tensor,
                   inplace: bool = False,
                   use_fp8_w8a8: bool = False,
+                  use_int8_w8a8: bool = False, 
                   use_int8_w8a16: bool = False,
                   use_int4_w4a16: bool = False,
                   w1_scale: Optional[torch.Tensor] = None,
@@ -1102,16 +1178,18 @@ def fused_experts(hidden_states: torch.Tensor,
                   a2_scale: Optional[torch.Tensor] = None,
                   block_shape: Optional[List[int]] = None):
     if inplace:
-        torch.ops.vllm.inplace_fused_experts(hidden_states, w1, w2,
+        #torch.ops.vllm.inplace_fused_experts(hidden_states, w1, w2,
+        inplace_fused_experts(hidden_states, w1, w2,
                                              topk_weights, topk_ids,
-                                             use_fp8_w8a8, use_int8_w8a16,
+                                             use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16,
                                              use_int4_w4a16, w1_scale,
                                              w2_scale, w1_zp, w2_zp, a1_scale,
                                              a2_scale, block_shape)
         return hidden_states
     else:
-        return torch.ops.vllm.outplace_fused_experts(
-            hidden_states, w1, w2, topk_weights, topk_ids, use_fp8_w8a8,
+        #return torch.ops.vllm.outplace_fused_experts(
+        return outplace_fused_experts(
+            hidden_states, w1, w2, topk_weights, topk_ids, use_fp8_w8a8, use_int8_w8a8,
             use_int8_w8a16, use_int4_w4a16, w1_scale, w2_scale, w1_zp, w2_zp,
             a1_scale, a2_scale, block_shape)
 
@@ -1123,6 +1201,7 @@ def fused_experts_impl(hidden_states: torch.Tensor,
                        topk_ids: torch.Tensor,
                        inplace: bool = False,
                        use_fp8_w8a8: bool = False,
+                       use_int8_w8a8: bool = False,
                        use_int8_w8a16: bool = False,
                        use_int4_w4a16: bool = False,
                        w1_scale: Optional[torch.Tensor] = None,
@@ -1154,6 +1233,7 @@ def fused_experts_impl(hidden_states: torch.Tensor,
     CHUNK_SIZE = envs.VLLM_FUSED_MOE_CHUNK_SIZE
     M = min(num_tokens, CHUNK_SIZE)
     config_dtype = get_config_dtype_str(use_fp8_w8a8=use_fp8_w8a8,
+                                        use_int8_w8a8=use_int8_w8a8, 
                                         use_int8_w8a16=use_int8_w8a16,
                                         use_int4_w4a16=use_int4_w4a16,
                                         dtype=hidden_states.dtype)
@@ -1169,15 +1249,44 @@ def fused_experts_impl(hidden_states: torch.Tensor,
 
     config = get_config_func(M)
 
-    intermediate_cache1 = torch.empty((M, topk_ids.shape[1], N),
-                                      device=hidden_states.device,
-                                      dtype=hidden_states.dtype)
+    stage1_config = config["stage1"] if "stage1" in config else config
+    stage2_config = config["stage2"] if "stage2" in config else config
+    if 'ACCF32' not in stage1_config:
+        stage1_config['ACCF32'] = False
+    if 'ACCF32' not in stage2_config:
+        stage2_config['ACCF32'] = False
+    if 'SPLIT_K' not in stage1_config:
+        stage1_config['SPLIT_K'] = 1
+    if 'SPLIT_K' not in stage2_config:
+        stage2_config['SPLIT_K'] = 1
+
+    if stage1_config['ACCF32']:
+       acc_type1 = torch.float32
+    else:
+       acc_type1 = hidden_states.dtype
+    if stage2_config['ACCF32']:
+       acc_type2 = torch.float32
+    else:
+       acc_type2 = hidden_states.dtype
+    if stage1_config['SPLIT_K'] > 1:
+        intermediate_cache1 = torch.zeros((M, topk_ids.shape[1], N),
+                                          device=hidden_states.device,
+                                          dtype=acc_type1)
+    else:
+        intermediate_cache1 = torch.empty((M, topk_ids.shape[1], N),
+                                          device=hidden_states.device,
+                                          dtype=hidden_states.dtype)
     intermediate_cache2 = torch.empty((M * topk_ids.shape[1], N // 2),
                                       device=hidden_states.device,
                                       dtype=hidden_states.dtype)
-    intermediate_cache3 = torch.empty((M, topk_ids.shape[1], w2.shape[1]),
-                                      device=hidden_states.device,
-                                      dtype=hidden_states.dtype)
+    if stage2_config['SPLIT_K'] > 1:
+        intermediate_cache3 = torch.zeros((M, topk_ids.shape[1], w2.shape[1]),
+                                          device=hidden_states.device,
+                                          dtype=acc_type2)
+    else:
+        intermediate_cache3 = torch.empty((M, topk_ids.shape[1], w2.shape[1]),
+                                          device=hidden_states.device,
+                                          dtype=hidden_states.dtype)
 
     if hidden_states.dtype == torch.bfloat16:
         compute_type = tl.bfloat16
@@ -1216,51 +1325,71 @@ def fused_experts_impl(hidden_states: torch.Tensor,
         curr_topk_ids = topk_ids[begin_chunk_idx:end_chunk_idx]
         curr_topk_weights = topk_weights[begin_chunk_idx:end_chunk_idx]
 
+       
         sorted_token_ids, expert_ids, num_tokens_post_padded = (
-            moe_align_block_size(curr_topk_ids, config['BLOCK_SIZE_M'], E))
-
-        invoke_fused_moe_kernel(curr_hidden_states,
-                                w1,
-                                intermediate_cache1,
-                                a1_scale,
-                                w1_scale,
-                                w1_zp,
-                                curr_topk_weights,
-                                curr_topk_ids,
-                                sorted_token_ids,
-                                expert_ids,
-                                num_tokens_post_padded,
-                                False,
-                                topk_ids.shape[1],
-                                config,
-                                compute_type=compute_type,
-                                use_fp8_w8a8=use_fp8_w8a8,
-                                use_int8_w8a16=use_int8_w8a16,
-                                use_int4_w4a16=use_int4_w4a16,
-                                block_shape=block_shape)
+            moe_align_block_size(curr_topk_ids, stage1_config['BLOCK_SIZE_M'], E))
+
+        if (stage1_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and (topk_ids.shape[1] == 1 or topk_ids.shape[1] == 2) and
+            (curr_hidden_states.dtype == torch.bfloat16 or curr_hidden_states.dtype == torch.float16) and
+            w1.shape[1] % 4 == 0 and w1.shape[2] % 8 == 0):
+            ops.fused_moe_kernel(curr_hidden_states, w1, intermediate_cache1,
+                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
+                                expert_ids, num_tokens_post_padded, False,
+                                topk_ids.shape[1], 0)
+        
+        else:
+            intermediate_cache1 = invoke_fused_moe_kernel(curr_hidden_states,
+                                                          w1,
+                                                          intermediate_cache1,
+                                                          a1_scale,
+                                                          w1_scale,
+                                                          w1_zp,
+                                                          curr_topk_weights,
+                                                          curr_topk_ids,
+                                                          sorted_token_ids,
+                                                          expert_ids,
+                                                          num_tokens_post_padded,
+                                                          False,
+                                                          topk_ids.shape[1],
+                                                          stage1_config,
+                                                          compute_type=compute_type,
+                                                          use_fp8_w8a8=use_fp8_w8a8,
+                                                          use_int8_w8a8=use_int8_w8a8,
+                                                          use_int8_w8a16=use_int8_w8a16,
+                                                          use_int4_w4a16=use_int4_w4a16,
+                                                          orig_acc_dtype=hidden_states.dtype,
+                                                          block_shape=block_shape)
 
         torch.ops._C.silu_and_mul(intermediate_cache2,
                                   intermediate_cache1.view(-1, N))
 
-        invoke_fused_moe_kernel(intermediate_cache2,
-                                w2,
-                                intermediate_cache3,
-                                a2_scale,
-                                w2_scale,
-                                w2_zp,
-                                curr_topk_weights,
-                                curr_topk_ids,
-                                sorted_token_ids,
-                                expert_ids,
-                                num_tokens_post_padded,
-                                True,
-                                1,
-                                config,
-                                compute_type=compute_type,
-                                use_fp8_w8a8=use_fp8_w8a8,
-                                use_int8_w8a16=use_int8_w8a16,
-                                use_int4_w4a16=use_int4_w4a16,
-                                block_shape=block_shape)
+        if (stage2_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and w2.shape[1] % 4 == 0 and w2.shape[2] % 8 == 0 and
+            (hidden_states.dtype == torch.bfloat16 or hidden_states.dtype == torch.float16)):
+            ops.fused_moe_kernel(intermediate_cache2, w2, intermediate_cache3,
+                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
+                                expert_ids, num_tokens_post_padded, True, 1, 0)
+        else:
+            intermediate_cache3 = invoke_fused_moe_kernel(intermediate_cache2,
+                                                          w2,
+                                                          intermediate_cache3,
+                                                          a2_scale,
+                                                          w2_scale,
+                                                          w2_zp,
+                                                          curr_topk_weights,
+                                                          curr_topk_ids,
+                                                          sorted_token_ids,
+                                                          expert_ids,
+                                                          num_tokens_post_padded,
+                                                          True,
+                                                          1,
+                                                          stage2_config,
+                                                          compute_type=compute_type,
+                                                          use_fp8_w8a8=use_fp8_w8a8,
+                                                          use_int8_w8a8=use_int8_w8a8,
+                                                          use_int8_w8a16=use_int8_w8a16,
+                                                          use_int4_w4a16=use_int4_w4a16,
+                                                          orig_acc_dtype=hidden_states.dtype,
+                                                          block_shape=block_shape)
 
         ops.moe_sum(intermediate_cache3.view(*intermediate_cache3.shape),
                     out_hidden_states[begin_chunk_idx:end_chunk_idx])
@@ -1280,6 +1409,7 @@ def fused_moe(
     topk_group: Optional[int] = None,
     custom_routing_function: Optional[Callable] = None,
     use_fp8_w8a8: bool = False,
+    use_int8_w8a8: bool = False,
     use_int8_w8a16: bool = False,
     use_int4_w4a16: bool = False,
     w1_scale: Optional[torch.Tensor] = None,
@@ -1310,7 +1440,9 @@ def fused_moe(
         note: Deepseekv2 model uses grouped_topk
     - use_fp8_w8a8 (bool): If True, use fp8 arithmetic to compute the inner
         products for w1 and w2. Defaults to False.
-    - use_int8_w8a16 (bool): If True, use matmul of int8 weight and bf16/fp16
+    - use_int8_w8a8 (bool): If True, use weight int8, activation int8 arithmetic to compute the inner
+        products for w1 and w2. Defaults to False.
+    - use_int8_w8a16 (bool): If True, use weight int8, activation int16 arithmetic to compute the inner
         activation to compute the inner products for w1 and w2.
         Defaults to False.
     - use_int4_w4a16 (bool): If True, use matmul of int4 weight and bf16/fp16
@@ -1352,6 +1484,7 @@ def fused_moe(
                          topk_ids,
                          inplace=inplace,
                          use_fp8_w8a8=use_fp8_w8a8,
+                         use_int8_w8a8=use_int8_w8a8,
                          use_int8_w8a16=use_int8_w8a16,
                          use_int4_w4a16=use_int4_w4a16,
                          w1_scale=w1_scale,
diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
index da8db08fe..94499f673 100644
--- a/vllm/model_executor/layers/linear.py
+++ b/vllm/model_executor/layers/linear.py
@@ -1,1159 +1,1202 @@
-# SPDX-License-Identifier: Apache-2.0
-
-import itertools
-from abc import abstractmethod
-from typing import Optional
-
-import torch
-import torch.nn.functional as F
-from torch.nn.parameter import Parameter, UninitializedParameter
-
-from vllm.distributed import (divide, get_tensor_model_parallel_rank,
-                              get_tensor_model_parallel_world_size,
-                              split_tensor_along_last_dim,
-                              tensor_model_parallel_all_gather,
-                              tensor_model_parallel_all_reduce)
-from vllm.logger import init_logger
-from vllm.model_executor.layers.quantization.base_config import (
-    QuantizationConfig, QuantizeMethodBase)
-# yapf: disable
-from vllm.model_executor.parameter import (BasevLLMParameter,
-                                           BlockQuantScaleParameter,
-                                           PackedColumnParameter,
-                                           PackedvLLMParameter,
-                                           PerTensorScaleParameter,
-                                           RowvLLMParameter)
-# yapf: enable
-from vllm.model_executor.utils import set_weight_attrs
-
-logger = init_logger(__name__)
-
-WEIGHT_LOADER_V2_SUPPORTED = [
-    "CompressedTensorsLinearMethod", "AWQMarlinLinearMethod",
-    "AWQLinearMethod", "GPTQMarlinLinearMethod", "Fp8LinearMethod",
-    "MarlinLinearMethod", "QQQLinearMethod", "GPTQMarlin24LinearMethod",
-    "TPUInt8LinearMethod", "GPTQLinearMethod", "FBGEMMFp8LinearMethod",
-    "ModelOptFp8LinearMethod", "IPEXAWQLinearMethod", "IPEXGPTQLinearMethod",
-    "HQQMarlinMethod", "QuarkLinearMethod"
-]
-
-
-def adjust_marlin_shard(param, shard_size, shard_offset):
-    marlin_tile_size = getattr(param, "marlin_tile_size", None)
-    if marlin_tile_size is None:
-        return shard_size, shard_offset
-
-    return shard_size * marlin_tile_size, shard_offset * marlin_tile_size
-
-
-def adjust_bitsandbytes_4bit_shard(param: Parameter,
-                                   shard_offsets: dict[str, tuple[int, int]],
-                                   loaded_shard_id: str) -> tuple[int, int]:
-    """Adjust the quantization offsets and sizes for BitsAndBytes sharding."""
-
-    total, _ = shard_offsets["total"]
-    orig_offset, orig_size = shard_offsets[loaded_shard_id]
-
-    quantized_total = param.data.shape[0]
-    quantized_offset = orig_offset * quantized_total // total
-    quantized_size = orig_size * quantized_total // total
-
-    return quantized_size, quantized_offset
-
-
-def adjust_scalar_to_fused_array(param, loaded_weight, shard_id):
-    """For fused modules (QKV and MLP) we have an array of length
-    N that holds 1 scale for each "logical" matrix. So the param
-    is an array of length N. The loaded_weight corresponds to 
-    one of the shards on disk. Here, we slice the param based on 
-    the shard_id for loading.
-    """
-    qkv_idxs = {"q": 0, "k": 1, "v": 2}
-
-    if isinstance(shard_id, str):
-        shard_id = qkv_idxs[shard_id]
-    elif not isinstance(shard_id, int):
-        raise ValueError(f"Unknown Shard Id {shard_id}")
-
-    # AutoFP8 scales do not have a shape
-    # compressed-tensors scales do have a shape
-    if len(loaded_weight.shape) != 0:
-        assert loaded_weight.shape[0] == 1
-        loaded_weight = loaded_weight[0]
-
-    return param[shard_id], loaded_weight
-
-
-class LinearMethodBase(QuantizeMethodBase):
-    """Base class for different (maybe quantized) linear methods."""
-
-    @abstractmethod
-    def create_weights(self, layer: torch.nn.Module,
-                       input_size_per_partition: int,
-                       output_partition_sizes: list[int], input_size: int,
-                       output_size: int, params_dtype: torch.dtype,
-                       **extra_weight_attrs):
-        """Create weights for a linear layer. 
-           The weights will be set as attributes of the layer.
-
-        Args:
-            layer: The layer that is using the LinearMethodBase factory.
-            input_size_per_partition: Size of the weight input dim on rank X.
-            output_partition_sizes: Sizes of the output dim of each logical 
-                weight on rank X. E.g., output_partition_sizes for QKVLinear
-                is a list contains the width of Wq, Wk, Wv on rank X.
-            input_size: Size of the input dim of the weight across all ranks.
-            output_size: Size of the output dim of the weight across all ranks.
-            params_dtype: Datatype of the parameters.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def apply(self,
-              layer: torch.nn.Module,
-              x: torch.Tensor,
-              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
-        """Apply the weights in layer to the input tensor.
-        Expects create_weights to have been called before on the layer."""
-        raise NotImplementedError
-
-
-class UnquantizedLinearMethod(LinearMethodBase):
-    """Linear method without quantization."""
-
-    def create_weights(self, layer: torch.nn.Module,
-                       input_size_per_partition: int,
-                       output_partition_sizes: list[int], input_size: int,
-                       output_size: int, params_dtype: torch.dtype,
-                       **extra_weight_attrs):
-        weight = Parameter(torch.empty(sum(output_partition_sizes),
-                                       input_size_per_partition,
-                                       dtype=params_dtype),
-                           requires_grad=False)
-        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
-        layer.register_parameter("weight", weight)
-        set_weight_attrs(weight, extra_weight_attrs)
-
-    def apply(self,
-              layer: torch.nn.Module,
-              x: torch.Tensor,
-              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
-
-        return F.linear(x, layer.weight, bias)
-
-
-class LinearBase(torch.nn.Module):
-    """Base linear layer.
-
-    Args:
-        input_size: input dimension of the linear layer.
-        output_size: output dimension of the linear layer.
-        bias: If true, add bias.
-        skip_bias_add: If true, skip adding bias but instead return it.
-        params_dtype: Data type for the parameters.
-        quant_config: Quantization configure.
-    """
-
-    def __init__(
-        self,
-        input_size: int,
-        output_size: int,
-        skip_bias_add: bool = False,
-        params_dtype: Optional[torch.dtype] = None,
-        quant_config: Optional[QuantizationConfig] = None,
-        prefix: str = "",
-    ):
-        super().__init__()
-
-        # Keep input parameters
-        self.input_size = input_size
-        self.output_size = output_size
-        self.skip_bias_add = skip_bias_add
-        if params_dtype is None:
-            params_dtype = torch.get_default_dtype()
-        self.params_dtype = params_dtype
-        if quant_config is None:
-            self.quant_method: Optional[
-                QuantizeMethodBase] = UnquantizedLinearMethod()
-        else:
-            self.quant_method = quant_config.get_quant_method(self,
-                                                              prefix=prefix)
-
-    def forward(self,
-                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
-        raise NotImplementedError
-
-
-class ReplicatedLinear(LinearBase):
-    """Replicated linear layer.
-
-    Args:
-        input_size: input dimension of the linear layer.
-        output_size: output dimension of the linear layer.
-        bias: If true, add bias.
-        skip_bias_add: If true, skip adding bias but instead return it.
-        params_dtype: Data type for the parameters.
-        quant_config: Quantization configure.
-        prefix: The name of the layer in the state dict, including all parents
-                        (e.g. model.layers.0.qkv_proj)
-    """
-
-    def __init__(self,
-                 input_size: int,
-                 output_size: int,
-                 bias: bool = True,
-                 skip_bias_add: bool = False,
-                 params_dtype: Optional[torch.dtype] = None,
-                 quant_config: Optional[QuantizationConfig] = None,
-                 prefix: str = ""):
-        super().__init__(input_size,
-                         output_size,
-                         skip_bias_add,
-                         params_dtype,
-                         quant_config,
-                         prefix=prefix)
-
-        # All the linear layer supports quant method.
-        assert self.quant_method is not None
-        self.quant_method.create_weights(self,
-                                         self.input_size, [self.output_size],
-                                         self.input_size,
-                                         self.output_size,
-                                         self.params_dtype,
-                                         weight_loader=self.weight_loader)
-
-        if bias:
-            self.bias = Parameter(
-                torch.empty(self.output_size, dtype=self.params_dtype))
-            set_weight_attrs(self.bias, {
-                "output_dim": 0,
-                "weight_loader": self.weight_loader,
-            })
-        else:
-            self.register_parameter("bias", None)
-
-    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
-        # If the weight on disk does not have a shape, give it one
-        # (such scales for AutoFp8).
-        if len(loaded_weight.shape) == 0:
-            loaded_weight = loaded_weight.reshape(1)
-
-        assert param.size() == loaded_weight.size()
-        param.data.copy_(loaded_weight)
-
-    def forward(self,
-                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
-        bias = self.bias if not self.skip_bias_add else None
-        assert self.quant_method is not None
-        output = self.quant_method.apply(self, x, bias)
-        output_bias = self.bias if self.skip_bias_add else None
-        return output, output_bias
-
-    def extra_repr(self) -> str:
-        s = f"in_features={self.input_size}"
-        s += f", output_features={self.output_size}"
-        s += f", bias={self.bias is not None}"
-        return s
-
-
-class ColumnParallelLinear(LinearBase):
-    """Linear layer with column parallelism.
-
-    The linear layer is defined as Y = XA + b. A is parallelized along
-    its second dimension as A = [A_1, ..., A_p].
-
-    Args:
-        input_size: first dimension of matrix A.
-        output_size: second dimension of matrix A.
-        bias: If true, add bias.
-        gather_output: If true, call all-gather on output and make Y available
-                       to all GPUs, otherwise, every GPU will have its output
-                       which is Y_i = XA_i
-        skip_bias_add: This was added to enable performance optimizations where
-                       bias can be fused with other element-wise operations. we
-                       skip adding bias but instead return it.
-        params_dtype: Data type for the parameters.
-        quant_config: Quantization configure.
-        output_sizes: list of output sizes packed into one output, like for QKV
-                       the list would be size 3.
-        prefix: The name of the layer in the state dict, including all parents
-                        (e.g. model.layers.0.qkv_proj) 
-    """
-
-    def __init__(self,
-                 input_size: int,
-                 output_size: int,
-                 bias: bool = True,
-                 gather_output: bool = False,
-                 skip_bias_add: bool = False,
-                 params_dtype: Optional[torch.dtype] = None,
-                 quant_config: Optional[QuantizationConfig] = None,
-                 output_sizes: Optional[list[int]] = None,
-                 prefix: str = ""):
-        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
-                         quant_config, prefix)
-
-        self.gather_output = gather_output
-
-        # Divide the weight matrix along the last dimension.
-        tp_size = get_tensor_model_parallel_world_size()
-        assert self.quant_method is not None
-        self.output_size_per_partition = divide(self.output_size, tp_size)
-        self.output_partition_sizes = [self.output_size_per_partition]
-        # If QKV or MergedColumn, use output size of each partition.
-        if hasattr(self, "output_sizes"):
-            self.output_partition_sizes = [
-                divide(output_size, tp_size)
-                for output_size in self.output_sizes
-            ]
-
-        if output_sizes is None:
-            output_sizes = [output_size]
-
-        self.quant_method.create_weights(
-            layer=self,
-            input_size_per_partition=self.input_size,
-            output_partition_sizes=self.output_partition_sizes,
-            input_size=self.input_size,
-            output_size=self.output_size,
-            params_dtype=self.params_dtype,
-            weight_loader=(
-                self.weight_loader_v2 if self.quant_method.__class__.__name__
-                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
-        if bias:
-            self.bias = Parameter(
-                torch.empty(self.output_size_per_partition,
-                            dtype=params_dtype))
-            set_weight_attrs(self.bias, {
-                "output_dim": 0,
-                "weight_loader": self.weight_loader,
-            })
-        else:
-            self.register_parameter("bias", None)
-
-    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
-        tp_rank = get_tensor_model_parallel_rank()
-        output_dim = getattr(param, "output_dim", None)
-
-        # Special case for GGUF
-        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-        if is_gguf_weight_type:
-            param.weight_type = loaded_weight.item()
-
-        # Materialize GGUF UninitializedParameter
-        if is_gguf_weight and isinstance(param, UninitializedParameter):
-            param.materialize(loaded_weight.shape, dtype=loaded_weight.dtype)
-
-        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
-        is_sharded_weight = getattr(param, "is_sharded_weight", False)
-        # bitsandbytes loads the weights of the specific portion
-        # no need to narrow
-        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-
-        param_data = param.data
-        if output_dim is not None and not is_sharded_weight:
-            shard_size = param_data.shape[output_dim]
-            start_idx = tp_rank * shard_size
-            loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-                                                 shard_size)
-
-        # Special case for loading scales off disk, which often do not
-        # have a shape (such as in the case of AutoFP8).
-        if len(loaded_weight.shape) == 0:
-            loaded_weight = loaded_weight.reshape(1)
-
-        assert param_data.shape == loaded_weight.shape
-        param_data.copy_(loaded_weight)
-
-    def weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor):
-        # Special case for loading scales off disk, which often do not
-        # have a shape (such as in the case of AutoFP8).
-        if len(loaded_weight.shape) == 0:
-            assert loaded_weight.numel() == 1
-            loaded_weight = loaded_weight.reshape(1)
-        param.load_column_parallel_weight(loaded_weight=loaded_weight)
-
-    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
-        bias = self.bias if not self.skip_bias_add else None
-
-        # Matrix multiply.
-        assert self.quant_method is not None
-        output_parallel = self.quant_method.apply(self, input_, bias)
-        if self.gather_output:
-            # All-gather across the partitions.
-            output = tensor_model_parallel_all_gather(output_parallel)
-        else:
-            output = output_parallel
-        output_bias = self.bias if self.skip_bias_add else None
-        return output, output_bias
-
-    def extra_repr(self) -> str:
-        s = f"in_features={self.input_size}"
-        s += f", output_features={self.output_size_per_partition}"
-        s += f", bias={self.bias is not None}"
-        s += f", tp_size={get_tensor_model_parallel_world_size()}"
-        s += f", gather_output={self.gather_output}"
-        return s
-
-
-class MergedColumnParallelLinear(ColumnParallelLinear):
-    """Packed linear layers with column parallelism.
-
-    Similar to ColumnParallelLinear, but the weight matrix is concatenated
-    along the output dimension. When the weight matrix is loaded, the
-    different partitions are sharded separately.
-
-    Args:
-        input_size: input dimension of the linear layer.
-        output_sizes: list of output dimensions of the linear layer.
-        bias: If true, add bias.
-        gather_output: If true, call all-gather on output and make the output
-                       available to all GPUs, otherwise, every GPU will have
-                       its own output.
-        skip_bias_add: This was added to enable performance optimizations where
-                       bias can be fused with other element-wise operations. we
-                       skip adding bias but instead return it.
-        params_dtype: Data type for the parameters.
-        quant_config: Quantization configure.
-        prefix: The name of the layer in the state dict, including all parents
-                        (e.g. model.layers.0.qkv_proj)
-    """
-
-    def __init__(self,
-                 input_size: int,
-                 output_sizes: list[int],
-                 bias: bool = True,
-                 gather_output: bool = False,
-                 skip_bias_add: bool = False,
-                 params_dtype: Optional[torch.dtype] = None,
-                 quant_config: Optional[QuantizationConfig] = None,
-                 prefix: str = ""):
-        self.output_sizes = output_sizes
-        tp_size = get_tensor_model_parallel_world_size()
-        assert all(output_size % tp_size == 0 for output_size in output_sizes)
-        super().__init__(input_size=input_size,
-                         output_size=sum(output_sizes),
-                         bias=bias,
-                         gather_output=gather_output,
-                         skip_bias_add=skip_bias_add,
-                         params_dtype=params_dtype,
-                         quant_config=quant_config,
-                         prefix=prefix)
-
-    def weight_loader(self,
-                      param: Parameter,
-                      loaded_weight: torch.Tensor,
-                      loaded_shard_id: Optional[int] = None):
-
-        # Special case for GGUF
-        # initialize GGUF param after we know the quantize type
-        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-        if is_gguf_weight_type:
-            if loaded_shard_id is not None:
-                param.data[loaded_shard_id].copy_(loaded_weight)
-                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
-            else:
-                param.shard_weight_type = {
-                    i: loaded_weight.item()
-                    for i, _ in enumerate(self.output_sizes)
-                }
-            return
-
-        if is_gguf_weight:
-            tp_size = get_tensor_model_parallel_world_size()
-            tp_rank = get_tensor_model_parallel_rank()
-
-            output_dim = getattr(param, "output_dim", None)
-            shard_size = loaded_weight.size(output_dim) // tp_size
-            start_idx = tp_rank * shard_size
-
-            if loaded_shard_id is not None:
-                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-                                                     shard_size)
-                param.shard_id.append(loaded_shard_id)
-                param.shard_id_map[loaded_shard_id] = len(param.data_container)
-                param.data_container.append(loaded_weight)
-                if len(param.data_container) == 2:
-                    self.qweight = param.materialize_nested()
-                return
-
-        param_data = param.data
-        output_dim = getattr(param, "output_dim", None)
-        # Special case for AQLM codebooks.
-        is_metadata = getattr(param, "is_metadata", False)
-        # Special case for per-tensor scale to load scalar into fused array.
-        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
-
-        if loaded_shard_id is None:
-            # Loaded weight is already fused on disk (mlp).
-            # (e.g., Phi-3's gate_up_proj).
-            if output_dim is None:
-                if needs_scalar_to_array:
-                    param_data, loaded_weight = adjust_scalar_to_fused_array(
-                        param_data, loaded_weight, 0)
-
-                assert param_data.shape == loaded_weight.shape
-                param_data.copy_(loaded_weight)
-                return
-            current_shard_offset = 0
-            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-                                            False)
-            shard_offsets: list[tuple[int, int, int]] = []
-            for i, output_size in enumerate(self.output_sizes):
-                shard_offsets.append((i, current_shard_offset, output_size))
-                current_shard_offset += output_size
-            packed_dim = getattr(param, "packed_dim", None)
-            for shard_id, shard_offset, shard_size in shard_offsets:
-                # Special case for Quantization.
-                # If quantized, we need to adjust the offset and size to account
-                # for the packing.
-                if packed_dim == output_dim:
-                    shard_size = shard_size // param.pack_factor
-                    shard_offset = shard_offset // param.pack_factor
-                    # Special case for Marlin.
-                    shard_size, shard_offset = adjust_marlin_shard(
-                        param, shard_size, shard_offset)
-
-                if use_bitsandbytes_4bit:
-                    index = list(itertools.accumulate([0] + self.output_sizes))
-                    orig_offsets = {
-                        str(i): (index[i], size)
-                        for i, size in enumerate(self.output_sizes)
-                    }
-                    orig_offsets["total"] = (self.output_size, 0)
-                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
-                        param, orig_offsets, str(shard_id))
-
-                loaded_weight_shard = loaded_weight.narrow(
-                    output_dim, shard_offset, shard_size)
-                self.weight_loader(param, loaded_weight_shard, shard_id)
-            return
-
-        assert loaded_shard_id < len(self.output_sizes)
-        tp_rank = get_tensor_model_parallel_rank()
-        tp_size = get_tensor_model_parallel_world_size()
-        if output_dim is not None:
-            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
-            shard_size = self.output_sizes[loaded_shard_id] // tp_size
-            # Special case for quantization.
-            # If quantized, we need to adjust the offset and size to account
-            # for the packing.
-            packed_dim = getattr(param, "packed_dim", None)
-            if packed_dim == output_dim:
-                shard_size = shard_size // param.pack_factor
-                shard_offset = shard_offset // param.pack_factor
-                # Special case for Marlin.
-                shard_size, shard_offset = adjust_marlin_shard(
-                    param, shard_size, shard_offset)
-
-            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-                                            False)
-            is_sharded_weight = getattr(param, "is_sharded_weight", False)
-            # bitsandbytes loads the weights of the specific portion
-            # no need to narrow
-            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-
-            if use_bitsandbytes_4bit:
-                shard_size = loaded_weight.shape[output_dim]
-                shard_offset = loaded_weight.shape[output_dim] * \
-                    loaded_shard_id
-
-            param_data = param_data.narrow(output_dim, shard_offset,
-                                           shard_size)
-            start_idx = tp_rank * shard_size
-            if not is_sharded_weight:
-                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-                                                     shard_size)
-        # Special case for AQLM codebooks.
-        elif is_metadata:
-            # metadata indicates fixed size concatenated along dim 0
-            shard_size = loaded_weight.shape[0]
-            shard_offset = loaded_shard_id * shard_size
-            param_data = param_data.narrow(0, shard_offset, shard_size)
-
-        # Special case for per-tensor scales in fused case.
-        elif needs_scalar_to_array:
-            param_data, loaded_weight = adjust_scalar_to_fused_array(
-                param_data, loaded_weight, loaded_shard_id)
-
-        else:
-            ignore_warning = getattr(param, "ignore_warning", False)
-            if not ignore_warning:
-                logger.warning(
-                    "Loading a weight without `output_dim` attribute in "
-                    "MergedColumnParallelLinear, assume the weight is "
-                    "the same for all partitions.")
-
-        assert param_data.shape == loaded_weight.shape
-        param_data.copy_(loaded_weight)
-
-    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
-                                           loaded_weight: torch.Tensor):
-        """
-        Handle special case for models where MLP layers are already
-        fused on disk. In this case, we have no shard id. This function
-        determmines the shard id by splitting these layers and then calls
-        the weight loader using the shard id.
-
-        An example of a model with these fused layers:
-        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
-        """
-
-        current_shard_offset = 0
-        shard_offsets: list[tuple[int, int, int]] = []
-        for i, output_size in enumerate(self.output_sizes):
-            shard_offsets.append((i, current_shard_offset, output_size))
-            current_shard_offset += output_size
-
-        for shard_id, shard_offset, shard_size in shard_offsets:
-            # Special case for Quantization.
-            # If quantized, we need to adjust the offset and size to account
-            # for the packing.
-            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
-                                  )) and param.packed_dim == param.output_dim:
-                shard_size, shard_offset = \
-                    param.adjust_shard_indexes_for_packing(
-                    shard_size=shard_size, shard_offset=shard_offset)
-
-            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
-                                                       shard_offset,
-                                                       shard_size)
-            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
-
-    def weight_loader_v2(self,
-                         param: BasevLLMParameter,
-                         loaded_weight: torch.Tensor,
-                         loaded_shard_id: Optional[int] = None):
-        if loaded_shard_id is None:
-            if isinstance(param, PerTensorScaleParameter):
-                param.load_merged_column_weight(loaded_weight=loaded_weight,
-                                                shard_id=0)
-                return
-            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
-                param.load_merged_column_weight(loaded_weight=loaded_weight)
-                return
-            # TODO: @dsikka - move to parameter.py
-            self._load_fused_module_from_checkpoint(param, loaded_weight)
-            return
-
-        assert loaded_shard_id < len(self.output_sizes)
-
-        tp_size = get_tensor_model_parallel_world_size()
-
-        if isinstance(param, BlockQuantScaleParameter):
-            from vllm.model_executor.layers.quantization.fp8 import (
-                Fp8LinearMethod, Fp8MoEMethod)
-            assert self.quant_method is not None
-            assert isinstance(self.quant_method,
-                              (Fp8LinearMethod, Fp8MoEMethod))
-            weight_block_size = self.quant_method.quant_config.weight_block_size
-            assert weight_block_size is not None
-            block_n, _ = weight_block_size[0], weight_block_size[1]
-            shard_offset = (
-                (sum(self.output_sizes[:loaded_shard_id]) + block_n - 1) //
-                block_n) // tp_size
-            shard_size = ((self.output_sizes[loaded_shard_id] + block_n - 1) //
-                          block_n // tp_size)
-        else:
-            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
-            shard_size = self.output_sizes[loaded_shard_id] // tp_size
-
-        param.load_merged_column_weight(loaded_weight=loaded_weight,
-                                        shard_id=loaded_shard_id,
-                                        shard_offset=shard_offset,
-                                        shard_size=shard_size)
-
-
-class QKVParallelLinear(ColumnParallelLinear):
-    """Linear layers for the attention's QKV transformation.
-
-    Linear layers for the linear transformation of the query, key, and value
-    vectors in the attention layer. The weight matrix is concatenated along
-    the output dimension. The layer is parallelized along the head dimension.
-    When the number of key/value heads is smaller than the number of query
-    heads (e.g., multi-query/grouped-query attention), the key/value head may
-    be replicated while the query heads are partitioned.
-
-    Args:
-        hidden_size: input hidden state size of the transformer.
-        head_size: size of each attention head.
-        total_num_heads: total number of attention query heads.
-        total_num_kv_heads: total number of attention key/value heads. If
-                            None, assume total_num_kv_heads = total_num_heads.
-        bias: If true, add bias.
-        skip_bias_add: This was added to enable performance optimizations where
-                       bias can be fused with other element-wise operations. we
-                       skip adding bias but instead return it.
-        params_dtype: Data type for the parameters.
-        quant_config: Quantization configure.
-        prefix: The name of the layer in the state dict, including all parents
-                        (e.g. model.layers.0.qkv_proj)
-    """
-
-    def __init__(self,
-                 hidden_size: int,
-                 head_size: int,
-                 total_num_heads: int,
-                 total_num_kv_heads: Optional[int] = None,
-                 bias: bool = True,
-                 skip_bias_add: bool = False,
-                 params_dtype: Optional[torch.dtype] = None,
-                 quant_config: Optional[QuantizationConfig] = None,
-                 prefix: str = ""):
-        self.hidden_size = hidden_size
-        self.head_size = head_size
-        self.total_num_heads = total_num_heads
-        if total_num_kv_heads is None:
-            total_num_kv_heads = total_num_heads
-        self.total_num_kv_heads = total_num_kv_heads
-        # Divide the weight matrix along the last dimension.
-        tp_size = get_tensor_model_parallel_world_size()
-        self.num_heads = divide(self.total_num_heads, tp_size)
-        if tp_size >= self.total_num_kv_heads:
-            self.num_kv_heads = 1
-            self.num_kv_head_replicas = divide(tp_size,
-                                               self.total_num_kv_heads)
-        else:
-            self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)
-            self.num_kv_head_replicas = 1
-        input_size = self.hidden_size
-        output_size = (self.num_heads +
-                       2 * self.num_kv_heads) * tp_size * self.head_size
-        self.output_sizes = [
-            self.num_heads * self.head_size * tp_size,  # q_proj
-            self.num_kv_heads * self.head_size * tp_size,  # k_proj
-            self.num_kv_heads * self.head_size * tp_size,  # v_proj 
-        ]
-
-        super().__init__(input_size=input_size,
-                         output_size=output_size,
-                         bias=bias,
-                         gather_output=False,
-                         skip_bias_add=skip_bias_add,
-                         params_dtype=params_dtype,
-                         quant_config=quant_config,
-                         prefix=prefix)
-
-    def _get_shard_offset_mapping(self, loaded_shard_id: str):
-        shard_offset_mapping = {
-            "q": 0,
-            "k": self.num_heads * self.head_size,
-            "v": (self.num_heads + self.num_kv_heads) * self.head_size,
-            "total": (self.num_heads + 2 * self.num_kv_heads) * self.head_size
-        }
-        return shard_offset_mapping.get(loaded_shard_id)
-
-    def _get_shard_size_mapping(self, loaded_shard_id: str):
-        shard_size_mapping = {
-            "q": self.num_heads * self.head_size,
-            "k": self.num_kv_heads * self.head_size,
-            "v": self.num_kv_heads * self.head_size,
-        }
-        return shard_size_mapping.get(loaded_shard_id)
-
-    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
-                                           loaded_weight: torch.Tensor):
-        """
-        Handle special case for models where QKV layers are already 
-        fused on disk. In this case, we have no shard id. This function
-        determmines the shard id by splitting these layers and then calls
-        the weight loader using the shard id.
-
-        An example of a model with these fused layers:
-        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
-        """
-        shard_offsets = [
-            # (shard_id, shard_offset, shard_size)
-            ("q", 0, self.total_num_heads * self.head_size),
-            ("k", self.total_num_heads * self.head_size,
-             self.total_num_kv_heads * self.head_size),
-            ("v",
-             (self.total_num_heads + self.total_num_kv_heads) * self.head_size,
-             self.total_num_kv_heads * self.head_size),
-        ]
-
-        for shard_id, shard_offset, shard_size in shard_offsets:
-            # Special case for Quantization.
-            # If quantized, we need to adjust the offset and size to account
-            # for the packing.
-            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
-                                  )) and param.packed_dim == param.output_dim:
-                shard_size, shard_offset = \
-                    param.adjust_shard_indexes_for_packing(
-                    shard_size=shard_size, shard_offset=shard_offset)
-
-            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
-                                                       shard_offset,
-                                                       shard_size)
-            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
-
-    def weight_loader_v2(self,
-                         param: BasevLLMParameter,
-                         loaded_weight: torch.Tensor,
-                         loaded_shard_id: Optional[str] = None):
-        if loaded_shard_id is None:  # special case for certain models
-            if isinstance(param, PerTensorScaleParameter):
-                param.load_qkv_weight(loaded_weight=loaded_weight, shard_id=0)
-                return
-            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
-                param.load_qkv_weight(loaded_weight=loaded_weight)
-                return
-            # TODO: @dsikka - move to parameter.py
-            self._load_fused_module_from_checkpoint(param, loaded_weight)
-            return
-
-        assert loaded_shard_id in ["q", "k", "v"]
-
-        shard_offset = self._get_shard_offset_mapping(loaded_shard_id)
-        shard_size = self._get_shard_size_mapping(loaded_shard_id)
-
-        param.load_qkv_weight(loaded_weight=loaded_weight,
-                              num_heads=self.num_kv_head_replicas,
-                              shard_id=loaded_shard_id,
-                              shard_offset=shard_offset,
-                              shard_size=shard_size)
-
-    def weight_loader(self,
-                      param: Parameter,
-                      loaded_weight: torch.Tensor,
-                      loaded_shard_id: Optional[str] = None):
-
-        # Special case for GGUF
-        # initialize GGUF param after we know the quantize type
-        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-        if is_gguf_weight_type:
-            idx_map = {"q": 0, "k": 1, "v": 2}
-            if loaded_shard_id is not None:
-                param.data[idx_map[loaded_shard_id]].copy_(loaded_weight)
-                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
-            else:
-                param.shard_weight_type = {
-                    k: loaded_weight.item()
-                    for k in idx_map
-                }
-            return
-
-        if is_gguf_weight:
-            tp_size = get_tensor_model_parallel_world_size()
-            tp_rank = get_tensor_model_parallel_rank()
-
-            output_dim = getattr(param, "output_dim", None)
-            shard_size = loaded_weight.size(output_dim) // tp_size
-            start_idx = tp_rank * shard_size
-
-            if loaded_shard_id is not None:
-                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-                                                     shard_size)
-                param.shard_id.append(loaded_shard_id)
-                param.shard_id_map[loaded_shard_id] = len(param.data_container)
-                param.data_container.append(loaded_weight)
-                if len(param.data_container) == 3:
-                    self.qweight = param.materialize_nested()
-                return
-
-        param_data = param.data
-        output_dim = getattr(param, "output_dim", None)
-        # Special case for AQLM codebooks.
-        is_metadata = getattr(param, "is_metadata", False)
-
-        # Special case for per-tensor scales in fused case.
-        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
-
-        if loaded_shard_id is None:
-            # Loaded weight is already fused on disk (qkv).
-            # (e.g., Phi-3's qkv_proj).
-            if output_dim is None:
-                if needs_scalar_to_array:
-                    param_data, loaded_weight = adjust_scalar_to_fused_array(
-                        param_data, loaded_weight, 0)
-
-                assert param_data.shape == loaded_weight.shape
-                param_data.copy_(loaded_weight)
-                return
-            shard_offsets = [
-                # (shard_id, shard_offset, shard_size)
-                ("q", 0, self.total_num_heads * self.head_size),
-                ("k", self.total_num_heads * self.head_size,
-                 self.total_num_kv_heads * self.head_size),
-                ("v", (self.total_num_heads + self.total_num_kv_heads) *
-                 self.head_size, self.total_num_kv_heads * self.head_size),
-            ]
-            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-                                            False)
-
-            packed_dim = getattr(param, "packed_dim", None)
-            for shard_id, shard_offset, shard_size in shard_offsets:
-                # Special case for Quantized Weights.
-                # If quantized, we need to adjust the offset and size to account
-                # for the packing.
-                if packed_dim == output_dim:
-                    shard_size = shard_size // param.pack_factor
-                    shard_offset = shard_offset // param.pack_factor
-
-                    # Special case for Marlin.
-                    shard_size, shard_offset = adjust_marlin_shard(
-                        param, shard_size, shard_offset)
-
-                if use_bitsandbytes_4bit:
-                    orig_qkv_offsets = {
-                        "q": (0, self.total_num_heads * self.head_size),
-                        "k": (self.total_num_heads * self.head_size,
-                              self.total_num_kv_heads * self.head_size),
-                        "v":
-                        ((self.total_num_heads + self.total_num_kv_heads) *
-                         self.head_size,
-                         self.total_num_kv_heads * self.head_size),
-                        "total":
-                        ((self.total_num_heads + 2 * self.total_num_kv_heads) *
-                         self.head_size, 0)
-                    }
-
-                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
-                        param, orig_qkv_offsets, shard_id)
-
-                loaded_weight_shard = loaded_weight.narrow(
-                    output_dim, shard_offset, shard_size)
-                self.weight_loader(param, loaded_weight_shard, shard_id)
-            return
-
-        tp_rank = get_tensor_model_parallel_rank()
-        assert loaded_shard_id in ["q", "k", "v"]
-
-        # If output dim is defined, use the default loading process.
-        if output_dim is not None:
-            if loaded_shard_id == "q":
-                shard_offset = 0
-                shard_size = self.num_heads * self.head_size
-            elif loaded_shard_id == "k":
-                shard_offset = self.num_heads * self.head_size
-                shard_size = self.num_kv_heads * self.head_size
-            elif loaded_shard_id == "v":
-                shard_offset = (self.num_heads +
-                                self.num_kv_heads) * self.head_size
-                shard_size = self.num_kv_heads * self.head_size
-            # Special case for Quantized Weights.
-            # If quantized, we need to adjust the offset and size to account
-            # for the packing.
-            packed_dim = getattr(param, "packed_dim", None)
-            if packed_dim == output_dim:
-                shard_size = shard_size // param.pack_factor
-                shard_offset = shard_offset // param.pack_factor
-
-                # Special case for Marlin.
-                shard_size, shard_offset = adjust_marlin_shard(
-                    param, shard_size, shard_offset)
-
-            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
-                                            False)
-            is_sharded_weight = getattr(param, "is_sharded_weight", False)
-            # bitsandbytes loads the weights of the specific portion
-            # no need to narrow
-            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-
-            if use_bitsandbytes_4bit:
-                orig_qkv_offsets = {
-                    "q": (0, self.num_heads * self.head_size),
-                    "k": (self.num_heads * self.head_size,
-                          self.num_kv_heads * self.head_size),
-                    "v":
-                    ((self.num_heads + self.num_kv_heads) * self.head_size,
-                     self.num_kv_heads * self.head_size),
-                    "total":
-                    ((self.num_heads + 2 * self.num_kv_heads) * self.head_size,
-                     0)
-                }
-                shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
-                    param, orig_qkv_offsets, loaded_shard_id)
-
-            param_data = param_data.narrow(output_dim, shard_offset,
-                                           shard_size)
-            if loaded_shard_id == "q":
-                shard_id = tp_rank
-            else:
-                shard_id = tp_rank // self.num_kv_head_replicas
-            start_idx = shard_id * shard_size
-
-            if not is_sharded_weight:
-                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
-                                                     shard_size)
-
-        # Special case for for AQLM codebooks.
-        elif is_metadata:
-            # metadata indicates fixed size concatenated along dim 0
-            shard_size = loaded_weight.shape[0]
-            shard_index = ["q", "k", "v"].index(loaded_shard_id)
-            param_data = param_data.narrow(0, shard_index * shard_size,
-                                           shard_size)
-        # Special case for per-tensor scales in fused case.
-        elif needs_scalar_to_array:
-            param_data, loaded_weight = adjust_scalar_to_fused_array(
-                param_data, loaded_weight, loaded_shard_id)
-        else:
-            ignore_warning = getattr(param, "ignore_warning", False)
-            if not ignore_warning:
-                logger.warning(
-                    "Loading a weight without `output_dim` attribute in "
-                    "QKVParallelLinear, assume the weight is the same "
-                    "for all partitions.")
-
-        assert param_data.shape == loaded_weight.shape
-        param_data.copy_(loaded_weight)
-
-
-class RowParallelLinear(LinearBase):
-    """Linear layer with row parallelism.
-
-    The linear layer is defined as Y = XA + b. A is parallelized along
-    its first dimension and X along its second dimension as:
-               -   -
-              | A_1 |
-              | .   |
-          A = | .   |        X = [X_1, ..., X_p]
-              | .   |
-              | A_p |
-               -   -
-    Arguments:
-        input_size: first dimension of matrix A.
-        output_size: second dimension of matrix A.
-        bias: If true, add bias. Note that bias is not parallelized.
-        input_is_parallel: If true, we assume that the input is already
-                           split across the GPUs and we do not split
-                           again.
-        skip_bias_add: This was added to enable performance optimization where
-                       bias can be fused with other element-wise operations.
-                       We skip adding bias but instead return it.
-        params_dtype: Data type for the parameters.
-        quant_config: Quantization configure.
-    """
-
-    def __init__(self,
-                 input_size: int,
-                 output_size: int,
-                 bias: bool = True,
-                 input_is_parallel: bool = True,
-                 skip_bias_add: bool = False,
-                 params_dtype: Optional[torch.dtype] = None,
-                 reduce_results: bool = True,
-                 quant_config: Optional[QuantizationConfig] = None,
-                 prefix: str = ""):
-        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
-                         quant_config, prefix)
-
-        self.input_is_parallel = input_is_parallel
-        self.reduce_results = reduce_results
-
-        # Divide the weight matrix along the last dimension.
-        self.tp_rank = get_tensor_model_parallel_rank()
-        self.tp_size = get_tensor_model_parallel_world_size()
-        self.input_size_per_partition = divide(input_size, self.tp_size)
-        assert self.quant_method is not None
-
-        self.quant_method.create_weights(
-            layer=self,
-            input_size_per_partition=self.input_size_per_partition,
-            output_partition_sizes=[self.output_size],
-            input_size=self.input_size,
-            output_size=self.output_size,
-            params_dtype=self.params_dtype,
-            weight_loader=(
-                self.weight_loader_v2 if self.quant_method.__class__.__name__
-                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
-        if not reduce_results and (bias and not skip_bias_add):
-            raise ValueError("When not reduce the results, adding bias to the "
-                             "results can lead to incorrect results")
-
-        if bias:
-            self.bias = Parameter(
-                torch.empty(self.output_size, dtype=params_dtype))
-            set_weight_attrs(self.bias, {
-                "output_dim": 0,
-                "weight_loader": self.weight_loader,
-            })
-        else:
-            self.register_parameter("bias", None)
-
-    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
-        tp_rank = get_tensor_model_parallel_rank()
-        tp_size = get_tensor_model_parallel_world_size()
-        input_dim = getattr(param, "input_dim", None)
-        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
-        is_sharded_weight = getattr(param, "is_sharded_weight", False)
-        # bitsandbytes loads the weights of the specific portion
-        # no need to narrow
-        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
-
-        # Special case for GGUF
-        is_gguf_weight = getattr(param, "is_gguf_weight", False)
-        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
-        if is_gguf_weight_type:
-            param.weight_type = loaded_weight.item()
-
-        # Materialize GGUF UninitializedParameter
-        if is_gguf_weight and isinstance(param, UninitializedParameter):
-            weight_shape = list(loaded_weight.shape)
-            if input_dim:
-                weight_shape[input_dim] = weight_shape[input_dim] // tp_size
-            param.materialize(tuple(weight_shape), dtype=loaded_weight.dtype)
-
-        param_data = param.data
-        if input_dim is not None and not is_sharded_weight:
-            shard_size = param_data.shape[input_dim]
-            start_idx = tp_rank * shard_size
-            loaded_weight = loaded_weight.narrow(input_dim, start_idx,
-                                                 shard_size)
-
-        # Special case for loading scales off disk, which often do not
-        # have a shape (such as in the case of AutoFP8).
-        if len(loaded_weight.shape) == 0:
-            loaded_weight = loaded_weight.reshape(1)
-
-        assert param_data.shape == loaded_weight.shape
-        param_data.copy_(loaded_weight)
-
-    def weight_loader_v2(self, param: BasevLLMParameter,
-                         loaded_weight: torch.Tensor):
-
-        # Special case for loading scales off disk, which often do not
-        # have a shape (such as in the case of AutoFP8).
-        if len(loaded_weight.shape) == 0:
-            assert loaded_weight.numel() == 1
-            loaded_weight = loaded_weight.reshape(1)
-
-        param.load_row_parallel_weight(loaded_weight=loaded_weight)
-
-    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
-        if self.input_is_parallel:
-            input_parallel = input_
-        else:
-            tp_rank = get_tensor_model_parallel_rank()
-            splitted_input = split_tensor_along_last_dim(
-                input_, num_partitions=self.tp_size)
-            input_parallel = splitted_input[tp_rank].contiguous()
-
-        # Matrix multiply.
-        assert self.quant_method is not None
-        # Only fuse bias add into GEMM for rank 0 (this ensures that
-        # bias will not get added more than once in TP>1 case)
-        bias_ = None if (self.tp_rank > 0 or self.skip_bias_add) else self.bias
-        output_parallel = self.quant_method.apply(self,
-                                                  input_parallel,
-                                                  bias=bias_)
-        if self.reduce_results and self.tp_size > 1:
-            output = tensor_model_parallel_all_reduce(output_parallel)
-        else:
-            output = output_parallel
-
-        output_bias = self.bias if self.skip_bias_add else None
-
-        return output, output_bias
-
-    def extra_repr(self) -> str:
-        s = f"input_features={self.input_size_per_partition}"
-        s += f", output_features={self.output_size}"
-        s += f", bias={self.bias is not None}"
-        s += f", tp_size={self.tp_size}"
-        s += f", reduce_results={self.reduce_results}"
-        return s
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# SPDX-License-Identifier: Apache-2.0
+
+import itertools
+from abc import abstractmethod
+from typing import Optional
+
+import torch
+import torch.nn.functional as F
+from torch.nn.parameter import Parameter, UninitializedParameter
+
+from vllm.distributed import (divide, get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              split_tensor_along_last_dim,
+                              tensor_model_parallel_all_gather,
+                              tensor_model_parallel_all_reduce)
+from vllm.logger import init_logger
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig, QuantizeMethodBase)
+# yapf: disable
+from vllm.model_executor.parameter import (BasevLLMParameter,
+                                           BlockQuantScaleParameter,
+                                           PackedColumnParameter,
+                                           PackedvLLMParameter,
+                                           PerTensorScaleParameter,
+                                           RowvLLMParameter)
+# yapf: enable
+from vllm.model_executor.utils import set_weight_attrs
+
+logger = init_logger(__name__)
+
+WEIGHT_LOADER_V2_SUPPORTED = [
+    "CompressedTensorsLinearMethod", "AWQMarlinLinearMethod",
+    "AWQLinearMethod", "GPTQMarlinLinearMethod", "Fp8LinearMethod",
+    "MarlinLinearMethod", "QQQLinearMethod", "GPTQMarlin24LinearMethod",
+    "TPUInt8LinearMethod", "GPTQLinearMethod", "FBGEMMFp8LinearMethod",
+    "ModelOptFp8LinearMethod", "IPEXAWQLinearMethod", "IPEXGPTQLinearMethod",
+    "HQQMarlinMethod", "QuarkLinearMethod"
+]
+
+
+def adjust_marlin_shard(param, shard_size, shard_offset):
+    marlin_tile_size = getattr(param, "marlin_tile_size", None)
+    if marlin_tile_size is None:
+        return shard_size, shard_offset
+
+    return shard_size * marlin_tile_size, shard_offset * marlin_tile_size
+
+
+def adjust_bitsandbytes_4bit_shard(param: Parameter,
+                                   shard_offsets: dict[str, tuple[int, int]],
+                                   loaded_shard_id: str) -> tuple[int, int]:
+    """Adjust the quantization offsets and sizes for BitsAndBytes sharding."""
+
+    total, _ = shard_offsets["total"]
+    orig_offset, orig_size = shard_offsets[loaded_shard_id]
+
+    quantized_total = param.data.shape[0]
+    quantized_offset = orig_offset * quantized_total // total
+    quantized_size = orig_size * quantized_total // total
+
+    return quantized_size, quantized_offset
+
+
+def adjust_scalar_to_fused_array(param, loaded_weight, shard_id):
+    """For fused modules (QKV and MLP) we have an array of length
+    N that holds 1 scale for each "logical" matrix. So the param
+    is an array of length N. The loaded_weight corresponds to 
+    one of the shards on disk. Here, we slice the param based on 
+    the shard_id for loading.
+    """
+    qkv_idxs = {"q": 0, "k": 1, "v": 2}
+
+    if isinstance(shard_id, str):
+        shard_id = qkv_idxs[shard_id]
+    elif not isinstance(shard_id, int):
+        raise ValueError(f"Unknown Shard Id {shard_id}")
+
+    # AutoFP8 scales do not have a shape
+    # compressed-tensors scales do have a shape
+    if len(loaded_weight.shape) != 0:
+        assert loaded_weight.shape[0] == 1
+        loaded_weight = loaded_weight[0]
+
+    return param[shard_id], loaded_weight.t()
+    #return param[shard_id], loaded_weight
+
+
+class LinearMethodBase(QuantizeMethodBase):
+    """Base class for different (maybe quantized) linear methods."""
+
+    @abstractmethod
+    def create_weights(self, layer: torch.nn.Module,
+                       input_size_per_partition: int,
+                       output_partition_sizes: list[int], input_size: int,
+                       output_size: int, params_dtype: torch.dtype,
+                       **extra_weight_attrs):
+        """Create weights for a linear layer. 
+           The weights will be set as attributes of the layer.
+
+        Args:
+            layer: The layer that is using the LinearMethodBase factory.
+            input_size_per_partition: Size of the weight input dim on rank X.
+            output_partition_sizes: Sizes of the output dim of each logical 
+                weight on rank X. E.g., output_partition_sizes for QKVLinear
+                is a list contains the width of Wq, Wk, Wv on rank X.
+            input_size: Size of the input dim of the weight across all ranks.
+            output_size: Size of the output dim of the weight across all ranks.
+            params_dtype: Datatype of the parameters.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def apply(self,
+              layer: torch.nn.Module,
+              x: torch.Tensor,
+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        """Apply the weights in layer to the input tensor.
+        Expects create_weights to have been called before on the layer."""
+        raise NotImplementedError
+
+
+class UnquantizedLinearMethod(LinearMethodBase):
+    """Linear method without quantization."""
+
+    def create_weights(self, layer: torch.nn.Module,
+                       input_size_per_partition: int,
+                       output_partition_sizes: list[int], input_size: int,
+                       output_size: int, params_dtype: torch.dtype,
+                       **extra_weight_attrs):
+        weight = Parameter(torch.empty(input_size_per_partition,
+                                       sum(output_partition_sizes),
+                                       dtype=params_dtype),
+                           requires_grad=False)
+        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
+        layer.register_parameter("weight", weight)
+        set_weight_attrs(weight, extra_weight_attrs)
+
+    def apply(self,
+              layer: torch.nn.Module,
+              x: torch.Tensor,
+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+
+        #return F.linear(x, layer.weight, bias)
+        if x.shape[-1]==layer.weight.shape[0]:
+            return F.linear(x, layer.weight.t(), bias)
+        else:
+            return F.linear(x, layer.weight, bias)
+
+
+class LinearBase(torch.nn.Module):
+    """Base linear layer.
+
+    Args:
+        input_size: input dimension of the linear layer.
+        output_size: output dimension of the linear layer.
+        bias: If true, add bias.
+        skip_bias_add: If true, skip adding bias but instead return it.
+        params_dtype: Data type for the parameters.
+        quant_config: Quantization configure.
+    """
+
+    def __init__(
+        self,
+        input_size: int,
+        output_size: int,
+        skip_bias_add: bool = False,
+        params_dtype: Optional[torch.dtype] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+
+        # Keep input parameters
+        self.input_size = input_size
+        self.output_size = output_size
+        self.skip_bias_add = skip_bias_add
+        if params_dtype is None:
+            params_dtype = torch.get_default_dtype()
+        self.params_dtype = params_dtype
+        if quant_config is None:
+            self.quant_method: Optional[
+                QuantizeMethodBase] = UnquantizedLinearMethod()
+        else:
+            self.quant_method = quant_config.get_quant_method(self,
+                                                              prefix=prefix)
+
+    def forward(self,
+                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
+        raise NotImplementedError
+
+
+class ReplicatedLinear(LinearBase):
+    """Replicated linear layer.
+
+    Args:
+        input_size: input dimension of the linear layer.
+        output_size: output dimension of the linear layer.
+        bias: If true, add bias.
+        skip_bias_add: If true, skip adding bias but instead return it.
+        params_dtype: Data type for the parameters.
+        quant_config: Quantization configure.
+        prefix: The name of the layer in the state dict, including all parents
+                        (e.g. model.layers.0.qkv_proj)
+    """
+
+    def __init__(self,
+                 input_size: int,
+                 output_size: int,
+                 bias: bool = True,
+                 skip_bias_add: bool = False,
+                 params_dtype: Optional[torch.dtype] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = ""):
+        super().__init__(input_size,
+                         output_size,
+                         skip_bias_add,
+                         params_dtype,
+                         quant_config,
+                         prefix=prefix)
+
+        # All the linear layer supports quant method.
+        assert self.quant_method is not None
+        self.quant_method.create_weights(self,
+                                         self.input_size, [self.output_size],
+                                         self.input_size,
+                                         self.output_size,
+                                         self.params_dtype,
+                                         weight_loader=self.weight_loader)
+
+        if bias:
+            self.bias = Parameter(
+                torch.empty(self.output_size, dtype=self.params_dtype))
+            set_weight_attrs(self.bias, {
+                "output_dim": 0,
+                "weight_loader": self.weight_loader,
+            })
+        else:
+            self.register_parameter("bias", None)
+
+    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
+        # If the weight on disk does not have a shape, give it one
+        # (such scales for AutoFp8).
+        if len(loaded_weight.shape) == 0:
+            loaded_weight = loaded_weight.reshape(1)
+
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
+
+        assert param.size() == loaded_weight.size()
+        param.data.copy_(loaded_weight)
+
+    def forward(self,
+                x: torch.Tensor) -> tuple[torch.Tensor, Optional[Parameter]]:
+        bias = self.bias if not self.skip_bias_add else None
+        assert self.quant_method is not None
+        output = self.quant_method.apply(self, x, bias)
+        output_bias = self.bias if self.skip_bias_add else None
+        return output, output_bias
+
+    def extra_repr(self) -> str:
+        s = f"in_features={self.input_size}"
+        s += f", output_features={self.output_size}"
+        s += f", bias={self.bias is not None}"
+        return s
+
+
+class ColumnParallelLinear(LinearBase):
+    """Linear layer with column parallelism.
+
+    The linear layer is defined as Y = XA + b. A is parallelized along
+    its second dimension as A = [A_1, ..., A_p].
+
+    Args:
+        input_size: first dimension of matrix A.
+        output_size: second dimension of matrix A.
+        bias: If true, add bias.
+        gather_output: If true, call all-gather on output and make Y available
+                       to all GPUs, otherwise, every GPU will have its output
+                       which is Y_i = XA_i
+        skip_bias_add: This was added to enable performance optimizations where
+                       bias can be fused with other element-wise operations. we
+                       skip adding bias but instead return it.
+        params_dtype: Data type for the parameters.
+        quant_config: Quantization configure.
+        output_sizes: list of output sizes packed into one output, like for QKV
+                       the list would be size 3.
+        prefix: The name of the layer in the state dict, including all parents
+                        (e.g. model.layers.0.qkv_proj) 
+    """
+
+    def __init__(self,
+                 input_size: int,
+                 output_size: int,
+                 bias: bool = True,
+                 gather_output: bool = False,
+                 skip_bias_add: bool = False,
+                 params_dtype: Optional[torch.dtype] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 output_sizes: Optional[list[int]] = None,
+                 prefix: str = ""):
+        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
+                         quant_config, prefix)
+
+        self.gather_output = gather_output
+
+        # Divide the weight matrix along the last dimension.
+        tp_size = get_tensor_model_parallel_world_size()
+        self.input_size_per_partition = self.input_size
+        assert self.quant_method is not None
+        self.output_size_per_partition = divide(self.output_size, tp_size)
+        self.output_partition_sizes = [self.output_size_per_partition]
+        # If QKV or MergedColumn, use output size of each partition.
+        if hasattr(self, "output_sizes"):
+            self.output_partition_sizes = [
+                divide(output_size, tp_size)
+                for output_size in self.output_sizes
+            ]
+
+        if output_sizes is None:
+            output_sizes = [output_size]
+
+        self.quant_method.create_weights(
+            layer=self,
+            input_size_per_partition=self.input_size,
+            output_partition_sizes=self.output_partition_sizes,
+            input_size=self.input_size,
+            output_size=self.output_size,
+            params_dtype=self.params_dtype,
+            weight_loader=(
+                self.weight_loader_v2 if self.quant_method.__class__.__name__
+                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
+        if bias:
+            self.bias = Parameter(
+                torch.empty(self.output_size_per_partition,
+                            dtype=params_dtype))
+            set_weight_attrs(self.bias, {
+                "output_dim": 0,
+                "weight_loader": self.weight_loader,
+            })
+        else:
+            self.register_parameter("bias", None)
+
+    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
+        tp_rank = get_tensor_model_parallel_rank()
+        output_dim = getattr(param, "output_dim", None)
+
+        # Special case for GGUF
+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
+        if is_gguf_weight_type:
+            param.weight_type = loaded_weight.item()
+
+        # Materialize GGUF UninitializedParameter
+        if is_gguf_weight and isinstance(param, UninitializedParameter):
+            param.materialize(loaded_weight.shape, dtype=loaded_weight.dtype)
+
+        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
+        is_sharded_weight = getattr(param, "is_sharded_weight", False)
+        # bitsandbytes loads the weights of the specific portion
+        # no need to narrow
+        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+
+        param_data = param.data
+        if output_dim is not None and not is_sharded_weight:
+            #shard_size = param_data.shape[output_dim]
+            shard_size = param_data.shape[output_dim] if len(param_data.shape)==1 or is_quantization else param_data.shape[int(not(output_dim))]
+            start_idx = tp_rank * shard_size
+            loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+                                                 shard_size)
+
+        # Special case for loading scales off disk, which often do not
+        # have a shape (such as in the case of AutoFP8).
+        if len(loaded_weight.shape) == 0:
+            loaded_weight = loaded_weight.reshape(1)
+
+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
+        assert param_data.shape == loaded_weight.shape
+        param_data.copy_(loaded_weight)
+
+    def weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor):
+        # Special case for loading scales off disk, which often do not
+        # have a shape (such as in the case of AutoFP8).
+        if len(loaded_weight.shape) == 0:
+            assert loaded_weight.numel() == 1
+            loaded_weight = loaded_weight.reshape(1)
+        param.load_column_parallel_weight(loaded_weight=loaded_weight)
+
+    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
+        bias = self.bias if not self.skip_bias_add else None
+
+        # Matrix multiply.
+        assert self.quant_method is not None
+        output_parallel = self.quant_method.apply(self, input_, bias)
+        if self.gather_output:
+            # All-gather across the partitions.
+            output = tensor_model_parallel_all_gather(output_parallel)
+        else:
+            output = output_parallel
+        output_bias = self.bias if self.skip_bias_add else None
+        return output, output_bias
+
+    def extra_repr(self) -> str:
+        s = f"in_features={self.input_size}"
+        s += f", output_features={self.output_size_per_partition}"
+        s += f", bias={self.bias is not None}"
+        s += f", tp_size={get_tensor_model_parallel_world_size()}"
+        s += f", gather_output={self.gather_output}"
+        return s
+
+
+class MergedColumnParallelLinear(ColumnParallelLinear):
+    """Packed linear layers with column parallelism.
+
+    Similar to ColumnParallelLinear, but the weight matrix is concatenated
+    along the output dimension. When the weight matrix is loaded, the
+    different partitions are sharded separately.
+
+    Args:
+        input_size: input dimension of the linear layer.
+        output_sizes: list of output dimensions of the linear layer.
+        bias: If true, add bias.
+        gather_output: If true, call all-gather on output and make the output
+                       available to all GPUs, otherwise, every GPU will have
+                       its own output.
+        skip_bias_add: This was added to enable performance optimizations where
+                       bias can be fused with other element-wise operations. we
+                       skip adding bias but instead return it.
+        params_dtype: Data type for the parameters.
+        quant_config: Quantization configure.
+        prefix: The name of the layer in the state dict, including all parents
+                        (e.g. model.layers.0.qkv_proj)
+    """
+
+    def __init__(self,
+                 input_size: int,
+                 output_sizes: list[int],
+                 bias: bool = True,
+                 gather_output: bool = False,
+                 skip_bias_add: bool = False,
+                 params_dtype: Optional[torch.dtype] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = ""):
+        self.output_sizes = output_sizes
+        tp_size = get_tensor_model_parallel_world_size()
+        assert all(output_size % tp_size == 0 for output_size in output_sizes)
+        super().__init__(input_size=input_size,
+                         output_size=sum(output_sizes),
+                         bias=bias,
+                         gather_output=gather_output,
+                         skip_bias_add=skip_bias_add,
+                         params_dtype=params_dtype,
+                         quant_config=quant_config,
+                         prefix=prefix)
+
+    def weight_loader(self,
+                      param: Parameter,
+                      loaded_weight: torch.Tensor,
+                      loaded_shard_id: Optional[int] = None):
+
+        # Special case for GGUF
+        # initialize GGUF param after we know the quantize type
+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
+        if is_gguf_weight_type:
+            if loaded_shard_id is not None:
+                param.data[loaded_shard_id].copy_(loaded_weight)
+                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
+            else:
+                param.shard_weight_type = {
+                    i: loaded_weight.item()
+                    for i, _ in enumerate(self.output_sizes)
+                }
+            return
+
+        if is_gguf_weight:
+            tp_size = get_tensor_model_parallel_world_size()
+            tp_rank = get_tensor_model_parallel_rank()
+
+            output_dim = getattr(param, "output_dim", None)
+            shard_size = loaded_weight.size(output_dim) // tp_size
+            start_idx = tp_rank * shard_size
+
+            if loaded_shard_id is not None:
+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+                                                     shard_size)
+                param.shard_id.append(loaded_shard_id)
+                param.shard_id_map[loaded_shard_id] = len(param.data_container)
+                param.data_container.append(loaded_weight)
+                if len(param.data_container) == 2:
+                    self.qweight = param.materialize_nested()
+                return
+
+        param_data = param.data
+        output_dim = getattr(param, "output_dim", None)
+        # Special case for AQLM codebooks.
+        is_metadata = getattr(param, "is_metadata", False)
+        # Special case for per-tensor scale to load scalar into fused array.
+        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
+
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+
+        if loaded_shard_id is None:
+            # Loaded weight is already fused on disk (mlp).
+            # (e.g., Phi-3's gate_up_proj).
+            if output_dim is None:
+                if needs_scalar_to_array:
+                    param_data, loaded_weight = adjust_scalar_to_fused_array(
+                        param_data, loaded_weight, 0)
+
+                assert param_data.shape == loaded_weight.shape
+                param_data.copy_(loaded_weight)
+                return
+            current_shard_offset = 0
+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
+                                            False)
+            shard_offsets: list[tuple[int, int, int]] = []
+            for i, output_size in enumerate(self.output_sizes):
+                shard_offsets.append((i, current_shard_offset, output_size))
+                current_shard_offset += output_size
+            packed_dim = getattr(param, "packed_dim", None)
+            for shard_id, shard_offset, shard_size in shard_offsets:
+                # Special case for Quantization.
+                # If quantized, we need to adjust the offset and size to account
+                # for the packing.
+                if packed_dim == output_dim:
+                    shard_size = shard_size // param.pack_factor
+                    shard_offset = shard_offset // param.pack_factor
+                    # Special case for Marlin.
+                    shard_size, shard_offset = adjust_marlin_shard(
+                        param, shard_size, shard_offset)
+
+                if use_bitsandbytes_4bit:
+                    index = list(itertools.accumulate([0] + self.output_sizes))
+                    orig_offsets = {
+                        str(i): (index[i], size)
+                        for i, size in enumerate(self.output_sizes)
+                    }
+                    orig_offsets["total"] = (self.output_size, 0)
+                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
+                        param, orig_offsets, str(shard_id))
+
+                loaded_weight_shard = loaded_weight.narrow(
+                    output_dim, shard_offset, shard_size)
+                self.weight_loader(param, loaded_weight_shard, shard_id)
+            return
+
+        assert loaded_shard_id < len(self.output_sizes)
+        tp_rank = get_tensor_model_parallel_rank()
+        tp_size = get_tensor_model_parallel_world_size()
+        if output_dim is not None:
+            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
+            shard_size = self.output_sizes[loaded_shard_id] // tp_size
+            # Special case for quantization.
+            # If quantized, we need to adjust the offset and size to account
+            # for the packing.
+            packed_dim = getattr(param, "packed_dim", None)
+            if packed_dim == output_dim:
+                shard_size = shard_size // param.pack_factor
+                shard_offset = shard_offset // param.pack_factor
+                # Special case for Marlin.
+                shard_size, shard_offset = adjust_marlin_shard(
+                    param, shard_size, shard_offset)
+
+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
+                                            False)
+            is_sharded_weight = getattr(param, "is_sharded_weight", False)
+            # bitsandbytes loads the weights of the specific portion
+            # no need to narrow
+            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
+
+            if use_bitsandbytes_4bit:
+                shard_size = loaded_weight.shape[output_dim]
+                shard_offset = loaded_weight.shape[output_dim] * \
+                    loaded_shard_id
+
+            #param_data = param_data.narrow(output_dim, shard_offset,
+            #                               shard_size)
+            if is_quantization:
+                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
+            else:
+                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
+
+            start_idx = tp_rank * shard_size
+            if not is_sharded_weight:
+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+                                                     shard_size)
+        # Special case for AQLM codebooks.
+        elif is_metadata:
+            # metadata indicates fixed size concatenated along dim 0
+            shard_size = loaded_weight.shape[0]
+            shard_offset = loaded_shard_id * shard_size
+            param_data = param_data.narrow(0, shard_offset, shard_size)
+
+        # Special case for per-tensor scales in fused case.
+        elif needs_scalar_to_array:
+            param_data, loaded_weight = adjust_scalar_to_fused_array(
+                param_data, loaded_weight, loaded_shard_id)
+
+        else:
+            ignore_warning = getattr(param, "ignore_warning", False)
+            if not ignore_warning:
+                logger.warning(
+                    "Loading a weight without `output_dim` attribute in "
+                    "MergedColumnParallelLinear, assume the weight is "
+                    "the same for all partitions.")
+
+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
+        assert param_data.shape == loaded_weight.shape
+        param_data.copy_(loaded_weight)
+
+    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
+                                           loaded_weight: torch.Tensor):
+        """
+        Handle special case for models where MLP layers are already
+        fused on disk. In this case, we have no shard id. This function
+        determmines the shard id by splitting these layers and then calls
+        the weight loader using the shard id.
+
+        An example of a model with these fused layers:
+        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
+        """
+
+        current_shard_offset = 0
+        shard_offsets: list[tuple[int, int, int]] = []
+        for i, output_size in enumerate(self.output_sizes):
+            shard_offsets.append((i, current_shard_offset, output_size))
+            current_shard_offset += output_size
+
+        for shard_id, shard_offset, shard_size in shard_offsets:
+            # Special case for Quantization.
+            # If quantized, we need to adjust the offset and size to account
+            # for the packing.
+            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
+                                  )) and param.packed_dim == param.output_dim:
+                shard_size, shard_offset = \
+                    param.adjust_shard_indexes_for_packing(
+                    shard_size=shard_size, shard_offset=shard_offset)
+
+            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
+                                                       shard_offset,
+                                                       shard_size)
+            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
+
+    def weight_loader_v2(self,
+                         param: BasevLLMParameter,
+                         loaded_weight: torch.Tensor,
+                         loaded_shard_id: Optional[int] = None):
+        if loaded_shard_id is None:
+            if isinstance(param, PerTensorScaleParameter):
+                param.load_merged_column_weight(loaded_weight=loaded_weight,
+                                                shard_id=0)
+                return
+            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
+                param.load_merged_column_weight(loaded_weight=loaded_weight)
+                return
+            # TODO: @dsikka - move to parameter.py
+            self._load_fused_module_from_checkpoint(param, loaded_weight)
+            return
+
+        assert loaded_shard_id < len(self.output_sizes)
+
+        tp_size = get_tensor_model_parallel_world_size()
+
+        if isinstance(param, BlockQuantScaleParameter):
+            from vllm.model_executor.layers.quantization.fp8 import (
+                Fp8LinearMethod, Fp8MoEMethod)
+            assert self.quant_method is not None
+            assert isinstance(self.quant_method,
+                              (Fp8LinearMethod, Fp8MoEMethod))
+            weight_block_size = self.quant_method.quant_config.weight_block_size
+            assert weight_block_size is not None
+            block_n, _ = weight_block_size[0], weight_block_size[1]
+            shard_offset = (
+                (sum(self.output_sizes[:loaded_shard_id]) + block_n - 1) //
+                block_n) // tp_size
+            shard_size = ((self.output_sizes[loaded_shard_id] + block_n - 1) //
+                          block_n // tp_size)
+        else:
+            shard_offset = sum(self.output_sizes[:loaded_shard_id]) // tp_size
+            shard_size = self.output_sizes[loaded_shard_id] // tp_size
+
+        param.load_merged_column_weight(loaded_weight=loaded_weight,
+                                        shard_id=loaded_shard_id,
+                                        shard_offset=shard_offset,
+                                        shard_size=shard_size)
+
+
+class QKVParallelLinear(ColumnParallelLinear):
+    """Linear layers for the attention's QKV transformation.
+
+    Linear layers for the linear transformation of the query, key, and value
+    vectors in the attention layer. The weight matrix is concatenated along
+    the output dimension. The layer is parallelized along the head dimension.
+    When the number of key/value heads is smaller than the number of query
+    heads (e.g., multi-query/grouped-query attention), the key/value head may
+    be replicated while the query heads are partitioned.
+
+    Args:
+        hidden_size: input hidden state size of the transformer.
+        head_size: size of each attention head.
+        total_num_heads: total number of attention query heads.
+        total_num_kv_heads: total number of attention key/value heads. If
+                            None, assume total_num_kv_heads = total_num_heads.
+        bias: If true, add bias.
+        skip_bias_add: This was added to enable performance optimizations where
+                       bias can be fused with other element-wise operations. we
+                       skip adding bias but instead return it.
+        params_dtype: Data type for the parameters.
+        quant_config: Quantization configure.
+        prefix: The name of the layer in the state dict, including all parents
+                        (e.g. model.layers.0.qkv_proj)
+    """
+
+    def __init__(self,
+                 hidden_size: int,
+                 head_size: int,
+                 total_num_heads: int,
+                 total_num_kv_heads: Optional[int] = None,
+                 bias: bool = True,
+                 skip_bias_add: bool = False,
+                 params_dtype: Optional[torch.dtype] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = ""):
+        self.hidden_size = hidden_size
+        self.head_size = head_size
+        self.total_num_heads = total_num_heads
+        if total_num_kv_heads is None:
+            total_num_kv_heads = total_num_heads
+        self.total_num_kv_heads = total_num_kv_heads
+        # Divide the weight matrix along the last dimension.
+        tp_size = get_tensor_model_parallel_world_size()
+        self.num_heads = divide(self.total_num_heads, tp_size)
+        if tp_size >= self.total_num_kv_heads:
+            self.num_kv_heads = 1
+            self.num_kv_head_replicas = divide(tp_size,
+                                               self.total_num_kv_heads)
+        else:
+            self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)
+            self.num_kv_head_replicas = 1
+        input_size = self.hidden_size
+        output_size = (self.num_heads +
+                       2 * self.num_kv_heads) * tp_size * self.head_size
+        self.output_sizes = [
+            self.num_heads * self.head_size * tp_size,  # q_proj
+            self.num_kv_heads * self.head_size * tp_size,  # k_proj
+            self.num_kv_heads * self.head_size * tp_size,  # v_proj 
+        ]
+
+        super().__init__(input_size=input_size,
+                         output_size=output_size,
+                         bias=bias,
+                         gather_output=False,
+                         skip_bias_add=skip_bias_add,
+                         params_dtype=params_dtype,
+                         quant_config=quant_config,
+                         prefix=prefix)
+
+    def _get_shard_offset_mapping(self, loaded_shard_id: str):
+        shard_offset_mapping = {
+            "q": 0,
+            "k": self.num_heads * self.head_size,
+            "v": (self.num_heads + self.num_kv_heads) * self.head_size,
+            "total": (self.num_heads + 2 * self.num_kv_heads) * self.head_size
+        }
+        return shard_offset_mapping.get(loaded_shard_id)
+
+    def _get_shard_size_mapping(self, loaded_shard_id: str):
+        shard_size_mapping = {
+            "q": self.num_heads * self.head_size,
+            "k": self.num_kv_heads * self.head_size,
+            "v": self.num_kv_heads * self.head_size,
+        }
+        return shard_size_mapping.get(loaded_shard_id)
+
+    def _load_fused_module_from_checkpoint(self, param: BasevLLMParameter,
+                                           loaded_weight: torch.Tensor):
+        """
+        Handle special case for models where QKV layers are already 
+        fused on disk. In this case, we have no shard id. This function
+        determmines the shard id by splitting these layers and then calls
+        the weight loader using the shard id.
+
+        An example of a model with these fused layers:
+        https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
+        """
+        shard_offsets = [
+            # (shard_id, shard_offset, shard_size)
+            ("q", 0, self.total_num_heads * self.head_size),
+            ("k", self.total_num_heads * self.head_size,
+             self.total_num_kv_heads * self.head_size),
+            ("v",
+             (self.total_num_heads + self.total_num_kv_heads) * self.head_size,
+             self.total_num_kv_heads * self.head_size),
+        ]
+
+        for shard_id, shard_offset, shard_size in shard_offsets:
+            # Special case for Quantization.
+            # If quantized, we need to adjust the offset and size to account
+            # for the packing.
+            if isinstance(param, (PackedColumnParameter, PackedvLLMParameter
+                                  )) and param.packed_dim == param.output_dim:
+                shard_size, shard_offset = \
+                    param.adjust_shard_indexes_for_packing(
+                    shard_size=shard_size, shard_offset=shard_offset)
+
+            loaded_weight_shard = loaded_weight.narrow(param.output_dim,
+                                                       shard_offset,
+                                                       shard_size)
+            self.weight_loader_v2(param, loaded_weight_shard, shard_id)
+
+    def weight_loader_v2(self,
+                         param: BasevLLMParameter,
+                         loaded_weight: torch.Tensor,
+                         loaded_shard_id: Optional[str] = None):
+        if loaded_shard_id is None:  # special case for certain models
+            if isinstance(param, PerTensorScaleParameter):
+                param.load_qkv_weight(loaded_weight=loaded_weight, shard_id=0)
+                return
+            elif type(param) in (RowvLLMParameter, BasevLLMParameter):
+                param.load_qkv_weight(loaded_weight=loaded_weight)
+                return
+            # TODO: @dsikka - move to parameter.py
+            self._load_fused_module_from_checkpoint(param, loaded_weight)
+            return
+
+        assert loaded_shard_id in ["q", "k", "v"]
+
+        shard_offset = self._get_shard_offset_mapping(loaded_shard_id)
+        shard_size = self._get_shard_size_mapping(loaded_shard_id)
+
+        # Note(simon): This is needed for Qwen3's fp8 quantization.
+        if isinstance(param, BlockQuantScaleParameter):
+            assert self.quant_method is not None
+            assert hasattr(self.quant_method, "quant_config")
+            weight_block_size = self.quant_method.quant_config.weight_block_size
+            block_n, _ = weight_block_size[0], weight_block_size[1]
+            shard_offset = (shard_offset + block_n - 1) // block_n
+            shard_size = (shard_size + block_n - 1) // block_n
+
+        param.load_qkv_weight(loaded_weight=loaded_weight,
+                              num_heads=self.num_kv_head_replicas,
+                              shard_id=loaded_shard_id,
+                              shard_offset=shard_offset,
+                              shard_size=shard_size)
+
+    def weight_loader(self,
+                      param: Parameter,
+                      loaded_weight: torch.Tensor,
+                      loaded_shard_id: Optional[str] = None):
+
+        # Special case for GGUF
+        # initialize GGUF param after we know the quantize type
+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
+        if is_gguf_weight_type:
+            idx_map = {"q": 0, "k": 1, "v": 2}
+            if loaded_shard_id is not None:
+                param.data[idx_map[loaded_shard_id]].copy_(loaded_weight)
+                param.shard_weight_type[loaded_shard_id] = loaded_weight.item()
+            else:
+                param.shard_weight_type = {
+                    k: loaded_weight.item()
+                    for k in idx_map
+                }
+            return
+
+        if is_gguf_weight:
+            tp_size = get_tensor_model_parallel_world_size()
+            tp_rank = get_tensor_model_parallel_rank()
+
+            output_dim = getattr(param, "output_dim", None)
+            shard_size = loaded_weight.size(output_dim) // tp_size
+            start_idx = tp_rank * shard_size
+
+            if loaded_shard_id is not None:
+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+                                                     shard_size)
+                param.shard_id.append(loaded_shard_id)
+                param.shard_id_map[loaded_shard_id] = len(param.data_container)
+                param.data_container.append(loaded_weight)
+                if len(param.data_container) == 3:
+                    self.qweight = param.materialize_nested()
+                return
+
+        param_data = param.data
+        output_dim = getattr(param, "output_dim", None)
+        # Special case for AQLM codebooks.
+        is_metadata = getattr(param, "is_metadata", False)
+
+        # Special case for per-tensor scales in fused case.
+        needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+
+        if loaded_shard_id is None:
+            # Loaded weight is already fused on disk (qkv).
+            # (e.g., Phi-3's qkv_proj).
+            if output_dim is None:
+                if needs_scalar_to_array:
+                    param_data, loaded_weight = adjust_scalar_to_fused_array(
+                        param_data, loaded_weight, 0)
+
+                assert param_data.shape == loaded_weight.shape
+                param_data.copy_(loaded_weight)
+                return
+            shard_offsets = [
+                # (shard_id, shard_offset, shard_size)
+                ("q", 0, self.total_num_heads * self.head_size),
+                ("k", self.total_num_heads * self.head_size,
+                 self.total_num_kv_heads * self.head_size),
+                ("v", (self.total_num_heads + self.total_num_kv_heads) *
+                 self.head_size, self.total_num_kv_heads * self.head_size),
+            ]
+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
+                                            False)
+
+            packed_dim = getattr(param, "packed_dim", None)
+            for shard_id, shard_offset, shard_size in shard_offsets:
+                # Special case for Quantized Weights.
+                # If quantized, we need to adjust the offset and size to account
+                # for the packing.
+                if packed_dim == output_dim:
+                    shard_size = shard_size // param.pack_factor
+                    shard_offset = shard_offset // param.pack_factor
+
+                    # Special case for Marlin.
+                    shard_size, shard_offset = adjust_marlin_shard(
+                        param, shard_size, shard_offset)
+
+                if use_bitsandbytes_4bit:
+                    orig_qkv_offsets = {
+                        "q": (0, self.total_num_heads * self.head_size),
+                        "k": (self.total_num_heads * self.head_size,
+                              self.total_num_kv_heads * self.head_size),
+                        "v":
+                        ((self.total_num_heads + self.total_num_kv_heads) *
+                         self.head_size,
+                         self.total_num_kv_heads * self.head_size),
+                        "total":
+                        ((self.total_num_heads + 2 * self.total_num_kv_heads) *
+                         self.head_size, 0)
+                    }
+
+                    shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
+                        param, orig_qkv_offsets, shard_id)
+
+                loaded_weight_shard = loaded_weight.narrow(
+                    output_dim, shard_offset, shard_size)
+                self.weight_loader(param, loaded_weight_shard, shard_id)
+            return
+
+        tp_rank = get_tensor_model_parallel_rank()
+        assert loaded_shard_id in ["q", "k", "v"]
+
+        # If output dim is defined, use the default loading process.
+        if output_dim is not None:
+            if loaded_shard_id == "q":
+                shard_offset = 0
+                shard_size = self.num_heads * self.head_size
+            elif loaded_shard_id == "k":
+                shard_offset = self.num_heads * self.head_size
+                shard_size = self.num_kv_heads * self.head_size
+            elif loaded_shard_id == "v":
+                shard_offset = (self.num_heads +
+                                self.num_kv_heads) * self.head_size
+                shard_size = self.num_kv_heads * self.head_size
+            # Special case for Quantized Weights.
+            # If quantized, we need to adjust the offset and size to account
+            # for the packing.
+            packed_dim = getattr(param, "packed_dim", None)
+            if packed_dim == output_dim:
+                shard_size = shard_size // param.pack_factor
+                shard_offset = shard_offset // param.pack_factor
+
+                # Special case for Marlin.
+                shard_size, shard_offset = adjust_marlin_shard(
+                    param, shard_size, shard_offset)
+
+            use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit",
+                                            False)
+            is_sharded_weight = getattr(param, "is_sharded_weight", False)
+            # bitsandbytes loads the weights of the specific portion
+            # no need to narrow
+            is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
+
+            if use_bitsandbytes_4bit:
+                orig_qkv_offsets = {
+                    "q": (0, self.num_heads * self.head_size),
+                    "k": (self.num_heads * self.head_size,
+                          self.num_kv_heads * self.head_size),
+                    "v":
+                    ((self.num_heads + self.num_kv_heads) * self.head_size,
+                     self.num_kv_heads * self.head_size),
+                    "total":
+                    ((self.num_heads + 2 * self.num_kv_heads) * self.head_size,
+                     0)
+                }
+                shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
+                    param, orig_qkv_offsets, loaded_shard_id)
+
+            #param_data = param_data.narrow(output_dim, shard_offset,
+            #                               shard_size)
+            if len(param_data.shape)==1 or is_quantization:
+                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
+            else:
+                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
+
+            if loaded_shard_id == "q":
+                shard_id = tp_rank
+            else:
+                shard_id = tp_rank // self.num_kv_head_replicas
+            start_idx = shard_id * shard_size
+
+            if not is_sharded_weight:
+                loaded_weight = loaded_weight.narrow(output_dim, start_idx,
+                                                     shard_size)
+
+        # Special case for for AQLM codebooks.
+        elif is_metadata:
+            # metadata indicates fixed size concatenated along dim 0
+            shard_size = loaded_weight.shape[0]
+            shard_index = ["q", "k", "v"].index(loaded_shard_id)
+            param_data = param_data.narrow(0, shard_index * shard_size,
+                                           shard_size)
+        # Special case for per-tensor scales in fused case.
+        elif needs_scalar_to_array:
+            param_data, loaded_weight = adjust_scalar_to_fused_array(
+                param_data, loaded_weight, loaded_shard_id)
+        else:
+            ignore_warning = getattr(param, "ignore_warning", False)
+            if not ignore_warning:
+                logger.warning(
+                    "Loading a weight without `output_dim` attribute in "
+                    "QKVParallelLinear, assume the weight is the same "
+                    "for all partitions.")
+
+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
+        assert param_data.shape == loaded_weight.shape
+        param_data.copy_(loaded_weight)
+
+
+class RowParallelLinear(LinearBase):
+    """Linear layer with row parallelism.
+
+    The linear layer is defined as Y = XA + b. A is parallelized along
+    its first dimension and X along its second dimension as:
+               -   -
+              | A_1 |
+              | .   |
+          A = | .   |        X = [X_1, ..., X_p]
+              | .   |
+              | A_p |
+               -   -
+    Arguments:
+        input_size: first dimension of matrix A.
+        output_size: second dimension of matrix A.
+        bias: If true, add bias. Note that bias is not parallelized.
+        input_is_parallel: If true, we assume that the input is already
+                           split across the GPUs and we do not split
+                           again.
+        skip_bias_add: This was added to enable performance optimization where
+                       bias can be fused with other element-wise operations.
+                       We skip adding bias but instead return it.
+        params_dtype: Data type for the parameters.
+        quant_config: Quantization configure.
+    """
+
+    def __init__(self,
+                 input_size: int,
+                 output_size: int,
+                 bias: bool = True,
+                 input_is_parallel: bool = True,
+                 skip_bias_add: bool = False,
+                 params_dtype: Optional[torch.dtype] = None,
+                 reduce_results: bool = True,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = ""):
+        super().__init__(input_size, output_size, skip_bias_add, params_dtype,
+                         quant_config, prefix)
+
+        self.input_is_parallel = input_is_parallel
+        self.reduce_results = reduce_results
+
+        # Divide the weight matrix along the last dimension.
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.input_size_per_partition = divide(input_size, self.tp_size)
+        assert self.quant_method is not None
+
+        self.quant_method.create_weights(
+            layer=self,
+            input_size_per_partition=self.input_size_per_partition,
+            output_partition_sizes=[self.output_size],
+            input_size=self.input_size,
+            output_size=self.output_size,
+            params_dtype=self.params_dtype,
+            weight_loader=(
+                self.weight_loader_v2 if self.quant_method.__class__.__name__
+                in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))
+        if not reduce_results and (bias and not skip_bias_add):
+            raise ValueError("When not reduce the results, adding bias to the "
+                             "results can lead to incorrect results")
+
+        if bias:
+            self.bias = Parameter(
+                torch.empty(self.output_size, dtype=params_dtype))
+            set_weight_attrs(self.bias, {
+                "output_dim": 0,
+                "weight_loader": self.weight_loader,
+            })
+        else:
+            self.register_parameter("bias", None)
+
+    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
+        tp_rank = get_tensor_model_parallel_rank()
+        tp_size = get_tensor_model_parallel_world_size()
+        input_dim = getattr(param, "input_dim", None)
+        use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
+        is_sharded_weight = getattr(param, "is_sharded_weight", False)
+        # bitsandbytes loads the weights of the specific portion
+        # no need to narrow
+        is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
+
+        # Special case for GGUF
+        is_gguf_weight = getattr(param, "is_gguf_weight", False)
+        is_gguf_weight_type = getattr(param, "is_gguf_weight_type", False)
+        if is_gguf_weight_type:
+            param.weight_type = loaded_weight.item()
+
+        # Materialize GGUF UninitializedParameter
+        if is_gguf_weight and isinstance(param, UninitializedParameter):
+            weight_shape = list(loaded_weight.shape)
+            if input_dim:
+                weight_shape[input_dim] = weight_shape[input_dim] // tp_size
+            param.materialize(tuple(weight_shape), dtype=loaded_weight.dtype)
+
+        param_data = param.data
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+        if input_dim is not None and not is_sharded_weight:
+            #shard_size = param_data.shape[input_dim]
+            if is_quantization:
+                shard_size = param_data.shape[input_dim]
+            else:
+                shard_size = param_data.shape[int(not(input_dim))]
+            start_idx = tp_rank * shard_size
+            loaded_weight = loaded_weight.narrow(input_dim, start_idx,
+                                                 shard_size)
+
+        # Special case for loading scales off disk, which often do not
+        # have a shape (such as in the case of AutoFP8).
+        if len(loaded_weight.shape) == 0:
+            loaded_weight = loaded_weight.reshape(1)
+
+        loaded_weight = loaded_weight if is_quantization else loaded_weight.t()
+        assert param_data.shape == loaded_weight.shape
+        param_data.copy_(loaded_weight)
+
+    def weight_loader_v2(self, param: BasevLLMParameter,
+                         loaded_weight: torch.Tensor):
+
+        # Special case for loading scales off disk, which often do not
+        # have a shape (such as in the case of AutoFP8).
+        if len(loaded_weight.shape) == 0:
+            assert loaded_weight.numel() == 1
+            loaded_weight = loaded_weight.reshape(1)
+
+        param.load_row_parallel_weight(loaded_weight=loaded_weight)
+
+    def forward(self, input_) -> tuple[torch.Tensor, Optional[Parameter]]:
+        if self.input_is_parallel:
+            input_parallel = input_
+        else:
+            tp_rank = get_tensor_model_parallel_rank()
+            splitted_input = split_tensor_along_last_dim(
+                input_, num_partitions=self.tp_size)
+            input_parallel = splitted_input[tp_rank].contiguous()
+
+        # Matrix multiply.
+        assert self.quant_method is not None
+        # Only fuse bias add into GEMM for rank 0 (this ensures that
+        # bias will not get added more than once in TP>1 case)
+        bias_ = None if (self.tp_rank > 0 or self.skip_bias_add) else self.bias
+        output_parallel = self.quant_method.apply(self,
+                                                  input_parallel,
+                                                  bias=bias_)
+        if self.reduce_results and self.tp_size > 1:
+            output = tensor_model_parallel_all_reduce(output_parallel)
+        else:
+            output = output_parallel
+
+        output_bias = self.bias if self.skip_bias_add else None
+
+        return output, output_bias
+
+    def extra_repr(self) -> str:
+        s = f"input_features={self.input_size_per_partition}"
+        s += f", output_features={self.output_size}"
+        s += f", bias={self.bias is not None}"
+        s += f", tp_size={self.tp_size}"
+        s += f", reduce_results={self.reduce_results}"
+        return s
diff --git a/vllm/model_executor/layers/logits_processor.py b/vllm/model_executor/layers/logits_processor.py
index cdc67ca83..8230f7618 100644
--- a/vllm/model_executor/layers/logits_processor.py
+++ b/vllm/model_executor/layers/logits_processor.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """A layer that compute logits from hidden_stats."""
 import inspect
@@ -64,6 +65,11 @@ class LogitsProcessor(nn.Module):
         sampling_metadata: Optional[SamplingMetadata] = None,
         embedding_bias: Optional[torch.Tensor] = None,
     ) -> Optional[torch.Tensor]:
+    
+        if sampling_metadata is None:
+            logits = self._get_logits(hidden_states, lm_head, embedding_bias)
+            return logits
+
         if self.logits_as_input:
             logits = hidden_states
         else:
diff --git a/vllm/model_executor/layers/quantization/__init__.py b/vllm/model_executor/layers/quantization/__init__.py
index 6ded3874f..41b1f85b1 100644
--- a/vllm/model_executor/layers/quantization/__init__.py
+++ b/vllm/model_executor/layers/quantization/__init__.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import Dict, List, Type
@@ -8,6 +9,7 @@ from vllm.model_executor.layers.quantization.base_config import (
 QUANTIZATION_METHODS: List[str] = [
     "aqlm",
     "awq",
+    "gptq",
     "deepspeedfp",
     "tpu_int8",
     "fp8",
@@ -20,7 +22,7 @@ QUANTIZATION_METHODS: List[str] = [
     "gptq_marlin_24",
     "gptq_marlin",
     "awq_marlin",
-    "gptq",
+    #"gptq",
     "compressed-tensors",
     "bitsandbytes",
     "qqq",
@@ -115,8 +117,10 @@ def get_quantization_config(quantization: str) -> Type[QuantizationConfig]:
         "marlin": MarlinConfig,
         "gguf": GGUFConfig,
         "gptq_marlin_24": GPTQMarlin24Config,
-        "gptq_marlin": GPTQMarlinConfig,
-        "awq_marlin": AWQMarlinConfig,
+        #"gptq_marlin": GPTQMarlinConfig,
+        #"awq_marlin": AWQMarlinConfig,
+        "gptq_marlin": GPTQConfig,
+        "awq_marlin": AWQConfig,
         "gptq": GPTQConfig,
         "compressed-tensors": CompressedTensorsConfig,
         "bitsandbytes": BitsAndBytesConfig,
diff --git a/vllm/model_executor/layers/quantization/awq.py b/vllm/model_executor/layers/quantization/awq.py
index ff77af44d..fb07159f1 100644
--- a/vllm/model_executor/layers/quantization/awq.py
+++ b/vllm/model_executor/layers/quantization/awq.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import Any, Dict, List, Optional
@@ -47,7 +48,7 @@ class AWQConfig(QuantizationConfig):
         return "awq"
 
     def get_supported_act_dtypes(self) -> List[torch.dtype]:
-        return [torch.half]
+        return [torch.half, torch.bfloat16]
 
     @classmethod
     def get_min_capability(cls) -> int:
@@ -157,6 +158,12 @@ class AWQLinearMethod(LinearMethodBase):
                                           requires_grad=False)
         layer.scales = torch.nn.Parameter(layer.scales.data,
                                           requires_grad=False)
+        # warmup
+        if self.quant_config.group_size != 128:
+            pass
+        else:
+            qweight = ops.awq_to_gptq_4bit(layer.qweight)
+            layer.qweight = torch.nn.Parameter(qweight, requires_grad=False)
 
     def apply(self,
               layer: torch.nn.Module,
@@ -165,19 +172,28 @@ class AWQLinearMethod(LinearMethodBase):
         qweight = layer.qweight
         scales = layer.scales
         qzeros = layer.qzeros
+        out_shape = ()
         pack_factor = self.quant_config.pack_factor
-        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
         reshaped_x = x.reshape(-1, x.shape[-1])
+        out = torch.empty(0)
 
         # num_tokens >= threshold
-        FP16_MATMUL_HEURISTIC_CONDITION = x.shape[:-1].numel() >= 256
-
-        if FP16_MATMUL_HEURISTIC_CONDITION:
+        FP16_MATMUL_HEURISTIC_CONDITION =x.shape[:-1].numel() >= 256
+        # if (FP16_MATMUL_HEURISTIC_CONDITION and reshaped_x.dtype == torch.half) or self.quant_config.group_size != 128:
+        if self.quant_config.group_size != 128:
+            out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
             out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)
             out = torch.matmul(reshaped_x, out)
         else:
-            out = ops.awq_gemm(reshaped_x, qweight, scales, qzeros,
-                               pack_factor)
+            num_out_channel = qweight.shape[0]
+            out_shape = (x.shape[:-1] + (num_out_channel, ))
+            temp_space = torch.empty(0, dtype=torch.float32, device=x.device)
+            if reshaped_x.dtype == torch.bfloat16:
+                temp_space = torch.zeros(reshaped_x.shape[0], num_out_channel,
+                                         dtype=torch.float32, device=x.device)
+            out = ops.awq_gemm(reshaped_x, qweight, qzeros, scales,
+                               pack_factor, temp_space,
+                               True if reshaped_x.dtype == torch.bfloat16 else False)
         if bias is not None:
             out.add_(bias)
         return out.reshape(out_shape)
diff --git a/vllm/model_executor/layers/quantization/base_config.py b/vllm/model_executor/layers/quantization/base_config.py
index c0d8553c0..22ad62d96 100644
--- a/vllm/model_executor/layers/quantization/base_config.py
+++ b/vllm/model_executor/layers/quantization/base_config.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import inspect
@@ -103,7 +104,10 @@ class QuantizationConfig(ABC):
            this method should only be overwritten by subclasses in exceptional 
            circumstances
         """
-        return None
+        if(user_quant != None):
+            return user_quant
+        else:
+            return hf_quant_cfg["quant_method"]
 
     @staticmethod
     def get_from_keys(config: Dict[str, Any], keys: List[str]) -> Any:
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
index 6ee3e9362..16d2d7d31 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from contextlib import suppress
@@ -193,7 +194,8 @@ class CompressedTensorsConfig(QuantizationConfig):
                                 min_capability: int,
                                 error: bool = True) -> bool:
         capability_tuple = current_platform.get_device_capability()
-
+        
+        """
         if capability_tuple is not None:
             capability = capability_tuple.to_int()
             supported = capability >= min_capability
@@ -205,6 +207,7 @@ class CompressedTensorsConfig(QuantizationConfig):
             return supported
         else:
             return False
+        """
 
     def _is_static_tensor_w8a8(self, weight_quant: BaseModel,
                                input_quant: BaseModel) -> bool:
@@ -262,6 +265,16 @@ class CompressedTensorsConfig(QuantizationConfig):
             input_quant.strategy == QuantizationStrategy.TENSOR)
         return is_symmetric_activation and is_per_tensor_activation
 
+    def _is_dynamic_token_int8_w8a8(self, weight_quant: BaseModel,
+                     input_quant: BaseModel) -> bool:
+        # Confirm weights and activations quantized.
+        if weight_quant is None or input_quant is None:
+            return False
+            
+        is_int8_w8a8 = (weight_quant.type == QuantizationType.INT and input_quant.type == QuantizationType.INT)
+        return is_int8_w8a8 and self._is_dynamic_token_w8a8(weight_quant, input_quant)  
+
+
     def _is_fp8_w8a16(self, weight_quant: BaseModel,
                       input_quant: BaseModel) -> bool:
         # Confirm weights quantized.
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
index db8e8a4b6..53ee538ac 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import enum
@@ -51,10 +52,125 @@ class CompressedTensorsMoEMethod(FusedMoEMethodBase):
             return CompressedTensorsWNA16MoEMethod(quant_config)
         elif quant_config._is_fp8_w8a8(weight_quant, input_quant):
             return CompressedTensorsW8A8Fp8MoEMethod(quant_config)
+        elif quant_config._is_dynamic_token_int8_w8a8(weight_quant, input_quant):
+            return CompressedTensorsW8A8Int8MoEMethod(quant_config)
         else:
             raise RuntimeError(
                 f"Unsupported FusedMoe scheme: {weight_quant}, {input_quant}")
 
+class CompressedTensorsW8A8Int8MoEMethod(CompressedTensorsMoEMethod):
+    def __init__(
+        self,
+        quant_config: "CompressedTensorsConfig"  # type: ignore # noqa E501
+    ):
+        self.quant_config = quant_config
+        self.weight_quant = self.quant_config.target_scheme_map["Linear"].get(
+                "weights")
+        self.input_quant = self.quant_config.target_scheme_map["Linear"].get(
+            "input_activations")
+
+        if not (self.weight_quant.strategy == QuantizationStrategy.CHANNEL
+                and self.input_quant.strategy == QuantizationStrategy.TOKEN):
+            raise ValueError(
+                "For INT8 Fused MoE layers, only per-channel scales"
+                "for activations and per-token scales for activations are supported. Found "
+                f"{self.weight_quant}, {self.input_quant}")
+
+        self.static_input_scales = not self.input_quant.dynamic
+
+
+    def create_weights(self, layer: torch.nn.Module, num_experts: int,
+                       hidden_size: int, intermediate_size_per_partition: int,
+                       params_dtype: torch.dtype, **extra_weight_attrs):
+
+        params_dtype = torch.int8
+
+        # WEIGHTS
+        w13_weight = torch.nn.Parameter(torch.empty(num_experts,
+                                                    2 * intermediate_size_per_partition,
+                                                    hidden_size,
+                                                    dtype=params_dtype),
+                                        requires_grad=False)
+
+        layer.register_parameter("w13_weight", w13_weight)
+        set_weight_attrs(w13_weight, extra_weight_attrs)
+
+        w2_weight = torch.nn.Parameter(torch.empty(num_experts,
+                                                   hidden_size,
+                                                   intermediate_size_per_partition,
+                                                   dtype=params_dtype),
+                                                    requires_grad=False)
+        layer.register_parameter("w2_weight", w2_weight)
+        set_weight_attrs(w2_weight, extra_weight_attrs)
+
+        w13_weight_scale = torch.nn.Parameter(torch.ones(num_experts,
+                                                         2 * intermediate_size_per_partition,
+                                                         1,
+                                                         dtype=torch.float32),
+                                                         requires_grad=False)
+        layer.register_parameter("w13_weight_scale", w13_weight_scale)
+
+        w2_weight_scale = torch.nn.Parameter(torch.ones(num_experts,
+                                                        hidden_size,
+                                                        1,
+                                                        dtype=torch.float32),
+                                                        requires_grad=False)
+        layer.register_parameter("w2_weight_scale", w2_weight_scale)
+
+        extra_weight_attrs.update({"quant_method": FusedMoeWeightScaleSupported.CHANNEL.value})
+        set_weight_attrs(w13_weight_scale, extra_weight_attrs)
+        set_weight_attrs(w2_weight_scale, extra_weight_attrs)
+
+        # INPUT_SCALES
+        if self.static_input_scales:
+            raise ValueError(
+                "For INT8 Fused MoE layers, only dynamic scales"
+                "for activations are supported. Found "
+                f"{self.input_quant}")
+        else:
+            layer.w13_input_scale = None
+            layer.w2_input_scale = None
+
+
+    def apply(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        router_logits: torch.Tensor,
+        top_k: int,
+        renormalize: bool = True,
+        use_grouped_topk: bool = False,
+        num_expert_group: Optional[int] = None,
+        topk_group: Optional[int] = None,
+        custom_routing_function: Optional[Callable] = None,
+        scoring_func: str = "softmax",
+        e_score_correction_bias: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+
+        from vllm.model_executor.layers.fused_moe import fused_experts
+        topk_weights, topk_ids = FusedMoE.select_experts(
+            hidden_states=x,
+            router_logits=router_logits,
+            use_grouped_topk=use_grouped_topk,
+            top_k=top_k,
+            renormalize=renormalize,
+            topk_group=topk_group,
+            num_expert_group=num_expert_group,
+            custom_routing_function=custom_routing_function,
+            scoring_func=scoring_func,
+            e_score_correction_bias=e_score_correction_bias,)
+
+        return fused_experts(x,
+                             layer.w13_weight,
+                             layer.w2_weight,
+                             topk_weights=topk_weights,
+                             topk_ids=topk_ids,
+                             inplace=True,
+                             use_int8_w8a8=True,
+                             w1_scale=layer.w13_weight_scale,
+                             w2_scale=layer.w2_weight_scale,
+                             a1_scale=layer.w13_input_scale,
+                             a2_scale=layer.w2_input_scale)
 
 class CompressedTensorsW8A8Fp8MoEMethod(CompressedTensorsMoEMethod):
 
diff --git a/vllm/model_executor/layers/quantization/gptq.py b/vllm/model_executor/layers/quantization/gptq.py
index 0cb77a754..65900e08d 100644
--- a/vllm/model_executor/layers/quantization/gptq.py
+++ b/vllm/model_executor/layers/quantization/gptq.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import enum
@@ -55,7 +56,8 @@ class GPTQConfig(QuantizationConfig):
 
     @classmethod
     def get_supported_act_dtypes(cls) -> List[torch.dtype]:
-        return [torch.half]
+        return [torch.half, torch.bfloat16]
+        #return [torch.half]
 
     @classmethod
     # Need to figure it out
@@ -219,7 +221,7 @@ class GPTQLinearMethod(LinearMethodBase):
 
         # exllama needs to shuffle the weight after the weight is loaded
         # here we do the shuffle on first forward pass
-        if layer.exllama_state == ExllamaState.UNINITIALIZED:
+        if self.quant_config.group_size == 128 or self.quant_config.group_size == 64:
             if self.quant_config.desc_act:
                 layer.g_idx.data = torch.argsort(layer.g_idx).to(torch.int)
             else:
@@ -229,6 +231,41 @@ class GPTQLinearMethod(LinearMethodBase):
             layer.exllama_state = ExllamaState.READY
             ops.gptq_shuffle(layer.qweight, layer.g_idx,
                              self.quant_config.weight_bits)
+            if layer.scales.dtype != torch.bfloat16:
+                perm_space = torch.empty(0)
+                temp_space = torch.empty(0)
+                if self.quant_config.weight_bits == 4:
+                    # warmup
+                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*8, dtype=layer.scales.dtype, device="cuda")
+                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
+                                   layer.scales, layer.g_idx,
+                                   layer.exllama_state == ExllamaState.READY,
+                                   self.quant_config.weight_bits,
+                                   self.quant_config.group_size,
+                                   perm_space, temp_space,
+                                   False)
+                if self.quant_config.weight_bits == 8:
+                    # warmup
+                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*4, dtype=layer.scales.dtype, device="cuda")
+                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
+                                   layer.scales, layer.g_idx,
+                                   layer.exllama_state == ExllamaState.READY,
+                                   self.quant_config.weight_bits,
+                                   self.quant_config.group_size,
+                                   perm_space, temp_space,
+                                   False)
+        else:
+            if layer.exllama_state == ExllamaState.UNINITIALIZED:
+                if self.quant_config.desc_act:
+                    layer.g_idx.data = torch.argsort(layer.g_idx).to(torch.int)
+                else:
+                    layer.g_idx.data = torch.empty((0, ),
+                                                   dtype=torch.int,
+                                                   device=layer.g_idx.device)
+                layer.exllama_state = ExllamaState.READY
+                ops.gptq_shuffle(layer.qweight, layer.g_idx,
+                                 self.quant_config.weight_bits)
+
 
     def apply(self,
               layer: torch.nn.Module,
@@ -237,10 +274,25 @@ class GPTQLinearMethod(LinearMethodBase):
         out_shape = x.shape[:-1] + (layer.qweight.shape[-1], )
         reshaped_x = x.reshape(-1, x.shape[-1])
 
+        perm_space = torch.empty(0)
+        temp_space = torch.empty(0)
+        if self.quant_config.weight_bits == 4 or self.quant_config.weight_bits == 8:
+            if self.quant_config.group_size == 128 or self.quant_config.group_size == 64:
+                if self.quant_config.desc_act:
+                    perm_space = torch.empty(reshaped_x.shape[0], reshaped_x.shape[1],
+                                             dtype=torch.float16, device="cuda")
+                if reshaped_x.dtype == torch.bfloat16:
+                    temp_space = torch.zeros(reshaped_x.shape[0], layer.qweight.shape[1],
+                                             dtype=torch.float32, device="cuda")
+
         output = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
                                layer.scales, layer.g_idx,
                                layer.exllama_state == ExllamaState.READY,
-                               self.quant_config.weight_bits)
+                               self.quant_config.weight_bits,
+                               self.quant_config.group_size,
+                               perm_space, temp_space,
+                               True if reshaped_x.dtype == torch.bfloat16 else False)
+
         if bias is not None:
             output.add_(bias)
         return output.reshape(out_shape)
diff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py
index 56fa597e2..04b93eea1 100644
--- a/vllm/model_executor/layers/quantization/moe_wna16.py
+++ b/vllm/model_executor/layers/quantization/moe_wna16.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import Any, Callable, Dict, List, Optional
@@ -35,24 +36,24 @@ class MoeWNA16Config(QuantizationConfig):
         self.linear_quant_method = linear_quant_method
         self.full_config = full_config
         self.use_marlin = False
-        if self.linear_quant_method == "gptq":
-            self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(
-                full_config)
-        elif self.linear_quant_method == "awq":
-            capability_tuple = current_platform.get_device_capability()
-            device_capability = (-1 if capability_tuple is None else
-                                 capability_tuple.to_int())
-            awq_min_capability = AWQConfig.get_min_capability()
-            if device_capability < awq_min_capability:
-                raise ValueError(
-                    "The quantization method moe_wna16 + awq is not supported "
-                    "for the current GPU. "
-                    f"Minimum capability: {awq_min_capability}. "
-                    f"Current capability: {device_capability}.")
-            self.use_marlin = AWQMarlinConfig.is_awq_marlin_compatible(
-                full_config)
-        else:
-            raise ValueError("moe_wna16 only support gptq and awq.")
+        #if self.linear_quant_method == "gptq":
+        #    self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(
+        #        full_config)
+        #elif self.linear_quant_method == "awq":
+        #    capability_tuple = current_platform.get_device_capability()
+        #    device_capability = (-1 if capability_tuple is None else
+        #                         capability_tuple.to_int())
+        #    awq_min_capability = AWQConfig.get_min_capability()
+        #    if device_capability < awq_min_capability:
+        #        raise ValueError(
+        #            "The quantization method moe_wna16 + awq is not supported "
+        #            "for the current GPU. "
+        #            f"Minimum capability: {awq_min_capability}. "
+        #            f"Current capability: {device_capability}.")
+        #    self.use_marlin = AWQMarlinConfig.is_awq_marlin_compatible(
+        #        full_config)
+        #else:
+        #    raise ValueError("moe_wna16 only support gptq and awq.")
 
         if modules_to_not_convert is None:
             self.modules_to_not_convert = []
diff --git a/vllm/model_executor/layers/quantization/utils/fp8_utils.py b/vllm/model_executor/layers/quantization/utils/fp8_utils.py
index 9895537c2..5dc604ff2 100644
--- a/vllm/model_executor/layers/quantization/utils/fp8_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/fp8_utils.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Adapted from https://github.com/sgl-project/sglang/pull/2575
@@ -28,7 +29,8 @@ current_platform_fp8_dtype = (torch.float8_e4m3fnuz
 def is_fp8(x: Union[torch.dtype, torch.Tensor]) -> bool:
     if isinstance(x, torch.Tensor):
         x = x.dtype
-    return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz
+    #return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz
+    return False
 
 
 def apply_w8a8_block_fp8_linear(
diff --git a/vllm/model_executor/layers/vocab_parallel_embedding.py b/vllm/model_executor/layers/vocab_parallel_embedding.py
index e409094dd..493de2d5e 100644
--- a/vllm/model_executor/layers/vocab_parallel_embedding.py
+++ b/vllm/model_executor/layers/vocab_parallel_embedding.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from dataclasses import dataclass
@@ -135,13 +136,15 @@ class VocabParallelEmbeddingShardIndices:
         assert self.num_added_elements <= self.num_added_elements_padded
 
 
-@torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
+#@torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
+@torch.jit.script
 def get_masked_input_and_mask(
         input_: torch.Tensor, org_vocab_start_index: int,
         org_vocab_end_index: int, num_org_vocab_padding: int,
         added_vocab_start_index: int,
         added_vocab_end_index: int) -> Tuple[torch.Tensor, torch.Tensor]:
     # torch.compile will fuse all of the pointwise ops below
+    # torch.jit.script will fuse all of the pointwise ops below
     # into a single kernel, making it very fast
     org_vocab_mask = (input_ >= org_vocab_start_index) & (
         input_ < org_vocab_end_index)
@@ -260,13 +263,22 @@ class VocabParallelEmbedding(torch.nn.Module):
             self.shard_indices.added_vocab_end_index -
             self.shard_indices.added_vocab_start_index)
 
-        self.linear_method.create_weights(self,
+        if isinstance(self.linear_method, UnquantizedEmbeddingMethod):
+            self.linear_method.create_weights(self,
                                           self.embedding_dim,
                                           [self.num_embeddings_per_partition],
                                           self.embedding_dim,
                                           self.num_embeddings_padded,
                                           params_dtype=params_dtype,
                                           weight_loader=self.weight_loader)
+        else:
+            self.linear_method.create_weights(self,
+                                          self.num_embeddings_per_partition,
+                                          [self.embedding_dim],
+                                          self.embedding_dim,
+                                          self.num_embeddings_padded,
+                                          params_dtype=params_dtype,
+                                          weight_loader=self.weight_loader)
 
     @classmethod
     def _get_indices(cls, vocab_size_padded: int, org_vocab_size_padded: int,
diff --git a/vllm/model_executor/model_loader/__init__.py b/vllm/model_executor/model_loader/__init__.py
index 9048c70c7..ea7a063d9 100644
--- a/vllm/model_executor/model_loader/__init__.py
+++ b/vllm/model_executor/model_loader/__init__.py
@@ -1,8 +1,11 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from torch import nn
 
 from vllm.config import VllmConfig
+from vllm.distributed.kv_transfer.kv_transfer_utils import (
+    maybe_register_PD_disagg_hooks)
 from vllm.model_executor.model_loader.loader import (BaseModelLoader,
                                                      get_model_loader)
 from vllm.model_executor.model_loader.utils import (
@@ -10,8 +13,14 @@ from vllm.model_executor.model_loader.utils import (
 
 
 def get_model(*, vllm_config: VllmConfig) -> nn.Module:
+
     loader = get_model_loader(vllm_config.load_config)
-    return loader.load_model(vllm_config=vllm_config)
+
+    model = loader.load_model(vllm_config=vllm_config)
+
+    maybe_register_PD_disagg_hooks(model, vllm_config)
+
+    return model
 
 
 __all__ = [
diff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py
index 2a2c2523b..c8f9cb1d9 100644
--- a/vllm/model_executor/model_loader/loader.py
+++ b/vllm/model_executor/model_loader/loader.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # ruff: noqa: SIM117
@@ -153,6 +154,30 @@ def _initialize_model(
         return model_class(**kwargs)
 
 
+def _process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
+                                   target_device: torch.device) -> None:
+    for _, module in model.named_modules():
+        quant_method = getattr(module, "quant_method", None)
+        if isinstance(quant_method, QuantizeMethodBase):
+            # When quant methods need to process weights after loading
+            # (for repacking, quantizing, etc), they expect parameters
+            # to be on the global target device. This scope is for the
+            # case where cpu offloading is used, where we will move the
+            # parameters onto device for processing and back off after.
+            with device_loading_context(module, target_device):
+                quant_method.process_weights_after_loading(module)
+
+    # Currently only used by MLA.
+    # NOTE: This intentionally happens after other modules so we can easily
+    # decompress the weights for MLA.
+    for _, module in model.named_modules():
+        if isinstance(module, Attention) and \
+            hasattr(module, "process_weights_after_loading"):
+            # TODO(lucas): see if there is a way to unify the signatures
+            # of process_weights_after_loading
+            module.process_weights_after_loading(model_config.dtype)
+
+
 class BaseModelLoader(ABC):
     """Base class for model loaders."""
 
@@ -394,23 +419,8 @@ class DefaultModelLoader(BaseModelLoader):
                         "Following weights were not initialized from "
                         f"checkpoint: {weights_not_loaded}")
 
-            for _, module in model.named_modules():
-                quant_method = getattr(module, "quant_method", None)
-                if isinstance(quant_method, QuantizeMethodBase):
-                    # When quant methods need to process weights after loading
-                    # (for repacking, quantizing, etc), they expect parameters
-                    # to be on the global target device. This scope is for the
-                    # case where cpu offloading is used, where we will move the
-                    # parameters onto device for processing and back off after.
-                    with device_loading_context(module, target_device):
-                        quant_method.process_weights_after_loading(module)
-                if isinstance(module, Attention) and \
-                    hasattr(module, "process_weights_after_loading"):
-                    # When attention modules need to process weights after
-                    # currently only used by MLA
-                    # TODO(lucas): see if there is a way to unify the signatures
-                    # of process_weights_after_loading
-                    module.process_weights_after_loading(model_config.dtype)
+            _process_weights_after_loading(model, model_config, target_device)
+
         return model.eval()
 
 
@@ -429,29 +439,15 @@ class DummyModelLoader(BaseModelLoader):
     def load_model(self, vllm_config: VllmConfig) -> nn.Module:
         device_config = vllm_config.device_config
         model_config = vllm_config.model_config
+        target_device = torch.device(device_config.device)
         with set_default_torch_dtype(model_config.dtype):
-            with torch.device(device_config.device):
+            with target_device:
                 model = _initialize_model(vllm_config=vllm_config)
             # NOTE(woosuk): For accurate performance evaluation, we assign
             # random values to the weights.
             initialize_dummy_weights(model)
 
-            for _, module in model.named_modules():
-                quant_method = getattr(module, "quant_method", None)
-                if quant_method is not None:
-                    # When quant methods need to process weights after loading
-                    # (for repacking, quantizing, etc), they expect parameters
-                    # to be on the global target device. This scope is for the
-                    # case where cpu offloading is used, where we will move the
-                    # parameters onto device for processing and back off after.
-                    with device_loading_context(
-                            module, torch.device(device_config.device)):
-                        quant_method.process_weights_after_loading(module)
-                if isinstance(module, Attention) and \
-                    hasattr(module, "process_weights_after_loading"):
-                    # When attention modules need to process weights after
-                    # currently only used by MLA
-                    module.process_weights_after_loading(model_config.dtype)
+            _process_weights_after_loading(model, model_config, target_device)
         return model.eval()
 
 
@@ -632,6 +628,7 @@ class ShardedStateLoader(BaseModelLoader):
     def load_model(self, vllm_config: VllmConfig) -> nn.Module:
         device_config = vllm_config.device_config
         model_config = vllm_config.model_config
+        target_device = torch.device(device_config.device)
         from safetensors.torch import safe_open
 
         from vllm.distributed import get_tensor_model_parallel_rank
@@ -640,18 +637,10 @@ class ShardedStateLoader(BaseModelLoader):
                                                  model_config.revision)
 
         with set_default_torch_dtype(model_config.dtype):
-            with torch.device(device_config.device):
+            with target_device:
                 model = _initialize_model(vllm_config=vllm_config)
-                for _, module in model.named_modules():
-                    quant_method = getattr(module, "quant_method", None)
-                    if quant_method is not None:
-                        quant_method.process_weights_after_loading(module)
-                    if isinstance(module, Attention) and \
-                        hasattr(module, "process_weights_after_loading"):
-                        # When attention modules need to process weights after
-                        # currently only used by MLA
-                        module.process_weights_after_loading(
-                            model_config.dtype)
+                _process_weights_after_loading(model, model_config,
+                                               target_device)
             rank = get_tensor_model_parallel_rank()
             pattern = os.path.join(
                 local_model_path,
@@ -1401,16 +1390,7 @@ class RunaiModelStreamerLoader(BaseModelLoader):
                 self._get_weights_iterator(model_weights,
                                            model_config.revision))
 
-            for _, module in model.named_modules():
-                quant_method = getattr(module, "quant_method", None)
-                if quant_method is not None:
-                    with device_loading_context(module, target_device):
-                        quant_method.process_weights_after_loading(module)
-                if isinstance(module, Attention) and \
-                    hasattr(module, "process_weights_after_loading"):
-                    # When attention modules need to process weights after
-                    # currently only used by MLA
-                    module.process_weights_after_loading(model_config.dtype)
+            _process_weights_after_loading(model, model_config, target_device)
         return model.eval()
 
 
diff --git a/vllm/model_executor/models/baichuan_moe.py b/vllm/model_executor/models/baichuan_moe.py
new file mode 100644
index 000000000..a1c7c4679
--- /dev/null
+++ b/vllm/model_executor/models/baichuan_moe.py
@@ -0,0 +1,689 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+import time
+"""Inference-only Baichuan-MOE model."""
+from transformers.configuration_utils import PretrainedConfig
+class BaiChuanMoEConfig(PretrainedConfig):
+    model_type = "baichuan-moe"
+    keys_to_ignore_at_inference = ["past_key_values"]
+
+    def __init__(
+        self,
+        vocab_size=64000,
+        hidden_size=4096,
+        intermediate_size=11008,
+        num_hidden_layers=32,
+        num_attention_heads=32,
+        hidden_act="silu",
+        max_position_embeddings=4096,
+        initializer_range=0.02,
+        rms_norm_eps=1e-6,
+        rope_base=1e6,
+        use_cache=True,
+        pad_token_id=0,
+        bos_token_id=1,
+        eos_token_id=2,
+        tie_word_embeddings=False,
+        moe_experts_fixed=0,
+        moe_experts_selected=2,
+        moe_experts_routed=8,
+        num_experts_fixed_per_layer=None, # "0,0,0,1,0,2"
+        num_experts_selected_per_layer=None, # "1,2,1,1,1,2"
+        num_experts_routed_per_layer=None, # "1,8,1,8,1,16"
+        **kwargs,
+    ):
+        self.vocab_size = vocab_size
+        self.max_position_embeddings = max_position_embeddings
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size
+        self.num_hidden_layers = num_hidden_layers
+        self.num_attention_heads = num_attention_heads
+        self.hidden_act = hidden_act
+        self.initializer_range = initializer_range
+        self.rms_norm_eps = rms_norm_eps
+        self.rope_base = rope_base
+        self.use_cache = use_cache
+        self.moe_experts_fixed = moe_experts_fixed
+        self.moe_experts_selected = moe_experts_selected
+        self.moe_experts_routed = moe_experts_routed
+        if num_experts_routed_per_layer:
+            self.num_experts_routed_per_layer = [int(_.strip()) for _ in num_experts_routed_per_layer.split(",")]
+            assert len(self.num_experts_routed_per_layer) == self.num_hidden_layers
+            assert all([_ >= 1 for _ in self.num_experts_routed_per_layer])
+        else:
+            self.num_experts_routed_per_layer = None
+
+        if num_experts_selected_per_layer:
+            self.num_experts_selected_per_layer = [int(_.strip()) for _ in num_experts_selected_per_layer.split(",")]
+            assert len(self.num_experts_selected_per_layer) == self.num_hidden_layers
+            assert all([x >= y for x, y in zip(self.num_experts_routed_per_layer, self.num_experts_selected_per_layer)])
+        else:
+            self.num_experts_selected_per_layer = None
+
+        if num_experts_fixed_per_layer:
+            self.num_experts_fixed_per_layer = [int(_.strip()) for _ in num_experts_fixed_per_layer.split(",")]
+            assert len(self.num_experts_fixed_per_layer) == self.num_hidden_layers
+        else:
+            self.num_experts_fixed_per_layer = None
+
+        super().__init__(
+            pad_token_id=pad_token_id,
+            bos_token_id=bos_token_id,
+            eos_token_id=eos_token_id,
+            tie_word_embeddings=tie_word_embeddings,
+            **kwargs,
+        )
+
+import math
+import copy
+from typing import List, Optional, Iterable, Tuple
+import torch
+from torch import nn
+import torch.nn.functional as F
+from transformers.activations import ACT2FN
+
+from vllm.attention import Attention, AttentionMetadata
+
+from vllm.config import CacheConfig, LoRAConfig, VllmConfig
+from vllm.model_executor.layers.fused_moe import fused_moe
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (LinearMethodBase,
+                                               ColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               ReplicatedLinear,
+                                               MergedColumnParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.activation import SiluAndMul,GeluAndMul
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import Sampler, SamplerOutput 
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    DEFAULT_VOCAB_PADDING_SIZE, ParallelLMHead, VocabParallelEmbedding)
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.model_executor.utils import set_weight_attrs
+#from vllm.model_executor.weight_utils import (default_weight_loader,
+#                                              hf_model_weights_iterator)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.sequence import IntermediateTensors
+from .interfaces import SupportsPP
+from .utils import PPMissingLayer, is_pp_missing_parameter, make_layers, make_empty_intermediate_tensors_factory, maybe_prefix
+
+class MLP(nn.Module):
+    def __init__(
+            self,
+            hidden_size: int,
+            intermediate_size: int,
+            hidden_act: str,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = ""
+    ):
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size, [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj"
+            )
+        self.down_proj = RowParallelLinear(intermediate_size,
+                                           hidden_size,
+                                           bias=False,
+                                           quant_config=quant_config,
+                                           prefix=f"{prefix}.down_proj",
+                                           )
+        if hidden_act not in ["silu", "gelu"]:
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu and gelu are supported for now.")
+        self.act_fn = SiluAndMul() if hidden_act == "silu" else GeluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        ret, _ = self.down_proj(x)
+
+        return ret
+
+
+class MixtralMLP(nn.Module):
+    """
+    This implementation is
+    strictly equivalent to standard MoE with full capacity (no
+    dropped tokens). It's faster since it formulates MoE operations
+    in terms of block-sparse operations to accomodate imbalanced
+    assignments of tokens to experts, whereas standard MoE either
+    (1) drop tokens at the cost of reduced performance or (2) set
+    capacity factor to number of experts and thus waste computation
+    and memory on padding.
+    """
+
+    def __init__(self,
+                hidden_size,
+                intermediate_size,
+                hidden_act,
+                moe_experts_routed,
+                moe_experts_selected,
+                moe_experts_fixed,
+                quant_config: Optional[QuantizationConfig] = None,
+                params_dtype: Optional[torch.dtype] = None,
+                tp_size: Optional[int] = None,
+                prefix: str = ""):
+        super().__init__()
+        self.tp_size = tp_size or get_tensor_model_parallel_world_size()
+        self.num_experts_routed = moe_experts_routed
+        self.num_local_experts_routed = self.num_experts_routed // 1
+        self.top_k = moe_experts_selected
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size // self.tp_size
+
+
+        if params_dtype is None:
+            params_dtype = torch.get_default_dtype()
+        self.params_dtype = params_dtype
+        self.router = ReplicatedLinear(self.hidden_size,
+                                        self.num_experts_routed,
+                                        bias=False,
+                                        quant_config=quant_config,
+                                        params_dtype=self.params_dtype,
+                                        )
+
+        self.ws = nn.Parameter(
+            torch.empty(self.num_experts_routed,
+                        2 * self.intermediate_size,
+                        self.hidden_size,
+                        device="cuda",
+                        dtype=self.params_dtype))
+        self.w2s = nn.Parameter(
+            torch.empty(self.num_experts_routed,
+                        self.hidden_size,
+                        self.intermediate_size,
+                        device="cuda",
+                        dtype=self.params_dtype))
+        
+        set_weight_attrs(self.ws, {
+            "weight_loader": self.weight_loader,
+        })
+        set_weight_attrs(self.w2s, {
+            "weight_loader": self.weight_loader,
+        })
+
+
+        if moe_experts_fixed >= 1:
+            self.local_experts_fixed = MLP(hidden_size, intermediate_size*moe_experts_fixed, hidden_act, quant_config=quant_config, prefix=f"{prefix}.mlp")
+        else:
+            self.local_experts_fixed = None
+
+    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor,
+                      weight_name: str, expert_id: int):
+        tp_rank = get_tensor_model_parallel_rank()
+        param_data = param.data
+        shard_size = self.intermediate_size
+        shard = slice(tp_rank * shard_size, (tp_rank + 1) * shard_size)
+        if weight_name.endswith("gate_proj.weight"):
+            param_data[expert_id, 0:shard_size, :] = loaded_weight[shard, :]
+        if weight_name.endswith("up_proj.weight"):
+            param_data[expert_id, shard_size:2 * shard_size, :] = loaded_weight[shard, :]
+        if weight_name.endswith("down_proj.weight"):
+            param_data[expert_id, :, :] = loaded_weight[:, shard]
+
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        """ """
+        num_tokens, hidden_size = hidden_states.shape
+        hidden_states = hidden_states.view(-1, self.hidden_size)
+        router_logits, _ = self.router(hidden_states)
+        final_hidden_states = fused_moe(hidden_states,
+                                        self.ws,
+                                        self.w2s,
+                                        router_logits,
+                                        self.top_k,
+                                        renormalize=True)
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+        
+        final_hidden_states = final_hidden_states.view(num_tokens, hidden_size)
+        
+        if self.local_experts_fixed:
+            final_hidden_states += self.local_experts_fixed(hidden_states).reshape(num_tokens, hidden_size)
+            final_hidden_states /= 2
+        
+        ret = final_hidden_states.reshape(num_tokens, hidden_size)
+        return ret
+
+
+class MixtralAttention(nn.Module):
+
+    def __init__(self,
+                 hidden_size: int,
+                 num_heads: int,
+                 num_kv_heads: int,
+                 max_position: int = 4096 * 32,
+                 rope_theta: float = 10000,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = "",) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = hidden_size // self.total_num_heads
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+
+        self.W_pack = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.o_proj",
+        )
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position,
+            base=int(self.rope_theta),
+            is_neox_style=True,
+        )
+        self.attn = Attention(
+            self.num_heads,
+            self.head_dim,
+            self.scaling,
+            num_kv_heads=self.num_kv_heads,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.attn"
+        )
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+    ) -> torch.Tensor:
+        qkv, _ = self.W_pack(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class DecoderLayer(nn.Module):
+    def __init__(
+        self,
+        config: BaiChuanMoEConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = ""
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        # Requires transformers > 4.32.0
+        rope_theta = getattr(config, "rope_base", 10000)
+        self.self_attn = MixtralAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_attention_heads,
+            rope_theta=rope_theta,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn")
+        
+        
+        # Dense
+        if config.moe_experts_routed == 1:
+            self.mlp = MLP(hidden_size=config.hidden_size,
+                            intermediate_size=config.intermediate_size,
+                            hidden_act=config.hidden_act, quant_config=quant_config,
+                            prefix=f"{prefix}.mlp")
+        # MoE
+        else:
+            self.mlp = MixtralMLP(config.hidden_size,
+                                    config.intermediate_size,
+                                    config.hidden_act,
+                                    config.moe_experts_routed,
+                                    config.moe_experts_selected,
+                                    config.moe_experts_fixed,
+                                    quant_config=quant_config,
+                                    prefix=f"{prefix}.mlp")
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+    
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+        residual: Optional[torch.Tensor],
+    ) -> torch.Tensor:
+
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+            kv_cache=kv_cache,
+            attn_metadata=attn_metadata,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        
+        hidden_states = self.mlp(hidden_states)
+
+        return hidden_states, residual
+
+def layer_function(prefix, config, cache_config, quant_config):
+    index = int(prefix.split(".")[-1])  
+    config_ = copy.deepcopy(config)
+
+    config_.moe_experts_fixed = config.num_experts_fixed_per_layer[index]
+    config_.moe_experts_selected = config.num_experts_selected_per_layer[index]
+    config_.moe_experts_routed = config.num_experts_routed_per_layer[index]
+
+    return DecoderLayer(config=config_, cache_config=cache_config, quant_config=quant_config, prefix=prefix)
+
+class Model(nn.Module):
+    def __init__(self, vllm_config: VllmConfig, prefix: str = "") -> None:
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.padding_idx = config.pad_token_id
+        lora_vocab = (lora_config.lora_extra_vocab_size *
+                      (lora_config.max_loras or 1)) if lora_config else 0
+        self.vocab_size = config.vocab_size + lora_vocab
+        self.org_vocab_size = config.vocab_size
+
+        if get_pp_group().is_first_rank:
+            self.embed_tokens = VocabParallelEmbedding(
+                self.vocab_size,
+                config.hidden_size,
+                org_num_embeddings=config.vocab_size,
+            )
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        if config.num_experts_routed_per_layer:
+            self.start_layer, self.end_layer, self.layers = make_layers(
+                num_hidden_layers=config.num_hidden_layers,
+                layer_fn=lambda prefix: layer_function(prefix, config, cache_config, quant_config),
+                prefix=f"{prefix}.layers",  
+            )
+        else:
+            self.start_layer, self.end_layer, self.layers = make_layers(
+                num_hidden_layers=config.num_hidden_layers,
+                layer_fn = lambda prefix: DecoderLayer(
+                    config=config,
+                    cache_config=cache_config,
+                    quant_config=quant_config,
+                    prefix=prefix
+                ),
+                prefix=f"{prefix}.layers",  
+            )
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors],
+    ) -> torch.Tensor:
+        if get_pp_group().is_first_rank:
+            hidden_states = self.embed_tokens(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(
+                positions,
+                hidden_states,
+                kv_caches[i - self.start_layer],
+                attn_metadata,
+                residual,
+            )
+        
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        
+        hidden_states, _ = self.norm(hidden_states, residual)
+        
+        return hidden_states
+
+class NormHead(nn.Module):
+    def __init__(self, hidden_size, vocab_size, bias=False):
+        super().__init__()
+        self.weight = nn.Parameter(torch.empty((vocab_size, hidden_size)))
+        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
+        self.norm_weight = nn.functional.normalize(self.weight)
+
+    def forward(self, hidden_states):
+        return nn.functional.linear(hidden_states, self.norm_weight)
+
+class BaiChuanMoEForCausalLM(nn.Module, SupportsPP):
+    # packed_modules_mapping = {
+    #     "qkv_proj": [
+    #         "q_proj",
+    #         "k_proj",
+    #         "v_proj",
+    #     ],
+    # }
+
+    # # LoRA specific attributes
+    # supported_lora_modules = [
+    #     "qkv_proj",
+    #     "o_proj",
+    #     "embed_tokens",
+    #     "lm_head",
+    # ]
+    embedding_modules = {
+        "embed_tokens": "input_embeddings",
+        "lm_head": "output_embeddings",
+    }
+    embedding_padding_modules = ["lm_head"]
+
+    def __init__(self, vllm_config: VllmConfig, prefix: str = "") -> None:
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.quant_config = quant_config
+        self.model = Model(vllm_config=vllm_config, prefix=maybe_prefix(prefix, "model"))
+        if get_pp_group().is_last_rank:
+            self.unpadded_vocab_size = config.vocab_size
+            if lora_config:
+                self.unpadded_vocab_size += lora_config.lora_extra_vocab_size
+            self.lm_head = ParallelLMHead(
+                self.unpadded_vocab_size,
+                config.hidden_size,
+                quant_config=quant_config,
+                org_num_embeddings=config.vocab_size,
+                padding_size=DEFAULT_VOCAB_PADDING_SIZE
+                # We need bigger padding if using lora for kernel
+                # compatibility
+                if not lora_config else lora_config.lora_vocab_padding_size,
+                prefix=maybe_prefix(prefix, "lm_head")
+            )
+            self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,
+                                                config.vocab_size)
+            self.sampler = Sampler()
+        else:
+            self.lm_head = PPMissingLayer()
+    
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None
+    ) -> torch.Tensor:
+        hidden_states = self.model(input_ids, positions, kv_caches,
+                                   attn_metadata, intermediate_tensors)
+        return hidden_states
+
+    def compute_logits(self, hidden_states: torch.Tensor,
+                       sampling_metadata: SamplingMetadata) -> torch.Tensor:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+    
+    def make_empty_intermediate_tensors(
+            self, batch_size: int, dtype: torch.dtype,
+            device: torch.device) -> IntermediateTensors:
+        return IntermediateTensors({
+            "hidden_states":
+            torch.zeros((batch_size, self.config.hidden_size),
+                        dtype=dtype,
+                        device=device),
+            "residual":
+            torch.zeros((batch_size, self.config.hidden_size),
+                        dtype=dtype,
+                        device=device),
+        })
+
+    def sample(
+        self,
+        logits: Optional[torch.Tensor],
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("mlp.gate_up_proj", "mlp.gate_proj", 0),
+            ("mlp.gate_up_proj", "mlp.up_proj", 1),
+            ("mlp.local_experts_fixed.gate_up_proj", "mlp.local_experts_fixed.gate_proj", 0),
+            ("mlp.local_experts_fixed.gate_up_proj", "mlp.local_experts_fixed.up_proj", 1),
+        ]
+
+        expert_params_mapping = [
+            # (param_name, weight_name, expert_id)
+            ("ws" if weight_name in ["gate_proj", "up_proj"] else "w2s",
+             f"local_experts_routed.{expert_id}.{weight_name}.weight", expert_id)
+            for expert_id in range(16)
+            for weight_name in ["gate_proj", "down_proj", "up_proj"]
+        ]
+
+        params_dict = dict(self.named_parameters())
+        
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for param_name, weight_name, expert_id in expert_params_mapping:
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  weight_name,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    # Skip loading extra bias for GPTQ models.
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict.get(name, None)
+
+                    if name == "lm_head.weight":
+                        # do norm
+                        norm_weight = nn.functional.normalize(loaded_weight)
+                        weight_loader = getattr(param, "weight_loader",
+                                                default_weight_loader)
+                        weight_loader(param, norm_weight)
+                    else:
+                        weight_loader = getattr(param, "weight_loader",
+                                                default_weight_loader)
+                        weight_loader(param, loaded_weight)
+
diff --git a/vllm/model_executor/models/deepseek.py b/vllm/model_executor/models/deepseek.py
index 9599e1df6..9f3403bcc 100644
--- a/vllm/model_executor/models/deepseek.py
+++ b/vllm/model_executor/models/deepseek.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Adapted from
@@ -144,14 +145,20 @@ class DeepseekMoE(nn.Module):
         w1s = torch._utils._unflatten_dense_tensors(self.w1, w1)
         for data, param in zip(w1s, w1):
             param.data = data
-        self.w1 = self.w1.view(len(w1), *w1s[0].shape)
+        #self.w1 = self.w1.view(len(w1), *w1s[0].shape)
+        self.w1 = self.w1.view(len(w1), *w1s[0].shape).permute(0, 2, 1).contiguous()
+        for expert, w in zip(self.experts, self.w1):
+            expert.gate_up_proj.weight.data = w.permute(1, 0)
 
         self.w2 = torch._utils._flatten_dense_tensors(w2)
         w2s = torch._utils._unflatten_dense_tensors(self.w2, w2)
         for data, param in zip(w2s, w2):
             param.data = data
 
-        self.w2 = self.w2.view(len(w2), *w2s[0].shape)
+        #self.w2 = self.w2.view(len(w2), *w2s[0].shape)
+        self.w2 = self.w2.view(len(w2), *w2s[0].shape).permute(0, 2, 1).contiguous()
+        for expert, w in zip(self.experts, self.w2):
+            expert.down_proj.weight.data = w.permute(1, 0)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         num_tokens, hidden_dim = hidden_states.shape
diff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py
index 773f5abe7..45c1de169 100644
--- a/vllm/model_executor/models/deepseek_v2.py
+++ b/vllm/model_executor/models/deepseek_v2.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Adapted from
@@ -664,9 +665,12 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP):
         self.quant_config = quant_config
         self.model = DeepseekV2Model(vllm_config=vllm_config,
                                      prefix=maybe_prefix(prefix, "model"))
-        self.lm_head = ParallelLMHead(config.vocab_size,
-                                      config.hidden_size,
-                                      quant_config=quant_config)
+        if get_pp_group().is_last_rank:
+            self.lm_head = ParallelLMHead(config.vocab_size,
+                                          config.hidden_size,
+                                          quant_config=quant_config)
+        else:
+            self.lm_head = PPMissingLayer()
         self.logits_processor = LogitsProcessor(config.vocab_size)
         self.sampler = get_sampler()
         self.make_empty_intermediate_tensors = (
diff --git a/vllm/model_executor/models/qwen.py b/vllm/model_executor/models/qwen.py
index 897066124..075729e43 100644
--- a/vllm/model_executor/models/qwen.py
+++ b/vllm/model_executor/models/qwen.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 # Adapted from
@@ -724,8 +725,12 @@ class QWenVLProcessor:
 
         special_tokens: dict[str,
                              int] = tokenizer.special_tokens  # type: ignore
-        self.img_start_id = special_tokens[IMG_START]
-        self.img_end_id = special_tokens[IMG_END]
+        if IMG_START in special_tokens:
+            self.img_start_id = special_tokens[IMG_START]
+            self.img_end_id = special_tokens[IMG_END]
+        else:
+            self.img_start_id = None
+            self.img_end_id = None
 
     def __call__(
         self,
@@ -873,9 +878,14 @@ class QWenVLMultiModalProcessor(BaseMultiModalProcessor[QWenVLProcessingInfo]):
         special_tokens: dict[str,
                              int] = tokenizer.special_tokens  # type: ignore
 
-        img_start_id = special_tokens[IMG_START]
-        img_end_id = special_tokens[IMG_END]
-        img_pad_id = special_tokens[IMG_PAD]
+        if IMG_START in special_tokens:
+            img_start_id = special_tokens[IMG_START]
+            img_end_id = special_tokens[IMG_END]
+            img_pad_id = special_tokens[IMG_PAD]
+        else:
+            img_start_id = None
+            img_end_id = None
+            img_pad_id = None
 
         num_image_tokens = self.info.get_num_image_tokens()
         image_tokens = [img_pad_id] * num_image_tokens
diff --git a/vllm/model_executor/models/qwen2.py b/vllm/model_executor/models/qwen2.py
index e3de6b64f..9c1821941 100644
--- a/vllm/model_executor/models/qwen2.py
+++ b/vllm/model_executor/models/qwen2.py
@@ -1,594 +1,600 @@
-# SPDX-License-Identifier: Apache-2.0
-
-# Adapted from
-# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/qwen2/modeling_qwen2.py
-# Copyright 2024 The Qwen team.
-# Copyright 2023 The vLLM team.
-# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
-#
-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
-# and OPT implementations in this library. It has been modified from its
-# original forms to accommodate minor architectural differences compared
-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-"""Inference-only Qwen2 model compatible with HuggingFace weights."""
-from typing import Iterable, List, Optional, Set, Tuple, Union
-
-import torch
-from torch import nn
-from transformers import Qwen2Config
-
-from vllm.attention import Attention, AttentionMetadata, AttentionType
-from vllm.compilation.decorators import support_torch_compile
-from vllm.config import CacheConfig, VllmConfig
-from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
-from vllm.logger import init_logger
-from vllm.model_executor.layers.activation import SiluAndMul
-from vllm.model_executor.layers.layernorm import RMSNorm
-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
-                                               QKVParallelLinear,
-                                               RowParallelLinear)
-from vllm.model_executor.layers.logits_processor import LogitsProcessor
-from vllm.model_executor.layers.pooler import Pooler, PoolingType
-from vllm.model_executor.layers.quantization import QuantizationConfig
-from vllm.model_executor.layers.rotary_embedding import get_rope
-from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
-from vllm.model_executor.layers.vocab_parallel_embedding import (
-    ParallelLMHead, VocabParallelEmbedding)
-from vllm.model_executor.model_loader.weight_utils import (
-    default_weight_loader, maybe_remap_kv_scale_name)
-from vllm.model_executor.pooling_metadata import PoolingMetadata
-from vllm.model_executor.sampling_metadata import SamplingMetadata
-from vllm.sequence import IntermediateTensors, PoolerOutput
-
-from .interfaces import SupportsLoRA, SupportsPP
-from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
-                    is_pp_missing_parameter,
-                    make_empty_intermediate_tensors_factory, make_layers,
-                    maybe_prefix)
-
-logger = init_logger(__name__)
-
-
-class Qwen2MLP(nn.Module):
-
-    def __init__(
-        self,
-        hidden_size: int,
-        intermediate_size: int,
-        hidden_act: str,
-        quant_config: Optional[QuantizationConfig] = None,
-        prefix: str = "",
-    ) -> None:
-        super().__init__()
-        self.gate_up_proj = MergedColumnParallelLinear(
-            hidden_size,
-            [intermediate_size] * 2,
-            bias=False,
-            quant_config=quant_config,
-            prefix=f"{prefix}.gate_up_proj",
-        )
-        self.down_proj = RowParallelLinear(
-            intermediate_size,
-            hidden_size,
-            bias=False,
-            quant_config=quant_config,
-            prefix=f"{prefix}.down_proj",
-        )
-        if hidden_act != "silu":
-            raise ValueError(f"Unsupported activation: {hidden_act}. "
-                             "Only silu is supported for now.")
-        self.act_fn = SiluAndMul()
-
-    def forward(self, x):
-        gate_up, _ = self.gate_up_proj(x)
-        x = self.act_fn(gate_up)
-        x, _ = self.down_proj(x)
-        return x
-
-
-class Qwen2Attention(nn.Module):
-
-    def __init__(self,
-                 hidden_size: int,
-                 num_heads: int,
-                 num_kv_heads: int,
-                 max_position: int = 4096 * 32,
-                 rope_theta: float = 10000,
-                 cache_config: Optional[CacheConfig] = None,
-                 quant_config: Optional[QuantizationConfig] = None,
-                 rope_scaling: Optional[Tuple] = None,
-                 prefix: str = "",
-                 attn_type: str = AttentionType.DECODER) -> None:
-        super().__init__()
-        self.hidden_size = hidden_size
-        tp_size = get_tensor_model_parallel_world_size()
-        self.total_num_heads = num_heads
-        assert self.total_num_heads % tp_size == 0
-        self.num_heads = self.total_num_heads // tp_size
-        self.total_num_kv_heads = num_kv_heads
-        if self.total_num_kv_heads >= tp_size:
-            # Number of KV heads is greater than TP size, so we partition
-            # the KV heads across multiple tensor parallel GPUs.
-            assert self.total_num_kv_heads % tp_size == 0
-        else:
-            # Number of KV heads is less than TP size, so we replicate
-            # the KV heads across multiple tensor parallel GPUs.
-            assert tp_size % self.total_num_kv_heads == 0
-        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
-        self.head_dim = hidden_size // self.total_num_heads
-        self.q_size = self.num_heads * self.head_dim
-        self.kv_size = self.num_kv_heads * self.head_dim
-        self.scaling = self.head_dim**-0.5
-        self.rope_theta = rope_theta
-
-        self.qkv_proj = QKVParallelLinear(
-            hidden_size,
-            self.head_dim,
-            self.total_num_heads,
-            self.total_num_kv_heads,
-            bias=True,
-            quant_config=quant_config,
-            prefix=f"{prefix}.qkv_proj",
-        )
-        self.o_proj = RowParallelLinear(
-            self.total_num_heads * self.head_dim,
-            hidden_size,
-            bias=False,
-            quant_config=quant_config,
-            prefix=f"{prefix}.o_proj",
-        )
-
-        self.rotary_emb = get_rope(
-            self.head_dim,
-            rotary_dim=self.head_dim,
-            max_position=max_position,
-            base=self.rope_theta,
-            rope_scaling=rope_scaling,
-        )
-        self.attn = Attention(self.num_heads,
-                              self.head_dim,
-                              self.scaling,
-                              num_kv_heads=self.num_kv_heads,
-                              cache_config=cache_config,
-                              quant_config=quant_config,
-                              prefix=f"{prefix}.attn",
-                              attn_type=attn_type)
-
-    def forward(
-        self,
-        positions: torch.Tensor,
-        hidden_states: torch.Tensor,
-        kv_cache: torch.Tensor,
-        attn_metadata: AttentionMetadata,
-    ) -> torch.Tensor:
-        qkv, _ = self.qkv_proj(hidden_states)
-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
-        q, k = self.rotary_emb(positions, q, k)
-        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
-        output, _ = self.o_proj(attn_output)
-        return output
-
-
-class Qwen2DecoderLayer(nn.Module):
-
-    def __init__(
-        self,
-        config: Qwen2Config,
-        cache_config: Optional[CacheConfig] = None,
-        quant_config: Optional[QuantizationConfig] = None,
-        prefix: str = "",
-    ) -> None:
-        super().__init__()
-        self.hidden_size = config.hidden_size
-        # Requires transformers > 4.32.0
-        rope_theta = getattr(config, "rope_theta", 1000000)
-        rope_scaling = getattr(config, "rope_scaling", None)
-
-        # By default, Qwen2 uses causal attention as it is a decoder-only model.
-        # You can override the HF config with `is_causal=False` to enable
-        # bidirectional attention, which is used in some embedding models
-        # (e.g. Alibaba-NLP/gte-Qwen2-7B-instruct)
-        if getattr(config, "is_causal", True):
-            attn_type = AttentionType.DECODER
-        else:
-            attn_type = AttentionType.ENCODER_ONLY
-
-        self.self_attn = Qwen2Attention(
-            hidden_size=self.hidden_size,
-            num_heads=config.num_attention_heads,
-            max_position=config.max_position_embeddings,
-            num_kv_heads=config.num_key_value_heads,
-            rope_theta=rope_theta,
-            cache_config=cache_config,
-            quant_config=quant_config,
-            rope_scaling=rope_scaling,
-            prefix=f"{prefix}.self_attn",
-            attn_type=attn_type,
-        )
-        self.mlp = Qwen2MLP(
-            hidden_size=self.hidden_size,
-            intermediate_size=config.intermediate_size,
-            hidden_act=config.hidden_act,
-            quant_config=quant_config,
-            prefix=f"{prefix}.mlp",
-        )
-        self.input_layernorm = RMSNorm(config.hidden_size,
-                                       eps=config.rms_norm_eps)
-        self.post_attention_layernorm = RMSNorm(config.hidden_size,
-                                                eps=config.rms_norm_eps)
-
-    def forward(
-        self,
-        positions: torch.Tensor,
-        hidden_states: torch.Tensor,
-        kv_cache: torch.Tensor,
-        attn_metadata: AttentionMetadata,
-        residual: Optional[torch.Tensor],
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        # Self Attention
-        if residual is None:
-            residual = hidden_states
-            hidden_states = self.input_layernorm(hidden_states)
-        else:
-            hidden_states, residual = self.input_layernorm(
-                hidden_states, residual)
-        hidden_states = self.self_attn(
-            positions=positions,
-            hidden_states=hidden_states,
-            kv_cache=kv_cache,
-            attn_metadata=attn_metadata,
-        )
-
-        # Fully Connected
-        hidden_states, residual = self.post_attention_layernorm(
-            hidden_states, residual)
-        hidden_states = self.mlp(hidden_states)
-        return hidden_states, residual
-
-
-@support_torch_compile(
-    dynamic_arg_dims={
-        "input_ids": 0,
-        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
-        # otherwise (seq_len, ).
-        "positions": -1,
-        "intermediate_tensors": 0,
-        "inputs_embeds": 0,
-    })
-class Qwen2Model(nn.Module):
-
-    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-        super().__init__()
-
-        config = vllm_config.model_config.hf_config
-        cache_config = vllm_config.cache_config
-        quant_config = vllm_config.quant_config
-
-        # TODO (@robertgshaw2): see if this can be moved out
-        if (cache_config.sliding_window is not None
-                and hasattr(config, "max_window_layers")):
-            raise ValueError("Sliding window for some but all layers is not "
-                             "supported. This model uses sliding window "
-                             "but `max_window_layers` = {} is less than "
-                             "`num_hidden_layers` = {}. Please open an issue "
-                             "to discuss this feature.".format(
-                                 config.max_window_layers,
-                                 config.num_hidden_layers,
-                             ))
-
-        self.config = config
-        self.quant_config = quant_config
-        self.padding_idx = config.pad_token_id
-        self.vocab_size = config.vocab_size
-
-        if get_pp_group().is_first_rank or (config.tie_word_embeddings
-                                            and get_pp_group().is_last_rank):
-            self.embed_tokens = VocabParallelEmbedding(
-                config.vocab_size,
-                config.hidden_size,
-                quant_config=quant_config,
-                prefix=f"{prefix}.embed_tokens",
-            )
-        else:
-            self.embed_tokens = PPMissingLayer()
-
-        self.start_layer, self.end_layer, self.layers = make_layers(
-            config.num_hidden_layers,
-            lambda prefix: Qwen2DecoderLayer(config=config,
-                                             cache_config=cache_config,
-                                             quant_config=quant_config,
-                                             prefix=prefix),
-            prefix=f"{prefix}.layers",
-        )
-
-        self.make_empty_intermediate_tensors = (
-            make_empty_intermediate_tensors_factory(
-                ["hidden_states", "residual"], config.hidden_size))
-        if get_pp_group().is_last_rank:
-            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
-        else:
-            self.norm = PPMissingLayer()
-
-    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-        return self.embed_tokens(input_ids)
-
-    def forward(
-        self,
-        input_ids: torch.Tensor,
-        positions: torch.Tensor,
-        kv_caches: List[torch.Tensor],
-        attn_metadata: AttentionMetadata,
-        intermediate_tensors: Optional[IntermediateTensors] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-    ) -> Union[torch.Tensor, IntermediateTensors]:
-        if get_pp_group().is_first_rank:
-            if inputs_embeds is not None:
-                hidden_states = inputs_embeds
-            else:
-                hidden_states = self.get_input_embeddings(input_ids)
-            residual = None
-        else:
-            assert intermediate_tensors is not None
-            hidden_states = intermediate_tensors["hidden_states"]
-            residual = intermediate_tensors["residual"]
-        for i in range(self.start_layer, self.end_layer):
-            layer = self.layers[i]
-            hidden_states, residual = layer(
-                positions,
-                hidden_states,
-                kv_caches[i - self.start_layer],
-                attn_metadata,
-                residual,
-            )
-        if not get_pp_group().is_last_rank:
-            return IntermediateTensors({
-                "hidden_states": hidden_states,
-                "residual": residual
-            })
-        hidden_states, _ = self.norm(hidden_states, residual)
-        return hidden_states
-
-    def load_weights(self, weights: Iterable[Tuple[str,
-                                                   torch.Tensor]]) -> Set[str]:
-        stacked_params_mapping = [
-            # (param_name, shard_name, shard_id)
-            ("qkv_proj", "q_proj", "q"),
-            ("qkv_proj", "k_proj", "k"),
-            ("qkv_proj", "v_proj", "v"),
-            ("gate_up_proj", "gate_proj", 0),
-            ("gate_up_proj", "up_proj", 1),
-        ]
-        params_dict = dict(self.named_parameters(remove_duplicate=False))
-        loaded_params: Set[str] = set()
-        for name, loaded_weight in weights:
-            if "rotary_emb.inv_freq" in name:
-                continue
-            if (self.quant_config is not None and
-                (scale_name := self.quant_config.get_cache_scale(name))):
-                # Loading kv cache quantization scales
-                param = params_dict[scale_name]
-                weight_loader = getattr(param, "weight_loader",
-                                        default_weight_loader)
-                loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
-                                 loaded_weight[0])
-                weight_loader(param, loaded_weight)
-                loaded_params.add(scale_name)
-                continue
-            for (param_name, weight_name, shard_id) in stacked_params_mapping:
-                if weight_name not in name:
-                    continue
-                name = name.replace(weight_name, param_name)
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
-                    continue
-                if is_pp_missing_parameter(name, self):
-                    continue
-                param = params_dict[name]
-                weight_loader = param.weight_loader
-                weight_loader(param, loaded_weight, shard_id)
-                break
-            else:
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
-                    continue
-                # Remapping the name of FP8 kv-scale.
-                name = maybe_remap_kv_scale_name(name, params_dict)
-                if name is None:
-                    continue
-                if is_pp_missing_parameter(name, self):
-                    continue
-                param = params_dict[name]
-                weight_loader = getattr(param, "weight_loader",
-                                        default_weight_loader)
-                weight_loader(param, loaded_weight)
-            loaded_params.add(name)
-        return loaded_params
-
-
-class Qwen2ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
-    packed_modules_mapping = {
-        "qkv_proj": [
-            "q_proj",
-            "k_proj",
-            "v_proj",
-        ],
-        "gate_up_proj": [
-            "gate_proj",
-            "up_proj",
-        ],
-    }
-
-    # LoRA specific attributes
-    supported_lora_modules = [
-        "qkv_proj",
-        "o_proj",
-        "gate_up_proj",
-        "down_proj",
-    ]
-    embedding_modules = {}
-    embedding_padding_modules = []
-
-    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-        super().__init__()
-        config = vllm_config.model_config.hf_config
-        quant_config = vllm_config.quant_config
-        lora_config = vllm_config.lora_config
-
-        self.config = config
-        self.lora_config = lora_config
-
-        self.quant_config = quant_config
-        self.model = Qwen2Model(vllm_config=vllm_config,
-                                prefix=maybe_prefix(prefix, "model"))
-
-        if get_pp_group().is_last_rank:
-            if config.tie_word_embeddings:
-                self.lm_head = self.model.embed_tokens
-            else:
-                self.lm_head = ParallelLMHead(config.vocab_size,
-                                              config.hidden_size,
-                                              quant_config=quant_config,
-                                              prefix=maybe_prefix(
-                                                  prefix, "lm_head"))
-        else:
-            self.lm_head = PPMissingLayer()
-
-        self.logits_processor = LogitsProcessor(config.vocab_size)
-        self.sampler = get_sampler()
-
-        self.make_empty_intermediate_tensors = (
-            self.model.make_empty_intermediate_tensors)
-
-    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
-        return self.model.get_input_embeddings(input_ids)
-
-    def forward(
-        self,
-        input_ids: torch.Tensor,
-        positions: torch.Tensor,
-        kv_caches: List[torch.Tensor],
-        attn_metadata: AttentionMetadata,
-        intermediate_tensors: Optional[IntermediateTensors] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-    ) -> Union[torch.Tensor, IntermediateTensors]:
-        hidden_states = self.model(input_ids, positions, kv_caches,
-                                   attn_metadata, intermediate_tensors,
-                                   inputs_embeds)
-        return hidden_states
-
-    def compute_logits(
-        self,
-        hidden_states: torch.Tensor,
-        sampling_metadata: SamplingMetadata,
-    ) -> Optional[torch.Tensor]:
-        logits = self.logits_processor(self.lm_head, hidden_states,
-                                       sampling_metadata)
-        return logits
-
-    def sample(
-        self,
-        logits: torch.Tensor,
-        sampling_metadata: SamplingMetadata,
-    ) -> Optional[SamplerOutput]:
-        next_tokens = self.sampler(logits, sampling_metadata)
-        return next_tokens
-
-    def load_weights(self, weights: Iterable[Tuple[str,
-                                                   torch.Tensor]]) -> Set[str]:
-        loader = AutoWeightsLoader(
-            self,
-            skip_prefixes=(["lm_head."]
-                           if self.config.tie_word_embeddings else None),
-        )
-        return loader.load_weights(weights)
-
-
-class Qwen2EmbeddingModel(nn.Module, SupportsLoRA, SupportsPP):
-    packed_modules_mapping = {
-        "qkv_proj": [
-            "q_proj",
-            "k_proj",
-            "v_proj",
-        ],
-        "gate_up_proj": [
-            "gate_proj",
-            "up_proj",
-        ],
-    }
-
-    # LoRA specific attributes
-    supported_lora_modules = [
-        "qkv_proj",
-        "o_proj",
-        "gate_up_proj",
-        "down_proj",
-    ]
-    embedding_modules = {}
-    embedding_padding_modules = []
-
-    hf_to_vllm_mapper = WeightsMapper(orig_to_new_prefix={"model.": ""})
-
-    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
-        super().__init__()
-        config = vllm_config.model_config.hf_config
-        quant_config = vllm_config.quant_config
-        lora_config = vllm_config.lora_config
-        pooler_config = vllm_config.model_config.pooler_config
-
-        self.config = config
-        self.lora_config = lora_config
-
-        self.quant_config = quant_config
-        self.model = Qwen2Model(vllm_config=vllm_config,
-                                prefix=maybe_prefix(prefix, "model"))
-
-        # TODO: Replace this model class with as_embedding_model(
-        # Qwen2ForCausalLM) after changing the default pooling method
-        if pooler_config.pooling_type is None:
-            logger.warning(
-                "This embedding model will default to last-token pooling in "
-                "an upcoming version. To avoid breaking changes, you should "
-                "pass `--override-pooler-config '{\"pooling_type\": \"MEAN\"}'`"
-                " explicitly.")
-
-        self._pooler = Pooler.from_config_with_defaults(
-            pooler_config,
-            pooling_type=PoolingType.MEAN,
-            normalize=True,
-            softmax=False)
-
-    def forward(
-        self,
-        input_ids: torch.Tensor,
-        positions: torch.Tensor,
-        kv_caches: List[torch.Tensor],
-        attn_metadata: AttentionMetadata,
-        intermediate_tensors: Optional[IntermediateTensors] = None,
-    ) -> torch.Tensor:
-        return self.model(input_ids, positions, kv_caches, attn_metadata,
-                          intermediate_tensors)
-
-    def pooler(
-        self,
-        hidden_states: torch.Tensor,
-        pooling_metadata: PoolingMetadata,
-    ) -> Optional[PoolerOutput]:
-        return self._pooler(hidden_states, pooling_metadata)
-
-    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
-        weights = self.hf_to_vllm_mapper.apply(weights)
-        weights = ((name, data) for name, data in weights
-                   if not name.startswith("lm_head."))
-        self.model.load_weights(weights)
+# SPDX-License-Identifier: Apache-2.0
+
+# Adapted from
+# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/qwen2/modeling_qwen2.py
+# Copyright 2024 The Qwen team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only Qwen2 model compatible with HuggingFace weights."""
+from typing import Iterable, List, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+from transformers import Qwen2Config
+
+from vllm.attention import Attention, AttentionMetadata, AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
+from vllm.logger import init_logger
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.pooler import Pooler, PoolingType
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.pooling_metadata import PoolingMetadata
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors, PoolerOutput
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
+                    is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+logger = init_logger(__name__)
+
+
+class Qwen2MLP(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        intermediate_size: int,
+        hidden_act: str,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size,
+            [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj",
+        )
+        self.down_proj = RowParallelLinear(
+            intermediate_size,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.down_proj",
+        )
+        if hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu is supported for now.")
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class Qwen2Attention(nn.Module):
+
+    def __init__(self,
+                 hidden_size: int,
+                 num_heads: int,
+                 num_kv_heads: int,
+                 max_position: int = 4096 * 32,
+                 rope_theta: float = 10000,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 rope_scaling: Optional[Tuple] = None,
+                 prefix: str = "",
+                 attn_type: str = AttentionType.DECODER) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = hidden_size // self.total_num_heads
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=True,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.o_proj",
+        )
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position,
+            base=self.rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn",
+                              attn_type=attn_type)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Qwen2DecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: Qwen2Config,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        # Requires transformers > 4.32.0
+        rope_theta = getattr(config, "rope_theta", 1000000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        # By default, Qwen2 uses causal attention as it is a decoder-only model.
+        # You can override the HF config with `is_causal=False` to enable
+        # bidirectional attention, which is used in some embedding models
+        # (e.g. Alibaba-NLP/gte-Qwen2-7B-instruct)
+        if getattr(config, "is_causal", True):
+            attn_type = AttentionType.DECODER
+        else:
+            attn_type = AttentionType.ENCODER_ONLY
+
+        self.self_attn = Qwen2Attention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn",
+            attn_type=attn_type,
+        )
+        self.mlp = Qwen2MLP(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mlp",
+        )
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+        residual: Optional[torch.Tensor],
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+            kv_cache=kv_cache,
+            attn_metadata=attn_metadata,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
+        # otherwise (seq_len, ).
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class Qwen2Model(nn.Module):
+
+    def __init__(self,
+                 *,
+                 vllm_config: VllmConfig,
+                 prefix: str = "",
+                 decoder_layer_type: type[nn.Module] = Qwen2DecoderLayer):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        # TODO (@robertgshaw2): see if this can be moved out
+        if (cache_config.sliding_window is not None
+                and hasattr(config, "max_window_layers")):
+            raise ValueError("Sliding window for some but all layers is not "
+                             "supported. This model uses sliding window "
+                             "but `max_window_layers` = {} is less than "
+                             "`num_hidden_layers` = {}. Please open an issue "
+                             "to discuss this feature.".format(
+                                 config.max_window_layers,
+                                 config.num_hidden_layers,
+                             ))
+
+        self.config = config
+        self.quant_config = quant_config
+        self.padding_idx = config.pad_token_id
+        self.vocab_size = config.vocab_size
+
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.embed_tokens = VocabParallelEmbedding(
+                config.vocab_size,
+                config.hidden_size,
+                quant_config=quant_config,
+                prefix=f"{prefix}.embed_tokens",
+            )
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        # Use the provided decoder layer type or default to Qwen2DecoderLayer
+        decoder_layer_type = decoder_layer_type or Qwen2DecoderLayer
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: decoder_layer_type(config=config,
+                                              cache_config=cache_config,
+                                              quant_config=quant_config,
+                                              prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.embed_tokens(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(
+                positions,
+                hidden_states,
+                kv_caches[i - self.start_layer],
+                attn_metadata,
+                residual,
+            )
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            if (self.quant_config is not None and
+                (scale_name := self.quant_config.get_cache_scale(name))):
+                # Loading kv cache quantization scales
+                param = params_dict[scale_name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
+                                 loaded_weight[0])
+                weight_loader(param, loaded_weight)
+                loaded_params.add(scale_name)
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                # Remapping the name of FP8 kv-scale.
+                name = maybe_remap_kv_scale_name(name, params_dict)
+                if name is None:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+class Qwen2ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    # LoRA specific attributes
+    supported_lora_modules = [
+        "qkv_proj",
+        "o_proj",
+        "gate_up_proj",
+        "down_proj",
+    ]
+    embedding_modules = {}
+    embedding_padding_modules = []
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = Qwen2Model(vllm_config=vllm_config,
+                                prefix=maybe_prefix(prefix, "model"))
+
+        if get_pp_group().is_last_rank:
+            if config.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, kv_caches,
+                                   attn_metadata, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.config.tie_word_embeddings else None),
+        )
+        return loader.load_weights(weights)
+
+
+class Qwen2EmbeddingModel(nn.Module, SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    # LoRA specific attributes
+    supported_lora_modules = [
+        "qkv_proj",
+        "o_proj",
+        "gate_up_proj",
+        "down_proj",
+    ]
+    embedding_modules = {}
+    embedding_padding_modules = []
+
+    hf_to_vllm_mapper = WeightsMapper(orig_to_new_prefix={"model.": ""})
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+        pooler_config = vllm_config.model_config.pooler_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = Qwen2Model(vllm_config=vllm_config,
+                                prefix=maybe_prefix(prefix, "model"))
+
+        # TODO: Replace this model class with as_embedding_model(
+        # Qwen2ForCausalLM) after changing the default pooling method
+        if pooler_config.pooling_type is None:
+            logger.warning(
+                "This embedding model will default to last-token pooling in "
+                "an upcoming version. To avoid breaking changes, you should "
+                "pass `--override-pooler-config '{\"pooling_type\": \"MEAN\"}'`"
+                " explicitly.")
+
+        self._pooler = Pooler.from_config_with_defaults(
+            pooler_config,
+            pooling_type=PoolingType.MEAN,
+            normalize=True,
+            softmax=False)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+    ) -> torch.Tensor:
+        return self.model(input_ids, positions, kv_caches, attn_metadata,
+                          intermediate_tensors)
+
+    def pooler(
+        self,
+        hidden_states: torch.Tensor,
+        pooling_metadata: PoolingMetadata,
+    ) -> Optional[PoolerOutput]:
+        return self._pooler(hidden_states, pooling_metadata)
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
+        weights = self.hf_to_vllm_mapper.apply(weights)
+        weights = ((name, data) for name, data in weights
+                   if not name.startswith("lm_head."))
+        self.model.load_weights(weights)
diff --git a/vllm/model_executor/models/qwen3.py b/vllm/model_executor/models/qwen3.py
new file mode 100644
index 000000000..54831961a
--- /dev/null
+++ b/vllm/model_executor/models/qwen3.py
@@ -0,0 +1,337 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Copyright 2024 The Qwen team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only Qwen3 model compatible with HuggingFace weights."""
+from typing import Iterable, List, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+from transformers import Qwen3Config
+
+from vllm.attention import Attention, AttentionMetadata, AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
+from vllm.logger import init_logger
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .qwen2 import Qwen2MLP as Qwen3MLP
+from .qwen2 import Qwen2Model
+from .utils import AutoWeightsLoader, PPMissingLayer, maybe_prefix
+
+logger = init_logger(__name__)
+
+
+class Qwen3Attention(nn.Module):
+
+    def __init__(self,
+                 hidden_size: int,
+                 num_heads: int,
+                 num_kv_heads: int,
+                 max_position: int = 4096 * 32,
+                 head_dim: Optional[int] = None,
+                 rms_norm_eps: float = 1e-06,
+                 qkv_bias: bool = False,
+                 rope_theta: float = 10000,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 rope_scaling: Optional[Tuple] = None,
+                 prefix: str = "",
+                 attn_type: str = AttentionType.DECODER) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or hidden_size // self.total_num_heads
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=qkv_bias,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.o_proj",
+        )
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position,
+            base=self.rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn",
+                              attn_type=attn_type)
+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # Add qk-norm
+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                           self.head_dim)
+        q_by_head = self.q_norm.forward_native(q_by_head)
+        q = q_by_head.view(q.shape)
+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                           self.head_dim)
+        k_by_head = self.k_norm.forward_native(k_by_head)
+        k = k_by_head.view(k.shape)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Qwen3DecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: Qwen3Config,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        # Requires transformers > 4.32.0
+        rope_theta = getattr(config, "rope_theta", 1000000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        # By default, Qwen3 uses causal attention as it is a decoder-only model.
+        # You can override the HF config with `is_causal=False` to enable
+        # bidirectional attention, which is used in some embedding models
+        # (e.g. Alibaba-NLP/gte-Qwen3-7B-instruct)
+        if getattr(config, "is_causal", True):
+            attn_type = AttentionType.DECODER
+        else:
+            attn_type = AttentionType.ENCODER_ONLY
+
+        self.self_attn = Qwen3Attention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn",
+            attn_type=attn_type,
+        )
+        self.mlp = Qwen3MLP(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mlp",
+        )
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+        residual: Optional[torch.Tensor],
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+            kv_cache=kv_cache,
+            attn_metadata=attn_metadata,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+ALL_DECODER_LAYER_TYPES = {
+    "attention": Qwen3DecoderLayer,
+}
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
+        # otherwise (seq_len, ).
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class Qwen3Model(Qwen2Model):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config,
+                         prefix=prefix,
+                         decoder_layer_type=Qwen3DecoderLayer)
+
+
+class Qwen3ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = Qwen3Model(vllm_config=vllm_config,
+                                prefix=maybe_prefix(prefix, "model"))
+
+        if get_pp_group().is_last_rank:
+            if config.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.config.tie_word_embeddings else None),
+        )
+        return loader.load_weights(weights)
diff --git a/vllm/model_executor/models/qwen3_moe.py b/vllm/model_executor/models/qwen3_moe.py
new file mode 100644
index 000000000..4ef45b75b
--- /dev/null
+++ b/vllm/model_executor/models/qwen3_moe.py
@@ -0,0 +1,550 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Copyright 2024 The Qwen team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only Qwen3MoE model compatible with HuggingFace weights."""
+from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+from transformers import PretrainedConfig
+
+from vllm.attention import Attention, AttentionMetadata
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.logger import init_logger
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               ReplicatedLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsPP
+from .utils import (AutoWeightsLoader, extract_layer_index,
+                    is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+logger = init_logger(__name__)
+
+
+class Qwen3MoeMLP(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        intermediate_size: int,
+        hidden_act: str,
+        quant_config: Optional[QuantizationConfig] = None,
+        reduce_results: bool = True,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size, [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj")
+        self.down_proj = RowParallelLinear(intermediate_size,
+                                           hidden_size,
+                                           bias=False,
+                                           quant_config=quant_config,
+                                           reduce_results=reduce_results,
+                                           prefix=f"{prefix}.down_proj")
+        if hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu is supported for now.")
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class Qwen3MoeSparseMoeBlock(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+        self.tp_size = get_tensor_model_parallel_world_size()
+
+        if self.tp_size > config.num_experts:
+            raise ValueError(
+                f"Tensor parallel size {self.tp_size} is greater than "
+                f"the number of experts {config.num_experts}.")
+
+        self.experts = FusedMoE(num_experts=config.num_experts,
+                                top_k=config.num_experts_per_tok,
+                                hidden_size=config.hidden_size,
+                                intermediate_size=config.moe_intermediate_size,
+                                reduce_results=False,
+                                renormalize=config.norm_topk_prob,
+                                quant_config=quant_config,
+                                prefix=f"{prefix}.experts")
+
+        self.gate = ReplicatedLinear(config.hidden_size,
+                                     config.num_experts,
+                                     bias=False,
+                                     quant_config=None,
+                                     prefix=f"{prefix}.gate")
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        # NOTE: hidden_states can have either 1D or 2D shape.
+        orig_shape = hidden_states.shape
+        hidden_dim = hidden_states.shape[-1]
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(hidden_states=hidden_states,
+                                           router_logits=router_logits)
+        final_hidden_states = final_hidden_states
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+
+        return final_hidden_states.view(orig_shape)
+
+
+class Qwen3MoeAttention(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[Dict[str, Any]] = None,
+        max_position_embeddings: int = 8192,
+        head_dim: Optional[int] = None,
+        rms_norm_eps: float = 1e-06,
+        qkv_bias: bool = False,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or (hidden_size // self.total_num_heads)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+
+        self.qkv_proj = QKVParallelLinear(hidden_size,
+                                          self.head_dim,
+                                          self.total_num_heads,
+                                          self.total_num_kv_heads,
+                                          bias=qkv_bias,
+                                          quant_config=quant_config,
+                                          prefix=f"{prefix}.qkv_proj")
+
+        self.o_proj = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                        hidden_size,
+                                        bias=False,
+                                        quant_config=quant_config,
+                                        prefix=f"{prefix}.o_proj")
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position_embeddings,
+            base=rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn")
+
+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # Add qk-norm
+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                           self.head_dim)
+        q_by_head = self.q_norm.forward_native(q_by_head)
+        q = q_by_head.view(q.shape)
+
+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                           self.head_dim)
+        k_by_head = self.k_norm.forward_native(k_by_head)
+        k = k_by_head.view(k.shape)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Qwen3MoeDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        self.self_attn = Qwen3MoeAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+        )
+
+        # `mlp_only_layers` in the config.
+        layer_idx = extract_layer_index(prefix)
+        mlp_only_layers = ([] if not hasattr(config, "mlp_only_layers") else
+                           config.mlp_only_layers)
+        if (layer_idx not in mlp_only_layers) and (
+                config.num_experts > 0 and
+            (layer_idx + 1) % config.decoder_sparse_step == 0):
+            self.mlp = Qwen3MoeSparseMoeBlock(config=config,
+                                              quant_config=quant_config,
+                                              prefix=f"{prefix}.mlp")
+        else:
+            self.mlp = Qwen3MoeMLP(hidden_size=config.hidden_size,
+                                   intermediate_size=config.intermediate_size,
+                                   hidden_act=config.hidden_act,
+                                   quant_config=quant_config,
+                                   prefix=f"{prefix}.mlp")
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+        residual: Optional[torch.Tensor],
+    ) -> torch.Tensor:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+            kv_cache=kv_cache,
+            attn_metadata=attn_metadata,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+@support_torch_compile
+class Qwen3MoeModel(nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.padding_idx = config.pad_token_id
+        self.vocab_size = config.vocab_size
+        self.config = config
+        self.embed_tokens = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+            prefix=f"{prefix}.embed_tokens")
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: Qwen3MoeDecoderLayer(config=config,
+                                                cache_config=cache_config,
+                                                quant_config=quant_config,
+                                                prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.embed_tokens(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(positions, hidden_states, kv_caches[i - self.start_layer],
+                attn_metadata, residual)
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        # Params for weights, fp8 weight scales, fp8 activation scales
+        # (param_name, weight_name, expert_id, shard_id)
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts)
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                if "mlp.experts" in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if ((name.endswith(".bias") or name.endswith("_bias"))
+                        and name not in params_dict):
+                    continue
+                # Skip layers on other devices.
+                if is_pp_missing_parameter(name, self):
+                    continue
+                if name not in params_dict:
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Remapping the name of FP8 kv-scale.
+                    if name.endswith("kv_scale"):
+                        remapped_kv_scale_name = name.replace(
+                            ".kv_scale", ".attn.kv_scale")
+                        if remapped_kv_scale_name not in params_dict:
+                            logger.warning_once(
+                                "Found kv scale in the checkpoint "
+                                f"(e.g. {name}), but not found the expected "
+                                f"name in the model "
+                                f"(e.g. {remapped_kv_scale_name}). "
+                                "kv-scale is not loaded.")
+                            continue
+                        else:
+                            name = remapped_kv_scale_name
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+ 
+class Qwen3MoeForCausalLM(nn.Module, SupportsPP):
+
+    fall_back_to_pt_during_load = False
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        self.model = Qwen3MoeModel(vllm_config=vllm_config,
+                                   prefix=maybe_prefix(prefix, "model"))
+        self.lm_head = ParallelLMHead(config.vocab_size,
+                                      config.hidden_size,
+                                      quant_config=quant_config)
+        if self.config.tie_word_embeddings:
+            self.lm_head.weight = self.model.embed_tokens.weight
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        kv_caches: List[torch.Tensor],
+        attn_metadata: AttentionMetadata,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: Optional[torch.Tensor],
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+    
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["rotary_emb.inv_freq"]),
+        )
+        return loader.load_weights(weights)
+
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index 3b2a7069e..6af50ae2b 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """
 Whenever you add an architecture to this page, please also update
@@ -37,6 +38,7 @@ _TEXT_GENERATION_MODELS = {
     "BaiChuanForCausalLM": ("baichuan", "BaiChuanForCausalLM"),
     # baichuan-13b, lower case 'c' in the class name
     "BaichuanForCausalLM": ("baichuan", "BaichuanForCausalLM"),
+    "BaiChuanMoEForCausalLM": ("baichuan_moe", "BaiChuanMoEForCausalLM"), # baichuan-moe
     "BloomForCausalLM": ("bloom", "BloomForCausalLM"),
     # ChatGLMModel supports multimodal
     "CohereForCausalLM": ("commandr", "CohereForCausalLM"),
@@ -92,12 +94,15 @@ _TEXT_GENERATION_MODELS = {
     # QWenLMHeadModel supports multimodal
     "Qwen2ForCausalLM": ("qwen2", "Qwen2ForCausalLM"),
     "Qwen2MoeForCausalLM": ("qwen2_moe", "Qwen2MoeForCausalLM"),
+    "Qwen3ForCausalLM": ("qwen3", "Qwen3ForCausalLM"),
+    "Qwen3MoeForCausalLM": ("qwen3_moe", "Qwen3MoeForCausalLM"),
     "RWForCausalLM": ("falcon", "FalconForCausalLM"),
     "StableLMEpochForCausalLM": ("stablelm", "StablelmForCausalLM"),
     "StableLmForCausalLM": ("stablelm", "StablelmForCausalLM"),
     "Starcoder2ForCausalLM": ("starcoder2", "Starcoder2ForCausalLM"),
     "SolarForCausalLM": ("solar", "SolarForCausalLM"),
     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
+    "TelechatForCausalLM": ("telechat", "TelechatForCausalLM"),
     "XverseForCausalLM": ("llama", "LlamaForCausalLM"),
     # [Encoder-decoder]
     "BartModel": ("bart", "BartForConditionalGeneration"),
@@ -130,6 +135,7 @@ _EMBEDDING_MODELS = {
     "Qwen2ForRewardModel": ("qwen2_rm", "Qwen2ForRewardModel"),
     "Qwen2ForProcessRewardModel": ("qwen2_rm", "Qwen2ForProcessRewardModel"),
     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
+    "TelechatForCausalLM": ("telechat", "TelechatForCausalLM"),
     # [Multimodal]
     "LlavaNextForConditionalGeneration": ("llava_next", "LlavaNextForConditionalGeneration"),  # noqa: E501
     "Phi3VForCausalLM": ("phi3v", "Phi3VForCausalLM"),
diff --git a/vllm/model_executor/models/telechat.py b/vllm/model_executor/models/telechat.py
new file mode 100644
index 000000000..58fdcbd43
--- /dev/null
+++ b/vllm/model_executor/models/telechat.py
@@ -0,0 +1,139 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Iterable, Set, Tuple
+
+import torch
+
+from vllm.config import VllmConfig
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.llama import LlamaForCausalLM, LlamaModel
+
+from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
+                    is_pp_missing_parameter)
+
+
+class TeleChatModel(LlamaModel):
+
+    hf_to_vllm_mapper = WeightsMapper(
+        orig_to_new_prefix={
+            "transformer.": "model.",
+        },
+        # for different telechat model: telechat-12B & telechat-7B
+        orig_to_new_substr={
+            ".h.": ".layers.",
+            "h.": "model.layers.",
+            ".self_attention.": ".self_attn.",
+            ".word_embeddings.": ".embed_tokens.",
+            "word_embeddings.": "model.embed_tokens.",
+            ".dense.": ".o_proj.",
+            ".ln_f.": ".norm.",
+            "ln_f.": "model.norm.",
+        },
+    )
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        # 1. Initialize the LlamaModel with bias
+        vllm_config.model_config.hf_config.bias = True
+        vllm_config.model_config.hf_config.mlp_bias = True
+        vllm_config.model_config.hf_config.num_key_value_heads = vllm_config.model_config.hf_config.num_attention_heads
+        super().__init__(vllm_config=vllm_config, prefix=prefix)
+        # 2. Remove the bias from the qkv_proj and gate_up_proj based on config
+        # Telechat2's gate_up_proj and qkv_proj don't have bias
+        # see: https://github.com/vllm-project/vllm/pull/10311#issuecomment-2490297566
+        for layer in self.layers:
+            if not isinstance(layer, PPMissingLayer):
+                layer.self_attn.qkv_proj.bias = None
+                layer.self_attn.qkv_proj.skip_bias_add = True
+                layer.mlp.gate_up_proj.bias = None
+                layer.mlp.gate_up_proj.skip_bias_add = True
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            ('gate_up_proj', 'gate_proj', 0),
+            ('gate_up_proj', 'up_proj', 1),
+        ]
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        total_num_heads = self.config.n_head
+        head_dim = self.config.hidden_size // total_num_heads
+        for name, loaded_weight in weights:
+            if "self_attn.key_value" in name:
+                k_weight = []
+                v_weight = []
+                for i in range(total_num_heads):
+                    start = i * head_dim * 2
+                    k_weight.append(loaded_weight[start:start + head_dim, :])
+                    v_weight.append(loaded_weight[start + head_dim:start +
+                                                  2 * head_dim:])
+                k_weight = torch.cat(k_weight, dim=0)
+                v_weight = torch.cat(v_weight, dim=0)
+                name = name.replace("key_value", "qkv_proj")
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, k_weight, "k")
+                weight_loader(param, v_weight, "v")
+            elif "query" in name:
+                name = name.replace("query", "qkv_proj")
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, "q")
+            else:
+                for param_name, weight_name, shard_id in stacked_params_mapping:
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param, loaded_weight, shard_id)
+                    break
+                else:
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+class TelechatForCausalLM(LlamaForCausalLM):
+
+    def _init_model(self, vllm_config: VllmConfig, prefix: str = ""):
+        return TeleChatModel(vllm_config=vllm_config, prefix=prefix)
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.config.tie_word_embeddings else None),
+        )
+        return loader.load_weights(weights, mapper=self.model.hf_to_vllm_mapper)
diff --git a/vllm/platforms/__init__.py b/vllm/platforms/__init__.py
index e4767a378..750786b26 100644
--- a/vllm/platforms/__init__.py
+++ b/vllm/platforms/__init__.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import logging
@@ -33,11 +34,14 @@ def cuda_platform_plugin() -> Optional[str]:
     is_cuda = False
 
     try:
-        from vllm.utils import import_pynvml
-        pynvml = import_pynvml()
+        #from vllm.utils import import_pynvml
+        #pynvml = import_pynvml()
+        import vllm.platforms.pynvml as pynvml
         pynvml.nvmlInit()
         try:
-            if pynvml.nvmlDeviceGetCount() > 0:
+            import torch
+            #if pynvml.nvmlDeviceGetCount() > 0:
+            if torch.cuda.device_count() > 0:
                 is_cuda = True
         finally:
             pynvml.nvmlShutdown()
diff --git a/vllm/platforms/cuda.py b/vllm/platforms/cuda.py
index 991d55ac8..b40acdec1 100644
--- a/vllm/platforms/cuda.py
+++ b/vllm/platforms/cuda.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Code inside this file can safely assume cuda platform, e.g. importing
 pynvml. However, it should not initialize cuda context.
@@ -33,7 +34,7 @@ pynvml = import_pynvml()
 
 # pytorch 2.5 uses cudnn sdpa by default, which will cause crash on some models
 # see https://github.com/huggingface/diffusers/issues/9704 for details
-torch.backends.cuda.enable_cudnn_sdp(False)
+#torch.backends.cuda.enable_cudnn_sdp(False)
 
 
 def device_id_to_physical_device_id(device_id: int) -> int:
@@ -204,9 +205,14 @@ class CudaPlatformBase(Platform):
         # installed.
         if target_backend == _Backend.FLASH_ATTN:
             try:
-                import vllm.vllm_flash_attn  # noqa: F401
-                from vllm.attention.backends.flash_attn import (  # noqa: F401
-                    FlashAttentionBackend)
+                import flash_attn  # noqa: F401
+                pg_method = os.getenv("MACA_VLLM_PG_OPT", None)
+                if pg_method is None:
+                    from vllm.attention.backends.flash_attn import (  # noqa: F401
+                        FlashAttentionBackend)
+                else:
+                    from vllm.attention.backends.flash_attn_pg import (  # noqa: F401
+                        FlashAttentionBackend)
 
                 supported_sizes = \
                     FlashAttentionBackend.get_supported_head_sizes()
@@ -228,7 +234,12 @@ class CudaPlatformBase(Platform):
             return "vllm.attention.backends.xformers.XFormersBackend"
 
         logger.info("Using Flash Attention backend.")
-        return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
+        pg_method = os.getenv("MACA_VLLM_PG_OPT", None)
+        if pg_method is None:
+            return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
+        else:
+            return "vllm.attention.backends.flash_attn_pg.FlashAttentionBackend"
+        #return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
 
     @classmethod
     def get_punica_wrapper(cls) -> str:
@@ -379,7 +390,8 @@ finally:
     if nvml_available:
         pynvml.nvmlShutdown()
 
-CudaPlatform = NvmlCudaPlatform if nvml_available else NonNvmlCudaPlatform
+#CudaPlatform = NvmlCudaPlatform if nvml_available else NonNvmlCudaPlatform
+CudaPlatform = NonNvmlCudaPlatform
 
 try:
     from sphinx.ext.autodoc.mock import _MockModule
diff --git a/vllm/platforms/pynvml.py b/vllm/platforms/pynvml.py
new file mode 100644
index 000000000..76aa09e97
--- /dev/null
+++ b/vllm/platforms/pynvml.py
@@ -0,0 +1,5440 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#####
+# Copyright (c) 2011-2023, NVIDIA Corporation.  All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+#    * Redistributions of source code must retain the above copyright notice,
+#      this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#    * Neither the name of the NVIDIA Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived from
+#      this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+# THE POSSIBILITY OF SUCH DAMAGE.
+#####
+
+##
+# Python bindings for the MXSMLEX library
+##
+from ctypes import *
+from ctypes.util import find_library
+from functools import wraps
+import sys
+import os
+import threading
+import string
+
+## C Type mappings ##
+## Enums
+_nvmlEnableState_t = c_uint
+MXSMLEX_FEATURE_DISABLED    = 0
+MXSMLEX_FEATURE_ENABLED     = 1
+
+_nvmlBrandType_t = c_uint
+MXSMLEX_BRAND_UNKNOWN             = 0
+MXSMLEX_BRAND_QUADRO              = 1
+MXSMLEX_BRAND_TESLA               = 2
+MXSMLEX_BRAND_NVS                 = 3
+MXSMLEX_BRAND_GRID                = 4   # Deprecated from API reporting. Keeping definition for backward compatibility.
+MXSMLEX_BRAND_GEFORCE             = 5
+MXSMLEX_BRAND_TITAN               = 6
+MXSMLEX_BRAND_NVIDIA_VAPPS        = 7   # NVIDIA Virtual Applications
+MXSMLEX_BRAND_NVIDIA_VPC          = 8   # NVIDIA Virtual PC
+MXSMLEX_BRAND_NVIDIA_VCS          = 9   # NVIDIA Virtual Compute Server
+MXSMLEX_BRAND_NVIDIA_VWS          = 10  # NVIDIA RTX Virtual Workstation
+MXSMLEX_BRAND_NVIDIA_CLOUD_GAMING = 11  # NVIDIA Cloud Gaming
+MXSMLEX_BRAND_NVIDIA_VGAMING      = MXSMLEX_BRAND_NVIDIA_CLOUD_GAMING # Deprecated from API reporting. Keeping definition for backward compatibility.
+MXSMLEX_BRAND_QUADRO_RTX          = 12
+MXSMLEX_BRAND_NVIDIA_RTX          = 13
+MXSMLEX_BRAND_NVIDIA              = 14
+MXSMLEX_BRAND_GEFORCE_RTX         = 15  # Unused
+MXSMLEX_BRAND_TITAN_RTX           = 16  # Unused
+MXSMLEX_BRAND_COUNT               = 17
+
+_nvmlTemperatureThresholds_t = c_uint
+MXSMLEX_TEMPERATURE_THRESHOLD_SHUTDOWN      = 0
+MXSMLEX_TEMPERATURE_THRESHOLD_SLOWDOWN      = 1
+MXSMLEX_TEMPERATURE_THRESHOLD_MEM_MAX       = 2
+MXSMLEX_TEMPERATURE_THRESHOLD_GPU_MAX       = 3
+MXSMLEX_TEMPERATURE_THRESHOLD_ACOUSTIC_MIN  = 4
+MXSMLEX_TEMPERATURE_THRESHOLD_ACOUSTIC_CURR = 5
+MXSMLEX_TEMPERATURE_THRESHOLD_ACOUSTIC_MAX  = 6
+MXSMLEX_TEMPERATURE_THRESHOLD_COUNT         = 7
+
+_nvmlTemperatureSensors_t = c_uint
+MXSMLEX_TEMPERATURE_GPU     = 0
+MXSMLEX_TEMPERATURE_COUNT   = 1
+
+
+_nvmlComputeMode_t = c_uint
+MXSMLEX_COMPUTEMODE_DEFAULT           = 0
+MXSMLEX_COMPUTEMODE_EXCLUSIVE_THREAD  = 1  ## Support Removed
+MXSMLEX_COMPUTEMODE_PROHIBITED        = 2
+MXSMLEX_COMPUTEMODE_EXCLUSIVE_PROCESS = 3
+MXSMLEX_COMPUTEMODE_COUNT             = 4
+
+_nvmlMemoryLocation_t = c_uint
+MXSMLEX_MEMORY_LOCATION_L1_CACHE = 0
+MXSMLEX_MEMORY_LOCATION_L2_CACHE = 1
+MXSMLEX_MEMORY_LOCATION_DEVICE_MEMORY = 2
+MXSMLEX_MEMORY_LOCATION_DRAM = 2
+MXSMLEX_MEMORY_LOCATION_REGISTER_FILE = 3
+MXSMLEX_MEMORY_LOCATION_TEXTURE_MEMORY = 4
+MXSMLEX_MEMORY_LOCATION_TEXTURE_SHM = 5
+MXSMLEX_MEMORY_LOCATION_CBU = 6
+MXSMLEX_MEMORY_LOCATION_SRAM = 7
+MXSMLEX_MEMORY_LOCATION_COUNT = 8
+
+MXSMLEX_NVLINK_MAX_LINKS = 18
+
+# For backwards compatibility, maintain the incorrectly-named "LANES" define
+MXSMLEX_NVLINK_MAX_LANES = MXSMLEX_NVLINK_MAX_LINKS
+
+_nvmlNvLinkErrorCounter_t = c_uint
+MXSMLEX_NVLINK_ERROR_DL_REPLAY = 0
+MXSMLEX_NVLINK_ERROR_DL_RECOVERY = 1
+MXSMLEX_NVLINK_ERROR_DL_CRC_FLIT = 2
+MXSMLEX_NVLINK_ERROR_DL_CRC_DATA = 3
+MXSMLEX_NVLINK_ERROR_DL_ECC_DATA = 4
+MXSMLEX_NVLINK_ERROR_COUNT = 5
+
+_nvmlNvLinkEccLaneErrorCounter_t = c_uint
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE0 = 0
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE1 = 1
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE2 = 2
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE3 = 3
+MXSMLEX_NVLINK_ERROR_DL_ECC_COUNT = 5
+
+_nvmlNvLinkCapability_t = c_uint
+MXSMLEX_NVLINK_CAP_P2P_SUPPORTED = 0
+MXSMLEX_NVLINK_CAP_SYSMEM_ACCESS = 1
+MXSMLEX_NVLINK_CAP_P2P_ATOMICS   = 2
+MXSMLEX_NVLINK_CAP_SYSMEM_ATOMICS= 3
+MXSMLEX_NVLINK_CAP_SLI_BRIDGE    = 4
+MXSMLEX_NVLINK_CAP_VALID         = 5
+MXSMLEX_NVLINK_CAP_COUNT         = 6
+
+_nvmlNvLinkUtilizationCountPktTypes_t = c_uint
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_NOP        = 0x1
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_READ       = 0x2
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_WRITE      = 0x4
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_RATOM      = 0x8
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_NRATOM     = 0x10
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_FLUSH      = 0x20
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_RESPDATA   = 0x40
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_RESPNODATA = 0x80
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_ALL        = 0xFF
+
+_nvmlNvLinkUtilizationCountUnits_t = c_uint
+MXSMLEX_NVLINK_COUNTER_UNIT_CYCLES   = 0
+MXSMLEX_NVLINK_COUNTER_UNIT_PACKETS  = 1
+MXSMLEX_NVLINK_COUNTER_UNIT_BYTES    = 2
+MXSMLEX_NVLINK_COUNTER_UNIT_RESERVED = 3
+MXSMLEX_NVLINK_COUNTER_UNIT_COUNT    = 4
+
+_nvmlNvLinkDeviceType_t = c_uint
+MXSMLEX_NVLINK_DEVICE_TYPE_GPU     = 0x00
+MXSMLEX_NVLINK_DEVICE_TYPE_IBMNPU  = 0x01
+MXSMLEX_NVLINK_DEVICE_TYPE_SWITCH  = 0x02
+MXSMLEX_NVLINK_DEVICE_TYPE_UNKNOWN = 0xFF
+
+# These are deprecated, instead use _nvmlMemoryErrorType_t
+_nvmlEccBitType_t = c_uint
+MXSMLEX_SINGLE_BIT_ECC    = 0
+MXSMLEX_DOUBLE_BIT_ECC    = 1
+MXSMLEX_ECC_ERROR_TYPE_COUNT = 2
+
+_nvmlEccCounterType_t = c_uint
+MXSMLEX_VOLATILE_ECC      = 0
+MXSMLEX_AGGREGATE_ECC     = 1
+MXSMLEX_ECC_COUNTER_TYPE_COUNT = 2
+
+_nvmlMemoryErrorType_t = c_uint
+MXSMLEX_MEMORY_ERROR_TYPE_CORRECTED   = 0
+MXSMLEX_MEMORY_ERROR_TYPE_UNCORRECTED = 1
+MXSMLEX_MEMORY_ERROR_TYPE_COUNT       = 2
+
+_nvmlClockType_t = c_uint
+MXSMLEX_CLOCK_GRAPHICS  = 0
+MXSMLEX_CLOCK_SM        = 1
+MXSMLEX_CLOCK_MEM       = 2
+MXSMLEX_CLOCK_VIDEO     = 3
+MXSMLEX_CLOCK_COUNT     = 4
+
+_nvmlClockId_t = c_uint
+MXSMLEX_CLOCK_ID_CURRENT            = 0
+MXSMLEX_CLOCK_ID_APP_CLOCK_TARGET   = 1
+MXSMLEX_CLOCK_ID_APP_CLOCK_DEFAULT  = 2
+MXSMLEX_CLOCK_ID_CUSTOMER_BOOST_MAX = 3
+MXSMLEX_CLOCK_ID_COUNT              = 4
+
+_nvmlDriverModel_t = c_uint
+MXSMLEX_DRIVER_WDDM       = 0
+MXSMLEX_DRIVER_WDM        = 1
+MXSMLEX_DRIVER_MCDM       = 2
+
+MXSMLEX_MAX_GPU_PERF_PSTATES = 16
+
+_nvmlPstates_t = c_uint
+MXSMLEX_PSTATE_0               = 0
+MXSMLEX_PSTATE_1               = 1
+MXSMLEX_PSTATE_2               = 2
+MXSMLEX_PSTATE_3               = 3
+MXSMLEX_PSTATE_4               = 4
+MXSMLEX_PSTATE_5               = 5
+MXSMLEX_PSTATE_6               = 6
+MXSMLEX_PSTATE_7               = 7
+MXSMLEX_PSTATE_8               = 8
+MXSMLEX_PSTATE_9               = 9
+MXSMLEX_PSTATE_10              = 10
+MXSMLEX_PSTATE_11              = 11
+MXSMLEX_PSTATE_12              = 12
+MXSMLEX_PSTATE_13              = 13
+MXSMLEX_PSTATE_14              = 14
+MXSMLEX_PSTATE_15              = 15
+MXSMLEX_PSTATE_UNKNOWN         = 32
+
+_nvmlInforomObject_t = c_uint
+MXSMLEX_INFOROM_OEM            = 0
+MXSMLEX_INFOROM_ECC            = 1
+MXSMLEX_INFOROM_POWER          = 2
+MXSMLEX_INFOROM_COUNT          = 3
+
+_nvmlReturn_t = c_uint
+MXSMLEX_SUCCESS                         = 0
+MXSMLEX_ERROR_UNINITIALIZED             = 1
+MXSMLEX_ERROR_INVALID_ARGUMENT          = 2
+MXSMLEX_ERROR_NOT_SUPPORTED             = 3
+MXSMLEX_ERROR_NO_PERMISSION             = 4
+MXSMLEX_ERROR_ALREADY_INITIALIZED       = 5
+MXSMLEX_ERROR_NOT_FOUND                 = 6
+MXSMLEX_ERROR_INSUFFICIENT_SIZE         = 7
+MXSMLEX_ERROR_INSUFFICIENT_POWER        = 8
+MXSMLEX_ERROR_DRIVER_NOT_LOADED         = 9
+MXSMLEX_ERROR_TIMEOUT                   = 10
+MXSMLEX_ERROR_IRQ_ISSUE                 = 11
+MXSMLEX_ERROR_LIBRARY_NOT_FOUND         = 12
+MXSMLEX_ERROR_FUNCTION_NOT_FOUND        = 13
+MXSMLEX_ERROR_CORRUPTED_INFOROM         = 14
+MXSMLEX_ERROR_GPU_IS_LOST               = 15
+MXSMLEX_ERROR_RESET_REQUIRED            = 16
+MXSMLEX_ERROR_OPERATING_SYSTEM          = 17
+MXSMLEX_ERROR_LIB_RM_VERSION_MISMATCH   = 18
+MXSMLEX_ERROR_IN_USE                    = 19
+MXSMLEX_ERROR_MEMORY                    = 20
+MXSMLEX_ERROR_NO_DATA                   = 21
+MXSMLEX_ERROR_VGPU_ECC_NOT_SUPPORTED    = 22
+MXSMLEX_ERROR_INSUFFICIENT_RESOURCES    = 23
+MXSMLEX_ERROR_FREQ_NOT_SUPPORTED        = 24
+MXSMLEX_ERROR_ARGUMENT_VERSION_MISMATCH = 25
+MXSMLEX_ERROR_DEPRECATED                = 26
+MXSMLEX_ERROR_NOT_READY                 = 27
+MXSMLEX_ERROR_GPU_NOT_FOUND             = 28
+MXSMLEX_ERROR_INVALID_STATE             = 29
+MXSMLEX_ERROR_UNKNOWN                   = 999
+
+_nvmlFanState_t = c_uint
+MXSMLEX_FAN_NORMAL             = 0
+MXSMLEX_FAN_FAILED             = 1
+
+_nvmlFanControlPolicy_t = c_uint
+MXSMLEX_FAN_POLICY_TEMPERATURE_CONTINOUS_SW = 0
+MXSMLEX_FAN_POLICY_MANUAL                   = 1
+
+_nvmlLedColor_t = c_uint
+MXSMLEX_LED_COLOR_GREEN        = 0
+MXSMLEX_LED_COLOR_AMBER        = 1
+
+_nvmlGpuOperationMode_t = c_uint
+MXSMLEX_GOM_ALL_ON                 = 0
+MXSMLEX_GOM_COMPUTE                = 1
+MXSMLEX_GOM_LOW_DP                 = 2
+
+_nvmlPageRetirementCause_t = c_uint
+MXSMLEX_PAGE_RETIREMENT_CAUSE_MULTIPLE_SINGLE_BIT_ECC_ERRORS = 0
+MXSMLEX_PAGE_RETIREMENT_CAUSE_DOUBLE_BIT_ECC_ERROR           = 1
+MXSMLEX_PAGE_RETIREMENT_CAUSE_COUNT                          = 2
+
+_nvmlRestrictedAPI_t = c_uint
+MXSMLEX_RESTRICTED_API_SET_APPLICATION_CLOCKS                = 0
+MXSMLEX_RESTRICTED_API_SET_AUTO_BOOSTED_CLOCKS               = 1
+MXSMLEX_RESTRICTED_API_COUNT                                 = 2
+
+_nvmlBridgeChipType_t = c_uint
+MXSMLEX_BRIDGE_CHIP_PLX = 0
+MXSMLEX_BRIDGE_CHIP_BRO4 = 1
+MXSMLEX_MAX_PHYSICAL_BRIDGE = 128
+
+_nvmlValueType_t = c_uint
+MXSMLEX_VALUE_TYPE_DOUBLE = 0
+MXSMLEX_VALUE_TYPE_UNSIGNED_INT = 1
+MXSMLEX_VALUE_TYPE_UNSIGNED_LONG = 2
+MXSMLEX_VALUE_TYPE_UNSIGNED_LONG_LONG = 3
+MXSMLEX_VALUE_TYPE_SIGNED_LONG_LONG = 4
+MXSMLEX_VALUE_TYPE_SIGNED_INT = 5
+MXSMLEX_VALUE_TYPE_COUNT = 6
+
+_nvmlPerfPolicyType_t = c_uint
+MXSMLEX_PERF_POLICY_POWER = 0
+MXSMLEX_PERF_POLICY_THERMAL = 1
+MXSMLEX_PERF_POLICY_SYNC_BOOST = 2
+MXSMLEX_PERF_POLICY_BOARD_LIMIT = 3
+MXSMLEX_PERF_POLICY_LOW_UTILIZATION = 4
+MXSMLEX_PERF_POLICY_RELIABILITY = 5
+MXSMLEX_PERF_POLICY_TOTAL_APP_CLOCKS = 10
+MXSMLEX_PERF_POLICY_TOTAL_BASE_CLOCKS = 11
+MXSMLEX_PERF_POLICY_COUNT = 12
+
+_nvmlEncoderQueryType_t = c_uint
+MXSMLEX_ENCODER_QUERY_H264 = 0
+MXSMLEX_ENCODER_QUERY_HEVC = 1
+MXSMLEX_ENCODER_QUERY_AV1 = 2
+MXSMLEX_ENCODER_QUERY_UNKNOWN = 255
+
+_nvmlFBCSessionType_t = c_uint
+MXSMLEX_FBC_SESSION_TYPE_UNKNOWN = 0
+MXSMLEX_FBC_SESSION_TYPE_TOSYS = 1
+MXSMLEX_FBC_SESSION_TYPE_CUDA = 2
+MXSMLEX_FBC_SESSION_TYPE_VID = 3
+MXSMLEX_FBC_SESSION_TYPE_HWENC = 4
+
+_nvmlDetachGpuState_t = c_uint
+MXSMLEX_DETACH_GPU_KEEP = 0
+MXSMLEX_DETACH_GPU_REMOVE = 1
+
+_nvmlPcieLinkState_t = c_uint
+MXSMLEX_PCIE_LINK_KEEP = 0
+MXSMLEX_PCIE_LINK_SHUT_DOWN = 1
+
+_nvmlSamplingType_t = c_uint
+MXSMLEX_TOTAL_POWER_SAMPLES = 0
+MXSMLEX_GPU_UTILIZATION_SAMPLES = 1
+MXSMLEX_MEMORY_UTILIZATION_SAMPLES = 2
+MXSMLEX_ENC_UTILIZATION_SAMPLES = 3
+MXSMLEX_DEC_UTILIZATION_SAMPLES = 4
+MXSMLEX_PROCESSOR_CLK_SAMPLES = 5
+MXSMLEX_MEMORY_CLK_SAMPLES = 6
+MXSMLEX_MODULE_POWER_SAMPLES = 7
+MXSMLEX_JPG_UTILIZATION_SAMPLES = 8
+MXSMLEX_OFA_UTILIZATION_SAMPLES = 9
+MXSMLEX_SAMPLINGTYPE_COUNT = 10
+
+_nvmlPcieUtilCounter_t = c_uint
+MXSMLEX_PCIE_UTIL_TX_BYTES = 0
+MXSMLEX_PCIE_UTIL_RX_BYTES = 1
+MXSMLEX_PCIE_UTIL_COUNT = 2
+
+_nvmlGpuTopologyLevel_t = c_uint
+MXSMLEX_TOPOLOGY_INTERNAL = 0
+MXSMLEX_TOPOLOGY_SINGLE = 10
+MXSMLEX_TOPOLOGY_MULTIPLE = 20
+MXSMLEX_TOPOLOGY_HOSTBRIDGE = 30
+MXSMLEX_TOPOLOGY_NODE = 40
+MXSMLEX_TOPOLOGY_CPU = MXSMLEX_TOPOLOGY_NODE
+MXSMLEX_TOPOLOGY_SYSTEM = 50
+
+_nvmlGpuP2PCapsIndex_t = c_uint
+MXSMLEX_P2P_CAPS_INDEX_READ = 0,
+MXSMLEX_P2P_CAPS_INDEX_WRITE = 1
+#MXSMLEX_P2P_CAPS_INDEX_MXLINK =2
+NVML_P2P_CAPS_INDEX_NVLINK = 2
+MXSMLEX_P2P_CAPS_INDEX_ATOMICS = 3
+#
+# MXSMLEX_P2P_CAPS_INDEX_PROP is deprecated.
+# Use MXSMLEX_P2P_CAPS_INDEX_PCI instead.
+#
+MXSMLEX_P2P_CAPS_INDEX_PROP = 4
+MXSMLEX_P2P_CAPS_INDEX_PCI = 4
+MXSMLEX_P2P_CAPS_INDEX_UNKNOWN = 5
+
+_nvmlGpuP2PStatus_t = c_uint
+#MXSMLEX_P2P_STATUS_OK     = 0
+NVML_P2P_STATUS_OK = 0
+MXSMLEX_P2P_STATUS_CHIPSET_NOT_SUPPORED = 1
+MXSMLEX_P2P_STATUS_CHIPSET_NOT_SUPPORTED = MXSMLEX_P2P_STATUS_CHIPSET_NOT_SUPPORED
+MXSMLEX_P2P_STATUS_GPU_NOT_SUPPORTED = 2
+MXSMLEX_P2P_STATUS_IOH_TOPOLOGY_NOT_SUPPORTED =3
+MXSMLEX_P2P_STATUS_DISABLED_BY_REGKEY =4
+MXSMLEX_P2P_STATUS_NOT_SUPPORTED =5
+MXSMLEX_P2P_STATUS_UNKNOWN =6
+
+_nvmlDeviceArchitecture_t = c_uint
+MXSMLEX_DEVICE_ARCH_KEPLER   = 2
+MXSMLEX_DEVICE_ARCH_MAXWELL  = 3
+MXSMLEX_DEVICE_ARCH_PASCAL   = 4
+MXSMLEX_DEVICE_ARCH_VOLTA    = 5
+MXSMLEX_DEVICE_ARCH_TURING   = 6
+MXSMLEX_DEVICE_ARCH_AMPERE   = 7
+MXSMLEX_DEVICE_ARCH_ADA      = 8
+MXSMLEX_DEVICE_ARCH_HOPPER   = 9
+MXSMLEX_DEVICE_ARCH_UNKNOWN  = 0xffffffff
+
+# PCI bus Types
+_nvmlBusType_t = c_uint
+MXSMLEX_BUS_TYPE_UNKNOWN = 0
+MXSMLEX_BUS_TYPE_PCI     = 1
+MXSMLEX_BUS_TYPE_PCIE    = 2
+MXSMLEX_BUS_TYPE_FPCI    = 3
+MXSMLEX_BUS_TYPE_AGP     = 4
+
+_nvmlPowerSource_t = c_uint
+MXSMLEX_POWER_SOURCE_AC         = 0x00000000
+MXSMLEX_POWER_SOURCE_BATTERY    = 0x00000001
+MXSMLEX_POWER_SOURCE_UNDERSIZED = 0x00000002
+
+_nvmlAdaptiveClockInfoStatus_t = c_uint
+MXSMLEX_ADAPTIVE_CLOCKING_INFO_STATUS_DISABLED = 0x00000000
+MXSMLEX_ADAPTIVE_CLOCKING_INFO_STATUS_ENABLED = 0x00000001
+
+_nvmlClockLimitId_t = c_uint
+MXSMLEX_CLOCK_LIMIT_ID_RANGE_START = 0xffffff00
+MXSMLEX_CLOCK_LIMIT_ID_TDP         = 0xffffff01
+MXSMLEX_CLOCK_LIMIT_ID_UNLIMITED   = 0xffffff02
+
+_nvmlPcieLinkMaxSpeed_t = c_uint
+MXSMLEX_PCIE_LINK_MAX_SPEED_INVALID   = 0x00000000
+MXSMLEX_PCIE_LINK_MAX_SPEED_2500MBPS  = 0x00000001
+MXSMLEX_PCIE_LINK_MAX_SPEED_5000MBPS  = 0x00000002
+MXSMLEX_PCIE_LINK_MAX_SPEED_8000MBPS  = 0x00000003
+MXSMLEX_PCIE_LINK_MAX_SPEED_16000MBPS = 0x00000004
+MXSMLEX_PCIE_LINK_MAX_SPEED_32000MBPS = 0x00000005
+MXSMLEX_PCIE_LINK_MAX_SPEED_64000MBPS = 0x00000006
+
+_nvmlAffinityScope_t = c_uint
+MXSMLEX_AFFINITY_SCOPE_NODE   = 0
+MXSMLEX_AFFINITY_SCOPE_SOCKET = 1
+
+# C preprocessor defined values
+nvmlFlagDefault             = 0
+nvmlFlagForce               = 1
+MXSMLEX_INIT_FLAG_NO_GPUS      = 1
+MXSMLEX_INIT_FLAG_NO_ATTACH    = 2
+
+MXSMLEX_MAX_GPC_COUNT          = 32
+
+# buffer size
+MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE      = 16
+MXSMLEX_DEVICE_UUID_BUFFER_SIZE                 = 80
+MXSMLEX_DEVICE_UUID_V2_BUFFER_SIZE              = 96
+MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE       = 80
+MXSMLEX_SYSTEM_MXSMLEX_VERSION_BUFFER_SIZE         = 80
+MXSMLEX_DEVICE_NAME_BUFFER_SIZE                 = 64
+MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE              = 96
+MXSMLEX_DEVICE_SERIAL_BUFFER_SIZE               = 30
+MXSMLEX_DEVICE_PART_NUMBER_BUFFER_SIZE          = 80
+MXSMLEX_DEVICE_GPU_PART_NUMBER_BUFFER_SIZE      = 80
+MXSMLEX_DEVICE_VBIOS_VERSION_BUFFER_SIZE        = 32
+MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE           = 32
+MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE        = 16
+MXSMLEX_GRID_LICENSE_BUFFER_SIZE                = 128
+MXSMLEX_VGPU_NAME_BUFFER_SIZE                   = 64
+MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT          = 3
+MXSMLEX_VGPU_METADATA_OPAQUE_DATA_SIZE          = sizeof(c_uint) + 256
+MXSMLEX_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE     = 256
+MXSMLEX_DEVICE_GPU_FRU_PART_NUMBER_BUFFER_SIZE  = 0x14 # NV2080_GPU_MAX_PRODUCT_PART_NUMBER_LENGTH
+
+# Format strings
+MXSMLEX_DEVICE_PCI_BUS_ID_LEGACY_FMT   = "%04X:%02X:%02X.0"
+MXSMLEX_DEVICE_PCI_BUS_ID_FMT          = "%08X:%02X:%02X.0"
+
+MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong = c_ulonglong(-1)
+MXSMLEX_VALUE_NOT_AVAILABLE_uint = c_uint(-1)
+
+'''
+ Field Identifiers.
+
+ All Identifiers pertain to a device. Each ID is only used once and is guaranteed never to change.
+'''
+MXSMLEX_FI_DEV_ECC_CURRENT          = 1   # Current ECC mode. 1=Active. 0=Inactive
+MXSMLEX_FI_DEV_ECC_PENDING          = 2   # Pending ECC mode. 1=Active. 0=Inactive
+
+#ECC Count Totals
+MXSMLEX_FI_DEV_ECC_SBE_VOL_TOTAL    = 3   # Total single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_TOTAL    = 4   # Total double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_TOTAL    = 5   # Total single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_TOTAL    = 6   # Total double bit aggregate (persistent) ECC errors
+#Individual ECC locations
+MXSMLEX_FI_DEV_ECC_SBE_VOL_L1       = 7   # L1 cache single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_L1       = 8   # L1 cache double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_L2       = 9   # L2 cache single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_L2       = 10  # L2 cache double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_DEV      = 11  # Device memory single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_DEV      = 12  # Device memory double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_REG      = 13  # Register file single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_REG      = 14  # Register file double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_TEX      = 15  # Texture memory single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_TEX      = 16  # Texture memory double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_CBU      = 17  # CBU double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_L1       = 18  # L1 cache single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_L1       = 19  # L1 cache double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_L2       = 20  # L2 cache single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_L2       = 21  # L2 cache double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_DEV      = 22  # Device memory single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_DEV      = 23  # Device memory double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_REG      = 24  # Register File single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_REG      = 25  # Register File double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_TEX      = 26  # Texture memory single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_TEX      = 27  # Texture memory double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_CBU      = 28  # CBU double bit aggregate ECC errors
+
+# Page Retirement
+MXSMLEX_FI_DEV_RETIRED_SBE          = 29  # Number of retired pages because of single bit errors
+MXSMLEX_FI_DEV_RETIRED_DBE          = 30  # Number of retired pages because of double bit errors
+MXSMLEX_FI_DEV_RETIRED_PENDING      = 31  # If any pages are pending retirement. 1=yes. 0=no.
+
+# NvLink Flit Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L0   = 32 # NVLink flow control CRC  Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L1   = 33 # NVLink flow control CRC  Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L2   = 34 # NVLink flow control CRC  Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L3   = 35 # NVLink flow control CRC  Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L4   = 36 # NVLink flow control CRC  Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L5   = 37 # NVLink flow control CRC  Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_TOTAL = 38 # NVLink flow control CRC  Error Counter total for all Lanes
+
+# NvLink CRC Data Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L0   = 39 # NVLink data CRC Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L1   = 40 # NVLink data CRC Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L2   = 41 # NVLink data CRC Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L3   = 42 # NVLink data CRC Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L4   = 43 # NVLink data CRC Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L5   = 44 # NVLink data CRC Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_TOTAL = 45 # NvLink data CRC Error Counter total for all Lanes
+
+# NvLink Replay Error Counters
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L0     = 46 # NVLink Replay Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L1     = 47 # NVLink Replay Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L2     = 48 # NVLink Replay Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L3     = 49 # NVLink Replay Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L4     = 50 # NVLink Replay Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L5     = 51 # NVLink Replay Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_TOTAL  = 52 # NVLink Replay Error Counter total for all Lanes
+
+# NvLink Recovery Error Counters
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L0   = 53 # NVLink Recovery Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L1   = 54 # NVLink Recovery Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L2   = 55 # NVLink Recovery Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L3   = 56 # NVLink Recovery Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L4   = 57 # NVLink Recovery Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L5   = 58 # NVLink Recovery Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_TOTAL = 59 # NVLink Recovery Error Counter total for all Lanes
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L0    = 60 # NVLink Bandwidth Counter for Counter Set 0, Lane 0
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L1    = 61 # NVLink Bandwidth Counter for Counter Set 0, Lane 1
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L2    = 62 # NVLink Bandwidth Counter for Counter Set 0, Lane 2
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L3    = 63 # NVLink Bandwidth Counter for Counter Set 0, Lane 3
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L4    = 64 # NVLink Bandwidth Counter for Counter Set 0, Lane 4
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L5    = 65 # NVLink Bandwidth Counter for Counter Set 0, Lane 5
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_TOTAL = 66 # NVLink Bandwidth Counter Total for Counter Set 0, All Lanes
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L0    = 67 # NVLink Bandwidth Counter for Counter Set 1, Lane 0
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L1    = 68 # NVLink Bandwidth Counter for Counter Set 1, Lane 1
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L2    = 69 # NVLink Bandwidth Counter for Counter Set 1, Lane 2
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L3    = 70 # NVLink Bandwidth Counter for Counter Set 1, Lane 3
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L4    = 71 # NVLink Bandwidth Counter for Counter Set 1, Lane 4
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L5    = 72 # NVLink Bandwidth Counter for Counter Set 1, Lane 5
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_TOTAL = 73 # NVLink Bandwidth Counter Total for Counter Set 1, All Lanes
+
+# Perf Policy Counters
+MXSMLEX_FI_DEV_PERF_POLICY_POWER             = 74   # Perf Policy Counter for Power Policy
+MXSMLEX_FI_DEV_PERF_POLICY_THERMAL           = 75   # Perf Policy Counter for Thermal Policy
+MXSMLEX_FI_DEV_PERF_POLICY_SYNC_BOOST        = 76   # Perf Policy Counter for Sync boost Policy
+MXSMLEX_FI_DEV_PERF_POLICY_BOARD_LIMIT       = 77   # Perf Policy Counter for Board Limit
+MXSMLEX_FI_DEV_PERF_POLICY_LOW_UTILIZATION   = 78   # Perf Policy Counter for Low GPU Utilization Policy
+MXSMLEX_FI_DEV_PERF_POLICY_RELIABILITY       = 79   # Perf Policy Counter for Reliability Policy
+MXSMLEX_FI_DEV_PERF_POLICY_TOTAL_APP_CLOCKS  = 80   # Perf Policy Counter for Total App Clock Policy
+MXSMLEX_FI_DEV_PERF_POLICY_TOTAL_BASE_CLOCKS = 81   # Perf Policy Counter for Total Base Clocks Policy
+
+# Memory temperatures
+MXSMLEX_FI_DEV_MEMORY_TEMP  = 82 # Memory temperature for the device
+
+# Energy Counter
+MXSMLEX_FI_DEV_TOTAL_ENERGY_CONSUMPTION = 83 # Total energy consumption for the GPU in mJ since the driver was last reloaded
+
+# NVLink Speed
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L0     = 84
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L1     = 85
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L2     = 86
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L3     = 87
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L4     = 88
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L5     = 89
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_COMMON = 90
+
+# NVLink Link Count
+MXSMLEX_FI_DEV_NVLINK_LINK_COUNT = 91
+
+# Page Retirement pending fields
+MXSMLEX_FI_DEV_RETIRED_PENDING_SBE = 92
+MXSMLEX_FI_DEV_RETIRED_PENDING_DBE = 93
+
+# PCIe replay and replay rollover counters
+MXSMLEX_FI_DEV_PCIE_REPLAY_COUNTER = 94
+MXSMLEX_FI_DEV_PCIE_REPLAY_ROLLOVER_COUNTER = 95
+
+# NvLink Flit Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L6   = 96 # NVLink flow control CRC  Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L7   = 97 # NVLink flow control CRC  Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L8   = 98 # NVLink flow control CRC  Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L9   = 99 # NVLink flow control CRC  Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L10  = 100 # NVLink flow control CRC  Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L11  = 101 # NVLink flow control CRC  Error Counter for Lane 11
+
+# NvLink CRC Data Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L6   = 102 # NVLink data CRC Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L7   = 103 # NVLink data CRC Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L8   = 104 # NVLink data CRC Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L9   = 105 # NVLink data CRC Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L10  = 106 # NVLink data CRC Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L11  = 107 # NVLink data CRC Error Counter for Lane 11
+
+# NvLink Replay Error Counters
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L6     = 108 # NVLink Replay Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L7     = 109 # NVLink Replay Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L8     = 110 # NVLink Replay Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L9     = 111 # NVLink Replay Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L10    = 112 # NVLink Replay Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L11    = 113 # NVLink Replay Error Counter for Lane 11
+
+# NvLink Recovery Error Counters
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L6   = 114 # NVLink Recovery Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L7   = 115 # NVLink Recovery Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L8   = 116 # NVLink Recovery Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L9   = 117 # NVLink Recovery Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L10  = 118 # NVLink Recovery Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L11  = 119 # NVLink Recovery Error Counter for Lane 11
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L6    = 120 # NVLink Bandwidth Counter for Counter Set 0, Lane 6
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L7    = 121 # NVLink Bandwidth Counter for Counter Set 0, Lane 7
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L8    = 122 # NVLink Bandwidth Counter for Counter Set 0, Lane 8
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L9    = 123 # NVLink Bandwidth Counter for Counter Set 0, Lane 9
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L10   = 124 # NVLink Bandwidth Counter for Counter Set 0, Lane 10
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L11   = 125 # NVLink Bandwidth Counter for Counter Set 0, Lane 11
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L6    = 126 # NVLink Bandwidth Counter for Counter Set 1, Lane 6
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L7    = 127 # NVLink Bandwidth Counter for Counter Set 1, Lane 7
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L8    = 128 # NVLink Bandwidth Counter for Counter Set 1, Lane 8
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L9    = 129 # NVLink Bandwidth Counter for Counter Set 1, Lane 9
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L10   = 130 # NVLink Bandwidth Counter for Counter Set 1, Lane 10
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L11   = 131 # NVLink Bandwidth Counter for Counter Set 1, Lane 11
+
+# NVLink Speed
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L6     = 132
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L7     = 133
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L8     = 134
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L9     = 135
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L10    = 136
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L11    = 137
+
+# NVLink Throughput Counters
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_DATA_TX = 138 # NVLink TX Data throughput in KiB
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_DATA_RX = 139 # NVLink RX Data throughput in KiB
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_RAW_TX  = 140 # NVLink TX Data + protocol overhead in KiB
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_RAW_RX  = 141 # NVLink RX Data + protocol overhead in KiB
+
+# Row Remapper
+MXSMLEX_FI_DEV_REMAPPED_COR        = 142
+MXSMLEX_FI_DEV_REMAPPED_UNC        = 143
+MXSMLEX_FI_DEV_REMAPPED_PENDING    = 144
+MXSMLEX_FI_DEV_REMAPPED_FAILURE    = 145
+
+#Remote device NVLink ID
+MXSMLEX_FI_DEV_NVLINK_REMOTE_NVLINK_ID = 146
+
+# Number of NVLinks connected to NVSwitch
+MXSMLEX_FI_DEV_NVSWITCH_CONNECTED_LINK_COUNT = 147
+
+# NvLink ECC Data Error Counters
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L0    = 148 #< NVLink data ECC Error Counter for Link 0
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L1    = 149 #< NVLink data ECC Error Counter for Link 1
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L2    = 150 #< NVLink data ECC Error Counter for Link 2
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L3    = 151 #< NVLink data ECC Error Counter for Link 3
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L4    = 152 #< NVLink data ECC Error Counter for Link 4
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L5    = 153 #< NVLink data ECC Error Counter for Link 5
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L6    = 154 #< NVLink data ECC Error Counter for Link 6
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L7    = 155 #< NVLink data ECC Error Counter for Link 7
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L8    = 156 #< NVLink data ECC Error Counter for Link 8
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L9    = 157 #< NVLink data ECC Error Counter for Link 9
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L10   = 158 #< NVLink data ECC Error Counter for Link 10
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L11   = 159 #< NVLink data ECC Error Counter for Link 11
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_TOTAL = 160 #< NvLink data ECC Error Counter total for all Links
+
+MXSMLEX_FI_DEV_NVLINK_ERROR_DL_REPLAY            = 161
+MXSMLEX_FI_DEV_NVLINK_ERROR_DL_RECOVERY          = 162
+MXSMLEX_FI_DEV_NVLINK_ERROR_DL_CRC               = 163
+MXSMLEX_FI_DEV_NVLINK_GET_SPEED                  = 164
+MXSMLEX_FI_DEV_NVLINK_GET_STATE                  = 165
+MXSMLEX_FI_DEV_NVLINK_GET_VERSION                = 166
+
+MXSMLEX_FI_DEV_NVLINK_GET_POWER_STATE            = 167
+MXSMLEX_FI_DEV_NVLINK_GET_POWER_THRESHOLD        = 168
+
+MXSMLEX_FI_DEV_PCIE_L0_TO_RECOVERY_COUNTER       = 169
+
+MXSMLEX_FI_DEV_C2C_LINK_COUNT                    = 170
+MXSMLEX_FI_DEV_C2C_LINK_GET_STATUS               = 171
+MXSMLEX_FI_DEV_C2C_LINK_GET_MAX_BW               = 172
+
+MXSMLEX_FI_DEV_PCIE_COUNT_CORRECTABLE_ERRORS     = 173
+MXSMLEX_FI_DEV_PCIE_COUNT_NAKS_RECEIVED          = 174
+MXSMLEX_FI_DEV_PCIE_COUNT_RECEIVER_ERROR         = 175
+MXSMLEX_FI_DEV_PCIE_COUNT_BAD_TLP                = 176
+MXSMLEX_FI_DEV_PCIE_COUNT_NAKS_SENT              = 177
+MXSMLEX_FI_DEV_PCIE_COUNT_BAD_DLLP               = 178
+MXSMLEX_FI_DEV_PCIE_COUNT_NON_FATAL_ERROR        = 179
+MXSMLEX_FI_DEV_PCIE_COUNT_FATAL_ERROR            = 180
+MXSMLEX_FI_DEV_PCIE_COUNT_UNSUPPORTED_REQ        = 181
+MXSMLEX_FI_DEV_PCIE_COUNT_LCRC_ERROR             = 182
+MXSMLEX_FI_DEV_PCIE_COUNT_LANE_ERROR             = 183
+
+MXSMLEX_FI_DEV_IS_RESETLESS_MIG_SUPPORTED        = 184
+
+MXSMLEX_FI_DEV_POWER_AVERAGE                     = 185
+MXSMLEX_FI_DEV_POWER_INSTANT                     = 186
+MXSMLEX_FI_DEV_POWER_MIN_LIMIT                   = 187
+MXSMLEX_FI_DEV_POWER_MAX_LIMIT                   = 188
+MXSMLEX_FI_DEV_POWER_DEFAULT_LIMIT               = 189
+MXSMLEX_FI_DEV_POWER_CURRENT_LIMIT               = 190
+MXSMLEX_FI_DEV_ENERGY                            = 191
+MXSMLEX_FI_DEV_POWER_REQUESTED_LIMIT             = 192
+
+MXSMLEX_FI_DEV_TEMPERATURE_SHUTDOWN_TLIMIT       = 193
+MXSMLEX_FI_DEV_TEMPERATURE_SLOWDOWN_TLIMIT       = 194
+MXSMLEX_FI_DEV_TEMPERATURE_MEM_MAX_TLIMIT        = 195
+MXSMLEX_FI_DEV_TEMPERATURE_GPU_MAX_TLIMIT        = 196
+
+MXSMLEX_FI_DEV_IS_MIG_MODE_INDEPENDENT_MIG_QUERY_CAPABLE   = 199
+
+MXSMLEX_FI_MAX = 200 # One greater than the largest field ID defined above
+
+
+## Enums needed for the method nvmlDeviceGetVirtualizationMode and nvmlDeviceSetVirtualizationMode
+MXSMLEX_GPU_VIRTUALIZATION_MODE_NONE        = 0  # Represents Bare Metal GPU
+MXSMLEX_GPU_VIRTUALIZATION_MODE_PASSTHROUGH = 1  # Device is associated with GPU-Passthorugh
+MXSMLEX_GPU_VIRTUALIZATION_MODE_VGPU        = 2  # Device is associated with vGPU inside virtual machine.
+MXSMLEX_GPU_VIRTUALIZATION_MODE_HOST_VGPU   = 3  # Device is associated with VGX hypervisor in vGPU mode
+MXSMLEX_GPU_VIRTUALIZATION_MODE_HOST_VSGA   = 4  # Device is associated with VGX hypervisor in vSGA mode
+
+## Lib loading ##
+nvmlLib = None
+libLoadLock = threading.Lock()
+_nvmlLib_refcount = 0 # Incremented on each nvmlInit and decremented on nvmlShutdown
+
+## vGPU Management
+_nvmlVgpuTypeId_t   = c_uint
+_nvmlVgpuInstance_t = c_uint
+
+_nvmlVgpuVmIdType_t = c_uint
+MXSMLEX_VGPU_VM_ID_DOMAIN_ID    = 0
+MXSMLEX_VGPU_VM_ID_UUID         = 1
+
+_nvmlGridLicenseFeatureCode_t = c_uint
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_UNKNOWN      = 0
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_VGPU         = 1
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_NVIDIA_RTX   = 2
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_VWORKSTATION = 2 # deprecated, use MXSMLEX_GRID_LICENSE_FEATURE_CODE_NVIDIA_RTX.
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_GAMING       = 3
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_COMPUTE      = 4
+
+_nvmlGridLicenseExpiryStatus_t = c_uint8
+MXSMLEX_GRID_LICENSE_EXPIRY_NOT_AVAILABLE    = 0,   # Expiry information not available
+MXSMLEX_GRID_LICENSE_EXPIRY_INVALID          = 1,   # Invalid expiry or error fetching expiry
+MXSMLEX_GRID_LICENSE_EXPIRY_VALID            = 2,   # Valid expiry
+MXSMLEX_GRID_LICENSE_EXPIRY_NOT_APPLICABLE   = 3,   # Expiry not applicable
+MXSMLEX_GRID_LICENSE_EXPIRY_PERMANENT        = 4,   # Permanent expiry
+
+_nvmlVgpuCapability_t = c_uint
+MXSMLEX_VGPU_CAP_NVLINK_P2P                    = 0  # vGPU P2P over NVLink is supported
+MXSMLEX_VGPU_CAP_GPUDIRECT                     = 1  # GPUDirect capability is supported
+MXSMLEX_VGPU_CAP_MULTI_VGPU_EXCLUSIVE          = 2  # vGPU profile cannot be mixed with other vGPU profiles in same VM
+MXSMLEX_VGPU_CAP_EXCLUSIVE_TYPE                = 3  # vGPU profile cannot run on a GPU alongside other profiles of different type
+MXSMLEX_VGPU_CAP_EXCLUSIVE_SIZE                = 4  # vGPU profile cannot run on a GPU alongside other profiles of different size
+MXSMLEX_VGPU_CAP_COUNT                         = 5
+
+_nvmlVgpuDriverCapability_t = c_uint
+MXSMLEX_VGPU_DRIVER_CAP_HETEROGENEOUS_MULTI_VGPU          = 0  # Supports mixing of different vGPU profiles within one guest VM
+MXSMLEX_VGPU_DRIVER_CAP_COUNT                             = 1
+
+_nvmlDeviceVgpuCapability_t = c_uint
+MXSMLEX_DEVICE_VGPU_CAP_FRACTIONAL_MULTI_VGPU             = 0  # Query if the fractional vGPU profiles on this GPU can be used in multi-vGPU configurations
+MXSMLEX_DEVICE_VGPU_CAP_HETEROGENEOUS_TIMESLICE_PROFILES  = 1  # Query if the GPU supports concurrent execution of timesliced vGPU profiles of differing types
+MXSMLEX_DEVICE_VGPU_CAP_HETEROGENEOUS_TIMESLICE_SIZES     = 2  # Query if the GPU supports concurrent execution of timesliced vGPU profiles of differing framebuffer sizes
+MXSMLEX_DEVICE_VGPU_CAP_READ_DEVICE_BUFFER_BW             = 3  # Query the GPU's read_device_buffer expected bandwidth capacity in megabytes per second
+MXSMLEX_DEVICE_VGPU_CAP_WRITE_DEVICE_BUFFER_BW            = 4  # Query the GPU's write_device_buffer expected bandwidth capacity in megabytes per second
+MXSMLEX_DEVICE_VGPU_CAP_DEVICE_STREAMING                  = 5  # Query if vGPU profiles on the GPU supports migration data streaming
+MXSMLEX_DEVICE_VGPU_CAP_MINI_QUARTER_GPU                  = 6  # Set/Get support of mini-quarter vGPU profiles
+MXSMLEX_DEVICE_VGPU_CAP_COMPUTE_MEDIA_ENGINE_GPU          = 7  # Set/Get support for compute media engine vGPU profiles
+MXSMLEX_DEVICE_VGPU_CAP_COUNT                             = 8
+
+_nvmlVgpuGuestInfoState_t = c_uint
+MXSMLEX_VGPU_INSTANCE_GUEST_INFO_STATE_UNINITIALIZED = 0
+MXSMLEX_VGPU_INSTANCE_GUEST_INFO_STATE_INITIALIZED   = 1
+
+_nvmlVgpuVmCompatibility_t = c_uint
+MXSMLEX_VGPU_VM_COMPATIBILITY_NONE         = 0x0
+MXSMLEX_VGPU_VM_COMPATIBILITY_COLD         = 0x1
+MXSMLEX_VGPU_VM_COMPATIBILITY_HIBERNATE    = 0x2
+MXSMLEX_VGPU_VM_COMPATIBILITY_SLEEP        = 0x4
+MXSMLEX_VGPU_VM_COMPATIBILITY_LIVE         = 0x8
+
+_nvmlVgpuPgpuCompatibilityLimitCode_t = c_uint
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_NONE          = 0x0
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_HOST_DRIVER   = 0x1
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_GUEST_DRIVER  = 0x2
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_GPU           = 0x4
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_OTHER         = 0x80000000
+
+_nvmlHostVgpuMode_t = c_uint
+MXSMLEX_HOST_VGPU_MODE_NON_SRIOV   = 0
+MXSMLEX_HOST_VGPU_MODE_SRIOV       = 1
+
+_nvmlConfComputeGpusReadyState_t = c_uint
+MXSMLEX_CC_ACCEPTING_CLIENT_REQUESTS_FALSE = 0
+MXSMLEX_CC_ACCEPTING_CLIENT_REQUESTS_TRUE = 1
+
+_nvmlConfComputeGpuCaps_t = c_uint
+MXSMLEX_CC_SYSTEM_GPUS_CC_NOT_CAPABLE = 0
+MXSMLEX_CC_SYSTEM_GPUS_CC_CAPABLE = 1
+
+_nvmlConfComputeCpuCaps_t = c_uint
+MXSMLEX_CC_SYSTEM_CPU_CAPS_NONE = 0
+MXSMLEX_CC_SYSTEM_CPU_CAPS_AMD_SEV = 1
+MXSMLEX_CC_SYSTEM_CPU_CAPS_INTEL_TDX = 2
+
+_nvmlConfComputeDevToolsMode_t = c_uint
+MXSMLEX_CC_SYSTEM_DEVTOOLS_MODE_OFF = 0
+MXSMLEX_CC_SYSTEM_DEVTOOLS_MODE_ON = 1
+ 
+MXSMLEX_CC_SYSTEM_ENVIRONMENT_UNAVAILABLE = 0
+MXSMLEX_CC_SYSTEM_ENVIRONMENT_SIM = 1
+MXSMLEX_CC_SYSTEM_ENVIRONMENT_PROD = 2
+ 
+_nvmlConfComputeCcFeature_t = c_uint
+MXSMLEX_CC_SYSTEM_FEATURE_DISABLED = 0
+MXSMLEX_CC_SYSTEM_FEATURE_ENABLED = 1
+
+_nvmlConfComputeCcKeyRotationThreshAttackerAdv_t = c_uint
+MXSMLEX_CC_KEY_ROTATION_THRESH_ATTACKER_ADVANTAGE_MIN = 50
+MXSMLEX_CC_KEY_ROTATION_THRESH_ATTACKER_ADVANTAGE_MAX = 75
+
+# GSP firmware
+MXSMLEX_GSP_FIRMWARE_VERSION_BUF_SIZE = 0x40
+
+class MXSMLEXLibraryMismatchError(Exception):
+    pass
+
+## Error Checking ##
+class NVMLError(Exception):
+    _valClassMapping = dict()
+    # List of currently known error codes
+    _errcode_to_string = {
+        MXSMLEX_ERROR_UNINITIALIZED:       "Uninitialized",
+        MXSMLEX_ERROR_INVALID_ARGUMENT:    "Invalid Argument",
+        MXSMLEX_ERROR_NOT_SUPPORTED:       "Not Supported",
+        MXSMLEX_ERROR_NO_PERMISSION:       "Insufficient Permissions",
+        MXSMLEX_ERROR_ALREADY_INITIALIZED: "Already Initialized",
+        MXSMLEX_ERROR_NOT_FOUND:           "Not Found",
+        MXSMLEX_ERROR_INSUFFICIENT_SIZE:   "Insufficient Size",
+        MXSMLEX_ERROR_INSUFFICIENT_POWER:  "Insufficient External Power",
+        MXSMLEX_ERROR_DRIVER_NOT_LOADED:   "Driver Not Loaded",
+        MXSMLEX_ERROR_TIMEOUT:             "Timeout",
+        MXSMLEX_ERROR_IRQ_ISSUE:           "Interrupt Request Issue",
+        MXSMLEX_ERROR_LIBRARY_NOT_FOUND:   "MXSMLEX Shared Library Not Found",
+        MXSMLEX_ERROR_FUNCTION_NOT_FOUND:  "Function Not Found",
+        MXSMLEX_ERROR_CORRUPTED_INFOROM:   "Corrupted infoROM",
+        MXSMLEX_ERROR_GPU_IS_LOST:         "GPU is lost",
+        MXSMLEX_ERROR_RESET_REQUIRED:      "GPU requires restart",
+        MXSMLEX_ERROR_OPERATING_SYSTEM:    "The operating system has blocked the request.",
+        MXSMLEX_ERROR_LIB_RM_VERSION_MISMATCH: "RM has detected an MXSMLEX/RM version mismatch.",
+        MXSMLEX_ERROR_MEMORY:              "Insufficient Memory",
+        MXSMLEX_ERROR_UNKNOWN:             "Unknown Error",
+        }
+    def __new__(typ, value):
+        '''
+        Maps value to a proper subclass of NVMLError.
+        See _extractNVMLErrorsAsClasses function for more details
+        '''
+        if typ == NVMLError:
+            typ = NVMLError._valClassMapping.get(value, typ)
+        obj = Exception.__new__(typ)
+        obj.value = value
+        return obj
+    def __str__(self):
+        try:
+            if self.value not in NVMLError._errcode_to_string:
+                NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value))
+            return NVMLError._errcode_to_string[self.value]
+        except NVMLError:
+            return "MXSMLEX Error with code %d" % self.value
+    def __eq__(self, other):
+        return self.value == other.value
+
+def nvmlExceptionClass(nvmlErrorCode):
+    if nvmlErrorCode not in NVMLError._valClassMapping:
+        raise ValueError('nvmlErrorCode %s is not valid' % nvmlErrorCode)
+    return NVMLError._valClassMapping[nvmlErrorCode]
+
+def _extractNVMLErrorsAsClasses():
+    '''
+    Generates a hierarchy of classes on top of NVMLError class.
+
+    Each MXSMLEX Error gets a new NVMLError subclass. This way try,except blocks can filter appropriate
+    exceptions more easily.
+
+    NVMLError is a parent class. Each MXSMLEX_ERROR_* gets it's own subclass.
+    e.g. MXSMLEX_ERROR_ALREADY_INITIALIZED will be turned into NVMLError_AlreadyInitialized
+    '''
+    this_module = sys.modules[__name__]
+    nvmlErrorsNames = [x for x in dir(this_module) if x.startswith("MXSMLEX_ERROR_")]
+    for err_name in nvmlErrorsNames:
+        # e.g. Turn MXSMLEX_ERROR_ALREADY_INITIALIZED into NVMLError_AlreadyInitialized
+        class_name = "NVMLError_" + string.capwords(err_name.replace("MXSMLEX_ERROR_", ""), "_").replace("_", "")
+        err_val = getattr(this_module, err_name)
+        def gen_new(val):
+            def new(typ):
+                obj = NVMLError.__new__(typ, val)
+                return obj
+            return new
+        new_error_class = type(class_name, (NVMLError,), {'__new__': gen_new(err_val)})
+        new_error_class.__module__ = __name__
+        setattr(this_module, class_name, new_error_class)
+        NVMLError._valClassMapping[err_val] = new_error_class
+_extractNVMLErrorsAsClasses()
+
+def _nvmlCheckReturn(ret):
+    if (ret != MXSMLEX_SUCCESS):
+        raise NVMLError(ret)
+    return ret
+
+## Function access ##
+_nvmlGetFunctionPointer_cache = dict() # function pointers are cached to prevent unnecessary libLoadLock locking
+def _nvmlGetFunctionPointer(name):
+    global nvmlLib
+
+    if name in _nvmlGetFunctionPointer_cache:
+        return _nvmlGetFunctionPointer_cache[name]
+
+    libLoadLock.acquire()
+    try:
+        # ensure library was loaded
+        if (nvmlLib == None):
+            raise NVMLError(MXSMLEX_ERROR_UNINITIALIZED)
+        try:
+            _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name)
+            return _nvmlGetFunctionPointer_cache[name]
+        except AttributeError:
+            print("nvml error")
+            #raise NVMLError(MXSMLEX_ERROR_FUNCTION_NOT_FOUND)
+    finally:
+        # lock is always freed
+        libLoadLock.release()
+
+## Alternative object
+# Allows the object to be printed
+# Allows mismatched types to be assigned
+#  - like None when the Structure variant requires c_uint
+class nvmlFriendlyObject(object):
+    def __init__(self, dictionary):
+        for x in dictionary:
+            setattr(self, x, dictionary[x])
+    def __str__(self):
+        return self.__dict__.__str__()
+
+def nvmlStructToFriendlyObject(struct):
+    d = {}
+    for x in struct._fields_:
+        key = x[0]
+        value = getattr(struct, key)
+        # only need to convert from bytes if bytes, no need to check python version.
+        d[key] = value.decode() if isinstance(value, bytes) else value
+    obj = nvmlFriendlyObject(d)
+    return obj
+
+# pack the object so it can be passed to the MXSMLEX library
+def nvmlFriendlyObjectToStruct(obj, model):
+    for x in model._fields_:
+        key = x[0]
+        value = obj.__dict__[key]
+        # any c_char_p in python3 needs to be bytes, default encoding works fine.
+        if sys.version_info >= (3,):
+            setattr(model, key, value.encode())
+        else:
+            setattr(model, key, value)
+    return model
+
+## Unit structures
+class struct_c_nvmlUnit_t(Structure):
+    pass # opaque handle
+c_nvmlUnit_t = POINTER(struct_c_nvmlUnit_t)
+
+class _PrintableStructure(Structure):
+    """
+    Abstract class that produces nicer __str__ output than ctypes.Structure.
+    e.g. instead of:
+      >>> print str(obj)
+      <class_name object at 0x7fdf82fef9e0>
+    this class will print
+      class_name(field_name: formatted_value, field_name: formatted_value)
+
+    _fmt_ dictionary of <str _field_ name> -> <str format>
+    e.g. class that has _field_ 'hex_value', c_uint could be formatted with
+      _fmt_ = {"hex_value" : "%08X"}
+    to produce nicer output.
+    Default fomratting string for all fields can be set with key "<default>" like:
+      _fmt_ = {"<default>" : "%d MHz"} # e.g all values are numbers in MHz.
+    If not set it's assumed to be just "%s"
+
+    Exact format of returned str from this class is subject to change in the future.
+    """
+    _fmt_ = {}
+    def __str__(self):
+        result = []
+        for x in self._fields_:
+            key = x[0]
+            value = getattr(self, key)
+            fmt = "%s"
+            if key in self._fmt_:
+                fmt = self._fmt_[key]
+            elif "<default>" in self._fmt_:
+                fmt = self._fmt_["<default>"]
+            result.append(("%s: " + fmt) % (key, value))
+        return self.__class__.__name__ + "(" +  ", ".join(result) + ")"
+
+    def __getattribute__(self, name):
+        res = super(_PrintableStructure, self).__getattribute__(name)
+        # need to convert bytes to unicode for python3 don't need to for python2
+        # Python 2 strings are of both str and bytes
+        # Python 3 strings are not of type bytes
+        # ctypes should convert everything to the correct values otherwise
+        if isinstance(res, bytes):
+            if isinstance(res, str):
+                return res
+            return res.decode()
+        return res
+
+    def __setattr__(self, name, value):
+        if isinstance(value, str):
+            # encoding a python2 string returns the same value, since python2 strings are bytes already
+            # bytes passed in python3 will be ignored.
+            value = value.encode()
+        super(_PrintableStructure, self).__setattr__(name, value)
+
+class c_nvmlUnitInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('name', c_char * 96),
+        ('id', c_char * 96),
+        ('serial', c_char * 96),
+        ('firmwareVersion', c_char * 96),
+    ]
+
+class c_nvmlC2cModeInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('isC2cEnabled', c_uint)
+    ]
+
+nvmlC2cModeInfo_v1 = 0x1000008;
+
+class c_nvmlLedState_t(_PrintableStructure):
+    _fields_ = [
+        ('cause', c_char * 256),
+        ('color', _nvmlLedColor_t),
+    ]
+
+class c_nvmlPSUInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('state', c_char * 256),
+        ('current', c_uint),
+        ('voltage', c_uint),
+        ('power', c_uint),
+    ]
+
+class c_nvmlUnitFanInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('speed', c_uint),
+        ('state', _nvmlFanState_t),
+    ]
+
+class c_nvmlUnitFanSpeeds_t(_PrintableStructure):
+    _fields_ = [
+        ('fans', c_nvmlUnitFanInfo_t * 24),
+        ('count', c_uint)
+    ]
+
+## Device structures
+class struct_c_nvmlDevice_t(Structure):
+    pass # opaque handle
+c_nvmlDevice_t = POINTER(struct_c_nvmlDevice_t)
+
+class nvmlPciInfoExt_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('domain', c_uint),
+        ('bus', c_uint),
+        ('device', c_uint),
+        ('pciDeviceId', c_uint),
+        ('pciSubSystemId', c_uint),
+        ('baseClass', c_uint),
+        ('subClass', c_uint),
+        ('busId', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE),
+    ]
+    _fmt_ = {
+            'version'        : "0x%04X",
+            'domain'         : "0x%04X",
+            'bus'            : "0x%02X",
+            'device'         : "0x%02X",
+            'pciDeviceId'    : "0x%08X",
+            'pciSubSystemId' : "0x%08X",
+            'baseClass'      : "0x%01X",
+            'subClass'       : "0x%01X",
+            }
+
+nvmlPciInfoExt_v1 = 0x1000040
+
+# Legacy pciInfo used for _v1 and _v2
+class nvmlPciInfo_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('busId', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE),
+        ('domain', c_uint),
+        ('bus', c_uint),
+        ('device', c_uint),
+        ('pciDeviceId', c_uint),
+
+        # Added in 2.285
+        ('pciSubSystemId', c_uint),
+        ('reserved0', c_uint),
+        ('reserved1', c_uint),
+        ('reserved2', c_uint),
+        ('reserved3', c_uint),
+    ]
+    _fmt_ = {
+            'domain'         : "0x%04X",
+            'bus'            : "0x%02X",
+            'device'         : "0x%02X",
+            'pciDeviceId'    : "0x%08X",
+            'pciSubSystemId' : "0x%08X",
+            }
+
+class nvmlPciInfo_t(_PrintableStructure):
+    _fields_ = [
+        # Moved to the new busId location below
+        ('busIdLegacy', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE),
+        ('domain', c_uint),
+        ('bus', c_uint),
+        ('device', c_uint),
+        ('pciDeviceId', c_uint),
+
+        # Added in 2.285
+        ('pciSubSystemId', c_uint),
+        # New busId replaced the long deprecated and reserved fields with a
+        # field of the same size in 9.0
+        ('busId', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE),
+    ]
+    _fmt_ = {
+            'domain'         : "0x%08X",
+            'bus'            : "0x%02X",
+            'device'         : "0x%02X",
+            'pciDeviceId'    : "0x%08X",
+            'pciSubSystemId' : "0x%08X",
+            }
+
+class c_nvmlExcludedDeviceInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('pci', nvmlPciInfo_t),
+        ('uuid', c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    ]
+
+class nvmlNvLinkUtilizationControl_t(_PrintableStructure):
+    _fields_ = [
+        ('units', _nvmlNvLinkUtilizationCountUnits_t),
+        ('pktfilter', _nvmlNvLinkUtilizationCountPktTypes_t),
+    ]
+
+class c_nvmlMemory_t(_PrintableStructure):
+    _fields_ = [
+        ('total', c_ulonglong),
+        ('free', c_ulonglong),
+        ('used', c_ulonglong),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+class c_nvmlMemory_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('total', c_ulonglong),
+        ('reserved', c_ulonglong),
+        ('free', c_ulonglong),
+        ('used', c_ulonglong),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+nvmlMemory_v2 = 0x02000028
+
+class c_nvmlBAR1Memory_t(_PrintableStructure):
+    _fields_ = [
+        ('bar1Total', c_ulonglong),
+        ('bar1Free', c_ulonglong),
+        ('bar1Used', c_ulonglong),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+class nvmlClkMonFaultInfo_t(Structure):
+    _fields_ = [("clkApiDomain", c_uint),
+                ("clkDomainFaultMask", c_uint)
+    ]
+
+class nvmlClkMonStatus_t(Structure):
+    _fields_ = [("bGlobalStatus", c_uint),
+                ("clkMonListSize", c_uint),
+                ("clkMonList", nvmlClkMonFaultInfo_t)
+    ]
+
+# On Windows with the WDDM driver, usedGpuMemory is reported as None
+# Code that processes this structure should check for None, I.E.
+#
+# if (info.usedGpuMemory == None):
+#     # TODO handle the error
+#     pass
+# else:
+#    print("Using %d MiB of memory" % (info.usedGpuMemory / 1024 / 1024))
+# endif
+#
+# See MXSMLEX documentation for more information
+class c_nvmlProcessInfo_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('pid', c_uint),
+        ('usedGpuMemory', c_ulonglong),
+        ('gpuInstanceId', c_uint),
+        ('computeInstanceId', c_uint),
+    ]
+    _fmt_ = {'usedGpuMemory': "%d B"}
+
+c_nvmlProcessInfo_v3_t = c_nvmlProcessInfo_v2_t
+
+c_nvmlProcessInfo_t = c_nvmlProcessInfo_v3_t
+
+_nvmlProcessMode_t = c_uint
+MXSMLEX_PROCESS_MODE_COMPUTE  = 0
+MXSMLEX_PROCESS_MODE_GRAPHICS = 1
+MXSMLEX_PROCESS_MODE_MPS      = 2
+
+class c_nvmlProcessDetail_v1_t(Structure):
+    _fields_ = [
+        ('pid', c_uint),
+        ('usedGpuMemory', c_ulonglong),
+        ('gpuInstanceId', c_uint),
+        ('computeInstanceId', c_uint),
+        ('usedGpuCcProtectedMemory', c_ulonglong),
+    ]
+
+class c_nvmlProcessDetailList_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('mode', _nvmlProcessMode_t),
+        ('numProcArrayEntries', c_uint),
+        ('procArray', POINTER(c_nvmlProcessDetail_v1_t)),
+    ]
+    _fmt_ = {'numProcArrayEntries': "%d B"}
+
+c_nvmlProcessDetailList_t = c_nvmlProcessDetailList_v1_t
+
+nvmlProcessDetailList_v1 = 0x1000018
+
+class c_nvmlBridgeChipInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('type', _nvmlBridgeChipType_t),
+        ('fwVersion', c_uint),
+    ]
+
+class c_nvmlBridgeChipHierarchy_t(_PrintableStructure):
+    _fields_ = [
+        ('bridgeCount', c_uint),
+        ('bridgeChipInfo', c_nvmlBridgeChipInfo_t * 128),
+    ]
+
+class c_nvmlEccErrorCounts_t(_PrintableStructure):
+    _fields_ = [
+        ('l1Cache', c_ulonglong),
+        ('l2Cache', c_ulonglong),
+        ('deviceMemory', c_ulonglong),
+        ('registerFile', c_ulonglong),
+    ]
+
+class c_nvmlUtilization_t(_PrintableStructure):
+    _fields_ = [
+        ('gpu', c_uint),
+        ('memory', c_uint),
+    ]
+    _fmt_ = {'<default>': "%d %%"}
+
+# Added in 2.285
+class c_nvmlHwbcEntry_t(_PrintableStructure):
+    _fields_ = [
+        ('hwbcId', c_uint),
+        ('firmwareVersion', c_char * 32),
+    ]
+
+class c_nvmlValue_t(Union):
+    _fields_ = [
+        ('dVal', c_double),
+        ('uiVal', c_uint),
+        ('ulVal', c_ulong),
+        ('ullVal', c_ulonglong),
+        ('sllVal', c_longlong),
+        ('siVal', c_int),
+    ]
+
+class c_nvmlSample_t(_PrintableStructure):
+    _fields_ = [
+        ('timeStamp', c_ulonglong),
+        ('sampleValue', c_nvmlValue_t),
+    ]
+
+class c_nvmlViolationTime_t(_PrintableStructure):
+    _fields_ = [
+        ('referenceTime', c_ulonglong),
+        ('violationTime', c_ulonglong),
+    ]
+
+class c_nvmlFieldValue_t(_PrintableStructure):
+    _fields_ = [
+        ('fieldId', c_uint32),
+        ('scopeId', c_uint32),
+        ('timestamp', c_int64),
+        ('latencyUsec', c_int64),
+        ('valueType', _nvmlValueType_t),
+        ('nvmlReturn', _nvmlReturn_t),
+        ('value', c_nvmlValue_t)
+    ]
+
+class c_nvmlVgpuHeterogeneousMode_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('mode', c_uint),
+    ]
+
+VgpuHeterogeneousMode_v1 = 0x1000008
+
+class c_nvmlVgpuPlacementId_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('placementId', c_uint),
+    ]
+
+VgpuPlacementId_v1 = 0x1000008
+
+class c_nvmlVgpuPlacementList_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('count', c_uint),
+        ('placementSize', c_uint),
+        ('placementIds', POINTER(c_uint)),
+    ]
+
+VgpuPlacementList_v1 = 0x1000018
+
+class c_nvmlVgpuInstanceUtilizationSample_t(_PrintableStructure):
+    _fields_ = [
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('timeStamp', c_ulonglong),
+        ('smUtil', c_nvmlValue_t),
+        ('memUtil', c_nvmlValue_t),
+        ('encUtil', c_nvmlValue_t),
+        ('decUtil', c_nvmlValue_t),
+    ]
+
+class c_nvmlVgpuInstanceUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('timeStamp', c_ulonglong),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('smUtil', c_nvmlValue_t),
+        ('memUtil', c_nvmlValue_t),
+        ('encUtil', c_nvmlValue_t),
+        ('decUtil', c_nvmlValue_t),
+        ('jpgUtil', c_nvmlValue_t),
+        ('ofaUtil', c_nvmlValue_t),
+    ]
+
+class c_nvmlVgpuInstancesUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('sampleValType', _nvmlValueType_t),
+        ('vgpuInstanceCount', c_uint),
+        ('lastSeenTimeStamp', c_ulonglong),
+        ('vgpuUtilArray', POINTER(c_nvmlVgpuInstanceUtilizationInfo_v1_t)),
+    ]
+
+VgpuInstancesUtilizationInfo_v1 = 0x01000020
+
+class c_nvmlVgpuProcessUtilizationSample_t(_PrintableStructure):
+    _fields_ = [
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('pid', c_uint),
+        ('processName', c_char * MXSMLEX_VGPU_NAME_BUFFER_SIZE),
+        ('timeStamp', c_ulonglong),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+    ]
+
+class c_nvmlVgpuProcessUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('processName', c_char * MXSMLEX_VGPU_NAME_BUFFER_SIZE),
+        ('timeStamp', c_ulonglong),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('pid', c_uint),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+        ('jpgUtil', c_uint),
+        ('ofaUtil', c_uint),
+    ]
+
+class c_nvmlVgpuProcessesUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('vgpuProcessCount', c_uint),
+        ('lastSeenTimeStamp', c_ulonglong),
+        ('vgpuProcUtilArray', POINTER(c_nvmlVgpuProcessUtilizationInfo_v1_t)),
+    ]
+
+VgpuProcessesUtilizationInfo_v1 = 0x01000018
+
+class c_nvmlVgpuLicenseExpiry_t(_PrintableStructure):
+    _fields_ = [
+        ('year',    c_uint32),
+        ('month',   c_uint16),
+        ('day',     c_uint16),
+        ('hour',    c_uint16),
+        ('min',     c_uint16),
+        ('sec',     c_uint16),
+        ('status',  c_uint8),
+    ]
+
+MXSMLEX_GRID_LICENSE_STATE_UNKNOWN                 = 0
+MXSMLEX_GRID_LICENSE_STATE_UNINITIALIZED           = 1
+MXSMLEX_GRID_LICENSE_STATE_UNLICENSED_UNRESTRICTED = 2
+MXSMLEX_GRID_LICENSE_STATE_UNLICENSED_RESTRICTED   = 3
+MXSMLEX_GRID_LICENSE_STATE_UNLICENSED              = 4
+MXSMLEX_GRID_LICENSE_STATE_LICENSED                = 5
+
+class c_nvmlVgpuLicenseInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('isLicensed',      c_uint8),
+        ('licenseExpiry',   c_nvmlVgpuLicenseExpiry_t),
+        ('currentState',    c_uint),
+    ]
+
+class c_nvmlEncoderSession_t(_PrintableStructure):
+    _fields_ = [
+        ('sessionId', c_uint),
+        ('pid', c_uint),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('codecType', c_uint),
+        ('hResolution', c_uint),
+        ('vResolution', c_uint),
+        ('averageFps', c_uint),
+        ('encodeLatency', c_uint),
+    ]
+
+class c_nvmlProcessUtilizationSample_t(_PrintableStructure):
+    _fields_ = [
+        ('pid', c_uint),
+        ('timeStamp', c_ulonglong),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+    ]
+
+class c_nvmlProcessUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('timeStamp', c_ulonglong),
+        ('pid', c_uint),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+        ('jpgUtil', c_uint),
+        ('ofaUtil', c_uint),
+    ]
+
+class c_nvmlProcessesUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('processSamplesCount', c_uint),
+        ('lastSeenTimeStamp', c_ulonglong),
+        ('procUtilArray', POINTER(c_nvmlProcessUtilizationInfo_v1_t)),
+    ]
+
+ProcessesUtilizationInfo_v1 = 0x01000018
+
+class c_nvmlGridLicenseExpiry_t(_PrintableStructure):
+    _fields_ = [
+        ('year',    c_uint32),
+        ('month',   c_uint16),
+        ('day',     c_uint16),
+        ('hour',    c_uint16),
+        ('min',     c_uint16),
+        ('sec',     c_uint16),
+        ('status',  c_uint8),
+    ]
+
+class c_nvmlGridLicensableFeature_v4_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode',    _nvmlGridLicenseFeatureCode_t),
+        ('featureState',   c_uint),
+        ('licenseInfo',    c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('productName',    c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('featureEnabled', c_uint),
+        ('licenseExpiry',  c_nvmlGridLicenseExpiry_t),
+    ]
+
+class c_nvmlGridLicensableFeatures_v4_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported',  c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures',  c_nvmlGridLicensableFeature_v4_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+class c_nvmlGridLicensableFeature_v3_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode', _nvmlGridLicenseFeatureCode_t),
+        ('featureState', c_uint),
+        ('licenseInfo', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('productName', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('featureEnabled', c_uint),
+    ]
+
+class c_nvmlGridLicensableFeatures_v3_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported', c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_v3_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+class c_nvmlGridLicensableFeature_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode', _nvmlGridLicenseFeatureCode_t),
+        ('featureState', c_uint),
+        ('licenseInfo', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('productName', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+    ]
+
+class c_nvmlGridLicensableFeatures_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported', c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_v2_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+class c_nvmlGridLicensableFeature_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode', _nvmlGridLicenseFeatureCode_t),
+        ('featureState', c_uint),
+        ('licenseInfo', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+    ]
+
+class c_nvmlGridLicensableFeatures_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported', c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+## Event structures
+class struct_c_nvmlEventSet_t(Structure):
+    pass # opaque handle
+c_nvmlEventSet_t = POINTER(struct_c_nvmlEventSet_t)
+
+nvmlEventTypeSingleBitEccError     = 0x0000000000000001
+nvmlEventTypeDoubleBitEccError     = 0x0000000000000002
+nvmlEventTypePState                = 0x0000000000000004
+nvmlEventTypeXidCriticalError      = 0x0000000000000008
+nvmlEventTypeClock                 = 0x0000000000000010
+nvmlEventTypePowerSourceChange     = 0x0000000000000080
+nvmlEventMigConfigChange           = 0x0000000000000100
+nvmlEventTypeNone                  = 0x0000000000000000
+nvmlEventTypeAll                   = (
+                                        nvmlEventTypeNone
+                                        | nvmlEventTypeSingleBitEccError
+                                        | nvmlEventTypeDoubleBitEccError
+                                        | nvmlEventTypePState
+                                        | nvmlEventTypeClock
+                                        | nvmlEventTypePowerSourceChange
+                                        | nvmlEventTypeXidCriticalError
+                                        | nvmlEventMigConfigChange
+                                     )
+
+## Clock Event Reasons defines
+nvmlClocksEventReasonGpuIdle              = 0x0000000000000001
+nvmlClocksEventReasonApplicationsClocksSetting = 0x0000000000000002
+nvmlClocksEventReasonUserDefinedClocks         = nvmlClocksEventReasonApplicationsClocksSetting # deprecated, use nvmlClocksEventReasonApplicationsClocksSetting
+nvmlClocksEventReasonSwPowerCap           = 0x0000000000000004
+nvmlClocksEventReasonHwSlowdown           = 0x0000000000000008
+nvmlClocksEventReasonSyncBoost            = 0x0000000000000010
+nvmlClocksEventReasonSwThermalSlowdown    = 0x0000000000000020
+nvmlClocksEventReasonHwThermalSlowdown    = 0x0000000000000040
+nvmlClocksEventReasonHwPowerBrakeSlowdown = 0x0000000000000080
+nvmlClocksEventReasonDisplayClockSetting  = 0x0000000000000100
+nvmlClocksEventReasonNone                 = 0x0000000000000000
+nvmlClocksEventReasonAll                  = (
+                                                  nvmlClocksEventReasonNone |
+                                                  nvmlClocksEventReasonGpuIdle |
+                                                  nvmlClocksEventReasonApplicationsClocksSetting |
+                                                  nvmlClocksEventReasonSwPowerCap |
+                                                  nvmlClocksEventReasonHwSlowdown |
+                                                  nvmlClocksEventReasonSyncBoost |
+                                                  nvmlClocksEventReasonSwThermalSlowdown |
+                                                  nvmlClocksEventReasonHwThermalSlowdown |
+                                                  nvmlClocksEventReasonHwPowerBrakeSlowdown |
+                                                  nvmlClocksEventReasonDisplayClockSetting
+                                               )
+
+## Following have been deprecated
+nvmlClocksThrottleReasonGpuIdle              = 0x0000000000000001
+nvmlClocksThrottleReasonApplicationsClocksSetting = 0x0000000000000002
+nvmlClocksThrottleReasonUserDefinedClocks         = nvmlClocksThrottleReasonApplicationsClocksSetting # deprecated, use nvmlClocksThrottleReasonApplicationsClocksSetting
+nvmlClocksThrottleReasonSwPowerCap           = 0x0000000000000004
+nvmlClocksThrottleReasonHwSlowdown           = 0x0000000000000008
+nvmlClocksThrottleReasonSyncBoost            = 0x0000000000000010
+nvmlClocksThrottleReasonSwThermalSlowdown    = 0x0000000000000020
+nvmlClocksThrottleReasonHwThermalSlowdown    = 0x0000000000000040
+nvmlClocksThrottleReasonHwPowerBrakeSlowdown = 0x0000000000000080
+nvmlClocksThrottleReasonDisplayClockSetting  = 0x0000000000000100
+nvmlClocksThrottleReasonNone                 = 0x0000000000000000
+nvmlClocksThrottleReasonAll                  = (
+                                                  nvmlClocksThrottleReasonNone |
+                                                  nvmlClocksThrottleReasonGpuIdle |
+                                                  nvmlClocksThrottleReasonApplicationsClocksSetting |
+                                                  nvmlClocksThrottleReasonSwPowerCap |
+                                                  nvmlClocksThrottleReasonHwSlowdown |
+                                                  nvmlClocksThrottleReasonSyncBoost |
+                                                  nvmlClocksThrottleReasonSwThermalSlowdown |
+                                                  nvmlClocksThrottleReasonHwThermalSlowdown |
+                                                  nvmlClocksThrottleReasonHwPowerBrakeSlowdown |
+                                                  nvmlClocksThrottleReasonDisplayClockSetting
+                                               )
+
+class c_nvmlEventData_t(_PrintableStructure):
+    _fields_ = [
+        ('device', c_nvmlDevice_t),
+        ('eventType', c_ulonglong),
+        ('eventData', c_ulonglong),
+        ('gpuInstanceId', c_uint),
+        ('computeInstanceId', c_uint)
+    ]
+    _fmt_ = {'eventType': "0x%08X"}
+
+class c_nvmlAccountingStats_t(_PrintableStructure):
+    _fields_ = [
+        ('gpuUtilization', c_uint),
+        ('memoryUtilization', c_uint),
+        ('maxMemoryUsage', c_ulonglong),
+        ('time', c_ulonglong),
+        ('startTime', c_ulonglong),
+        ('isRunning', c_uint),
+        ('reserved', c_uint * 5)
+    ]
+
+class c_nvmlVgpuVersion_t(Structure):
+    _fields_ = [("minVersion", c_uint),
+                ("maxVersion", c_uint)
+               ]
+
+class c_nvmlVgpuMetadata_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("revision", c_uint),
+                ("guestInfoState", _nvmlVgpuGuestInfoState_t),
+                ("guestDriverVersion", c_char * MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),
+                ("hostDriverVersion", c_char * MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),
+                ("reserved", c_uint * 6),
+                ("vgpuVirtualizationCaps", c_uint),
+                ("guestVgpuVersion", c_uint),
+                ("opaqueDataSize", c_uint),
+                ("opaqueData", c_char * MXSMLEX_VGPU_METADATA_OPAQUE_DATA_SIZE)
+               ]
+
+class c_nvmlVgpuPgpuMetadata_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("revision", c_uint),
+                ("hostDriverVersion", c_char * MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),
+                ("pgpuVirtualizationCaps", c_uint),
+                ("reserved", c_uint * 5),
+                ("hostSupportedVgpuRange", c_nvmlVgpuVersion_t),
+                ("opaqueDataSize", c_uint),
+                ("opaqueData", c_char * MXSMLEX_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE)
+               ]
+
+class c_nvmlVgpuPgpuCompatibility_t(Structure):
+    _fields_ = [("vgpuVmCompatibility", _nvmlVgpuVmCompatibility_t),
+                ("compatibilityLimitCode", _nvmlVgpuPgpuCompatibilityLimitCode_t)
+               ]
+
+## vGPU scheduler policy defines
+MXSMLEX_VGPU_SCHEDULER_POLICY_UNKNOWN      = 0
+MXSMLEX_VGPU_SCHEDULER_POLICY_BEST_EFFORT  = 1
+MXSMLEX_VGPU_SCHEDULER_POLICY_EQUAL_SHARE  = 2
+MXSMLEX_VGPU_SCHEDULER_POLICY_FIXED_SHARE  = 3
+
+## Supported vGPU scheduler policy count
+MXSMLEX_SUPPORTED_VGPU_SCHEDULER_POLICY_COUNT  = 3
+
+MXSMLEX_SCHEDULER_SW_MAX_LOG_ENTRIES           = 200
+
+MXSMLEX_VGPU_SCHEDULER_ARR_DEFAULT   = 0
+MXSMLEX_VGPU_SCHEDULER_ARR_DISABLE   = 1
+MXSMLEX_VGPU_SCHEDULER_ARR_ENABLE    = 2
+
+class c_nvmlVgpuSchedDataWithARR_t(_PrintableStructure):
+    _fields_ = [
+        ('avgFactor',   c_uint),
+        ('timeslice',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedData_t(_PrintableStructure):
+    _fields_ = [
+        ('timeslice',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedulerParams_t(Union):
+    _fields_ = [
+        ('vgpuSchedDataWithARR', c_nvmlVgpuSchedDataWithARR_t),
+        ('vgpuSchedData',        c_nvmlVgpuSchedData_t),
+    ]
+
+class c_nvmlVgpuSchedulerLogEntry_t(_PrintableStructure):
+    _fields_ = [
+        ('timestamp',                   c_ulonglong),
+        ('timeRunTotal',                c_ulonglong),
+        ('timeRun',                     c_ulonglong),
+        ('swRunlistId',                 c_uint),
+        ('targetTimeSlice',             c_ulonglong),
+        ('cumulativePreemptionTime',    c_ulonglong),
+    ]
+
+class c_nvmlVgpuSchedulerLog_t(_PrintableStructure):
+    _fields_ = [
+        ('engineId',        c_uint),
+        ('schedulerPolicy', c_uint),
+        ('arrMode',         c_uint),
+        ('schedulerParams', c_nvmlVgpuSchedulerParams_t),
+        ('entriesCount',    c_uint),
+        ('logEntries',      c_nvmlVgpuSchedulerLogEntry_t * MXSMLEX_SCHEDULER_SW_MAX_LOG_ENTRIES),
+    ]
+
+class c_nvmlVgpuSchedulerGetState_t(_PrintableStructure):
+    _fields_ = [
+        ('schedulerPolicy', c_uint),
+        ('arrMode',         c_uint),
+        ('schedulerParams', c_nvmlVgpuSchedulerParams_t),
+    ]
+
+class c_nvmlVgpuSchedSetDataWithARR_t(_PrintableStructure):
+    _fields_ = [
+        ('avgFactor',   c_uint),
+        ('frequency',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedSetData_t(_PrintableStructure):
+    _fields_ = [
+        ('timeslice',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedulerSetParams_t(Union):
+    _fields_ = [
+        ('vgpuSchedDataWithARR', c_nvmlVgpuSchedSetDataWithARR_t),
+        ('vgpuSchedData',        c_nvmlVgpuSchedSetData_t),
+    ]
+
+class c_nvmlVgpuSchedulerSetState_t(_PrintableStructure):
+    _fields_ = [
+        ('schedulerPolicy', c_uint),
+        ('enableARRMode',   c_uint),
+        ('schedulerParams', c_nvmlVgpuSchedulerSetParams_t),
+    ]
+
+class c_nvmlVgpuSchedulerCapabilities_t(_PrintableStructure):
+    _fields_ = [
+        ('supportedSchedulers', c_uint * MXSMLEX_SUPPORTED_VGPU_SCHEDULER_POLICY_COUNT),
+        ('maxTimeslice',        c_uint),
+        ('minTimeslice',        c_uint),
+        ('isArrModeSupported',  c_uint),
+        ('maxFrequencyForARR',  c_uint),
+        ('minFrequencyForARR',  c_uint),
+        ('maxAvgFactorForARR',  c_uint),
+        ('minAvgFactorForARR',  c_uint),
+    ]
+
+class c_nvmlFBCStats_t(Structure):
+    _fields_ = [("sessionsCount", c_uint),
+                ("averageFPS", c_uint),
+                ("averageLatency", c_uint)
+               ]
+
+class c_nvmlFBCSession_t(_PrintableStructure):
+    _fields_ = [
+        ('sessionId', c_uint),
+        ('pid', c_uint),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('displayOrdinal', c_uint),
+        ('sessionType', c_uint),
+        ('sessionFlags', c_uint),
+        ('hMaxResolution', c_uint),
+        ('vMaxResolution', c_uint),
+        ('hResolution', c_uint),
+        ('vResolution', c_uint),
+        ('averageFPS', c_uint),
+        ('averageLatency', c_uint),
+    ]
+
+MXSMLEX_DEVICE_MIG_DISABLE = 0x0
+MXSMLEX_DEVICE_MIG_ENABLE  = 0x1
+
+MXSMLEX_GPU_INSTANCE_PROFILE_1_SLICE      = 0x0
+MXSMLEX_GPU_INSTANCE_PROFILE_2_SLICE      = 0x1
+MXSMLEX_GPU_INSTANCE_PROFILE_3_SLICE      = 0x2
+MXSMLEX_GPU_INSTANCE_PROFILE_4_SLICE      = 0x3
+MXSMLEX_GPU_INSTANCE_PROFILE_7_SLICE      = 0x4
+MXSMLEX_GPU_INSTANCE_PROFILE_8_SLICE      = 0x5
+MXSMLEX_GPU_INSTANCE_PROFILE_6_SLICE      = 0x6
+MXSMLEX_GPU_INSTANCE_PROFILE_1_SLICE_REV1 = 0x7
+MXSMLEX_GPU_INSTANCE_PROFILE_2_SLICE_REV1 = 0x8
+MXSMLEX_GPU_INSTANCE_PROFILE_1_SLICE_REV2 = 0x9
+MXSMLEX_GPU_INSTANCE_PROFILE_COUNT        = 0xA
+
+class c_nvmlGpuInstancePlacement_t(Structure):
+    _fields_ = [("start", c_uint),
+                ("size", c_uint)
+               ]
+
+class c_nvmlGpuInstanceProfileInfo_t(Structure):
+    _fields_ = [("id", c_uint),
+                ("isP2pSupported", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("copyEngineCount", c_uint),
+                ("decoderCount", c_uint),
+                ("encoderCount", c_uint),
+                ("jpegCount", c_uint),
+                ("ofaCount", c_uint),
+                ("memorySizeMB", c_ulonglong),
+               ]
+
+nvmlGpuInstanceProfileInfo_v2 = 0x02000098
+
+class c_nvmlGpuInstanceProfileInfo_v2_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("id", c_uint),
+                ("isP2pSupported", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("copyEngineCount", c_uint),
+                ("decoderCount", c_uint),
+                ("encoderCount", c_uint),
+                ("jpegCount", c_uint),
+                ("ofaCount", c_uint),
+                ("memorySizeMB", c_ulonglong),
+                ("name", c_char * MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE)
+               ]
+    
+    def __init__(self):
+        super(c_nvmlGpuInstanceProfileInfo_v2_t, self).__init__(version=nvmlGpuInstanceProfileInfo_v2)
+
+class c_nvmlGpuInstanceInfo_t(Structure):
+    _fields_ = [("device", c_nvmlDevice_t),
+                ("id", c_uint),
+                ("profileId", c_uint),
+                ("placement", c_nvmlGpuInstancePlacement_t)
+               ]
+
+class struct_c_nvmlGpuInstance_t(Structure):
+    pass # opaque handle
+c_nvmlGpuInstance_t = POINTER(struct_c_nvmlGpuInstance_t)
+
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_1_SLICE      = 0x0
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_2_SLICE      = 0x1
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_3_SLICE      = 0x2
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_4_SLICE      = 0x3
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_7_SLICE      = 0x4
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_8_SLICE      = 0x5
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_6_SLICE      = 0x6
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_1_SLICE_REV1 = 0x7
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_COUNT        = 0x8
+
+MXSMLEX_COMPUTE_INSTANCE_ENGINE_PROFILE_SHARED = 0x0
+MXSMLEX_COMPUTE_INSTANCE_ENGINE_PROFILE_COUNT = 0x1
+
+class c_nvmlComputeInstancePlacement_t(Structure):
+    _fields_ = [("start", c_uint),
+                ("size", c_uint)
+               ]
+
+class c_nvmlComputeInstanceProfileInfo_t(Structure):
+    _fields_ = [("id", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("sharedCopyEngineCount", c_uint),
+                ("sharedDecoderCount", c_uint),
+                ("sharedEncoderCount", c_uint),
+                ("sharedJpegCount", c_uint),
+                ("sharedOfaCount", c_uint)
+               ]
+
+nvmlComputeInstanceProfileInfo_v2 = 0x02000088
+
+class c_nvmlComputeInstanceProfileInfo_v2_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("id", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("sharedCopyEngineCount", c_uint),
+                ("sharedDecoderCount", c_uint),
+                ("sharedEncoderCount", c_uint),
+                ("sharedJpegCount", c_uint),
+                ("sharedOfaCount", c_uint),
+                ("name", c_char * MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE)
+               ]
+
+    def __init__(self):
+        super(c_nvmlComputeInstanceProfileInfo_v2_t, self).__init__(version=nvmlComputeInstanceProfileInfo_v2)
+
+class c_nvmlComputeInstanceInfo_t(Structure):
+    _fields_ = [("device", c_nvmlDevice_t),
+                ("gpuInstance", c_nvmlGpuInstance_t),
+                ("id", c_uint),
+                ("profileId", c_uint),
+                ("placement", c_nvmlComputeInstancePlacement_t)
+               ]
+
+MXSMLEX_MAX_GPU_UTILIZATIONS = 8
+MXSMLEX_GPU_UTILIZATION_DOMAIN_GPU    = 0
+MXSMLEX_GPU_UTILIZATION_DOMAIN_FB     = 1
+MXSMLEX_GPU_UTILIZATION_DOMAIN_VID    = 2
+MXSMLEX_GPU_UTILIZATION_DOMAIN_BUS    = 3
+class c_nvmlGpuDynamicPstatesUtilization_t(Structure):
+    _fields_ = [("bIsPresent", c_uint, 1),
+                ("percentage", c_uint),
+                ("incThreshold", c_uint),
+                ("decThreshold", c_uint)]
+class c_nvmlGpuDynamicPstatesInfo_t(Structure):
+    _fields_ = [("flags", c_uint),
+                ("utilization", c_nvmlGpuDynamicPstatesUtilization_t * MXSMLEX_MAX_GPU_UTILIZATIONS)]
+
+MXSMLEX_MAX_THERMAL_SENSORS_PER_GPU = 3
+
+MXSMLEX_THERMAL_TARGET_NONE          = 0
+MXSMLEX_THERMAL_TARGET_GPU           = 1
+MXSMLEX_THERMAL_TARGET_MEMORY        = 2
+MXSMLEX_THERMAL_TARGET_POWER_SUPPLY  = 4
+MXSMLEX_THERMAL_TARGET_BOARD         = 8
+MXSMLEX_THERMAL_TARGET_VCD_BOARD     = 9
+MXSMLEX_THERMAL_TARGET_VCD_INLET     = 10
+MXSMLEX_THERMAL_TARGET_VCD_OUTLET    = 11
+MXSMLEX_THERMAL_TARGET_ALL           = 15
+MXSMLEX_THERMAL_TARGET_UNKNOWN       = -1
+
+MXSMLEX_THERMAL_CONTROLLER_NONE            = 0
+MXSMLEX_THERMAL_CONTROLLER_GPU_INTERNAL    = 1
+MXSMLEX_THERMAL_CONTROLLER_ADM1032         = 2
+MXSMLEX_THERMAL_CONTROLLER_ADT7461         = 3
+MXSMLEX_THERMAL_CONTROLLER_MAX6649         = 4
+MXSMLEX_THERMAL_CONTROLLER_MAX1617         = 5
+MXSMLEX_THERMAL_CONTROLLER_LM99            = 6
+MXSMLEX_THERMAL_CONTROLLER_LM89            = 7
+MXSMLEX_THERMAL_CONTROLLER_LM64            = 8
+MXSMLEX_THERMAL_CONTROLLER_G781            = 9
+MXSMLEX_THERMAL_CONTROLLER_ADT7473         = 10
+MXSMLEX_THERMAL_CONTROLLER_SBMAX6649       = 11
+MXSMLEX_THERMAL_CONTROLLER_VBIOSEVT        = 12
+MXSMLEX_THERMAL_CONTROLLER_OS              = 13
+MXSMLEX_THERMAL_CONTROLLER_NVSYSCON_CANOAS = 14
+MXSMLEX_THERMAL_CONTROLLER_NVSYSCON_E551   = 15
+MXSMLEX_THERMAL_CONTROLLER_MAX6649R        = 16
+MXSMLEX_THERMAL_CONTROLLER_ADT7473S        = 17
+MXSMLEX_THERMAL_CONTROLLER_UNKNOWN         = -1
+
+class c_nvmlGpuThermalSensor_t(Structure):
+    _fields_ = [("controller", c_int),
+                ("defaultMinTemp", c_int),
+                ("defaultMaxTemp", c_int),
+                ("currentTemp", c_int),
+                ("target", c_int)]
+class c_nvmlGpuThermalSettings_t(Structure):
+    _fields_ = [("count", c_uint),
+                ("sensor", c_nvmlGpuThermalSensor_t * MXSMLEX_MAX_THERMAL_SENSORS_PER_GPU)]
+
+class struct_c_nvmlComputeInstance_t(Structure):
+    pass # opaque handle
+c_nvmlComputeInstance_t = POINTER(struct_c_nvmlComputeInstance_t)
+
+class c_nvmlDeviceAttributes(Structure):
+    _fields_ = [("multiprocessorCount", c_uint),
+                ("sharedCopyEngineCount", c_uint),
+                ("sharedDecoderCount", c_uint),
+                ("sharedEncoderCount", c_uint),
+                ("sharedJpegCount", c_uint),
+                ("sharedOfaCount", c_uint),
+                ("gpuInstanceSliceCount", c_uint),
+                ("computeInstanceSliceCount", c_uint),
+                ("memorySizeMB", c_ulonglong),
+               ]
+
+class c_nvmlRowRemapperHistogramValues(Structure):
+    _fields_ = [("max", c_uint),
+                ("high", c_uint),
+                ("partial", c_uint),
+                ("low", c_uint),
+                ("none", c_uint)
+               ]
+
+MXSMLEX_GPU_CERT_CHAIN_SIZE                = 0x1000
+MXSMLEX_GPU_ATTESTATION_CERT_CHAIN_SIZE    = 0x1400
+MXSMLEX_CC_GPU_CEC_NONCE_SIZE              = 0x20
+MXSMLEX_CC_GPU_ATTESTATION_REPORT_SIZE     = 0x2000
+MXSMLEX_CC_GPU_CEC_ATTESTATION_REPORT_SIZE = 0x1000
+MXSMLEX_CC_CEC_ATTESTATION_REPORT_NOT_PRESENT = 0
+MXSMLEX_CC_CEC_ATTESTATION_REPORT_PRESENT     = 1
+
+class c_nvmlConfComputeSystemState_t(Structure):
+    _fields_ = [('environment', c_uint),
+                ('ccFeature', c_uint),
+                ('devToolsMode', c_uint),
+               ]
+
+nvmlSystemConfComputeSettings_v1 = 0x1000014
+
+class c_nvmlSystemConfComputeSettings_v1_t(Structure):
+    _fields_ = [('version', c_uint),
+                ('environment', c_uint),
+                ('ccFeature', c_uint),
+                ('devToolsMode', c_uint),
+                ('multiGpuMode', c_uint),
+               ]
+    def __init__(self):
+        super(c_nvmlSystemConfComputeSettings_v1_t, self).__init__(version=nvmlSystemConfComputeSettings_v1)
+
+class c_nvmlConfComputeSystemCaps_t(Structure):
+    _fields_ = [('cpuCaps', c_uint),
+                ('gpusCaps', c_uint),
+               ]
+
+class c_nvmlConfComputeMemSizeInfo_t(Structure):
+    _fields_ = [('protectedMemSizeKib', c_ulonglong),
+                ('unprotectedMemSizeKib', c_ulonglong),
+               ]
+
+class c_nvmlConfComputeGpuCertificate_t(Structure):
+    _fields_ = [('certChainSize', c_uint),
+                ('attestationCertChainSize', c_uint),
+                ('certChain', c_uint8 * MXSMLEX_GPU_CERT_CHAIN_SIZE),
+                ('attestationCertChain', c_uint8 * MXSMLEX_GPU_ATTESTATION_CERT_CHAIN_SIZE),
+               ]
+
+class c_nvmlConfComputeGpuAttestationReport_t(Structure):
+    _fields_ = [('isCecAttestationReportPresent', c_uint),
+                ('attestationReportSize', c_uint),
+                ('cecAttestationReportSize', c_uint),
+                ('nonce', c_uint8 * MXSMLEX_CC_GPU_CEC_NONCE_SIZE),
+                ('attestationReport', c_uint8 * MXSMLEX_CC_GPU_ATTESTATION_REPORT_SIZE),
+                ('cecAttestationReport', c_uint8 * MXSMLEX_CC_GPU_CEC_ATTESTATION_REPORT_SIZE),
+               ]
+
+class c_nvmlConfComputeSetKeyRotationThresholdInfo_t(Structure):
+    _fields_ = [('version', c_uint),
+                ('maxAttackerAdvantage', c_ulong),
+               ]
+ConfComputeSetKeyRotationThresholdInfo_v1 = 0x1000010
+
+class c_nvmlConfComputeGetKeyRotationThresholdInfo_t(Structure):
+    _fields_ = [('version', c_uint),
+                ('attackerAdvantage', c_ulong),
+               ]
+ConfComputeGetKeyRotationThresholdInfo_v1 = 0x1000010
+
+
+## string/bytes conversion for ease of use
+def convertStrBytes(func):
+    '''
+    In python 3, strings are unicode instead of bytes, and need to be converted for ctypes
+    Args from caller: (1, 'string', <__main__.c_nvmlDevice_t at 0xFFFFFFFF>)
+    Args passed to function: (1, b'string', <__main__.c_nvmlDevice_t at 0xFFFFFFFF)>
+    ----
+    Returned from function: b'returned string'
+    Returned to caller: 'returned string'
+    '''
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        # encoding a str returns bytes in python 2 and 3
+        args = [arg.encode() if isinstance(arg, str) else arg for arg in args]
+        res = func(*args, **kwargs)
+        # In python 2, str and bytes are the same
+        # In python 3, str is unicode and should be decoded.
+        # Ctypes handles most conversions, this only effects c_char and char arrays.
+        if isinstance(res, bytes):
+            if isinstance(res, str):
+                return res
+            return res.decode()
+        return res
+
+    if sys.version_info >= (3,):
+        return wrapper
+    return func
+
+def throwOnVersionMismatch(func):
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        try:
+            return func(*args, **kwargs)
+        except NVMLError_FunctionNotFound:
+            raise MXSMLEXLibraryMismatchError("Unversioned function called and the "
+                                           "pyMXSMLEX version does not match the MXSMLEX lib version. "
+                                           "Either use matching pyMXSMLEX and MXSMLEX lib versions or "
+                                           "use a versioned function such as " + func.__name__ + "_v2")
+    return wrapper
+
+## C function wrappers ##
+def nvmlInitWithFlags(flags):
+    _LoadNvmlLibrary()
+
+    #
+    # Initialize the library
+    #
+    fn = _nvmlGetFunctionPointer("mxSmlExInit")
+    ret = fn(flags)
+    _nvmlCheckReturn(ret)
+
+    # Atomically update refcount
+    global _nvmlLib_refcount
+    libLoadLock.acquire()
+    _nvmlLib_refcount += 1
+    libLoadLock.release()
+    return None
+
+def nvmlInit():
+    nvmlInitWithFlags(0)
+    return None
+
+def _LoadNvmlLibrary():
+    '''
+    Load the library if it isn't loaded already
+    '''
+    global nvmlLib
+
+    if (nvmlLib == None):
+        # lock to ensure only one caller loads the library
+        libLoadLock.acquire()
+
+        try:
+            # ensure the library still isn't loaded
+            if (nvmlLib == None):
+                try:
+                    if (sys.platform[:3] == "win"):
+                        # cdecl calling convention
+                        try:
+                            # Check for nvml.dll in System32 first for DCH drivers
+                            nvmlLib = CDLL(os.path.join(os.getenv("WINDIR", "C:/Windows"), "System32/nvml.dll"))
+                        except OSError as ose:
+                            # If nvml.dll is not found in System32, it should be in ProgramFiles
+                            # load nvml.dll from %ProgramFiles%/NVIDIA Corporation/NVSMI/nvml.dll
+                            nvmlLib = CDLL(os.path.join(os.getenv("ProgramFiles", "C:/Program Files"), "NVIDIA Corporation/NVSMI/nvml.dll"))
+                    else:
+                        # assume linux
+                        nvmlLib = CDLL("libmxsml.so")
+                except OSError as ose:
+                    _nvmlCheckReturn(MXSMLEX_ERROR_LIBRARY_NOT_FOUND)
+                if (nvmlLib == None):
+                    _nvmlCheckReturn(MXSMLEX_ERROR_LIBRARY_NOT_FOUND)
+        finally:
+            # lock is always freed
+            libLoadLock.release()
+
+def nvmlShutdown():
+    #
+    # Leave the library loaded, but shutdown the interface
+    #
+    fn = _nvmlGetFunctionPointer("mxSmlExShutdown")
+    ret = fn()
+    _nvmlCheckReturn(ret)
+
+    # Atomically update refcount
+    global _nvmlLib_refcount
+    libLoadLock.acquire()
+    if (0 < _nvmlLib_refcount):
+        _nvmlLib_refcount -= 1
+    libLoadLock.release()
+    return None
+
+# Added in 2.285
+@convertStrBytes
+def nvmlErrorString(result):
+    fn = _nvmlGetFunctionPointer("mxSmlExErrorString")
+    fn.restype = c_char_p # otherwise return is an int
+    ret = fn(result)
+    return ret
+
+# Added in 2.285
+@convertStrBytes
+def nvmlSystemGetMXSMLEXVersion():
+    c_version = create_string_buffer(MXSMLEX_SYSTEM_MXSMLEX_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetMXSMLEXVersion")
+    ret = fn(c_version, c_uint(MXSMLEX_SYSTEM_MXSMLEX_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+def nvmlSystemGetCudaDriverVersion():
+    c_cuda_version = c_int()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetCudaDriverVersion")
+    ret = fn(byref(c_cuda_version))
+    _nvmlCheckReturn(ret)
+    return c_cuda_version.value
+
+def nvmlSystemGetCudaDriverVersion_v2():
+    c_cuda_version = c_int()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetCudaDriverVersion_v2")
+    ret = fn(byref(c_cuda_version))
+    _nvmlCheckReturn(ret)
+    return c_cuda_version.value
+
+# Added in 2.285
+@convertStrBytes
+def nvmlSystemGetProcessName(pid):
+    c_name = create_string_buffer(1024)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetProcessName")
+    ret = fn(c_uint(pid), c_name, c_uint(1024))
+    _nvmlCheckReturn(ret)
+    return c_name.value
+
+@convertStrBytes
+def nvmlSystemGetDriverVersion():
+    c_version = create_string_buffer(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetDriverVersion")
+    ret = fn(c_version, c_uint(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 2.285
+def nvmlSystemGetHicVersion():
+    c_count = c_uint(0)
+    hics = None
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetHicVersion")
+
+    # get the count
+    ret = fn(byref(c_count), None)
+
+    # this should only fail with insufficient size
+    if ((ret != MXSMLEX_SUCCESS) and
+        (ret != MXSMLEX_ERROR_INSUFFICIENT_SIZE)):
+        raise NVMLError(ret)
+
+    # If there are no hics
+    if (c_count.value == 0):
+        return []
+
+    hic_array = c_nvmlHwbcEntry_t * c_count.value
+    hics = hic_array()
+    ret = fn(byref(c_count), hics)
+    _nvmlCheckReturn(ret)
+    return hics
+
+## Unit get functions
+def nvmlUnitGetCount():
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetCount")
+    ret = fn(byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlUnitGetHandleByIndex(index):
+    c_index = c_uint(index)
+    unit = c_nvmlUnit_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetHandleByIndex")
+    ret = fn(c_index, byref(unit))
+    _nvmlCheckReturn(ret)
+    return unit
+
+def nvmlUnitGetUnitInfo(unit):
+    c_info = c_nvmlUnitInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetUnitInfo")
+    ret = fn(unit, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlUnitGetLedState(unit):
+    c_state =  c_nvmlLedState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetLedState")
+    ret = fn(unit, byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state
+
+def nvmlUnitGetPsuInfo(unit):
+    c_info = c_nvmlPSUInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetPsuInfo")
+    ret = fn(unit, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlUnitGetTemperature(unit, type):
+    c_temp = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetTemperature")
+    ret = fn(unit, c_uint(type), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return c_temp.value
+
+def nvmlUnitGetFanSpeedInfo(unit):
+    c_speeds = c_nvmlUnitFanSpeeds_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetFanSpeedInfo")
+    ret = fn(unit, byref(c_speeds))
+    _nvmlCheckReturn(ret)
+    return c_speeds
+
+# added to API
+def nvmlUnitGetDeviceCount(unit):
+    c_count = c_uint(0)
+    # query the unit to determine device count
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetDevices")
+    ret = fn(unit, byref(c_count), None)
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = MXSMLEX_SUCCESS
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlUnitGetDevices(unit):
+    c_count = c_uint(nvmlUnitGetDeviceCount(unit))
+    device_array = c_nvmlDevice_t * c_count.value
+    c_devices = device_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetDevices")
+    ret = fn(unit, byref(c_count), c_devices)
+    _nvmlCheckReturn(ret)
+    return c_devices
+
+## Device get functions
+def nvmlDeviceGetCount():
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCount_v2")
+    ret = fn(byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetHandleByIndex(index):
+    c_index = c_uint(index)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetDeviceHandleByIndex")
+    ret = fn(c_index, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetHandleBySerial(serial):
+    c_serial = c_char_p(serial)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHandleBySerial")
+    ret = fn(c_serial, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetHandleByUUID(uuid):
+    c_uuid = c_char_p(uuid)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHandleByUUID")
+    ret = fn(c_uuid, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetHandleByPciBusId(pciBusId):
+    c_busId = c_char_p(pciBusId)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHandleByPciBusId_v2")
+    ret = fn(c_busId, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetName(handle):
+    c_name = create_string_buffer(MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetName")
+    ret = fn(handle, c_name, c_uint(MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_name.value
+
+def nvmlDeviceGetBoardId(handle):
+    c_id = c_uint();
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBoardId")
+    ret = fn(handle, byref(c_id))
+    _nvmlCheckReturn(ret)
+    return c_id.value
+
+def nvmlDeviceGetMultiGpuBoard(handle):
+    c_multiGpu = c_uint();
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMultiGpuBoard")
+    ret = fn(handle, byref(c_multiGpu))
+    _nvmlCheckReturn(ret)
+    return c_multiGpu.value
+
+def nvmlDeviceGetBrand(handle):
+    c_type = _nvmlBrandType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBrand")
+    ret = fn(handle, byref(c_type))
+    _nvmlCheckReturn(ret)
+    return c_type.value
+
+def nvmlDeviceGetC2cModeInfoV1(handle):
+    c_info = c_nvmlC2cModeInfo_v1_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetC2cModeInfoV")
+    ret = fn(handle, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlDeviceGetC2cModeInfoV(handle):
+    return nvmlDeviceGetC2cModeInfoV1(handle)
+
+@convertStrBytes
+def nvmlDeviceGetBoardPartNumber(handle):
+    c_part_number = create_string_buffer(MXSMLEX_DEVICE_PART_NUMBER_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBoardPartNumber")
+    ret = fn(handle, c_part_number, c_uint(MXSMLEX_DEVICE_PART_NUMBER_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_part_number.value
+
+@convertStrBytes
+def nvmlDeviceGetSerial(handle):
+    c_serial = create_string_buffer(MXSMLEX_DEVICE_SERIAL_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSerial")
+    ret = fn(handle, c_serial, c_uint(MXSMLEX_DEVICE_SERIAL_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_serial.value
+
+def nvmlDeviceGetModuleId(handle, moduleId):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetModuleId")
+    ret = fn(handle, moduleId)
+    return ret
+
+def nvmlDeviceGetMemoryAffinity(handle, nodeSetSize, scope):
+    affinity_array = c_ulonglong * nodeSetSize
+    c_affinity = affinity_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryAffinity")
+    ret = fn(handle, nodeSetSize, byref(c_affinity), _nvmlAffinityScope_t(scope))
+    _nvmlCheckReturn(ret)
+    return c_affinity
+
+def nvmlDeviceGetCpuAffinityWithinScope(handle, cpuSetSize, scope):
+    affinity_array = c_ulonglong * cpuSetSize
+    c_affinity = affinity_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCpuAffinityWithinScope")
+    ret = fn(handle, cpuSetSize, byref(c_affinity), _nvmlAffinityScope_t(scope))
+    _nvmlCheckReturn(ret)
+    return c_affinity
+
+def nvmlDeviceGetCpuAffinity(handle, cpuSetSize):
+    affinity_array = c_ulonglong * cpuSetSize
+    c_affinity = affinity_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCpuAffinity")
+    ret = fn(handle, cpuSetSize, byref(c_affinity))
+    _nvmlCheckReturn(ret)
+    return c_affinity
+
+def nvmlDeviceSetCpuAffinity(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetCpuAffinity")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceClearCpuAffinity(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearCpuAffinity")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetNumaNodeId(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNumaNodeId")
+    node = c_int()
+    ret = fn(handle, byref(node))
+    _nvmlCheckReturn(ret)
+    return node.value
+
+def nvmlDeviceGetMinorNumber(handle):
+    c_minor_number = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMinorNumber")
+    ret = fn(handle, byref(c_minor_number))
+    _nvmlCheckReturn(ret)
+    return c_minor_number.value
+
+@convertStrBytes
+def nvmlDeviceGetUUID(handle):
+    c_uuid = create_string_buffer(MXSMLEX_DEVICE_UUID_V2_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetUUID")
+    ret = fn(handle, c_uuid, c_uint(MXSMLEX_DEVICE_UUID_V2_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_uuid.value
+
+@convertStrBytes
+def nvmlDeviceGetInforomVersion(handle, infoRomObject):
+    c_version = create_string_buffer(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetInforomVersion")
+    ret = fn(handle, _nvmlInforomObject_t(infoRomObject),
+                 c_version, c_uint(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 4.304
+@convertStrBytes
+def nvmlDeviceGetInforomImageVersion(handle):
+    c_version = create_string_buffer(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetInforomImageVersion")
+    ret = fn(handle, c_version, c_uint(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 4.304
+def nvmlDeviceGetInforomConfigurationChecksum(handle):
+    c_checksum = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetInforomConfigurationChecksum")
+    ret = fn(handle, byref(c_checksum))
+    _nvmlCheckReturn(ret)
+    return c_checksum.value
+
+# Added in 4.304
+def nvmlDeviceValidateInforom(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceValidateInforom")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetLastBBXFlushTime(handle):
+    c_timestamp = c_ulonglong()
+    c_durationUs = c_ulong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetLastBBXFlushTime")
+    ret = fn(handle, byref(c_timestamp), byref(c_durationUs))
+    _nvmlCheckReturn(ret)
+    return [c_timestamp.value, c_durationUs.value]
+
+def nvmlDeviceGetDisplayMode(handle):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDisplayMode")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlDeviceGetDisplayActive(handle):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDisplayActive")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+
+def nvmlDeviceGetPersistenceMode(handle):
+    c_state = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPersistenceMode")
+    ret = fn(handle, byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state.value
+
+def nvmlDeviceGetPciInfoExt(handle, c_info):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPciInfoExt")
+    ret = fn(handle, c_info)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetPciInfo_v3(handle):
+    c_info = nvmlPciInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPciInfo_v3")
+    ret = fn(handle, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlDeviceGetPciInfo(handle):
+    return nvmlDeviceGetPciInfo_v3(handle)
+
+def nvmlDeviceGetClockInfo(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetClockInfo")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 2.285
+def nvmlDeviceGetMaxClockInfo(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxClockInfo")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 4.304
+def nvmlDeviceGetApplicationsClock(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetApplicationsClock")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+def nvmlDeviceGetMaxCustomerBoostClock(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxCustomerBoostClock")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+def nvmlDeviceGetClock(handle, type, id):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetClock")
+    ret = fn(handle, _nvmlClockType_t(type), _nvmlClockId_t(id), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 5.319
+def nvmlDeviceGetDefaultApplicationsClock(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDefaultApplicationsClock")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 4.304
+def nvmlDeviceGetSupportedMemoryClocks(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedMemoryClocks")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no clocks
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        clocks_array = c_uint * c_count.value
+        c_clocks = clocks_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_clocks)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            procs.append(c_clocks[i])
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+# Added in 4.304
+def nvmlDeviceGetSupportedGraphicsClocks(handle, memoryClockMHz):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedGraphicsClocks")
+    ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no clocks
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        clocks_array = c_uint * c_count.value
+        c_clocks = clocks_array()
+
+        # make the call again
+        ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), c_clocks)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            procs.append(c_clocks[i])
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetFanSpeed(handle):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFanSpeed")
+    ret = fn(handle, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetFanSpeed_v2(handle, fan):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFanSpeed_v2")
+    ret = fn(handle, fan, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetTargetFanSpeed(handle, fan):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTargetFanSpeed")
+    ret = fn(handle, fan, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetNumFans(device):
+    c_numFans = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNumFans")
+    ret = fn(device, byref(c_numFans))
+    _nvmlCheckReturn(ret)
+    return c_numFans.value
+
+def nvmlDeviceSetDefaultFanSpeed_v2(handle, index):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetDefaultFanSpeed_v2");
+    ret = fn(handle, index)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMinMaxFanSpeed(handle, minSpeed, maxSpeed):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMinMaxFanSpeed")
+    ret = fn(handle, minSpeed, maxSpeed)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetFanControlPolicy_v2(handle, fan, fanControlPolicy):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFanControlPolicy_v2")
+    ret = fn(handle, fan, fanControlPolicy)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceSetFanControlPolicy(handle, fan, fanControlPolicy):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetFanControlPolicy")
+    ret = fn(handle, fan, _nvmlFanControlPolicy_t(fanControlPolicy))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetTemperature(handle, sensor):
+    c_temp = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTemperature")
+    ret = fn(handle, _nvmlTemperatureSensors_t(sensor), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return c_temp.value
+
+def nvmlDeviceGetTemperatureThreshold(handle, threshold):
+    c_temp = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTemperatureThreshold")
+    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return c_temp.value
+
+def nvmlDeviceSetTemperatureThreshold(handle, threshold, temp):
+    c_temp = c_uint()
+    c_temp.value = temp
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetTemperatureThreshold")
+    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return None
+
+# DEPRECATED use nvmlDeviceGetPerformanceState
+def nvmlDeviceGetPowerState(handle):
+    c_pstate = _nvmlPstates_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerState")
+    ret = fn(handle, byref(c_pstate))
+    _nvmlCheckReturn(ret)
+    return c_pstate.value
+
+def nvmlDeviceGetPerformanceState(handle):
+    c_pstate = _nvmlPstates_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPerformanceState")
+    ret = fn(handle, byref(c_pstate))
+    _nvmlCheckReturn(ret)
+    return c_pstate.value
+
+def nvmlDeviceGetPowerManagementMode(handle):
+    c_pcapMode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementMode")
+    ret = fn(handle, byref(c_pcapMode))
+    _nvmlCheckReturn(ret)
+    return c_pcapMode.value
+
+def nvmlDeviceGetPowerManagementLimit(handle):
+    c_limit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementLimit")
+    ret = fn(handle, byref(c_limit))
+    _nvmlCheckReturn(ret)
+    return c_limit.value
+
+# Added in 4.304
+def nvmlDeviceGetPowerManagementLimitConstraints(handle):
+    c_minLimit = c_uint()
+    c_maxLimit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementLimitConstraints")
+    ret = fn(handle, byref(c_minLimit), byref(c_maxLimit))
+    _nvmlCheckReturn(ret)
+    return [c_minLimit.value, c_maxLimit.value]
+
+# Added in 4.304
+def nvmlDeviceGetPowerManagementDefaultLimit(handle):
+    c_limit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementDefaultLimit")
+    ret = fn(handle, byref(c_limit))
+    _nvmlCheckReturn(ret)
+    return c_limit.value
+
+
+# Added in 331
+def nvmlDeviceGetEnforcedPowerLimit(handle):
+    c_limit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEnforcedPowerLimit")
+    ret = fn(handle, byref(c_limit))
+    _nvmlCheckReturn(ret)
+    return c_limit.value
+
+def nvmlDeviceGetPowerUsage(handle):
+    c_watts = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerUsage")
+    ret = fn(handle, byref(c_watts))
+    _nvmlCheckReturn(ret)
+    return c_watts.value
+
+def nvmlDeviceGetTotalEnergyConsumption(handle):
+    c_millijoules = c_uint64()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTotalEnergyConsumption")
+    ret = fn(handle, byref(c_millijoules))
+    _nvmlCheckReturn(ret)
+    return c_millijoules.value
+
+# Added in 4.304
+def nvmlDeviceGetGpuOperationMode(handle):
+    c_currState = _nvmlGpuOperationMode_t()
+    c_pendingState = _nvmlGpuOperationMode_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuOperationMode")
+    ret = fn(handle, byref(c_currState), byref(c_pendingState))
+    _nvmlCheckReturn(ret)
+    return [c_currState.value, c_pendingState.value]
+
+# Added in 4.304
+def nvmlDeviceGetCurrentGpuOperationMode(handle):
+    return nvmlDeviceGetGpuOperationMode(handle)[0]
+
+# Added in 4.304
+def nvmlDeviceGetPendingGpuOperationMode(handle):
+    return nvmlDeviceGetGpuOperationMode(handle)[1]
+
+def nvmlDeviceGetMemoryInfo(handle, version=None):
+    if not version:
+        c_memory = c_nvmlMemory_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryInfo")
+    else:
+        c_memory = c_nvmlMemory_v2_t()
+        c_memory.version = version
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryInfo_v2")
+    ret = fn(handle, byref(c_memory))
+    _nvmlCheckReturn(ret)
+    return c_memory
+
+def nvmlDeviceGetBAR1MemoryInfo(handle):
+    c_bar1_memory = c_nvmlBAR1Memory_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBAR1MemoryInfo")
+    ret = fn(handle, byref(c_bar1_memory))
+    _nvmlCheckReturn(ret)
+    return c_bar1_memory
+
+def nvmlDeviceGetComputeMode(handle):
+    c_mode = _nvmlComputeMode_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeMode")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlDeviceGetCudaComputeCapability(handle):
+    c_major = c_int()
+    c_minor = c_int()
+    #fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCudaComputeCapability")
+    #ret = fn(handle, byref(c_major), byref(c_minor))
+    #_nvmlCheckReturn(ret)
+    return (c_major.value, c_minor.value)
+
+def nvmlDeviceGetEccMode(handle):
+    c_currState = _nvmlEnableState_t()
+    c_pendingState = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEccMode")
+    ret = fn(handle, byref(c_currState), byref(c_pendingState))
+    _nvmlCheckReturn(ret)
+    return [c_currState.value, c_pendingState.value]
+
+# added to API
+def nvmlDeviceGetCurrentEccMode(handle):
+    return nvmlDeviceGetEccMode(handle)[0]
+
+# added to API
+def nvmlDeviceGetPendingEccMode(handle):
+    return nvmlDeviceGetEccMode(handle)[1]
+
+def nvmlDeviceGetDefaultEccMode(handle):
+    c_defaultState = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDefaultEccMode")
+    ret = fn(handle, byref(c_defaultState))
+    _nvmlCheckReturn(ret)
+    return [c_defaultState.value]
+
+def nvmlDeviceGetTotalEccErrors(handle, errorType, counterType):
+    c_count = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTotalEccErrors")
+    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),
+                 _nvmlEccCounterType_t(counterType), byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+# This is deprecated, instead use nvmlDeviceGetMemoryErrorCounter
+def nvmlDeviceGetDetailedEccErrors(handle, errorType, counterType):
+    c_counts = c_nvmlEccErrorCounts_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDetailedEccErrors")
+    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),
+                 _nvmlEccCounterType_t(counterType), byref(c_counts))
+    _nvmlCheckReturn(ret)
+    return c_counts
+
+# Added in 4.304
+def nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType, locationType):
+    c_count = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryErrorCounter")
+    ret = fn(handle,
+             _nvmlMemoryErrorType_t(errorType),
+             _nvmlEccCounterType_t(counterType),
+             _nvmlMemoryLocation_t(locationType),
+             byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetUtilizationRates(handle):
+    c_util = c_nvmlUtilization_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetUtilizationRates")
+    ret = fn(handle, byref(c_util))
+    _nvmlCheckReturn(ret)
+    return c_util
+
+def nvmlDeviceGetEncoderUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetDecoderUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDecoderUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetJpgUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetJpgUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetOfaUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetOfaUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetPcieReplayCounter(handle):
+    c_replay = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieReplayCounter")
+    ret = fn(handle, byref(c_replay))
+    _nvmlCheckReturn(ret)
+    return c_replay.value
+
+def nvmlDeviceGetDriverModel(handle):
+    c_currModel = _nvmlDriverModel_t()
+    c_pendingModel = _nvmlDriverModel_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDriverModel")
+    ret = fn(handle, byref(c_currModel), byref(c_pendingModel))
+    _nvmlCheckReturn(ret)
+    return [c_currModel.value, c_pendingModel.value]
+
+# added to API
+def nvmlDeviceGetCurrentDriverModel(handle):
+    return nvmlDeviceGetDriverModel(handle)[0]
+
+# added to API
+def nvmlDeviceGetPendingDriverModel(handle):
+    return nvmlDeviceGetDriverModel(handle)[1]
+
+# Added in 2.285
+@convertStrBytes
+def nvmlDeviceGetVbiosVersion(handle):
+    c_version = create_string_buffer(MXSMLEX_DEVICE_VBIOS_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVbiosVersion")
+    ret = fn(handle, c_version, c_uint(MXSMLEX_DEVICE_VBIOS_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 2.285
+def nvmlDeviceGetComputeRunningProcesses_v2(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeRunningProcesses_v2")
+    ret = fn(handle, byref(c_count), None)
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v2_t * c_count.value
+        c_procs = proc_array()
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+# Added in 2.285
+def nvmlDeviceGetComputeRunningProcesses_v3(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeRunningProcesses_v3")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v3_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+@throwOnVersionMismatch
+def nvmlDeviceGetComputeRunningProcesses(handle):
+    return nvmlDeviceGetComputeRunningProcesses_v3(handle)
+
+def nvmlDeviceGetGraphicsRunningProcesses_v2(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGraphicsRunningProcesses_v2")
+    ret = fn(handle, byref(c_count), None)
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v2_t * c_count.value
+        c_procs = proc_array()
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetGraphicsRunningProcesses_v3(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGraphicsRunningProcesses_v3")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v3_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+@throwOnVersionMismatch
+def nvmlDeviceGetGraphicsRunningProcesses(handle):
+    return nvmlDeviceGetGraphicsRunningProcesses_v3(handle)
+
+@throwOnVersionMismatch
+def nvmlDeviceGetMPSComputeRunningProcesses(handle):
+    return nvmlDeviceGetMPSComputeRunningProcesses_v3(handle)
+
+def nvmlDeviceGetMPSComputeRunningProcesses_v2(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMPSComputeRunningProcesses_v2")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v2_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetMPSComputeRunningProcesses_v3(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMPSComputeRunningProcesses_v3")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v3_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetRunningProcessDetailList(handle, version, mode):
+    c_processDetailList = c_nvmlProcessDetailList_t()
+    c_processDetailList.version = version
+    c_processDetailList.mode = mode
+
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRunningProcessDetailList")
+
+    # first call to get the size
+    ret = fn(handle, byref(c_processDetailList))
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        c_procs = c_nvmlProcessDetail_v1_t * c_processDetailList.numProcArrayEntries
+        c_processDetailList.procArray = cast((c_procs)(), POINTER(c_nvmlProcessDetail_v1_t))
+
+        # make the call again
+        ret = fn(handle, byref(c_processDetailList))
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_processDetailList.numProcArrayEntries):
+            # use an alternative struct for this object
+            obj = c_processDetailList.procArray[i]
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                obj.usedGpuMemory = None
+            if (obj.usedGpuCcProtectedMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                obj.usedGpuCcProtectedMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetAutoBoostedClocksEnabled(handle):
+    c_isEnabled = _nvmlEnableState_t()
+    c_defaultIsEnabled = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAutoBoostedClocksEnabled")
+    ret = fn(handle, byref(c_isEnabled), byref(c_defaultIsEnabled))
+    _nvmlCheckReturn(ret)
+    return [c_isEnabled.value, c_defaultIsEnabled.value]
+    #Throws MXSMLEX_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks
+
+## Set functions
+def nvmlUnitSetLedState(unit, color):
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitSetLedState")
+    ret = fn(unit, _nvmlLedColor_t(color))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetPersistenceMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPersistenceMode")
+    ret = fn(handle, _nvmlEnableState_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetComputeMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetComputeMode")
+    ret = fn(handle, _nvmlComputeMode_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetEccMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetEccMode")
+    ret = fn(handle, _nvmlEnableState_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceClearEccErrorCounts(handle, counterType):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearEccErrorCounts")
+    ret = fn(handle, _nvmlEccCounterType_t(counterType))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetDriverModel(handle, model):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetDriverModel")
+    ret = fn(handle, _nvmlDriverModel_t(model))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetAutoBoostedClocksEnabled(handle, enabled):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetAutoBoostedClocksEnabled")
+    ret = fn(handle, _nvmlEnableState_t(enabled))
+    _nvmlCheckReturn(ret)
+    return None
+    #Throws MXSMLEX_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks
+
+def nvmlDeviceSetDefaultAutoBoostedClocksEnabled(handle, enabled, flags):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetDefaultAutoBoostedClocksEnabled")
+    ret = fn(handle, _nvmlEnableState_t(enabled), c_uint(flags))
+    _nvmlCheckReturn(ret)
+    return None
+    #Throws MXSMLEX_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks
+
+def nvmlDeviceSetGpuLockedClocks(handle, minGpuClockMHz, maxGpuClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetGpuLockedClocks")
+    ret = fn(handle, c_uint(minGpuClockMHz), c_uint(maxGpuClockMHz))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceResetGpuLockedClocks(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetGpuLockedClocks")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetMemoryLockedClocks(handle, minMemClockMHz, maxMemClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetMemoryLockedClocks")
+    ret = fn(handle, c_uint(minMemClockMHz), c_uint(maxMemClockMHz))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceResetMemoryLockedClocks(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetMemoryLockedClocks")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetClkMonStatus(handle, c_clkMonInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetClkMonStatus")
+    ret = fn(handle, c_clkMonInfo)
+    return ret
+
+# Added in 4.304
+def nvmlDeviceSetApplicationsClocks(handle, maxMemClockMHz, maxGraphicsClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetApplicationsClocks")
+    ret = fn(handle, c_uint(maxMemClockMHz), c_uint(maxGraphicsClockMHz))
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 4.304
+def nvmlDeviceResetApplicationsClocks(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetApplicationsClocks")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 4.304
+def nvmlDeviceSetPowerManagementLimit(handle, limit):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPowerManagementLimit")
+    ret = fn(handle, c_uint(limit))
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 4.304
+def nvmlDeviceSetGpuOperationMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetGpuOperationMode")
+    ret = fn(handle, _nvmlGpuOperationMode_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 2.285
+def nvmlEventSetCreate():
+    fn = _nvmlGetFunctionPointer("mxSmlExEventSetCreate")
+    eventSet = c_nvmlEventSet_t()
+    ret = fn(byref(eventSet))
+    _nvmlCheckReturn(ret)
+    return eventSet
+
+# Added in 2.285
+def nvmlDeviceRegisterEvents(handle, eventTypes, eventSet):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceRegisterEvents")
+    ret = fn(handle, c_ulonglong(eventTypes), eventSet)
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 2.285
+def nvmlDeviceGetSupportedEventTypes(handle):
+    c_eventTypes = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedEventTypes")
+    ret = fn(handle, byref(c_eventTypes))
+    _nvmlCheckReturn(ret)
+    return c_eventTypes.value
+
+# raises MXSMLEX_ERROR_TIMEOUT exception on timeout
+def nvmlEventSetWait_v2(eventSet, timeoutms):
+    fn = _nvmlGetFunctionPointer("mxSmlExEventSetWait_v2")
+    data = c_nvmlEventData_t()
+    ret = fn(eventSet, byref(data), c_uint(timeoutms))
+    _nvmlCheckReturn(ret)
+    return data
+
+def nvmlEventSetWait(eventSet, timeoutms):
+    return nvmlEventSetWait_v2(eventSet, timeoutms)
+
+# Added in 2.285
+def nvmlEventSetFree(eventSet):
+    fn = _nvmlGetFunctionPointer("mxSmlExEventSetFree")
+    ret = fn(eventSet)
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 3.295
+def nvmlDeviceOnSameBoard(handle1, handle2):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceOnSameBoard")
+    onSameBoard = c_int()
+    ret = fn(handle1, handle2, byref(onSameBoard))
+    _nvmlCheckReturn(ret)
+    return (onSameBoard.value != 0)
+
+# Added in 3.295
+def nvmlDeviceGetCurrPcieLinkGeneration(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrPcieLinkGeneration")
+    gen = c_uint()
+    ret = fn(handle, byref(gen))
+    _nvmlCheckReturn(ret)
+    return gen.value
+
+# Added in 3.295
+def nvmlDeviceGetMaxPcieLinkGeneration(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxPcieLinkGeneration")
+    gen = c_uint()
+    ret = fn(handle, byref(gen))
+    _nvmlCheckReturn(ret)
+    return gen.value
+
+# Added in 3.295
+def nvmlDeviceGetCurrPcieLinkWidth(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrPcieLinkWidth")
+    width = c_uint()
+    ret = fn(handle, byref(width))
+    _nvmlCheckReturn(ret)
+    return width.value
+
+# Added in 3.295
+def nvmlDeviceGetMaxPcieLinkWidth(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxPcieLinkWidth")
+    width = c_uint()
+    ret = fn(handle, byref(width))
+    _nvmlCheckReturn(ret)
+    return width.value
+
+def nvmlDeviceGetGpuMaxPcieLinkGeneration(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuMaxPcieLinkGeneration")
+    gen = c_uint()
+    ret = fn(handle, byref(gen))
+    _nvmlCheckReturn(ret)
+    return gen.value
+
+# Added in 4.304
+def nvmlDeviceGetSupportedClocksThrottleReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedClocksThrottleReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+def nvmlDeviceGetSupportedClocksEventReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedClocksEventReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+# Added in 4.304
+def nvmlDeviceGetCurrentClocksThrottleReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrentClocksThrottleReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+def nvmlDeviceGetCurrentClocksEventReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrentClocksEventReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+# Added in 5.319
+def nvmlDeviceGetIndex(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetIndex")
+    c_index = c_uint()
+    ret = fn(handle, byref(c_index))
+    _nvmlCheckReturn(ret)
+    return c_index.value
+
+# Added in 5.319
+def nvmlDeviceGetAccountingMode(handle):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingMode")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlDeviceSetAccountingMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetAccountingMode")
+    ret = fn(handle, _nvmlEnableState_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceClearAccountingPids(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearAccountingPids")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetAccountingStats(handle, pid):
+    stats = c_nvmlAccountingStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingStats")
+    ret = fn(handle, c_uint(pid), byref(stats))
+    _nvmlCheckReturn(ret)
+    if (stats.maxMemoryUsage == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+        # special case for WDDM on Windows, see comment above
+        stats.maxMemoryUsage = None
+    return stats
+
+def nvmlDeviceGetAccountingPids(handle):
+    count = c_uint(nvmlDeviceGetAccountingBufferSize(handle))
+    pids = (c_uint * count.value)()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingPids")
+    ret = fn(handle, byref(count), pids)
+    _nvmlCheckReturn(ret)
+    return list(map(int, pids[0:count.value]))
+
+def nvmlDeviceGetAccountingBufferSize(handle):
+    bufferSize = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingBufferSize")
+    ret = fn(handle, byref(bufferSize))
+    _nvmlCheckReturn(ret)
+    return int(bufferSize.value)
+
+def nvmlDeviceGetRetiredPages(device, sourceFilter):
+    c_source = _nvmlPageRetirementCause_t(sourceFilter)
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRetiredPages")
+
+    # First call will get the size
+    ret = fn(device, c_source, byref(c_count), None)
+
+    # this should only fail with insufficient size
+    if ((ret != MXSMLEX_SUCCESS) and
+        (ret != MXSMLEX_ERROR_INSUFFICIENT_SIZE)):
+        raise NVMLError(ret)
+
+    # call again with a buffer
+    # oversize the array for the rare cases where additional pages
+    # are retired between MXSMLEX calls
+    c_count.value = c_count.value * 2 + 5
+    page_array = c_ulonglong * c_count.value
+    c_pages = page_array()
+    ret = fn(device, c_source, byref(c_count), c_pages)
+    _nvmlCheckReturn(ret)
+    return list(map(int, c_pages[0:c_count.value]))
+
+def nvmlDeviceGetRetiredPages_v2(device, sourceFilter):
+    c_source = _nvmlPageRetirementCause_t(sourceFilter)
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRetiredPages_v2")
+
+    # First call will get the size
+    ret = fn(device, c_source, byref(c_count), None)
+
+    # this should only fail with insufficient size
+    if ((ret != MXSMLEX_SUCCESS) and
+        (ret != MXSMLEX_ERROR_INSUFFICIENT_SIZE)):
+        raise NVMLError(ret)
+
+    # call again with a buffer
+    # oversize the array for the rare cases where additional pages
+    # are retired between MXSMLEX calls
+    c_count.value = c_count.value * 2 + 5
+    page_array = c_ulonglong * c_count.value
+    c_pages = page_array()
+    times_array = c_ulonglong * c_count.value
+    c_times = times_array()
+    ret = fn(device, c_source, byref(c_count), c_pages, c_times)
+    _nvmlCheckReturn(ret)
+    return [ { 'address': int(c_pages[i]), 'timestamp': int(c_times[i]) } for i in range(c_count.value) ];
+
+def nvmlDeviceGetRetiredPagesPendingStatus(device):
+    c_pending = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRetiredPagesPendingStatus")
+    ret = fn(device, byref(c_pending))
+    _nvmlCheckReturn(ret)
+    return int(c_pending.value)
+
+def nvmlDeviceGetAPIRestriction(device, apiType):
+    c_permission = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAPIRestriction")
+    ret = fn(device, _nvmlRestrictedAPI_t(apiType), byref(c_permission))
+    _nvmlCheckReturn(ret)
+    return int(c_permission.value)
+
+def nvmlDeviceSetAPIRestriction(handle, apiType, isRestricted):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetAPIRestriction")
+    ret = fn(handle, _nvmlRestrictedAPI_t(apiType), _nvmlEnableState_t(isRestricted))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetBridgeChipInfo(handle):
+    bridgeHierarchy = c_nvmlBridgeChipHierarchy_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBridgeChipInfo")
+    ret = fn(handle, byref(bridgeHierarchy))
+    _nvmlCheckReturn(ret)
+    return bridgeHierarchy
+
+def nvmlDeviceGetSamples(device, sampling_type, timeStamp):
+    c_sampling_type = _nvmlSamplingType_t(sampling_type)
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_sample_count = c_uint(0)
+    c_sample_value_type = _nvmlValueType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSamples")
+
+    ## First Call gets the size
+    ret = fn(device, c_sampling_type, c_time_stamp, byref(c_sample_value_type), byref(c_sample_count), None)
+
+    # Stop if this fails
+    if (ret != MXSMLEX_SUCCESS):
+        raise NVMLError(ret)
+
+    sampleArray = c_sample_count.value * c_nvmlSample_t
+    c_samples = sampleArray()
+    ret = fn(device, c_sampling_type, c_time_stamp,  byref(c_sample_value_type), byref(c_sample_count), c_samples)
+    _nvmlCheckReturn(ret)
+    return (c_sample_value_type.value, c_samples[0:c_sample_count.value])
+
+def nvmlDeviceGetViolationStatus(device, perfPolicyType):
+    c_perfPolicy_type = _nvmlPerfPolicyType_t(perfPolicyType)
+    c_violTime = c_nvmlViolationTime_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetViolationStatus")
+
+    ## Invoke the method to get violation time
+    ret = fn(device, c_perfPolicy_type, byref(c_violTime))
+    _nvmlCheckReturn(ret)
+    return c_violTime
+
+def nvmlDeviceGetPcieThroughput(device, counter):
+    c_util = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieThroughput")
+    ret = fn(device, _nvmlPcieUtilCounter_t(counter), byref(c_util))
+    _nvmlCheckReturn(ret)
+    return c_util.value
+
+def nvmlSystemGetTopologyGpuSet(cpuNumber):
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetTopologyGpuSet")
+
+    # First call will get the size
+    ret = fn(cpuNumber, byref(c_count), None)
+
+    if ret != MXSMLEX_SUCCESS:
+        raise NVMLError(ret)
+    # call again with a buffer
+    device_array = c_nvmlDevice_t * c_count.value
+    c_devices = device_array()
+    ret = fn(cpuNumber, byref(c_count), c_devices)
+    _nvmlCheckReturn(ret)
+    return list(c_devices[0:c_count.value])
+
+def nvmlDeviceGetTopologyNearestGpus(device, level):
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTopologyNearestGpus")
+
+    # First call will get the size
+    ret = fn(device, level, byref(c_count), None)
+
+    if ret != MXSMLEX_SUCCESS:
+        raise NVMLError(ret)
+
+    # call again with a buffer
+    device_array = c_nvmlDevice_t * c_count.value
+    c_devices = device_array()
+    ret = fn(device, level, byref(c_count), c_devices)
+    _nvmlCheckReturn(ret)
+    return list(c_devices[0:c_count.value])
+
+def nvmlDeviceGetTopologyCommonAncestor(device1, device2):
+    c_level = _nvmlGpuTopologyLevel_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTopologyCommonAncestor")
+    ret = fn(device1, device2, byref(c_level))
+    _nvmlCheckReturn(ret)
+    return c_level.value
+
+def nvmlDeviceGetNvLinkUtilizationCounter(device, link, counter):
+    c_rxcounter = c_ulonglong()
+    c_txcounter = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkUtilizationCounter")
+    ret = fn(device, link, counter, byref(c_rxcounter), byref(c_txcounter))
+    _nvmlCheckReturn(ret)
+    return (c_rxcounter.value, c_txcounter.value)
+
+def nvmlDeviceFreezeNvLinkUtilizationCounter(device, link, counter, freeze):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceFreezeNvLinkUtilizationCounter")
+    ret = fn(device, link, counter, freeze)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceResetNvLinkUtilizationCounter(device, link, counter):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetNvLinkUtilizationCounter")
+    ret = fn(device, link, counter)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetNvLinkUtilizationControl(device, link, counter, control, reset):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetNvLinkUtilizationControl")
+    ret = fn(device, link, counter, byref(control), reset)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetNvLinkUtilizationControl(device, link, counter):
+    c_control = nvmlNvLinkUtilizationControl_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkUtilizationControl")
+    ret = fn(device, link, counter, byref(c_control))
+    _nvmlCheckReturn(ret)
+    return c_control
+
+def nvmlDeviceGetNvLinkCapability(device, link, capability):
+    c_capResult = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkCapability")
+    ret = fn(device, link, capability, byref(c_capResult))
+    _nvmlCheckReturn(ret)
+    return c_capResult.value
+
+def nvmlDeviceGetNvLinkErrorCounter(device, link, counter):
+    c_result = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkErrorCounter")
+    ret = fn(device, link, counter, byref(c_result))
+    _nvmlCheckReturn(ret)
+    return c_result.value
+
+def nvmlDeviceResetNvLinkErrorCounters(device, link):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetNvLinkErrorCounters")
+    ret = fn(device, link)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetNvLinkRemotePciInfo(device, link):
+    c_pci = nvmlPciInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkRemotePciInfo_v2")
+    ret = fn(device, link, byref(c_pci))
+    _nvmlCheckReturn(ret)
+    return c_pci
+
+def nvmlDeviceGetNvLinkRemoteDeviceType(handle, link):
+    c_type = _nvmlNvLinkDeviceType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkRemoteDeviceType")
+    ret = fn(handle, link, byref(c_type))
+    _nvmlCheckReturn(ret)
+    return c_type.value
+
+def nvmlDeviceGetNvLinkState(device, link):
+    c_isActive = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkState")
+    ret = fn(device, link, byref(c_isActive))
+    _nvmlCheckReturn(ret)
+    return c_isActive.value
+
+def nvmlDeviceGetNvLinkVersion(device, link):
+    c_version = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkVersion")
+    ret = fn(device, link, byref(c_version))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+def nvmlDeviceModifyDrainState(pciInfo, newState):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceModifyDrainState")
+    ret = fn(pointer(pciInfo), newState)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceQueryDrainState(pciInfo):
+    c_newState = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceQueryDrainState")
+    ret = fn(pointer(pciInfo), byref(c_newState))
+    _nvmlCheckReturn(ret)
+    return c_newState.value
+
+def nvmlDeviceRemoveGpu(pciInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceRemoveGpu")
+    ret = fn(pointer(pciInfo))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceDiscoverGpus(pciInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceDiscoverGpus")
+    ret = fn(pointer(pciInfo))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetFieldValues(handle, fieldIds):
+    values_arr = c_nvmlFieldValue_t * len(fieldIds)
+    values = values_arr()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFieldValues")
+
+    for i, fieldId in enumerate(fieldIds):
+        try:
+            (values[i].fieldId, values[i].scopeId) = fieldId
+        except TypeError:
+            values[i].fieldId = fieldId
+
+    ret = fn(handle, c_int32(len(fieldIds)), byref(values))
+    _nvmlCheckReturn(ret)
+    return values
+
+def nvmlDeviceClearFieldValues(handle, fieldIds):
+    values_arr = c_nvmlFieldValue_t * len(fieldIds)
+    values = values_arr()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearFieldValues")
+
+    for i, fieldId in enumerate(fieldIds):
+        try:
+            (values[i].fieldId, values[i].scopeId) = fieldId
+        except TypeError:
+            values[i].fieldId = fieldId
+
+    ret = fn(handle, c_int32(len(fieldIds)), byref(values))
+    _nvmlCheckReturn(ret)
+    return values
+
+def nvmlDeviceGetVirtualizationMode(handle):
+    c_virtualization_mode = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVirtualizationMode")
+    ret = fn(handle, byref(c_virtualization_mode))
+    _nvmlCheckReturn(ret)
+    return c_virtualization_mode.value
+
+def nvmlDeviceSetVirtualizationMode(handle, virtualization_mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVirtualizationMode")
+    return fn(handle, virtualization_mode)
+
+def nvmlDeviceGetVgpuHeterogeneousMode(handle):
+    c_vgpuHeterogeneousMode = c_nvmlVgpuHeterogeneousMode_v1_t(0)
+    c_vgpuHeterogeneousMode.version = VgpuHeterogeneousMode_v1
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuHeterogeneousMode")
+    ret = fn(handle, byref(c_vgpuHeterogeneousMode))
+    _nvmlCheckReturn(ret)
+    return c_vgpuHeterogeneousMode.mode
+
+def nvmlDeviceSetVgpuHeterogeneousMode(handle, heterogeneous_mode):
+    c_vgpuHeterogeneousMode = c_nvmlVgpuHeterogeneousMode_v1_t(0)
+    c_vgpuHeterogeneousMode.version = VgpuHeterogeneousMode_v1
+    c_vgpuHeterogeneousMode.mode = heterogeneous_mode
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVgpuHeterogeneousMode")
+    ret = fn(handle, byref(c_vgpuHeterogeneousMode))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlVgpuInstanceGetPlacementId(vgpuInstance):
+    c_placement = c_nvmlVgpuPlacementId_v1_t(0)
+    c_placement.version = VgpuPlacementId_v1
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetPlacementId")
+    ret = fn(vgpuInstance, byref(c_placement))
+    _nvmlCheckReturn(ret)
+    return c_placement.placementId
+
+def nvmlDeviceGetVgpuTypeSupportedPlacements(handle, vgpuTypeId):
+    c_vgpu_placements = c_nvmlVgpuPlacementList_v1_t()
+    c_max_instances = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstances")
+    ret = fn(handle, vgpuTypeId, byref(c_max_instances))
+    _nvmlCheckReturn(ret)
+    c_placements = c_uint * c_max_instances.value
+    c_vgpu_placements.version = VgpuPlacementList_v1
+    c_vgpu_placements.placementIds = c_placements()
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuTypeSupportedPlacements")
+    ret = fn(handle, vgpuTypeId, byref(c_vgpu_placements))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_placements
+
+def nvmlDeviceGetVgpuTypeCreatablePlacements(handle, vgpuTypeId):
+    c_vgpu_placements = c_nvmlVgpuPlacementList_v1_t()
+    c_max_instances = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstances")
+    ret = fn(handle, vgpuTypeId, byref(c_max_instances))
+    _nvmlCheckReturn(ret)
+    c_placements = c_uint * c_max_instances.value
+    c_vgpu_placements.version = VgpuPlacementList_v1
+    c_vgpu_placements.placementIds = c_placements()
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuTypeCreatablePlacements")
+    ret = fn(handle, vgpuTypeId, byref(c_vgpu_placements))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_placements
+
+def nvmlGetVgpuDriverCapabilities(capability):
+    c_capResult = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetVgpuDriverCapabilities")
+    ret = fn(_nvmlVgpuDriverCapability_t(capability), byref(c_capResult))
+    _nvmlCheckReturn(ret)
+    return c_capResult.value
+
+def nvmlDeviceGetVgpuCapabilities(handle, capability):
+    c_capResult = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuCapabilities")
+    ret = fn(handle, _nvmlDeviceVgpuCapability_t(capability), byref(c_capResult))
+    _nvmlCheckReturn(ret)
+    return c_capResult.value
+
+def nvmlDeviceSetVgpuCapabilities(handle, capability, state):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVgpuCapabilities")
+    ret = fn(handle, _nvmlDeviceVgpuCapability_t(capability), state)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetSupportedVgpus(handle):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+
+    fn =  _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedVgpus")
+    ret = fn(handle, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no supported vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        vgpu_type_ids_array = _nvmlVgpuTypeId_t * c_vgpu_count.value
+        c_vgpu_type_ids = vgpu_type_ids_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpu_count), c_vgpu_type_ids)
+        _nvmlCheckReturn(ret)
+        vgpus = []
+        for i in range(c_vgpu_count.value):
+            vgpus.append(c_vgpu_type_ids[i])
+        return vgpus
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetCreatableVgpus(handle):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+
+    fn =  _nvmlGetFunctionPointer("mxSmlExDeviceGetCreatableVgpus")
+    ret = fn(handle, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no supported vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        vgpu_type_ids_array = _nvmlVgpuTypeId_t * c_vgpu_count.value
+        c_vgpu_type_ids = vgpu_type_ids_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpu_count), c_vgpu_type_ids)
+        _nvmlCheckReturn(ret)
+        vgpus = []
+        for i in range(c_vgpu_count.value):
+            vgpus.append(c_vgpu_type_ids[i])
+        return vgpus
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuTypeGetGpuInstanceProfileId(vgpuTypeId):
+    c_profile_id = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetGpuInstanceProfileId")
+    ret = fn(vgpuTypeId, byref(c_profile_id))
+    _nvmlCheckReturn(ret)
+    return (c_profile_id.value)
+
+@convertStrBytes
+def nvmlVgpuTypeGetClass(vgpuTypeId):
+    c_class = create_string_buffer(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetClass")
+    ret = fn(vgpuTypeId, c_class, byref(c_buffer_size))
+    _nvmlCheckReturn(ret)
+    return c_class.value
+
+@convertStrBytes
+def nvmlVgpuTypeGetName(vgpuTypeId):
+    c_name = create_string_buffer(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetName")
+    ret = fn(vgpuTypeId, c_name, byref(c_buffer_size))
+    _nvmlCheckReturn(ret)
+    return c_name.value
+
+def nvmlVgpuTypeGetDeviceID(vgpuTypeId):
+    c_device_id    = c_ulonglong(0)
+    c_subsystem_id = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetDeviceID")
+    ret = fn(vgpuTypeId, byref(c_device_id), byref(c_subsystem_id))
+    _nvmlCheckReturn(ret)
+    return (c_device_id.value, c_subsystem_id.value)
+
+def nvmlVgpuTypeGetFramebufferSize(vgpuTypeId):
+    c_fb_size = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetFramebufferSize")
+    ret = fn(vgpuTypeId, byref(c_fb_size))
+    _nvmlCheckReturn(ret)
+    return c_fb_size.value
+
+def nvmlVgpuTypeGetNumDisplayHeads(vgpuTypeId):
+    c_num_heads = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetNumDisplayHeads")
+    ret = fn(vgpuTypeId, byref(c_num_heads))
+    _nvmlCheckReturn(ret)
+    return c_num_heads.value
+
+def nvmlVgpuTypeGetResolution(vgpuTypeId):
+    c_xdim = c_uint(0)
+    c_ydim = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetResolution")
+    ret = fn(vgpuTypeId, 0, byref(c_xdim), byref(c_ydim))
+    _nvmlCheckReturn(ret)
+    return (c_xdim.value, c_ydim.value)
+
+@convertStrBytes
+def nvmlVgpuTypeGetLicense(vgpuTypeId):
+    c_license = create_string_buffer(MXSMLEX_GRID_LICENSE_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_GRID_LICENSE_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetLicense")
+    ret = fn(vgpuTypeId, c_license, c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_license.value
+
+def nvmlVgpuTypeGetFrameRateLimit(vgpuTypeId):
+    c_frl_config = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetFrameRateLimit")
+    ret = fn(vgpuTypeId, byref(c_frl_config))
+    _nvmlCheckReturn(ret)
+    return c_frl_config.value
+
+def nvmlVgpuTypeGetGspHeapSize(vgpuTypeId):
+    c_gsp_heap = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetGspHeapSize")
+    ret = fn(vgpuTypeId, byref(c_gsp_heap))
+    _nvmlCheckReturn(ret)
+    return c_gsp_heap.value
+
+def nvmlVgpuTypeGetFbReservation(vgpuTypeId):
+    c_fb_reservation = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetFbReservation")
+    ret = fn(vgpuTypeId, byref(c_fb_reservation))
+    _nvmlCheckReturn(ret)
+    return c_fb_reservation.value
+
+def nvmlVgpuTypeGetMaxInstances(handle, vgpuTypeId):
+    c_max_instances = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstances")
+    ret = fn(handle, vgpuTypeId, byref(c_max_instances))
+    _nvmlCheckReturn(ret)
+    return c_max_instances.value
+
+def nvmlVgpuTypeGetMaxInstancesPerVm(vgpuTypeId):
+    c_max_instances_per_vm = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstancesPerVm")
+    ret = fn(vgpuTypeId, byref(c_max_instances_per_vm))
+    _nvmlCheckReturn(ret)
+    return c_max_instances_per_vm.value
+
+def nvmlDeviceGetActiveVgpus(handle):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetActiveVgpus")
+    ret = fn(handle, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        vgpu_instance_array = _nvmlVgpuInstance_t * c_vgpu_count.value
+        c_vgpu_instances = vgpu_instance_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpu_count), c_vgpu_instances)
+        _nvmlCheckReturn(ret)
+        vgpus = []
+        for i in range(c_vgpu_count.value):
+            vgpus.append(c_vgpu_instances[i])
+        return vgpus
+    else:
+        # error case
+        raise NVMLError(ret)
+
+@convertStrBytes
+def nvmlVgpuInstanceGetVmID(vgpuInstance):
+    c_vm_id = create_string_buffer(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_GRID_LICENSE_BUFFER_SIZE)
+    c_vm_id_type  = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetVmID")
+    ret = fn(vgpuInstance, byref(c_vm_id), c_buffer_size, byref(c_vm_id_type))
+    _nvmlCheckReturn(ret)
+    return (c_vm_id.value, c_vm_id_type.value)
+
+@convertStrBytes
+def nvmlVgpuInstanceGetUUID(vgpuInstance):
+    c_uuid = create_string_buffer(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetUUID")
+    ret = fn(vgpuInstance, byref(c_uuid), c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_uuid.value
+
+@convertStrBytes
+def nvmlVgpuInstanceGetMdevUUID(vgpuInstance):
+    c_uuid = create_string_buffer(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetMdevUUID")
+    ret = fn(vgpuInstance, byref(c_uuid), c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_uuid.value
+
+@convertStrBytes
+def nvmlVgpuInstanceGetVmDriverVersion(vgpuInstance):
+    c_driver_version = create_string_buffer(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetVmDriverVersion")
+    ret = fn(vgpuInstance, byref(c_driver_version), c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_driver_version.value
+
+def nvmlVgpuInstanceGetLicenseStatus(vgpuInstance):
+    c_license_status = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetLicenseStatus")
+    ret = fn(vgpuInstance, byref(c_license_status))
+    _nvmlCheckReturn(ret)
+    return c_license_status.value
+
+def nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance):
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetLicenseInfo_v2")
+    c_license_info = c_nvmlVgpuLicenseInfo_t()
+    ret = fn(vgpuInstance, byref(c_license_info))
+    _nvmlCheckReturn(ret)
+    return c_license_info
+
+def nvmlVgpuInstanceGetLicenseInfo(vgpuInstance):
+    return nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance)
+
+def nvmlVgpuInstanceGetFrameRateLimit(vgpuInstance):
+    c_frl = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFrameRateLimit")
+    ret = fn(vgpuInstance, byref(c_frl))
+    _nvmlCheckReturn(ret)
+    return c_frl.value
+
+def nvmlVgpuInstanceGetEccMode(vgpuInstance):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEccMode")
+    ret = fn(vgpuInstance, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlVgpuInstanceGetType(vgpuInstance):
+    c_vgpu_type = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetType")
+    ret = fn(vgpuInstance, byref(c_vgpu_type))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_type.value
+
+def nvmlVgpuInstanceGetEncoderCapacity(vgpuInstance):
+    c_encoder_capacity = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEncoderCapacity")
+    ret = fn(vgpuInstance, byref(c_encoder_capacity))
+    _nvmlCheckReturn(ret)
+    return c_encoder_capacity.value
+
+def nvmlVgpuInstanceSetEncoderCapacity(vgpuInstance, encoder_capacity):
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceSetEncoderCapacity")
+    return fn(vgpuInstance, encoder_capacity)
+
+def nvmlVgpuInstanceGetFbUsage(vgpuInstance):
+    c_fb_usage = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFbUsage")
+    ret = fn(vgpuInstance, byref(c_fb_usage))
+    _nvmlCheckReturn(ret)
+    return c_fb_usage.value
+
+def nvmlVgpuTypeGetCapabilities(vgpuTypeId, capability):
+    c_cap_result = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetCapabilities")
+    ret = fn(vgpuTypeId, _nvmlVgpuCapability_t(capability), byref(c_cap_result))
+    _nvmlCheckReturn(ret)
+    return (c_cap_result.value)
+
+def nvmlVgpuInstanceGetGpuInstanceId(vgpuInstance):
+    c_id = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetGpuInstanceId")
+    ret = fn(vgpuInstance, byref(c_id))
+    _nvmlCheckReturn(ret)
+    return (c_id.value)
+
+@convertStrBytes
+def nvmlVgpuInstanceGetGpuPciId(vgpuInstance):
+    c_vgpuPciId = create_string_buffer(MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetGpuPciId")
+    ret = fn(vgpuInstance, c_vgpuPciId, byref(c_uint(MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE)))
+    _nvmlCheckReturn(ret)
+    return c_vgpuPciId.value
+
+def nvmlDeviceGetVgpuUtilization(handle, timeStamp):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_sample_value_type = _nvmlValueType_t()
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuUtilization")
+    ret = fn(handle, c_time_stamp, byref(c_sample_value_type), byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpu_count.value * c_nvmlVgpuInstanceUtilizationSample_t
+        c_samples = sampleArray()
+
+        # make the call again
+        ret = fn(handle, c_time_stamp, byref(c_sample_value_type), byref(c_vgpu_count), c_samples)
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpu_count.value]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetVgpuInstancesUtilizationInfo(handle, timeStamp):
+    # first call to get the size
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_vgpuUtilInfo = c_nvmlVgpuInstancesUtilizationInfo_v1_t(0)
+    c_vgpuUtilInfo.version = VgpuInstancesUtilizationInfo_v1
+    c_vgpuUtilInfo.sampleValType = _nvmlValueType_t()
+    c_vgpuUtilInfo.vgpuInstanceCount = c_uint(0)
+    c_vgpuUtilInfo.lastSeenTimeStamp = c_time_stamp
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuInstancesUtilizationInfo")
+    ret = fn(handle, byref(c_vgpuUtilInfo))
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpuUtilInfo.vgpuInstanceCount * c_nvmlVgpuInstanceUtilizationInfo_v1_t
+        c_samples = sampleArray()
+        c_vgpuUtilInfo.vgpuUtilArray = c_samples
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpuUtilInfo))
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpuUtilInfo.vgpuInstanceCount]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetP2PStatus(device1, device2, p2pIndex):
+    c_p2pstatus = _nvmlGpuP2PStatus_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetP2PStatus")
+    ret = fn(device1, device2,p2pIndex, byref(c_p2pstatus))
+    _nvmlCheckReturn(ret)
+    return c_p2pstatus.value
+
+def nvmlDeviceGetGridLicensableFeatures_v4(handle):
+    c_get_grid_licensable_features = c_nvmlGridLicensableFeatures_v4_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGridLicensableFeatures_v4")
+    ret = fn(handle, byref(c_get_grid_licensable_features))
+    _nvmlCheckReturn(ret)
+
+    return (c_get_grid_licensable_features)
+
+def nvmlDeviceGetGridLicensableFeatures(handle):
+    return nvmlDeviceGetGridLicensableFeatures_v4(handle)
+
+def nvmlDeviceGetGspFirmwareVersion(handle, version):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGspFirmwareVersion")
+    ret = fn(handle, version)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGspFirmwareMode(handle, isEnabled, defaultMode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGspFirmwareMode")
+    ret = fn(handle, isEnabled, defaultMode)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetEncoderCapacity(handle, encoderQueryType):
+    c_encoder_capacity = c_ulonglong(0)
+    c_encoderQuery_type = _nvmlEncoderQueryType_t(encoderQueryType)
+
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderCapacity")
+    ret = fn(handle, c_encoderQuery_type, byref(c_encoder_capacity))
+    _nvmlCheckReturn(ret)
+    return c_encoder_capacity.value
+
+def nvmlDeviceGetVgpuProcessUtilization(handle, timeStamp):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+    c_time_stamp = c_ulonglong(timeStamp)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuProcessUtilization")
+    ret = fn(handle, c_time_stamp, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpu_count.value * c_nvmlVgpuProcessUtilizationSample_t
+        c_samples = sampleArray()
+
+        # make the call again
+        ret = fn(handle, c_time_stamp, byref(c_vgpu_count), c_samples)
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpu_count.value]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetVgpuProcessesUtilizationInfo(handle, timeStamp):
+    # first call to get the size
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_vgpuProcUtilInfo = c_nvmlVgpuProcessesUtilizationInfo_v1_t(0)
+    c_vgpuProcUtilInfo.version = VgpuProcessesUtilizationInfo_v1
+    c_vgpuProcUtilInfo.vgpuProcessCount = c_uint(0)
+    c_vgpuProcUtilInfo.lastSeenTimeStamp = c_time_stamp
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuProcessesUtilizationInfo")
+    ret = fn(handle, byref(c_vgpuProcUtilInfo))
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpuProcUtilInfo.vgpuProcessCount * c_nvmlVgpuProcessUtilizationInfo_v1_t
+        c_samples = sampleArray()
+        c_vgpuProcUtilInfo.vgpuProcUtilArray = c_samples
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpuProcUtilInfo))
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpuProcUtilInfo.vgpuProcessCount]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetEncoderStats(handle):
+    c_encoderCount = c_ulonglong(0)
+    c_encodeFps = c_ulonglong(0)
+    c_encoderLatency = c_ulonglong(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderStats")
+    ret = fn(handle, byref(c_encoderCount), byref(c_encodeFps), byref(c_encoderLatency))
+    _nvmlCheckReturn(ret)
+    return (c_encoderCount.value, c_encodeFps.value, c_encoderLatency.value)
+
+def nvmlDeviceGetEncoderSessions(handle):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderSessions")
+    ret = fn(handle, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlEncoderSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(handle, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetFBCStats(handle):
+    c_fbcStats = c_nvmlFBCStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFBCStats")
+    ret = fn(handle, byref(c_fbcStats))
+    _nvmlCheckReturn(ret)
+    return c_fbcStats
+
+def nvmlDeviceGetFBCSessions(handle):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetFBCSessions")
+    ret = fn(handle, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlFBCSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(handle, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuInstanceGetEncoderStats(vgpuInstance):
+    c_encoderCount    = c_ulonglong(0)
+    c_encodeFps       = c_ulonglong(0)
+    c_encoderLatency  = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEncoderStats")
+    ret = fn(vgpuInstance, byref(c_encoderCount), byref(c_encodeFps), byref(c_encoderLatency))
+    _nvmlCheckReturn(ret)
+    return (c_encoderCount.value, c_encodeFps.value, c_encoderLatency.value)
+
+def nvmlVgpuInstanceGetEncoderSessions(vgpuInstance):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEncoderSessions")
+    ret = fn(vgpuInstance, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlEncoderSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(vgpuInstance, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuInstanceGetFBCStats(vgpuInstance):
+    c_fbcStats = c_nvmlFBCStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFBCStats")
+    ret = fn(vgpuInstance, byref(c_fbcStats))
+    _nvmlCheckReturn(ret)
+    return c_fbcStats
+
+def nvmlVgpuInstanceGetFBCSessions(vgpuInstance):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFBCSessions")
+    ret = fn(vgpuInstance, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlFBCSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(vgpuInstance, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetProcessUtilization(handle, timeStamp):
+    # first call to get the size
+    c_count = c_uint(0)
+    c_time_stamp = c_ulonglong(timeStamp)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetProcessUtilization")
+    ret = fn(handle, None, byref(c_count), c_time_stamp)
+
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_count.value * c_nvmlProcessUtilizationSample_t
+        c_samples = sampleArray()
+
+        # make the call again
+        ret = fn(handle, c_samples, byref(c_count), c_time_stamp)
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_count.value]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetProcessesUtilizationInfo(handle, timeStamp):
+    # first call to get the size
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_processesUtilInfo = c_nvmlProcessesUtilizationInfo_v1_t(0)
+    c_processesUtilInfo.version = ProcessesUtilizationInfo_v1
+    c_processesUtilInfo.processSamplesCount = c_uint(0)
+    c_processesUtilInfo.lastSeenTimeStamp = c_time_stamp
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetProcessesUtilizationInfo")
+    ret = fn(handle, byref(c_processesUtilInfo))
+
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_processesUtilInfo.processSamplesCount * c_nvmlProcessUtilizationInfo_v1_t
+        c_samples = sampleArray()
+        c_processesUtilInfo.procUtilArray = c_samples
+
+        # make the call again
+        ret = fn(handle, byref(c_processesUtilInfo))
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_processesUtilInfo.processSamplesCount]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuInstanceGetMetadata(vgpuInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetMetadata")
+    c_vgpuMetadata = c_nvmlVgpuMetadata_t()
+    c_bufferSize = c_uint(0)
+    # Make the first MXSMLEX API call to get the c_bufferSize value.
+    # We have already allocated required buffer above.
+    ret = fn(vgpuInstance, byref(c_vgpuMetadata), byref(c_bufferSize))
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = fn(vgpuInstance, byref(c_vgpuMetadata), byref(c_bufferSize))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return c_vgpuMetadata
+
+def nvmlDeviceGetVgpuMetadata(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuMetadata")
+    c_vgpuPgpuMetadata = c_nvmlVgpuPgpuMetadata_t()
+    c_bufferSize = c_uint(0)
+    # Make the first MXSMLEX API call to get the c_bufferSize value.
+    # We have already allocated required buffer above.
+    ret = fn(handle, byref(c_vgpuPgpuMetadata), byref(c_bufferSize))
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = fn(handle, byref(c_vgpuPgpuMetadata), byref(c_bufferSize))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return c_vgpuPgpuMetadata
+
+def nvmlGetVgpuCompatibility(vgpuMetadata, pgpuMetadata):
+    fn = _nvmlGetFunctionPointer("mxSmlExGetVgpuCompatibility")
+    c_vgpuPgpuCompatibility = c_nvmlVgpuPgpuCompatibility_t()
+    ret = fn(byref(vgpuMetadata), byref(pgpuMetadata), byref(c_vgpuPgpuCompatibility))
+    _nvmlCheckReturn(ret)
+    return c_vgpuPgpuCompatibility
+
+@convertStrBytes
+def nvmlDeviceGetPgpuMetadataString(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPgpuMetadataString")
+    c_pgpuMetadata = create_string_buffer(MXSMLEX_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE)
+    c_bufferSize = c_uint(0)
+    # Make the first MXSMLEX API call to get the c_bufferSize value.
+    # We have already allocated required buffer above.
+    ret = fn(handle, byref(c_pgpuMetadata), byref(c_bufferSize))
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = fn(handle, byref(c_pgpuMetadata), byref(c_bufferSize))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return (c_pgpuMetadata.value, c_bufferSize.value)
+
+def nvmlDeviceGetVgpuSchedulerLog(handle):
+    c_vgpu_sched_log = c_nvmlVgpuSchedulerLog_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuSchedulerLog")
+    ret = fn(handle, byref(c_vgpu_sched_log))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_sched_log
+
+def nvmlDeviceGetVgpuSchedulerState(handle):
+    c_vgpu_sched_state = c_nvmlVgpuSchedulerGetState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuSchedulerState")
+    ret = fn(handle, byref(c_vgpu_sched_state))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_sched_state
+
+def nvmlDeviceGetVgpuSchedulerCapabilities(handle):
+    c_vgpu_sched_caps = c_nvmlVgpuSchedulerCapabilities_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuSchedulerCapabilities")
+    ret = fn(handle, byref(c_vgpu_sched_caps))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_sched_caps
+
+def nvmlDeviceSetVgpuSchedulerState(handle, sched_state):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVgpuSchedulerState")
+    ret = fn(handle, byref(sched_state))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSetVgpuVersion(vgpuVersion):
+    fn = _nvmlGetFunctionPointer("mxSmlExSetVgpuVersion")
+    ret = fn(byref(vgpuVersion))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGetVgpuVersion(supported, current):
+    fn = _nvmlGetFunctionPointer("mxSmlExGetVgpuVersion")
+    ret = fn(byref(supported), byref(current))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlVgpuInstanceGetAccountingMode(vgpuInstance):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetAccountingMode")
+    ret = fn(vgpuInstance, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlVgpuInstanceGetAccountingPids(vgpuInstance):
+    c_pidCount = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetAccountingPids")
+    ret = fn(vgpuInstance, byref(c_pidCount), None)
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        sampleArray = c_pidCount.value * c_uint
+        c_pidArray = sampleArray()
+        ret = fn(vgpuInstance, byref(c_pidCount), byref(c_pidArray))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return (c_pidCount, c_pidArray)
+
+def nvmlVgpuInstanceGetAccountingStats(vgpuInstance, pid):
+    c_accountingStats = c_nvmlAccountingStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetAccountingStats")
+    ret = fn(vgpuInstance, pid, byref(c_accountingStats))
+    _nvmlCheckReturn(ret)
+    return c_accountingStats
+
+def nvmlVgpuInstanceClearAccountingPids(vgpuInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceClearAccountingPids")
+    ret = fn(vgpuInstance)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGetExcludedDeviceCount():
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetExcludedDeviceCount")
+    ret = fn(byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlGetExcludedDeviceInfoByIndex(index):
+    c_index = c_uint(index)
+    info = c_nvmlExcludedDeviceInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetExcludedDeviceInfoByIndex")
+    ret = fn(c_index, byref(info))
+    _nvmlCheckReturn(ret)
+    return info
+
+def nvmlDeviceGetHostVgpuMode(handle):
+    c_host_vgpu_mode = _nvmlHostVgpuMode_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHostVgpuMode")
+    ret = fn(handle, byref(c_host_vgpu_mode))
+    _nvmlCheckReturn(ret)
+    return c_host_vgpu_mode.value
+
+def nvmlDeviceSetMigMode(device, mode):
+    c_activationStatus = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetMigMode")
+    ret = fn(device, mode, byref(c_activationStatus))
+    _nvmlCheckReturn(ret)
+    return c_activationStatus.value
+
+def nvmlDeviceGetMigMode(device):
+    c_currentMode = c_uint()
+    c_pendingMode = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMigMode")
+    ret = fn(device, byref(c_currentMode), byref(c_pendingMode))
+    _nvmlCheckReturn(ret)
+    return [c_currentMode.value, c_pendingMode.value]
+
+def nvmlDeviceGetGpuInstanceProfileInfo(device, profile, version=2):
+    if version == 2:
+        c_info = c_nvmlGpuInstanceProfileInfo_v2_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceProfileInfoV")
+    elif version == 1:
+        c_info = c_nvmlGpuInstanceProfileInfo_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceProfileInfo")
+    else:
+        raise NVMLError(MXSMLEX_ERROR_FUNCTION_NOT_FOUND)
+    ret = fn(device, profile, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+# Define function alias for the API exposed by MXSMLEX
+nvmlDeviceGetGpuInstanceProfileInfoV = nvmlDeviceGetGpuInstanceProfileInfo
+
+def nvmlDeviceGetGpuInstanceRemainingCapacity(device, profileId):
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceRemainingCapacity")
+    ret = fn(device, profileId, byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetGpuInstancePossiblePlacements(device, profileId, placementsRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstancePossiblePlacements_v2")
+    ret = fn(device, profileId, placementsRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceCreateGpuInstance(device, profileId):
+    c_instance = c_nvmlGpuInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceCreateGpuInstance")
+    ret = fn(device, profileId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlDeviceCreateGpuInstanceWithPlacement(device, profileId, placement):
+    c_instance = c_nvmlGpuInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceCreateGpuInstanceWithPlacement")
+    ret = fn(device, profileId, placement, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlGpuInstanceDestroy(gpuInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceDestroy")
+    ret = fn(gpuInstance)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpuInstances(device, profileId, gpuInstancesRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstances")
+    ret = fn(device, profileId, gpuInstancesRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpuInstanceById(device, gpuInstanceId):
+    c_instance = c_nvmlGpuInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceById")
+    ret = fn(device, gpuInstanceId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlGpuInstanceGetInfo(gpuInstance):
+    c_info = c_nvmlGpuInstanceInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetInfo")
+    ret = fn(gpuInstance, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlGpuInstanceGetComputeInstanceProfileInfo(device, profile, engProfile, version=2):
+    if version == 2:
+        c_info = c_nvmlComputeInstanceProfileInfo_v2_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceProfileInfoV")
+    elif version == 1:
+        c_info = c_nvmlComputeInstanceProfileInfo_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceProfileInfo")
+    else:
+        raise NVMLError(MXSMLEX_ERROR_FUNCTION_NOT_FOUND) 
+    ret = fn(device, profile, engProfile, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+# Define function alias for the API exposed by MXSMLEX
+nvmlGpuInstanceGetComputeInstanceProfileInfoV = nvmlGpuInstanceGetComputeInstanceProfileInfo
+
+def nvmlGpuInstanceGetComputeInstanceRemainingCapacity(gpuInstance, profileId):
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceRemainingCapacity")
+    ret = fn(gpuInstance, profileId, byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlGpuInstanceGetComputeInstancePossiblePlacements(gpuInstance, profileId, placementsRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstancePossiblePlacements")
+    ret = fn(gpuInstance, profileId, placementsRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpuInstanceCreateComputeInstance(gpuInstance, profileId):
+    c_instance = c_nvmlComputeInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceCreateComputeInstance")
+    ret = fn(gpuInstance, profileId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlGpuInstanceCreateComputeInstanceWithPlacement(gpuInstance, profileId, placement):
+    c_instance = c_nvmlComputeInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceCreateComputeInstanceWithPlacement")
+    ret = fn(gpuInstance, profileId, placement, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlComputeInstanceDestroy(computeInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExComputeInstanceDestroy")
+    ret = fn(computeInstance)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpuInstanceGetComputeInstances(gpuInstance, profileId, computeInstancesRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstances")
+    ret = fn(gpuInstance, profileId, computeInstancesRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpuInstanceGetComputeInstanceById(gpuInstance, computeInstanceId):
+    c_instance = c_nvmlComputeInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceById")
+    ret = fn(gpuInstance, computeInstanceId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlComputeInstanceGetInfo_v2(computeInstance):
+    c_info = c_nvmlComputeInstanceInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExComputeInstanceGetInfo_v2")
+    ret = fn(computeInstance, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlComputeInstanceGetInfo(computeInstance):
+    return nvmlComputeInstanceGetInfo_v2(computeInstance)
+
+def nvmlDeviceIsMigDeviceHandle(device):
+    c_isMigDevice = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceIsMigDeviceHandle")
+    ret = fn(device, byref(c_isMigDevice))
+    _nvmlCheckReturn(ret)
+    return c_isMigDevice
+
+def nvmlDeviceGetGpuInstanceId(device):
+    c_gpuInstanceId = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceId")
+    ret = fn(device, byref(c_gpuInstanceId))
+    _nvmlCheckReturn(ret)
+    return c_gpuInstanceId.value
+
+def nvmlDeviceGetComputeInstanceId(device):
+    c_computeInstanceId = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeInstanceId")
+    ret = fn(device, byref(c_computeInstanceId))
+    _nvmlCheckReturn(ret)
+    return c_computeInstanceId.value
+
+def nvmlDeviceGetMaxMigDeviceCount(device):
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxMigDeviceCount")
+    ret = fn(device, byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetMigDeviceHandleByIndex(device, index):
+    c_index = c_uint(index)
+    migDevice = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMigDeviceHandleByIndex")
+    ret = fn(device, c_index, byref(migDevice))
+    _nvmlCheckReturn(ret)
+    return migDevice
+
+def nvmlDeviceGetDeviceHandleFromMigDeviceHandle(migDevice):
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDeviceHandleFromMigDeviceHandle")
+    ret = fn(migDevice, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+def nvmlDeviceGetAttributes_v2(device):
+    c_attrs = c_nvmlDeviceAttributes()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAttributes_v2")
+    ret = fn(device, byref(c_attrs))
+    _nvmlCheckReturn(ret)
+    return c_attrs
+
+def nvmlDeviceGetAttributes(device):
+    return nvmlDeviceGetAttributes_v2(device)
+
+def nvmlDeviceGetRemappedRows(device):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRemappedRows")
+    c_corr = c_uint()
+    c_unc = c_uint()
+    c_bpending = c_uint()
+    c_bfailure = c_uint()
+    ret = fn(device, byref(c_corr), byref(c_unc), byref(c_bpending), byref(c_bfailure))
+    _nvmlCheckReturn(ret)
+    return (c_corr.value, c_unc.value, c_bpending.value, c_bfailure.value)
+
+def nvmlDeviceGetRowRemapperHistogram(device):
+    c_vals = c_nvmlRowRemapperHistogramValues()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRowRemapperHistogram")
+    ret = fn(device, byref(c_vals))
+    _nvmlCheckReturn(ret)
+    return c_vals
+
+def nvmlDeviceGetArchitecture(device):
+    arch = _nvmlDeviceArchitecture_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetArchitecture")
+    ret = fn(device, byref(arch))
+    _nvmlCheckReturn(ret)
+    return arch.value
+
+def nvmlDeviceGetBusType(device):
+    c_busType = _nvmlBusType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBusType")
+    ret = fn(device, byref(c_busType))
+    _nvmlCheckReturn(ret)
+    return c_busType.value
+
+def nvmlDeviceGetIrqNum(device):
+    c_irqNum = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetIrqNum")
+    ret = fn(device, byref(c_irqNum))
+    _nvmlCheckReturn(ret)
+    return c_irqNum.value
+
+def nvmlDeviceGetNumGpuCores(device):
+    c_numCores = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNumGpuCores")
+    ret = fn(device, byref(c_numCores))
+    _nvmlCheckReturn(ret)
+    return c_numCores.value
+
+def nvmlDeviceGetPowerSource(device):
+    c_powerSource = _nvmlPowerSource_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerSource")
+    ret = fn(device, byref(c_powerSource))
+    _nvmlCheckReturn(ret)
+    return c_powerSource.value
+
+def nvmlDeviceGetMemoryBusWidth(device):
+    c_memBusWidth = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryBusWidth")
+    ret = fn(device, byref(c_memBusWidth))
+    _nvmlCheckReturn(ret)
+    return c_memBusWidth.value
+
+def nvmlDeviceGetPcieLinkMaxSpeed(device):
+    c_speed = _nvmlPcieLinkMaxSpeed_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieLinkMaxSpeed")
+    ret = fn(device, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetAdaptiveClockInfoStatus(device):
+    c_adaptiveClockInfoStatus = _nvmlAdaptiveClockInfoStatus_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAdaptiveClockInfoStatus")
+    ret = fn(device, byref(c_adaptiveClockInfoStatus))
+    _nvmlCheckReturn(ret)
+    return c_adaptiveClockInfoStatus.value
+
+def nvmlDeviceGetPcieSpeed(device):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieSpeed")
+    ret = fn(device, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetDynamicPstatesInfo(device, c_dynamicpstatesinfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDynamicPstatesInfo");
+    ret = fn(device, c_dynamicpstatesinfo)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceSetFanSpeed_v2(handle, index, speed):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetFanSpeed_v2");
+    ret = fn(handle, index, speed)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetThermalSettings(device, sensorindex, c_thermalsettings):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetThermalSettings");
+    ret = fn(device, sensorindex, c_thermalsettings)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMinMaxClockOfPState(device, type, pstate, minClockMHz, maxClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMinMaxClockOfPState");
+    ret = fn(device, _nvmlClockType_t(type), _nvmlClockType_t(pstate), minClockMHz, maxClockMHz)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetSupportedPerformanceStates(device):
+    pstates = []
+    c_count = c_uint(MXSMLEX_MAX_GPU_PERF_PSTATES)
+    c_size = sizeof(c_uint)*c_count.value
+
+    # NOTE: use 'c_uint' to represent the size of the nvmlPstate_t enumeration.
+    pstates_array = _nvmlPstates_t * c_count.value
+    c_pstates = pstates_array()
+
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedPerformanceStates")
+    ret = fn(device, c_pstates, c_size)
+    _nvmlCheckReturn(ret)
+
+    for value in c_pstates:
+        if value != MXSMLEX_PSTATE_UNKNOWN:
+            pstates.append(value)
+
+    return pstates
+
+def nvmlDeviceGetGpcClkVfOffset(device):
+    offset = c_int32()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpcClkVfOffset")
+    ret = fn(device, byref(offset))
+    _nvmlCheckReturn(ret)
+    return offset.value
+
+def nvmlDeviceSetGpcClkVfOffset(device, offset):
+    c_offset = c_int32(offset)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetGpcClkVfOffset")
+    ret = fn(device, c_offset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpcClkMinMaxVfOffset(device, minOffset, maxOffset):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpcClkMinMaxVfOffset")
+    ret = fn(device, minOffset, maxOffset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMemClkVfOffset(device):
+    offset = c_int32()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemClkVfOffset")
+    ret = fn(device, byref(offset))
+    _nvmlCheckReturn(ret)
+    return offset.value
+
+def nvmlDeviceSetMemClkVfOffset(device, offset):
+    c_offset = c_int32(offset)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetMemClkVfOffset")
+    ret = fn(device, c_offset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMemClkMinMaxVfOffset(device, minOffset, maxOffset):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemClkMinMaxVfOffset")
+    ret = fn(device, minOffset, maxOffset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemSetConfComputeGpusReadyState(state):
+    c_state = c_uint(state)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetConfComputeGpusReadyState")
+    ret = fn(c_state)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemGetConfComputeGpusReadyState():
+    c_state = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeGpusReadyState")
+    ret = fn(byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state.value
+
+def nvmlSystemGetConfComputeCapabilities():
+    c_ccSysCaps = c_nvmlConfComputeSystemCaps_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeCapabilities")
+    ret = fn(byref(c_ccSysCaps))
+    _nvmlCheckReturn(ret)
+    return c_ccSysCaps
+
+def nvmlSystemGetConfComputeState():
+    c_state = c_nvmlConfComputeSystemState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeState")
+    ret = fn(byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state
+
+def nvmlSystemGetConfComputeSettings(settings):
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeSettings")
+    return fn(settings)
+
+def nvmlDeviceSetConfComputeUnprotectedMemSize(device, c_ccMemSize):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetConfComputeUnprotectedMemSize")
+    ret = fn(device, c_ccMemSize)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetConfComputeMemSizeInfo(device):
+    c_ccMemSize = c_nvmlConfComputeMemSizeInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeMemSizeInfo")
+    ret = fn(device, byref(c_ccMemSize))
+    _nvmlCheckReturn(ret)
+    return c_ccMemSize
+
+def nvmlDeviceGetConfComputeProtectedMemoryUsage(device):
+    c_memory = c_nvmlMemory_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeProtectedMemoryUsage")
+    ret = fn(device, byref(c_memory))
+    _nvmlCheckReturn(ret)
+    return c_memory
+
+def nvmlDeviceGetConfComputeGpuCertificate(device):
+    c_cert = c_nvmlConfComputeGpuCertificate_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeGpuCertificate")
+    ret = fn(device, byref(c_cert))
+    _nvmlCheckReturn(ret)
+    return c_cert
+
+def nvmlDeviceGetConfComputeGpuAttestationReport(device, c_nonce):
+    c_attestReport = c_nvmlConfComputeGpuAttestationReport_t()
+    c_nonce_arr = (c_uint8 * len(c_nonce))(*(c_nonce))
+    setattr(c_attestReport, 'nonce', c_nonce_arr)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeGpuAttestationReport")
+    ret = fn(device, byref(c_attestReport))
+    _nvmlCheckReturn(ret)
+    return c_attestReport
+
+def nvmlSystemSetConfComputeKeyRotationThresholdInfo(max_atk_adv):
+    c_keyRotationThrInfo = c_nvmlConfComputeSetKeyRotationThresholdInfo_t(0)
+    c_keyRotationThrInfo.version = ConfComputeSetKeyRotationThresholdInfo_v1
+    c_keyRotationThrInfo.maxAttackerAdvantage = max_atk_adv
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetConfComputeKeyRotationThresholdInfo")
+    ret = fn(byref(c_keyRotationThrInfo))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemGetConfComputeKeyRotationThresholdInfo():
+    c_keyRotationThrInfo = c_nvmlConfComputeGetKeyRotationThresholdInfo_t(0)
+    c_keyRotationThrInfo.version = ConfComputeGetKeyRotationThresholdInfo_v1
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeKeyRotationThresholdInfo")
+    ret = fn(byref(c_keyRotationThrInfo))
+    _nvmlCheckReturn(ret)
+    return c_keyRotationThrInfo
+
+## GPM ##
+#########
+
+## Enums/defines
+
+#### GPM Metric Identifiers
+MXSMLEX_GPM_METRIC_GRAPHICS_UTIL           = 1 # Percentage of time any compute/graphics app was active on the GPU. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_SM_UTIL                 = 2 # Percentage of SMs that were busy. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_SM_OCCUPANCY            = 3 # Percentage of warps that were active vs theoretical maximum. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_INTEGER_UTIL            = 4 # Percentage of time the GPU's SMs were doing integer operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_ANY_TENSOR_UTIL         = 5 # Percentage of time the GPU's SMs were doing ANY tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_DFMA_TENSOR_UTIL        = 6 # Percentage of time the GPU's SMs were doing DFMA tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_HMMA_TENSOR_UTIL        = 7 # Percentage of time the GPU's SMs were doing HMMA tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_IMMA_TENSOR_UTIL        = 9 # Percentage of time the GPU's SMs were doing IMMA tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_DRAM_BW_UTIL            = 10 # Percentage of DRAM bw used vs theoretical maximum. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_FP64_UTIL               = 11 # Percentage of time the GPU's SMs were doing non-tensor FP64 math. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_FP32_UTIL               = 12 # Percentage of time the GPU's SMs were doing non-tensor FP32 math. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_FP16_UTIL               = 13 # Percentage of time the GPU's SMs were doing non-tensor FP16 math. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_PCIE_TX_PER_SEC         = 20 # PCIe traffic from this GPU in MiB/sec
+MXSMLEX_GPM_METRIC_PCIE_RX_PER_SEC         = 21 # PCIe traffic to this GPU in MiB/sec
+MXSMLEX_GPM_METRIC_NVDEC_0_UTIL            = 30 # Percent utilization of NVDEC 0. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_1_UTIL            = 31 # Percent utilization of NVDEC 1. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_2_UTIL            = 32 # Percent utilization of NVDEC 2. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_3_UTIL            = 33 # Percent utilization of NVDEC 3. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_4_UTIL            = 34 # Percent utilization of NVDEC 4. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_5_UTIL            = 35 # Percent utilization of NVDEC 5. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_6_UTIL            = 36 # Percent utilization of NVDEC 6. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_7_UTIL            = 37 # Percent utilization of NVDEC 7. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_0_UTIL            = 40 # Percent utilization of NVJPG 0. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_1_UTIL            = 41 # Percent utilization of NVJPG 1. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_2_UTIL            = 42 # Percent utilization of NVJPG 2. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_3_UTIL            = 43 # Percent utilization of NVJPG 3. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_4_UTIL            = 44 # Percent utilization of NVJPG 4. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_5_UTIL            = 45 # Percent utilization of NVJPG 5. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_6_UTIL            = 46 # Percent utilization of NVJPG 6. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_7_UTIL            = 47 # Percent utilization of NVJPG 7. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVOFA_0_UTIL            = 50 # Percent utilization of NVOFA 0. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVLINK_TOTAL_RX_PER_SEC = 60 # NvLink read bandwidth for all links in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_TOTAL_TX_PER_SEC = 61 # NvLink write bandwidth for all links in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L0_RX_PER_SEC    = 62 # NvLink read bandwidth for link 0 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L0_TX_PER_SEC    = 63 # NvLink write bandwidth for link 0 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L1_RX_PER_SEC    = 64 # NvLink read bandwidth for link 1 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L1_TX_PER_SEC    = 65 # NvLink write bandwidth for link 1 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L2_RX_PER_SEC    = 66 # NvLink read bandwidth for link 2 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L2_TX_PER_SEC    = 67 # NvLink write bandwidth for link 2 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L3_RX_PER_SEC    = 68 # NvLink read bandwidth for link 3 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L3_TX_PER_SEC    = 69 # NvLink write bandwidth for link 3 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L4_RX_PER_SEC    = 70 # NvLink read bandwidth for link 4 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L4_TX_PER_SEC    = 71 # NvLink write bandwidth for link 4 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L5_RX_PER_SEC    = 72 # NvLink read bandwidth for link 5 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L5_TX_PER_SEC    = 73 # NvLink write bandwidth for link 5 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L6_RX_PER_SEC    = 74 # NvLink read bandwidth for link 6 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L6_TX_PER_SEC    = 75 # NvLink write bandwidth for link 6 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L7_RX_PER_SEC    = 76 # NvLink read bandwidth for link 7 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L7_TX_PER_SEC    = 77 # NvLink write bandwidth for link 7 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L8_RX_PER_SEC    = 78 # NvLink read bandwidth for link 8 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L8_TX_PER_SEC    = 79 # NvLink write bandwidth for link 8 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L9_RX_PER_SEC    = 80 # NvLink read bandwidth for link 9 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L9_TX_PER_SEC    = 81 # NvLink write bandwidth for link 9 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L10_RX_PER_SEC   = 82 # NvLink read bandwidth for link 10 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L10_TX_PER_SEC   = 83 # NvLink write bandwidth for link 10 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L11_RX_PER_SEC   = 84 # NvLink read bandwidth for link 11 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L11_TX_PER_SEC   = 85 # NvLink write bandwidth for link 11 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L12_RX_PER_SEC   = 86 # NvLink read bandwidth for link 12 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L12_TX_PER_SEC   = 87 # NvLink write bandwidth for link 12 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L13_RX_PER_SEC   = 88 # NvLink read bandwidth for link 13 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L13_TX_PER_SEC   = 89 # NvLink write bandwidth for link 13 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L14_RX_PER_SEC   = 90 # NvLink read bandwidth for link 14 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L14_TX_PER_SEC   = 91 # NvLink write bandwidth for link 14 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L15_RX_PER_SEC   = 92 # NvLink read bandwidth for link 15 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L15_TX_PER_SEC   = 93 # NvLink write bandwidth for link 15 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L16_RX_PER_SEC   = 94 # NvLink read bandwidth for link 16 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L16_TX_PER_SEC   = 95 # NvLink write bandwidth for link 16 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L17_RX_PER_SEC   = 96 # NvLink read bandwidth for link 17 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L17_TX_PER_SEC   = 97 # NvLink write bandwidth for link 17 in MiB/sec
+MXSMLEX_GPM_METRIC_MAX                     = 98
+
+## Structs
+
+class c_nvmlUnitInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('name', c_char * 96),
+        ('id', c_char * 96),
+        ('serial', c_char * 96),
+        ('firmwareVersion', c_char * 96),
+    ]
+
+class struct_c_nvmlGpmSample_t(Structure):
+    pass # opaque handle
+c_nvmlGpmSample_t = POINTER(struct_c_nvmlGpmSample_t)
+
+class c_metricInfo_t(Structure):
+    _fields_ = [
+        ("shortName", c_char_p),
+        ("longName", c_char_p),
+        ("unit", c_char_p),
+    ]
+
+class c_nvmlGpmMetric_t(_PrintableStructure):
+    _fields_ = [
+        ('metricId', c_uint),
+        ('nvmlReturn', _nvmlReturn_t),
+        ('value', c_double),
+        ('metricInfo', c_metricInfo_t)
+    ]
+
+class c_nvmlGpmMetricsGet_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('numMetrics', c_uint),
+        ('sample1', c_nvmlGpmSample_t),
+        ('sample2', c_nvmlGpmSample_t),
+        ('metrics', c_nvmlGpmMetric_t * MXSMLEX_GPM_METRIC_MAX)
+    ]
+
+MXSMLEX_GPM_METRICS_GET_VERSION = 1
+
+class c_nvmlGpmSupport_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('isSupportedDevice', c_uint),
+    ]
+
+MXSMLEX_GPM_SUPPORT_VERSION = 1
+
+## Functions
+
+def nvmlGpmMetricsGet(metricsGet):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmMetricsGet")
+    ret = fn(byref(metricsGet))
+    _nvmlCheckReturn(ret)
+    return metricsGet
+
+def nvmlGpmSampleFree(gpmSample):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSampleFree")
+    ret = fn(gpmSample)
+    _nvmlCheckReturn(ret)
+    return
+
+def nvmlGpmSampleAlloc():
+    gpmSample = c_nvmlGpmSample_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSampleAlloc")
+    ret = fn(byref(gpmSample))
+    _nvmlCheckReturn(ret)
+    return gpmSample
+
+def nvmlGpmSampleGet(device, gpmSample):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSampleGet")
+    ret = fn(device, gpmSample)
+    _nvmlCheckReturn(ret)
+    return gpmSample
+
+def nvmlGpmMigSampleGet(device, gpuInstanceId, gpmSample):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmMigSampleGet")
+    ret = fn(device, gpuInstanceId, gpmSample)
+    _nvmlCheckReturn(ret)
+    return gpmSample
+
+def nvmlGpmQueryDeviceSupport(device):
+    gpmSupport = c_nvmlGpmSupport_t()
+    gpmSupport.version = MXSMLEX_GPM_SUPPORT_VERSION
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmQueryDeviceSupport")
+    ret = fn(device, byref(gpmSupport))
+    _nvmlCheckReturn(ret)
+    return gpmSupport
+
+def nvmlGpmSetStreamingEnabled(device, state):
+    c_state = c_uint(state)
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSetStreamingEnabled")
+    ret = fn(device, c_state)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpmQueryIfStreamingEnabled(device):
+    c_state = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmQueryIfStreamingEnabled")
+    ret = fn(device, byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state.value
+
+# Low Power Structure and Function
+
+MXSMLEX_NVLINK_POWER_STATE_HIGH_SPEED    = 0x0
+MXSMLEX_NVLINK_POWER_STATE_LOW           = 0x1
+
+MXSMLEX_NVLINK_LOW_POWER_THRESHOLD_MIN   = 0x1
+MXSMLEX_NVLINK_LOW_POWER_THRESHOLD_MAX   = 0x1FFF
+MXSMLEX_NVLINK_LOW_POWER_THRESHOLD_RESET = 0xFFFFFFFF
+
+class c_nvmlNvLinkPowerThres_t(Structure):
+    _fields_ = [
+        ("lowPwrThreshold", c_uint),
+    ]
+
+def nvmlDeviceSetNvLinkDeviceLowPowerThreshold(device, l1threshold):
+    c_info = c_nvmlNvLinkPowerThres_t()
+    c_info.lowPwrThreshold = l1threshold
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetNvLinkDeviceLowPowerThreshold")
+    ret = fn(device, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return ret 
+
+_nvmlGpuFabricState_t = c_uint
+MXSMLEX_GPU_FABRIC_STATE_NOT_SUPPORTED = 0
+MXSMLEX_GPU_FABRIC_STATE_NOT_STARTED   = 1
+MXSMLEX_GPU_FABRIC_STATE_IN_PROGRESS   = 2
+MXSMLEX_GPU_FABRIC_STATE_COMPLETED     = 3
+
+class c_nvmlGpuFabricInfo_t(_PrintableStructure):
+    _fields_ = [
+        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
+        ("status", _nvmlReturn_t),
+        ("cliqueId", c_uint32),
+        ("state", _nvmlGpuFabricState_t)
+    ]
+
+nvmlGpuFabricInfo_v2 = 0x02000024
+
+class c_nvmlGpuFabricInfoV_t(_PrintableStructure):
+    _fields_ = [
+        ("version", c_uint),
+        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
+        ("status", _nvmlReturn_t),
+        ("cliqueId", c_uint32),
+        ("state", _nvmlGpuFabricState_t),
+        ("healthMask", c_uint32)
+    ]
+
+    def __init__(self):
+        super(c_nvmlGpuFabricInfoV_t, self).__init__(version=nvmlGpuFabricInfo_v2)
+
+def nvmlDeviceGetGpuFabricInfo(device, gpuFabricInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfo");
+    ret = fn(device, gpuFabricInfo)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpuFabricInfoV(device, gpuFabricInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfoV");
+    ret = fn(device, gpuFabricInfo)
+    _nvmlCheckReturn(ret)
+    return ret
+
+######################
+## Enums/defines
+#### MXSMLEX GPU NVLINK BW MODE
+MXSMLEX_GPU_NVLINK_BW_MODE_FULL      = 0x0
+MXSMLEX_GPU_NVLINK_BW_MODE_OFF       = 0x1
+MXSMLEX_GPU_NVLINK_BW_MODE_MIN       = 0x2
+MXSMLEX_GPU_NVLINK_BW_MODE_HALF      = 0x3
+MXSMLEX_GPU_NVLINK_BW_MODE_3QUARTER  = 0x4
+MXSMLEX_GPU_NVLINK_BW_MODE_COUNT     = 0x5
+
+def nvmlSystemSetNvlinkBwMode(mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetNvlinkBwMode")
+    ret = fn(mode)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemGetNvlinkBwMode():
+    mode = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetNvlinkBwMode")
+    ret = fn(byref(mode))
+    _nvmlCheckReturn(ret)
+    return mode.value
+
+_nvmlPowerScopeType_t = c_uint
+MXSMLEX_POWER_SCOPE_GPU     = 0
+MXSMLEX_POWER_SCOPE_MODULE  = 1
+MXSMLEX_POWER_SCOPE_MEMORY  = 2
+
+class c_nvmlPowerValue_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('powerScope', _nvmlPowerScopeType_t),
+        ('powerValueMw', c_uint),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+nvmlPowerValue_v2 = 0x0200000C
+
+def nvmlDeviceSetPowerManagementLimit_v2(device, powerScope, powerLimit, version=nvmlPowerValue_v2):
+    c_powerScope = _nvmlPowerScopeType_t(powerScope)
+    c_powerValue = c_nvmlPowerValue_v2_t()
+    c_powerValue.version = c_uint(version)
+    c_powerValue.powerScope = c_powerScope
+    c_powerValue.powerValueMw = c_uint(powerLimit)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPowerManagementLimit_v2")
+    ret = fn(device, byref(c_powerValue))
+    return ret
+
+class c_nvmlEccSramErrorStatus_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('aggregateUncParity', c_ulonglong),
+        ('aggregateUncSecDed', c_ulonglong),
+        ('aggregateCor', c_ulonglong),
+        ('volatileUncParity', c_ulonglong),
+        ('volatileUncSecDed', c_ulonglong),
+        ('volatileCor', c_ulonglong),
+        ('aggregateUncBucketL2', c_ulonglong),
+        ('aggregateUncBucketSm', c_ulonglong),
+        ('aggregateUncBucketPcie', c_ulonglong),
+        ('aggregateUncBucketMcu', c_ulonglong),
+        ('aggregateUncBucketOther', c_ulonglong),
+        ('bThresholdExceeded', c_uint)
+    ]
+
+    def __init__(self):
+        super(c_nvmlEccSramErrorStatus_v1_t, self).__init__(version=nvmlEccSramErrorStatus_v1)
+
+nvmlEccSramErrorStatus_v1 = 0x1000068
+
+def nvmlDeviceGetSramEccErrorStatus(device, status):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSramEccErrorStatus")
+    ret = fn(device, status)
+    _nvmlCheckReturn(ret)
+    return ret
+
diff --git a/vllm/triton_utils/custom_cache_manager.py b/vllm/triton_utils/custom_cache_manager.py
index 4163969c9..df44adda0 100644
--- a/vllm/triton_utils/custom_cache_manager.py
+++ b/vllm/triton_utils/custom_cache_manager.py
@@ -1,9 +1,11 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import os
 
-from triton.runtime.cache import (FileCacheManager, default_cache_dir,
-                                  default_dump_dir, default_override_dir)
+#from triton.runtime.cache import (FileCacheManager, default_cache_dir,
+#                                  default_dump_dir, default_override_dir)
+from triton.runtime.cache import FileCacheManager, default_cache_dir
 
 from vllm.logger import init_logger
 
diff --git a/vllm/utils.py b/vllm/utils.py
index 8b9269598..676ad5ff5 100644
--- a/vllm/utils.py
+++ b/vllm/utils.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import argparse
@@ -151,6 +152,7 @@ STR_DTYPE_TO_TORCH_DTYPE = {
     "fp8": torch.uint8,
     "fp8_e4m3": torch.uint8,
     "fp8_e5m2": torch.uint8,
+    "int8": torch.int8,
 }
 
 TORCH_DTYPE_TO_NUMPY_DTYPE = {
@@ -658,6 +660,7 @@ def create_kv_caches_with_random(
     model_dtype: Optional[Union[str, torch.dtype]] = None,
     seed: int = 0,
     device: Optional[str] = "cuda",
+    new_layerout:Optional[bool] = False,
 ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
 
     if cache_dtype == "fp8" and head_size % 16:
@@ -685,6 +688,11 @@ def create_kv_caches_with_random(
             raise ValueError(
                 f"Does not support key cache of type {cache_dtype}")
         key_caches.append(key_cache)
+        if new_layerout:
+            key_cache_new = torch.empty(size=key_cache_shape,
+                                        dtype=torch_dtype,
+                                        device=device)
+            key_caches.append(key_cache_new)
 
     value_cache_shape = (num_blocks, num_heads, head_size, block_size)
     value_caches: List[torch.Tensor] = []
@@ -700,6 +708,12 @@ def create_kv_caches_with_random(
             raise ValueError(
                 f"Does not support value cache of type {cache_dtype}")
         value_caches.append(value_cache)
+        if new_layerout:
+            value_cache_new = torch.empty(size=value_cache_shape,
+                                    dtype=torch_dtype,
+                                    device=device)
+            value_caches.append(value_cache_new)
+
     return key_caches, value_caches
 
 
@@ -942,7 +956,7 @@ def find_nccl_library() -> str:
             so_file)
     else:
         if torch.version.cuda is not None:
-            so_file = "libnccl.so.2"
+            so_file = "libmccl.so"
         elif torch.version.hip is not None:
             so_file = "librccl.so.1"
         else:
@@ -1875,12 +1889,12 @@ def direct_register_custom_op(
 
     if not supports_custom_op():
         from vllm.platforms import current_platform
-        assert not current_platform.is_cuda_alike(), (
-            "cuda platform needs torch>=2.4 to support custom op, "
-            "chances are you are using an old version of pytorch "
-            "or a custom build of pytorch. It is recommended to "
-            "use vLLM in a fresh new environment and let it install "
-            "the required dependencies.")
+        #assert not current_platform.is_cuda_alike(), (
+        #    "cuda platform needs torch>=2.4 to support custom op, "
+        #    "chances are you are using an old version of pytorch "
+        #    "or a custom build of pytorch. It is recommended to "
+        #    "use vLLM in a fresh new environment and let it install "
+        #    "the required dependencies.")
         return
 
     import torch.library
@@ -2246,7 +2260,8 @@ def import_pynvml():
     `pynvml.py` module from the `nvidia-ml-py` package.
     """
     if TYPE_CHECKING:
-        import pynvml
+        #import pynvml
+        import vllm.platforms.pynvml as pynvml
         return pynvml
     if "pynvml" in sys.modules:
         import pynvml
diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
old mode 100755
new mode 100644
index 837d7faf4..4bcb6a3dc
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 """Attention layer with FlashAttention."""
 from dataclasses import dataclass
@@ -14,9 +15,11 @@ from vllm.envs import VLLM_FLASH_ATTN_VERSION
 from vllm.logger import init_logger
 from vllm.platforms import current_platform
 from vllm.utils import cdiv
-from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
-                                  flash_attn_varlen_func,
-                                  is_fa_version_supported)
+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
+#                                  flash_attn_varlen_func,
+#                                  is_fa_version_supported)
+from flash_attn import (flash_attn_varlen_func,
+                        flash_attn_with_kvcache)
 
 logger = init_logger(__name__)
 
@@ -27,7 +30,7 @@ class FlashAttentionBackend(AttentionBackend):
 
     @staticmethod
     def get_supported_head_sizes() -> List[int]:
-        return [32, 64, 96, 128, 160, 192, 224, 256]
+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
 
     @staticmethod
     def get_name() -> str:
@@ -139,6 +142,7 @@ class FlashAttentionImpl(AttentionImpl):
         # if hopper default to FA3, otherwise stick to FA2 for now
         # TODO(lucas): profile FA3 on ampere to see if it makes sense to
         #  use FA3 as default for both
+        """
         if current_platform.get_device_capability()[0] >= 9:
             self.fa_version = 3 if is_fa_version_supported(3) else 2
         else:
@@ -154,6 +158,7 @@ class FlashAttentionImpl(AttentionImpl):
                          fa_version_unsupported_reason(self.fa_version))
 
         assert is_fa_version_supported(self.fa_version)
+        """
 
     def forward(
         self,
@@ -212,14 +217,18 @@ class FlashAttentionImpl(AttentionImpl):
         # Compute attention and update output up to `num_actual_tokens`.
         if not attn_metadata.use_cascade:
             # Regular attention (common case).
-            flash_attn_varlen_func(
+            #print("is ok")
+            cu_prefix_kv_lens = torch.tensor([0] + attn_metadata.seq_lens.tolist(), device='cuda:0',dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
+            #print(key_cache)
+            output[:num_actual_tokens] = flash_attn_varlen_func(
                 q=query[:num_actual_tokens],
                 k=key_cache,
                 v=value_cache,
-                out=output[:num_actual_tokens],
+                #out=output[:num_actual_tokens],
                 cu_seqlens_q=attn_metadata.query_start_loc,
                 max_seqlen_q=attn_metadata.max_query_len,
-                seqused_k=attn_metadata.seq_lens,
+                #seqused_k=attn_metadata.seq_lens,
+                cu_seqlens_k=cu_prefix_kv_lens,     #Maybe error
                 max_seqlen_k=attn_metadata.max_seq_len,
                 softmax_scale=self.scale,
                 causal=True,
@@ -227,7 +236,7 @@ class FlashAttentionImpl(AttentionImpl):
                 window_size=self.sliding_window,
                 block_table=attn_metadata.block_table,
                 softcap=self.logits_soft_cap,
-                fa_version=self.fa_version,
+                #fa_version=self.fa_version,
             )
             return output
 
@@ -249,7 +258,7 @@ class FlashAttentionImpl(AttentionImpl):
             logits_soft_cap=self.logits_soft_cap,
             block_table=attn_metadata.block_table,
             common_prefix_len=attn_metadata.common_prefix_len,
-            fa_version=self.fa_version,
+            #fa_version=self.fa_version,
         )
         return output
 
@@ -353,12 +362,15 @@ def cascade_attention(
     assert num_common_kv_blocks > 0
 
     # Process shared prefix.
-    prefix_output, prefix_lse = flash_attn_varlen_func(
+    #prefix_output, prefix_lse = flash_attn_varlen_func(
+    cu_prefix_kv_lens = torch.tensor([0] + prefix_kv_lens.tolist(), dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
+    prefix_output, prefix_lse, prefix_s_dmask = flash_attn_varlen_func(
         q=query,
         k=key_cache,
         v=value_cache,
         cu_seqlens_q=cu_prefix_query_lens,
-        seqused_k=prefix_kv_lens,
+        cu_seqlens_k=cu_prefix_kv_lens,
+        #seqused_k=prefix_kv_lens,
         max_seqlen_q=num_tokens,
         max_seqlen_k=common_prefix_len,
         softmax_scale=softmax_scale,
@@ -366,17 +378,21 @@ def cascade_attention(
         window_size=sliding_window,
         block_table=block_table[:1],
         softcap=logits_soft_cap,
-        return_softmax_lse=True,
-        fa_version=fa_version,
+        return_attn_probs=True,
+        #return_softmax_lse=True,
+        #fa_version=fa_version,
     )
 
     # Process suffix per query.
-    suffix_output, suffix_lse = flash_attn_varlen_func(
+    #suffix_output, suffix_lse = flash_attn_varlen_func(
+    cu_suffix_kv_lens = torch.tensor([0] + suffix_kv_lens.tolist(), dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
+    suffix_output, suffix_lse, suffix_s_dmask = flash_attn_varlen_func(
         q=query,
         k=key_cache,
         v=value_cache,
         cu_seqlens_q=cu_query_lens,
-        seqused_k=suffix_kv_lens,
+        cu_seqlens_k=cu_suffix_kv_lens,
+        #seqused_k=suffix_kv_lens,
         max_seqlen_q=max_query_len,
         max_seqlen_k=max_kv_len - common_prefix_len,
         softmax_scale=softmax_scale,
@@ -384,8 +400,9 @@ def cascade_attention(
         window_size=sliding_window,
         block_table=block_table[:, num_common_kv_blocks:],
         softcap=logits_soft_cap,
-        return_softmax_lse=True,
-        fa_version=fa_version,
+        return_attn_probs=True,
+        #return_softmax_lse=True,
+        #fa_version=fa_version,
     )
 
     # Merge prefix and suffix outputs, and store the result in output.
diff --git a/vllm/v1/core/scheduler.py b/vllm/v1/core/scheduler.py
index fb5e83fe0..e2981940b 100644
--- a/vllm/v1/core/scheduler.py
+++ b/vllm/v1/core/scheduler.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from collections import deque
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 29a9ac186..9fb13df28 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import pickle
diff --git a/vllm/v1/executor/abstract.py b/vllm/v1/executor/abstract.py
index ac10d43eb..6d6fd312a 100644
--- a/vllm/v1/executor/abstract.py
+++ b/vllm/v1/executor/abstract.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 from typing import Type
diff --git a/vllm/version.py b/vllm/version.py
index 70cd0289b..d5c02d75d 100644
--- a/vllm/version.py
+++ b/vllm/version.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 try:
@@ -5,9 +6,9 @@ try:
 except Exception as e:
     import warnings
 
-    warnings.warn(f"Failed to read commit hash:\n{e}",
-                  RuntimeWarning,
-                  stacklevel=2)
+    #warnings.warn(f"Failed to read commit hash:\n{e}",
+    #              RuntimeWarning,
+    #              stacklevel=2)
 
-    __version__ = "dev"
+    __version__ = "0.7.2"
     __version_tuple__ = (0, 0, __version__)
diff --git a/vllm/worker/cache_engine.py b/vllm/worker/cache_engine.py
index 3960392cf..32f1895a6 100644
--- a/vllm/worker/cache_engine.py
+++ b/vllm/worker/cache_engine.py
@@ -154,8 +154,7 @@ class CacheEngine:
 
         key_cache_entry = num_heads * head_size
         if CacheEngine._align_cache(model_config):
-            key_cache_entry = align_to_256bytes(key_cache_entry,
-                                                model_config.dtype)
+            key_cache_entry = align_to_256bytes(key_cache_entry, dtype)
 
         # For MLA there is no value cache, since the latent vector
         # is joint keys and values.
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 12baecde6..bb315094d 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import dataclasses
@@ -72,8 +73,21 @@ TModelInputForGPU = TypeVar('TModelInputForGPU', bound="ModelInputForGPU")
 
 # For now, bump up cache limits for recompilations during CUDA graph warmups.
 torch._dynamo.config.cache_size_limit = 128
-torch._dynamo.config.accumulated_cache_size_limit = 128
-
+#torch._dynamo.config.accumulated_cache_size_limit = 128
+
+# --- FLAGSCALE MODIFICATION BEG ---
+# Know more about FlagGems: https://github.com/FlagOpen/FlagGems
+import os
+if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
+    try:
+        import flag_gems
+        flag_gems.enable()
+        logger.info("Successfully enabled flag_gems as default ops implementation.")
+    except ImportError:
+        logger.warning("Failed to import 'flag_gems'. Falling back to default implementation.")
+    except Exception as e:
+        logger.warning(f"Failed to enable 'flag_gems': {e}. Falling back to default implementation.")
+# --- FLAGSCALE MODIFICATION END ---
 
 @dataclass(frozen=True)
 class ModelInputForGPU(ModelRunnerInputBase):
@@ -1699,7 +1713,8 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
                     # layers.
                     model_executable,
                     model_input,
-                    kv_caches=kv_caches
+                    kv_caches=kv_caches,
+                    block_size=self.block_size,
                 )
 
         multi_modal_kwargs = model_input.multi_modal_kwargs or {}
@@ -1732,16 +1747,16 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
 
         # Sending KV cache in distributed KV cache transfer setting
         # NOTE: the send operation is non-blocking
-        if self.need_send_kv(model_input, kv_caches):
-            get_kv_transfer_group().send_kv_caches_and_hidden_states(
-                # model_executable is used to know which layer the current
-                # worker is working on, so that we can send KV for only those
-                # layers.
-                model_executable,
-                model_input,
-                kv_caches,
-                hidden_or_intermediate_states,
-            )
+        #if self.need_send_kv(model_input, kv_caches):
+        #    get_kv_transfer_group().send_kv_caches_and_hidden_states(
+        #        # model_executable is used to know which layer the current
+        #        # worker is working on, so that we can send KV for only those
+        #        # layers.
+        #        model_executable,
+        #        model_input,
+        #        kv_caches,
+        #        hidden_or_intermediate_states,
+        #    )
 
         # Compute the logits in the last pipeline stage.
         if not get_pp_group().is_last_rank:
@@ -1809,6 +1824,179 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
             output.hidden_states = hidden_states
 
         return [output]
+    
+    @torch.inference_mode()
+    def execute_model_return_hidden(
+        self,
+        model_input: ModelInputForGPUWithSamplingMetadata,
+        kv_caches: List[torch.Tensor],
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        num_steps: int = 1,
+    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:
+        if num_steps > 1:
+            raise ValueError("num_steps > 1 is not supported in ModelRunner")
+
+        if self.lora_config:
+            assert model_input.lora_requests is not None
+            assert model_input.lora_mapping is not None
+            self.set_active_loras(model_input.lora_requests,
+                                  model_input.lora_mapping)
+
+        if self.prompt_adapter_config:
+            assert model_input.prompt_adapter_requests is not None
+            assert model_input.prompt_adapter_mapping is not None
+            self.set_active_prompt_adapters(
+                model_input.prompt_adapter_requests,
+                model_input.prompt_adapter_mapping)
+
+        self.attn_state.begin_forward(model_input)
+
+        # Currently cuda graph is only supported by the decode phase.
+        assert model_input.attn_metadata is not None
+        prefill_meta = model_input.attn_metadata.prefill_metadata
+        decode_meta = model_input.attn_metadata.decode_metadata
+        # TODO(andoorve): We can remove this once all
+        # virtual engines share the same kv cache.
+        virtual_engine = model_input.virtual_engine
+        if prefill_meta is None and decode_meta.use_cuda_graph:
+            assert model_input.input_tokens is not None
+            graph_batch_size = model_input.input_tokens.shape[0]
+            model_executable = self.graph_runners[virtual_engine][
+                graph_batch_size]
+        else:
+            model_executable = self.model
+
+        # Receive KV cache in distributed KV cache transfer setting
+        # In disagg prefill setting, it will also recv hidden states and bypass
+        # model forwarding
+        # In KV cache database setting, it will change the model input so that
+        # we can skip prefilling on tokens that successfully received KV caches
+        # NOTE: The receive operation is blocking
+        bypass_model_exec = False
+        if self.need_recv_kv(model_input, kv_caches):
+            hidden_or_intermediate_states, bypass_model_exec, model_input = \
+                get_kv_transfer_group().recv_kv_caches_and_hidden_states(
+                    # model is used to know which layer the current worker
+                    # is working on, so that we can receive KV for only those
+                    # layers.
+                    model_executable,
+                    model_input,
+                    kv_caches=kv_caches
+                )
+
+        multi_modal_kwargs = model_input.multi_modal_kwargs or {}
+        seqlen_agnostic_kwargs = {
+            "finished_requests_ids": model_input.finished_requests_ids,
+            "request_ids_to_seq_ids": model_input.request_ids_to_seq_ids,
+        } if self.has_inner_state else {}
+        if (self.observability_config is not None
+                and self.observability_config.collect_model_forward_time):
+            model_forward_start = torch.cuda.Event(enable_timing=True)
+            model_forward_end = torch.cuda.Event(enable_timing=True)
+            model_forward_start.record()
+
+        if not bypass_model_exec:
+            with set_forward_context(model_input.attn_metadata,
+                                     self.vllm_config):
+                hidden_or_intermediate_states = model_executable(
+                    input_ids=model_input.input_tokens,
+                    positions=model_input.input_positions,
+                    kv_caches=kv_caches,
+                    attn_metadata=model_input.attn_metadata,
+                    intermediate_tensors=intermediate_tensors,
+                    **MultiModalKwargs.as_kwargs(multi_modal_kwargs,
+                                                 device=self.device),
+                    **seqlen_agnostic_kwargs)
+
+        if (self.observability_config is not None
+                and self.observability_config.collect_model_forward_time):
+            model_forward_end.record()
+
+        # Sending KV cache in distributed KV cache transfer setting
+        # NOTE: the send operation is non-blocking
+        if self.need_send_kv(model_input, kv_caches):
+            get_kv_transfer_group().send_kv_caches_and_hidden_states(
+                # model_executable is used to know which layer the current
+                # worker is working on, so that we can send KV for only those
+                # layers.
+                model_executable,
+                model_input,
+                kv_caches,
+                hidden_or_intermediate_states,
+            )
+
+        # Compute the logits in the last pipeline stage.
+        if not get_pp_group().is_last_rank:
+            if (self.is_driver_worker
+                    and hidden_or_intermediate_states is not None
+                    and isinstance(hidden_or_intermediate_states,
+                                   IntermediateTensors)
+                    and self.observability_config is not None
+                    and self.observability_config.collect_model_forward_time):
+                model_forward_end.synchronize()
+                model_forward_time = model_forward_start.elapsed_time(
+                    model_forward_end)
+                orig_model_forward_time = 0.0
+                if intermediate_tensors is not None:
+                    orig_model_forward_time = intermediate_tensors.tensors.get(
+                        "model_forward_time", torch.tensor(0.0)).item()
+                hidden_or_intermediate_states.tensors["model_forward_time"] = (
+                    torch.tensor(model_forward_time + orig_model_forward_time))
+            return hidden_or_intermediate_states
+
+        logits = self.model.compute_logits(hidden_or_intermediate_states,
+                                           model_input.sampling_metadata)
+
+        if model_input.input_positions.flatten()[0] == 0:
+            logits_rt = self.model.compute_logits(hidden_or_intermediate_states, None)
+        else:
+            logits_rt = logits
+        
+        if not self.is_driver_worker:
+            return []
+
+        if model_input.async_callback is not None:
+            model_input.async_callback()
+
+        # Sample the next token.
+        output: SamplerOutput = self.model.sample(
+            logits=logits,
+            sampling_metadata=model_input.sampling_metadata,
+        )
+        if (self.observability_config is not None
+                and self.observability_config.collect_model_forward_time
+                and output is not None):
+            model_forward_end.synchronize()
+            model_forward_time = model_forward_start.elapsed_time(
+                model_forward_end)
+            orig_model_forward_time = 0.0
+            if intermediate_tensors is not None:
+                orig_model_forward_time = intermediate_tensors.tensors.get(
+                    "model_forward_time", torch.tensor(0.0)).item()
+            # If there are multiple workers, we are still tracking the latency
+            # from the start time of the driver worker to the end time of the
+            # driver worker. The model forward time will then end up covering
+            # the communication time as well.
+            output.model_forward_time = (orig_model_forward_time +
+                                         model_forward_time)
+
+        if self.return_hidden_states or True:
+            # we only need to pass hidden states of most recent token
+            # assert model_input.sampling_metadata is not None
+            # indices = model_input.sampling_metadata.selected_token_indices
+            # if model_input.is_prompt:
+            #     hidden_states = hidden_or_intermediate_states.index_select(
+            #         0, indices)
+            #     output.prefill_hidden_states = hidden_or_intermediate_states
+            # elif decode_meta.use_cuda_graph:
+            #     hidden_states = hidden_or_intermediate_states[:len(indices)]
+            # else:
+            #     hidden_states = hidden_or_intermediate_states
+
+            # output.hidden_states = hidden_states
+            output.hidden_states = logits_rt
+
+        return [output]
 
     def need_recv_kv(self, model_input, kv_caches) -> bool:
         """Check if we need to receive kv-cache from the other worker.
@@ -1858,8 +2046,8 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
         is_prefill_run = prefill_meta is not None
 
         return self.vllm_config.kv_transfer_config.is_kv_producer and (
-            not is_profile_run) and is_prefill_run
-
+            not is_profile_run) and is_prefill_run and (
+            not self.vllm_config.kv_transfer_config.is_layerwise_kv_transfer)
 
 # NOTE: this is nn.Module so the profiler can properly capture/group
 #  kernels calls made within the graph
diff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py
index 90771e8ac..4eaad8449 100644
--- a/vllm/worker/multi_step_model_runner.py
+++ b/vllm/worker/multi_step_model_runner.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import dataclasses
@@ -32,7 +33,7 @@ if TYPE_CHECKING:
 logger = init_logger(__name__)
 
 MULTI_STEP_ATTENTION_BACKENDS = [
-    "FLASH_ATTN", "ROCM_FLASH", "FLASHINFER", "NO_ATTENTION"
+    "FLASH_ATTN", "ROCM_FLASH", "FLASHINFER", "NO_ATTENTION", "TRITON_MLA"
 ]
 MULTI_STEP_CHUNKED_PREFILL_ATTENTION_BACKENDS = ["FLASH_ATTN", "FLASHINFER"]
 
diff --git a/vllm/worker/worker_base.py b/vllm/worker/worker_base.py
index 819b81fbf..d08e92966 100644
--- a/vllm/worker/worker_base.py
+++ b/vllm/worker/worker_base.py
@@ -1,3 +1,4 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 # SPDX-License-Identifier: Apache-2.0
 
 import dataclasses
@@ -82,6 +83,7 @@ class WorkerBase(ABC):
         """
         raise NotImplementedError
 
+
     def start_worker_execution_loop(self) -> None:
         """Execute model loop in parallel worker.
 
@@ -94,6 +96,18 @@ class WorkerBase(ABC):
                 if output is None:
                     return None
 
+
+    def start_worker_execution_loop_return_hidden(self) -> None:
+        """Execute model loop in parallel worker.
+
+        You can stop the loop by executing a driver worker with an empty output.
+        See `stop_remote_worker_execution_loop` for more details.
+        """
+        while True:
+            output = self.execute_model_return_hidden(execute_model_req=None)
+            if output is None:
+                return None
+    
     @abstractmethod
     def get_model(self) -> nn.Module:
         raise NotImplementedError
@@ -104,6 +118,13 @@ class WorkerBase(ABC):
         execute_model_req: Optional[ExecuteModelRequest] = None
     ) -> Optional[List[SamplerOutput]]:
         raise NotImplementedError
+    
+    @abstractmethod
+    def execute_model_return_hidden(
+        self,
+        execute_model_req: Optional[ExecuteModelRequest] = None
+    ) -> Optional[List[SamplerOutput]]:
+        raise NotImplementedError
 
     @abstractmethod
     def get_cache_block_size_bytes(self) -> int:
@@ -439,6 +460,67 @@ class LocalOrDistributedWorkerBase(WorkerBase):
 
         # output is List[SamplerOutput]
         return output
+    
+    def execute_model_return_hidden(
+        self,
+        execute_model_req: Optional[ExecuteModelRequest] = None,
+    ) -> Optional[List[SamplerOutput]]:
+        """Executes at least one model step on the given sequences, unless no
+        sequences are provided."""
+        start_time = time.perf_counter()
+
+        inputs = self.prepare_input(execute_model_req)
+        if inputs is None:
+            return None
+
+        model_input, worker_input, kwargs = inputs
+        num_steps = worker_input.num_steps
+
+        self.execute_worker(worker_input)
+
+        # If there is no input, we don't need to execute the model.
+        if worker_input.num_seq_groups == 0:
+            return []
+
+        intermediate_tensors = None
+        orig_model_execute_time = 0.0
+        if not get_pp_group().is_first_rank:
+            intermediate_tensors = IntermediateTensors(
+                get_pp_group().recv_tensor_dict(
+                    all_gather_group=get_tp_group()))
+            if (self.observability_config is not None
+                    and self.observability_config.collect_model_execute_time):
+                orig_model_execute_time = intermediate_tensors.tensors.get(
+                    "model_execute_time", torch.tensor(0)).item()
+
+        output = self.model_runner.execute_model_return_hidden(
+            model_input=model_input,
+            kv_caches=self.kv_cache[worker_input.virtual_engine]
+            if self.kv_cache is not None else None,
+            intermediate_tensors=intermediate_tensors,
+            num_steps=num_steps,
+            **kwargs,
+        )
+
+        model_execute_time = time.perf_counter() - start_time
+        if not get_pp_group().is_last_rank:
+            # output is IntermediateTensors
+            if (self.observability_config is not None
+                    and self.observability_config.collect_model_execute_time):
+                output.tensors["model_execute_time"] = torch.tensor(
+                    model_execute_time + orig_model_execute_time)
+            get_pp_group().send_tensor_dict(output.tensors,
+                                            all_gather_group=get_tp_group())
+            return [None]
+        if (self.observability_config is not None
+                and self.observability_config.collect_model_execute_time
+                and output is not None):
+            for o in output:
+                o.model_execute_time = (orig_model_execute_time +
+                                        model_execute_time)
+
+        # output is List[SamplerOutput]
+        return output
 
     def _execute_model_spmd(
         self,
