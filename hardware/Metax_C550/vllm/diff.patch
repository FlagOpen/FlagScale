diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3314f05fd..90b3dfabd 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -19,6 +19,32 @@ set(VLLM_TARGET_DEVICE "cuda" CACHE STRING "Target device backend for vLLM")
 message(STATUS "Build type: ${CMAKE_BUILD_TYPE}")
 message(STATUS "Target device: ${VLLM_TARGET_DEVICE}")
 
+option(USE_MACA "Enable MACA Support" ON)
+option(ENABLE_BLAS_API "Enable blas api for fused moe" ON)
+
+if (USE_MACA)
+  message(STATUS "###########################")
+  message(STATUS "Detected Using MACA")
+  message(STATUS "###########################")
+
+  add_compile_definitions(
+    MACA_VERSION_MAJOR=${MACA_VERSION_MAJOR}
+    MACA_VERSION_MINOR=${MACA_VERSION_MINOR}
+    MACA_VERSION_PATCH=${MACA_VERSION_PATCH}
+    MACA_VERSION_BUILD=${MACA_VERSION_BUILD}
+  )
+  message(STATUS "MACA version: ${MACA_VERSION_MAJOR}.${MACA_VERSION_MINOR}.${MACA_VERSION_PATCH}.${MACA_VERSION_BUILD}")
+
+  set(MACA_PATH "$ENV{MACA_PATH}")
+  add_compile_definitions(USE_MACA)
+
+  if (MACA_PATH AND EXISTS ${MACA_PATH})
+    message(STATUS "MACA found at ${MACA_PATH}")
+  else()
+    message(FATAL_ERROR "MACA not found or invalid path, please check your MACA_PATH")
+  endif()
+endif()
+
 include(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)
 
 # Suppress potential warnings about unused manually-specified variables
@@ -46,6 +72,7 @@ set(HIP_SUPPORTED_ARCHS "gfx906;gfx908;gfx90a;gfx942;gfx950;gfx1030;gfx1100;gfx1
 # requirements.txt files and should be kept consistent.  The ROCm torch
 # versions are derived from docker/Dockerfile.rocm
 #
+
 set(TORCH_SUPPORTED_VERSION_CUDA "2.6.0")
 set(TORCH_SUPPORTED_VERSION_ROCM "2.6.0")
 
@@ -154,7 +181,7 @@ endif()
 # `VLLM_GPU_LANG`.
 # The final set of arches is stored in `VLLM_GPU_FLAGS`.
 #
-get_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG})
+get_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG} USE_MACA)
 
 #
 # Set nvcc parallelism.
@@ -246,6 +273,13 @@ set(VLLM_EXT_SRC
   "csrc/custom_all_reduce.cu"
   "csrc/torch_bindings.cpp")
 
+# support opt of gptq-marlin
+set_source_files_properties(
+  "csrc/quantization/gptq/q_gemm.cu"
+  PROPERTIES
+  COMPILE_FLAGS "-mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128"
+)
+
 if(VLLM_GPU_LANG STREQUAL "CUDA")
   SET(CUTLASS_ENABLE_HEADERS_ONLY ON CACHE BOOL "Enable only the header library")
 
@@ -258,6 +292,12 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     set(VLLM_CUTLASS_SRC_DIR $ENV{VLLM_CUTLASS_SRC_DIR})
   endif()
 
+  # Substitue CUTLASS with MATLASS 
+  if (USE_MACA)
+    message(WARNING "Use MACA, Overwrite VLLM_CUTLASS_SRC_DIR.")
+    set(VLLM_CUTLASS_SRC_DIR $ENV{MACA_PATH}/include)
+  endif()
+
   if(VLLM_CUTLASS_SRC_DIR)
     if(NOT IS_ABSOLUTE VLLM_CUTLASS_SRC_DIR)
       get_filename_component(VLLM_CUTLASS_SRC_DIR "${VLLM_CUTLASS_SRC_DIR}" ABSOLUTE)
@@ -278,12 +318,12 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
         GIT_SHALLOW TRUE
     )
   endif()
-  FetchContent_MakeAvailable(cutlass)
+  #FetchContent_MakeAvailable(cutlass)
 
   list(APPEND VLLM_EXT_SRC
     "csrc/mamba/mamba_ssm/selective_scan_fwd.cu"
     "csrc/mamba/causal_conv1d/causal_conv1d.cu"
-    "csrc/quantization/aqlm/gemm_kernels.cu"
+    # "csrc/quantization/aqlm/gemm_kernels.cu"
     "csrc/quantization/awq/gemm_kernels.cu"
     "csrc/permute_cols.cu"
     "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu"
@@ -297,11 +337,25 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     SRCS "${VLLM_EXT_SRC}"
     CUDA_ARCHS "${CUDA_ARCHS}")
 
+  # support opt of gptq-marlin
+  set_source_files_properties(
+    "csrc/quantization/awq/gemm_kernels.cu"
+    PROPERTIES
+    COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -metaxgpu-igroup-config=B8 -maxmregcount=128"
+  )
+
+  # support opt of cutlass w8a8 scale mm
+  set_source_files_properties(
+    "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu"
+    PROPERTIES
+    COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -misched-postra=true"
+  )  
+
   # Only build Marlin kernels if we are building for at least some compatible archs.
   # Keep building Marlin for 9.0 as there are some group sizes and shapes that
   # are not supported by Machete yet.
   cuda_archs_loose_intersection(MARLIN_ARCHS "8.0;8.6;8.7;8.9;9.0;10.0;10.1;12.0" "${CUDA_ARCHS}")
-  if (MARLIN_ARCHS)
+  if (MARLIN_ARCHS AND NOT USE_MACA)
     set(MARLIN_SRCS
        "csrc/quantization/fp8/fp8_marlin.cu"
        "csrc/quantization/marlin/dense/marlin_cuda_kernel.cu"
@@ -316,13 +370,14 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     list(APPEND VLLM_EXT_SRC "${MARLIN_SRCS}")
     message(STATUS "Building Marlin kernels for archs: ${MARLIN_ARCHS}")
   else()
+    message(WARNING "Use MACA, Skip MARLIN_ARCHS.")
     message(STATUS "Not building Marlin kernels as no compatible archs found"
                    " in CUDA target architectures")
   endif()
 
   # Only build AllSpark kernels if we are building for at least some compatible archs.
   cuda_archs_loose_intersection(ALLSPARK_ARCHS "8.0;8.6;8.7;8.9" "${CUDA_ARCHS}")
-  if (ALLSPARK_ARCHS)
+  if (ALLSPARK_ARCHS AND NOT USE_MACA)
     set(ALLSPARK_SRCS
        "csrc/quantization/gptq_allspark/allspark_repack.cu"
        "csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu")
@@ -332,6 +387,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     list(APPEND VLLM_EXT_SRC "${ALLSPARK_SRCS}")
     message(STATUS "Building AllSpark kernels for archs: ${ALLSPARK_ARCHS}")
   else()
+    message(WARNING "Use MACA, Skip ALLSPARK_ARCHS.")
     message(STATUS "Not building AllSpark kernels as no compatible archs found"
                    " in CUDA target architectures")
   endif()
@@ -341,7 +397,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
   # The cutlass_scaled_mm kernels for Hopper (c3x, i.e. CUTLASS 3.x) require
   # CUDA 12.0 or later
   cuda_archs_loose_intersection(SCALED_MM_ARCHS "9.0a;" "${CUDA_ARCHS}")
-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS)
+  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS AND NOT USE_MACA)
     set(SRCS
        "csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90.cu"
        "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu"
@@ -357,6 +413,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     list(APPEND SCALED_MM_3X_ARCHS "${SCALED_MM_ARCHS}")
     message(STATUS "Building scaled_mm_c3x_sm90 for archs: ${SCALED_MM_ARCHS}")
   else()
+    message(WARNING "Use MACA, Skip SCALED_MM_ARCHS.")
     if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS)
       message(STATUS "Not building scaled_mm_c3x_sm90 as CUDA Compiler version is "
                      "not >= 12.0, we recommend upgrading to CUDA 12.0 or "
@@ -371,7 +428,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
   # The cutlass_scaled_mm kernels for Blackwell (c3x, i.e. CUTLASS 3.x) require
   # CUDA 12.8 or later
   cuda_archs_loose_intersection(SCALED_MM_ARCHS "10.0a;10.1a;12.0a" "${CUDA_ARCHS}")
-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND SCALED_MM_ARCHS)
+  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND SCALED_MM_ARCHS AND NOT USE_MACA)
     set(SRCS
       "csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm100.cu"
       "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8.cu"
@@ -385,6 +442,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     list(APPEND SCALED_MM_3X_ARCHS "${SCALED_MM_ARCHS}")
     message(STATUS "Building scaled_mm_c3x_sm100 for archs: ${SCALED_MM_ARCHS}")
   else()
+    message(WARNING "Use MACA, Skip SCALED_MM_ARCHS.")
     if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND SCALED_MM_ARCHS)
       message(STATUS "Not building scaled_mm_c3x_sm100 as CUDA Compiler version is "
                      "not >= 12.8, we recommend upgrading to CUDA 12.8 or "
@@ -410,6 +468,13 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
       CUDA_ARCHS "${SCALED_MM_2X_ARCHS}")
     list(APPEND VLLM_EXT_SRC "${SRCS}")
     list(APPEND VLLM_GPU_FLAGS "-DENABLE_SCALED_MM_C2X=1")
+    
+    # support opt of cutlass w8a8 scale mm
+    set_source_files_properties(
+      "csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu"
+      PROPERTIES
+      COMPILE_FLAGS " -mllvm -metaxgpu-igroup=true -mllvm -misched-postra=true"
+    )
     message(STATUS "Building scaled_mm_c2x for archs: ${SCALED_MM_2X_ARCHS}")
   else()
     if (SCALED_MM_3X_ARCHS)
@@ -448,7 +513,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
 
   # FP4 Archs and flags
   cuda_archs_loose_intersection(FP4_ARCHS "10.0a" "${CUDA_ARCHS}")
-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND FP4_ARCHS)
+  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND FP4_ARCHS AND NOT USE_MACA)
     set(SRCS
       "csrc/quantization/fp4/nvfp4_quant_kernels.cu"
       "csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu")
@@ -459,6 +524,7 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
     list(APPEND VLLM_GPU_FLAGS "-DENABLE_NVFP4=1")
     message(STATUS "Building NVFP4 for archs: ${FP4_ARCHS}")
   else()
+    message(WARNING "Use MACA, Skip FP4_ARCHS.")
     message(STATUS "Not building NVFP4 as no compatible archs were found.")
     # clear FP4_ARCHS
     set(FP4_ARCHS)
@@ -610,6 +676,13 @@ set(VLLM_MOE_EXT_SRC
   "csrc/moe/moe_align_sum_kernels.cu"
   "csrc/moe/topk_softmax_kernels.cu")
 
+if (USE_MACA AND ENABLE_BLAS_API)
+  list(APPEND VLLM_MOE_EXT_SRC "csrc/moe/moe_ops.cpp")
+
+  set(MCBLAS_INCLUDE_DIR $ENV{MACA_PATH}/include/mcblas)
+  set(MCBLAS_LIB $ENV{MACA_PATH}/lib/libmcblas.so)
+endif()
+
 if(VLLM_GPU_LANG STREQUAL "CUDA")
   list(APPEND VLLM_MOE_EXT_SRC "csrc/moe/moe_wna16.cu")
 endif()
@@ -628,46 +701,17 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
 
   list(APPEND VLLM_MOE_EXT_SRC "${VLLM_MOE_WNA16_SRC}")
   cuda_archs_loose_intersection(MARLIN_MOE_ARCHS "8.0;8.6;8.7;8.9;9.0;10.0;10.1;12.0" "${CUDA_ARCHS}")
-  if (MARLIN_MOE_ARCHS)
-
-    #
-    # For the Marlin MOE kernels we automatically generate sources for various
-    # preselected input type pairs and schedules.
-    # Generate sources:
-    set(MOE_MARLIN_GEN_SCRIPT
-      ${CMAKE_CURRENT_SOURCE_DIR}/csrc/moe/marlin_moe_wna16/generate_kernels.py)
-    file(MD5 ${MOE_MARLIN_GEN_SCRIPT} MOE_MARLIN_GEN_SCRIPT_HASH)
-
-    message(STATUS "Marlin MOE generation script hash: ${MOE_MARLIN_GEN_SCRIPT_HASH}")
-    message(STATUS "Last run Marlin MOE generate script hash: $CACHE{MOE_MARLIN_GEN_SCRIPT_HASH}")
-
-    if (NOT DEFINED CACHE{MOE_MARLIN_GEN_SCRIPT_HASH}
-        OR NOT $CACHE{MOE_MARLIN_GEN_SCRIPT_HASH} STREQUAL ${MOE_MARLIN_GEN_SCRIPT_HASH})
-      execute_process(
-        COMMAND ${CMAKE_COMMAND} -E env
-        PYTHONPATH=${CMAKE_CURRENT_SOURCE_DIR}/csrc/cutlass_extensions/:${CUTLASS_DIR}/python/:${VLLM_PYTHON_PATH}:$PYTHONPATH
-          ${Python_EXECUTABLE} ${MOE_MARLIN_GEN_SCRIPT}
-        RESULT_VARIABLE moe_marlin_generation_result
-        OUTPUT_VARIABLE moe_marlin_generation_output
-        OUTPUT_FILE ${CMAKE_CURRENT_BINARY_DIR}/moe_marlin_generation.log
-        ERROR_FILE ${CMAKE_CURRENT_BINARY_DIR}/moe_marlin_generation.log
-      )
+  if (MARLIN_MOE_ARCHS AND NOT USE_MACA)
+    set(MARLIN_MOE_SRC
+        "csrc/moe/marlin_kernels/marlin_moe_kernel.h"
+        "csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.h"
+        "csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.cu"
+        "csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.h"
+        "csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.cu"
+        "csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.h"
+        "csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.cu"
+        "csrc/moe/marlin_moe_ops.cu")
 
-      if (NOT moe_marlin_generation_result EQUAL 0)
-        message(FATAL_ERROR "Marlin MOE generation failed."
-                            " Result: \"${moe_marlin_generation_result}\""
-                            "\nCheck the log for details: "
-                            "${CMAKE_CURRENT_BINARY_DIR}/moe_marlin_generation.log")
-      else()
-        set(MOE_MARLIN_GEN_SCRIPT_HASH ${MOE_MARLIN_GEN_SCRIPT_HASH}
-            CACHE STRING "Last run Marlin MOE generate script hash" FORCE)
-        message(STATUS "Marlin MOE generation completed successfully.")
-      endif()
-    else()
-      message(STATUS "Marlin MOE generation script has not changed, skipping generation.")
-    endif()
-
-    file(GLOB MOE_WNAA16_MARLIN_SRC "csrc/moe/marlin_moe_wna16/*.cu")
     set_gencode_flags_for_srcs(
       SRCS "${MOE_WNAA16_MARLIN_SRC}"
       CUDA_ARCHS "${MARLIN_MOE_ARCHS}")
@@ -676,12 +720,22 @@ if(VLLM_GPU_LANG STREQUAL "CUDA")
 
     message(STATUS "Building Marlin MOE kernels for archs: ${MARLIN_MOE_ARCHS}")
   else()
+    message(WARNING "Use MACA, Skip MARLIN_MOE_ARCHS.")
     message(STATUS "Not building Marlin MOE kernels as no compatible archs found"
                    " in CUDA target architectures")
   endif()
 endif()
 
 message(STATUS "Enabling moe extension.")
+
+set(BLAS_API_ARGS "")
+if(USE_MACA AND ENABLE_BLAS_API)
+  message(STATUS "Blas API for fused moe enabled")
+  list(APPEND BLAS_API_ARGS 
+      INCLUDE_DIRECTORIES ${MCBLAS_INCLUDE_DIR}
+      LIBRARIES ${MCBLAS_LIB})
+endif()
+
 define_gpu_extension_target(
   _moe_C
   DESTINATION vllm
@@ -689,6 +743,7 @@ define_gpu_extension_target(
   SOURCES ${VLLM_MOE_EXT_SRC}
   COMPILE_FLAGS ${VLLM_GPU_FLAGS}
   ARCHITECTURES ${VLLM_GPU_ARCHES}
+  ${BLAS_API_ARGS}
   USE_SABI 3
   WITH_SOABI)
 
@@ -713,7 +768,7 @@ if(VLLM_GPU_LANG STREQUAL "HIP")
 endif()
 
 # For CUDA we also build and ship some external projects.
-if (VLLM_GPU_LANG STREQUAL "CUDA")
+if (VLLM_GPU_LANG STREQUAL "CUDA" AND NOT USE_MACA)
     include(cmake/external_projects/flashmla.cmake)
     include(cmake/external_projects/vllm_flash_attn.cmake)
 endif ()
diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index c9cd099b8..fd192f7bb 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -89,7 +89,7 @@ endfunction()
 #
 # Get additional GPU compiler flags from torch.
 #
-function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
+function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG USE_MACA)
   if (${GPU_LANG} STREQUAL "CUDA")
     #
     # Get common NVCC flags from torch.
@@ -98,6 +98,19 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
       "from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))"
       "Failed to determine torch nvcc compiler flags")
 
+    if (USE_MACA)
+      message(WARNING "Use MACA, Overwrite GPU_FLAGS.")
+      set(GPU_FLAGS 
+        "-D__CUDA_NO_HALF_OPERATORS__"
+        "-D__CUDA_NO_HALF_CONVERSIONS__"
+        "-D__CUDA_NO_HALF2_OPERATORS__"
+        "--expt-relaxed-constexpr")
+      list(APPEND GPU_FLAGS 
+        "-mllvm" 
+        "-metaxgpu-GridDim-UseLdu")
+    endif()
+
+
     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)
       list(APPEND GPU_FLAGS "-DENABLE_FP8")
     endif()
@@ -105,8 +118,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
       list(REMOVE_ITEM GPU_FLAGS
         "-D__CUDA_NO_HALF_OPERATORS__"
         "-D__CUDA_NO_HALF_CONVERSIONS__"
-        "-D__CUDA_NO_BFLOAT16_CONVERSIONS__"
         "-D__CUDA_NO_HALF2_OPERATORS__")
+      if (not USE_MACA)
+        list(REMOVE_ITEM GPU_FLAGS
+        "-D__CUDA_NO_BFLOAT16_CONVERSIONS__")
+      endif()
     endif()
 
   elseif(${GPU_LANG} STREQUAL "HIP")
@@ -148,6 +164,13 @@ macro(clear_cuda_arches CUDA_ARCH_FLAGS)
     string(REGEX MATCHALL "-gencode arch=[^ ]+" CUDA_ARCH_FLAGS
       ${CMAKE_CUDA_FLAGS})
 
+    # change the config's value of metaxgpu-disable-bsm-offset
+    string(REPLACE "-metaxgpu-disable-bsm-offset=1" "-metaxgpu-disable-bsm-offset=0"
+            CMAKE_CUDA_FLAGS ${CMAKE_CUDA_FLAGS})
+
+    # opt of cutlass w8a8
+    string(APPEND CMAKE_CUDA_FLAGS " -mllvm -structurizecfg-skip-uniform-regions=true")
+
     # Remove all `-gencode` flags from `CMAKE_CUDA_FLAGS` since they will be modified
     # and passed back via the `CUDA_ARCHITECTURES` property.
     string(REGEX REPLACE "-gencode arch=[^ ]+ *" "" CMAKE_CUDA_FLAGS
diff --git a/csrc/activation_kernels.cu b/csrc/activation_kernels.cu
index 88275dbdd..fc416fb78 100644
--- a/csrc/activation_kernels.cu
+++ b/csrc/activation_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <ATen/cuda/CUDAContext.h>
 #include <torch/all.h>
 #include <c10/cuda/CUDAGuard.h>
@@ -31,10 +32,120 @@ __global__ void act_and_mul_kernel(
   }
 }
 
+template<typename scalar_t, typename VT, int N, scalar_t (*ACT_FN)(const scalar_t&),
+        bool act_first>
+__global__ void act_and_mul_kernel_bd_opt(
+  scalar_t* __restrict__ out,               // [..., d]
+  const scalar_t* __restrict__ input,       // [..., 2, d]
+  const int d,
+  const int blockDim_x
+){
+    const int64_t token_idx = blockIdx.y;
+    int x_offset = (blockIdx.x * blockDim_x + threadIdx.x) * N;
+    if(x_offset >= d) return;
+    int64_t offset0 = token_idx * d;
+    int64_t offset1 = offset0 << 1;
+    const scalar_t* ptr_input = input + offset1;
+    const scalar_t* ptr_input0 = ptr_input + x_offset;
+    const scalar_t* ptr_input1 = ptr_input0 + d;
+    scalar_t* ptr_output = out + offset0 + x_offset;
+    VT vsrc0 = *(VT*)(ptr_input0);
+    VT vsrc1 = *(VT*)(ptr_input1);
+    VT vdst;
+    scalar_t* ptr_src0 = (scalar_t*)&vsrc0;
+    scalar_t* ptr_src1 = (scalar_t*)&vsrc1;
+    scalar_t* ptr_dst = (scalar_t*)&vdst;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+        ptr_dst[i] = compute<scalar_t, ACT_FN, act_first>(ptr_src0[i], ptr_src1[i]);
+    }
+    *(VT*)(ptr_output) = vdst;
+}
+
+template<typename scalar_t, typename VT, int N, scalar_t (*ACT_FN)(const scalar_t&),
+        bool act_first>
+__global__ void act_and_mul_kernel_sd_opt(
+  scalar_t* __restrict__ out,               // [..., d]
+  const scalar_t* __restrict__ input,       // [..., 2, d]
+  const int d,
+  const int blockDim_x,
+  const int gridDim_x,
+  const int token_per_block,
+  const int max_token_num) {
+    __shared__ int8_t sm_buffer[16384];
+    int token_offset = blockIdx.x * token_per_block;
+    int out_offset = token_offset * d;
+    int in_offset = out_offset << 1;
+    int num_token = min(max_token_num - token_offset, token_per_block);
+    if(num_token <= 0) return;
+    const scalar_t *ptr_block_input = input + in_offset;
+    scalar_t* ptr_block_output = out + out_offset;
+    int output_size = num_token * d;
+    int input_size = output_size << 1;
+    
+    scalar_t* ptr_sm_buffer = (scalar_t*)sm_buffer;
+    int stride = blockDim_x * N;
+    for(int i = threadIdx.x*N; i < input_size; i += stride) {
+        *(VT*)(ptr_sm_buffer + i) = *(VT*)(ptr_block_input + i);
+    }
+    __syncthreads();
+    for(int i = threadIdx.x; i < output_size; i += blockDim_x) {
+      int token_id = i / d;
+      int x_offset = i % d;
+      scalar_t *ptr_input0 = ptr_sm_buffer + token_id * d * 2 + x_offset;
+      scalar_t *ptr_input1 = ptr_input0 + d;
+      *(ptr_block_output + i) = compute<scalar_t, ACT_FN, act_first>(*ptr_input0, ptr_input1[0]);
+    }
+}
+
+template<typename scalar_t, typename VT, int N, scalar_t (*ACT_FN)(const scalar_t&),
+        bool act_first>
+__global__ void act_and_mul_kernel_sd_fast_opt(
+  scalar_t* __restrict__ out,               // [..., d]
+  const scalar_t* __restrict__ input,       // [..., 2, d]
+  const int d,
+  const int blockDim_x,
+  const int gridDim_x,
+  const int token_per_block,
+  const int max_token_num) {
+    __shared__ int8_t sm_buffer[16384];
+    int token_offset = blockIdx.x * token_per_block;
+    int out_offset = token_offset * d;
+    int in_offset = out_offset << 1;
+    int num_token = min(max_token_num - token_offset, token_per_block);
+    if(num_token <= 0) return;
+    const scalar_t *ptr_block_input = input + in_offset;
+    scalar_t* ptr_block_output = out + out_offset;
+    int output_size = num_token * d;
+    int input_size = output_size << 1;
+    
+    scalar_t* ptr_sm_buffer = (scalar_t*)sm_buffer;
+    int stride = blockDim_x * N;
+    for(int i = threadIdx.x*N; i < input_size; i += stride) {
+        *(VT*)(ptr_sm_buffer + i) = *(VT*)(ptr_block_input + i);
+    }
+    __syncthreads();
+    for(int i = threadIdx.x*N; i < output_size; i += stride) {
+      int token_id = i / d;
+      int x_offset = i % d;
+      scalar_t *ptr_input0 = ptr_sm_buffer + token_id * d * 2 + x_offset;
+      scalar_t *ptr_input1 = ptr_input0 + d;
+      VT vdst;
+      scalar_t *ptr_dst = (scalar_t*)&vdst;
+      #pragma unroll N
+      for(int j = 0; j < N; j++) {
+        ptr_dst[j] = compute<scalar_t, ACT_FN, act_first>(ptr_input0[j], ptr_input1[j]);
+      }
+      *(VT*)(ptr_block_output + i) = vdst;
+    }
+}
+
 template <typename T>
 __device__ __forceinline__ T silu_kernel(const T& x) {
   // x * sigmoid(x)
-  return (T)(((float)x) / (1.0f + expf((float)-x)));
+  // return (T)(((float)x) / (1.0f + expf((float)-x)));
+  float x_f = (float)x;
+  return (T) ((x_f) / (1.0f + __builtin_expf(-x_f)));
 }
 
 template <typename T>
@@ -68,16 +179,68 @@ __device__ __forceinline__ T gelu_tanh_kernel(const T& x) {
 #define LAUNCH_ACTIVATION_GATE_KERNEL(KERNEL, ACT_FIRST)                 \
   int d = input.size(-1) / 2;                                            \
   int64_t num_tokens = input.numel() / input.size(-1);                   \
+  int n = 16 / input.element_size();                                                          \
+  if(((d&(n - 1)) == 0) && d >= 512 * n) {\
+    int blocksize = 512;                                                                  \
+    dim3 gridsize((d + 512*n - 1) / (512*n), num_tokens,1);                               \
+    const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                     \
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                         \
+    VLLM_DISPATCH_FLOATING_TYPES(                                                         \
+      input.scalar_type(),                                                                \
+      "act_and_mul_kernel_bd_opt",                                                               \
+      [&] {                                                                               \
+        vllm::act_and_mul_kernel_bd_opt<scalar_t, float4, 16 / sizeof(scalar_t), KERNEL<scalar_t>, ACT_FIRST><<<gridsize, blocksize, 0, stream>>>(   \
+          out.data_ptr<scalar_t>(),                                                         \
+          input.data_ptr<scalar_t>(),                                                       \
+          d, blocksize);                                                                    \
+      });                                                                                   \
+  } else if(d < 512 && (d & (n - 1)) == 0) {                                                \
+        int block_token = 16384 / input.element_size() / 2 / d;                             \
+        block_token = block_token / n * n;                                                  \
+        int blocksize = 512;                                                                \
+        int gridsize = (num_tokens + block_token - 1) / block_token;                        \
+        const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                   \
+        const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                       \
+        VLLM_DISPATCH_FLOATING_TYPES(                                                       \
+        input.scalar_type(),                                                                \
+        "act_and_mul_kernel_sd_fast_opt",                                                   \
+        [&] {                                                                               \
+        vllm::act_and_mul_kernel_sd_fast_opt<scalar_t, float4, 16 / sizeof(scalar_t), KERNEL<scalar_t>, ACT_FIRST><<<gridsize, blocksize, 0, stream>>>(   \
+        out.data_ptr<scalar_t>(),                                                       \
+        input.data_ptr<scalar_t>(),                                                     \
+        d, blocksize,gridsize,block_token,num_tokens);                                  \
+        });                                                                                 \
+  } else if(d < 512) {                                                                      \
+        int block_token = 16384 / input.element_size() / 2 / d;                             \
+        block_token = block_token / n * n;                                                  \
+        int blocksize = 512;                                                                \
+        int gridsize = (num_tokens + block_token - 1) / block_token;                        \
+        const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                   \
+        const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                       \
+        VLLM_DISPATCH_FLOATING_TYPES(                                                       \
+        input.scalar_type(),                                                                \
+        "act_and_mul_kernel_sd_opt",                                                        \
+        [&] {                                                                               \
+          vllm::act_and_mul_kernel_sd_opt<scalar_t, float4, 16 / sizeof(scalar_t), KERNEL<scalar_t>, ACT_FIRST><<<gridsize, blocksize, 0, stream>>>(   \
+            out.data_ptr<scalar_t>(),                                                       \
+            input.data_ptr<scalar_t>(),                                                     \
+            d, blocksize,gridsize,block_token,num_tokens);                                  \
+        });                                                                                 \
+  } else {                                                                                  \
   dim3 grid(num_tokens);                                                 \
   dim3 block(std::min(d, 1024));                                         \
   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));      \
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();          \
   VLLM_DISPATCH_FLOATING_TYPES(                                          \
-      input.scalar_type(), "act_and_mul_kernel", [&] {                   \
-        vllm::act_and_mul_kernel<scalar_t, KERNEL<scalar_t>, ACT_FIRST>  \
-            <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(),       \
-                                         input.data_ptr<scalar_t>(), d); \
-      });
+      input.scalar_type(),                                                                  \
+      "act_and_mul_kernel",                                                                 \
+      [&] {                                                                                 \
+        vllm::act_and_mul_kernel<scalar_t, KERNEL<scalar_t>, ACT_FIRST><<<grid, block, 0, stream>>>(   \
+          out.data_ptr<scalar_t>(),                                                         \
+          input.data_ptr<scalar_t>(),                                                       \
+          d);                                                                               \
+      });                                                                                   \
+  }
 
 void silu_and_mul(torch::Tensor& out,    // [..., d]
                   torch::Tensor& input)  // [..., 2 * d]
diff --git a/csrc/attention/attention_kernels.cuh b/csrc/attention/attention_kernels.cuh
index eb216dc8b..76784395f 100644
--- a/csrc/attention/attention_kernels.cuh
+++ b/csrc/attention/attention_kernels.cuh
@@ -43,6 +43,10 @@ typedef __hip_bfloat16 __nv_bfloat16;
 #define MIN(a, b) ((a) < (b) ? (a) : (b))
 #define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))
 
+typedef __NATIVE_VECTOR__(2, float) v2f;
+typedef __NATIVE_VECTOR__(2, _Float16) v2h;
+typedef __NATIVE_VECTOR__(4, float) v4f;
+
 namespace vllm {
 
 // Utility function for attention softmax.
@@ -50,7 +54,7 @@ template <int NUM_WARPS>
 inline __device__ float block_sum(float* red_smem, float sum) {
   // Decompose the thread index into warp / lane.
   int warp = threadIdx.x / WARP_SIZE;
-  int lane = threadIdx.x % WARP_SIZE;
+  int lane = threadIdx.x & (WARP_SIZE - 1);
 
   // Compute the sum per warp.
 #pragma unroll
@@ -81,6 +85,229 @@ inline __device__ float block_sum(float* red_smem, float sum) {
   return VLLM_SHFL_SYNC(sum, 0);
 }
 
+template<int NUM_WARPS>
+inline __device__ float mxblock_sum(float* red_smem, float sum) {
+  // Decompose the thread index into warp / lane.
+  int warp = threadIdx.x >> 6;
+  int lane = threadIdx.x & (MXWARP_SIZE - 1);
+
+  // Compute the sum per warp.
+#pragma unroll
+  for (int mask = MXWARP_SIZE / 2; mask >= 1; mask /= 2) {
+    sum += MXVLLM_SHFL_XOR_SYNC(sum, mask);
+  }
+
+  // Warp leaders store the data to shared memory.
+  if (lane == 0) {
+    red_smem[warp] = sum;
+  }
+
+  // Make sure the data is in shared memory.
+  __syncthreads();
+
+  // The warps compute the final sums.
+  if (lane < NUM_WARPS) {
+    sum = red_smem[lane];
+  }
+ // Parallel reduction inside the warp.
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    sum += MXVLLM_SHFL_XOR_SYNC(sum, mask);
+  }
+
+  // Broadcast to other threads.
+  return MXVLLM_SHFL_SYNC(sum, 0);
+}
+template<typename scalar_t>
+__device__  float __forceinline__ atten_mul(scalar_t *a, float b, int j) {
+  printf("not support\n");
+}
+
+template<>
+__device__ float __forceinline__ atten_mul(uint16_t *a, float b, int j) {
+    return __half2float(*((half*)a + j)) * __half2float(__float2half(b));
+}
+
+template<>
+__device__ float __forceinline__ atten_mul(__nv_bfloat16 *a, float b, int j) {
+    return __bfloat162float(*(a + j)) * __bfloat162float(__float2bfloat16(b));
+}
+
+template<typename scalar_t>
+__device__  float __forceinline__ atten_mul_opt(scalar_t *a, float b, int j) {
+  printf("not support\n");
+}
+
+template<>
+__device__ float __forceinline__ atten_mul_opt(uint16_t *a, float b, int j) {
+    return __half2float(*((half*)a + j)) * b;
+}
+
+template<>
+__device__ float __forceinline__ atten_mul_opt(__nv_bfloat16 *a, float b, int j) {
+    return __bfloat162float(*(a + j)) * b;
+}
+
+template<typename scalar_t>
+__device__  void __forceinline__ atten_mul_opt2(scalar_t *a, float b, int j, float &r0, float &r1) {
+  printf("not support\n");
+}
+template<>
+__device__ void __forceinline__ atten_mul_opt2(uint16_t *a, float b, int j, float &r0, float &r1) {
+    v2f vacc; vacc[0] = r0; vacc[1] = r1;
+    v2f vb; vb[0] = b; vb[1] = b;
+    v2h a_2h = *(v2h*)(a + j);
+    v2f va = __builtin_mxc_cvt_pk_f16tof32(a_2h);
+    vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
+    r0 = vacc[0]; r1 = vacc[1];
+}
+
+template<>
+__device__ void __forceinline__ atten_mul_opt2(__nv_bfloat16 *a, float b, int j, float &r0, float &r1) {
+    v2f vacc; vacc[0] = r0; vacc[1] = r1;
+    v2f vb; vb[0] = b; vb[1] = b;
+    v2f va; va[0] = __bfloat162float(*(a + j)); va[1] = __bfloat162float(*(a + j + 1));
+    vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
+    r0 = vacc[0]; r1 = vacc[1];
+}
+
+template<typename scalar_t, typename cache_t>
+__device__ float __forceinline__ atten_dot(scalar_t* a, cache_t *b ,int i){
+  printf("not support\n");
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(uint16_t* a, uint16_t *b ,int i){
+  return __half2float(*((half*)a + i)) * __half2float(*((half*)b + i));
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(float* a, uint16_t *b ,int i) {
+  return *(a + i) * __half2float(*((half*)b + i));
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(__nv_bfloat16* a, __nv_bfloat16 *b ,int i){
+  return __bfloat162float(a[i]) * __bfloat162float(b[i]);
+}
+
+template<>
+__device__ float __forceinline__ atten_dot(float *a, __nv_bfloat16* b, int i) {
+  return *(a + i) * __bfloat162float(b[i]);
+}
+
+template<typename scalar_t, typename cache_t, typename T, int Vec_size>
+__device__ void __forceinline__ atten_dot(scalar_t &v1, cache_t &v2, T&qk) {
+  printf("not support\n");
+}
+
+template<>
+__device__  void __forceinline__ atten_dot<Float8_, uint4,v2f, 8>(Float8_ &v1, uint4 &v2,v2f &vdst) {
+    v2h *ptr_v2 = (v2h*)&v2;
+    v4f* ptr_v1 = (v4f*)&v1;
+    v4f reg_v1_0 = ptr_v1[0], reg_v1_1 = ptr_v1[1];
+    v2f v1_2f, v2_2f;
+    v2h v2_2h;
+    v1_2f[0] = reg_v1_0[0];                  v1_2f[1] = reg_v1_0[1];
+    v2_2h = ptr_v2[0];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = reg_v1_0[2];                  v1_2f[1] = reg_v1_0[3];
+    v2_2h = ptr_v2[1];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = reg_v1_1[0];                  v1_2f[1] = reg_v1_1[1];
+    v2_2h = ptr_v2[2];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = reg_v1_1[2];                  v1_2f[1] = reg_v1_1[3];
+    v2_2h = ptr_v2[3];
+    v2_2f = __builtin_mxc_cvt_pk_f16tof32(v2_2h);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+}
+template<>
+__device__ void __forceinline__ atten_dot<Float8_, bf16_8_t, v2f, 8>(Float8_ &v1, bf16_8_t &v2, v2f &vdst) {
+    __nv_bfloat16 * ptr_v2 = (__nv_bfloat16*)&v2;
+    v2f v1_2f, v2_2f;
+    v1_2f[0] = v1.x.x;                  v1_2f[1] = v1.x.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[0]); v2_2f[1] = __bfloat162float(ptr_v2[1]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = v1.y.x;                  v1_2f[1] = v1.y.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[2]); v2_2f[1] = __bfloat162float(ptr_v2[3]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = v1.z.x;                  v1_2f[1] = v1.z.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[4]); v2_2f[1] = __bfloat162float(ptr_v2[5]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+    v1_2f[0] = v1.w.x;                  v1_2f[1] = v1.w.y;
+    v2_2f[0] = __bfloat162float(ptr_v2[6]); v2_2f[1] = __bfloat162float(ptr_v2[7]);
+    vdst = __builtin_mxc_pk_fma_f32(v1_2f, v2_2f, vdst);
+}
+
+template<typename T, typename Vec_T0, typename Vec_T1, int Vec_size>
+__device__ __forceinline__ void convert(Vec_T0 & src , Vec_T1 &dst){
+    printf("not support\n");
+}
+
+template<>
+__device__ __forceinline__ void convert<uint16_t, uint4, Float8_, 8>(uint4 & src , Float8_ &dst) {
+  half * ptr_src = (half *)&src;
+  dst.x.x = __half2float(ptr_src[0]);
+  dst.x.y = __half2float(ptr_src[1]);
+  dst.y.x = __half2float(ptr_src[2]);
+  dst.y.y = __half2float(ptr_src[3]);
+  dst.z.x = __half2float(ptr_src[4]);
+  dst.z.y = __half2float(ptr_src[5]);
+  dst.w.x = __half2float(ptr_src[6]);
+  dst.w.y = __half2float(ptr_src[7]);
+}
+template<>
+__device__ __forceinline__ void convert<__nv_bfloat16, bf16_8_t, Float8_, 8>(bf16_8_t & src , Float8_ &dst) {
+  __nv_bfloat16 * ptr_src = (__nv_bfloat16 *)&src;
+  dst.x.x = __bfloat162float(ptr_src[0]);
+  dst.x.y = __bfloat162float(ptr_src[1]);
+  dst.y.x = __bfloat162float(ptr_src[2]);
+  dst.y.y = __bfloat162float(ptr_src[3]);
+  dst.z.x = __bfloat162float(ptr_src[4]);
+  dst.z.y = __bfloat162float(ptr_src[5]);
+  dst.w.x = __bfloat162float(ptr_src[6]);
+  dst.w.y = __bfloat162float(ptr_src[7]);
+}
+
+template<typename T>
+__device__ __forceinline__ float convert(float src){
+  printf("not support\n");
+}
+
+template<>
+__device__ __forceinline__ float convert<uint16_t>(float src) {
+   return __half2float(__float2half(src));
+}
+
+template<>
+__device__ __forceinline__ float convert<__nv_bfloat16>(float src) {
+   return __bfloat162float(__float2bfloat16(src));
+}
+
+template<typename scalar_t>
+__device__ void to_v2f(scalar_t& a, scalar_t& b, v2f& vdst){
+printf("not support\n");
+}
+
+template<>
+__device__ void to_v2f(uint16_t& a, uint16_t& b, v2f &vdst) {
+  v2h f_d;
+  _Float16 * ptr_a = (_Float16*)&f_d;
+  ptr_a[0] = *(_Float16*)&a;
+  ptr_a[1] = *(_Float16*)&b;
+  vdst = __builtin_mxc_cvt_pk_f16tof32(f_d);
+}
+
+template<>
+__device__ void to_v2f(__nv_bfloat16& a, __nv_bfloat16& b, v2f &vdst) {
+  vdst[0] = __bfloat162float(a);
+  vdst[1] = __bfloat162float(b);
+}
+
 // TODO(woosuk): Merge the last two dimensions of the grid.
 // Grid: (num_heads, num_seqs, max_num_partitions).
 template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
@@ -111,37 +338,30 @@ __device__ void paged_attention_kernel(
   const int seq_idx = blockIdx.y;
   const int partition_idx = blockIdx.z;
   const int max_num_partitions = gridDim.z;
+  const int blockDim_x = blockDim.x;
   constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
   const int seq_len = seq_lens[seq_idx];
   if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
     // No work to do. Terminate the thread block.
     return;
   }
-
   const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
-  const int num_blocks_per_partition =
-      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
+  const int num_blocks_per_partition = USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
 
   // [start_block_idx, end_block_idx) is the range of blocks to process.
-  const int start_block_idx =
-      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
-  const int end_block_idx =
-      MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
   const int num_blocks = end_block_idx - start_block_idx;
 
   // [start_token_idx, end_token_idx) is the range of tokens to process.
   const int start_token_idx = start_block_idx * BLOCK_SIZE;
-  const int end_token_idx =
-      MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
   const int num_tokens = end_token_idx - start_token_idx;
 
   constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
-  constexpr int NUM_THREAD_GROUPS =
-      NUM_THREADS / THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
-                                        // divides NUM_THREADS
+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
   assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
-  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
-      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
   const int thread_idx = threadIdx.x;
   const int warp_idx = thread_idx / WARP_SIZE;
@@ -151,18 +371,14 @@ __device__ void paged_attention_kernel(
   const int num_heads = gridDim.x;
   const int num_queries_per_kv = num_heads / num_kv_heads;
   const int kv_head_idx = head_idx / num_queries_per_kv;
-  const float alibi_slope =
-      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
-
-  // A vector type to store a part of a key or a query.
-  // The vector size is configured in such a way that the threads in a thread
-  // group fetch or compute 16 bytes at a time. For example, if the size of a
-  // thread group is 4 and the data type is half, then the vector size is 16 /
-  // (4 * sizeof(half)) == 2.
-  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
   using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
   using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
+#ifdef ENABLE_FP8_E5M2
   using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+#endif
 
   constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
   constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
@@ -172,21 +388,20 @@ __device__ void paged_attention_kernel(
 
   // Load the query to registers.
   // Each thread in a thread group has a different part of the query.
-  // For example, if the the thread group size is 4, then the first thread in
-  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
-  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
-  // q is split from a qkv tensor, it may not be contiguous.
+  // For example, if the the thread group size is 4, then the first thread in the group
+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
+  // th vectors of the query, and so on.
+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
   const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
-  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
 #pragma unroll
-  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
-       i += NUM_THREAD_GROUPS) {
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
+    Q_vec_l dst;
     const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
-    q_vecs[thread_group_offset][i] =
-        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
   }
-  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
-                    // memory wall right before we use q_vecs
+  __syncthreads(); // TODO(naed90): possible speedup if this is replaced with a memory wall right before we use q_vecs
 
   // Memory planning.
   extern __shared__ char shared_mem[];
@@ -195,9 +410,8 @@ __device__ void paged_attention_kernel(
   // Workspace for reduction.
   __shared__ float red_smem[2 * NUM_WARPS];
 
-  // x == THREAD_GROUP_SIZE * VEC_SIZE
   // Each thread group fetches x elements from the key at a time.
-  constexpr int x = 16 / sizeof(cache_t);
+  constexpr int x = 16;
   float qk_max = -FLT_MAX;
 
   // Iterate over the key blocks.
@@ -224,13 +438,96 @@ __device__ void paged_attention_kernel(
                         1;
   }
 
-  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
-       block_idx += NUM_WARPS) {
-    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
-    // int64 because int32 can lead to overflow when this variable is multiplied
-    // by large numbers (e.g., kv_block_stride).
-    // For blocksparse attention: skip computation on blocks that are not
-    // attended
+  int block_idx0 = start_block_idx + warp_idx;
+  int kv_offset0, kv_offset1;
+  K_vec load_k[NUM_VECS_PER_THREAD];
+  K_vec compute_k[NUM_VECS_PER_THREAD];
+  
+  int k_offset[NUM_VECS_PER_THREAD];
+  kv_offset0 = block_table[block_idx0];
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  #pragma unroll
+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
+    const int offset1 = vec_idx >> 4;
+    const int offset2 = vec_idx & 15;
+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
+  }
+  if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if(is_remote || is_local) {
+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+            const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+            const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                              + kv_head_idx * kv_head_stride
+                                              + physical_block_offset * x;
+            
+        #pragma unroll
+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }    
+      } 
+  } else {
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+      const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                        + kv_head_idx * kv_head_stride
+                                        + physical_block_offset * x;
+      
+  #pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+      }
+    }
+  } 
+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        compute_k[j] = load_k[j];
+      }
+      if(block_idx < end_block_idx - NUM_WARPS) {
+          kv_offset0 = kv_offset1;
+          int nblock_idx = block_idx + NUM_WARPS;
+          if(block_idx < end_block_idx - (NUM_WARPS << 1)) {
+            kv_offset1 = block_table[block_idx + (NUM_WARPS<<1)];
+          }
+          if constexpr (IS_BLOCK_SPARSE) {
+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
+            const bool is_remote =
+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+            const bool is_local =
+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+            if(is_remote || is_local) {
+              const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                      + kv_head_idx * kv_head_stride
+                                      + physical_block_offset * x;
+              #pragma unroll NUM_VECS_PER_THREAD
+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+              }  
+            }
+          } else {
+            const cache_t* k_ptr = k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride
+                                      + kv_head_idx * kv_head_stride
+                                      + physical_block_offset * x;
+            #pragma unroll NUM_VECS_PER_THREAD
+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          } 
+    }
     if constexpr (IS_BLOCK_SPARSE) {
       const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
       const bool is_remote =
@@ -254,48 +551,26 @@ __device__ void paged_attention_kernel(
         continue;
       }
     }
-    const int64_t physical_block_number =
-        static_cast<int64_t>(block_table[block_idx]);
-
-    // Load a key to registers.
-    // Each thread in a thread group has a different part of the key.
-    // For example, if the the thread group size is 4, then the first thread in
-    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
-    // has 1, 5, 9, ... th vectors of the key, and so on.
-    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
-      const int physical_block_offset =
-          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
-      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-      K_vec k_vecs[NUM_VECS_PER_THREAD];
-
-#pragma unroll
-      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
-        const cache_t* k_ptr =
-            k_cache + physical_block_number * kv_block_stride +
-            kv_head_idx * kv_head_stride + physical_block_offset * x;
-        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
-        const int offset1 = (vec_idx * VEC_SIZE) / x;
-        const int offset2 = (vec_idx * VEC_SIZE) % x;
-
-        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
-          k_vecs[j] = *reinterpret_cast<const K_vec*>(
-              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
-        } else {
-          // Vector conversion from Quant_vec to K_vec.
-          Quant_vec k_vec_quant = *reinterpret_cast<const Quant_vec*>(
-              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
-          k_vecs[j] = fp8::scaled_convert<K_vec, Quant_vec, KV_DTYPE>(
-              k_vec_quant, *k_scale);
-        }
-      }
 
       // Compute dot product.
       // This includes a reduction across the threads in the same thread group.
-      float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
-                             q_vecs[thread_group_offset], k_vecs);
+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
+      float qk = 0.0f;
+      v2f f2_qk = {0,0};
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+	atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
+      }
+      qk = f2_qk[0] + f2_qk[1];
+  
+      #pragma unroll
+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+        qk += VLLM_SHFL_XOR_SYNC(qk, mask);
+      }
+      qk = scale * qk;
       // Add the ALiBi bias if slopes are given.
       qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
-
+      
       if (thread_group_offset == 0) {
         // Store the partial reductions to shared memory.
         // NOTE(woosuk): It is required to zero out the masked logits.
@@ -318,7 +593,6 @@ __device__ void paged_attention_kernel(
     red_smem[warp_idx] = qk_max;
   }
   __syncthreads();
-
   // TODO(woosuk): Refactor this part.
   // Get the max qk value for the sequence.
   qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
@@ -332,14 +606,14 @@ __device__ void paged_attention_kernel(
   // Get the sum of the exp values.
   float exp_sum = 0.f;
   for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
-    float val = __expf(logits[i] - qk_max);
+    float val = __builtin_expf(logits[i] - qk_max);
     logits[i] = val;
     exp_sum += val;
   }
   exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
 
   // Compute softmax.
-  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
   for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
     logits[i] *= inv_sum;
   }
@@ -347,201 +621,216 @@ __device__ void paged_attention_kernel(
 
   // If partitioning is enabled, store the max logit and exp_sum.
   if (USE_PARTITIONING && thread_idx == 0) {
-    float* max_logits_ptr = max_logits +
-                            seq_idx * num_heads * max_num_partitions +
-                            head_idx * max_num_partitions + partition_idx;
+    float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
+                                       + head_idx * max_num_partitions
+                                       + partition_idx;
     *max_logits_ptr = qk_max;
-    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
-                          head_idx * max_num_partitions + partition_idx;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
+                                   + head_idx * max_num_partitions
+                                   + partition_idx;
     *exp_sums_ptr = exp_sum;
   }
-
-  // Each thread will fetch 16 bytes from the value cache at a time.
-  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
+  constexpr int NUM_COLS_PER_ITER = MAX(WARP_SIZE / NUM_V_VECS_PER_THREAD,1);
+  constexpr int NUM_VALID_THREAD = NUM_COLS_PER_ITER * NUM_V_VECS_PER_THREAD;
+  constexpr int NUM_LGT_PER_COL = (BLOCK_SIZE + NUM_COLS_PER_ITER - 1) / NUM_COLS_PER_ITER;
+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
   using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
   using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
-  using V_quant_vec = typename Vec<cache_t, V_VEC_SIZE>::Type;
   using Float_L_vec = typename FloatVec<L_vec>::Type;
-
-  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
-  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
-  constexpr int NUM_ROWS_PER_THREAD =
-      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
-
-  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
-  float accs[NUM_ROWS_PER_THREAD];
-#pragma unroll
-  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-    accs[i] = 0.f;
-  }
-
-  scalar_t zero_value;
-  zero(zero_value);
-  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
-       block_idx += NUM_WARPS) {
-    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
-    // int64 because int32 can lead to overflow when this variable is multiplied
-    // by large numbers (e.g., kv_block_stride).
-    // For blocksparse attention: skip computation on blocks that are not
-    // attended
+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
+  V_vec v_vecs[NUM_LGT_PER_COL];
+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
+  float accs[V_VEC_SIZE];
+  float reg_log[NUM_LGT_PER_COL];
+  float reg_prev_log[NUM_LGT_PER_COL];
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    accs[i] = 0.0f;
+  }
+  int token_idx, kv_stride, block_offset;
+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
+  kv_offset0 = block_table[block_idx0];
+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE; 
+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+  float *ptr_logits = logits + token_idx - start_token_idx;
+  if(lane < NUM_VALID_THREAD) {
     if constexpr (IS_BLOCK_SPARSE) {
-      int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
-      if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
-          !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
-        continue;
-      }
+          int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+          if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+              ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+              if(block_idx0 == num_seq_blocks - 1) {
+              #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                  if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                  }
+                }
+              } else {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                  if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                  }
+                }
+              }
+          }
     }
-    const int64_t physical_block_number =
-        static_cast<int64_t>(block_table[block_idx]);
-    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
-    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
-    L_vec logits_vec;
-    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
-                                                           start_token_idx));
-
-    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
-                           kv_head_idx * kv_head_stride;
-#pragma unroll
-    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-      if (row_idx < HEAD_SIZE) {
-        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
-        V_vec v_vec;
-
-        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
-          v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+    else {
+      if(block_idx0 == num_seq_blocks - 1) {
+        #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+	    if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+          }
         } else {
-          V_quant_vec v_quant_vec =
-              *reinterpret_cast<const V_quant_vec*>(v_ptr + offset);
-          // Vector conversion from V_quant_vec to V_vec.
-          v_vec = fp8::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec,
-                                                                    *v_scale);
-        }
-        if (block_idx == num_seq_blocks - 1) {
-          // NOTE(woosuk): When v_vec contains the tokens that are out of the
-          // context, we should explicitly zero out the values since they may
-          // contain NaNs. See
-          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
-          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
-#pragma unroll
-          for (int j = 0; j < V_VEC_SIZE; j++) {
-            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;
+          #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+	    if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
           }
         }
-        accs[i] += dot(logits_vec, v_vec);
-      }
     }
-  }
-
-  // Perform reduction within each warp.
-#pragma unroll
-  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-    float acc = accs[i];
-#pragma unroll
-    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
-      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
+    
+    for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+        int next_block = block_idx + NUM_WARPS;
+        int nnext_block = next_block + NUM_WARPS;
+        for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+            v_vecs[i] = v_prev_vecs[i];
+            reg_log[i] = reg_prev_log[i];
+        }
+        if(next_block < end_block_idx) {
+            kv_offset0 = kv_offset1;
+            if(nnext_block < end_block_idx) {
+            kv_offset1 = block_table[nnext_block];
+            }
+            token_idx = next_block * BLOCK_SIZE + physical_block_offset;
+            const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+            ptr_logits = logits + token_idx - start_token_idx;
+            if constexpr (IS_BLOCK_SPARSE) {
+            int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
+            if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+                ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+                    if(next_block == num_seq_blocks - 1) {
+                    #pragma unroll
+                    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                    }
+                    }
+                } else {
+                    #pragma unroll
+                    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                    }
+                    }
+                }
+            }
+            } else {
+            if(next_block == num_seq_blocks - 1) {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+                }
+            } else {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+                }
+            }
+            }
+        }
+        
+      if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          continue;
+        }
+      }
+      
+      token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+        if(token_idx + i * NUM_COLS_PER_ITER < seq_len && i * NUM_COLS_PER_ITER + physical_block_offset < BLOCK_SIZE) {
+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
+	  #pragma unroll
+          for(int j = 0; j < V_VEC_SIZE; j+=2) {
+            atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
+          }
+        }
+      } 
     }
-    accs[i] = acc;
   }
-
-  // NOTE(woosuk): A barrier is required because the shared memory space for
-  // logits is reused for the output.
   __syncthreads();
-
-  // Perform reduction across warps.
+  //need move
   float* out_smem = reinterpret_cast<float*>(shared_mem);
-#pragma unroll
-  for (int i = NUM_WARPS; i > 1; i /= 2) {
-    int mid = i / 2;
-    // Upper warps write to shared memory.
-    if (warp_idx >= mid && warp_idx < i) {
-      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
-#pragma unroll
-      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
-          dst[row_idx] = accs[i];
-        }
-      }
-    }
-    __syncthreads();
+  for(int i = threadIdx.x; i < NUM_WARPS * NUM_COLS_PER_ITER * HEAD_SIZE; i += blockDim_x) {
+    out_smem[i] = 0.0f;
+  }
+  __syncthreads();
 
-    // Lower warps update the output.
-    if (warp_idx < mid) {
-      const float* src = &out_smem[warp_idx * HEAD_SIZE];
-#pragma unroll
-      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
-          accs[i] += src[row_idx];
-        }
-      }
+  if(lane < NUM_VALID_THREAD) {
+    float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
+    for(int i = 0; i < V_VEC_SIZE; i++) {
+      ptr_out_smem[i] = accs[i];
     }
-    __syncthreads();
   }
-
-  // Write the final output.
-  if (warp_idx == 0) {
-    scalar_t* out_ptr =
-        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
-        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
-#pragma unroll
-    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
-      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
-      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
-        from_float(*(out_ptr + row_idx), accs[i]);
+  __syncthreads();
+  scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                        + head_idx * max_num_partitions * HEAD_SIZE
+                        + partition_idx * HEAD_SIZE;
+  if(threadIdx.x < HEAD_SIZE) {
+    int length = NUM_LANE * HEAD_SIZE;
+      float r = 0;
+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
+        r += out_smem[i];
       }
-    }
+      from_float(*(out_ptr + threadIdx.x), r);
   }
 }
 
-// Grid: (num_heads, num_seqs, 1).
-template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
-          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
-          bool IS_BLOCK_SPARSE>
-__global__ void paged_attention_v1_kernel(
-    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
-    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size/x, block_size, x]
-    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size, block_size]
-    const int num_kv_heads,               // [num_heads]
-    const float scale,
-    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
-    const int* __restrict__ seq_lens,      // [num_seqs]
-    const int max_num_blocks_per_seq,
-    const float* __restrict__ alibi_slopes,  // [num_heads]
-    const int q_stride, const int kv_block_stride, const int kv_head_stride,
-    const float* k_scale, const float* v_scale, const int tp_rank,
-    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-                         KV_DTYPE, IS_BLOCK_SPARSE>(
-      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
-      v_cache, num_kv_heads, scale, block_tables, seq_lens,
-      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
-      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
-      blocksparse_vert_stride, blocksparse_block_size,
-      blocksparse_head_sliding_step);
-}
-
-// Grid: (num_heads, num_seqs, max_num_partitions).
 template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
           int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
           bool IS_BLOCK_SPARSE,
-          int PARTITION_SIZE>
-__global__ void paged_attention_v2_kernel(
+          int PARTITION_SIZE = 0>  // Zero means no partitioning.
+__device__ __forceinline__ void paged_attention_kernel_32N(
     float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
-    float* __restrict__ max_logits,       // [num_seqs, num_heads,
-                                          // max_num_partitions]
-    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
-                                          // max_num_partitions, head_size]
+    float* __restrict__ max_logits,  // [num_seqs, num_heads,
+                                     // max_num_partitions]
+    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,head_size]
     const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
-    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size/x, block_size, x]
-    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
-                                          // head_size, block_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads, head_size/x, block_size, x]->[num_blocks, num_kv_heads, head_size/16, block_size, 16]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads, head_size, block_size]->[num_blocks, num_kv_heads, block_size, head_size]
     const int num_kv_heads,               // [num_heads]
     const float scale,
     const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
@@ -549,16 +838,1186 @@ __global__ void paged_attention_v2_kernel(
     const int max_num_blocks_per_seq,
     const float* __restrict__ alibi_slopes,  // [num_heads]
     const int q_stride, const int kv_block_stride, const int kv_head_stride,
-    const float* k_scale, const float* v_scale, const int tp_rank,
-    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
-  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
-                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
-      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
-      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
-      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
-      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
-      blocksparse_head_sliding_step);
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
+  const int seq_idx = blockIdx.y;
+  const int partition_idx = blockIdx.z;
+  // const int max_num_partitions = gridDim.z;
+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
+  const int seq_len = seq_lens[seq_idx];
+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
+    // No work to do. Terminate the thread block.
+    return;
+  }
+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
+  const int num_blocks_per_partition = USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+  constexpr int THREAD_GROUP_SIZE = MAX(MXWARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
+  // assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, MXWARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS >> 6;
+  const int thread_idx = threadIdx.x;
+  const int warp_idx = thread_idx >> 6;
+  const int lane = thread_idx & 63;
+
+  const int head_idx = blockIdx.x;
+  //const int num_heads = gridDim.x;
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+  // A vector type to store a part of a key or a query.
+  // The vector size is configured in such a way that the threads in a thread group
+  // fetch or compute 16 bytes at a time.
+  // For example, if the size of a thread group is 4 and the data type is half,
+  // then the vector size is 16 / (4 * sizeof(half)) == 2.
+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
+#ifdef ENABLE_FP8_E5M2
+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+#endif
+  
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in the group
+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
+  // th vectors of the query, and so on.
+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
+    Q_vec_l dst;
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE); 
+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
+  }
+  __syncthreads(); // TODO(naed90): possible speedup if this is replaced with a memory wall right before we use q_vecs
+  // Memory planning.
+  extern __shared__ char shared_mem[];
+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
+  float* logits = reinterpret_cast<float*>(shared_mem);
+  // Workspace for reduction.
+  __shared__ float red_smem[2 * NUM_WARPS];
+
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16;
+  float qk_max = -FLT_MAX;
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
+  // blocksparse specific vars
+  int bs_block_offset;
+  int q_bs_block_id;
+  if constexpr (IS_BLOCK_SPARSE) {
+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
+    // blocksparse_block_size);
+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
+    if (blocksparse_head_sliding_step >= 0)
+      // sliding on q heads
+      bs_block_offset =
+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
+    else
+      // sliding on kv heads
+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
+                            (-blocksparse_head_sliding_step) +
+                        1;
+  }
+  int block_idx0 = start_block_idx + warp_idx;
+  int kv_offset0, kv_offset1;
+  K_vec load_k[NUM_VECS_PER_THREAD];
+  K_vec compute_k[NUM_VECS_PER_THREAD];
+  int k_offset[NUM_VECS_PER_THREAD];
+  kv_offset0 = block_table[block_idx0];
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  #pragma unroll
+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
+    const int offset1 = vec_idx >> 4;
+    const int offset2 = vec_idx & 15;
+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
+  }
+  const cache_t* ptr_k_cache = k_cache + kv_head_idx * kv_head_stride;
+  if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if(is_remote || is_local) {
+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+        #pragma unroll
+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }    
+      } 
+  } else {
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+        const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1) ;
+        const int token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+        const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+    #pragma unroll
+        for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+          load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+        } 
+    }
+  }
+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {  
+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        compute_k[j] = load_k[j];
+      }
+      if(block_idx < end_block_idx - NUM_WARPS) {
+          kv_offset0 = kv_offset1;
+	  int nblock_idx = block_idx + NUM_WARPS;
+          if(block_idx < end_block_idx - (NUM_WARPS << 1)) {
+            kv_offset1 = block_table[block_idx + (NUM_WARPS<<1)];
+          }
+          if constexpr (IS_BLOCK_SPARSE) {
+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
+            const bool is_remote =
+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+            const bool is_local =
+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+            if(is_remote || is_local) {
+              const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+              #pragma unroll NUM_VECS_PER_THREAD
+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+              }  
+            }
+          } else {
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+            #pragma unroll NUM_VECS_PER_THREAD
+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }
+      }
+      if constexpr (IS_BLOCK_SPARSE) {
+        const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        const bool is_remote =
+            ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+        const bool is_local =
+            (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+        if (!is_remote && !is_local) {
+            for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset =
+                (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+            const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+            if (thread_group_offset == 0) {
+                // NOTE(linxihui): assign very large number to skipped tokens to
+                // avoid contribution to the sumexp softmax normalizer. This will
+                // not be used at computing sum(softmax*v) as the blocks will be
+                // skipped.
+                logits[token_idx - start_token_idx] = -FLT_MAX;
+            }
+            }
+            continue;
+        }
+    }
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
+      float qk = 0.0f;
+      v2f f2_qk = {0,0};
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
+      }
+      qk = f2_qk[0] + f2_qk[1];
+      #pragma unroll
+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+        qk += MXVLLM_SHFL_XOR_SYNC(qk, mask);
+      }
+      qk = scale * qk;
+      // Add the ALiBi bias if slopes are given.
+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
+
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= seq_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
+      }
+    }
+  }
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+  #pragma unroll
+  for (int mask = MXWARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  __syncthreads();
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+  #pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  qk_max = MXVLLM_SHFL_SYNC(qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = __builtin_expf(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
+
+  // Compute softmax.
+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = logits[i];
+    val *= inv_sum;
+    logits[i] = convert<cache_t>(val);
+  }
+  __syncthreads();
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (USE_PARTITIONING && thread_idx == 0) {
+    float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
+                                       + head_idx * max_num_partitions
+                                       + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
+                                   + head_idx * max_num_partitions
+                                   + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
+  constexpr int NUM_COLS_PER_ITER = MAX(MXWARP_SIZE / NUM_V_VECS_PER_THREAD , 1); 
+  constexpr int NUM_LGT_PER_COL = BLOCK_SIZE / NUM_COLS_PER_ITER; 
+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
+  V_vec v_vecs[NUM_LGT_PER_COL];
+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
+  float accs[V_VEC_SIZE];
+  float reg_log[NUM_LGT_PER_COL];
+  float reg_prev_log[NUM_LGT_PER_COL];
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    accs[i] = 0.0f;
+  }
+  int token_idx, kv_stride, block_offset;
+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
+  kv_offset0 = block_table[block_idx0];
+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
+  if(block_idx0 + NUM_WARPS < end_block_idx) {
+    kv_offset1 = block_table[block_idx0 + NUM_WARPS];
+  }
+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE; 
+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+  float *ptr_logits = logits + token_idx - start_token_idx;
+  if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+                const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+            }
+        }
+    } else {
+          #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+            if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+          }
+    }
+  for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    int next_block = block_idx + NUM_WARPS;
+    int nnext_block = next_block + NUM_WARPS;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      v_vecs[i] = v_prev_vecs[i];
+      reg_log[i] = reg_prev_log[i];
+    }
+    if(next_block < end_block_idx) {
+      kv_offset0 = kv_offset1;
+      if(nnext_block < end_block_idx) {
+        kv_offset1 = block_table[nnext_block];
+      }
+      token_idx = next_block * BLOCK_SIZE + physical_block_offset;
+      const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+      ptr_logits = logits + token_idx - start_token_idx;
+      if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+            }
+        }
+        } else  {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+        }
+    }
+    if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          continue;
+        }
+    }
+    
+    token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    float *ptr_logits = logits + token_idx - start_token_idx;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+        scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
+        #pragma unroll
+        for(int j = 0; j < V_VEC_SIZE; j+=2) {
+          atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
+        }
+      }
+    } 
+  }
+  __syncthreads();
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+  float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    ptr_out_smem[i] = accs[i];
+  }
+   __syncthreads();
+  
+  scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                        + head_idx * max_num_partitions * HEAD_SIZE
+                        + partition_idx * HEAD_SIZE;
+  if(threadIdx.x < HEAD_SIZE) {
+    int length = NUM_LANE * HEAD_SIZE;
+      float r = 0;
+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
+        r += out_smem[i];
+      }
+      from_float(*(out_ptr + threadIdx.x), r);
+  }
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+	  int PARTITION_SIZE = 512>  // Zero means no partitioning.
+__device__ __forceinline__ void paged_attention_kernel_32N_final(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,  // [num_seqs, num_heads,
+                                     // max_num_partitions]
+    int* __restrict__ block_count,          // [num_seqs, num_heads]
+    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,
+                                 // head_size]
+    scalar_t* __restrict__ final_out,      // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads, head_size/x, block_size, x]->[num_blocks, num_kv_heads, head_size/16, block_size, 16]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads, head_size, block_size]->[num_blocks, num_kv_heads, block_size, head_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step, const int max_num_partitions,
+    const int blockDim_x,
+    const int num_heads,
+    const int grid_dim_y,
+    const bool count_init_once) {
+  const int seq_idx = blockIdx.y;
+  const int partition_idx = blockIdx.z;
+  // const int max_num_partitions = gridDim.z;
+  // const int blockDim_x = blockDim.x;
+  const int head_idx = blockIdx.x;
+  // const int num_heads = gridDim.x;
+  // const int grid_dim_y = gridDim.y;
+  const int seq_len = seq_lens[seq_idx];
+
+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
+  const int num_blocks_per_partition = PARTITION_SIZE / BLOCK_SIZE;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx = partition_idx * num_blocks_per_partition;
+  const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx = MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+
+  constexpr int THREAD_GROUP_SIZE = MAX(MXWARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS = NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE divides NUM_THREADS
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, MXWARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS >> 6;
+  const int thread_idx = threadIdx.x;
+  const int warp_idx = thread_idx >> 6;
+  const int lane = thread_idx & 63;
+
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  const float alibi_slope = alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+  int offset0 = seq_idx * num_heads;
+  int offset1 = offset0 * HEAD_SIZE;
+  int offset2 = head_idx * HEAD_SIZE;
+  int offset3 = head_idx * max_num_partitions;
+  int offset4 = offset0 * max_num_partitions;
+  int offset5 = seq_idx * max_num_blocks_per_seq;
+  int num_warps2 = NUM_WARPS << 1;
+
+  // The vector size is configured in such a way that the threads in a thread group
+  // fetch or compute 16 bytes at a time.
+  // For example, if the size of a thread group is 4 and the data type is half,
+  // then the vector size is 16 / (4 * sizeof(half)) == 2.
+  constexpr int VEC_SIZE = 16 / sizeof(scalar_t);
+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec_l = typename FloatVec<Q_vec>::Type;
+#ifdef ENABLE_FP8_E5M2
+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+#endif
+
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in the group
+  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
+  // th vectors of the query, and so on.
+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+  __shared__ Q_vec_l q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    Q_vec reg = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+    convert<scalar_t, Q_vec, Q_vec_l, VEC_SIZE>(reg, q_vecs[thread_group_offset][i]);
+  }
+  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
+                    // memory wall right before we use q_vecs
+
+  // Memory planning.
+  extern __shared__ char shared_mem[];
+  float* red_smem = reinterpret_cast<float*>(shared_mem);
+  int * block_table_smem = reinterpret_cast<int*>(red_smem + num_warps2);
+  float * logits = reinterpret_cast<float*>(block_table_smem + 512 + num_warps2);
+
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16;
+  float qk_max = -FLT_MAX;
+
+  // blocksparse specific vars
+  int bs_block_offset;
+  int q_bs_block_id;
+  if constexpr (IS_BLOCK_SPARSE) {
+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
+    // blocksparse_block_size);
+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
+    if (blocksparse_head_sliding_step >= 0)
+      // sliding on q heads
+      bs_block_offset =
+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
+    else
+      // sliding on kv heads
+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
+                            (-blocksparse_head_sliding_step) +
+                        1;
+  }
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + offset5 + start_block_idx;
+  //load block_table to share memory
+  for(int i = threadIdx.x; i < num_blocks; i += blockDim_x){
+    block_table_smem[i] = block_table[i];
+  }
+  int block_idx0 = start_block_idx + warp_idx;
+  __syncthreads();
+  int kv_offset0, kv_offset1;
+  K_vec load_k[NUM_VECS_PER_THREAD];
+  K_vec compute_k[NUM_VECS_PER_THREAD];
+  int k_offset[NUM_VECS_PER_THREAD];
+  kv_offset0 = block_table_smem[block_idx0 - start_block_idx];
+  kv_offset1 = block_table_smem[block_idx0 + NUM_WARPS - start_block_idx];
+  #pragma unroll
+  for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+    const int vec_idx = (thread_group_offset + j * THREAD_GROUP_SIZE)*VEC_SIZE;
+    const int offset1 = vec_idx >> 4;
+    const int offset2 = vec_idx & 15;
+    k_offset[j] = offset1 * BLOCK_SIZE * x + offset2;
+  }
+
+  const cache_t* ptr_k_cache = k_cache + kv_head_idx * kv_head_stride;
+  if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if(is_remote || is_local) {
+          for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+            const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+        #pragma unroll
+            for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+              load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }    
+      } 
+  } else {
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+      const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+    #pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+      }
+    }
+  }
+  
+  for (int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    for(int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset = (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        compute_k[j] = load_k[j];
+      }
+      if(block_idx < end_block_idx - NUM_WARPS) {
+          kv_offset0 = kv_offset1;
+	  int nblock_idx = block_idx + NUM_WARPS;
+          kv_offset1 = block_table_smem[block_idx + num_warps2 - start_block_idx];
+          if constexpr (IS_BLOCK_SPARSE) {
+            const int k_bs_block_id = nblock_idx * BLOCK_SIZE / blocksparse_block_size;
+            const bool is_remote =
+                ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+            const bool is_local =
+                (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+            if(is_remote || is_local) {
+              const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+              #pragma unroll NUM_VECS_PER_THREAD
+              for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                  load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+              }  
+            }
+          } else {
+            const cache_t* k_ptr = ptr_k_cache + static_cast<int64_t>(kv_offset0) * kv_block_stride + physical_block_offset * x;
+            #pragma unroll NUM_VECS_PER_THREAD
+            for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                load_k[j] = *reinterpret_cast<const K_vec*>(k_ptr + k_offset[j]);
+            }
+          }
+      }
+      if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if (!is_remote && !is_local) {
+        for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+          const int physical_block_offset =
+              (thread_group_idx + i * MXWARP_SIZE) & (BLOCK_SIZE - 1);
+          const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+          if (thread_group_offset == 0) {
+            // NOTE(linxihui): assign very large number to skipped tokens to
+            // avoid contribution to the sumexp softmax normalizer. This will
+            // not be used at computing sum(softmax*v) as the blocks will be
+            // skipped.
+            logits[token_idx - start_token_idx] = -FLT_MAX;
+          }
+        }
+        continue;
+      }
+    }
+
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Compute the parallel products for Q*K^T (treat vector lanes separately).
+      float qk = 0.0f;
+      v2f f2_qk = {0,0};
+      #pragma unroll
+      for(int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        atten_dot<Q_vec_l, K_vec, v2f, VEC_SIZE>(q_vecs[thread_group_offset][j], compute_k[j],f2_qk);
+      }
+      qk = f2_qk[0] + f2_qk[1];
+      #pragma unroll
+      for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+        qk += MXVLLM_SHFL_XOR_SYNC(qk, mask);
+      }
+
+      qk = scale * qk;
+      // Add the ALiBi bias if slopes are given.
+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
+
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= seq_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
+      }
+    }
+  }
+
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+#pragma unroll
+  for (int mask = MXWARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  __syncthreads();
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    qk_max = fmaxf(qk_max, MXVLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  qk_max = MXVLLM_SHFL_SYNC(qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = __builtin_expf(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
+
+  // Compute softmax.
+  const float inv_sum = __builtin_mxc_rcpf(exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = logits[i];
+    val *= inv_sum;
+    logits[i] = convert<cache_t>(val);
+  }
+  __syncthreads();
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (thread_idx == 0) {
+    float* max_logits_ptr = max_logits + offset4
+                                       + offset3
+                                       + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + offset4
+                                   + offset3
+                                   + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+
+  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);
+  constexpr int NUM_V_VECS_PER_THREAD = HEAD_SIZE / V_VEC_SIZE; 
+  constexpr int NUM_COLS_PER_ITER = MAX(MXWARP_SIZE / NUM_V_VECS_PER_THREAD , 1); 
+  constexpr int NUM_LGT_PER_COL = BLOCK_SIZE / NUM_COLS_PER_ITER; 
+  constexpr int NUM_LANE = NUM_WARPS * NUM_COLS_PER_ITER;
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  const int physical_block_offset = lane / NUM_V_VECS_PER_THREAD;
+  const int laneid = lane % NUM_V_VECS_PER_THREAD;
+  V_vec v_vecs[NUM_LGT_PER_COL];
+  V_vec v_prev_vecs[NUM_LGT_PER_COL];
+  float accs[V_VEC_SIZE];
+  float reg_log[NUM_LGT_PER_COL];
+  float reg_prev_log[NUM_LGT_PER_COL];
+
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    accs[i] = 0.0f;
+  }
+  int token_idx, kv_stride, block_offset;
+  kv_stride = BLOCK_SIZE * HEAD_SIZE ;
+  kv_offset0 = block_table_smem[block_idx0 - start_block_idx];
+  block_offset = NUM_COLS_PER_ITER * HEAD_SIZE;
+  kv_offset1 = block_table_smem[block_idx0 + NUM_WARPS - start_block_idx];
+  token_idx = block_idx0 * BLOCK_SIZE + physical_block_offset;
+  const cache_t *v_ptr0 = v_cache + kv_head_idx * kv_stride + physical_block_offset * HEAD_SIZE;
+  const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+  float *ptr_logits = logits + token_idx - start_token_idx;
+
+  if constexpr (IS_BLOCK_SPARSE) {
+      int v_bs_block_id = block_idx0 * BLOCK_SIZE / blocksparse_block_size;
+      if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+          ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          #pragma unroll
+          for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+              if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+              }
+          }
+      }
+  } else {
+        #pragma unroll
+        for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+          if(token_idx + i * NUM_COLS_PER_ITER < seq_len ) {
+            const int idx = laneid * V_VEC_SIZE + i * block_offset;
+            v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+            reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+          }
+        }
+    }
+
+  for(int block_idx = block_idx0; block_idx < end_block_idx; block_idx += NUM_WARPS) {
+    int next_block = block_idx + NUM_WARPS;
+    int nnext_block = next_block + NUM_WARPS;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      v_vecs[i] = v_prev_vecs[i]; reg_log[i] = reg_prev_log[i];
+    }
+    if(next_block < end_block_idx) {
+      kv_offset0 = kv_offset1;
+      kv_offset1 = block_table_smem[nnext_block - start_block_idx];
+      token_idx = next_block * BLOCK_SIZE + physical_block_offset;
+      const cache_t* v_ptr = v_ptr0 + static_cast<int64_t>(kv_offset0) * kv_block_stride;
+      ptr_logits = logits + token_idx - start_token_idx;
+      if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = next_block * BLOCK_SIZE / blocksparse_block_size;
+        if (((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) ||
+            ((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+              if(next_block == num_seq_blocks - 1) {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+                        const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                        v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                        reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                    }
+                }
+              } else {
+                #pragma unroll
+                for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                    const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                    v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                    reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+                }
+              }
+        }
+        } else {
+          if(next_block == num_seq_blocks - 1) {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+                if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+                const int idx = laneid * V_VEC_SIZE + i * block_offset;
+                v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+                reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+              }
+            }
+          } else {
+            #pragma unroll
+            for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+              const int idx = laneid * V_VEC_SIZE + i * block_offset;
+              v_prev_vecs[i] = *reinterpret_cast<const V_vec*>(v_ptr + idx);
+              reg_prev_log[i] = ptr_logits[i * NUM_COLS_PER_ITER];
+            }
+          }
+        }
+    }
+    if constexpr (IS_BLOCK_SPARSE) {
+        int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+        if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+            !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+          continue;
+        }
+    }
+
+    token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    float *ptr_logits = logits + token_idx - start_token_idx;
+    for(int i = 0; i < NUM_LGT_PER_COL; i++) {
+      if(token_idx + i * NUM_COLS_PER_ITER < seq_len) {
+        scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vecs[i]);
+        #pragma unroll
+        for(int j = 0; j < V_VEC_SIZE; j+=2) {
+          atten_mul_opt2(v_vec_ptr, reg_log[i], j, accs[j],accs[j + 1]);
+        }
+      }
+    } 
+  }
+
+  __syncthreads();
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+  float*ptr_out_smem = out_smem + warp_idx * HEAD_SIZE*NUM_COLS_PER_ITER + physical_block_offset * HEAD_SIZE + laneid* V_VEC_SIZE;
+  #pragma unroll
+  for(int i = 0; i < V_VEC_SIZE; i++) {
+    ptr_out_smem[i] = accs[i];
+  }
+   __syncthreads();
+  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);
+
+  if(partition_idx * PARTITION_SIZE < seq_len) {
+    scalar_t* out_ptr = out + (offset4 + offset3 + partition_idx) * HEAD_SIZE;
+    if(threadIdx.x < HEAD_SIZE) {
+      int length = NUM_LANE * HEAD_SIZE;
+      float r = 0;
+      for(int i = threadIdx.x; i < length; i += HEAD_SIZE) {
+        r += out_smem[i];
+      }
+      from_float(*(out_ptr + threadIdx.x), r);
+    }
+  }
+
+  __syncthreads();
+  bool last_block = false;
+  if(threadIdx.x == blockDim_x - 1) {
+    if(atomicAdd(block_count + head_idx * grid_dim_y + seq_idx, 1) == max_num_partitions - 1) {
+      last_block = true;
+    }
+  }
+  if (__syncthreads_or(last_block)) {
+      if(count_init_once) {
+        if(threadIdx.x == blockDim_x - 2){
+          *(block_count + head_idx * grid_dim_y + seq_idx) = 0;
+        }
+      }
+      if(num_partitions == 1) {
+        scalar_t* out_ptr = final_out + offset1 + offset2;
+        const scalar_t* tmp_out_ptr = out + (offset4 + offset3) * HEAD_SIZE;
+        V_vec* ptr_vec_out = (V_vec*)out_ptr;
+        V_vec* ptr_vec_in = (V_vec*)tmp_out_ptr;
+        int num = HEAD_SIZE / V_VEC_SIZE;
+        for (int i = threadIdx.x; i < num; i += blockDim_x) {
+          ptr_vec_out[i] = ptr_vec_in[i];
+        }
+	return;
+      }
+      // Load max logits to shared memory.
+      float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
+      float * red_smem = shared_max_logits + (num_partitions << 1 );
+      const float* max_logits_ptr = max_logits + offset4 + offset3;
+      float max_logit = -FLT_MAX;
+      for (int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        const float l = max_logits_ptr[i];
+        shared_max_logits[i] = l;
+      }
+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        max_logit = fmaxf(max_logit, shared_max_logits[i]);
+      }
+      __syncthreads();
+      // Get the global max logit.
+      // Reduce within the warp.
+      #pragma unroll
+      for (int mask = MXWARP_SIZE / 2; mask >= 1; mask /= 2) {
+        max_logit = fmaxf(max_logit, MXVLLM_SHFL_XOR_SYNC(max_logit, mask));
+      }
+      
+      if (lane == 0) {
+        red_smem[warp_idx] = max_logit;
+      }
+      __syncthreads();
+      // Reduce across warps.
+      max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+    #pragma unroll
+      for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+        max_logit = fmaxf(max_logit, MXVLLM_SHFL_XOR_SYNC(max_logit, mask));
+      }
+      // Broadcast the max value to all threads.
+      max_logit = MXVLLM_SHFL_SYNC(max_logit, 0);
+
+      // Load rescaled exp sums to shared memory.
+      float* shared_exp_sums = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+      const float* exp_sums_ptr = exp_sums + offset4 + offset3;
+
+      float global_exp_sum = 0.0f;  
+      float * out_sm_ptr = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions * 2);
+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        out_sm_ptr[i] = exp_sums_ptr[i];
+      }
+
+      for (int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+        float l = shared_max_logits[i];
+        float rescaled_exp_sum = out_sm_ptr[i] * __builtin_expf(l - max_logit);
+        global_exp_sum += rescaled_exp_sum;
+        shared_exp_sums[i] = rescaled_exp_sum;
+      }
+      __syncthreads();
+
+      global_exp_sum = mxblock_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum);
+      const float inv_global_exp_sum = __builtin_mxc_rcpf(global_exp_sum + 1e-6f);
+      
+      for(int i = threadIdx.x; i < num_partitions; i += blockDim_x) {
+          shared_exp_sums[i] = shared_exp_sums[i] * inv_global_exp_sum;
+      }
+
+      // Aggregate tmp_out to out.
+      scalar_t* out_ptr = final_out + offset1 + offset2;
+      const scalar_t* tmp_out_ptr = out + (offset1 + offset2) * max_num_partitions;
+      scalar_t * out_sm_ptr_2 = reinterpret_cast<scalar_t*>(out_sm_ptr);
+      int buffer_size = num_partitions * HEAD_SIZE / V_VEC_SIZE;
+      for(int i = threadIdx.x; i < buffer_size; i += blockDim_x) {
+          int offset = i * V_VEC_SIZE;
+          V_vec reg = *reinterpret_cast<const V_vec*>(tmp_out_ptr + offset);
+          *(V_vec *)(out_sm_ptr_2  + offset) = reg;
+      }
+      __syncthreads();
+      
+      if(threadIdx.x < HEAD_SIZE) {
+        scalar_t * ptr_sm_out_ptr = out_sm_ptr_2 + threadIdx.x;
+        float acc = 0.0f;
+        int num_partitions2 = num_partitions >> 1 << 1;
+        int j = 0;
+        v2f vacc; vacc[0] = 0.0f; vacc[1] = 0.0f;
+        for(; j < num_partitions2; j += 2) {
+          v2f va; 
+          scalar_t a0, a1;
+          a0 = *(ptr_sm_out_ptr); ptr_sm_out_ptr += HEAD_SIZE;
+          a1 = *(ptr_sm_out_ptr); ptr_sm_out_ptr += HEAD_SIZE;
+          to_v2f(a0,a1,va);
+          v2f vb; 
+          vb[0] = shared_exp_sums[j]; vb[1] = shared_exp_sums[j + 1];
+          vacc = __builtin_mxc_pk_fma_f32(va, vb, vacc);
+        }
+        acc = vacc[0] + vacc[1];
+        for (; j < num_partitions; ++j) {
+          acc += to_float(*ptr_sm_out_ptr) * shared_exp_sums[j];
+          ptr_sm_out_ptr += HEAD_SIZE;
+        }
+        from_float(out_ptr[threadIdx.x], acc);
+      }
+  }
+}
+
+// Grid: (num_heads, num_seqs, 1).
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE>
+__global__ void paged_attention_v1_kernel(
+    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank,
+    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
+    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE>(
+      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
+      v_cache, num_kv_heads, scale, block_tables, seq_lens,
+      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
+      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
+      blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step);
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE>
+__global__ void paged_attention_v1_32N_kernel(
+    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
+  paged_attention_kernel_32N<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE>(
+      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,
+      v_cache, num_kv_heads, scale, block_tables, seq_lens,
+      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,
+      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,
+      blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step,max_num_partitions,num_heads);
+}
+
+// Grid: (num_heads, num_seqs, max_num_partitions).
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+          int PARTITION_SIZE>
+__global__ void paged_attention_v2_kernel(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank,
+    const int blocksparse_local_blocks, const int blocksparse_vert_stride,
+    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
+      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step);
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+          int PARTITION_SIZE>
+__global__ void paged_attention_v2_32N_kernel(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,const int num_heads) {
+  paged_attention_kernel_32N<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
+      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,
+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step,max_num_partitions,num_heads);
+}
+
+template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
+          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,
+          bool IS_BLOCK_SPARSE,
+          int PARTITION_SIZE>
+__global__ void paged_attention_v2_kernel_final(
+    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits,       // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    int* __restrict__ block_count,          // [num_seqs, num_heads]
+    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    scalar_t* __restrict__ final_out,      // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
+    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads,               // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ seq_lens,      // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes,  // [num_heads]
+    const int q_stride, const int kv_block_stride, const int kv_head_stride,
+    const float* k_scale, const float* v_scale, const int tp_rank, const int blocksparse_local_blocks,
+    const int blocksparse_vert_stride, const int blocksparse_block_size,
+    const int blocksparse_head_sliding_step,const int max_num_partitions,
+    const int blockDim_x,
+    const int num_heads,
+    const int grid_dim_y, const bool count_init_once) {
+  paged_attention_kernel_32N_final<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
+                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(
+      exp_sums, max_logits, block_count, tmp_out, final_out, q, k_cache, v_cache, num_kv_heads, scale,
+      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,
+      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,
+      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,
+      blocksparse_head_sliding_step,max_num_partitions,blockDim_x, num_heads,grid_dim_y,count_init_once);
 }
 
 // Grid: (num_heads, num_seqs).
@@ -581,11 +2040,9 @@ __global__ void paged_attention_v2_reduce_kernel(
   const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);
   if (num_partitions == 1) {
     // No need to reduce. Only copy tmp_out to out.
-    scalar_t* out_ptr =
-        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
-    const scalar_t* tmp_out_ptr =
-        tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
-        head_idx * max_num_partitions * HEAD_SIZE;
+    scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+    const scalar_t* tmp_out_ptr = tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                                          + head_idx * max_num_partitions * HEAD_SIZE;
     for (int i = threadIdx.x; i < HEAD_SIZE; i += blockDim.x) {
       out_ptr[i] = tmp_out_ptr[i];
     }
@@ -604,9 +2061,8 @@ __global__ void paged_attention_v2_reduce_kernel(
 
   // Load max logits to shared memory.
   float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
-  const float* max_logits_ptr = max_logits +
-                                seq_idx * num_heads * max_num_partitions +
-                                head_idx * max_num_partitions;
+  const float* max_logits_ptr = max_logits + seq_idx * num_heads * max_num_partitions
+                                           + head_idx * max_num_partitions;
   float max_logit = -FLT_MAX;
   for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {
     const float l = max_logits_ptr[i];
@@ -635,11 +2091,9 @@ __global__ void paged_attention_v2_reduce_kernel(
   max_logit = VLLM_SHFL_SYNC(max_logit, 0);
 
   // Load rescaled exp sums to shared memory.
-  float* shared_exp_sums =
-      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
-  const float* exp_sums_ptr = exp_sums +
-                              seq_idx * num_heads * max_num_partitions +
-                              head_idx * max_num_partitions;
+  float* shared_exp_sums = reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+  const float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions
+                                       + head_idx * max_num_partitions;
   float global_exp_sum = 0.0f;
   for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {
     float l = shared_max_logits[i];
@@ -652,17 +2106,14 @@ __global__ void paged_attention_v2_reduce_kernel(
   const float inv_global_exp_sum = __fdividef(1.0f, global_exp_sum + 1e-6f);
 
   // Aggregate tmp_out to out.
-  const scalar_t* tmp_out_ptr =
-      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
-      head_idx * max_num_partitions * HEAD_SIZE;
-  scalar_t* out_ptr =
-      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+  const scalar_t* tmp_out_ptr = tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE
+                                        + head_idx * max_num_partitions * HEAD_SIZE;
+  scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
 #pragma unroll
   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {
     float acc = 0.0f;
     for (int j = 0; j < num_partitions; ++j) {
-      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *
-             inv_global_exp_sum;
+      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] * inv_global_exp_sum;
     }
     from_float(out_ptr[i], acc);
   }
diff --git a/csrc/attention/attention_utils.cuh b/csrc/attention/attention_utils.cuh
index 826b0edff..176675e15 100644
--- a/csrc/attention/attention_utils.cuh
+++ b/csrc/attention/attention_utils.cuh
@@ -28,7 +28,7 @@ namespace vllm {
 
 // Q*K^T operation.
 template <int THREAD_GROUP_SIZE, typename Vec, int N>
-inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
+__forceinline__ __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
   using A_vec = typename FloatVec<Vec>::Type;
   // Compute the parallel products for Q*K^T (treat vector lanes separately).
   A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);
@@ -49,7 +49,7 @@ inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {
 template <typename T, int THREAD_GROUP_SIZE>
 struct Qk_dot {
   template <typename Vec, int N>
-  static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
+  static __forceinline__ __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {
     return qk_dot_<THREAD_GROUP_SIZE>(q, k);
   }
 };
diff --git a/csrc/attention/dtype_float16.cuh b/csrc/attention/dtype_float16.cuh
index 3a1815f0e..379c68de8 100644
--- a/csrc/attention/dtype_float16.cuh
+++ b/csrc/attention/dtype_float16.cuh
@@ -1,4 +1,6 @@
 /*
+ * 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+ *
  * Adapted from
  * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
  * and
@@ -22,7 +24,7 @@
 
 #include "attention_generic.cuh"
 #include "dtype_float32.cuh"
-
+#include "cuda_fp16.h"
 #ifdef USE_ROCM
   #include <hip/hip_fp16.h>
 #endif
@@ -69,6 +71,12 @@ struct FloatVec<uint4> {
 
 // Utility functions for type conversions.
 inline __device__ uint32_t h0_h0(uint16_t a) {
+#ifdef USE_MACA
+  uint32_t b;
+  b = a;
+  b = b << 16 | b;
+  return b;
+#else
 #ifndef USE_ROCM
   uint32_t b;
   asm volatile("mov.b32 %0, {%1, %1};" : "=r"(b) : "h"(a));
@@ -82,19 +90,35 @@ inline __device__ uint32_t h0_h0(uint16_t a) {
   tmp.u16[1] = a;
   return tmp.u32;
 #endif
+#endif  // USE_MACA
 }
 
 inline __device__ float half_to_float(uint16_t h) {
   float f;
+#ifdef USE_MACA
+  f = __half2float(*(__half*)&h);
+#else
 #ifndef USE_ROCM
   asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
 #else
   asm volatile("v_cvt_f32_f16 %0, %1;" : "=v"(f) : "v"(h));
 #endif
+#endif // USE_MACA
   return f;
 }
 
 inline __device__ float2 half2_to_float2(uint32_t v) {
+#ifdef USE_MACA
+  uint16_t lo, hi;
+  union {
+    uint32_t u32;
+    uint16_t u16[2];
+  } tmp;
+  tmp.u32 = v;
+  lo = tmp.u16[0];
+  hi = tmp.u16[1];
+  return make_float2(half_to_float(lo), half_to_float(hi));
+#else
 #ifndef USE_ROCM
   uint16_t lo, hi;
   asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo), "=h"(hi) : "r"(v));
@@ -110,6 +134,7 @@ inline __device__ float2 half2_to_float2(uint32_t v) {
   ret.y = half_to_float(tmp.u16[1]);
   return ret;
 #endif
+#endif // USE_MACA
 }
 
 inline __device__ uint16_t float_to_half(float f) {
@@ -117,11 +142,16 @@ inline __device__ uint16_t float_to_half(float f) {
     uint32_t u32;
     uint16_t u16[2];
   } tmp;
+#ifdef USE_MACA
+  __half __tmp = __float2half(f);
+  tmp.u16[0] = *(uint16_t*)&__tmp;
+#else
 #ifndef USE_ROCM
   asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f));
 #else
   asm volatile("v_cvt_f16_f32 %0, %1;\n" : "=v"(tmp.u32) : "v"(f));
 #endif
+#endif // USE MACA
   return tmp.u16[0];
 }
 
@@ -130,6 +160,15 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
     uint32_t u32;
     uint16_t u16[2];
   } tmp;
+#ifdef USE_MACA
+  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
+  __half2 __tmp = __half2(__float2half(f.x), __float2half(f.y));
+  tmp.u32 = *(uint32_t*)&__tmp;
+  #else
+  tmp.u16[0] = float_to_half(f.x);
+  tmp.u16[1] = float_to_half(f.y);
+  #endif
+#else
 #ifndef USE_ROCM
   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
   asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n"
@@ -143,27 +182,42 @@ inline __device__ uint32_t float2_to_half2(float2 f) {
   tmp.u16[0] = float_to_half(f.x);
   tmp.u16[1] = float_to_half(f.y);
 #endif
+#endif // USE_MACA
   return tmp.u32;
 }
 
 // Vector addition.
 inline __device__ uint16_t add(uint16_t a, uint16_t b) {
   uint16_t c;
+#ifdef USE_MACA
+  unsigned short __a=(a);
+  unsigned short __b=(b);
+  __half __d=__hadd(*(__half*)&__a,*(__half*)&__b);
+  (c)=*(unsigned short*)&__d;
+#else
 #ifndef USE_ROCM
   asm volatile("add.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
 #else
   asm volatile("v_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
+#endif // USE_MACA
   return c;
 }
 
 inline __device__ uint32_t add(uint32_t a, uint32_t b) {
   uint32_t c;
+#ifdef USE_MACA
+  unsigned int __a=(a);
+  unsigned int __b=(b);
+  __half2 __d=__hadd2(*(__half2*)&__a,*(__half2*)&__b);
+  (c)=*(unsigned int*)&__d;
+#else
 #ifndef USE_ROCM
   asm volatile("add.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
 #else
   asm volatile("v_pk_add_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
+#endif // USE_MACA
   return c;
 }
 
@@ -208,22 +262,36 @@ inline __device__ Float8_ add(uint4 a, Float8_ fb) {
 template <>
 inline __device__ uint16_t mul(uint16_t a, uint16_t b) {
   uint16_t c;
+#ifdef USE_MACA
+  unsigned short __a=(a);
+  unsigned short __b=(b);
+  __half __d=__hmul(*(__half*)&__a,*(__half*)&__b);
+  (c)=*(unsigned short*)&__d;
+#else
 #ifndef USE_ROCM
   asm volatile("mul.f16 %0, %1, %2;\n" : "=h"(c) : "h"(a), "h"(b));
 #else
   asm volatile("v_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
+#endif // USE_MACA
   return c;
 }
 
 template <>
 inline __device__ uint32_t mul(uint32_t a, uint32_t b) {
   uint32_t c;
+#ifdef USE_MACA
+  unsigned int __a=(a);
+  unsigned int __b=(b);
+  __half2 __d=__hmul2(*(__half2*)&__a,*(__half2*)&__b);
+  (c)=*(unsigned int*)&__d;
+#else
 #ifndef USE_ROCM
   asm volatile("mul.f16x2 %0, %1, %2;\n" : "=r"(c) : "r"(a), "r"(b));
 #else
   asm volatile("v_pk_mul_f16 %0, %1, %2;\n" : "=v"(c) : "v"(a), "v"(b));
 #endif
+#endif //USE_MACA
   return c;
 }
 
@@ -330,6 +398,13 @@ inline __device__ Float8_ mul(uint16_t a, uint4 b) {
 // Vector fused multiply-add.
 inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {
   uint32_t d;
+#ifdef USE_MACA
+  unsigned int __a=(a);
+  unsigned int __b=(b);
+  unsigned int __c=(c);
+  __half2 __d=__hfma2(*(__half2*)&__a,*(__half2*)&__b,*(__half2*)&__c);
+  (d)=*(unsigned int*)&__d;
+#else
 #ifndef USE_ROCM
   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                : "=r"(d)
@@ -339,6 +414,7 @@ inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {
                : "=v"(d)
                : "v"(a), "v"(b), "v"(c));
 #endif
+#endif // USE_MACA
   return d;
 }
 
diff --git a/csrc/attention/paged_attention_v1.cu b/csrc/attention/paged_attention_v1.cu
index 9b3a5c4b1..677385625 100644
--- a/csrc/attention/paged_attention_v1.cu
+++ b/csrc/attention/paged_attention_v1.cu
@@ -45,10 +45,136 @@
           blocksparse_vert_stride, blocksparse_block_size,                  \
           blocksparse_head_sliding_step);
 
+
+#define LAUNCH_PAGED_ATTENTION_V1_32N(HEAD_SIZE)                                \
+  VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \
+      ((void*)vllm::paged_attention_v1_32N_kernel<T, CACHE_T, HEAD_SIZE,        \
+                                              BLOCK_SIZE, NUM_THREADS,      \
+                                              KV_DTYPE, IS_BLOCK_SPARSE>),  \
+      shared_mem_size);                                                     \
+  vllm::paged_attention_v1_32N_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,        \
+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE>   \
+      <<<grid, block, shared_mem_size, stream>>>(                           \
+          out_ptr, query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, \
+          scale, block_tables_ptr, seq_lens_ptr, max_num_blocks_per_seq,    \
+          alibi_slopes_ptr, q_stride, kv_block_stride, kv_head_stride,      \
+          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,                      \
+          blocksparse_vert_stride, blocksparse_block_size,                  \
+          blocksparse_head_sliding_step, 1, num_heads);
+
+template< typename scalar_t>
+__global__ void reshape_k_layout_new(scalar_t * __restrict__ k_buffer, scalar_t* k_output,int num_blocks,int num_kv_heads, int head_size,int block_size, int x,int dst_x) {
+  int k_head_stride = head_size * block_size;
+  scalar_t *ptr_k_buffer = k_buffer + blockIdx.x * k_head_stride;
+  scalar_t *ptr_output = k_output + blockIdx.x * k_head_stride;
+  for(int t = threadIdx.x; t < k_head_stride; t += blockDim.x) {
+    int heightId = t / (block_size * dst_x);
+    int remain = t % (block_size * dst_x);
+    int blockId = remain / dst_x;
+    int wId = remain % dst_x;
+    int inId = heightId * dst_x + wId;
+    int in_y = inId / x;
+    int in_x = inId % x;
+    int inIndex = in_y  * block_size * x + blockId * x + in_x;
+    ptr_output[t] = ptr_k_buffer[inIndex];
+  }
+}
+// [num_blocks, num_kv_heads, head_size, block_size] -->   [num_blocks,  num_kv_heads, block_size,head_size]
+template<typename scalar_t>
+__global__ void reshape_v_layout(scalar_t * __restrict__ v_buffer, scalar_t* v_output,int num_blocks,int num_kv_heads, int head_size,int block_size) {
+      int v_block_stride = head_size * block_size * num_kv_heads;
+      int v_head_stride = head_size * block_size;
+      scalar_t *ptr_in = v_buffer + blockIdx.x * v_block_stride;
+      scalar_t *ptr_output = v_output + blockIdx.x * v_block_stride;
+      for(int t = threadIdx.x; t < v_block_stride; t += blockDim.x) {
+        int num_kv_headIdx = t / v_head_stride;
+        int remain = t % v_head_stride;
+        int headId_H = remain / block_size;
+        remain = remain % block_size;
+        int out_idx = num_kv_headIdx * head_size * block_size + remain * head_size + headId_H;
+        ptr_output[out_idx] = ptr_in[t];
+      }
+}
+
+template<
+  typename CACHE_T,
+  int BLOCK_SIZE>
+void reshape_kv_cache(
+  torch::Tensor& key_cache,
+  torch::Tensor& value_cache,
+  torch::Tensor& key_cache_new_layer,
+  torch::Tensor& value_cache_new_layer,
+  int num_seqs,
+  int num_heads,
+  int head_size,
+  int num_kv_heads) {
+  int kv_block_stride = key_cache.stride(0); // NU ,BLC ,HEAD, HEAD_DIM
+  int kv_head_stride = key_cache.stride(1);
+
+  CACHE_T* key_cache_ptr = reinterpret_cast<CACHE_T*>(key_cache.data_ptr());
+  CACHE_T* value_cache_ptr = reinterpret_cast<CACHE_T*>(value_cache.data_ptr());
+  CACHE_T* key_cache_tmp = reinterpret_cast<CACHE_T*>(key_cache_new_layer.data_ptr());
+  CACHE_T* value_cache_tmp = reinterpret_cast<CACHE_T*>(value_cache_new_layer.data_ptr());
+
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  reshape_k_layout_new<CACHE_T><<<dim3(key_cache.size(0)*num_kv_heads,1,1),dim3(256,1,1),0,stream>>>(key_cache_ptr,key_cache_tmp,key_cache.size(0),num_kv_heads,head_size,BLOCK_SIZE,8,16);
+  reshape_v_layout<CACHE_T><<<dim3(key_cache.size(0),1,1),dim3(256,1,1),0,stream>>>(value_cache_ptr,value_cache_tmp,key_cache.size(0),num_kv_heads,head_size,BLOCK_SIZE);
+}
+#define CALL_RESHAPE_LAUNCHER(CACHE_T, BLOCK_SIZE)       \
+  reshape_kv_cache<CACHE_T, BLOCK_SIZE>( \
+    key_cache,                                                               \
+    value_cache,                                                             \
+    key_cache_new_layer,                                                     \
+    value_cache_new_layer,                                                   \
+    num_seqs,\
+    num_heads,\
+    head_size,\
+    num_kv_heads);
+
+#define CALL_RESHAPE_BLOCK_SIZE(CACHE_T) \
+  switch (block_size) {                                               \
+    case 8:                                                           \
+      CALL_RESHAPE_LAUNCHER(CACHE_T, 8);          \
+      break;                                                          \
+    case 16:                                                          \
+      CALL_RESHAPE_LAUNCHER(CACHE_T, 16);         \
+      break;                                                          \
+    case 32:                                                          \
+      CALL_RESHAPE_LAUNCHER(CACHE_T, 32);         \
+      break;                                                          \
+    default:                                                          \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size);     \
+      break;                                                          \
+  }
+void page_reshape_kv_cache(
+  torch::Tensor& key_cache,       // [num_blocks, num_heads, head_size/x, block_size, x]
+  torch::Tensor& value_cache,     // [num_blocks, num_heads, head_size, block_size]
+  torch::Tensor& key_cache_new_layer, //[num_blocks, num_heads, head_size/16, block_size, 16]
+  torch::Tensor& value_cache_new_layer,//[num_blocks, num_heads, block_size, head_size]
+  int64_t num_seqs,
+  int64_t num_heads,
+  int64_t head_size,
+  int64_t num_kv_heads,               // [num_heads]
+  int64_t block_size,
+  const std::string& kv_cache_dtype) {
+  if (kv_cache_dtype == "auto") {
+    if (sizeof(key_cache.dtype())==4) {
+      CALL_RESHAPE_BLOCK_SIZE(float);
+    } else if (sizeof(key_cache.dtype()) == 2) {
+      CALL_RESHAPE_BLOCK_SIZE(uint16_t);
+    } else {
+      TORCH_CHECK(false, "Unsupported data type: ", key_cache.dtype());
+    }
+  }  else {
+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
+  }
+}
+
 // TODO(woosuk): Tune NUM_THREADS.
 template <typename T, typename CACHE_T, int BLOCK_SIZE,
           vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,
-          int NUM_THREADS = 128>
+          int NUM_THREADS = 256>
 void paged_attention_v1_launcher(
     torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int num_kv_heads, float scale,
@@ -61,12 +187,13 @@ void paged_attention_v1_launcher(
   int num_heads = query.size(1);
   int head_size = query.size(2);
   int max_num_blocks_per_seq = block_tables.size(1);
-  int q_stride = query.stride(0);
-  int kv_block_stride = key_cache.stride(0);
+  int q_stride = query.stride(0);  //num head head_dim 
+  int kv_block_stride = key_cache.stride(0);   // NU ,BLC ,HEAD, HEAD_DIM
   int kv_head_stride = key_cache.stride(1);
 
   [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
   assert(head_size % thread_group_size == 0);
+  assert((head_size & 7) == 0);
 
   // NOTE: alibi_slopes is optional.
   const float* alibi_slopes_ptr =
@@ -87,7 +214,12 @@ void paged_attention_v1_launcher(
   int padded_max_seq_len =
       DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;
   int logits_size = padded_max_seq_len * sizeof(float);
-  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+
+  int V_VEC_SIZE = 16 / sizeof(CACHE_T); 
+  int NUM_V_VECS_PER_THREAD = head_size / V_VEC_SIZE; 
+  int NUM_COLS_PER_ITER = MAX(WARP_SIZE / NUM_V_VECS_PER_THREAD, 1); 
+
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float) * NUM_COLS_PER_ITER;
   // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len
   // Keep that in sync with the logic here!
   int shared_mem_size = std::max(logits_size, outputs_size);
@@ -104,7 +236,7 @@ void paged_attention_v1_launcher(
       LAUNCH_PAGED_ATTENTION_V1(32);
       break;
     case 64:
-      LAUNCH_PAGED_ATTENTION_V1(64);
+      LAUNCH_PAGED_ATTENTION_V1_32N(64);
       break;
     case 80:
       LAUNCH_PAGED_ATTENTION_V1(80);
@@ -119,13 +251,16 @@ void paged_attention_v1_launcher(
       LAUNCH_PAGED_ATTENTION_V1(120);
       break;
     case 128:
-      LAUNCH_PAGED_ATTENTION_V1(128);
+      LAUNCH_PAGED_ATTENTION_V1_32N(128);
+      break;
+    case 160:
+      LAUNCH_PAGED_ATTENTION_V1(160);
       break;
     case 192:
       LAUNCH_PAGED_ATTENTION_V1(192);
       break;
     case 256:
-      LAUNCH_PAGED_ATTENTION_V1(256);
+      LAUNCH_PAGED_ATTENTION_V1_32N(256);
       break;
     default:
       TORCH_CHECK(false, "Unsupported head size: ", head_size);
diff --git a/csrc/attention/paged_attention_v2.cu b/csrc/attention/paged_attention_v2.cu
index 9935359e0..a0627d0e4 100644
--- a/csrc/attention/paged_attention_v2.cu
+++ b/csrc/attention/paged_attention_v2.cu
@@ -46,18 +46,52 @@
           out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
           max_num_partitions);
 
+#define LAUNCH_PAGED_ATTENTION_V2_32N(HEAD_SIZE)                                   \
+  vllm::paged_attention_v2_32N_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \
+                                  PARTITION_SIZE>                              \
+      <<<grid, block, shared_mem_size, stream>>>(                              \
+          exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \
+          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \
+          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \
+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,                  \
+          blocksparse_local_blocks, blocksparse_vert_stride,                   \
+          blocksparse_block_size, blocksparse_head_sliding_step, max_num_partitions, num_heads);              \
+  vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \
+                                         PARTITION_SIZE>                       \
+      <<<reduce_grid, block, reduce_shared_mem_size, stream>>>(                \
+          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
+          max_num_partitions);
+
+#define LAUNCH_PAGED_ATTENTION_V2_FINAL(HEAD_SIZE)                                   \
+  vllm::paged_attention_v2_kernel_final<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
+                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \
+                                  PARTITION_SIZE>                              \
+      <<<grid, block, shared_mem_size, stream>>>(                              \
+          exp_sums_ptr, max_logits_ptr, block_count_ptr, tmp_out_ptr, out_ptr, query_ptr, key_cache_ptr, \
+          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,                   \
+          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,         \
+          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,                        \
+          blocksparse_local_blocks, blocksparse_vert_stride,                        \
+          blocksparse_block_size, blocksparse_head_sliding_step,max_num_partitions, \
+          NUM_THREADS,                                                              \
+          num_heads,                                                                \
+          num_seqs,                                                                 \
+          count_init_once                                                           \
+	  );
+
 template <typename T, typename CACHE_T, int BLOCK_SIZE,
           vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,
-          int NUM_THREADS = 128, int PARTITION_SIZE = 512>
+          int NUM_THREADS = 256, int PARTITION_SIZE = 512>
 void paged_attention_v2_launcher(
     torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,
-    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
+    torch::Tensor& tmp_out, torch::Tensor& block_count, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int num_kv_heads, float scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,
     const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,
     torch::Tensor& v_scale, const int tp_rank,
     const int blocksparse_local_blocks, const int blocksparse_vert_stride,
-    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+    const int blocksparse_block_size, const int blocksparse_head_sliding_step, const bool count_init_once) {
   int num_seqs = query.size(0);
   int num_heads = query.size(1);
   int head_size = query.size(2);
@@ -68,6 +102,7 @@ void paged_attention_v2_launcher(
 
   [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
   assert(head_size % thread_group_size == 0);
+  assert((head_size & 7) == 0);
 
   // NOTE: alibi_slopes is optional.
   const float* alibi_slopes_ptr =
@@ -86,15 +121,20 @@ void paged_attention_v2_launcher(
   int* seq_lens_ptr = seq_lens.data_ptr<int>();
   const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());
   const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());
+  int* block_count_ptr = block_count.data_ptr<int>();
 
   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
   int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);
   int logits_size = PARTITION_SIZE * sizeof(float);
-  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  int V_VEC_SIZE = 16 / sizeof(CACHE_T);
+  int NUM_V_VECS_PER_THREAD = head_size / V_VEC_SIZE;
+  int NUM_COLS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_THREAD;
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float) * NUM_COLS_PER_ITER;
 
   // For paged attention v2 kernel.
   dim3 grid(num_heads, num_seqs, max_num_partitions);
-  int shared_mem_size = std::max(logits_size, outputs_size);
+  int shared_mem_size = std::max(std::max(logits_size, outputs_size) + (2 * NUM_WARPS + 512 + (NUM_WARPS<<1))*sizeof(float),
+     (2 * max_num_partitions + 2 * NUM_WARPS + max_num_partitions * head_size)*sizeof(float));
   // For paged attention v2 reduce kernel.
   dim3 reduce_grid(num_heads, num_seqs);
   int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
@@ -110,7 +150,7 @@ void paged_attention_v2_launcher(
       LAUNCH_PAGED_ATTENTION_V2(32);
       break;
     case 64:
-      LAUNCH_PAGED_ATTENTION_V2(64);
+      LAUNCH_PAGED_ATTENTION_V2_FINAL(64);
       break;
     case 80:
       LAUNCH_PAGED_ATTENTION_V2(80);
@@ -125,13 +165,16 @@ void paged_attention_v2_launcher(
       LAUNCH_PAGED_ATTENTION_V2(120);
       break;
     case 128:
-      LAUNCH_PAGED_ATTENTION_V2(128);
+      LAUNCH_PAGED_ATTENTION_V2_FINAL(128);
+      break;
+    case 160:
+      LAUNCH_PAGED_ATTENTION_V2(160);
       break;
     case 192:
       LAUNCH_PAGED_ATTENTION_V2(192);
       break;
     case 256:
-      LAUNCH_PAGED_ATTENTION_V2(256);
+      LAUNCH_PAGED_ATTENTION_V2_FINAL(256);
       break;
     default:
       TORCH_CHECK(false, "Unsupported head size: ", head_size);
@@ -142,11 +185,11 @@ void paged_attention_v2_launcher(
 #define CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, KV_DTYPE, IS_BLOCK_SPARSE)   \
   paged_attention_v2_launcher<T, CACHE_T, BLOCK_SIZE, KV_DTYPE,               \
                               IS_BLOCK_SPARSE>(                               \
-      out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,      \
+      out, exp_sums, max_logits, tmp_out, block_count, query, key_cache, value_cache,      \
       num_kv_heads, scale, block_tables, seq_lens, max_seq_len, alibi_slopes, \
       k_scale, v_scale, tp_rank, blocksparse_local_blocks,                    \
       blocksparse_vert_stride, blocksparse_block_size,                        \
-      blocksparse_head_sliding_step);
+      blocksparse_head_sliding_step, count_init_once);
 
 #define CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE) \
   if (is_block_sparse) {                                                   \
@@ -179,6 +222,7 @@ void paged_attention_v2(
     torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]
     torch::Tensor&
         tmp_out,  // [num_seqs, num_heads, max_num_partitions, head_size]
+    torch::Tensor& block_count,     // [num_seqs, num_heads]
     torch::Tensor& query,  // [num_seqs, num_heads, head_size]
     torch::Tensor&
         key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]
@@ -194,7 +238,7 @@ void paged_attention_v2(
     torch::Tensor& v_scale, const int64_t tp_rank,
     const int64_t blocksparse_local_blocks,
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
-    const int64_t blocksparse_head_sliding_step) {
+    const int64_t blocksparse_head_sliding_step, bool count_init_once) {
   const bool is_block_sparse = (blocksparse_vert_stride > 1);
   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,
                              CALL_V2_LAUNCHER_BLOCK_SIZE)
diff --git a/csrc/cache.h b/csrc/cache.h
index 0970b704b..83171e50b 100644
--- a/csrc/cache.h
+++ b/csrc/cache.h
@@ -24,6 +24,16 @@ void reshape_and_cache(torch::Tensor& key, torch::Tensor& value,
                        const std::string& kv_cache_dtype,
                        torch::Tensor& k_scale, torch::Tensor& v_scale);
 
+void reshape_and_cache_new(
+                        torch::Tensor& key,
+                        torch::Tensor& value,
+                        torch::Tensor& key_cache,
+                        torch::Tensor& value_cache,
+                        torch::Tensor& slot_mapping,
+                        const std::string& kv_cache_dtype,
+                        const double k_scale,
+                        const double v_scale);
+
 void reshape_and_cache_flash(torch::Tensor& key, torch::Tensor& value,
                              torch::Tensor& key_cache,
                              torch::Tensor& value_cache,
diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu
index 88559c8fe..cbb073cd2 100644
--- a/csrc/cache_kernels.cu
+++ b/csrc/cache_kernels.cu
@@ -261,6 +261,120 @@ __global__ void reshape_and_cache_kernel(
   }
 }
 
+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
+__global__ void reshape_and_cache_kernel_layout(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,
+                                         // block_size, x]
+    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,
+                                         // block_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride, const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const float kv_scale) {
+  const int64_t token_idx = blockIdx.x;
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = threadIdx.x; i < n; i += blockDim.x) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * (head_size / x) * block_size * x +
+        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + block_offset * head_size +
+        head_offset;
+    scalar_t tgt_key = key[src_key_idx];
+    scalar_t tgt_value = value[src_value_idx];
+    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
+      key_cache[tgt_key_idx] = tgt_key;
+      value_cache[tgt_value_idx] = tgt_value;
+    } else {
+      key_cache[tgt_key_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, kv_scale);
+      value_cache[tgt_value_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, kv_scale);
+    }
+  }
+}
+
+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
+__global__ void reshape_and_cache_kernel_layout_opt(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,
+                                         // block_size, x]
+    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,
+                                         // block_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride, const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const float kv_scale) {
+  const int64_t token_idx = blockIdx.x;
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size / 8;
+  for (int t = threadIdx.x; t < n; t += blockDim.x) {
+    int i = t << 3;
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + block_offset * head_size +
+        head_offset;
+
+    *(float4*)(key_cache + tgt_key_idx) = *(float4*)(key + src_key_idx);
+    *(float4*)(value_cache + tgt_value_idx) = *(float4*)(value + src_value_idx);
+   /*
+    scalar_t tgt_key = key[src_key_idx];
+    scalar_t tgt_value = value[src_value_idx];
+    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {
+      key_cache[tgt_key_idx] = tgt_key;
+      value_cache[tgt_value_idx] = tgt_value;
+    } else {
+      key_cache[tgt_key_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, kv_scale);
+      value_cache[tgt_value_idx] =
+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, kv_scale);
+    }
+    */
+  }
+}
+
 template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
 __global__ void reshape_and_cache_flash_kernel(
     const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
@@ -394,6 +508,83 @@ void reshape_and_cache(
                              CALL_RESHAPE_AND_CACHE)
 }
 
+#define CALL_RESHAPE_AND_CACHE_LAYOUT(KV_T, CACHE_T, KV_DTYPE)               \
+  vllm::reshape_and_cache_kernel_layout<KV_T, CACHE_T, KV_DTYPE>             \
+      <<<grid, block, 0, stream>>>(                                   \
+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
+          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \
+          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \
+          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \
+          num_heads, head_size, block_size, x, kv_scale);
+
+#define CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(KV_T, CACHE_T, KV_DTYPE)               \
+  vllm::reshape_and_cache_kernel_layout_opt<KV_T, CACHE_T, KV_DTYPE>             \
+      <<<grid, block, 0, stream>>>(                                   \
+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
+          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \
+          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \
+          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \
+          num_heads, head_size, block_size, x, kv_scale);
+
+void reshape_and_cache_new(
+    torch::Tensor& key,    // [num_tokens, num_heads, head_size]
+    torch::Tensor& value,  // [num_tokens, num_heads, head_size]
+    torch::Tensor&
+        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]
+    torch::Tensor&
+        value_cache,  // [num_blocks, num_heads, head_size, block_size]
+    torch::Tensor& slot_mapping,  // [num_tokens]
+    const std::string& kv_cache_dtype, const double kv_scale, const double v_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(3);
+  int x = key_cache.size(4);
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  dim3 grid(num_tokens);
+  dim3 block(std::min(num_heads * head_size, 512));
+  const at::cuda::OptionalCUDAGuard device_guard(device_of(key));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  if (kv_cache_dtype == "auto") {
+    if (key.dtype() == at::ScalarType::Float) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(float, float, vllm::Fp8KVCacheDataType::kAuto);
+    } else if (key.dtype() == at::ScalarType::Half) {
+      if((x & 7) == 0 && (head_size & 7) == 0) {
+        CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(half, half, vllm::Fp8KVCacheDataType::kAuto);
+      } else {
+        CALL_RESHAPE_AND_CACHE_LAYOUT(half, half, vllm::Fp8KVCacheDataType::kAuto);
+      }
+    }
+    else if (key.dtype() == at::ScalarType::BFloat16) {
+      if((x & 7) == 0 && (head_size & 7) == 0) {
+        CALL_RESHAPE_AND_CACHE_LAYOUT_OPT(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);
+      } else {
+        CALL_RESHAPE_AND_CACHE_LAYOUT(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);
+      }
+    }
+  }
+ /*
+  else if (kv_cache_dtype == "fp8_e5m2") {
+    if (key.dtype() == at::ScalarType::Float) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(float, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
+    } else if (key.dtype() == at::ScalarType::Half) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
+    } else if (key.dtype() == at::ScalarType::BFloat16) {
+      CALL_RESHAPE_AND_CACHE_LAYOUT(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kAuto);
+    }
+  }
+  */
+  else {
+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
+  }
+}
+
 // KV_T is the data type of key and value tensors.
 // CACHE_T is the stored data type of kv-cache.
 // KV_DTYPE is the real data type of kv-cache.
diff --git a/csrc/cuda_compat.h b/csrc/cuda_compat.h
index 82e55613d..1315569cf 100644
--- a/csrc/cuda_compat.h
+++ b/csrc/cuda_compat.h
@@ -47,3 +47,16 @@
   #define VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(FUNC, VAL) \
     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)
 #endif
+
+#define MXWARP_SIZE 64
+#ifndef USE_ROCM
+  #define MXVLLM_SHFL_SYNC(var, src_lane) __shfl_sync(uint64_t(-1), var, src_lane)
+#else
+  #define MXVLLM_SHFL_SYNC(var, src_lane) __shfl(var, src_lane)
+#endif
+
+#ifndef USE_ROCM
+  #define MXVLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor_sync(uint64_t(-1), var, lane_mask)
+#else
+  #define MXVLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor(var, lane_mask)
+#endif
diff --git a/csrc/cumem_allocator.cpp b/csrc/cumem_allocator.cpp
index fab6ca36d..41015355d 100644
--- a/csrc/cumem_allocator.cpp
+++ b/csrc/cumem_allocator.cpp
@@ -3,8 +3,6 @@
 // need to be unsigned long long
 #include <iostream>
 
-extern "C" {
-
 #define PY_SSIZE_T_CLEAN
 #include <Python.h>
 
@@ -12,6 +10,8 @@ extern "C" {
 #include <cuda_runtime_api.h>
 #include <cuda.h>
 
+extern "C" {
+
 char error_msg[10240];  // 10KB buffer to store error messages
 CUresult no_error = CUresult(0);
 CUresult error_code = no_error;  // store error code
diff --git a/csrc/custom_all_reduce.cuh b/csrc/custom_all_reduce.cuh
index 44709b459..c27912dde 100644
--- a/csrc/custom_all_reduce.cuh
+++ b/csrc/custom_all_reduce.cuh
@@ -56,9 +56,15 @@ struct Signal {
   alignas(128) FlagType _flag[kMaxBlocks];  // incremental flags for each rank
 };
 
+#ifdef USE_MACA
+struct __align__(16) RankData { 
+  const void* ptrs[8]; 
+};
+#else
 struct __align__(16) RankData {
   const void* ptrs[8];
 };
+#endif // USE_MACA
 
 struct __align__(16) RankSignals {
   Signal* signals[8];
@@ -155,18 +161,21 @@ DINLINE O downcast(array_t<float, O::size> val) {
 #if !defined(USE_ROCM)
 
 static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {
-  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
+#ifndef USE_MACA
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
   asm volatile("st.release.sys.global.u32 [%1], %0;" ::"r"(flag),
                "l"(flag_addr));
   #else
   asm volatile("membar.sys; st.volatile.global.u32 [%1], %0;" ::"r"(flag),
                "l"(flag_addr));
-  #endif
+#endif
+#endif // USE_MACA 
 }
 
 static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
   FlagType flag;
-  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
+#ifndef USE_MACA
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700
   asm volatile("ld.acquire.sys.global.u32 %0, [%1];"
                : "=r"(flag)
                : "l"(flag_addr));
@@ -174,19 +183,24 @@ static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {
   asm volatile("ld.volatile.global.u32 %0, [%1]; membar.gl;"
                : "=r"(flag)
                : "l"(flag_addr));
-  #endif
+#endif
+#endif // USE_MACA
   return flag;
 }
 
 static DINLINE void st_flag_volatile(FlagType* flag_addr, FlagType flag) {
+#ifndef USE_MACA
   asm volatile("st.volatile.global.u32 [%1], %0;" ::"r"(flag), "l"(flag_addr));
+#endif // USE_MACA
 }
 
 static DINLINE FlagType ld_flag_volatile(FlagType* flag_addr) {
   FlagType flag;
+#ifndef USE_MACA
   asm volatile("ld.volatile.global.u32 %0, [%1];"
                : "=r"(flag)
                : "l"(flag_addr));
+#endif // USE_MACA
   return flag;
 }
 
@@ -365,7 +379,9 @@ __global__ void __launch_bounds__(512, 1)
 
 using IPC_KEY = std::array<uint8_t, sizeof(cudaIpcMemHandle_t)>;
 static_assert(sizeof(IPC_KEY) == sizeof(cudaIpcMemHandle_t));
+#ifndef USE_MACA
 static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));
+#endif // USE_MACA
 
 class CustomAllreduce {
  public:
diff --git a/csrc/cutlass_extensions/common.hpp b/csrc/cutlass_extensions/common.hpp
index dbe0e30f5..adb3477ea 100644
--- a/csrc/cutlass_extensions/common.hpp
+++ b/csrc/cutlass_extensions/common.hpp
@@ -1,6 +1,11 @@
 #pragma once
 
+#ifndef USE_MACA
 #include "cutlass/cutlass.h"
+#else
+#include "mctlass/mctlass.h"
+#endif // USE_MACA
+
 #include <climits>
 #include "cuda_runtime.h"
 #include <iostream>
@@ -8,12 +13,21 @@
 /**
  * Helper function for checking CUTLASS errors
  */
+#ifndef USE_MACA
 #define CUTLASS_CHECK(status)                       \
   {                                                 \
     cutlass::Status error = status;                 \
     TORCH_CHECK(error == cutlass::Status::kSuccess, \
                 cutlassGetStatusString(error));     \
   }
+#else
+#define CUTLASS_CHECK(status)                       \
+  {                                                 \
+    mctlass::Status error = status;                 \
+    TORCH_CHECK(error == mctlass::Status::kSuccess, \
+                mctlassGetStatusString(error));     \
+  }
+#endif // USE_MACA
 
 /**
  * Panic wrapper for unwinding CUDA runtime errors
@@ -43,7 +57,11 @@ int32_t get_sm_version_num();
 template <typename Kernel>
 struct enable_sm90_or_later : Kernel {
   template <typename... Args>
+#ifndef USE_MACA
   CUTLASS_DEVICE void operator()(Args&&... args) {
+#else
+  MCTLASS_DEVICE void operator()(Args&&... args) {
+#endif // USE_MACA
 #if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 900
     Kernel::operator()(std::forward<Args>(args)...);
 #endif
@@ -53,7 +71,11 @@ struct enable_sm90_or_later : Kernel {
 template <typename Kernel>
 struct enable_sm90_only : Kernel {
   template <typename... Args>
+#ifndef USE_MACA
   CUTLASS_DEVICE void operator()(Args&&... args) {
+#else
+  MCTLASS_DEVICE void operator()(Args&&... args) {
+#endif // USE_MACA
 #if defined __CUDA_ARCH__ && __CUDA_ARCH__ == 900
     Kernel::operator()(std::forward<Args>(args)...);
 #endif
diff --git a/csrc/mamba/causal_conv1d/causal_conv1d.cu b/csrc/mamba/causal_conv1d/causal_conv1d.cu
index 98daf1a1b..cc0606f17 100644
--- a/csrc/mamba/causal_conv1d/causal_conv1d.cu
+++ b/csrc/mamba/causal_conv1d/causal_conv1d.cu
@@ -183,7 +183,7 @@ void causal_conv1d_fwd(const at::Tensor &x, const at::Tensor &weight,
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_fwd", [&] {
             causal_conv1d_fwd_cuda<input_t, weight_t>(params, stream);
@@ -276,7 +276,7 @@ void causal_conv1d_update(const at::Tensor &x,
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)x.get_device()};
+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(x.get_device())};
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(x.scalar_type(), "causal_conv1d_update", [&] {
             causal_conv1d_update_cuda<input_t, weight_t>(params, stream);
diff --git a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
index bd0a34119..2737b98f1 100644
--- a/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
+++ b/csrc/mamba/mamba_ssm/selective_scan_fwd.cu
@@ -328,10 +328,20 @@ void selective_scan_fwd_launch(SSMParamsBase &params, cudaStream_t stream) {
         });
     });
 }
-
+#define USE_MACA
 template<typename input_t, typename weight_t>
 void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {
-
+    #ifdef USE_MACA
+        if (params.seqlen <= 256) {
+            selective_scan_fwd_launch<64, 4, input_t, weight_t>(params, stream);
+        } else if (params.seqlen <= 512) {
+            selective_scan_fwd_launch<64, 8, input_t, weight_t>(params, stream);
+        } else if (params.seqlen <= 1024) {
+            selective_scan_fwd_launch<64, 16, input_t, weight_t>(params, stream);
+        } else {
+            selective_scan_fwd_launch<128, 16, input_t, weight_t>(params, stream);
+        }
+    #else
     #ifndef USE_ROCM
         if (params.seqlen <= 128) {           
             selective_scan_fwd_launch<32, 4, input_t, weight_t>(params, stream);
@@ -355,6 +365,7 @@ void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {
             selective_scan_fwd_launch<128, 16, input_t, weight_t>(params, stream);
         }
     #endif
+    #endif // USE_MACA
 }
 
 template void selective_scan_fwd_cuda<at::BFloat16, float>(SSMParamsBase &params, cudaStream_t stream);
@@ -649,7 +660,7 @@ void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,
     
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)u.get_device()};
+    at::cuda::CUDAGuard device_guard{static_cast<c10::DeviceIndex>(u.get_device())};
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(u.scalar_type(), "selective_scan_fwd", [&] {
         selective_scan_fwd_cuda<input_t, weight_t>(params, stream);
diff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu
index d7be76945..9b576204b 100644
--- a/csrc/moe/moe_align_sum_kernels.cu
+++ b/csrc/moe/moe_align_sum_kernels.cu
@@ -197,6 +197,240 @@ __global__ void moe_align_block_size_global_mem_kernel(
   }
 }
 
+
+
+__device__ __forceinline__ int32_t ScanWarp2(int32_t val) {
+  int32_t lane = threadIdx.x & 31;
+  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1, 8);
+  if (lane >= 1) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 2, 8);
+  if (lane >= 2) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 4, 8);
+  if (lane >= 4) {
+    val += tmp;
+  }
+  return val;
+}
+
+__device__ __forceinline__ int32_t ScanWarp(int32_t val) {
+  int32_t lane = threadIdx.x & 31;
+  int32_t tmp = __shfl_up_sync(0xffffffff, val, 1);
+  if (lane >= 1) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 2);
+  if (lane >= 2) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 4);
+  if (lane >= 4) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 8);
+  if (lane >= 8) {
+    val += tmp;
+  }
+  tmp = __shfl_up_sync(0xffffffff, val, 16);
+  if (lane >= 16) {
+    val += tmp;
+  }
+  return val;
+}
+
+
+__device__ inline void copy(const void* local, void* data)
+{
+    const float4* in = static_cast<const float4*>(local);
+    float4* out = static_cast<float4*>(data);
+    *out = *in;
+}
+
+template <typename scalar_t, int VPT>
+__global__ void opt_sgl_moe_align_block_size_kernel_stage_2(size_t numel, int32_t* local_offsets, scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids){
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+  if (idx * VPT >= numel) return;
+  int32_t expert_local[VPT];
+  copy(topk_ids + idx * VPT, expert_local);
+  for(int i = 0; i < VPT && i + idx * VPT < numel; ++i) {
+    int32_t expert_id = expert_local[i];
+    int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);
+    sorted_token_ids[rank_post_pad] = i + idx * VPT;
+  }
+}
+
+template <typename scalar_t>
+__global__ void opt_sgl_moe_align_block_size_kernel_stage_1(
+    scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids,
+    int32_t* expert_ids, int32_t* total_tokens_post_pad, int32_t num_experts,
+    int32_t block_size, size_t numel, int32_t* offsets) {
+  __shared__ int32_t shared_counts[32][8];
+  __shared__ int32_t local_offsets[256];
+  __shared__ int32_t cum_test[257];
+  __shared__ int32_t blocksum[32];
+
+  const int warp_id = threadIdx.x / 32;
+  const int lane_id = threadIdx.x % 32;
+  const int experts_per_warp = 8;
+  const int my_expert_start = warp_id * experts_per_warp;
+
+  int32_t idx = threadIdx.x;
+  if(idx < num_experts){
+    shared_counts[idx / 8][idx % 8] = 0;
+    cum_test[0] = 0;
+  }
+  __syncthreads();
+
+  const size_t tokens_per_thread = CEILDIV(numel, blockDim.x);
+  const size_t start_idx = threadIdx.x * tokens_per_thread;
+
+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
+    int expert_id = topk_ids[i];
+    int warp_idx = expert_id / experts_per_warp;
+    int expert_offset = expert_id % experts_per_warp;
+    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
+  }
+  __syncthreads();
+
+  int val = 0;
+  if (threadIdx.x < 256) {
+    int32_t final_val = 0;
+    int row = idx / 8;
+    int line = idx % 8;
+    val = shared_counts[row][line];
+    val = CEILDIV(val, block_size) * block_size;
+  }
+  __syncthreads();
+
+  if(idx < 256) {
+    int tmp = 0;
+    val = ScanWarp(val);
+    if(lane_id == 31) {
+      blocksum[warp_id] = val;
+    }
+  }
+  __syncthreads();
+
+  if(warp_id == 0 && lane_id < 8) {
+    int res = blocksum[lane_id];
+    blocksum[lane_id] = ScanWarp2(res);
+  }
+  __syncthreads();
+
+  if(threadIdx.x < 256 && warp_id > 0){
+    val += blocksum[warp_id - 1];
+  }
+  __syncthreads();
+
+  if(idx < 256){
+    cum_test[idx + 1] = val;
+  }
+  __syncthreads();
+
+  if (threadIdx.x < num_experts) {
+    for (int i = cum_test[threadIdx.x]; i < cum_test[threadIdx.x + 1];
+         i += block_size) {
+      expert_ids[i / block_size] = threadIdx.x;
+    }
+    local_offsets[threadIdx.x] = cum_test[threadIdx.x];
+    offsets[threadIdx.x] = cum_test[threadIdx.x];
+    if(threadIdx.x == 0){
+      *total_tokens_post_pad = cum_test[num_experts];
+    }
+  }
+}
+
+
+template <typename scalar_t>
+__global__ void opt_sgl_moe_align_block_size_kernel(
+    scalar_t* __restrict__ topk_ids, int32_t* sorted_token_ids,
+    int32_t* expert_ids, int32_t* total_tokens_post_pad, int32_t num_experts,
+    int32_t block_size, size_t numel, int32_t* cumsum) {
+  __shared__ int32_t shared_counts[32][8];
+  __shared__ int32_t local_offsets[256];
+  __shared__ int32_t cum_test[257];
+  __shared__ int32_t blocksum[32];
+
+  const int warp_id = threadIdx.x / 32;
+  const int lane_id = threadIdx.x % 32;
+  const int experts_per_warp = 8;
+  const int my_expert_start = warp_id * experts_per_warp;
+
+  int32_t idx = threadIdx.x;
+  if(idx < num_experts){
+    shared_counts[idx / 8][idx % 8] = 0;
+    cum_test[0] = 0;
+  }
+  __syncthreads();
+
+  const size_t tokens_per_thread = CEILDIV(numel, blockDim.x);
+  const size_t start_idx = threadIdx.x * tokens_per_thread;
+
+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
+    int expert_id = topk_ids[i];
+    int warp_idx = expert_id / experts_per_warp;
+    int expert_offset = expert_id % experts_per_warp;
+    atomicAdd(&shared_counts[warp_idx][expert_offset], 1);
+  }
+  __syncthreads();
+
+  int val = 0;
+  if (threadIdx.x < 256) {
+    int32_t final_val = 0;
+    int row = idx / 8;
+    int line = idx % 8;
+    val = shared_counts[row][line];
+    val = CEILDIV(val, block_size) * block_size;
+  }
+  __syncthreads();
+
+  if(idx < 256) {
+    int tmp = 0;
+    val = ScanWarp(val);
+    if(lane_id == 31) {
+      blocksum[warp_id] = val;
+    }
+  }
+  __syncthreads();
+
+  if(warp_id == 0 && lane_id < 8) {
+    int res = blocksum[lane_id];
+    blocksum[lane_id] = ScanWarp2(res);
+  }
+  __syncthreads();
+
+  if(threadIdx.x < 256 && warp_id > 0){
+    val += blocksum[warp_id - 1];
+  }
+  __syncthreads();
+
+  if(idx < 256){
+    cum_test[idx + 1] = val;
+  }
+  __syncthreads();
+
+  if (threadIdx.x < num_experts) {
+    for (int i = cum_test[threadIdx.x]; i < cum_test[threadIdx.x + 1];
+         i += block_size) {
+      expert_ids[i / block_size] = threadIdx.x;
+    }
+    local_offsets[threadIdx.x] = cum_test[threadIdx.x];
+    if(threadIdx.x == 0){
+      *total_tokens_post_pad = cum_test[num_experts];
+    }
+  }
+  __syncthreads();
+
+  for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {
+    int32_t expert_id = topk_ids[i];
+    int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);
+    sorted_token_ids[rank_post_pad] = i;
+  }
+}
+
 // taken from
 // https://github.com/sgl-project/sglang/commit/cdae77b03dfc6fec3863630550b45bbfc789f957
 template <typename scalar_t>
@@ -396,27 +630,44 @@ void sgl_moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,
         // calc needed amount of shared mem for `cumsum` tensors
         auto options_int =
             torch::TensorOptions().dtype(torch::kInt).device(topk_ids.device());
-        torch::Tensor cumsum_buffer =
+        torch::Tensor offsets_buffer =
             torch::zeros({num_experts + 1}, options_int);
 
-        auto align_kernel =
-            vllm::moe::sgl_moe_align_block_size_kernel<scalar_t>;
-        align_kernel<<<1, 1024, 0, stream>>>(
-            topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
-            experts_ids.data_ptr<int32_t>(),
-            num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
-            topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>());
-
-        const int block_threads = 256;
-        const int num_blocks =
-            (topk_ids.numel() + block_threads - 1) / block_threads;
-        const int max_blocks = 65535;
-        const int actual_blocks = std::min(num_blocks, max_blocks);
-        auto sort_kernel = vllm::moe::sgl_moe_token_sort_kernel<scalar_t>;
-        sort_kernel<<<actual_blocks, block_threads, 0, stream>>>(
-            topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
-            cumsum_buffer.data_ptr<int32_t>(), topk_ids.numel());
-      });
+        if(topk_ids.numel() <= 4096) {
+          auto align_kernel =
+              vllm::moe::opt_sgl_moe_align_block_size_kernel<scalar_t>;
+          align_kernel<<<1, 1024, 0, stream>>>(
+              topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
+              experts_ids.data_ptr<int32_t>(),
+              num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
+                topk_ids.numel(), offsets_buffer.data_ptr<int32_t>());
+        } else {
+           auto kernel = vllm::moe::opt_sgl_moe_align_block_size_kernel_stage_1<scalar_t>;
+          kernel<<<1, 1024, 0, stream>>>(
+              topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
+              experts_ids.data_ptr<int32_t>(),
+              num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,
+              topk_ids.numel(), offsets_buffer.data_ptr<int32_t>());
+
+          auto kernel_2 = vllm::moe::opt_sgl_moe_align_block_size_kernel_stage_2<scalar_t, 16 / sizeof(scalar_t)>;
+          int block_size = 256;
+          int grid_size = (topk_ids.numel() + block_size - 1) / block_size;
+          kernel_2<<<grid_size, block_size, 0, stream>>>(
+              topk_ids.numel(), offsets_buffer.data_ptr<int32_t>(), 
+              topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>());
+        }
+
+      //   const int block_threads = 256;
+      //   const int num_blocks =
+      //       (topk_ids.numel() + block_threads - 1) / block_threads;
+      //   const int max_blocks = 65535;
+      //   const int actual_blocks = std::min(num_blocks, max_blocks);
+      //   auto sort_kernel = vllm::moe::sgl_moe_token_sort_kernel<scalar_t>;
+      //   sort_kernel<<<actual_blocks, block_threads, 0, stream>>>(
+      //       topk_ids.data_ptr<scalar_t>(), sorted_token_ids.data_ptr<int32_t>(),
+      //       offsets_buffer.data_ptr<int32_t>(), topk_ids.numel());
+    }
+);
 }
 
 void moe_sum(torch::Tensor& input,   // [num_tokens, topk, hidden_size]
diff --git a/csrc/moe/moe_ops.cpp b/csrc/moe/moe_ops.cpp
new file mode 100644
index 000000000..a12aeff32
--- /dev/null
+++ b/csrc/moe/moe_ops.cpp
@@ -0,0 +1,64 @@
+#include "moe_ops.h"
+
+#include <ATen/cuda/CUDAContext.h>
+#include <mcblas.h>
+#include <maca_fp16.h>
+
+mcblasStatus_t mcblasFusedMoe(mcStream_t stream,
+                              const void *a_ptr,
+                              const void *b_ptr,
+                              void *c_ptr,
+                              const int *sorted_token_ids_ptr,
+                              const int *expert_ids_ptr,
+                              const int *num_tokens_post_padded,
+                              int N,
+                              int K,
+                              int num_valid_tokens,
+                              int sorted_token_ids_len,
+                              int stride_am,
+                              int stride_ak,
+                              int stride_be,
+                              int stride_bk,
+                              int stride_bn,
+                              int stride_cm,
+                              int stride_cn,
+                              int top_k,
+                              bool mul_routed_weight,
+                              const float *topk_weights_ptr,
+                              macaDataType compute_type,
+                              int tileConfig = 0);
+
+void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
+                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
+                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
+                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig) {
+    
+    assert(topk_weights.stride(1) == 1);
+    assert(sorted_token_ids.stride(0) == 1);
+
+    auto stream = at::cuda::getCurrentCUDAStream();
+    macaDataType compute_type = (A.dtype() == at::ScalarType::BFloat16) ? MACA_R_16BF : MACA_R_16F;
+    mcblasFusedMoe(stream,
+                  A.data_ptr(),
+                  B.data_ptr(),
+                  C.data_ptr(),
+                  sorted_token_ids.data_ptr<int>(),
+                  expert_ids.data_ptr<int>(),
+                  num_tokens_post_padded.data_ptr<int>(),
+                  B.size(1),
+                  B.size(2),
+                  topk_ids.numel(),
+                  sorted_token_ids.size(0),
+                  A.stride(0),
+                  A.stride(1),
+                  B.stride(0),
+                  B.stride(2),
+                  B.stride(1),
+                  C.stride(1),
+                  C.stride(2),
+                  static_cast<int>(top_k),
+                  mul_routed_weight,
+                  topk_weights.data_ptr<float>(),
+                  compute_type,
+                  static_cast<int>(tileConfig));
+}
\ No newline at end of file
diff --git a/csrc/moe/moe_ops.h b/csrc/moe/moe_ops.h
index 0bae119a7..4ba5608d3 100644
--- a/csrc/moe/moe_ops.h
+++ b/csrc/moe/moe_ops.h
@@ -28,4 +28,13 @@ torch::Tensor moe_wna16_gemm(torch::Tensor input, torch::Tensor output,
                              torch::Tensor num_tokens_post_pad, int64_t top_k,
                              int64_t BLOCK_SIZE_M, int64_t BLOCK_SIZE_N,
                              int64_t BLOCK_SIZE_K, int64_t bit);
+#endif
+
+#ifdef USE_MACA
+
+void fused_moe_kernel(const torch::Tensor& A, const torch::Tensor& B, const torch::Tensor& C,
+                    const torch::Tensor& topk_weights, const torch::Tensor& topk_ids,
+                    const torch::Tensor& sorted_token_ids, const torch::Tensor& expert_ids,
+                    const torch::Tensor& num_tokens_post_padded, bool mul_routed_weight, int64_t top_k, int64_t tileConfig);
+
 #endif
\ No newline at end of file
diff --git a/csrc/moe/moe_wna16.cu b/csrc/moe/moe_wna16.cu
index 7b6a111c0..f1badfe23 100644
--- a/csrc/moe/moe_wna16.cu
+++ b/csrc/moe/moe_wna16.cu
@@ -85,9 +85,15 @@ __global__ void moe_wna16_gemm_kernel(
     if (threadIdx.x >= BLOCK_SIZE_N || offset_n >= size_n) return;
 
     float res[64];  // assume BLOCK_SIZE_M <= 64
+#ifndef USE_MACA
     scalar_t2 res2;
     scalar_t2 scale_f2;
     scalar_t2 qzero_f2;
+#else
+    scalar_t2 res2{};
+    scalar_t2 scale_f2{};
+    scalar_t2 qzero_f2{};
+#endif
 
     // note that (size_n * size_k * expert_id) may greater than 2 ** 31
     constexpr int8_t pack_factor = 32 / bit;
@@ -190,7 +196,9 @@ __global__ void moe_wna16_gemm_kernel(
       dequant<scalar_t2, bit>(expert_qweight_tmp[tmp_k % 4], weight_half2);
 
       for (int m = 0; m < num_valid_tokens; m++) {
+#ifndef USE_MACA
         res2 = {};
+#endif
 
 #pragma unroll
         for (int i = 0; i < 16 / bit; i++) {
diff --git a/csrc/moe/moe_wna16_utils.h b/csrc/moe/moe_wna16_utils.h
index 4396b8024..712c5eb63 100644
--- a/csrc/moe/moe_wna16_utils.h
+++ b/csrc/moe/moe_wna16_utils.h
@@ -81,18 +81,22 @@ class ScalarType<nv_bfloat16> {
 template <int lut>
 __device__ inline int lop3(int a, int b, int c) {
   int res;
+#ifndef USE_MACA
   asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                : "=r"(res)
                : "r"(a), "r"(b), "r"(c), "n"(lut));
+#endif
   return res;
 }
 
 template <int start_byte, int mask>
 __device__ inline uint32_t prmt(uint32_t a) {
   uint32_t res;
+#ifndef USE_MACA
   asm volatile("prmt.b32 %0, %1, %2, %3;\n"
                : "=r"(res)
                : "r"(a), "n"(start_byte), "n"(mask));
+#endif
   return res;
 }
 
diff --git a/csrc/moe/torch_bindings.cpp b/csrc/moe/torch_bindings.cpp
index d0de42251..bb94beef0 100644
--- a/csrc/moe/torch_bindings.cpp
+++ b/csrc/moe/torch_bindings.cpp
@@ -57,6 +57,16 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, m) {
   // conditionally compiled so impl registration is in source file
 
 #endif
+
+#ifdef USE_MACA
+// Fused moe in mcblas
+  m.def(
+      "fused_moe_kernel(Tensor! A, Tensor! B, Tensor! C,"
+      "Tensor! topk_weights, Tensor! topk_ids,"
+      "Tensor! sorted_token_ids, Tensor! expert_ids,"
+      "Tensor! num_tokens_post_padded, bool mul_routed_weight, int top_k, int tileConfig) -> ()");
+  m.impl("fused_moe_kernel", torch::kCUDA, &fused_moe_kernel);
+#endif
 }
 
 REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
diff --git a/csrc/ops.h b/csrc/ops.h
index fe120af5d..3f0c8e08e 100644
--- a/csrc/ops.h
+++ b/csrc/ops.h
@@ -42,7 +42,7 @@ void paged_attention_v1(
 
 void paged_attention_v2(
     torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,
-    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,
+    torch::Tensor& tmp_out, torch::Tensor& block_count, torch::Tensor& query, torch::Tensor& key_cache,
     torch::Tensor& value_cache, int64_t num_kv_heads, double scale,
     torch::Tensor& block_tables, torch::Tensor& seq_lens, int64_t block_size,
     int64_t max_seq_len, const std::optional<torch::Tensor>& alibi_slopes,
@@ -50,7 +50,19 @@ void paged_attention_v2(
     torch::Tensor& v_scale, const int64_t tp_rank,
     const int64_t blocksparse_local_blocks,
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
-    const int64_t blocksparse_head_sliding_step);
+    const int64_t blocksparse_head_sliding_step, bool count_init_once);
+
+void page_reshape_kv_cache(
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& key_cache_new_layer,
+    torch::Tensor& value_cache_new_layer,
+    int64_t num_seqs,
+    int64_t num_heads,
+    int64_t head_size,
+    int64_t num_kv_heads,
+    int64_t block_size,
+    const std::string& kv_cache_dtype);
 
 #ifndef USE_ROCM
 void merge_attn_states(torch::Tensor& output,
@@ -149,13 +161,16 @@ torch::Tensor aqlm_dequant(
 
 torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
-                       int64_t split_k_iters);
+                       int64_t split_k_iters, torch::Tensor _temp_space, 
+                       bool dtype_bf16);
 
 torch::Tensor awq_dequantize(torch::Tensor _kernel,
                              torch::Tensor _scaling_factors,
                              torch::Tensor _zeros, int64_t split_k_iters,
                              int64_t thx, int64_t thy);
 
+torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight);
+
 torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm);
 #endif
 
@@ -241,7 +256,9 @@ void dynamic_scaled_int8_quant(torch::Tensor& out, torch::Tensor const& input,
 torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
                         torch::Tensor b_gptq_qzeros,
                         torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
-                        bool use_exllama, int64_t bit);
+                        bool use_exllama, int64_t bit, int64_t group_size, 
+                        torch::Tensor perm_space, torch::Tensor temp_space,
+			            bool dtype_bf16);
 
 void gptq_shuffle(torch::Tensor q_weight, torch::Tensor q_perm, int64_t bit);
 
diff --git a/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
new file mode 100644
index 000000000..614a6251e
--- /dev/null
+++ b/csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
@@ -0,0 +1,473 @@
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+#include "../gptq/Hgemm_common.cuh"
+#include "awq_4bits.cuh"
+#include "maca_fp16.h"
+
+#define input_type __half
+#define output_type __half
+#define scalar_type float
+#define acc_type float
+
+#define SEL0 0x01000504
+#define SEL1 0x03020706
+
+#define SWIZZLE_INDEX(index, phase) (index ^ phase)
+#define ONE_DIM_INDEX(x, y, lda) (y * lda + x)
+#define CLAMP(value, bound, align) (value < bound ? value : bound - align)
+
+#define LDG_B                                                       \
+    {                                                               \
+        ldg_b128_bsm_async(b_sts, dB[0], pred_n0, true);            \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1, true); \
+    }
+
+#define LDG_B_HEAD                                                            \
+    {                                                                         \
+        bool pred_k = rowB_swizzle < ktail;                                   \
+        ldg_b128_bsm_async(b_sts, dB[0], pred_n0 &&pred_k, true);             \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], pred_n1 && pred_k, true); \
+    }
+
+#define MMA_ABC1                                                             \
+    {                                                                        \
+        for (int index_n = 0; index_n < 2; ++index_n)                        \
+        {                                                                    \
+            for (int index_m = 0; index_m < 8; ++index_m)                    \
+            {                                                                \
+                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                                cast_b64(rgB)[ONE_DIM_INDEX(0, index_n, 2)], \
+                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
+            }                                                                \
+        }                                                                    \
+    }
+
+#define MMA_ABC2                                                             \
+    {                                                                        \
+        for (int index_n = 0; index_n < 2; ++index_n)                        \
+        {                                                                    \
+            for (int index_m = 0; index_m < 8; ++index_m)                    \
+            {                                                                \
+                mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                                cast_b64(rgB)[ONE_DIM_INDEX(1, index_n, 2)], \
+                                rgC[ONE_DIM_INDEX(index_m, index_n, 8)]);    \
+            }                                                                \
+        }                                                                    \
+    }
+
+#define LDS_B                                          \
+    {                                                  \
+        cast_b128(rgB)[0] = cast_b128(b_lds)[0];       \
+        cast_b128(rgB)[1] = cast_b128(b_lds)[16 * 16]; \
+    }
+
+#define PERM_C(C2perm)                             \
+    {                                              \
+        Float4VecType C_tmp[8];                    \
+        float *ptri = (float *)C2perm;             \
+        float *ptro = (float *)C_tmp;              \
+        for (int j = 0; j < 4; ++j)                \
+        {                                          \
+            for (int i = 0; i < 8; ++i)            \
+            {                                      \
+                ptro[j * 8 + i] = ptri[j + i * 4]; \
+            }                                      \
+        }                                          \
+        for (int i = 0; i < 8; ++i)                \
+        {                                          \
+            C2perm[i] = C_tmp[i];                  \
+        }                                          \
+    }
+
+#define STS_C(phase)                                            \
+    {                                                           \
+        cast_b128(c_sts[0])[0] = cast_b128(rgC)[0 + phase * 8]; \
+        cast_b128(c_sts[1])[0] = cast_b128(rgC)[1 + phase * 8]; \
+        cast_b128(c_sts[2])[0] = cast_b128(rgC)[2 + phase * 8]; \
+        cast_b128(c_sts[3])[0] = cast_b128(rgC)[3 + phase * 8]; \
+        cast_b128(c_sts[4])[0] = cast_b128(rgC)[4 + phase * 8]; \
+        cast_b128(c_sts[5])[0] = cast_b128(rgC)[5 + phase * 8]; \
+        cast_b128(c_sts[6])[0] = cast_b128(rgC)[6 + phase * 8]; \
+        cast_b128(c_sts[7])[0] = cast_b128(rgC)[7 + phase * 8]; \
+    }
+
+#define REDUCE_C(phase)                                                   \
+    {                                                                     \
+        cast_b128(rgC)[0 + phase * 8] = cast_b128(c_lds[0])[0];           \
+        cast_b128(rgC)[1 + phase * 8] = cast_b128(c_lds[1])[0];           \
+        cast_b128(rgC)[2 + phase * 8] = cast_b128(c_lds[0])[1 * 16 * 32]; \
+        cast_b128(rgC)[3 + phase * 8] = cast_b128(c_lds[1])[1 * 16 * 32]; \
+        cast_b128(rgC)[4 + phase * 8] = cast_b128(c_lds[0])[2 * 16 * 32]; \
+        cast_b128(rgC)[5 + phase * 8] = cast_b128(c_lds[1])[2 * 16 * 32]; \
+        cast_b128(rgC)[6 + phase * 8] = cast_b128(c_lds[0])[3 * 16 * 32]; \
+        cast_b128(rgC)[7 + phase * 8] = cast_b128(c_lds[1])[3 * 16 * 32]; \
+        float *reduc_c = reinterpret_cast<float *>(rgC);                  \
+        for (int loop = 0; loop < 8; ++loop)                              \
+        {                                                                 \
+            float acc = 0;                                                \
+            acc += reduc_c[loop + phase * 32 + 0 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 1 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 2 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 3 * 8];                    \
+            reduc_c[loop + phase * 8] = acc;                              \
+        }                                                                 \
+    }
+
+template <Operation_t TransA,
+          Operation_t TransB,
+          bool IsBetaZero,
+          int tileM,
+          int tileN,
+          int tileK,
+          bool splitk = false,
+          bool Swap = false>
+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_awq_4bit(int m,
+                                                                           int n,
+                                                                           int k,
+                                                                           const scalar_type alpha,
+                                                                           const scalar_type beta,
+                                                                           const quant_packed_type *dA_input,
+                                                                           int lda,
+                                                                           const input_type *dB_input,
+                                                                           int ldb,
+                                                                           output_type *dC_input,
+                                                                           output_type *dC_output,
+                                                                           int ldc,
+                                                                           quant_packed_type *d_zeros,
+                                                                           input_type *d_scales,
+                                                                           int splitk_iters = 1,
+                                                                           acc_type * d_acc_tmp=nullptr)
+{
+    __shared__ uint8_t smem_base[0x8000];
+    int bidx = Swap ? blockIdx.y : blockIdx.x;
+    int bidy = Swap ? blockIdx.x : blockIdx.y;
+    int bidz = blockIdx.z;
+    int tid = threadIdx.x;
+
+    uint64_t arowstride = TransA == OP_N ? 1 : lda;
+    uint64_t acolstride = TransA == OP_N ? lda : 1;
+    uint64_t browstride = TransB == OP_N ? 1 : ldb;
+    uint64_t bcolstride = TransB == OP_N ? ldb : 1;
+
+    const int malign = 8;
+    const int nalign = 1;
+    const int kalign = 8;
+    int align_m = (m + malign - 1) / malign * malign;
+    int align_n = (n + nalign - 1) / nalign * nalign;
+    int align_k = (k + kalign - 1) / kalign * kalign;
+
+    int k_num = (align_k + splitk_iters - 1) / splitk_iters;
+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
+    int k_begin = bidz * k_num;
+    int k_end = (bidz + 1) * k_num;
+    k_end = k_end > align_k ? align_k : k_end;
+    k_num = k_end - k_begin;
+    int ktail = k_num % tileK;
+    int kloop = k_begin;
+    if (k_begin > align_k)
+    {
+        return;
+    }    
+
+
+    int slot = tid / 64;
+    int lane = tid & 63;
+    int m64d16 = lane / 16;
+    int m64m16 = lane % 16;
+    output_type *c_base_i = dC_input;
+    output_type *c_base_o = dC_output;
+
+
+    quant_packed_type rga[8];
+    b128VecType rgb[2];
+
+    b64VecType rgA[16], rgB[4], rgCi[4];
+    Float4VecType rgC[16];
+    uint32_t rgZeros[1];
+    b128VecType rgScales[1];
+
+    quant_packed_type *dA[1];
+    input_type *dB[2];
+
+    // ldg A/B head
+
+    int rowA = m64m16 * 8;
+    int colA = m64d16 * 8 + slot * 32;
+    uint current_m = bidx * tileM + rowA;
+    bool pred_m = current_m < align_m;
+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m / PACK_RATIO) * (uint64_t)(arowstride) +
+            (uint64_t)(colA + k_begin) * (uint64_t)(acolstride / PACK_RATIO);
+
+    int colB = m64d16 + slot * 4;
+    int rowB = m64m16 * 8;
+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
+    int current_n = bidy * tileN + colB;
+    bool pred_n0 = current_n < align_n;
+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    current_n += 16;
+    bool pred_n1 = current_n < align_n;
+    dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    // ldgB to BSM need swizzle
+    input_type *b_sts = (input_type *)smem_base;
+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
+
+    // lds need swizzle
+    input_type *b_lds = (input_type *)smem_base;
+    int colB_lds = m64m16;
+    int rowB_lds = m64d16 * 8 + slot * 32;
+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
+
+    // ldg zeros and scales
+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO) * (kloop / tileK) + bidx * (tileM / PACK_RATIO) + tid;
+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + tid * 2;
+    quant_packed_type *lds_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO * sizeof(quant_packed_type)) + m64m16 * 8;
+
+    // clear C registers
+    rgC[0] = 0;
+    rgC[1] = 0;
+    rgC[2] = 0;
+    rgC[3] = 0;
+    rgC[4] = 0;
+    rgC[5] = 0;
+    rgC[6] = 0;
+    rgC[7] = 0;
+    rgC[8] = 0;
+    rgC[9] = 0;
+    rgC[10] = 0;
+    rgC[11] = 0;
+    rgC[12] = 0;
+    rgC[13] = 0;
+    rgC[14] = 0;
+    rgC[15] = 0;
+
+    int aincr = tileK * acolstride / PACK_RATIO;
+    int bincr = tileK * browstride;
+
+    if (k_num > ktail)
+    {
+        LDG_B;
+        LDG_ZEROS;
+        LDG_SCALES;
+        LDG_A;
+
+        dA[0] += aincr;
+        dB[0] += bincr;
+        dB[1] += bincr;
+
+        // lds_A, need swizzle
+        arrive_gvmcnt(8);
+        barrier();
+        LDS_B;
+        LDS_ZEROS;
+        LDS_SCALES;
+        arrive_bsmcnt(0);
+        barrier();
+
+        ldg_zeros_offset += lda / PACK_RATIO;
+        ldg_scales_offset += lda;
+
+// KLOOP
+#pragma unroll 1
+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
+        {
+            // load next B
+            LDG_B;
+            LDG_ZEROS;
+            LDG_SCALES;
+
+            // MMA_ABC1;
+            arrive_gvmcnt(4);
+            PERM_A;
+            LDG_A;
+            MMA;
+
+            // sts && lds
+            arrive_gvmcnt(8);
+            barrier();
+            LDS_B;
+            LDS_ZEROS;
+            LDS_SCALES;
+            arrive_bsmcnt(0);
+            barrier();
+
+            dA[0] += aincr;
+            dB[0] += bincr;
+            dB[1] += bincr;
+            ldg_zeros_offset += lda / PACK_RATIO;
+            ldg_scales_offset += lda;
+        }
+
+        arrive_gvmcnt(0);
+        PERM_A;
+        MMA;
+    }
+
+    // final tail kloop
+    if (ktail > 0)
+    {
+        LDG_B_HEAD;
+        LDG_ZEROS;
+        LDG_SCALES;
+        LDG_A_HEAD;
+        arrive_gvmcnt(8);
+        barrier();
+        LDS_B;
+        LDS_ZEROS;
+        LDS_SCALES;
+        arrive_bsmcnt(0);
+        barrier();
+        arrive_gvmcnt(0);
+        PERM_A;
+        MMA;
+    }
+
+    // store C registers into BSM & do reduction
+    int colC = m64m16 + slot * 16;
+    int rowC = m64d16 * 32;
+    scalar_type *c_sts[8];
+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 16, (colC % 4 * 4)), colC, 128);
+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 20, (colC % 4 * 4)), colC, 128);
+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 24, (colC % 4 * 4)), colC, 128);
+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 28, (colC % 4 * 4)), colC, 128);
+
+    colC = m64d16 + slot * 4;
+    rowC = m64m16 * 4;
+    scalar_type *c_lds[2];
+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+
+    PERM_C(rgC);
+    STS_C(0);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(0);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    PERM_C((rgC + 8))
+    STS_C(1);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(1);
+
+    arrive_bsmcnt(0);
+
+    // read C_input if beta!=0
+    colC = m64d16 + slot * 4 + bidy * tileN;
+    rowC = m64m16 * 4 + bidx * tileM;
+    int colC1 = colC + 16;
+    int rowC1 = rowC + 64;
+
+
+    if constexpr (!IsBetaZero && !splitk)
+    {
+        ldg_b64_reg_async(cast_b64(rgCi)[0],
+                          c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
+                          rowC < align_m && colC < align_n, true);
+        ldg_b64_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
+                          rowC1 < align_m && colC < align_n, true);
+        ldg_b64_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
+                          rowC < align_m && colC1 < align_n, true);
+        ldg_b64_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
+                          rowC1 < align_m && colC1 < align_n, true);
+        arrive_gvmcnt(0);
+    }
+
+    if constexpr (IsBetaZero && !splitk)
+    {
+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
+    }
+
+
+    if constexpr (!splitk)
+    {
+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC < align_m && colC < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC < align_m && colC1 < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            output_type result[4];
+            result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            ptrO[0] = cast_b64(result)[0];
+        }
+    }
+    else
+    {
+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC < align_m && colC < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC < align_m && colC1 < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
+        }
+    }
+}
\ No newline at end of file
diff --git a/csrc/quantization/awq/awq_4bits.cuh b/csrc/quantization/awq/awq_4bits.cuh
new file mode 100644
index 000000000..b5012b9b0
--- /dev/null
+++ b/csrc/quantization/awq/awq_4bits.cuh
@@ -0,0 +1,125 @@
+#pragma once
+
+#include "../gptq/Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+
+#define QBITS 4
+#define PACK_RATIO (32 / QBITS)
+
+#define LDG_A                                                                         \
+    {                                                                                 \
+        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m, true); \
+        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m, true); \
+    }
+
+#define LDG_A_HEAD                                                                    \
+    {                                                                                 \
+        bool predk = colA < ktail;                                                    \
+        ldg_b32_reg_async(rga[0], dA[0] + acolstride * 0 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[1], dA[0] + acolstride * 1 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[2], dA[0] + acolstride * 2 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[3], dA[0] + acolstride * 3 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[4], dA[0] + acolstride * 4 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[5], dA[0] + acolstride * 5 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[6], dA[0] + acolstride * 6 / PACK_RATIO, pred_m && predk, true); \
+        ldg_b32_reg_async(rga[7], dA[0] + acolstride * 7 / PACK_RATIO, pred_m && predk, true); \
+    }
+
+// blocktileM/PACK_RATIOtileMzeroszerosint432bits8zeros
+#define LDG_ZEROS                                                                                                                              \
+    {                                                                                                                                          \
+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO && (bidx * tileM + tid * PACK_RATIO < align_m), false); \
+    }
+
+// blocktileM/(sizeof(uint32_t)/sizeof(__half))tileMscalesscalefp1632bits2scales
+#define LDG_SCALES                                                                                                      \
+    {                                                                                                                   \
+        ldg_b32_bsm_async(sts_scales_offset, ldg_scales_offset, tid < 64 && (bidx * tileM + tid * 2 < align_m), false); \
+    }
+
+#define LDS_ZEROS                         \
+    {                                     \
+        rgZeros[0] = lds_zeros_offset[0]; \
+    }
+
+#define LDS_SCALES                                     \
+    {                                                  \
+        rgScales[0] = cast_b128(lds_scales_offset)[0]; \
+    }
+
+// zeros 0 2 4 6 1 3 5 7  index_shfl = index % 2 * 4 + index / 2
+//    int4zerosint32weightAfp16Weight_Q=Scale*(Weight_4Bit-ZeroPoint)
+#define PERM_ELEM(index)                                                                                                          \
+    {                                                                                                                             \
+        __half_raw elem;                                                                                                          \
+        if constexpr (index & 0x1)                                                                                                \
+        {                                                                                                                         \
+            elem.x = cast_b32(rgScales)[index / 2] >> 16;                                                                         \
+        }                                                                                                                         \
+        else                                                                                                                      \
+        {                                                                                                                         \
+            elem.x = cast_b32(rgScales)[index / 2] & 0xffff;                                                                      \
+        }                                                                                                                         \
+        __half scale = __half(elem);                                                                                              \
+        constexpr int index_shfl = index % 2 * 4 + index / 2;                                                                     \
+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[0], QBITS * index_shfl, QBITS);                                                \
+        cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[0], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[1], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[2], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[3], QBITS * index_shfl, QBITS) -  \
+                                                                         zero) , scale);                                                                                    \
+        cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[4], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+        cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[5], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+        cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[6], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+        cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[7], QBITS * index_shfl, QBITS) - \
+                                                                          zero) , scale);                                                                                   \
+    }
+
+#define PERM_A       \
+    {                \
+        PERM_ELEM(0) \
+        PERM_ELEM(1) \
+        PERM_ELEM(2) \
+        PERM_ELEM(3) \
+        PERM_ELEM(4) \
+        PERM_ELEM(5) \
+        PERM_ELEM(6) \
+        PERM_ELEM(7) \
+    }
+
+#define MMA_ELEM(index_m)                                        \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);
+
+#define MMA     \
+    MMA_ELEM(0) \
+    MMA_ELEM(1) \
+    MMA_ELEM(2) \
+    MMA_ELEM(3) \
+    MMA_ELEM(4) \
+    MMA_ELEM(5) \
+    MMA_ELEM(6) \
+    MMA_ELEM(7)
diff --git a/csrc/quantization/awq/dequant.cuh b/csrc/quantization/awq/dequant.cuh
new file mode 100644
index 000000000..7d6db705c
--- /dev/null
+++ b/csrc/quantization/awq/dequant.cuh
@@ -0,0 +1,10 @@
+#pragma once
+
+#define BFE(source, bit_start, num_bits) __builtin_mxc_ubfe(source, bit_start, num_bits);
+
+template <typename outputT, typename inputT, int qbits>
+__device__ __forceinline__ outputT extract(inputT *input, size_t loc)
+{
+    constexpr int widthPerElem = sizeof(inputT) * 8 / qbits;
+    return (outputT)BFE(input[loc / widthPerElem], loc % widthPerElem, qbits);
+}
\ No newline at end of file
diff --git a/csrc/quantization/awq/dequantize.cuh b/csrc/quantization/awq/dequantize.cuh
index 5fa4b5f64..1a47e3237 100644
--- a/csrc/quantization/awq/dequantize.cuh
+++ b/csrc/quantization/awq/dequantize.cuh
@@ -14,6 +14,7 @@ Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
 namespace vllm {
 namespace awq {
 
+template<typename VT>
 __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
 #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 750
   assert(false);
@@ -39,6 +40,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
   // Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW
   // dependency if we issue immediately before required.
   const uint32_t top_i4s = i4s >> 8;
+  #ifndef USE_MACA
   // Extract elt_01 - (i4s & 0x000f000f) | 0x64006400
   asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                : "=r"(h[0])
@@ -59,6 +61,63 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
                : "=r"(h[3])
                : "r"(top_i4s), "n"(TOP_MASK), "n"(I4s_TO_F16s_MAGIC_NUM),
                  "n"(immLut));
+#else
+      // >>>> PTX2CPP Success <<<<
+{
+(h[0])=0;
+if((immLut)&0x01)(h[0])|=~(i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[0])|=~(i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[0])|=~(i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[0])|=~(i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[0])|= (i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[0])|= (i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[0])|= (i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[0])|= (i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+
+    // Extract elt_23 (i4s & 0x00f000f0) | 0x64006400
+
+// >>>> PTX2CPP Success <<<<
+{
+(h[1])=0;
+if((immLut)&0x01)(h[1])|=~(i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[1])|=~(i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[1])|=~(i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[1])|=~(i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[1])|= (i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[1])|= (i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[1])|= (i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[1])|= (i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+// >>>> PTX2CPP Success <<<<
+{
+(h[2])=0;
+if((immLut)&0x01)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[2])|=~(top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[2])|=~(top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[2])|=~(top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[2])|= (top_i4s)&~(BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[2])|= (top_i4s)&~(BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[2])|= (top_i4s)& (BOTTOM_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[2])|= (top_i4s)& (BOTTOM_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+    // Extract elt_67 (top_i4s & 0x00f000f0) | 0x64006400
+
+// >>>> PTX2CPP Success <<<<
+{
+(h[3])=0;
+if((immLut)&0x01)(h[3])|=~(top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x02)(h[3])|=~(top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x04)(h[3])|=~(top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x08)(h[3])|=~(top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x10)(h[3])|= (top_i4s)&~(TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x20)(h[3])|= (top_i4s)&~(TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x40)(h[3])|= (top_i4s)& (TOP_MASK)&~(I4s_TO_F16s_MAGIC_NUM);
+if((immLut)&0x80)(h[3])|= (top_i4s)& (TOP_MASK)& (I4s_TO_F16s_MAGIC_NUM);
+}
+
+
+#endif //USE_MACA
 
   // I use inline PTX below because I am not sure if the compiler will emit
   // float2half instructions if I use the half2 ctor. In this case, I chose
@@ -77,6 +136,7 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
 
   // Finally, we construct the output numbers.
   // Convert elt_01
+  #ifndef USE_MACA
   asm volatile("sub.f16x2 %0, %1, %2;\n"
                : "=r"(h[0])
                : "r"(h[0]), "r"(FP16_TOP_MAGIC_NUM));
@@ -92,7 +152,54 @@ __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                : "=r"(h[3])
                : "r"(h[3]), "r"(ONE_SIXTEENTH), "r"(NEG_64));
+#else
+    // >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[0]);
+unsigned int __b=(FP16_TOP_MAGIC_NUM);
+VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
+(h[0])=*(unsigned int*)&__d;
+}
+}
+
+    // Convert elt_23
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[1]);
+unsigned int __b=(ONE_SIXTEENTH);
+unsigned int __c=(NEG_64);
+VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
+(h[1])=*(unsigned int*)&__d;
+}
+}
+    // Convert elt_45
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[2]);
+unsigned int __b=(FP16_TOP_MAGIC_NUM);
+VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
+(h[2])=*(unsigned int*)&__d;
+}
+}
+
+    // Convert elt_67
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(h[3]);
+unsigned int __b=(ONE_SIXTEENTH);
+unsigned int __c=(NEG_64);
+VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
+(h[3])=*(unsigned int*)&__d;
+}
+}
 
+#endif // USE_MACA
   return result;
 #endif
   __builtin_unreachable();  // Suppress missing return statement warning
diff --git a/csrc/quantization/awq/gemm_kernels.cu b/csrc/quantization/awq/gemm_kernels.cu
index 53c47679c..59f5d5279 100644
--- a/csrc/quantization/awq/gemm_kernels.cu
+++ b/csrc/quantization/awq/gemm_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 /*
 Adapted from https://github.com/mit-han-lab/llm-awq
 @article{lin2023awq,
@@ -14,9 +15,56 @@ Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
 
 #include <cuda_fp16.h>
 
+#include "../gptq/hgemm_gptq.h"
+#include "../gptq/scalar_type.hpp"
+
+//#include "hgemv_nn_splitk_awq.hpp"
+//#include "hgemv_selector.hpp"
+//#include "Hgemm_nn_128x32x128_8m1n8k_awq.hpp"
+
 namespace vllm {
 namespace awq {
+#define input_type __half
+#define output_type __half
+#define quant_packed_type uint32_t
+#define QUANT_GROUP 128
+
+struct DivModFast {
+    DivModFast(int d = 1)
+    {
+        d_ = (d == 0) ? 1 : d;
+        for (l_ = 0;; ++l_) {
+            if ((1U << l_) >= d_)
+                break;
+        }
+        uint64_t one = 1;
+        uint64_t m   = ((one << 32) * ((one << l_) - d_)) / d_ + 1;
+        m_           = static_cast<uint32_t>(m);
+    }
+
+    __device__ __inline__ int div(int idx) const
+    {
+        uint32_t tm = __umulhi(m_, idx); // get high 32-bit of the product
+        return (tm + idx) >> l_;
+    }
+
+    __device__ __inline__ int mod(int idx) const
+    {
+        return idx - d_ * div(idx);
+    }
 
+    __device__ __inline__ void divmod(int idx, int &quo, int &rem)
+    {
+        quo = div(idx);
+        rem = idx - quo * d_;
+    }
+    
+    uint32_t d_; // divisor
+    uint32_t l_; // ceil(log2(d_))
+    uint32_t m_; // m' in the papaer
+};
+
+#if 0
 template <int N>
 __global__ void __launch_bounds__(64)
     gemm_forward_4bit_cuda_m16nXk32(int G, int split_k_iters,
@@ -136,6 +184,7 @@ __global__ void __launch_bounds__(64)
       // - zero and * scale
       // TODO (Haotian): can save 4 assembly instructions if sormulate as deq =
       // q * scale - zero * scale.
+#ifdef MX_MACA
       asm volatile("sub.f16x2 %0, %1, %2;\n"
                    : "=r"(B_loaded_fp16.x)
                    : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
@@ -160,6 +209,7 @@ __global__ void __launch_bounds__(64)
       asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                    : "=r"(B_loaded_fp16.w)
                    : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
+#endif
       /*
       if (ax0_ax1_fused_0 == 0 && blockIdx_z == 0 && blockIdx_y == 0 && k_0_0 ==
       0 && threadIdx.x == 17 && threadIdx.y == 0){ printf("[x] %X %X %X %X\n",
@@ -176,6 +226,7 @@ __global__ void __launch_bounds__(64)
     for (int k_0_1 = 0; k_0_1 < 2; ++k_0_1) {
       {
         unsigned int addr;
+#ifdef MX_MACA
         __asm__ __volatile__(
             "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
             "addr; }\n"
@@ -192,11 +243,13 @@ __global__ void __launch_bounds__(64)
               "=r"(((unsigned*)(A_shared_warp + 0))[2]),
               "=r"(((unsigned*)(A_shared_warp + 0))[3])
             : "r"(addr));
+#endif
       }
 
       for (int ax1_0 = 0; ax1_0 < N / 32; ++ax1_0) {
         {
           unsigned int addr;
+#ifdef MX_MACA
           __asm__ __volatile__(
               "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, "
               "addr; }\n"
@@ -214,9 +267,11 @@ __global__ void __launch_bounds__(64)
                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[2]),
                 "=r"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[3])
               : "r"(addr));
+#endif
         }
       }
       for (int j_0_4 = 0; j_0_4 < N / 32; ++j_0_4) {
+#ifdef MX_MACA
   #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 750
         {
           __asm__ __volatile__(
@@ -329,12 +384,13 @@ __global__ void __launch_bounds__(64)
         }
 
   #endif
+#endif
       }
     }
   }
 
   // TODO: Shang: Hoist loop invariance.
-  for (int ax1_0_1 = 0; ax1_0_1 < (N / 32); ++ax1_0_1) {
+  for (int ax1_0_1 = 0; ax1_0_1 < 4; ++ax1_0_1) {
     for (int local_id = 0; local_id < 8; ++local_id) {
       int row_offset = (((int)blockIdx_y) / j_factors1) * 16 +
                        ((int)threadIdx.x) / 4 + (local_id % 4) / 2 * 8;
@@ -346,20 +402,22 @@ __global__ void __launch_bounds__(64)
   }
 #endif
 }
+#endif
 
+template<typename T, typename VT>
 __global__ void __launch_bounds__(64)
-    dequantize_weights(int* __restrict__ B, half* __restrict__ scaling_factors,
-                       int* __restrict__ zeros, half* __restrict__ C, int G) {
+    dequantize_weights(int* __restrict__ B, T* __restrict__ scaling_factors,
+                       int* __restrict__ zeros, T* __restrict__ C, int G) {
   static constexpr uint32_t ZERO = 0x0;
-  half B_shared[32 * (128 + 8)];
-
-  half* B_shared_ptr2 = B_shared;
+  T B_shared[8];
+  T B_loaded_scale[8];
+  T* B_shared_ptr2 = B_shared;
 
   int N = blockDim.x * gridDim.x;  // 2
   int col = (blockIdx.x * blockDim.x + threadIdx.x);
   int row = blockIdx.y * blockDim.y + threadIdx.y;
   int index1 = 8 * col + 8 * row * N;
-  half* C_ptr2 = C + index1;
+  T* C_ptr2 = C + index1;
 
   int index2 = col + row * N;
   int* B_ptr2 = B + index2;
@@ -367,14 +425,18 @@ __global__ void __launch_bounds__(64)
   int index3 = col + (int)(row / G) * N;
   int* zeros_ptr2 = zeros + index3;
   int index4 = 8 * col + (int)(row / G) * N * 8;
-  half* scaling_factors_ptr2 = scaling_factors + index4;
+  T* scaling_factors_ptr2 = scaling_factors + index4;
 
   uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr2);
-  uint4 B_loaded_zero = dequantize_s4_to_fp16x2(zeros_loaded);
-  uint4 B_loaded_scale = *(uint4*)(scaling_factors_ptr2);
-
+  *(uint4*)B_loaded_scale = *(uint4*)(scaling_factors_ptr2);
   uint32_t B_loaded = *(uint32_t*)B_ptr2;
-  uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2(B_loaded);
+#if 1
+  //zero //4bit scale 8bit b 4bit
+  hgemm_marlin_gptq::awq_dequant_4bits<T>(B_loaded,B_shared, B_loaded_scale, zeros_loaded);
+#else
+  uint4 B_loaded_zero = dequantize_s4_to_fp16x2<VT>(zeros_loaded);
+  uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2<VT>(B_loaded);
+#ifdef MX_MACA
   asm volatile("sub.f16x2 %0, %1, %2;\n"
                : "=r"(B_loaded_fp16.x)
                : "r"(B_loaded_fp16.x), "r"(B_loaded_zero.x));
@@ -399,17 +461,512 @@ __global__ void __launch_bounds__(64)
   asm volatile("fma.rn.f16x2 %0, %1, %2, %3;\n"
                : "=r"(B_loaded_fp16.w)
                : "r"(B_loaded_fp16.w), "r"(B_loaded_scale.w), "r"(ZERO));
+#else
+     // >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.x);
+unsigned int __b=(B_loaded_zero.x);
+VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
+(B_loaded_fp16.x)=*(unsigned int*)&__d;
+}
+}
+
 
-  *(uint4*)B_shared_ptr2 = B_loaded_fp16;
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.x);
+unsigned int __b=(B_loaded_scale.x);
+unsigned int __c=(ZERO);
+VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
+(B_loaded_fp16.x)=*(unsigned int*)&__d;
+}
+}
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.y);
+unsigned int __b=(B_loaded_zero.y);
+VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
+(B_loaded_fp16.y)=*(unsigned int*)&__d;
+}
+}
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.y);
+unsigned int __b=(B_loaded_scale.y);
+unsigned int __c=(ZERO);
+VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
+(B_loaded_fp16.y)=*(unsigned int*)&__d;
+}
+}
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.z);
+unsigned int __b=(B_loaded_zero.z);
+VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
+(B_loaded_fp16.z)=*(unsigned int*)&__d;
+}
+}
 
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.z);
+unsigned int __b=(B_loaded_scale.z);
+unsigned int __c=(ZERO);
+VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
+(B_loaded_fp16.z)=*(unsigned int*)&__d;
+}
+}
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.w);
+unsigned int __b=(B_loaded_zero.w);
+VT __d=__hsub2(*(VT*)&__a,*(VT*)&__b);
+(B_loaded_fp16.w)=*(unsigned int*)&__d;
+}
+}
+
+
+// >>>> PTX2CPP Success <<<<
+{
+{
+unsigned int __a=(B_loaded_fp16.w);
+unsigned int __b=(B_loaded_scale.w);
+unsigned int __c=(ZERO);
+VT __d=__hfma2(*(VT*)&__a,*(VT*)&__b,*(VT*)&__c);
+(B_loaded_fp16.w)=*(unsigned int*)&__d;
+}
+}
+*(uint4*)B_shared_ptr2 = B_loaded_fp16;
+#endif
+#endif
+  
   for (int i = 0; i < 8; ++i) {
     *(C_ptr2 + i) = B_shared[i];
   }
 }
 
+template<typename T>
+__global__ void dequantize_weights_opt(int* __restrict__ B, T* __restrict__ scaling_factors,
+                       int* __restrict__ zeros, T* __restrict__ C, int G, int length, int blocksize, int num_elems, DivModFast length_fast) {
+    constexpr int N = 8;
+    T B_loaded_scale[8];
+    T B_shared[8];
+    int tid = blockIdx.x * blocksize + threadIdx.x;
+    if(tid >= num_elems) return;
+    // int row = tid / length;
+    // int col = tid % length;
+    int row, col;
+    length_fast.divmod(tid, row, col);
+    int group_row = row / G;
+    int group_offset = group_row * length + col;
+    int offset = row * length + col;
+    uint32_t* ptr_zeros = (uint32_t*)(zeros + group_offset);
+    uint32_t* ptr_B = (uint32_t*)(B + offset);
+    T* ptr_scale = scaling_factors + group_offset * N;
+    T* ptr_C = C + offset * N;
+    uint32_t zeros_loaded = *(uint32_t*)ptr_zeros;
+    uint32_t B_loaded = *(uint32_t*)ptr_B;
+    *(uint4*)(B_loaded_scale) = *(uint4*)(ptr_scale);
+    hgemm_marlin_gptq::awq_dequant_4bits<T>(B_loaded,B_shared, B_loaded_scale, zeros_loaded);
+    *(float4*)(ptr_C) = *(float4*)(B_shared);
+}
+
+
+#if 0
+template <typename dstT, typename srcT, typename scalarT>
+__global__ void blasMemcpy(dstT *dst, const srcT *src, size_t cnt, scalarT beta) {
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (cnt + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
+        dst[loop * threads + tid] =
+            static_cast<double>(beta) * static_cast<double>(src[loop * threads + tid]);
+    }
+}
+
+#define SWITCH_CASE_BATCH(BlockDimX, SplitK, BATCH) \
+    case BATCH: {                                   \
+        CALL_GEMM(BlockDimX, SplitK, BATCH)         \
+        break;                                      \
+    }
+
+#define APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH) \
+    switch(BATCH) {                                 \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 1)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 2)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 3)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 4)           \
+        default: {                                          \
+            launched = false;                               \
+            printf("ERROR: Unsupported BATCH %d\n", BATCH); \
+            break;                                          \
+        }                                                   \
+    }
+
+#define SWITCH_CASE_BlockDimX(BlockDimX, SplitK, BATCH) \
+    case BlockDimX: {                                   \
+        APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH)    \
+        break;                                          \
+    }
+
+#define APPLY_HGEMM(BlockDimX, SplitK, BATCH)           \
+    switch (BlockDimX) {                                \
+        SWITCH_CASE_BlockDimX(16, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(32, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(64, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(128, SplitK, BATCH)       \
+        SWITCH_CASE_BlockDimX(256, SplitK, BATCH)       \
+        default: {                                                  \
+            launched = false;                                       \
+            printf("ERROR: Unsupported BlockDimX %d\n", BlockDimX); \
+            break;                                                  \
+        }                                                           \
+    }
+
+bool call_kernel(const half *srcB,
+    const quant_packed_type *srcA,
+    quant_packed_type *zeros, half *scales,
+    half* dst_D,
+    int m, int n, int k, int srcStride, int dstStride,
+    int block_x, int split_k,
+    const int* b_perm_D = nullptr) {
+    //constexpr int PACK_RATIO = 8;
+    constexpr int ThreadBlock = 256;
+    const dim3 threadBlock = {static_cast<unsigned int>(ThreadBlock)};
+    const dim3 gridBlock = {static_cast<unsigned int>((m + 8*block_x-1) / 8 / block_x), static_cast<unsigned int>(split_k)};
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    if (split_k * QUANT_GROUP > k || k % QUANT_GROUP != 0) return false;
+    if (block_x < 16 || n > 4) return false;
+    bool launched = true;
+    #define CALL_GEMM(BX, SK, N) \
+        if (SK * 128 == k) {   \
+            hgemv_nn_splitk_awq_kb128<BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+        } else { \
+            hgemv_nn_splitk_awq<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+        }
+    APPLY_HGEMM(block_x, split_k, n);
+    return launched;
+}
+
+void launch_gemm_awq(Operation_t trans_a,
+                      Operation_t trans_b,
+                      int m,
+                      int n,
+                      int k,
+                      const float alpha,
+                      const float beta,
+                      const uint32_t* dA,
+                      int lda,
+                      const half* dB,
+                      int ldb,
+                      half* dC,
+                      int ldc,
+                      uint32_t* d_zeros,
+                      half* d_scales,
+                      float* space_mid,
+                      cudaStream_t stream,
+                      int splitk_iters = 1) {
+    if (n <= 4) {
+            constexpr int thread_block = 256;
+            constexpr int m_per_thread = 8;
+            auto kernel_testing = [&](int bx, int sk) -> bool {
+                return call_kernel(dB, dA, d_zeros, d_scales, dC, m, n, k, m, m, bx, sk);
+            };
+            //Select parameters when warmup
+            auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,8,m_per_thread>::selector(m, n, k, true);
+            if (sl_warmup.valid()) {
+                sl_warmup.run(kernel_testing);
+            }
+    }
+    else {
+            const int threads_n = 256;
+            const int tileM = 128;
+            const int tileN = 32;
+            const int tileK = 128;
+
+            bool isSplitk = splitk_iters > 1;
+
+            uint32_t gridx = (m - 1) / tileM + 1;
+            uint32_t gridy = (n - 1) / tileN + 1;
+            uint32_t gridz = splitk_iters;
+
+            dim3 dimBlock(threads_n, 1, 1);
+            dim3 dimGrid(gridx, gridy, gridz);
+            bool isBetaZero = (beta == 0.0);
+
+            if (trans_a == OP_N && trans_b == OP_N && m % 8 == 0 && k % 8 == 0) {
+                if (!isSplitk) {
+                    if (isBetaZero)
+                        Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, true, tileM, tileN, tileK>
+                            <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC,
+                                                               dC, ldc, d_zeros, d_scales);
+                    else
+                        Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, false, tileM, tileN, tileK>
+                            <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC,
+                                                               dC, ldc, d_zeros, d_scales);
+                } else {
+                    if (!isBetaZero)
+                        blasMemcpy<<<104, 512, 0, stream>>>(space_mid, dC, m * n, beta);
+                    Hgemm_nn_128x32x128_8m1n8k_awq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true>
+                        <<<dimGrid, dimBlock, 0, stream>>>(m, n, k, alpha, beta, dA, lda, dB, ldb, dC, dC,
+                                                           ldc, d_zeros, d_scales, splitk_iters, space_mid);
+                    blasMemcpy<<<104, 512, 0, stream>>>(dC, space_mid, m * n, 1);
+                }
+            } else {
+                printf("Parameters not supported!\n");
+                return;
+            }
+    }
+}
+#endif
+
+template <int BLOCK_SIZE>
+__global__ void awq_to_gptq_4bit(uint32_t *output, const uint32_t *input, int k, int n) {
+    constexpr int COMPACT_FACTOR = 8;
+    constexpr int QBIT = 4;
+    int tid = threadIdx.x;
+    int tile_idx = blockIdx.x * BLOCK_SIZE + tid;
+    int N_COMPACT = (n + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
+    int K_COMPACT = (k + COMPACT_FACTOR - 1) / COMPACT_FACTOR;
+    int tile_n_idx = tile_idx / K_COMPACT;
+    int tile_k_idx = tile_idx % K_COMPACT;
+
+    uint32_t awq_data[COMPACT_FACTOR];
+    uint32_t temp_data[COMPACT_FACTOR];
+    uint32_t gptq_data[COMPACT_FACTOR];
+
+    int gptq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
+    int awq_shift[COMPACT_FACTOR] = {0, 4, 1, 5, 2, 6, 3, 7};
+
+    // load k8xn8
+    #pragma unroll
+    for (int i = 0; i < COMPACT_FACTOR; i++) {
+        int gvm_addr_offset = (tile_k_idx * COMPACT_FACTOR + i) * N_COMPACT + tile_n_idx;
+        int pred_k = tile_k_idx * COMPACT_FACTOR + i < k;
+        int pred_n = tile_n_idx * COMPACT_FACTOR < n;
+        if (pred_k && pred_n) {
+            awq_data[i] = *(input + gvm_addr_offset);
+        }
+    }
+
+    // decompress awq_data and recompress to gptq_data
+    #pragma unroll
+    for (int i = 0; i < COMPACT_FACTOR; i++) {
+        #pragma unroll
+        for(int j = 0; j < COMPACT_FACTOR; j++) {
+            temp_data[j] = ((awq_data[j] >> (awq_shift[i] * QBIT)) & 0xf);
+        }
+        #pragma unroll
+        for(int j = 0; j < COMPACT_FACTOR; j++) {
+            gptq_data[i] &= (~(0xf << (gptq_shift[j] * QBIT)));
+            gptq_data[i] |= temp_data[j] << (gptq_shift[j] * QBIT);
+
+        }
+    }
+
+    // store k8xn8
+    #pragma unroll
+    for (int i = 0; i < COMPACT_FACTOR; i++) {
+        int gvm_addr_offset = tile_k_idx * n + tile_n_idx * COMPACT_FACTOR + i;
+        int pred_k = tile_k_idx * COMPACT_FACTOR < k;
+        int pred_n = tile_n_idx * COMPACT_FACTOR + i < n;
+        if (pred_k && pred_n) {
+            *(output + gvm_addr_offset) = gptq_data[i];
+        } else {
+            *(output + gvm_addr_offset) = 0x00000000;
+        }
+    }
+}
+
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm_gptq(int m,
+                      int n,
+                      int k,
+                      int quant_group,
+                      const input_tp *dA,
+                      int lda,
+                      const quant_packed_tp *dB,
+                      int ldb,
+                      output_tp *dC,
+		      float *dC_temp,
+                      int ldc,
+                      quant_packed_tp *d_zeros,
+                      input_tp *d_scales,
+                      const cudaStream_t stream,
+                      int chunks = 1) {
+    using namespace hgemm_marlin_gptq;
+    if(n % 16 != 0) {
+        printf("n %% 16 != 0, n = %d\n", n);
+        return false;
+    }
+    if(k % 32 != 0) {
+        printf("k %% 32 != 0, k = %d\n", k);
+        return false;
+    }
+    //const vllm::ScalarTypeId w_type_id = vllm::kU4B8.id();
+    const int THREADS = 256;
+    int BLOCKS_M = div_ceil(m, SLICE_M);
+    if(BLOCKS_M >= MAX_BLOCKS_M && BLOCKS_M % MAX_BLOCKS_M != 0) {
+        printf("Error: input m is error, m = %d, blocks_m = %d\n", m, BLOCKS_M);
+        return false;
+    }
+    if (BLOCKS_M > MAX_BLOCKS_M) BLOCKS_M = MAX_BLOCKS_M;
+    int BLOCKS_N = 8;
+    //It is better let TILE_K = quant_group
+    //But if quant_group is too large, a quant_group can be divided into two parts
+    int BLOCKS_K = quant_group / SLICE_K;
+    if (quant_group > 128) BLOCKS_K = 128 / SLICE_K;
+    //if (BLOCKS_M == 1 || BLOCKS_M == 2) {
+    //    BLOCKS_N = 16;
+    //}
+    const bool HAS_ACT_ORDER = false;
+    const bool HAS_ZP = (w_type_id == vllm::kU4.id()) || (w_type_id == vllm::kU8.id());
+    int *g_idx = nullptr;
+    bool HAS_NK_PRED = true;
+    bool HAS_M_PRED = true;
+    if (n % TILE_N == 0 && k % TILE_K == 0) {
+        HAS_NK_PRED = false;
+    }
+    if (m % TILE_M == 0) {
+        HAS_M_PRED = false;
+    }
+
+#define LAUNCH_AWQ(threads, bm, bn, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    else if (THREADS == threads && BLOCKS_M == bm && BLOCKS_N == bn \
+        && BLOCKS_K == bk  && HAS_ACT_ORDER == has_act_order \
+        && HAS_ZP == has_zp \
+        && HAS_M_PRED == has_m_pred && HAS_NK_PRED == has_nk_pred) { \
+            launch_gemm_gptq_kernel<input_tp, w_type_id, \
+                    threads, bm, bn, bk, has_act_order, has_zp, has_m_pred, has_nk_pred>( \
+                    (const PackTypeInt4*)dA, \
+                    (const PackTypeInt4*)dB, \
+                    (PackTypeInt4*)dC, \
+                    (PackTypeInt4*)dC_temp, \
+                    (const PackTypeInt4*)d_scales, \
+                    (const PackTypeInt4*)d_zeros, \
+                    nullptr, m, n, k, quant_group, chunks,\
+                    stream); \
+    }
+
+#define LAUNCH_AWQ_K(bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 1, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 2, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 3, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 4, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 1, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ(256, 2, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred)
+
+
+#define LAUNCH_AWQ_ZP(has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(1, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(2, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(4, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_K(8, false, has_zp, has_nk_pred, has_m_pred)
+
+#define LAUNCH_AWQ_PRED(has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_ZP(false, has_nk_pred, has_m_pred) \
+    LAUNCH_AWQ_ZP(true, has_nk_pred, has_m_pred)
+
+    if (false) {
+
+    }
+    LAUNCH_AWQ_PRED(true, true)
+    LAUNCH_AWQ_PRED(true, false)
+    LAUNCH_AWQ_PRED(false, true)
+    LAUNCH_AWQ_PRED(false, false)
+    else {
+        printf("BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_k=%d, THREADS=%d, HAS_ACT_ORDER=%d, HAS_ZP=%d, quant_group=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d is not supported\n",
+        BLOCKS_M, BLOCKS_N, BLOCKS_K, THREADS, HAS_ACT_ORDER, HAS_ZP, quant_group, HAS_M_PRED, HAS_NK_PRED);
+        return false;
+    }
+
+    return true;
+}
+
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm(int quant_group,
+                int m,
+                int n,
+                int k,
+                const input_tp *dA,
+                int lda,
+                const quant_packed_tp *dB,
+                int ldb,
+                output_tp *dC,
+		            float* dC_temp,
+                int ldc,
+                quant_packed_tp *d_zeros,
+                input_tp *d_scales,
+		            const cudaStream_t stream) {
+    using namespace hgemm_marlin_gptq;
+    //constexpr int max_blocks_m = 4;
+    int total_m_blocks = div_ceil(m, SLICE_M);
+    int chunks = total_m_blocks / MAX_BLOCKS_M;
+    int rest_blocks_m = total_m_blocks % MAX_BLOCKS_M;
+    // printf("m=%d,n=%d,k=%d,lda=%d,ldb=%d,ldc=%d,total_m_blocks=%d,chunks=%d,rest_blocks_m=%d\n",
+    //     m, n, k, lda, ldb, ldc, total_m_blocks, chunks, rest_blocks_m
+    // );
+    //const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    // const int quant_group = 128;
+    bool ret = true;
+    if (chunks > 0) {
+        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
+        ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(real_m, n, k, quant_group, dA, lda, dB, ldb, dC, dC_temp, ldc, d_zeros, d_scales, stream, chunks);
+    }
+    if (rest_blocks_m > 0) {
+        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
+        ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(m - m_offset, n, k, quant_group, dA + lda * m_offset, lda, dB, ldb, dC + ldc * m_offset, dC_temp + ldc * m_offset, ldc, d_zeros, d_scales, stream, 1);
+    }
+
+    return ret;
+}
+
+
+
 }  // namespace awq
 }  // namespace vllm
 
+torch::Tensor awq_to_gptq_4bit(torch::Tensor qweight) {
+
+  const at::cuda::OptionalCUDAGuard device_guard(device_of(qweight));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  const uint32_t* qweight_ptr = reinterpret_cast<const uint32_t*>(qweight.data_ptr<int>());
+
+  int num_in_channels = qweight.size(0);
+  int num_out_channels = qweight.size(1) * 8;
+
+  int compact_n = (num_out_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;
+  int compact_output_k = (num_in_channels + hgemm_marlin_gptq::PACK_RATIO_4BITS - 1) / hgemm_marlin_gptq::PACK_RATIO_4BITS;;
+
+  int block_size = 256;
+  int tile_all_num = compact_n * compact_output_k;
+  int grid_size = (tile_all_num + 255) / 256;
+
+  auto options = torch::TensorOptions()
+                     .dtype(qweight.dtype())
+                     .device(qweight.device());
+
+  torch::Tensor out = torch::zeros({num_out_channels,  compact_output_k}, options);
+  uint32_t* out_ptr = reinterpret_cast<uint32_t*>(out.data_ptr<int>());
+
+  vllm::awq::awq_to_gptq_4bit<256><<<grid_size, block_size, 0, stream>>>((uint32_t*)out_ptr, (const uint32_t*)qweight_ptr, num_in_channels, num_out_channels);
+
+  return out;
+}
+
 torch::Tensor awq_dequantize(torch::Tensor _kernel,
                              torch::Tensor _scaling_factors,
                              torch::Tensor _zeros, int64_t split_k_iters,
@@ -418,7 +975,7 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
   int qout_c = _kernel.size(1);
   int out_c = qout_c * 8;
   int G = in_c / _scaling_factors.size(0);
-
+#if 0
   int x_thread = thx;
   int y_thread = thy;
 
@@ -436,27 +993,56 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
     x_blocks = (int)(qout_c / 8);
     y_blocks = (int)(in_c / 8);
   }
-
+#endif
   const at::cuda::OptionalCUDAGuard device_guard(device_of(_scaling_factors));
 
   auto options = torch::TensorOptions()
                      .dtype(_scaling_factors.dtype())
                      .device(_scaling_factors.device());
   at::Tensor _de_kernel = torch::empty({in_c, out_c}, options);
-
+  
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
   auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
-  auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());
-  auto scaling_factors =
-      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
   auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
-
+#if 1
+  int blocksize = 512;
+  int num_elems = in_c * qout_c;
+  int gridsize = (num_elems + blocksize - 1) / blocksize;
+  if(_scaling_factors.dtype() == at::ScalarType::Half) {
+    auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());
+    auto scaling_factors =
+        reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+    vllm::awq::dequantize_weights_opt<__half><<<gridsize, blocksize>>>(kernel, scaling_factors, zeros, de_kernel, G, qout_c, blocksize, num_elems, vllm::awq::DivModFast(qout_c));
+  } else if(_scaling_factors.dtype() == at::ScalarType::BFloat16) {
+    auto de_kernel = reinterpret_cast<maca_bfloat16*>(_de_kernel.data_ptr<at::BFloat16>());
+    auto scaling_factors =
+        reinterpret_cast<maca_bfloat16*>(_scaling_factors.data_ptr<at::BFloat16>());
+    vllm::awq::dequantize_weights_opt<maca_bfloat16><<<gridsize, blocksize>>>(kernel, scaling_factors, zeros, de_kernel, G, qout_c, blocksize, num_elems, vllm::awq::DivModFast(qout_c));
+  } else {
+    printf("not support this type\n");
+    assert(0);
+  }
+#else
   dim3 num_blocks(x_blocks, y_blocks);
   dim3 threads_per_block(x_thread, y_thread);
 
-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-  vllm::awq::dequantize_weights<<<num_blocks, threads_per_block, 0, stream>>>(
-      kernel, scaling_factors, zeros, de_kernel, G);
-
+  if(_scaling_factors.dtype() == at::ScalarType::Half) {
+    auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());
+    auto scaling_factors =
+        reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+    vllm::awq::dequantize_weights<__half,__half2><<<num_blocks, threads_per_block, 0, stream>>>(
+        kernel, scaling_factors, zeros, de_kernel, G);
+  } else if(_scaling_factors.dtype() == at::ScalarType::BFloat16) {
+    auto de_kernel = reinterpret_cast<maca_bfloat16*>(_de_kernel.data_ptr<at::BFloat16>());
+    auto scaling_factors =
+        reinterpret_cast<maca_bfloat16*>(_scaling_factors.data_ptr<at::BFloat16>());
+    vllm::awq::dequantize_weights<maca_bfloat16,maca_bfloat162><<<num_blocks, threads_per_block, 0, stream>>>(
+        kernel, scaling_factors, zeros, de_kernel, G);
+  } else {
+    printf("not support this type\n");
+    assert(0);
+  }
+#endif
   return _de_kernel;
 }
 
@@ -468,59 +1054,62 @@ torch::Tensor awq_dequantize(torch::Tensor _kernel,
 
 torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
                        torch::Tensor _scaling_factors, torch::Tensor _zeros,
-                       int64_t split_k_iters) {
+                       int64_t split_k_iters,
+                       torch::Tensor _temp_space,
+                       bool dtype_bf16) {
   int num_in_feats = _in_feats.size(0);
   int num_in_channels = _in_feats.size(1);
   const at::cuda::OptionalCUDAGuard device_guard(device_of(_in_feats));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 
   auto options = torch::TensorOptions()
                      .dtype(_in_feats.dtype())
                      .device(_in_feats.device());
+                     
+  // int num_out_channels = _kernel.size(1) * 8;
+  int num_out_channels = _kernel.size(0); 
   at::Tensor _out_feats =
-      torch::empty({split_k_iters, num_in_feats, _kernel.size(1) * 8}, options);
-  int num_out_feats = _out_feats.size(-2);
-  int num_out_channels = _out_feats.size(-1);
+      torch::zeros({num_in_feats, num_out_channels}, options);
 
-  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
+  //auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
   auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
-  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
-  auto scaling_factors =
-      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+  //auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
+  //auto scaling_factors =
+  //    reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
   auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
+  auto temp_space = reinterpret_cast<float*>(_temp_space.data_ptr<float>());
   int group_size = num_in_channels / _scaling_factors.size(0);
 
-  if (num_out_channels % 64 != 0)
-    throw std::invalid_argument("OC is not multiple of cta_N = 64");
-  if (num_out_channels % 8 != 0)
-    throw std::invalid_argument("OC is not multiple of pack_num = 8");
-  if (group_size % 32 != 0)
-    throw std::invalid_argument("Group size should be a multiple of 32");
-  if (num_out_channels % group_size != 0)
-    throw std::invalid_argument("OC is not multiple of Group size");
+#if 0
+  int lda = num_out_channels;
+  int ldb = num_in_channels;
+  int ldc = num_out_channels;
 
+  float alpha = 1.0, beta = 0.0;
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-  if (num_out_channels % 128 == 0) {
-    int j_factors1 = num_out_channels / 128 / 1;
-    dim3 num_blocks((num_out_feats + 16 - 1) / 16 * j_factors1 * split_k_iters);
-    // threadIdx.x: 32
-    // threadIdx.y: i_factors[2] * j_factors[2]
-    dim3 threads_per_block(32, 2);
-    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<128>
-        <<<num_blocks, threads_per_block, 0, stream>>>(
-            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
-            num_in_feats, num_in_channels, num_out_channels, out_feats);
-  } else if (num_out_channels % 64 == 0) {
-    int j_factors1 = num_out_channels / 64 / 1;
-    dim3 num_blocks(1 * (num_out_feats + 16 - 1) / 16 * j_factors1 *
-                    split_k_iters);
-
-    // threadIdx.x: 32
-    // threadIdx.y: i_factors[2] * j_factors[2]
-    dim3 threads_per_block(32, 2);
-    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<64>
-        <<<num_blocks, threads_per_block, 0, stream>>>(
-            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,
-            num_in_feats, num_in_channels, num_out_channels, out_feats);
+  vllm::awq::launch_gemm_awq(Operation_t(0), Operation_t(0), num_out_channels, num_in_feats, num_in_channels, alpha, beta, (const uint32_t*)kernel, lda,
+                             (const half*)in_feats, ldb,
+                             (half*)out_feats, ldc, (uint32_t*)zeros, (half*)scaling_factors, space_mid, stream, 3);
+#endif
+
+  int lda = num_in_channels;
+  int ldb = num_out_channels;
+  int ldc = num_out_channels;
+
+  if (dtype_bf16) {
+	  using scalar_t = __maca_bfloat16;
+	  vllm::awq::launch_gemm<scalar_t, vllm::kU4.id(), scalar_t, quant_packed_type>(group_size, num_in_feats, num_out_channels, num_in_channels,
+                                     (const scalar_t*)_in_feats.data_ptr(), lda, (const uint32_t*)kernel, ldb, (scalar_t*)_out_feats.data_ptr(), temp_space, ldc,
+                                     (uint32_t*)zeros, (scalar_t*)_scaling_factors.data_ptr(), stream);
+  } else {
+	  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());
+	  auto scaling_factors = reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());
+	  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());
+	  vllm::awq::launch_gemm<input_type, vllm::kU4.id(), output_type, quant_packed_type>(group_size, num_in_feats, num_out_channels, num_in_channels,
+                                     (const half*)in_feats, lda, (const uint32_t*)kernel, ldb, (half*)out_feats, nullptr, ldc,
+                                     (uint32_t*)zeros, (half*)scaling_factors, stream);
   }
-  return _out_feats.sum(0);
+
+
+  return _out_feats;
 }
diff --git a/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
new file mode 100644
index 000000000..39570b9da
--- /dev/null
+++ b/csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
@@ -0,0 +1,520 @@
+/**
+ * @file hgemv_nn_splitk.hpp
+ * @author Jiawen Yang (jiawen.yang@metax-tech.com)
+ * @brief *
+ * @version 0.1
+ * @date 2024-03-05
+ * @copyright Copyright (c) 2024
+ *
+ *   fp16 gemv kernel template for some gemv cases
+ *   Note:
+ *    1. BlockDimX * BlockDimY = 64, and BlockDimX should be 8/16/32/64
+ *    2. LoopNum % 2 == 0, so Load_B can use ldg_b32 or ldg_b64
+ *    3. m % (BlockDimX * 8) == 0
+ *    4. k % (ThreadBlock / BlockDimX * LoopNum * SplitKNum) = 0
+ *
+ *    A load layout:
+ *
+ *       **************************** Wave_0 ******************* | Wave_1  ...
+ *       ********* Repeat LoopNum *********                      |
+ *       tid_0(ldg_b128)   tid_0 ... tid_0 | tid_(BlockDimX) ... |
+ *       tid_1                             |                     |
+ *       tid_2                             |                     |
+ *                                       |                     |
+ *       tid_(BlockDimX-1)                 |                     |
+ *
+ */
+#pragma once
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include "../gptq/Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+typedef __NATIVE_VECTOR__(2, float) v2f;
+
+template<int N, int m_per_thread>
+__device__ __forceinline__ void dequant_fma_awq_int4(
+                const quant_packed_type& a,
+                const v2f (&scale)[m_per_thread/2],
+                const v2f (&zero)[m_per_thread/2],
+                const v2f (&b)[N],
+                v2f (&out)[N][m_per_thread/2]) {
+    uint32_t p0 = a & 0x0f0f0f0f;
+    float o1,o3;
+    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+    v2f a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[0], zero[0]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][0] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][0]);
+    }
+
+    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+    a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[2], zero[2]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][2] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][2]);
+    }
+
+    uint32_t p1 = (a >> 4) & 0x0f0f0f0f;
+    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p1));
+    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p1));
+    a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[1], zero[1]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][1] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][1]);
+    }
+
+    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o1):"r"(p1));
+    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o3):"r"(p1));
+    a0 = {o1, o3};
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale[3], zero[3]);
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        out[y][3] = __builtin_mxc_pk_fma_f32(a0, b[y], out[y][3]);
+    }
+};
+
+template <int ThreadBlock, int BlockDimX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    if (splitKOffset + k_block > k) k = k - splitKOffset;
+    else k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = PACK_RATIO;
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+
+    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
+    half *bsm_scales_ptr;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread/2];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
+    int m_index = tidRow * m_per_thread;
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        int loading_count = this_group_elements / loading_pack;
+        //Load needed zeros, scales
+        const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            int packed_group_offset = (x >> 2) << 3;
+            int packed_index = (x << 1) % 8;
+            int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
+            int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
+            //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
+            temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[dest_offset_0] = temp_scales[0];
+            bsm_scales_ptr[dest_offset_1] = temp_scales[1];
+        }
+
+        int loop_index = 0;
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        if (m_index < this_group_elements) {
+            v2f local_scales[m_per_thread/2];
+            for (int c = 0; c < m_per_thread / 2; c++) {
+                float s0 = (float)bsm_scales_ptr[m_index + c*2];
+                float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
+                local_scales[c] = {s0, s1};
+            }
+            v2f local_zeros[m_per_thread/2];
+            for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
+
+    #define DEQUANT_FMA(a, b) \
+            dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
+
+            quant_packed_type A[4];
+            const int packed_a_stride = srcAStride / PACK_RATIO;
+            int src_a_offset = (loop_index + i + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
+            A[0] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[1] = srcA[src_a_offset];
+
+            v2f local_b[4][N];
+            //#pragma unroll LoopNum / 4 - 1
+            for (; loop_index < LoopNum - 4; loop_index += 4) {
+                //Load A
+                src_a_offset += packed_a_stride;
+                A[2] = srcA[src_a_offset];
+                src_a_offset += packed_a_stride;
+                A[3] = srcA[src_a_offset];
+
+                for (int y = 0; y < N; y++) {
+                    float s[4];
+                    *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+                    local_b[0][y] = {s[0], s[0]};
+                    local_b[1][y] = {s[1], s[1]};
+                    local_b[2][y] = {s[2], s[2]};
+                    local_b[3][y] = {s[3], s[3]};
+                }
+                DEQUANT_FMA(A[0], local_b[0])
+                DEQUANT_FMA(A[1], local_b[1])
+                src_a_offset += packed_a_stride;
+                A[0] = srcA[src_a_offset];
+                src_a_offset += packed_a_stride;
+                A[1] = srcA[src_a_offset];
+                DEQUANT_FMA(A[2], local_b[2])
+                DEQUANT_FMA(A[3], local_b[3])
+            }
+            src_a_offset += packed_a_stride;
+            A[2] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[3] = srcA[src_a_offset];
+            for (int y = 0; y < N; y++) {
+                float s[4];
+                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+                local_b[0][y] = {s[0], s[0]};
+                local_b[1][y] = {s[1], s[1]};
+                local_b[2][y] = {s[2], s[2]};
+                local_b[3][y] = {s[3], s[3]};
+            }
+            DEQUANT_FMA(A[0], local_b[0])
+            DEQUANT_FMA(A[1], local_b[1])
+            DEQUANT_FMA(A[2], local_b[2])
+            DEQUANT_FMA(A[3], local_b[3])
+        }
+        __syncthreads();
+    }
+#undef DEQUANT_FMA
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        if (m_index < this_group_elements) {
+            for (int i = 0; i < m_per_thread/2; i++) {
+                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
+                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
+            }
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
+
+template <int BX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_awq_kb128(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int ThreadBlock = 256;
+    constexpr int BlockDimX = BX;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = 128;
+    const int splitKOffset = blockIdx.y * k_block;
+
+    k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = PACK_RATIO;
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr, *smem;
+    half *bsm_scales_ptr;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x / PACK_RATIO;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread/2];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread/2; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+    int this_group_elements = (blockIdx.x + 1) * group_elements <= m ? group_elements : m - blockIdx.x * group_elements;
+
+    constexpr int quant_group = 0;
+    constexpr int loading_pack = 2;
+    int loading_count = this_group_elements / loading_pack;
+    //Load needed zeros, scales
+    const int shuffle_rank[8] = {0, 2, 4, 6, 1, 3, 5, 7};
+    for (int x = tid; x < loading_count; x += ThreadBlock) {
+        uint8_t temp_zeros = 0;
+        temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+        half temp_scales[2];
+        int packed_group_offset = (x >> 2) << 3;
+        int packed_index = (x << 1) % 8;
+        int dest_offset_0 = packed_group_offset + shuffle_rank[packed_index];
+        int dest_offset_1 = packed_group_offset + shuffle_rank[packed_index + 1];
+        //*((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+        temp_scales[0] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_0];
+        temp_scales[1] = scales[quant_group * scales_stride + m_offset_scales + dest_offset_1];
+        uint32_t z = temp_zeros;
+        uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS);
+        uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS);
+        float s1 = (float)(temp_scales[0]);
+        float s2 = (float)(temp_scales[1]);
+        //Store to shared memory
+        bsm_zeros_ptr[dest_offset_0] = (float)z0 * s1 * -1.0f;
+        bsm_zeros_ptr[dest_offset_1] = (float)z1 * s2 * -1.0f;
+        bsm_scales_ptr[dest_offset_0] = temp_scales[0];
+        bsm_scales_ptr[dest_offset_1] = temp_scales[1];
+    }
+
+    int loop_index = 0;
+
+    //Load B and transform to float
+    if (b_perm != nullptr) {
+        for (int y = 0; y < N; y++) {
+            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + tidCol * LoopNum + x] + y * k_stride];
+            }
+        }
+    } else {
+        for (int y = 0; y < N; y++) {
+            for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + tidCol * LoopNum + x + y * k_stride];
+            }
+        }
+    }
+
+    __syncthreads();
+
+    //Load zero and scale from bsm
+    int m_index = tidRow * m_per_thread;
+    if (m_index < this_group_elements) {
+        v2f local_scales[m_per_thread/2];
+        for (int c = 0; c < m_per_thread / 2; c++) {
+            float s0 = (float)bsm_scales_ptr[m_index + c*2];
+            float s1 = (float)bsm_scales_ptr[m_index + c*2+1];
+            local_scales[c] = {s0, s1};
+        }
+        v2f local_zeros[m_per_thread/2];
+        for (int c = 0; c < m_per_thread/2; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c*2],bsm_zeros_ptr[m_index + c*2+1]};
+
+#define DEQUANT_FMA(a, b) \
+        dequant_fma_awq_int4<N, m_per_thread>(a, local_scales, local_zeros, b, c_splited);
+
+        quant_packed_type A[4];
+        const int packed_a_stride = srcAStride / PACK_RATIO;
+        int src_a_offset = (loop_index + tidCol * LoopNum) * srcAStride / PACK_RATIO + m_offset_a + m_per_thread * tidRow / PACK_RATIO;
+        A[0] = srcA[src_a_offset];
+        src_a_offset += packed_a_stride;
+        A[1] = srcA[src_a_offset];
+
+        v2f local_b[4][N];
+        #pragma unroll LoopNum / 4 - 1
+        for (; loop_index < LoopNum - 4; loop_index += 4) {
+            //Load A
+            src_a_offset += packed_a_stride;
+            A[2] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[3] = srcA[src_a_offset];
+
+            for (int y = 0; y < N; y++) {
+                float s[4];
+                *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+                local_b[0][y] = {s[0], s[0]};
+                local_b[1][y] = {s[1], s[1]};
+                local_b[2][y] = {s[2], s[2]};
+                local_b[3][y] = {s[3], s[3]};
+            }
+            DEQUANT_FMA(A[0], local_b[0])
+            DEQUANT_FMA(A[1], local_b[1])
+            src_a_offset += packed_a_stride;
+            A[0] = srcA[src_a_offset];
+            src_a_offset += packed_a_stride;
+            A[1] = srcA[src_a_offset];
+            DEQUANT_FMA(A[2], local_b[2])
+            DEQUANT_FMA(A[3], local_b[3])
+        }
+        src_a_offset += packed_a_stride;
+        A[2] = srcA[src_a_offset];
+        src_a_offset += packed_a_stride;
+        A[3] = srcA[src_a_offset];
+        for (int y = 0; y < N; y++) {
+            float s[4];
+            *(float4*)s = *(float4*)(bsm_b_ptr+tidCol*LoopNum+loop_index+y*thread_groups*LoopNum);
+            local_b[0][y] = {s[0], s[0]};
+            local_b[1][y] = {s[1], s[1]};
+            local_b[2][y] = {s[2], s[2]};
+            local_b[3][y] = {s[3], s[3]};
+        }
+        DEQUANT_FMA(A[0], local_b[0])
+        DEQUANT_FMA(A[1], local_b[1])
+        DEQUANT_FMA(A[2], local_b[2])
+        DEQUANT_FMA(A[3], local_b[3])
+    }
+    __syncthreads();
+
+#undef DEQUANT_FMA
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        if (m_index < this_group_elements) {
+            for (int i = 0; i < m_per_thread/2; i++) {
+                smem[tidCol + (tidRow * m_per_thread + i*2) * ThreadBlock / BlockDimX] = c_splited[y][i].x;
+                smem[tidCol + (tidRow * m_per_thread + i*2+1) * ThreadBlock / BlockDimX] = c_splited[y][i].y;
+            }
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < this_group_elements; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
diff --git a/csrc/quantization/awq/hgemv_selector.hpp b/csrc/quantization/awq/hgemv_selector.hpp
new file mode 100644
index 000000000..b9d12a4e1
--- /dev/null
+++ b/csrc/quantization/awq/hgemv_selector.hpp
@@ -0,0 +1,287 @@
+#include <memory>
+#include <vector>
+#include <algorithm>
+#include "mc_runtime.h"
+#include "maca_fp16.h"
+
+struct KernelEventRecorder {
+    mcEvent_t _start;
+    mcEvent_t _stop;
+    float _eventMs = -1.f;
+
+    KernelEventRecorder() {
+        mcEventCreate(&_start);
+        mcEventCreate(&_stop);
+    }
+
+    ~KernelEventRecorder() {
+        mcEventDestroy(_start);
+        mcEventDestroy(_stop);
+    }
+
+    void start() {
+        mcEventRecord(_start, NULL);
+    }
+
+    float stop() {
+        mcEventRecord(_stop, NULL);
+        mcEventSynchronize(_stop);
+        mcEventElapsedTime(&_eventMs, _start, _stop);
+        return _eventMs;
+    }
+};
+namespace hgemv_selector {
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+struct GemvParamAutoSelector {
+    int m;
+    int k;
+    int best_block_x = 0;
+    int best_split_k = 0;
+
+    std::pair<int,int> block_x_range;
+    std::pair<int,int> split_k_range;
+    bool _valid = false;
+
+private:
+    std::vector<std::pair<int, int>> param_candidates;
+    int warmup_iters = 0;
+    int current_block_x = 0;
+    int current_split_k = 0;
+    int current_perf_iter = 0;
+    std::vector<float> perf_times;
+    float kernel_best_time_ms_ave = 99999999.0f;
+    float best_band_width;
+    float data_size_gb;
+    std::shared_ptr<KernelEventRecorder> _r;
+    bool _selected = false;
+    const static int MAX_PERF_COUNT = 20;
+
+public:
+    GemvParamAutoSelector(int _m, int _n, int _k, bool allow_imcomplete_bx = false) : m(_m), k(_k)
+    {
+        if (!prepare_parameters(_m, _n, _k, block_x_range.second, split_k_range.second, allow_imcomplete_bx)) return;
+        block_x_range.first = block_x_range.second;
+        split_k_range.first = split_k_range.second;
+        for (int i = 0; i < 3; i++) if (block_x_range.first / 2 >= 16) block_x_range.first /= 2;
+        for (int i = 0; i < 3; i++) if (split_k_range.first % 2 == 0 && split_k_range.first / 2 > 0) split_k_range.first /= 2;
+        for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+            for (int j = split_k_range.first; j <= split_k_range.second; j *= 2) {
+                param_candidates.emplace_back(i, j);
+            }
+        }
+        if (split_k_range.second * quant_group != k) {
+            int max_split_k = k / quant_group;
+            if (max_split_k < 256) {
+                for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+                    param_candidates.emplace_back(i, max_split_k);
+                }
+            }
+        }
+
+        current_block_x = block_x_range.second;
+        current_split_k = split_k_range.second;
+        uint64_t data_size_in_bytes = _n * _k * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_n * _m * sizeof(half);
+        data_size_gb = float(data_size_in_bytes) / 1024 / 1024 / 1024;
+        warmup_iters = 4;
+        _valid = true;
+    }
+
+    void select_in_warmup(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (_selected) {
+            f(best_block_x, best_split_k);
+            return;
+        };
+        //Warmup
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            for (int i = 0; i < 5; i++) {
+                auto &p = *iter;
+                f(p.first, p.second);
+            }
+        }
+        _r.reset(new KernelEventRecorder());
+        kernel_best_time_ms_ave = 9999999.0f;
+        mcDeviceSynchronize();
+
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            auto &p = *iter;
+            auto &bx = p.first;
+            auto &sk = p.second;
+            mcDeviceSynchronize();
+            _r->start();
+            bool launched = false;
+            for (int i = 0; i < MAX_PERF_COUNT; i++) {
+                launched = f(bx, sk);
+            }
+            auto ms = _r->stop();
+            if (launched && ms / MAX_PERF_COUNT < kernel_best_time_ms_ave) {
+                best_block_x = bx;
+                best_split_k = sk;
+                kernel_best_time_ms_ave = ms / MAX_PERF_COUNT;
+            }
+        }
+
+        _r.reset();
+        _selected = true;
+        warmup_iters = 0;
+    }
+
+    void run(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (warmup_iters > 0) {
+            f(current_block_x, current_split_k);
+            //printf("Calling warmup current_block_x = %d, current_split_k = %d\n", current_block_x, current_split_k);
+            if (current_block_x > block_x_range.first) current_block_x /= 2;
+            else if (current_split_k > split_k_range.first) current_split_k /= 2;
+            if (current_block_x == block_x_range.first && current_split_k >= split_k_range.first) {
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+            }
+            warmup_iters--;
+            mcDeviceSynchronize();
+            return;
+        }
+
+        if (_selected) {
+            f(best_block_x, best_split_k);
+        } else {
+            if (!_r) {
+                _r = std::shared_ptr<KernelEventRecorder>(new KernelEventRecorder());
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+                current_perf_iter = MAX_PERF_COUNT;
+            }
+            _r->start();
+            auto launched = f(current_block_x, current_split_k);
+            auto ms = _r->stop();
+            //printf("Launched with bx=%d,sk=%d,iter=%d,ms=%fms\n", current_block_x, current_split_k, current_perf_iter, ms);
+            if (!launched) {
+                printf("ERROR: Cannot run gemv with bx = %d, sk = %d\n", current_block_x, current_split_k);
+                return;
+            }
+            if (current_perf_iter-- > 0) {
+                perf_times.emplace_back(ms);
+                return;
+            }
+
+            std::sort(perf_times.begin(), perf_times.end());
+            float total_tm = 0;
+            for (size_t i = 1; i < perf_times.size() - 1; i++) {
+                total_tm += perf_times[i];
+            }
+
+            ms = total_tm /= (MAX_PERF_COUNT - 2);
+            perf_times.clear();
+            current_perf_iter = MAX_PERF_COUNT;
+            //printf("get ave time %fms\n", ms);
+
+            if (ms < kernel_best_time_ms_ave) {
+                best_block_x = current_block_x;
+                best_split_k = current_split_k;
+                kernel_best_time_ms_ave = ms;
+            }
+
+            if (current_split_k > split_k_range.first) {
+                current_split_k /= 2;
+            } else if (current_block_x > block_x_range.first){
+                current_split_k = split_k_range.second;
+                current_block_x /= 2;
+            } else if (current_block_x == block_x_range.first && current_split_k == split_k_range.first) {
+                _selected = true;
+                _r.reset();
+            } else {
+                printf("Error: all parameters are tried! current_block_x = %d, current_split_k = %d, block_x_range = %d,%d, split_k_range = %d,%d, best_block_x = %d, best_split_k = %d\n",
+                    current_block_x, current_split_k,
+                    block_x_range.first, block_x_range.second,
+                    split_k_range.first, split_k_range.second,
+                    best_block_x, best_split_k
+                );
+            }
+        }
+    }
+
+    bool valid() const { return _valid; }
+    bool selected() const {return _selected; }
+
+    float gemv_ave_time_us_cost() {
+        return kernel_best_time_ms_ave;
+    }
+
+    float gemv_bandwidth() {
+        return data_size_gb / (kernel_best_time_ms_ave / 1000.0f);
+    }
+
+    static bool prepare_parameters(int m, int n, int k, int &bx, int &sk, bool allow_imcomplete_bx = false) {
+        if (n > 4) return false;
+        if (k % quant_group != 0) return false;
+        if (m < 16 * m_per_thread) return false;
+        int max_split_k = k / quant_group;
+        int proper_splitk = 1;
+        while (proper_splitk*2 <= 256 && proper_splitk*2 <= max_split_k) proper_splitk *= 2;
+
+        int thread_block = 256;  //A thread block of 512 will wait for more warps when doing syncthreads
+
+        int proper_bx = 16;
+        if (m % (proper_bx * m_per_thread) != 0) return false;
+        //proper_bx * m_per_threads corresponds to group_elements, this value should not be larger than 2048 as limit of shared memory
+        while (proper_bx * 2 * m_per_thread && (m % (proper_bx*2*m_per_thread) == 0) && proper_bx * 2 <= 256 && proper_bx * 2 * m_per_thread <= 2048) proper_bx*=2;
+        if (allow_imcomplete_bx) {
+            int may_proper_bx = proper_bx * 2;
+            if (may_proper_bx <= 256 && may_proper_bx * m_per_thread <= 2048 && (m % (may_proper_bx*m_per_thread)) > may_proper_bx * m_per_thread / 4) {
+                proper_bx = may_proper_bx;
+            }
+        }
+
+        bx = proper_bx;
+        sk = proper_splitk;
+        //printf("m=%d,n=%d,k=%d,get bx=%d,sk=%d,allow_imcomplete_bx=%d\n", m, n, k, bx, sk, allow_imcomplete_bx);
+        return true;
+    }
+};
+
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+class GemvSelectorHolder {
+private:
+    std::vector<GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>> _selectors;
+    static std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> _holder;
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> _invalid_selector;
+
+public:
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& selector(int m, int n, int k, bool allow_imcomplete_bx = false) {
+        if (!GemvSelectorHolder::_holder) {
+            GemvSelectorHolder::_holder.reset(new GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>());
+        }
+        int bx, sk;
+        if (!GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>::prepare_parameters(m, n, k, bx, sk, allow_imcomplete_bx)) {
+            return _invalid_selector;
+        }
+        auto iter = std::find_if(_holder->_selectors.begin(), _holder->_selectors.end(),
+            [&](const GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& p) -> bool {
+                return p.m == m && p.k == k;
+            });
+        if (iter != _holder->_selectors.end()) return *iter;
+        GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> sl(m, n, k, allow_imcomplete_bx);
+        if (!sl.valid()) {
+            return _invalid_selector;
+        }
+        _holder->_selectors.emplace_back(sl);
+        return _holder->_selectors.back();
+    }
+};
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_holder;
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_invalid_selector(0,8,0);
+}
\ No newline at end of file
diff --git a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
index e79785827..4a1ba8c16 100644
--- a/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
+++ b/csrc/quantization/compressed_tensors/int8_quant_kernels.cu
@@ -1,3 +1,4 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
 #include <ATen/cuda/CUDAContext.h>
 #include <torch/all.h>
 #include <cmath>
@@ -30,8 +31,13 @@ static inline __device__ int8_t float_to_int8_rn(float x) {
   return static_cast<int8_t>(dst);
 #else
   // CUDA path
-  uint32_t dst;
-  asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
+  //uint32_t dst;
+  //asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
+  //return reinterpret_cast<const int8_t&>(dst);
+  int32_t dst;
+  dst = __float2int_rn(x);
+  dst = min(dst, 127);
+  dst = max(dst, -127);
   return reinterpret_cast<const int8_t&>(dst);
 #endif
 }
@@ -65,9 +71,13 @@ static inline __device__ int32_t float_to_int32_rn(float x) {
   return static_cast<int32_t>(dst);
 #else
   // CUDA path
-  uint32_t dst;
-  asm volatile("cvt.rni.sat.s32.f32 %0, %1;" : "=r"(dst) : "f"(x));
-  return reinterpret_cast<const int32_t&>(dst);
+  static constexpr auto i32_min = std::numeric_limits<int32_t>::min();
+  static constexpr auto i32_min_f = static_cast<float>(i32_min);
+  static constexpr auto i32_max = std::numeric_limits<int32_t>::max();
+  static constexpr auto i32_max_f = static_cast<float>(i32_max);
+  x = min(x, i32_max_f);
+  x = max(x, i32_min_f);
+  return __float2int_rn(x);
 #endif
 }
 
@@ -83,9 +93,14 @@ static inline __device__ int8_t int32_to_int8(int32_t x) {
   return static_cast<int8_t>(dst);
 #else
   // CUDA path
-  uint32_t dst;
-  asm volatile("cvt.sat.s8.s32 %0, %1;" : "=r"(dst) : "r"(x));
-  return reinterpret_cast<const int8_t&>(dst);
+  static constexpr auto i8_min =
+      static_cast<int32_t>(std::numeric_limits<int8_t>::min());
+  static constexpr auto i8_max =
+      static_cast<int32_t>(std::numeric_limits<int8_t>::max());
+
+  // saturate
+  int32_t dst = std::clamp(x, i8_min, i8_max);
+  return static_cast<int8_t>(dst);
 #endif
 }
 
@@ -165,6 +180,211 @@ __global__ void dynamic_scaled_int8_quant_kernel(
   }
 }
 
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__global__ void dynamic_scaled_int8_quant_kernel_sreg_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, int blockDim_x) {
+  int const tid = threadIdx.x;
+  int64_t const token_idx = blockIdx.x;
+  scalar_t absmax_val = static_cast<scalar_t>(0.0f);
+  float const zero = 0.0f;
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  scalar_t reg_src0[N];
+  scalar_t const* ptr_input = input + token_idx * hidden_size;
+  int reg_length = blockDim_x * N;
+  int length = min(hidden_size, reg_length);
+  int index = tid * N;
+  if(index < length) {
+    *(VT*)reg_src0 = *(VT*)(ptr_input + index);
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      scalar_t val = abs(reg_src0[i]);
+      absmax_val = max(absmax_val, val);
+    }
+  }
+
+  using BlockReduce = cub::BlockReduce<scalar_t, 512>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  float const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
+  __shared__ scale_type block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
+    scale[token_idx] = static_cast<scale_type>(block_absmax_val / 127.0f);
+  }
+  __syncthreads();
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  if(index < length) {
+    VT1 vdst;
+    int8_t* ptr_reg = (int8_t*)&vdst;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      ptr_reg[i] = float_to_int8_rn(
+            static_cast<float>(reg_src0[i]) * tmp_scale);
+    }
+    *(VT1*)(ptr_output + index) = vdst;
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__global__ void dynamic_scaled_int8_quant_kernel_reg_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, int blockDim_x) {
+  int const tid = threadIdx.x;
+  int64_t const token_idx = blockIdx.x;
+  scalar_t absmax_val = static_cast<scalar_t>(0.0f);
+  float const zero = 0.0f;
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  scalar_t reg_src0[N];
+  scalar_t reg_src1[N];
+  scalar_t const* ptr_input = input + token_idx * hidden_size;
+  int reg_length = 2 * blockDim_x * N;
+  int length = min(hidden_size, reg_length);
+  int index = 2 * tid * N;
+  if(index < length) {
+    *(VT*)reg_src0 = *(VT*)(ptr_input + index);
+    *(VT*)reg_src1 = *(VT*)(ptr_input + index + N);
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      scalar_t val = abs(reg_src0[i]);
+      absmax_val =  max(val, absmax_val);
+      val = abs(reg_src1[i]);
+      absmax_val = max(val, absmax_val);
+    }
+  }
+
+  using BlockReduce = cub::BlockReduce<scalar_t, 512>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  scalar_t const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim_x);
+  __shared__ scale_type block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = static_cast<scale_type>(block_absmax_val_maybe);
+    scale[token_idx] = block_absmax_val / 127.0f;
+  }
+  __syncthreads();
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  if(index < length) {
+    VT1 vdst;
+    int8_t* ptr_reg = (int8_t*)&vdst;
+    constexpr int ON = 2 * N;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      ptr_reg[i] = float_to_int8_rn(
+             static_cast<float>(reg_src0[i]) * tmp_scale);
+    }
+    ptr_reg = ptr_reg + N;
+    #pragma unroll N
+    for(int i = 0; i < N; i++) {
+      ptr_reg[i] = float_to_int8_rn(
+            static_cast<float>(reg_src1[i]) * tmp_scale);
+    }
+    *(VT1*)(ptr_output + index) = vdst;
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__global__ void dynamic_scaled_int8_quant_kernel_sm_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, int blockDim_x) {
+  int const tid = threadIdx.x;
+  int64_t const token_idx = blockIdx.x;
+  float absmax_val = 0.0f;
+  float const zero = 0.0f;
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  int stride = blockDim_x * N;
+  __shared__ float sm_buffer[8064];
+  scalar_t const* ptr_input = input + token_idx * hidden_size;
+  for(int i = tid * N; i < hidden_size; i += stride) {
+    VT vsrc = *(VT*)(ptr_input + i);
+    scalar_t *ptr_src = (scalar_t*)&vsrc;
+    float* ptr_sm_buffer = sm_buffer + i;
+    #pragma unroll N
+    for(int j = 0; j < N; j++) {
+        float val = static_cast<float>(ptr_src[j]);
+        ptr_sm_buffer[j] = val;
+        val = val > zero ? val : -val;
+        absmax_val = val > absmax_val ? val : absmax_val;
+    }
+  }
+  using BlockReduce = cub::BlockReduce<float, 512>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  float const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
+  __shared__ float block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = block_absmax_val_maybe;
+    scale[token_idx] = block_absmax_val / 127.0f;
+  }
+  
+  __syncthreads();
+
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  for(int i = tid * N; i < hidden_size; i += stride) {
+    VT1 vdst;
+    int8_t* ptr_reg = (int8_t*)&vdst;
+    float* ptr_sm_buffer = sm_buffer + i;
+    #pragma unroll N
+    for(int j = 0; j < N; j++) {
+        ptr_reg[j] = float_to_int8_rn(
+            ptr_sm_buffer[j] * tmp_scale);
+    }
+    *(VT1*)(ptr_output + i) = vdst;
+  }
+}
+
+template <typename scalar_t, typename scale_type, typename VT, typename VT1>
+__launch_bounds__(1024) __global__ void dynamic_scaled_int8_quant_kernel_opt(
+    scalar_t const* __restrict__ input, int8_t* __restrict__ out,
+    scale_type* scale, const int hidden_size, const int blockDim_x) {
+  constexpr int N = sizeof(VT) / sizeof(scalar_t);
+  int const tid = threadIdx.x * N;
+  int64_t const token_idx = blockIdx.x;
+  float absmax_val = 0.0f;
+  int stride = blockDim_x * N;
+  const scalar_t * ptr_input = input + token_idx * hidden_size;
+
+  for (int i = tid ; i < hidden_size; i += stride) {
+    VT vsrc = *(VT*)(ptr_input + i);
+    scalar_t *ptr_src = (scalar_t*)&vsrc;
+    #pragma unroll N
+    for(int j = 0; j < N; j++) {
+        float val = static_cast<float>(ptr_src[j]);
+        val = val > 0 ? val : -val;
+        absmax_val = val > absmax_val ? val : absmax_val;
+    }
+  }
+
+  using BlockReduce = cub::BlockReduce<float, 1024>;
+  __shared__ typename BlockReduce::TempStorage reduceStorage;
+  float const block_absmax_val_maybe =
+      BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);
+  __shared__ float block_absmax_val;
+  if (tid == 0) {
+    block_absmax_val = block_absmax_val_maybe;
+    scale[token_idx] = block_absmax_val / 127.0f;
+  }
+  __syncthreads();
+
+  float const tmp_scale = 127.0f / block_absmax_val;
+  int8_t* ptr_output = out + token_idx * hidden_size;
+  for (int i = tid; i < hidden_size; i += stride) {
+    VT vsrc = *(VT*)(ptr_input + i);
+    VT1 vdst;
+    scalar_t *ptr_src = (scalar_t*)&vsrc;
+    int8_t* ptr_dst = (int8_t*)&vdst;
+    #pragma unroll N
+    for(int j = 0; j < N; ++j) {
+        ptr_dst[j] = float_to_int8_rn(
+        static_cast<float>(ptr_src[j]) * tmp_scale);
+    }
+    *(VT1*)(ptr_output + i) = vdst;
+  }
+}
+
 template <typename scalar_t, typename scale_type, typename azp_type>
 __global__ void dynamic_scaled_int8_azp_quant_kernel(
     scalar_t const* __restrict__ input, int8_t* __restrict__ out,
@@ -264,17 +484,46 @@ void dynamic_scaled_int8_quant(
   TORCH_CHECK(!azp || azp->is_contiguous());
 
   int const hidden_size = input.size(-1);
-  int const num_tokens = input.numel() / hidden_size;
+  int64_t const num_tokens = input.numel() / hidden_size;
   dim3 const grid(num_tokens);
   dim3 const block(std::min(hidden_size, 1024));
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
   VLLM_DISPATCH_FLOATING_TYPES(
       input.scalar_type(), "dynamic_scaled_int8_quant_kernel", [&] {
         if (!azp) {
-          vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>
+          int n = 16 / sizeof(scalar_t);
+          if(hidden_size <= 4096 && ((hidden_size & (n - 1)) == 0) && n == 8) {
+            int64_t gridsize = num_tokens;
+            int blocksize = 512;
+            vllm::dynamic_scaled_int8_quant_kernel_sreg_opt<scalar_t, float, float4, float2>
+                <<<gridsize, blocksize, 0, stream>>>(
+                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+                  scales.data_ptr<float>(), hidden_size,blocksize);
+          } else if(hidden_size > 4096 &&hidden_size <= 8192 && ((hidden_size & (2*n - 1)) == 0) && n == 8) {
+            int64_t gridsize = num_tokens; int blocksize = 512;
+            vllm::dynamic_scaled_int8_quant_kernel_reg_opt<scalar_t, float, float4, float4>
+                <<<gridsize, blocksize, 0, stream>>>(
+                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+                  scales.data_ptr<float>(), hidden_size,blocksize);
+          } else if(hidden_size <= 8064 && (hidden_size & (n - 1)) == 0 && n == 8) {
+            int64_t gridsize = num_tokens;
+            int blocksize = 512;
+            vllm::dynamic_scaled_int8_quant_kernel_sm_opt<scalar_t, float, float4, float2>
+                <<<gridsize, blocksize, 0, stream>>>(
+                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+                  scales.data_ptr<float>(), hidden_size,blocksize);
+          } else if (hidden_size > 8064 && ((hidden_size & (n - 1)) == 0 && n == 8)) {
+            int blocksize = 1024;
+            vllm::dynamic_scaled_int8_quant_kernel_opt<scalar_t, float, float4, float2>
+              <<<grid, blocksize, 0, stream>>>(
+                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
+                  scales.data_ptr<float>(), hidden_size,blocksize);
+          } else {
+            vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>
               <<<grid, block, 0, stream>>>(
                   input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),
                   scales.data_ptr<float>(), hidden_size);
+          }
         } else {
           vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t>
               <<<grid, block, 0, stream>>>(
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
index 865fef5ae..2d2edb1da 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
@@ -1,14 +1,22 @@
 #include <stddef.h>
 #include <torch/all.h>
+#ifdef USE_MACA
+#include "mctlass/mctlass.h"
+#include "mctlass/epilogue/thread/scale_type.h"
+#else
 #include "cutlass/cutlass.h"
+#endif
 
 #include "scaled_mm_c2x.cuh"
 #include "scaled_mm_c2x_sm75_dispatch.cuh"
+
+#ifndef USE_MACA
 #include "scaled_mm_c2x_sm80_dispatch.cuh"
 #include "scaled_mm_c2x_sm89_fp8_dispatch.cuh"
 #include "scaled_mm_c2x_sm89_int8_dispatch.cuh"
 
 #include "cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp"
+#endif
 
 using namespace vllm;
 
@@ -22,6 +30,7 @@ template <template <typename, typename> typename Epilogue,
 void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
                                      torch::Tensor const& b,
                                      EpilogueArgs&&... epilogue_args) {
+#ifndef USE_MACA
   TORCH_CHECK(a.dtype() == torch::kInt8);
   TORCH_CHECK(b.dtype() == torch::kInt8);
 
@@ -33,6 +42,7 @@ void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,
     return cutlass_gemm_sm75_dispatch<int8_t, cutlass::half_t, Epilogue>(
         out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);
   }
+#endif // USE_MACA
 }
 
 void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
@@ -40,6 +50,7 @@ void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
                             torch::Tensor const& a_scales,
                             torch::Tensor const& b_scales,
                             std::optional<torch::Tensor> const& bias) {
+#ifndef USE_MACA
   TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
   TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
   if (bias) {
@@ -51,6 +62,195 @@ void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,
     return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogue>(
         out, a, b, a_scales, b_scales);
   }
+#else
+int32_t m = a.size(0);
+int32_t n = b.size(1);
+int32_t k = a.size(1);
+int32_t batch_count = 1;
+if (a.dim() == 3 && b.dim() == 3) {
+    // a.size = [batch_size, M, K], b.size = [batch_size, K, N]
+    m = a.size(1);
+    n = b.size(2);
+    k = a.size(2);
+    batch_count = a.size(0);
+}
+
+using ArchTag = mctlass::arch::Sm80;
+using ElementA = int8_t;
+using ElementB = int8_t;
+using ElementC = mctlass::half_t;
+using ElementCompute = float;
+using LayoutA = mctlass::layout::RowMajor;
+//using LayoutB = mctlass::layout::RowMajor;
+using LayoutB = mctlass::layout::ColumnMajor;
+using LayoutC = mctlass::layout::RowMajor;
+
+if (out.dtype() == torch::kBFloat16)
+{
+  auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
+  auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
+  auto c_ptr = static_cast<maca_bfloat16*>(out.data_ptr());
+  auto scale_a = a_scales.data_ptr<float>();
+  auto scale_b = b_scales.data_ptr<float>();
+  auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
+  if (bias) {
+    mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+    mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
+    using mctlassGemmScaleOp = mctlassGemmScale<
+      ElementA,
+      LayoutA,
+      ElementB,
+      LayoutB,
+      maca_bfloat16,
+      LayoutC,
+      ElementCompute,
+      ArchTag,
+      scale_type
+    >;
+    maca_bfloat16 *bias_t;
+    bias_t = static_cast<maca_bfloat16 *>(bias.value().data_ptr());
+    mctlassGemmScaleOp mctlass_op;
+    mctlass::gemm::GemmCoord problem_size(m, n, k);
+    typename mctlassGemmScaleOp::Arguments arguments{
+        mctlass::gemm::GemmUniversalMode::kGemm,
+        problem_size,
+        batch_count,
+        {scale_a, scale_b, bias_t},
+        a_ptr,
+        b_ptr,
+        c_ptr,
+        c_ptr,
+        m * k,
+        n * k,
+        m * n,
+        m * n,
+        k,
+        n,
+        n,
+        n
+    };
+    mctlass_op(arguments, NULL, stream);
+  }
+  else{
+    mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+    mctlass::epilogue::thread::ScaleType::ScaleAvBv;
+    using mctlassGemmScaleOp = mctlassGemmScale<
+      ElementA,
+      LayoutA,
+      ElementB,
+      LayoutB,
+      maca_bfloat16,
+      LayoutC,
+      ElementCompute,
+      ArchTag,
+      scale_type
+    >;
+    mctlassGemmScaleOp mctlass_op;
+    mctlass::gemm::GemmCoord problem_size(m, n, k);
+    typename mctlassGemmScaleOp::Arguments arguments{
+        mctlass::gemm::GemmUniversalMode::kGemm,
+        problem_size,
+        batch_count,
+        {scale_a, scale_b, nullptr},
+        a_ptr,
+        b_ptr,
+        c_ptr,
+        c_ptr,
+        m * k,
+        n * k,
+        m * n,
+        m * n,
+        k,
+        n,
+        n,
+        n
+    };
+    mctlass_op(arguments, NULL, stream);
+  }
+}
+else{
+  auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
+  auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
+  auto c_ptr = static_cast<ElementC*>(out.data_ptr());
+  auto scale_a = a_scales.data_ptr<float>();
+  auto scale_b = b_scales.data_ptr<float>();
+  auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
+  if (bias) {
+    mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+    mctlass::epilogue::thread::ScaleType::ScaleAvBvBias;
+    using mctlassGemmScaleOp = mctlassGemmScale<
+      ElementA,
+      LayoutA,
+      ElementB,
+      LayoutB,
+      ElementC,
+      LayoutC,
+      ElementCompute,
+      ArchTag,
+      scale_type
+    >;
+    ElementC *bias_t;
+    bias_t = static_cast<ElementC *>(bias.value().data_ptr());
+    mctlassGemmScaleOp mctlass_op;
+    mctlass::gemm::GemmCoord problem_size(m, n, k);
+    typename mctlassGemmScaleOp::Arguments arguments{
+        mctlass::gemm::GemmUniversalMode::kGemm,
+        problem_size,
+        batch_count,
+        {scale_a, scale_b, bias_t},
+        a_ptr,
+        b_ptr,
+        c_ptr,
+        c_ptr,
+        m * k,
+        n * k,
+        m * n,
+        m * n,
+        k,
+        n,
+        n,
+        n
+    };
+    mctlass_op(arguments, NULL, stream);
+  }
+  else{
+    mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+    mctlass::epilogue::thread::ScaleType::ScaleAvBv;
+    using mctlassGemmScaleOp = mctlassGemmScale<
+      ElementA,
+      LayoutA,
+      ElementB,
+      LayoutB,
+      ElementC,
+      LayoutC,
+      ElementCompute,
+      ArchTag,
+      scale_type
+    >;
+    mctlassGemmScaleOp mctlass_op;
+    mctlass::gemm::GemmCoord problem_size(m, n, k);
+    typename mctlassGemmScaleOp::Arguments arguments{
+        mctlass::gemm::GemmUniversalMode::kGemm,
+        problem_size,
+        batch_count,
+        {scale_a, scale_b, nullptr},
+        a_ptr,
+        b_ptr,
+        c_ptr,
+        c_ptr,
+        m * k,
+        n * k,
+        m * n,
+        m * n,
+        k,
+        n,
+        n,
+        n
+    };
+    mctlass_op(arguments, NULL, stream);
+  }
+}
+#endif // USE_MACA
 }
 
 void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
@@ -60,6 +260,7 @@ void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
                                 torch::Tensor const& azp_adj,
                                 std::optional<torch::Tensor> const& azp,
                                 std::optional<torch::Tensor> const& bias) {
+#ifndef USE_MACA
   TORCH_CHECK(a_scales.dtype() == torch::kFloat32);
   TORCH_CHECK(b_scales.dtype() == torch::kFloat32);
 
@@ -70,8 +271,178 @@ void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,
     return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBiasAzp>(
         out, a, b, a_scales, b_scales, azp_adj, bias);
   }
+#elif (MACA_VERSION_MAJOR * 100 + MACA_VERSION_MINOR) >= 231 // MACA version >= 2.31.0.x
+  int32_t m = a.size(0);
+  int32_t n = b.size(1);
+  int32_t k = a.size(1);
+  int32_t batchsize = 1;
+  if (a.dim() == 3 && b.dim() == 3) {
+    // a.size = [batch_size, M, K], b.size = [batch_size, K, N]
+    m = a.size(1);
+    n = b.size(2);
+    k = a.size(2);
+    batchsize = a.size(0);
+  }
+
+  using ArchTag = mctlass::arch::Sm80;
+  using ElementA = int8_t;
+  using ElementB = int8_t;
+
+  using ElementCompute = float;
+  using ElementAccumulator = int32_t;
+
+  using LayoutA = mctlass::layout::RowMajor;
+  using LayoutB = mctlass::layout::ColumnMajor;
+  using LayoutC = mctlass::layout::RowMajor;
+
+  auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
+
+  if (out.dtype() == torch::kBFloat16) {
+    using ElementC = maca_bfloat16;
+    using ElementOutput = ElementC;
+
+    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
+    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
+    auto c_ptr = static_cast<ElementC*>(out.data_ptr());
+    auto scale_a = a_scales.data_ptr<float>();
+    auto scale_b = b_scales.data_ptr<float>();
+
+    ElementAccumulator* azp_ptr = NULL;
+    auto azp_adj_ptr = azp_adj.data_ptr<ElementAccumulator>();
+    ElementOutput* bias_t = static_cast<ElementOutput*>(bias.value().data_ptr());
+
+    if (azp) {
+      azp_ptr = static_cast<ElementAccumulator*>(azp.value().data_ptr());
+      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+          mctlass::epilogue::thread::ScaleType::ScaleAvBvBiasAzpPerTorken;
+      using mctlassGemmScaleOp =
+          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
+                           LayoutC, ElementCompute, ArchTag, scale_type>;
+      mctlassGemmScaleOp mctlass_op;
+      mctlass::gemm::GemmCoord problem_size(m, n, k);
+      typename mctlassGemmScaleOp::Arguments arguments{
+          mctlass::gemm::GemmUniversalMode::kGemm,
+          problem_size,
+          batchsize,
+          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
+          a_ptr,
+          b_ptr,
+          c_ptr,
+          c_ptr,
+          m * k,
+          n * k,
+          m * n,
+          m * n,
+          k,
+          n,
+          n,
+          n
+      };
+      mctlass_op(arguments, NULL, stream);
+    } else {
+      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+          mctlass::epilogue::thread::ScaleType::ScaleAsBvBiasAzp;
+      using mctlassGemmScaleOp =
+          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
+                           LayoutC, ElementCompute, ArchTag, scale_type>;
+      mctlassGemmScaleOp mctlass_op;
+      mctlass::gemm::GemmCoord problem_size(m, n, k);
+      typename mctlassGemmScaleOp::Arguments arguments{
+          mctlass::gemm::GemmUniversalMode::kGemm,
+          problem_size,
+          batchsize,
+          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
+          a_ptr,
+          b_ptr,
+          c_ptr,
+          c_ptr,
+          m * k,
+          n * k,
+          m * n,
+          m * n,
+          k,
+          n,
+          n,
+          n
+      };
+      mctlass_op(arguments, NULL, stream);
+    }
+  } else {
+    using ElementC = mctlass::half_t;
+    using ElementOutput = ElementC;
+
+    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
+    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
+    auto c_ptr = static_cast<ElementC*>(out.data_ptr());
+    auto scale_a = a_scales.data_ptr<float>();
+    auto scale_b = b_scales.data_ptr<float>();
+
+    ElementAccumulator* azp_ptr = nullptr;
+    auto azp_adj_ptr = azp_adj.data_ptr<ElementAccumulator>();
+    ElementOutput* bias_t = static_cast<ElementOutput*>(bias.value().data_ptr());
+
+    if (azp) {
+      azp_ptr = static_cast<ElementAccumulator*>(azp.value().data_ptr());
+
+      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+          mctlass::epilogue::thread::ScaleType::ScaleAvBvBiasAzpPerTorken;
+      using mctlassGemmScaleOp =
+          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
+                           LayoutC, ElementCompute, ArchTag, scale_type>;
+      mctlassGemmScaleOp mctlass_op;
+      mctlass::gemm::GemmCoord problem_size(m, n, k);
+      typename mctlassGemmScaleOp::Arguments arguments{
+          mctlass::gemm::GemmUniversalMode::kGemm,
+          problem_size,
+          batchsize,
+          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
+          a_ptr,
+          b_ptr,
+          c_ptr,
+          c_ptr,
+          m * k,
+          n * k,
+          m * n,
+          m * n,
+          k,
+          n,
+          n,
+          n
+      };
+      mctlass_op(arguments, NULL, stream);
+    } else {
+      mctlass::epilogue::thread::ScaleType::ScaleBiasKind const scale_type =
+          mctlass::epilogue::thread::ScaleType::ScaleAsBvBiasAzp;
+      using mctlassGemmScaleOp =
+          mctlassGemmScale<ElementA, LayoutA, ElementB, LayoutB, ElementC,
+                           LayoutC, ElementCompute, ArchTag, scale_type>;
+      mctlassGemmScaleOp mctlass_op;
+      mctlass::gemm::GemmCoord problem_size(m, n, k);
+      typename mctlassGemmScaleOp::Arguments arguments{
+          mctlass::gemm::GemmUniversalMode::kGemm,
+          problem_size,
+          batchsize,
+          {scale_a, scale_b, bias_t, azp_adj_ptr, azp_ptr},
+          a_ptr,
+          b_ptr,
+          c_ptr,
+          c_ptr,
+          m * k,
+          n * k,
+          m * n,
+          m * n,
+          k,
+          n,
+          n,
+          n
+      };
+      mctlass_op(arguments, NULL, stream);
+    }
+  }
+#endif //USE_MACA
 }
 
+#ifndef USE_MACA
 template <template <typename, typename> typename Epilogue,
           typename... EpilogueArgs>
 void cutlass_scaled_mm_sm80_epilogue(torch::Tensor& out, torch::Tensor const& a,
@@ -197,3 +568,4 @@ void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,
         out, a, b, a_scales, b_scales, azp_adj, bias);
   }
 }
+#endif // USE_MACA
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
index ce7cf2f35..4a5a63cf6 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
@@ -6,6 +6,7 @@
 
 // clang-format will break include orders
 // clang-format off
+#ifndef USE_MACA
 #include "cute/tensor.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cutlass/numeric_types.h"
@@ -21,6 +22,15 @@
 #include "cutlass/epilogue/threadblock/fusion/visitors.hpp"
 #include "cutlass/gemm/kernel/default_gemm_universal_with_visitor.h"
 
+#else
+#include "mctlass/mctlass_ex.h"
+#include "mctlass/half.h"
+#include "mctlass/layout/matrix.h"
+#include "mctlass/epilogue/thread/scale_type.h"
+#include "mctlass/util/command_line.h"
+#endif // USE_MACA
+
+
 #include "core/math.hpp"
 #include "cutlass_extensions/common.hpp"
 // clang-format on
@@ -42,6 +52,7 @@ namespace vllm {
 // reduce the size of the compiled binary.
 // __CUDA_ARCH__ is not defined in host code, so this lets us smuggle the ifdef
 // into code that will be executed on the device where it is defined.
+#ifndef USE_MACA
 template <typename Kernel>
 struct enable_sm75_to_sm80 : Kernel {
   template <typename... Args>
@@ -71,6 +82,9 @@ struct enable_sm89_to_sm90 : Kernel {
 #endif
   }
 };
+#endif // USE_MACA
+
+#ifndef USE_MACA
 template <typename Arch, template <typename> typename ArchGuard,
           typename ElementAB_, typename ElementD_,
           template <typename, typename> typename Epilogue_, typename TileShape,
@@ -128,11 +142,13 @@ struct cutlass_2x_gemm {
 
   using Op = cutlass::gemm::device::GemmUniversalAdapter<KernelType>;
 };
+#endif // USE_MACA
 
 template <typename Gemm, typename... EpilogueArgs>
 inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
                                 torch::Tensor const& b,
                                 EpilogueArgs&&... epilogue_params) {
+#ifndef USE_MACA
   using ElementAB = typename Gemm::ElementAB;
   using ElementD = typename Gemm::ElementD;
 
@@ -193,6 +209,7 @@ inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,
   CUTLASS_CHECK(gemm_op.can_implement(args));
   cutlass::Status status = gemm_op(args, workspace.data_ptr(), stream);
   CUTLASS_CHECK(status);
+#endif // USE_MACA
 }
 
 template <typename Gemm, typename FallbackGemm, typename... EpilogueArgs>
@@ -200,6 +217,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
                                          torch::Tensor const& a,
                                          torch::Tensor const& b,
                                          EpilogueArgs&&... args) {
+#ifndef USE_MACA
   // In some cases, the GPU isn't able to accommodate the
   // shared memory requirements of the Gemm. In such cases, use
   // the FallbackGemm instead.
@@ -220,6 +238,7 @@ inline void fallback_cutlass_gemm_caller(torch::Tensor& out,
     return cutlass_gemm_caller<FallbackGemm>(
         out, a, b, std::forward<EpilogueArgs>(args)...);
   }
+#endif // USE_MACA
 }
 
 }  // namespace vllm
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
index a562fd896..c7dec5784 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
@@ -9,6 +9,7 @@
 
 namespace vllm {
 
+#ifndef USE_MACA
 template <typename InType, typename OutType,
           template <typename, typename> typename Epilogue>
 struct sm75_config_default {
@@ -66,6 +67,7 @@ struct sm75_config_M32 {
       cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,
                       Epilogue, TileShape, WarpShape, InstructionShape, 2>;
 };
+#endif // USE_MACA
 
 template <typename InType, typename OutType,
           template <typename, typename> typename Epilogue,
@@ -74,6 +76,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
                                        torch::Tensor const& a,
                                        torch::Tensor const& b,
                                        EpilogueArgs&&... args) {
+#ifndef USE_MACA
   static_assert(std::is_same<InType, int8_t>());
   TORCH_CHECK(a.dtype() == torch::kInt8);
   TORCH_CHECK(b.dtype() == torch::kInt8);
@@ -118,6 +121,7 @@ inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,
     return fallback_cutlass_gemm_caller<Cutlass2xGemmDefault, FallbackGemm>(
         out, a, b, std::forward<EpilogueArgs>(args)...);
   }
+#endif
 }
 
 }  // namespace vllm
diff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
index 54b63894e..1075e5dfb 100644
--- a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
@@ -133,6 +133,9 @@ void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
                        torch::Tensor const& b, torch::Tensor const& a_scales,
                        torch::Tensor const& b_scales,
                        std::optional<torch::Tensor> const& bias) {
+#ifdef USE_MACA
+  cutlass_scaled_mm_sm75(c, a, b, a_scales, b_scales, bias);
+#else
   // Checks for conformality
   TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);
   TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&
@@ -193,6 +196,7 @@ void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
       "No compiled cutlass_scaled_mm for a compute capability less than "
       "CUDA device capability: ",
       version_num);
+#endif // USE_MACA
 }
 
 void cutlass_moe_mm(
@@ -274,6 +278,29 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
 
   at::cuda::OptionalCUDAGuard const device_guard(device_of(a));
 
+#ifdef USE_MACA
+
+  if (!bias) {
+    // mctlass not support None bias
+
+    int32_t n = b.size(1);
+    int32_t batchsize = 1;
+    if (a.dim() == 3 && b.dim() == 3) {
+      // a.size = [batch_size, M, K], b.size = [batch_size, K, N]
+      n = b.size(2);
+      batchsize = a.size(0);
+    }
+    auto options = torch::TensorOptions()
+                     .dtype(c.dtype())
+                     .device(a.device());
+    torch::Tensor zero_bias = torch::zeros({batchsize,  n}, options);
+    cutlass_scaled_mm_azp_sm75(c, a, b, a_scales, b_scales, azp_adj, azp, zero_bias);
+  } else {
+    cutlass_scaled_mm_azp_sm75(c, a, b, a_scales, b_scales, azp_adj, azp, bias);
+  }
+
+#else
+
   int32_t version_num = get_sm_version_num();
 
 #if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90
@@ -307,4 +334,6 @@ void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,
       "No compiled cutlass_scaled_mm_azp for a compute capability less than "
       "CUDA device capability: ",
       version_num);
+
+#endif // USE_MACA
 }
diff --git a/csrc/quantization/fp8/nvidia/quant_utils.cuh b/csrc/quantization/fp8/nvidia/quant_utils.cuh
index f8cd1dcba..ec16b4fda 100644
--- a/csrc/quantization/fp8/nvidia/quant_utils.cuh
+++ b/csrc/quantization/fp8/nvidia/quant_utils.cuh
@@ -568,6 +568,44 @@ __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
       }                                                                        \
     }
 
+  #define DISPATCH_BY_KV_CACHE_V2_DTYPE(SRC_DTYPE, KV_DTYPE, COUNT_INIT_ONCE, FN)                  \
+    if (KV_DTYPE == "auto") {                                                  \
+      if (SRC_DTYPE == at::ScalarType::Float) {                                \
+        FN(float, float, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);                     \
+      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \
+        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);               \
+      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \
+        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto, COUNT_INIT_ONCE);     \
+      } else {                                                                 \
+        TORCH_CHECK(false, "Unsupported input type of kv cache: ", SRC_DTYPE); \
+      }                                                                        \
+    } else {                                                                   \
+      if (KV_DTYPE == "fp8" || KV_DTYPE == "fp8_e4m3") {                       \
+        if (SRC_DTYPE == at::ScalarType::Float) {                              \
+          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);              \
+        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \
+          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);           \
+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
+          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3, COUNT_INIT_ONCE);      \
+        } else {                                                               \
+          TORCH_CHECK(false,                                                   \
+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
+        }                                                                      \
+      } else if (KV_DTYPE == "fp8_e5m2") {                                     \
+        if (SRC_DTYPE == at::ScalarType::Float) {                              \
+          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);              \
+        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \
+          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);           \
+        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \
+          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2, COUNT_INIT_ONCE);      \
+        } else {                                                               \
+          TORCH_CHECK(false,                                                   \
+                      "Unsupported input type of kv cache: ", SRC_DTYPE);      \
+        }                                                                      \
+      } else {                                                                 \
+        TORCH_CHECK(false, "Unsupported data type of kv cache: ", KV_DTYPE);   \
+      }                                                                        \
+    }
 }  // namespace fp8
 #endif  // not USE_ROCM
 }  // namespace vllm
diff --git a/csrc/quantization/fused_kernels/quant_conversions.cuh b/csrc/quantization/fused_kernels/quant_conversions.cuh
index 7c10aaa81..1b6c519e5 100644
--- a/csrc/quantization/fused_kernels/quant_conversions.cuh
+++ b/csrc/quantization/fused_kernels/quant_conversions.cuh
@@ -26,7 +26,12 @@ static __device__ __forceinline__ int8_t float_to_int8_rn(float const x) {
 #else
   // CUDA path
   uint32_t dst;
+#ifndef USE_MACA
   asm volatile("cvt.rni.sat.s8.f32 %0, %1;" : "=r"(dst) : "f"(x));
+#else
+  dst = (int32_t)(x > 0? x + 0.5: x - 0.5);
+  dst = (char)(dst > 127 ? 127 : (dst < -128 ? -128 : dst));
+#endif // USE_MACA
   return reinterpret_cast<const int8_t&>(dst);
 #endif
 }
diff --git a/csrc/quantization/gguf/ggml-common.h b/csrc/quantization/gguf/ggml-common.h
index 6bef5db3c..8dbab74a3 100644
--- a/csrc/quantization/gguf/ggml-common.h
+++ b/csrc/quantization/gguf/ggml-common.h
@@ -1147,4 +1147,12 @@ static __device__ __forceinline__ uint32_t __vsub4(const uint32_t a, const uint3
            (static_cast<uint8_t>(((a & 0x0000ff00) >>  8) - ((b & 0x0000ff00) >>  8)) <<  8) +
            (static_cast<uint8_t>(((a & 0x000000ff) >>  0) - ((b & 0x000000ff) >>  0)) <<  0);
 }
+#elif defined(USE_MACA)
+typedef int8_t int8x4_t __attribute__((ext_vector_type(4)));
+static __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {
+    const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);
+    const int8x4_t vb = reinterpret_cast<const int8x4_t&>(b);
+    c += va[0] * vb[0] + va[1] * vb[1] + va[2] * vb[2] + va[3] * vb[3];
+    return c;
+}
 #endif // defined(USE_ROCM)
diff --git a/csrc/quantization/gptq/Hgemm_common.cuh b/csrc/quantization/gptq/Hgemm_common.cuh
new file mode 100644
index 000000000..22723dd75
--- /dev/null
+++ b/csrc/quantization/gptq/Hgemm_common.cuh
@@ -0,0 +1,92 @@
+#pragma once
+#include "maca_fp16.h"
+
+using b32VecType = uint32_t;
+using b64VecType = __NATIVE_VECTOR__(2, uint32_t);
+using b128VecType = __NATIVE_VECTOR__(4, uint32_t);
+using b128VecType_i = __NATIVE_VECTOR__(4, int32_t);
+using Float4VecType = __NATIVE_VECTOR__(4, float);
+typedef enum
+{
+    OP_N = 0,
+    OP_T = 1
+} Operation_t;
+
+#define cast_half(ptr) reinterpret_cast<__half *>(ptr)
+#define cast_b16(ptr) reinterpret_cast<uint16_t *>(ptr)
+#define cast_b32(ptr) reinterpret_cast<uint32_t *>(ptr)
+#define cast_b64(ptr) reinterpret_cast<b64VecType *>(ptr)
+#define cast_u64(ptr) reinterpret_cast<uint64_t *>(ptr)
+#define cast_b128(ptr) reinterpret_cast<b128VecType *>(ptr)
+#define cast_b128_i(ptr) reinterpret_cast<b128VecType_i *>(ptr)
+
+#define ldg_b32_reg_noasync(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b32_predicator(cast_b32(base), 0, ret0_en, true, false, false, pred, 1, \
+                                           MACA_ICMP_EQ)[0];
+#define ldg_b64_reg_noasync(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b64_predicator(cast_b64(base), 0, ret0_en, true, false, false, pred, 1, \
+                                           MACA_ICMP_EQ);
+#define ldg_b128_reg_noasync(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b128_predicator(cast_b128(base), 0, ret0_en, true, false, false, pred, 1, \
+                                           MACA_ICMP_EQ);
+
+
+#define ldg_b32_reg_async(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b32_predicator(cast_b32(base), 0, ret0_en, true, false, true, pred, 1, \
+                                           MACA_ICMP_EQ)[0];
+#define ldg_b64_reg_async(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b64_predicator(cast_b64(base), 0, ret0_en, true, false, true, pred, 1, \
+                                           MACA_ICMP_EQ);
+#define ldg_b128_reg_async(dst, base, pred, ret0_en)                                                 \
+    dst = __builtin_mxc_ldg_b128_predicator(cast_b128(base), 0, ret0_en, true, false, true, pred, 1, \
+                                            MACA_ICMP_EQ);
+#define ldg_b64_v4h_reg_async(dst, base, pred, ret0_en)                                                \
+    dst = __builtin_mxc_ldg_b64_predicator(cast_b64(base), 0, ret0_en, true, false, true, pred, 1, \
+                                           MACA_ICMP_EQ);
+
+#define ldg_b32_bsm_noasync(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b32_bsm_predicator(cast_b32(saddr), cast_b32(base), 0, ret0_en, true, false, false, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b64_bsm_noasync(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b64_bsm_predicator(cast_b64(saddr), cast_b64(base), 0, ret0_en, true, false, false, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b128_bsm_noasync(saddr, base, pred, ret0_en)                                                \
+    __builtin_mxc_ldg_b128_bsm_predicator(cast_b128(saddr), cast_b128(base), 0, ret0_en, true, false, \
+                                          false, pred, 1, MACA_ICMP_EQ);
+
+
+#define ldg_b32_bsm_async(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b32_bsm_predicator(cast_b32(saddr), cast_b32(base), 0, ret0_en, true, false, true, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b64_bsm_async(saddr, base, pred, ret0_en)                                                    \
+    __builtin_mxc_ldg_b64_bsm_predicator(cast_b64(saddr), cast_b64(base), 0, ret0_en, true, false, true, \
+                                         pred, 1, MACA_ICMP_EQ);
+#define ldg_b128_bsm_async(saddr, base, pred, ret0_en)                                                \
+    __builtin_mxc_ldg_b128_bsm_predicator(cast_b128(saddr), cast_b128(base), 0, ret0_en, true, false, \
+                                          true, pred, 1, MACA_ICMP_EQ);
+
+#define stg_b32_async(data, base, pred)                                                   \
+    __builtin_mxc_stg_b32_predicator(cast_b32(base), 0, data, true, false, true, pred, 1, \
+                                     MACA_ICMP_EQ);
+#define stg_b64_async(data, base, pred)                                                   \
+    __builtin_mxc_stg_b64_predicator(cast_b64(base), 0, data, true, false, true, pred, 1, \
+                                     MACA_ICMP_EQ);
+#define stg_b128_async(data, base, pred)                                                    \
+    __builtin_mxc_stg_b128_predicator(cast_b128(base), 0, data, true, false, true, pred, 1, \
+                                      MACA_ICMP_EQ);
+
+#define perm_b32(dst, reg1, reg2, selector) dst = __builtin_mxc_byte_perm(reg1, reg2, selector)
+#define mma_16x16x16f16(a_reg, b_reg, c_reg) \
+    c_reg = __builtin_mxc_mma_16x16x16f16(a_reg, b_reg, c_reg)
+
+#define mma_16x16x16bf16(a_reg, b_reg, c_reg) \
+    c_reg = __builtin_mxc_mma_16x16x16bf16(a_reg, b_reg, c_reg)
+
+#define FENCE__ asm volatile(";")
+#define arrive_gvmcnt(num) __builtin_mxc_arrive(64 + num)
+#define arrive_bsmcnt(num) __builtin_mxc_arrive(4096 + 128 * num)
+#define arrive_gvm_bsmcnt(gvm, bsm) __builtin_mxc_arrive(4096 | (128 * bsm) | 64 | gvm)
+#define barrier __builtin_mxc_barrier_inst
+#define barrier_all __builtin_mxc_barrier_ex(0)
+#define barrier_bsm __builtin_mxc_barrier_ex(1)
+#define barrier_inst __builtin_mxc_barrier_ex(2)
diff --git a/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
new file mode 100644
index 000000000..5a97af2ad
--- /dev/null
+++ b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
@@ -0,0 +1,426 @@
+#pragma once
+#include "Hgemm_common.cuh"
+#include "gptq.cuh"
+#include "maca_fp16.h"
+
+template <Operation_t TransA,
+          Operation_t TransB,
+          bool IsBetaZero,
+          int tileM,
+          int tileN,
+          int tileK,
+          bool doShuffle = true,
+          bool splitk = false,
+          bool Swap = false>
+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_gptq_4bit(int m,
+                                                                            int n,
+                                                                            int k,
+                                                                            const scalar_type alpha,    // alpha is 1.0f for gptq
+                                                                            const scalar_type beta,     // beta is 0.0f for gptq
+                                                                            const quant_packed_type *dA_input,
+                                                                            int lda,
+                                                                            const input_type *dB_input,
+                                                                            int ldb,
+                                                                            output_type *dC_input,
+                                                                            output_type *dC_output,
+                                                                            int ldc,
+                                                                            quant_packed_type *d_zeros,
+                                                                            input_type *d_scales,
+                                                                            int splitk_iters = 1,
+                                                                            acc_type * d_acc_tmp=nullptr)
+{
+    __shared__ uint8_t smem_base[0x8000];
+    int bidx = Swap ? blockIdx.y : blockIdx.x;
+    int bidy = Swap ? blockIdx.x : blockIdx.y;
+    int bidz = blockIdx.z;
+    int tid = threadIdx.x;
+
+    // this kernel only support NN trans mode
+    uint64_t arowstride = 1;
+    uint64_t acolstride = lda;
+    uint64_t browstride = 1;
+    uint64_t bcolstride = ldb;
+
+    const int malign = 8;
+    const int nalign = 1;
+    const int kalign = 8;
+    int align_m = (m + malign - 1) / malign * malign;
+    int align_n = (n + nalign - 1) / nalign * nalign;
+    int align_k = (k + kalign - 1) / kalign * kalign;
+
+    int k_num = (align_k + splitk_iters -1) / splitk_iters;
+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
+    int k_begin = bidz * k_num;
+    int k_end = (bidz + 1) * k_num;
+    k_end = k_end > align_k ? align_k : k_end;
+    k_num = k_end - k_begin;
+    int ktail = k_num % tileK;
+    int kloop = k_begin;
+
+    // if k_begin > align_k(we force k_num to be aligned to 8, so it is possible), return immediately
+    if (k_begin >= align_k) 
+    {
+        return;
+    }
+
+    int slot = tid / 64;
+    int lane = tid & 63;
+    int m64d16 = lane / 16;
+    int m64m16 = lane % 16;
+    output_type *c_base_i = dC_input;
+    output_type *c_base_o = dC_output;
+
+
+    quant_packed_type rga[8], rgzeros[1];
+    b128VecType rgb[2];
+    input_type rgscales[2];
+
+    b64VecType rgA[16], rgB[4], rgCi[4];
+    Float4VecType rgC[16];
+    uint32_t rgZeros[1];
+    b64VecType rgScales[2];
+
+    quant_packed_type *dA[2];
+    input_type *dB[2];
+
+    // ldg A/B head
+    bool predm[2], predn[2];
+    int rowA = m64m16 * 4;
+    int colA = (m64d16 * 8 + slot * 32) / PACK_RATIO_4BITS;
+    int current_m = bidx * tileM + rowA;
+    predm[0] = current_m < align_m;
+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+            (uint64_t)(colA + k_begin / PACK_RATIO_4BITS) * (uint64_t)(acolstride);
+    current_m += 64;
+    predm[1] = current_m < align_m;
+    // dA[1] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+    //         (uint64_t)(colA + k_begin / PACK_RATIO_4BITS) * (uint64_t)(acolstride);
+    dA[1] = dA[0] + 64 * (uint64_t)(arowstride);
+
+    int colB = m64d16 + slot * 4;
+    int rowB = m64m16 * 8;
+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
+    int current_n = bidy * tileN + colB;
+    predn[0] = current_n < align_n;
+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    current_n += 16;
+    predn[1] = current_n < align_n;
+    // dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+    //         (uint64_t)(current_n) * (uint64_t)bcolstride;
+    dB[1] = dB[0] + 16 * (uint64_t)bcolstride;
+
+    // ldgB to BSM need swizzle
+    input_type *b_sts = (input_type *)smem_base;
+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
+
+    input_type *b_lds = (input_type *)smem_base;
+    int colB_lds = m64m16;
+    int rowB_lds = m64d16 * 8 + slot * 32;
+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
+
+    // ldg zeros and scales
+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO_4BITS) * (kloop / tileK) + bidx * (tileM / PACK_RATIO_4BITS) + tid;
+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_4BITS * sizeof(quant_packed_type)) + tid * 2;
+    uint16_t *lds_zeros_offset = (uint16_t *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_4BITS * sizeof(quant_packed_type)) + m64m16 * 4;
+
+    // clear C registers
+    rgC[0] = 0;
+    rgC[1] = 0;
+    rgC[2] = 0;
+    rgC[3] = 0;
+    rgC[4] = 0;
+    rgC[5] = 0;
+    rgC[6] = 0;
+    rgC[7] = 0;
+    rgC[8] = 0;
+    rgC[9] = 0;
+    rgC[10] = 0;
+    rgC[11] = 0;
+    rgC[12] = 0;
+    rgC[13] = 0;
+    rgC[14] = 0;
+    rgC[15] = 0;
+
+    int aincr = tileK * acolstride / PACK_RATIO_4BITS;
+    int bincr = tileK * browstride;
+
+    if (k_num > ktail)
+    {
+        LDG_B;      // 2
+        LDG_ZEROS_4BITS;  // 3
+        LDG_SCALES_4BITS; // 4
+        LDG_A1_4BITS;     // 5
+        LDG_A2_4BITS;     // 6
+
+        dA[0] += aincr;
+        dA[1] += aincr;
+        dB[0] += bincr;
+        dB[1] += bincr;
+
+        // lds_A, need swizzle
+        arrive_gvmcnt(2);
+        barrier();
+
+        LDS_B;
+        LDS_ZEROS_4BITS;
+        LDS_SCALES_4BITS;
+        arrive_bsmcnt(0);
+        barrier();
+        ldg_zeros_offset += lda / PACK_RATIO_4BITS;
+        ldg_scales_offset += lda;
+
+// KLOOP
+#pragma unroll 1
+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
+        {
+            // load next B
+            LDG_B;      // 4
+            LDG_ZEROS_4BITS;  // 5
+            LDG_SCALES_4BITS; // 6
+
+            arrive_gvmcnt(5);
+            PERM_A1_4BITS;
+            LDG_A1_4BITS;
+            MMA1;
+
+            arrive_gvmcnt(5);
+            PERM_A2_4BITS;
+            LDG_A2_4BITS; // 6
+            MMA2;
+
+            // sts && lds
+            arrive_gvmcnt(2);
+            barrier();
+            LDS_B;
+            LDS_ZEROS_4BITS;
+            LDS_SCALES_4BITS;
+            arrive_bsmcnt(0);
+            barrier();
+
+            dA[0] += aincr;
+            dA[1] += aincr;
+            dB[0] += bincr;
+            dB[1] += bincr;
+            ldg_zeros_offset += lda / PACK_RATIO_4BITS;
+            ldg_scales_offset += lda;
+        }
+
+        arrive_gvmcnt(1);
+        PERM_A1_4BITS;
+        MMA1;
+
+        arrive_gvmcnt(0);
+        PERM_A2_4BITS;
+        MMA2;
+    }
+
+    // final tail kloop
+    if (ktail > 0)
+    {
+        LDG_B_HEAD;
+        LDG_ZEROS_4BITS;
+        LDG_SCALES_4BITS;
+        LDG_A1_HEAD_4BITS;
+        LDG_A2_HEAD_4BITS;
+        arrive_gvmcnt(2);
+        barrier();
+        LDS_B;
+        LDS_ZEROS_4BITS;
+        LDS_SCALES_4BITS;
+        arrive_bsmcnt(0);
+        barrier();
+
+        arrive_gvmcnt(1);
+        PERM_A1_4BITS;
+        MMA1;
+        arrive_gvmcnt(0);
+        PERM_A2_4BITS;
+        MMA2;
+    }
+
+    // store C registers into BSM & do reduction
+    int colC = m64m16 + slot * 16;
+    int rowC = m64d16 * 16;
+    scalar_type *c_sts[8];
+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 68, (colC % 4 * 4)), colC, 128);
+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 72, (colC % 4 * 4)), colC, 128);
+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 76, (colC % 4 * 4)), colC, 128);
+
+    colC = m64d16 + slot * 4;
+    rowC = m64m16 * 4;
+    scalar_type *c_lds[2];
+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+
+    PERM_C(rgC);
+    STS_C(0);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(0);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    PERM_C((rgC + 8));
+    STS_C(1);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(1);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    // read C_input if beta!=0
+    colC = m64d16 + slot * 4 + bidy * tileN;
+    rowC = m64m16 * 4 + bidx * tileM;
+    int colC1 = colC + 16;
+    int rowC1 = rowC + 64;
+
+
+    if constexpr (!IsBetaZero && !splitk)
+    {
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[0],
+                              c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC < align_m && colC1 < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC1 < align_n, true);
+        arrive_gvmcnt(0);
+    }
+    
+    if constexpr (IsBetaZero && !splitk)
+    {
+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
+    }
+
+    if constexpr (!splitk)
+    {
+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
+        output_type result[4];
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[0][0]);
+            result[1] = static_cast<output_type>(rgC[0][1]);
+            result[2] = static_cast<output_type>(rgC[0][2]);
+            result[3] = static_cast<output_type>(rgC[0][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[1][0]);
+            result[1] = static_cast<output_type>(rgC[1][1]);
+            result[2] = static_cast<output_type>(rgC[1][2]);
+            result[3] = static_cast<output_type>(rgC[1][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[2][0]);
+            result[1] = static_cast<output_type>(rgC[2][1]);
+            result[2] = static_cast<output_type>(rgC[2][2]);
+            result[3] = static_cast<output_type>(rgC[2][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[3][0]);
+            result[1] = static_cast<output_type>(rgC[3][1]);
+            result[2] = static_cast<output_type>(rgC[3][2]);
+            result[3] = static_cast<output_type>(rgC[3][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+    }
+    else
+    {
+        // for acc_type is float
+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[0][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[0][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[0][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[0][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[1][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[1][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[1][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[1][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[2][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[2][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[2][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[2][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[3][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[3][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[3][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[3][3]));
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
new file mode 100644
index 000000000..2cd807f57
--- /dev/null
+++ b/csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
@@ -0,0 +1,440 @@
+#pragma once
+#include "Hgemm_common.cuh"
+#include "gptq.cuh"
+#include "maca_fp16.h"
+
+template <Operation_t TransA,
+          Operation_t TransB,
+          bool IsBetaZero,
+          int tileM,
+          int tileN,
+          int tileK,
+          bool doShuffle = true,
+          bool splitk = false,
+          bool Swap = false>
+__global__ __launch_bounds__(256) void Hgemm_nn_128x32x128_8m1n8k_gptq_8bit(int m,
+                                                                            int n,
+                                                                            int k,
+                                                                            const scalar_type alpha,    // alpha is 1.0f for gptq
+                                                                            const scalar_type beta,     // beta is 0.0f for gptq
+                                                                            const quant_packed_type *dA_input,
+                                                                            int lda,
+                                                                            const input_type *dB_input,
+                                                                            int ldb,
+                                                                            output_type *dC_input,
+                                                                            output_type *dC_output,
+                                                                            int ldc,
+                                                                            quant_packed_type *d_zeros,
+                                                                            input_type *d_scales,
+                                                                            int splitk_iters = 1,
+                                                                            acc_type * d_acc_tmp=nullptr,
+                                                                            half* dequanted = nullptr)
+{
+    __shared__ uint8_t smem_base[0x8000];
+    int bidx = Swap ? blockIdx.y : blockIdx.x;
+    int bidy = Swap ? blockIdx.x : blockIdx.y;
+    int bidz = blockIdx.z;
+    int tid = threadIdx.x;
+
+    // this kernel only support NN trans mode
+    uint64_t arowstride = 1;
+    uint64_t acolstride = lda;
+    uint64_t browstride = 1;
+    uint64_t bcolstride = ldb;
+
+    const int malign = 8;
+    const int nalign = 1;
+    const int kalign = 8;
+    int align_m = (m + malign - 1) / malign * malign;
+    int align_n = (n + nalign - 1) / nalign * nalign;
+    int align_k = (k + kalign - 1) / kalign * kalign;
+
+    int k_num = (align_k + splitk_iters -1) / splitk_iters;
+    k_num = (k_num + tileK - 1) / tileK * tileK; // k_num should be aligned to tileK
+    int k_begin = bidz * k_num;
+    int k_end = (bidz + 1) * k_num;
+    k_end = k_end > align_k ? align_k : k_end;
+    k_num = k_end - k_begin;
+    int ktail = k_num % tileK;
+    int kloop = k_begin;
+
+    // if k_begin > align_k(we force k_num to be aligned to 8, so it is possible), return immediately
+    if (k_begin >= align_k) 
+    {
+        return;
+    }
+
+    int slot = tid / 64;
+    int lane = tid & 63;
+    int m64d16 = lane / 16;
+    int m64m16 = lane % 16;
+    output_type *c_base_i = dC_input;
+    output_type *c_base_o = dC_output;
+
+
+    quant_packed_type rga[16], rgzeros[2];
+    b128VecType rgb[2];
+    input_type rgscales[2];
+
+    b64VecType rgA[16], rgB[4], rgCi[4];
+    Float4VecType rgC[16];
+    uint32_t rgZeros[2];
+    b64VecType rgScales[2];
+
+    quant_packed_type *dA[4];
+    input_type *dB[2];
+
+    // ldg A/B head
+    bool predm[2], predn[2];
+    int rowA = m64m16 * 4;
+    int colA = (m64d16 * 8 + slot * 32) / PACK_RATIO_8BITS;
+    int current_m = bidx * tileM + rowA;
+    predm[0] = current_m < align_m;
+    dA[0] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+            (uint64_t)(colA + k_begin / PACK_RATIO_8BITS) * (uint64_t)(acolstride);
+    dA[2] = dA[0] + align_m;
+    current_m += 64;
+    predm[1] = current_m < align_m;
+    // dA[1] = (quant_packed_type *)dA_input + (uint64_t)(current_m) * (uint64_t)(arowstride) +
+    //         (uint64_t)(colA + k_begin / PACK_RATIO_8BITS) * (uint64_t)(acolstride);
+    dA[1] = dA[0] + 64 * (uint64_t)(arowstride);
+    dA[3] = dA[1] + align_m;
+
+    int colB = m64d16 + slot * 4;
+    int rowB = m64m16 * 8;
+    int rowB_swizzle = SWIZZLE_INDEX(rowB, (colB % 4 * 8));
+    int current_n = bidy * tileN + colB;
+    predn[0] = current_n < align_n;
+    dB[0] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+            (uint64_t)(current_n) * (uint64_t)bcolstride;
+
+    current_n += 16;
+    predn[1] = current_n < align_n;
+    // dB[1] = (input_type *)dB_input + (uint64_t)(rowB_swizzle + k_begin) * (uint64_t)(browstride) +
+    //         (uint64_t)(current_n) * (uint64_t)bcolstride;
+    dB[1] = dB[0] + 16 * (uint64_t)bcolstride;
+
+    // ldgB to BSM need swizzle
+    input_type *b_sts = (input_type *)smem_base;
+    b_sts += ONE_DIM_INDEX(rowB, colB, 128);
+
+    input_type *b_lds = (input_type *)smem_base;
+    int colB_lds = m64m16;
+    int rowB_lds = m64d16 * 8 + slot * 32;
+    b_lds += ONE_DIM_INDEX(SWIZZLE_INDEX(rowB_lds, (colB_lds % 4 * 8)), colB_lds, 128);
+
+    // ldg zeros and scales
+    quant_packed_type *ldg_zeros_offset = d_zeros + (lda / PACK_RATIO_8BITS) * (kloop / tileK) + bidx * (tileM / PACK_RATIO_8BITS) + tid;
+    input_type *ldg_scales_offset = d_scales + lda * (kloop / tileK) + bidx * tileM + tid * 2;
+    quant_packed_type *sts_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + tid;
+    input_type *sts_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_8BITS * sizeof(quant_packed_type)) + tid * 2;
+    quant_packed_type *lds_zeros_offset = (quant_packed_type *)(smem_base + tileK * tileN * sizeof(input_type)) + m64m16;
+    input_type *lds_scales_offset = (input_type *)(smem_base + tileK * tileN * sizeof(input_type) + tileM / PACK_RATIO_8BITS * sizeof(quant_packed_type)) + m64m16 * 4;
+
+    // clear C registers
+    rgC[0] = 0;
+    rgC[1] = 0;
+    rgC[2] = 0;
+    rgC[3] = 0;
+    rgC[4] = 0;
+    rgC[5] = 0;
+    rgC[6] = 0;
+    rgC[7] = 0;
+    rgC[8] = 0;
+    rgC[9] = 0;
+    rgC[10] = 0;
+    rgC[11] = 0;
+    rgC[12] = 0;
+    rgC[13] = 0;
+    rgC[14] = 0;
+    rgC[15] = 0;
+
+    int aincr = tileK * acolstride / PACK_RATIO_8BITS;
+    int bincr = tileK * browstride;
+
+    if (k_num > ktail)
+    {
+        LDG_B;      // 2
+        LDG_ZEROS_8BITS;  // 3
+        LDG_SCALES_8BITS; // 4
+        LDG_A1_8BITS;     // 5
+        LDG_A3_8BITS;     // 6
+        LDG_A2_8BITS;     // 7
+        LDG_A4_8BITS;     // 8
+
+        dA[0] += aincr;
+        dA[1] += aincr;
+        dA[2] += aincr;
+        dA[3] += aincr;
+        dB[0] += bincr;
+        dB[1] += bincr;
+
+        // lds_A, need swizzle
+        arrive_gvmcnt(4);
+        barrier();
+
+        LDS_B;
+        LDS_ZEROS_8BITS;
+        LDS_SCALES_8BITS;
+        arrive_bsmcnt(0);
+        barrier();
+        ldg_zeros_offset += lda / PACK_RATIO_8BITS;
+        ldg_scales_offset += lda;
+        constexpr bool loading_filter = false;//blockIdx.x == 0 && threadIdx.x == 0;
+
+// KLOOP
+#pragma unroll 1
+        for (kloop = tileK; kloop + tileK <= k_num; kloop += tileK)
+        {
+            // load next B
+            LDG_B;      // 6
+            LDG_ZEROS_8BITS;  // 7
+            LDG_SCALES_8BITS; // 8
+
+            arrive_gvmcnt(6);
+            PERM_A1A3_8BITS;
+            LDG_A1_8BITS;
+            LDG_A3_8BITS;
+            MMA1;
+
+            arrive_gvmcnt(6);
+            PERM_A2A4_8BITS;
+            LDG_A2_8BITS; // 6
+            LDG_A4_8BITS;
+            MMA2;
+
+            // sts && lds
+            arrive_gvmcnt(4);
+            barrier();
+            LDS_B;
+            LDS_ZEROS_8BITS;
+            LDS_SCALES_8BITS;
+            arrive_bsmcnt(0);
+            barrier();
+
+            dA[0] += aincr;
+            dA[1] += aincr;
+            dA[2] += aincr;
+            dA[3] += aincr;
+            dB[0] += bincr;
+            dB[1] += bincr;
+            ldg_zeros_offset += lda / PACK_RATIO_8BITS;
+            ldg_scales_offset += lda;
+        }
+
+        arrive_gvmcnt(2);
+        PERM_A1A3_8BITS;
+        MMA1;
+
+        arrive_gvmcnt(0);
+        PERM_A2A4_8BITS;
+        MMA2;
+    }
+
+    // final tail kloop
+    if (ktail > 0)
+    {
+        LDG_B_HEAD;
+        LDG_ZEROS_8BITS;
+        LDG_SCALES_8BITS;
+        LDG_A1_HEAD_8BITS;
+        LDG_A3_HEAD_8BITS;
+        LDG_A2_HEAD_8BITS;
+        LDG_A4_HEAD_8BITS;
+        arrive_gvmcnt(4);
+        barrier();
+        LDS_B;
+        LDS_ZEROS_8BITS;
+        LDS_SCALES_8BITS;
+        arrive_bsmcnt(0);
+        barrier();
+
+        arrive_gvmcnt(2);
+        PERM_A1A3_8BITS;
+        MMA1;
+        arrive_gvmcnt(0);
+        PERM_A2A4_8BITS;
+        MMA2;
+    }
+
+    // store C registers into BSM & do reduction
+    int colC = m64m16 + slot * 16;
+    int rowC = m64d16 * 16;
+    scalar_type *c_sts[8];
+    c_sts[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_sts[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 4, (colC % 4 * 4)), colC, 128);
+    c_sts[2] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 8, (colC % 4 * 4)), colC, 128);
+    c_sts[3] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 12, (colC % 4 * 4)), colC, 128);
+    c_sts[4] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+    c_sts[5] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 68, (colC % 4 * 4)), colC, 128);
+    c_sts[6] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 72, (colC % 4 * 4)), colC, 128);
+    c_sts[7] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 76, (colC % 4 * 4)), colC, 128);
+
+    colC = m64d16 + slot * 4;
+    rowC = m64m16 * 4;
+    scalar_type *c_lds[2];
+    c_lds[0] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC, (colC % 4 * 4)), colC, 128);
+    c_lds[1] = (scalar_type *)smem_base + ONE_DIM_INDEX(SWIZZLE_INDEX(rowC + 64, (colC % 4 * 4)), colC, 128);
+
+    PERM_C(rgC);
+    STS_C(0);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(0);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    PERM_C((rgC + 8));
+    STS_C(1);
+    arrive_bsmcnt(0);
+    barrier();
+    REDUCE_C(1);
+
+    arrive_bsmcnt(0);
+    barrier();
+
+    // read C_input if beta!=0
+    colC = m64d16 + slot * 4 + bidy * tileN;
+    rowC = m64m16 * 4 + bidx * tileM;
+    int colC1 = colC + 16;
+    int rowC1 = rowC + 64;
+
+
+    if constexpr (!IsBetaZero && !splitk)
+    {
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[0],
+                              c_base_i + (uint64_t)rowC + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[1], c_base_i + rowC1 + (uint64_t)(colC) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[2], c_base_i + rowC + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC < align_m && colC1 < align_n, true);
+        ldg_b64_v4h_reg_async(cast_b64(rgCi)[3], c_base_i + rowC1 + (uint64_t)(colC1) * (uint64_t)(ldc),
+                              rowC1 < align_m && colC1 < align_n, true);
+        arrive_gvmcnt(0);
+    }
+    
+    if constexpr (IsBetaZero && !splitk)
+    {
+        for (int i = 0; i < 4; ++i) rgCi[i] = static_cast<b64VecType>(0);
+    }
+
+    if constexpr (!splitk)
+    {
+        output_type *ptrCi = reinterpret_cast<output_type *>(rgCi);
+        b64VecType *ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc));
+        output_type result[4];
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[0][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[0][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[0][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[0][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[0][0]);
+            result[1] = static_cast<output_type>(rgC[0][1]);
+            result[2] = static_cast<output_type>(rgC[0][2]);
+            result[3] = static_cast<output_type>(rgC[0][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[1][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[1][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[1][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[1][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[1][0]);
+            result[1] = static_cast<output_type>(rgC[1][1]);
+            result[2] = static_cast<output_type>(rgC[1][2]);
+            result[3] = static_cast<output_type>(rgC[1][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[2][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[2][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[2][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[2][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[2][0]);
+            result[1] = static_cast<output_type>(rgC[2][1]);
+            result[2] = static_cast<output_type>(rgC[2][2]);
+            result[3] = static_cast<output_type>(rgC[2][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+        ptrO = reinterpret_cast<b64VecType *>(c_base_o + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc));
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // result[0] = static_cast<output_type>(alpha * rgC[3][0] + beta * static_cast<scalar_type>(ptrCi[0]));
+            // result[1] = static_cast<output_type>(alpha * rgC[3][1] + beta * static_cast<scalar_type>(ptrCi[1]));
+            // result[2] = static_cast<output_type>(alpha * rgC[3][2] + beta * static_cast<scalar_type>(ptrCi[2]));
+            // result[3] = static_cast<output_type>(alpha * rgC[3][3] + beta * static_cast<scalar_type>(ptrCi[3]));
+            result[0] = static_cast<output_type>(rgC[3][0]);
+            result[1] = static_cast<output_type>(rgC[3][1]);
+            result[2] = static_cast<output_type>(rgC[3][2]);
+            result[3] = static_cast<output_type>(rgC[3][3]);
+            ptrO[0] = cast_b64(result)[0];
+        }
+    }
+    else
+    {
+        // for acc_type is float
+        acc_type *ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[0][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[0][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[0][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[0][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[0][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[1][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[1][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[1][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[1][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[1][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[2][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[2][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[2][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[2][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[2][3]));
+        }
+        ptrAcc = d_acc_tmp + (uint64_t)(rowC1) + (uint64_t)(colC1) * (uint64_t)(ldc);
+        if (rowC1 < align_m && colC1 < align_n)
+        {
+            // for gptq, alpha is 1.0f, beta is 0.0f
+            // atomicAdd(&ptrAcc[0], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][0]));
+            // atomicAdd(&ptrAcc[1], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][1]));
+            // atomicAdd(&ptrAcc[2], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][2]));
+            // atomicAdd(&ptrAcc[3], static_cast<acc_type>(alpha) * static_cast<acc_type>(rgC[3][3]));
+            atomicAdd(&ptrAcc[0], static_cast<acc_type>(rgC[3][0]));
+            atomicAdd(&ptrAcc[1], static_cast<acc_type>(rgC[3][1]));
+            atomicAdd(&ptrAcc[2], static_cast<acc_type>(rgC[3][2]));
+            atomicAdd(&ptrAcc[3], static_cast<acc_type>(rgC[3][3]));
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/dequant.cuh b/csrc/quantization/gptq/dequant.cuh
new file mode 100644
index 000000000..b192a7804
--- /dev/null
+++ b/csrc/quantization/gptq/dequant.cuh
@@ -0,0 +1,11 @@
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#define BFE(source, bit_start, num_bits) __builtin_mxc_ubfe(source, bit_start, num_bits);
+
+template <typename outputT, typename inputT, int qbits>
+__device__ __forceinline__ outputT extract(inputT *input, size_t loc)
+{
+    constexpr int widthPerElem = sizeof(inputT) * 8 / qbits;
+    return (outputT)BFE(input[loc / widthPerElem], loc % widthPerElem, qbits);
+}
\ No newline at end of file
diff --git a/csrc/quantization/gptq/gptq.cuh b/csrc/quantization/gptq/gptq.cuh
new file mode 100644
index 000000000..c91bf4168
--- /dev/null
+++ b/csrc/quantization/gptq/gptq.cuh
@@ -0,0 +1,365 @@
+#pragma once
+
+#include "Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+
+#define Q4BITS 4
+#define PACK_RATIO_4BITS (32 / Q4BITS)
+
+#define Q8BITS 8
+#define PACK_RATIO_8BITS (32 / Q8BITS)
+
+#define input_type __half
+#define output_type __half
+#define scalar_type float
+//#define acc_type float
+#define acc_type __half
+
+#define SEL0 0x01000504
+#define SEL1 0x03020706
+
+#define SWIZZLE_INDEX(index, phase) (index ^ phase)
+#define ONE_DIM_INDEX(x, y, lda) (y * lda + x)
+#define CLAMP(value, bound, align) (value < bound ? value : bound - align)
+
+#define LDG_A1                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[0], dA[0], predm[0], true); \
+    }
+
+#define LDG_A2                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[1], dA[1], predm[1], true); \
+    }
+
+#define LDG_A3                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[2], dA[2], predm[0], true); \
+    }
+
+#define LDG_A4                                                        \
+    {                                                                 \
+        ldg_b128_reg_async(cast_b128(rga)[3], dA[3], predm[1], true); \
+    }
+
+#define LDG_A1_4BITS LDG_A1
+#define LDG_A2_4BITS LDG_A2
+#define LDG_A1_8BITS LDG_A1
+#define LDG_A2_8BITS LDG_A2
+#define LDG_A3_8BITS LDG_A3
+#define LDG_A4_8BITS LDG_A4
+
+#define LDG_A1_HEAD_4BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_4BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[0], dA[0], predm[0] && predk, true); \
+    }
+
+#define LDG_A2_HEAD_4BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_4BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[1], dA[1], predm[1] && predk, true); \
+    }
+
+#define LDG_A1_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[0], dA[0], predm[0] && predk, true); \
+    }
+
+#define LDG_A2_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[1], dA[1], predm[1] && predk, true); \
+    }
+
+#define LDG_A3_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[2], dA[2], predm[0] && predk, true); \
+    }
+
+#define LDG_A4_HEAD_8BITS                                                      \
+    {                                                                          \
+        bool predk = colA < ktail / PACK_RATIO_8BITS;                          \
+        ldg_b128_reg_async(cast_b128(rga)[3], dA[3], predm[1] && predk, true); \
+    }
+
+#define LDG_ZEROS_4BITS                                                                                                                        \
+    {                                                                                                                                          \
+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO_4BITS && (bidx * tileM + tid * PACK_RATIO_4BITS < align_m), false); \
+    }
+
+#define LDG_ZEROS_8BITS                                                                                                                        \
+    {                                                                                                                                          \
+        ldg_b32_bsm_async(sts_zeros_offset, ldg_zeros_offset, tid < tileM / PACK_RATIO_8BITS && (bidx * tileM + tid * PACK_RATIO_8BITS < align_m), false); \
+    }
+
+#define LDG_SCALES                                                                                                      \
+    {                                                                                                                   \
+        ldg_b32_bsm_async(sts_scales_offset, ldg_scales_offset, tid < 64 && (bidx * tileM + tid * 2 < align_m), false); \
+    }
+
+#define LDG_SCALES_4BITS LDG_SCALES
+#define LDG_SCALES_8BITS LDG_SCALES
+
+#define LDS_ZEROS_4BITS                              \
+    {                                                \
+        cast_b16(rgZeros)[0] = lds_zeros_offset[0];  \
+        cast_b16(rgZeros)[1] = lds_zeros_offset[16]; \
+    }
+
+#define LDS_ZEROS_8BITS                              \
+    {                                                \
+        cast_b32(rgZeros)[0] = lds_zeros_offset[0];  \
+        cast_b32(rgZeros)[1] = lds_zeros_offset[16]; \
+    }
+
+#define LDS_SCALES                                               \
+    {                                                            \
+        cast_b64(rgScales)[0] = cast_b64(lds_scales_offset)[0];  \
+        cast_b64(rgScales)[1] = cast_b64(lds_scales_offset)[16]; \
+    }
+
+#define LDS_SCALES_4BITS LDS_SCALES
+#define LDS_SCALES_8BITS LDS_SCALES
+
+#define PERM_ELEM_4BITS(index)                                                                                                          \
+    {                                                                                                                                   \
+        __half_raw elem;                                                                                                                \
+        if constexpr (index & 0x1)                                                                                                      \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] >> 16;                                                                          \
+        }                                                                                                                               \
+        else                                                                                                                            \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] & 0xffff;                                                                       \
+        }                                                                                                                               \
+        __half scale = __half(elem);                                                                                                    \
+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[0], Q4BITS * index, Q4BITS) + 1;                                                     \
+        if constexpr (doShuffle)                                                                                                        \
+        {                                                                                                                               \
+            cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 0, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 4, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 1, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 5, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 2, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 6, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 3, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 7, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+        }                                                                                                                               \
+        else                                                                                                                            \
+        {                                                                                                                               \
+            cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 0, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 1, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 2, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 3, Q4BITS) -       \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 4, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 5, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 6, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+            cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q4BITS * 7, Q4BITS) -      \
+                                                                            zero) ,                                                     \
+                                            scale);                                                                                      \
+        }                                                                                                                               \
+    }
+
+#define PERM_A1_4BITS       \
+    {                       \
+        PERM_ELEM_4BITS(0)  \
+        PERM_ELEM_4BITS(1)  \
+        PERM_ELEM_4BITS(2)  \
+        PERM_ELEM_4BITS(3)  \
+    }
+
+#define PERM_A2_4BITS       \
+    {                       \
+        PERM_ELEM_4BITS(4)  \
+        PERM_ELEM_4BITS(5)  \
+        PERM_ELEM_4BITS(6)  \
+        PERM_ELEM_4BITS(7)  \
+    }
+
+#define PERM_ELEM_8BITS(index)                                                                                                          \
+    {                                                                                                                                   \
+        __half_raw elem;                                                                                                                \
+        if constexpr (index & 0x1)                                                                                                      \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] >> 16;                                                                          \
+        }                                                                                                                               \
+        else                                                                                                                            \
+        {                                                                                                                               \
+            elem.x = rgScales[index / 4][index % 4 / 2] & 0xffff;                                                                       \
+        }                                                                                                                               \
+        __half scale = __half(elem);                                                                                                    \
+        uint32_t zero = __builtin_mxc_ubfe(rgZeros[index/4], Q8BITS * index, Q8BITS) + 1;                                               \
+                                                                                                                                        \
+        cast_half(rgA)[index * 4 + 0] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 0, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 1] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 1, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 2] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 2, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 3] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index], Q8BITS * 3, Q8BITS) -           \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 32] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 0, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 33] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 1, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 34] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 2, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+        cast_half(rgA)[index * 4 + 35] = __hmul((__half)__builtin_mxc_i16_to_f16(__builtin_mxc_ubfe(rga[index+8], Q8BITS * 3, Q8BITS) -        \
+                                                                        zero) ,                                                         \
+                                        scale);                                                                                          \
+    }
+
+#define PERM_A1A3_8BITS     \
+    {                       \
+        PERM_ELEM_8BITS(0)  \
+        PERM_ELEM_8BITS(1)  \
+        PERM_ELEM_8BITS(2)  \
+        PERM_ELEM_8BITS(3)  \
+    }
+
+#define PERM_A2A4_8BITS     \
+    {                       \
+        PERM_ELEM_8BITS(4)  \
+        PERM_ELEM_8BITS(5)  \
+        PERM_ELEM_8BITS(6)  \
+        PERM_ELEM_8BITS(7)  \
+    }
+
+#define MMA_ELEM(index_m)                                        \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 0, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(0, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 0, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 0, 8)]);          \
+    mma_16x16x16f16(cast_b64(rgA)[ONE_DIM_INDEX(index_m, 1, 8)], \
+                    cast_b64(rgB)[ONE_DIM_INDEX(1, 1, 2)],       \
+                    rgC[ONE_DIM_INDEX(index_m, 1, 8)]);
+
+#define MMA1    \
+    MMA_ELEM(0) \
+    MMA_ELEM(1) \
+    MMA_ELEM(2) \
+    MMA_ELEM(3)
+
+#define MMA2    \
+    MMA_ELEM(4) \
+    MMA_ELEM(5) \
+    MMA_ELEM(6) \
+    MMA_ELEM(7)
+
+#define LDG_B                                                        \
+    {                                                                \
+        ldg_b128_bsm_async(b_sts, dB[0], predn[0], true);            \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], predn[1], true); \
+    }
+
+#define LDG_B_HEAD                                                            \
+    {                                                                         \
+        bool predk = rowB_swizzle < ktail;                                    \
+        ldg_b128_bsm_async(b_sts, dB[0], predn[0] && predk, true);            \
+        ldg_b128_bsm_async(b_sts + 128 * 16, dB[1], predn[1] && predk, true); \
+    }
+
+#define LDS_B                                          \
+    {                                                  \
+        cast_b128(rgB)[0] = cast_b128(b_lds)[0];       \
+        cast_b128(rgB)[1] = cast_b128(b_lds)[16 * 16]; \
+    }
+
+#define PERM_C(C2perm)                                       \
+    {                                                        \
+        Float4VecType C_tmp[8];                              \
+        float *ptri = (float *)C2perm;                       \
+        float *ptro = (float *)C_tmp;                        \
+        for (int j = 0; j < 4; ++j)                          \
+        {                                                    \
+            for (int i = 0; i < 4; ++i)                      \
+            {                                                \
+                ptro[j * 4 + i] = ptri[j + i * 4];           \
+                ptro[j * 4 + i + 16] = ptri[j + i * 4 + 16]; \
+            }                                                \
+        }                                                    \
+        for (int i = 0; i < 8; ++i)                          \
+        {                                                    \
+            C2perm[i] = C_tmp[i];                            \
+        }                                                    \
+    }
+
+#define STS_C(phase)                                            \
+    {                                                           \
+        cast_b128(c_sts[0])[0] = cast_b128(rgC)[0 + phase * 8]; \
+        cast_b128(c_sts[1])[0] = cast_b128(rgC)[1 + phase * 8]; \
+        cast_b128(c_sts[2])[0] = cast_b128(rgC)[2 + phase * 8]; \
+        cast_b128(c_sts[3])[0] = cast_b128(rgC)[3 + phase * 8]; \
+        cast_b128(c_sts[4])[0] = cast_b128(rgC)[4 + phase * 8]; \
+        cast_b128(c_sts[5])[0] = cast_b128(rgC)[5 + phase * 8]; \
+        cast_b128(c_sts[6])[0] = cast_b128(rgC)[6 + phase * 8]; \
+        cast_b128(c_sts[7])[0] = cast_b128(rgC)[7 + phase * 8]; \
+    }
+
+#define REDUCE_C(phase)                                                   \
+    {                                                                     \
+        cast_b128(rgC)[0 + phase * 8] = cast_b128(c_lds[0])[0];           \
+        cast_b128(rgC)[1 + phase * 8] = cast_b128(c_lds[1])[0];           \
+        cast_b128(rgC)[2 + phase * 8] = cast_b128(c_lds[0])[1 * 16 * 32]; \
+        cast_b128(rgC)[3 + phase * 8] = cast_b128(c_lds[1])[1 * 16 * 32]; \
+        cast_b128(rgC)[4 + phase * 8] = cast_b128(c_lds[0])[2 * 16 * 32]; \
+        cast_b128(rgC)[5 + phase * 8] = cast_b128(c_lds[1])[2 * 16 * 32]; \
+        cast_b128(rgC)[6 + phase * 8] = cast_b128(c_lds[0])[3 * 16 * 32]; \
+        cast_b128(rgC)[7 + phase * 8] = cast_b128(c_lds[1])[3 * 16 * 32]; \
+        float *reduc_c = reinterpret_cast<float *>(rgC);                  \
+        for (int loop = 0; loop < 8; ++loop)                              \
+        {                                                                 \
+            float acc = 0;                                                \
+            acc += reduc_c[loop + phase * 32 + 0 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 1 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 2 * 8];                    \
+            acc += reduc_c[loop + phase * 32 + 3 * 8];                    \
+            reduc_c[loop + phase * 8] = acc;                              \
+        }                                                                 \
+    }
diff --git a/csrc/quantization/gptq/hgemm_gptq.h b/csrc/quantization/gptq/hgemm_gptq.h
new file mode 100644
index 000000000..a591ad372
--- /dev/null
+++ b/csrc/quantization/gptq/hgemm_gptq.h
@@ -0,0 +1,2033 @@
+// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+/*
+hgemm gptq 4bits
+
+hgemm inputs:
+half/tf16 a[m*k];
+uint32_t b[k*n/pack_ratio]; //pack_ratio = sizeof(uint32_t) / 4;
+uint32_t zp[k*n] with kU4 or zp[0] with kU4B8;
+half scales[m*k/quant_group];
+
+MMA accepts a m16xk16 b k16xn16
+A tile should be m16n64k128 or m32n64k128
+When m>=32, we should load a when compute, implement this stragedge later?
+
+We will make a block size 256, which means 4 waves, we need all these 4 waves execute on one PEU
+So the maximum shared memory used by a block use not larger than 16K, and MTRegisters should be
+less than 128.
+
+A tile will be divied into serveral sub_tiles, a minimum sub_tile is m16n16k32, so we can have
+m_blocks*k_blocks when loading a and k_blocks*n_blocks when loading b.
+
+We will have n*k/TILE_N/TILE_K tiles in b, and these TILES are diveded into ITERS where ITERS*PEUS>=TILES
+for a proper shape.
+Of course some small shapes will nerver have chance to use all the PEUS
+
+Parallism is not suitable for C500 as parallism will dequant b more than once, that is not acceptable
+*/
+#include <iostream>
+#include <algorithm>
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include <maca_bfloat16.h>
+
+#include "Hgemm_common.cuh"
+#include "scalar_type.hpp"
+
+using cudaStream_t = mcStream_t;
+
+#define WAVES_PER_BLOCK (THREADS/WAVE)
+#define TILE_M (BLOCKS_M*SLICE_M)
+#define TILE_N (BLOCKS_N*SLICE_N)
+#define TILE_K (BLOCKS_K*SLICE_K)
+#define N_ITERS (TILE_N / (WAVES_PER_BLOCK*SLOT))
+#define LOADING_A_LOOP SLICE_K * TILE_M / (sizeof(PackType) / sizeof(scalar_t)) / THREADS
+#define AS_PTR_B128(x) ((PackTypeInt4*)x)
+#define AS_PTR_B64(x) ((PackTypeInt2*)x)
+#define AS_PTR_B32(x) ((float*)x)
+#define AS_PTR_B16(x) ((half*)x)
+#define AS_PTR_B8(x) ((uint8_t*)x)
+
+#define BF16_HIGH_PRECISION
+
+#define div_ceil(x, y) (x + y - 1) / (y)
+
+//Although quant_group can be any positive value, but it is not a good idea
+//to set quant_group to values that cannot fit the SLICE_K as we will get a
+//very low performance, and we are not ready to support these values
+//Here we annouce that we support quant_group = 32, 64, 128, but actually
+//quant_group = 2^n where n >= 5 is also supported, for very large k.
+static int get_power2(uint32_t v) {
+    uint32_t power = 0;
+    uint32_t mask = 0x00000001;
+    while (power < 32) {
+        if ((v & mask) > 0) break;
+        power++;
+        mask <<= 1;
+    }
+    if ((1 << power) != v) return -1;
+    return static_cast<int>(power);
+}
+
+
+
+namespace hgemm_marlin_gptq {
+
+constexpr static int clean_kernel_thread_num = 512;
+constexpr static int clean_kernel_pack_num = 4;
+constexpr static int reduce_kernel_thread_num = 512;
+constexpr static int reduce_kernel_pack_num = 4;
+
+//#define DEBUG
+using PackTypeInt4 = b128VecType;
+using PackTypeInt2 = b64VecType;
+using PackType = uint32_t;
+
+template<class scalar_t>
+__device__ __forceinline__ void mma_16x16x16(PackTypeInt2& a, PackTypeInt2& b, PackTypeInt4& c) {
+}
+
+template<>
+__device__ __forceinline__ void mma_16x16x16<half>(PackTypeInt2& a, PackTypeInt2& b, PackTypeInt4& c) {
+    mma_16x16x16f16(a, b, c);
+}
+
+template<>
+__device__ __forceinline__ void mma_16x16x16<__maca_bfloat16>(PackTypeInt2& a, PackTypeInt2& b, PackTypeInt4& c) {
+    mma_16x16x16bf16(a, b, c);
+}
+
+#if 0
+#ifdef BF16_HIGH_PRECISION
+__global__ void vectorized_elementwise_fp32tobf16(float* input, __maca_bfloat16* output, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        // printf("tid = %d, input = %f, output = %f\n", tid, input[tid], (float)(__maca_bfloat16)input[tid]);
+        *(__maca_bfloat16*)(output+tid) = (__maca_bfloat16)input[tid];
+    }
+}
+#else
+__global__ void vectorized_elementwise_fp16tobf16(__maca_bfloat16* input, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        input[tid] = (float)(*(half*)(input+tid));
+    }
+}
+#endif
+#endif
+
+template<
+    const int THREADS, // number of threads in a threadblock
+    const int PACK_NUM
+    >
+__global__ void clean_zero(float* data, size_t num_elem) {
+    int bidx = blockIdx.x;
+    int tidx = threadIdx.x;
+    int idx = (bidx * THREADS + tidx) * PACK_NUM;
+    float zeros[4] = {0.0, 0.0, 0.0, 0.0};
+    if (idx < num_elem) {
+        *((b128VecType*)(&data[idx])) = *((b128VecType*)zeros);
+    }
+}
+
+template<
+    const int THREADS, // number of threads in a threadblock
+    const int PACK_NUM,
+    const bool USE_C = false
+    >
+__global__ void all_reduce(float* in_data, maca_bfloat16* out_data, size_t num_elem) {
+    int bidx = blockIdx.x;
+    int tidx = threadIdx.x;
+    int idx = (bidx * THREADS + tidx) * PACK_NUM;
+
+    if constexpr(PACK_NUM == 4) {
+        if constexpr(USE_C == true) {
+            float temp_in_fp32[PACK_NUM];
+            float temp_out_fp32[PACK_NUM];
+            maca_bfloat16 temp_out_bf16[PACK_NUM];
+
+            bool pred = idx < num_elem;
+            ldg_b64_reg_noasync(*((b64VecType*)temp_out_bf16), ((b64VecType*)(out_data + idx)), pred, true);
+            ldg_b128_reg_noasync(*((b128VecType*)temp_in_fp32), ((b128VecType*)(in_data + idx)), pred, true);
+
+            #pragma unroll
+            for(int i = 0; i < PACK_NUM; i++) {
+                temp_out_fp32[i] = __bfloat162float(temp_out_bf16[i]);
+                temp_out_fp32[i] += temp_in_fp32[i];
+                temp_out_bf16[i] = __float2bfloat16(temp_out_fp32[i]);
+            }
+
+            if (pred) {
+                *((b64VecType*)(out_data + idx)) = *((b64VecType*)temp_out_bf16);
+            }
+        } else {
+            float temp_in_fp32[PACK_NUM];
+            maca_bfloat16 temp_out_bf16[PACK_NUM];
+
+            bool pred = idx < num_elem;
+            ldg_b128_reg_noasync(*((b128VecType*)temp_in_fp32), ((b128VecType*)(in_data + idx)), pred, true);
+
+            #pragma unroll
+            for(int i = 0; i < PACK_NUM; i++) {
+                temp_out_bf16[i] = __float2bfloat16(temp_in_fp32[i]);
+            }
+
+            if (pred) {
+                *((b64VecType*)(out_data + idx)) = *((b64VecType*)temp_out_bf16);
+            }
+        }
+    }
+}
+
+typedef __NATIVE_VECTOR__(2, float) v2f;
+using PackTypeFloat2 = v2f;
+constexpr static int Q4BITS = 4;
+constexpr static int Q8BITS = 8;
+constexpr static int PACK_RATIO_4BITS = sizeof(PackType) * 8 / Q4BITS;
+constexpr static int PACK_RATIO_8BITS = sizeof(PackType) * 8 / Q8BITS;
+constexpr static int SLICE_M = 16;
+constexpr static int SLICE_N = 16;
+constexpr static int SLICE_K = 32;
+constexpr static int PAD_SLICE_K = 40;
+constexpr static int SLOT    = 16;
+constexpr static int WAVE    = 64;
+constexpr static int WAVE_SLOTS = 4;
+constexpr static int PEUS = 13*8*4; //For C500, There are 8 DPC and each DPC have 13 APs, each AP have 4 PEUs
+constexpr static int MAX_BLOCKS_M = 4;
+constexpr static uint32_t seil = 0x03020706u;
+
+__device__ __forceinline__ void f32x2_cvt_bf16x2(uint32_t& dst, float src[2]) {
+    uint32_t tmp[2];
+    tmp[0] = __builtin_mxc_ubfe(*(reinterpret_cast<uint32_t*>(src)), 16, 1);
+    tmp[0] = tmp[0] + *reinterpret_cast<uint32_t*>(src);
+    tmp[0] = (uint32_t)0x7fff + tmp[0];
+    tmp[1] = __builtin_mxc_ubfe(*(reinterpret_cast<uint32_t*>(src + 1)), 16, 1);
+    tmp[1] = tmp[1] + *(reinterpret_cast<uint32_t*>(src + 1));
+    tmp[1] = (uint32_t)0x7fff + tmp[1];
+    dst = __builtin_mxc_byte_perm(tmp[0], tmp[1], seil);
+}
+
+
+// #define CVT_B0TOF32(q, out) asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(out):"r"(q));
+// #define CVT_B1TOF32(q, out) asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(out):"r"(q));
+// #define CVT_B2TOF32(q, out) asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(out):"r"(q));
+// #define CVT_B3TOF32(q, out) asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(out):"r"(q));
+
+#define CVT_B0TOF32(q, out) out = __builtin_mxc_b0_cast_to_f32(q);
+#define CVT_B1TOF32(q, out) out = __builtin_mxc_b1_cast_to_f32(q);
+#define CVT_B2TOF32(q, out) out = __builtin_mxc_b2_cast_to_f32(q);
+#define CVT_B3TOF32(q, out) out = __builtin_mxc_b3_cast_to_f32(q);
+
+//FIXME: We'd rather a quant group will not divided into serveral blocks
+template<int BLOCKS_M, int BLOCKS_N, int BLOCKS_K>
+struct TileManager {
+    int tile_start_row;
+    int tile_start_col;
+    int tiles_k;
+    int my_iters = 0;
+    bool global_pred = true;
+
+    __device__ __forceinline__ void init(int m, int n, int k, int bidx, int iters) {
+        //Calculate tile start row and cols so we can calculate the offset address of a b and c
+        int tile_idx = iters * bidx;
+        int tiles_n = div_ceil(n, TILE_N);
+        tiles_k = div_ceil(k, TILE_K);
+        //if (tile_idx >= tiles_n*tiles_k) return false;
+        global_pred = tile_idx < tiles_n * tiles_k;
+        int tile_col = tile_idx / tiles_k;
+        int tile_row = tile_idx - tile_col * tiles_k;
+        tile_start_col = tile_col;
+        tile_start_row = tile_row;
+        my_iters = tile_idx + iters >= tiles_n*tiles_k ? tiles_n*tiles_k - tile_idx : iters;
+        my_iters = global_pred ? my_iters : 0;
+    }
+
+    __device__ __forceinline__ void next_tile() {
+        tile_start_col = tile_start_row + 1 == tiles_k ? tile_start_col + 1 : tile_start_col;
+        tile_start_row = tile_start_row + 1 == tiles_k ? 0 : tile_start_row + 1;
+        --my_iters;
+        global_pred = my_iters > 0;
+    }
+
+    __device__ __host__ __forceinline__ bool need_save_data() {
+        if (global_pred && my_iters == 1) return true;
+        if (global_pred && tile_start_row + 1 == tiles_k) return true;
+        return false;
+    }
+
+    //support for preloading next tile in current tile calculation
+    //The point is when all quanted values are dequanted and all a are stored to bsm already
+    //Then the registers for scales, zeros, and a_caches are free to use, bsm for scales are already
+    //free to use.
+    //The main problem we will face is that no more registers can be used to cache tile information
+    //when we want to preload next tile data
+    bool flag_save_data = false;
+    int tile_start_col_cache; //Kept for saving result n calculation
+
+    __device__ __forceinline__ void next_tile_pre() {
+        flag_save_data = need_save_data();
+        tile_start_col_cache = tile_start_col;
+        next_tile();
+    }
+
+    __device__ __forceinline__ bool need_save_data_pre() {
+        return flag_save_data;
+    }
+};
+
+struct ThreadView {
+    int tid;
+    int wave_idx;
+    int wave_tid;
+    int slot_idx;
+    int slot_tid;
+
+    __device__ __forceinline__ void init() {
+        tid = threadIdx.x;
+        wave_idx = tid / WAVE;
+        wave_tid = tid % WAVE;
+        slot_idx = wave_tid / SLOT;
+        slot_tid = wave_tid % SLOT;
+    }
+};
+
+/*
+Currently, we develop a version that TILE size is m64n128k64
+So every TILE we load 64x64x2=8192bytes A, 128x64/8x4=4096bytes B, 128x4=512 scales, 128x4=512 zeros if needed
+
+If more batches is needed, I think it is better to run dequant again. We do not count on situation that m>128 now.
+
+Memory View:
+There are 1/2/4 waves according to BLOCKS_N, wave_count = BLOCKS_N, mostly, BLOCKS_N=4, and THREADS=256
+Each wave will process a m16n16k32 sub tile with two mma instructions, we should re-order data in bsm so
+we can load data from shared memory only with tid or wave_tid, where we can reduce the instructions of 
+calculate address offsets of each data:
+
+BSM A order(assume that BLOCKS_M=4):
+|---------------------------------8x4x16 half value ------------------------------------------------------------------------|
+|-use by wave  slotidx 0 slottid 0------|-use by wave  slotidx 0 slottid 1------|...|-use by wave  slotidx 0 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k00~07|m16k00~07|m32k00~07|m48k00~07|m01k00~07|m17k00~07|m33k00~07|m49k00~07|...|m15k00~07|m31k00~07|m47k00~07|m63k00~07|
+
+|-use by wave  slotidx 1 slottid 0------|-use by wave  slotidx 1 slottid 1------|...|-use by wave  slotidx 1 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k08~15|m16k08~15|m32k08~15|m48k08~15|m01k08~15|m17k08~15|m33k08~15|m49k08~15|...|m15k08~15|m31k08~15|m47k08~15|m63k08~15|
+
+...
+
+|-use by wave  slotidx 3 slottid 0------|-use by wave  slotidx 3 slottid 1------|...|-use by wave  slotidx 3 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k24~31|m16k24~31|m32k24~31|m48k24~31|m01k24~31|m17k24~31|m33k24~31|m49k24~31|...|m15k24~31|m31k24~31|m47k24~31|m63k24~31|
+
+---- next k ----
+|---------------------------------8x4x16 half value ------------------------------------------------------------------------|
+|-use by wave  slotidx 0 slottid 0------|-use by wave  slotidx 0 slottid 1------|...|-use by wave  slotidx 0 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k32~39|m16k32~39|m32k32~39|m48k32~39|m01k32~39|m17k32~39|m33k32~39|m49k32~39|...|m15k32~39|m31k32~39|m47k32~39|m63k32~39|
+
+...
+
+|---------------------------------8x4x16 half value ------------------------------------------------------------------------|
+|-use by wave  slotidx 3 slottid 0------|-use by wave  slotidx 3 slottid 1------|...|-use by wave  slotidx 3 slottid 15-----|
+|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|-8 half--|...|-8 half--|-8 half--|-8 half--|-8 half--|
+|m00k56~63|m16k56~63|m32k56~63|m48k56~63|m01k56~63|m17k56~63|m33k56~63|m49k56~63|...|m15k56~63|m31k56~63|m47k56~63|m63k56~63|
+
+When loading a from bsm, each thread only read 4x8 half value with offset wave_tid:
+b128 out[4];
+b128 *bsm_a_ptr;
+out[0] = bsm_a_ptr[wave_tid*BLOCKS_M];
+out[1] = bsm_a_ptr[wave_tid*BLOCKS_M+1];
+out[2] = bsm_a_ptr[wave_tid*BLOCKS_M+2];
+out[3] = bsm_a_ptr[wave_tid*BLOCKS_M+3];
+
+
+BSM B order: here k means the quanted k, which means each k represents 8 dequanted k data
+//Loop = BLOCKS_N*SLICE_N/16/WAVES
+Loop 0:
+|---------16 uint32_t loaded by wave0-|
+k00n00 k00n01 k00n02 k00n03 ... k00n15
+k01n00 k01n01 k01n02 k01n03 ... k01n15
+k02n00 k02n01 k02n02 k02n03 ... k02n15
+k03n00 k03n01 k03n02 k03n03 ... k03n15
+|---------16 uint32_t loaded by wave1-|
+k00n16 k00n17 k00n18 k00n19 ... k00n31
+...
+k03n16 k03n17 k03n18 k03n19 ... k03n31
+|---------16 uint32_t loaded by wave2-|
+k00n32 k00n33 k00n34 k00n35 ... k00n47
+...
+k03n32 k03n33 k03n34 k03n35 ... k03n47
+|---------16 uint32_t loaded by wave3-|
+k00n48 k00n49 k00n50 k00n51 ... k00n63
+...
+k03n48 k03n49 k03n50 k03n51 ... k03n63
+*/
+
+template<class scalar_t>
+__device__ __forceinline__ void awq_dequant_4bits(const uint32_t& p, scalar_t (&out)[8], scalar_t (&scale)[8], const uint32_t& scale_zero) {
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        v2f v2z,v2scale, v2neg;
+        v2neg[0] = -1.0f; v2neg[1] = -1.0f;
+        v2z[0] = 0.0f; v2z[1] = 0.0f;
+        v2f v2zero;
+        float tmp[2];
+        int p0 = p & 0x0f0f0f0f;
+        int z0 = scale_zero & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, tmp[0]);
+        CVT_B2TOF32(p0, tmp[1]);
+        CVT_B0TOF32(z0, v2zero[0]);
+        CVT_B2TOF32(z0, v2zero[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
+        v2scale[0] = (float)scale[0]; v2scale[1] = (float)scale[1];
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
+        f32x2_cvt_bf16x2(*((uint32_t*)out), tmp);
+
+        CVT_B1TOF32(p0, tmp[0]);
+        CVT_B3TOF32(p0, tmp[1]);
+        CVT_B1TOF32(z0, v2zero[0]);
+        CVT_B3TOF32(z0, v2zero[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
+        v2scale[0] = (float)scale[4]; v2scale[1] = (float)scale[5];
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 4)), tmp);
+
+        p0 = (p >> 4) & 0x0f0f0f0f;
+        z0 = (scale_zero >> 4) & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, tmp[0]);
+        CVT_B2TOF32(p0, tmp[1]);
+        CVT_B0TOF32(z0, v2zero[0]);
+        CVT_B2TOF32(z0, v2zero[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
+        v2scale[0] = (float)scale[2]; v2scale[1] = (float)scale[3];
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 2)), tmp);
+
+        CVT_B1TOF32(p0, tmp[0]);
+        CVT_B3TOF32(p0, tmp[1]);
+        CVT_B1TOF32(z0, v2zero[0]);
+        CVT_B3TOF32(z0, v2zero[1]);
+
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(v2zero, v2neg, *((v2f*)tmp));
+        v2scale[0] = (float)scale[6]; v2scale[1] = (float)scale[7];
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), v2scale, v2z);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 6)), tmp);
+    } else {
+        v2f a0, v2z, v2scale, v2zero, v2neg;
+        v2z[0] = 0; v2z[1] = 0;
+        v2neg[0] = -1.0f; v2neg[1] = -1.0f;
+        int p0 = p & 0x0f0f0f0f;
+        int z0 = scale_zero & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, a0.x);
+        CVT_B2TOF32(p0, a0.y);
+        CVT_B0TOF32(z0, v2zero[0]);
+        CVT_B2TOF32(z0, v2zero[1]);
+        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
+        v2scale[0] = (float)scale[0]; v2scale[1] = (float)scale[1];
+        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
+        out[0] = (scalar_t)a0.x;
+        out[1] = (scalar_t)a0.y;
+
+        CVT_B1TOF32(p0, a0.x);
+        CVT_B3TOF32(p0, a0.y);
+        CVT_B1TOF32(z0, v2zero[0]);
+        CVT_B3TOF32(z0, v2zero[1]);
+        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
+        v2scale[0] = (float)scale[4]; v2scale[1] = (float)scale[5];
+        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
+        out[4] = (scalar_t)a0.x;
+        out[5] = (scalar_t)a0.y;
+
+        p0 = (p >> 4) & 0x0f0f0f0f;
+        z0 = (scale_zero >> 4) & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, a0.x);
+        CVT_B2TOF32(p0, a0.y);
+        CVT_B0TOF32(z0, v2zero[0]);
+        CVT_B2TOF32(z0, v2zero[1]);
+        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
+        v2scale[0] = (float)scale[2]; v2scale[1] = (float)scale[3];
+        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
+        out[2] = (scalar_t)a0.x;
+        out[3] = (scalar_t)a0.y;
+
+        CVT_B1TOF32(p0, a0.x);
+        CVT_B3TOF32(p0, a0.y);
+        CVT_B1TOF32(z0, v2zero[0]);
+        CVT_B3TOF32(z0, v2zero[1]);
+        a0 = __builtin_mxc_pk_fma_f32(v2zero, v2neg, a0);
+        v2scale[0] = (float)scale[6]; v2scale[1] = (float)scale[7];
+        a0 = __builtin_mxc_pk_fma_f32(a0, v2scale, v2z);
+        out[6] = (scalar_t)a0.x;
+        out[7] = (scalar_t)a0.y;
+    }
+}
+
+//deqaunt a uint32_t
+template<class scalar_t>
+__device__ __forceinline__ void dequant_gptq_4bits(const PackType& p, scalar_t (&out)[8], const v2f& scale, const v2f& scale_zero) {
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        v2f a0;
+        float tmp[2];
+        int p0 = p & 0x0f0f0f0f;
+
+        // CVT_B0TOF32(p0, a0.x);
+        // CVT_B2TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[0] = __float2bfloat16(a0.x);
+        // out[1] = __float2bfloat16(a0.y);
+
+        CVT_B0TOF32(p0, tmp[0]);
+        CVT_B2TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)out), tmp);
+
+        // CVT_B1TOF32(p0, a0.x);
+        // CVT_B3TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[4] = __float2bfloat16(a0.x);
+        // out[5] = __float2bfloat16(a0.y);
+
+        CVT_B1TOF32(p0, tmp[0]);
+        CVT_B3TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 4)), tmp);
+
+        p0 = (p >> 4) & 0x0f0f0f0f;
+
+        // CVT_B0TOF32(p0, a0.x);
+        // CVT_B2TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[2] = __float2bfloat16(a0.x);
+        // out[3] = __float2bfloat16(a0.y);
+
+        CVT_B0TOF32(p0, tmp[0]);
+        CVT_B2TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 2)), tmp);
+
+        // CVT_B1TOF32(p0, a0.x);
+        // CVT_B3TOF32(p0, a0.y);
+        // a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        // out[6] = __float2bfloat16(a0.x);
+        // out[7] = __float2bfloat16(a0.y);
+
+        CVT_B1TOF32(p0, tmp[0]);
+        CVT_B3TOF32(p0, tmp[1]);
+        *((v2f*)tmp) = __builtin_mxc_pk_fma_f32(*((v2f*)tmp), scale, scale_zero);
+        f32x2_cvt_bf16x2(*((uint32_t*)(out + 6)), tmp);
+    } else {
+        v2f a0;
+        int p0 = p & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, a0.x);
+        CVT_B2TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[0] = (scalar_t)a0.x;
+        out[1] = (scalar_t)a0.y;
+
+        CVT_B1TOF32(p0, a0.x);
+        CVT_B3TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[4] = (scalar_t)a0.x;
+        out[5] = (scalar_t)a0.y;
+
+        p0 = (p >> 4) & 0x0f0f0f0f;
+        CVT_B0TOF32(p0, a0.x);
+        CVT_B2TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[2] = (scalar_t)a0.x;
+        out[3] = (scalar_t)a0.y;
+
+        CVT_B1TOF32(p0, a0.x);
+        CVT_B3TOF32(p0, a0.y);
+        a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+        out[6] = (scalar_t)a0.x;
+        out[7] = (scalar_t)a0.y;
+    }
+}
+
+template<class scalar_t>
+__device__ __forceinline__ void dequant_gptq_8bits(const PackType& p, scalar_t (&out)[4], const v2f& scale, const v2f& scale_zero) {
+    v2f a0;
+    CVT_B0TOF32(p, a0.x);
+    CVT_B1TOF32(p, a0.y);
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+    out[0] = (scalar_t)a0.x;
+    out[1] = (scalar_t)a0.y;
+
+    CVT_B2TOF32(p, a0.x);
+    CVT_B3TOF32(p, a0.y);
+    a0 = __builtin_mxc_pk_fma_f32(a0, scale, scale_zero);
+    out[2] = (scalar_t)a0.x;
+    out[3] = (scalar_t)a0.y;
+}
+
+// decompress zero
+__device__ __forceinline__ void decompress_zero_4bits(const PackType& zp, float (&out)[8]) {
+    v2f a0;
+    int p0 = zp & 0x0f0f0f0f;
+    CVT_B0TOF32(p0, a0.x);
+    CVT_B2TOF32(p0, a0.y);
+    out[0] = -a0.x;
+    out[1] = -a0.y;
+
+    CVT_B1TOF32(p0, a0.x);
+    CVT_B3TOF32(p0, a0.y);
+    out[4] = -a0.x;
+    out[5] = -a0.y;
+
+    p0 = (zp >> 4) & 0x0f0f0f0f;
+    CVT_B0TOF32(p0, a0.x);
+    CVT_B2TOF32(p0, a0.y);
+    out[2] = -a0.x;
+    out[3] = -a0.y;
+
+    CVT_B1TOF32(p0, a0.x);
+    CVT_B3TOF32(p0, a0.y);
+    out[6] = -a0.x;
+    out[7] = -a0.y;
+}
+
+namespace __hgemm_singular_blocks_k {
+template<typename scalar_t, const vllm::ScalarTypeId w_type_id, int THREADS, int BLOCKS_M, int BLOCKS_N, int BLOCKS_K, bool HAS_ACT, bool HAS_ZP, bool HAS_M_PRED, bool HAS_NK_PRED>
+struct LoadingManager {
+    constexpr static int FragACount = 2;
+    using FragA = PackType;
+    constexpr static int FragBCount = 1;
+    using FragB = PackType;
+    constexpr static int FragCCount = 4;
+    #ifdef BF16_HIGH_PRECISION
+    using FragC = scalar_t;
+    #else
+    //Directly use half as the final atomic type:
+    //1. Half precision and data range satisfies need of deepseek gemm
+    //2. C500 has no atomic instructions for bfloat16, we cannot atomic a bfloat16 memory
+    //3. The perfect precision type of atomic should be fp32, but the cost is too high to allocate a temp memory for float atomic
+    using atomic_type = half;
+    using FragC = atomic_type;
+    #endif
+    const FragA* A;
+    const FragA* A_loading;
+    const FragB* B;
+    const FragB* B_loading;
+    FragC* C;
+    float* C_temp;
+    using FragScaleLoading = half2;
+    using FragZeroLoading = uint32_t;
+    const FragScaleLoading* scales;
+    const FragScaleLoading* scales_loading;
+    const FragZeroLoading* zeros;
+    const FragZeroLoading* zeros_loading;
+    
+    int m;
+    int n;
+    int k;
+    int quant_group_power2;
+    uint8_t* smem_base;
+    int bidx;
+
+    PackTypeInt4* bsm_a_ptr;
+    scalar_t* bsm_scales_ptr;
+    float* bsm_zeros_ptr;
+    float* remaining_bsm_ptr;
+
+    PackTypeInt2 local_a[BLOCKS_M][2];
+    PackType local_b[N_ITERS];
+    PackType local_b_cache[N_ITERS];
+    scalar_t local_dequanted_b_8bits[N_ITERS][2][PACK_RATIO_8BITS];
+    scalar_t local_dequanted_b[N_ITERS][PACK_RATIO_4BITS];
+    v2f local_scales[N_ITERS];
+    v2f local_zeros[N_ITERS];
+    FragScaleLoading temp_scales;
+    PackType temp_zeros;
+    float output[BLOCKS_M][N_ITERS][4];
+    FragA temp_a[LOADING_A_LOOP];
+
+    TileManager<BLOCKS_M, BLOCKS_N, BLOCKS_K> tile_manager;
+    ThreadView tv;
+
+    __device__ __forceinline__ void set_address(const PackTypeInt4* a,
+        const PackTypeInt4* b,
+        PackTypeInt4* c,
+        PackTypeInt4* c_temp,
+        const PackTypeInt4* scale_ptr,
+        const PackTypeInt4* zp_ptr = nullptr) {
+            A = (const FragA*)a;
+            B = (const FragB*)b;
+            C = (FragC*)c;
+            C_temp = (float*)c_temp;
+            scales = (const FragScaleLoading*)scale_ptr;
+            if constexpr(w_type_id == vllm::kU4.id()) {
+                zeros = (const FragZeroLoading*)zp_ptr;
+            }
+    }
+
+    __device__ __forceinline__ bool debug() {
+        #ifdef DEBUG
+        bool do_print = tv.wave_idx == 1 && tv.slot_idx == 0 && tv.slot_tid == 0;
+        return do_print;
+        #else
+        return false;
+        #endif
+    }
+
+    __device__ __forceinline__ void next_k() {
+        //Update only bsm_a_ptr
+        bsm_a_ptr = (PackTypeInt4*)smem_base;
+        bsm_a_ptr += tv.slot_tid * (PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+    }
+
+    __device__ __forceinline__ void next_k_pre() {
+        A_loading += SLICE_K / FragACount;
+        //B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading += SLICE_K / PACK_RATIO_8BITS * n;
+        }
+    }
+
+    __device__ __forceinline__ void ldg_a(int k_idx) {
+        //32x64/2/256 = 16 / 4 = 4
+        int t = tv.tid;
+        int k_broad = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (SLICE_K / FragACount);
+            int reading_k = t % (SLICE_K / FragACount);
+            int gvm_offset = reading_m * k / FragACount + reading_k;
+            FragA* gvm_addr = (FragA*)A_loading + gvm_offset;
+            //FIXME: we cannot do slice k pad as ldg_b32_bsm_async seems does not support padding
+            if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                bool pred = reading_m < m;
+                bool pred_k = k_broad + reading_k * FragACount < k;
+                pred = pred && pred_k && tile_manager.global_pred;
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_M_PRED) {
+                bool pred = reading_m < m && tile_manager.global_pred;
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_NK_PRED) {
+                bool pred_k = k_broad + reading_k * FragACount < k && tile_manager.global_pred;
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, pred_k, true);
+            } else {
+                ldg_b32_reg_noasync(temp_a[i], gvm_addr, tile_manager.global_pred, true);
+            }
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void sts_a() {
+        FragA* to_bsm_a_ptr = (FragA*)smem_base;
+        int t = tv.tid;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (SLICE_K / FragACount);
+            int reading_k = t % (SLICE_K / FragACount);
+            int bsm_offset = reading_m * (PAD_SLICE_K / FragACount) + reading_k;
+            *(to_bsm_a_ptr + bsm_offset) = temp_a[i];
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void lds_a(int midx) {
+        *((PackTypeInt4*)local_a[midx]) = *bsm_a_ptr;
+        bsm_a_ptr += SLOT * (PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t)));
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    __device__ __forceinline__ void ldg_b(int k_idx, int korder = 0) {
+        if constexpr(HAS_NK_PRED) {
+            bool pred_k = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K + tv.slot_idx * PACK_RATIO_4BITS + korder < k;
+            bool pred_n = tile_manager.tile_start_col * TILE_N + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS < n;
+            bool pred = pred_n && pred_k && tile_manager.global_pred;
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), pred, true);
+            }
+        } else {
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), tile_manager.global_pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), tile_manager.global_pred, true);
+            }
+        }
+    }
+
+    __device__ __forceinline__ void swap_b_cache(int i) {
+        local_b[i] = local_b_cache[i];
+    }
+
+    __device__ __forceinline__ void ldg_scales() {
+        bool pred = tv.tid < TILE_N / (sizeof(FragScaleLoading) / sizeof(scalar_t)) && tile_manager.global_pred;
+        if constexpr(HAS_NK_PRED) {
+            pred = pred && tv.tid < (n - tile_manager.tile_start_col * TILE_N) / (sizeof(FragScaleLoading) / sizeof(scalar_t));
+        }
+        //FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x2000) + tv.tid;
+        FragScaleLoading *gvm_addr = (FragScaleLoading*)scales_loading + tv.tid;
+        //ldg_b32_bsm_async(scale_bsm, gvm_addr, pred, false);
+        ldg_b32_reg_noasync(*((PackType*)&temp_scales), gvm_addr, pred, true);
+    }
+
+    __device__ __forceinline__ void ldg_zp() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if constexpr(HAS_NK_PRED) {
+                pred = pred && tv.tid < ((n - tile_manager.tile_start_col * TILE_N) / PACK_RATIO_4BITS);
+            }
+            FragZeroLoading *gvm_addr = (FragZeroLoading*)zeros_loading + tv.tid;
+            ldg_b32_reg_noasync(*((PackType*)&temp_zeros), gvm_addr, pred, true);
+        }
+    }
+
+    __device__ __forceinline__ void sts_scales() {
+        FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x2000) + tv.tid;
+        *scale_bsm = temp_scales;
+    }
+
+    __device__ __forceinline__ void sts_zeros() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if (pred) {
+                float temp[PACK_RATIO_4BITS];
+                decompress_zero_4bits(temp_zeros, temp);
+                float *scale_bsm = (float*)(smem_base + 0x3000) + tv.tid * PACK_RATIO_4BITS;
+                for (int i = 0; i < PACK_RATIO_4BITS; i++) {
+                    *(scale_bsm + i) = temp[i];
+                }
+            }
+        }
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    __device__ __forceinline__ void lds_scales() {
+        if constexpr(N_ITERS==2) {
+            *((half2*)local_dequanted_b[0]) = *((half2*)bsm_scales_ptr);
+        } else if constexpr(N_ITERS==4) {
+            *((PackTypeInt2*)local_dequanted_b[0]) = *((PackTypeInt2*)bsm_scales_ptr);
+        }
+    }
+
+    __device__ __forceinline__ void pack_scales() {
+        if constexpr(w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -8 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU4.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s;
+                if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+                    s = __bfloat162float(local_dequanted_b[0][i]);
+                } else {
+                    s = local_dequanted_b[0][i];
+                }
+                float z = *(bsm_zeros_ptr + i);
+                z = z * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -128 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8.id()) {
+            // should apply zeros
+        }
+    }
+
+    __device__ __forceinline__ void dequant(int kdx, int korder = 0) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            dequant_gptq_4bits<scalar_t>(local_b[kdx], local_dequanted_b[kdx], local_scales[kdx], local_zeros[kdx]);
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            dequant_gptq_8bits<scalar_t>(local_b[kdx], local_dequanted_b_8bits[kdx][korder], local_scales[kdx], local_zeros[kdx]);
+        }
+    }
+
+    __device__ __forceinline__ void matmul(int mdx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b[i]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b[i] + 1), *((PackTypeInt4*)output[mdx][i]));
+	    }
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b_8bits[i][0]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b_8bits[i][1]), *((PackTypeInt4*)output[mdx][i]));
+            }
+        }
+    }
+
+    __device__ __forceinline__ void clear_c() {
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    output[miter][niter][miter2] = 0;
+                }
+            }
+        }
+    }
+
+    //functions for preloading next tile data
+    __device__ __forceinline__ void init_address_pre(int _m, int _n, int _k, int _quant_group_power2, int _bidx, int _iters, uint8_t *_smem_base) {
+        tv.init();
+        m = _m;
+        n = _n;
+        k = _k;
+        quant_group_power2 = _quant_group_power2;
+        bidx = _bidx;
+        smem_base = _smem_base;
+        tile_manager.init(m, n, k, bidx, _iters);
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+    __device__ __forceinline__ void init_tile_pre(int col, int row) {
+        //Initialize start slice address and set them to A_loading and B_loading
+        int offset_n = col * TILE_N;
+        int offset_k = row * TILE_K;
+        //A_loading address will always be valid
+        A_loading = A + offset_k / (FragACount);
+        //B_loading = B + (offset_k / PACK_RATIO_4BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_4BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_8BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n * 2 + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        }
+        scales_loading = scales + ((offset_k >> quant_group_power2) * n + offset_n) / (sizeof(FragScaleLoading)/sizeof(scalar_t));
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            zeros_loading = zeros + ((offset_k >> quant_group_power2) * n + offset_n) / PACK_RATIO_4BITS;
+        }
+    }
+
+    __device__ __forceinline__ void next_tile_pre() {
+        tile_manager.next_tile_pre();
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+    __device__ __forceinline__ void init_bsm_addr() {
+        bsm_a_ptr = (PackTypeInt4*)smem_base;           //use 8k bytes, will load at most 32x128*sizeof(half), either m32k128 or m128k32
+        remaining_bsm_ptr = (float*)(smem_base + 0x2000 + 0x1000); //3k bytes
+        bsm_a_ptr += tv.slot_tid * (PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+        bsm_scales_ptr = (scalar_t*)(smem_base + 0x2000);      //use 128xsizeof(float)*2 = 1k
+        bsm_scales_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bsm_zeros_ptr = (float*)(smem_base + 0x3000);
+            bsm_zeros_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        }
+    }
+
+    __device__ __forceinline__ void write_c(int offset, const float& v) {
+        if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+            #ifdef BF16_HIGH_PRECISION
+            atomicAdd(C_temp + offset, v);
+            #else
+            atomicAdd(C+offset, (atomic_type)v);
+            #endif
+        } else {
+            atomicAdd(C + offset, (scalar_t)v);
+        }
+    }
+
+    //atomic write to c
+    __device__ __forceinline__ void write_c_pre() {
+        int k_broad = tv.slot_idx * 4;
+        int n_broad = (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS + tile_manager.tile_start_col_cache * TILE_N;
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                int store_n = n_broad + niter;
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    int store_m = k_broad + miter * SLICE_M + miter2;
+                    if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                        if (store_m < m && store_n < n) {
+                            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+			            }
+                    } else if constexpr(HAS_M_PRED) {
+                        if (store_m < m) {
+                            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+			            }
+                    } else if constexpr(HAS_NK_PRED) {
+                        if (store_n < n) {
+                            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+			            }
+                    } else {
+                        write_c(store_m * n + store_n, output[miter][niter][miter2]);
+		            }
+                }
+            }
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters1(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            ldg_b(k_idx, 1);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            dequant(0);
+            swap_b_cache(0);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0); //dequant b0
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters2(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(1);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(1);   //dequant b64
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            ldg_b(k_idx, 1);
+            dequant(0); //dequant b0
+            dequant(1);
+            swap_b_cache(0);
+            swap_b_cache(1);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1);
+            dequant(1, 1);   //dequant b64
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters3(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(1);
+            dequant(1);
+            swap_b_cache(2);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(2); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            ldg_b(k_idx, 1);
+            dequant(0); //dequant b0
+            dequant(1); //dequant b0
+            dequant(2); //dequant b0
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+            dequant(1, 1); //dequant b0
+            dequant(2, 1); //dequant b0
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters4(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(1);
+            dequant(1); //dequant b1
+            swap_b_cache(2);
+            swap_b_cache(3);
+            dequant(2); //dequant b2
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(3); //dequant b3
+        } else {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            dequant(0); //dequant b0
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+            ldg_b(k_idx, 1);
+            dequant(1); //dequant b0
+            dequant(2); //dequant b0
+            dequant(3); //dequant b0
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            if constexpr(!KTAIL) {
+                next_k_pre(); // preload gvm a/b
+                ldg_b(k_idx + 1); //preload b for next k
+                ldg_a(k_idx + 1); //preload a for next k
+            }
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b3
+            dequant(1, 1); //dequant b3
+            dequant(2, 1); //dequant b3
+            dequant(3, 1); //dequant b3
+        }
+    }
+
+    template<bool KTAIL>
+    __device__ __forceinline__ void on_dequant(int kdx) {
+        if constexpr(N_ITERS == 1) on_dequant_niters1<KTAIL>(kdx);
+        else if constexpr(N_ITERS == 2) on_dequant_niters2<KTAIL>(kdx);
+        else if constexpr(N_ITERS == 3) on_dequant_niters3<KTAIL>(kdx);
+        else if constexpr(N_ITERS == 4) on_dequant_niters4<KTAIL>(kdx);
+    }
+};
+
+template<typename scalar_t,
+    const vllm::ScalarTypeId w_type_id,
+    const int THREADS,          // number of threads in a threadblock
+    const int BLOCKS_M,         // number of 16x16 blocks in the m
+                                // dimension (batchsize) of the
+                                // threadblock
+    const int BLOCKS_N,         // same for n dimension (output)
+    const int BLOCKS_K,         // same for k dimension (reduction)
+    const bool HAS_ACT_ORDER,   // whether act_order is enabled
+    const bool HAS_ZP,          // whether zero-points are enabled
+    const bool HAS_M_PRED = true,  //If we should use predictors to load m from gvm
+    const bool HAS_NK_PRED = true  //If we should use predictors to load nk from gvm
+    >
+__global__ void hgemm_gptq(
+    const PackTypeInt4* __restrict__ A,  // fp16 input matrix of shape mxk
+    const PackTypeInt4* __restrict__ B,  // 4bit quantized weight matrix of shape kxn
+    PackTypeInt4* __restrict__ C,        // fp16 output buffer of shape mxn
+    PackTypeInt4* __restrict__ C_tmp,    // fp32 tmp output buffer (for reduce)
+    const PackTypeInt4* __restrict__ scales_ptr,  // fp16 quantization scales of shape
+                                          // (k/groupsize)xn
+    const PackTypeInt4* __restrict__ zp_ptr,      // 4bit packed zero-points of shape
+                                          // (k/groupsize)x(n/pack_factor)
+    const int* __restrict__ g_idx,        // int32 group indices of shape k
+    int prob_m,           // batch dimension m
+    int prob_n,           // output dimension n
+    int prob_k,           // reduction dimension k
+    int quant_group_power2, // quant group means how many quanted values share the same scale and zero, this value restricts to 2^x where x >= 5
+    int max_iters,        // max tile iterations for one block
+    int* locks,           // extra global storage for barrier synchronization
+    bool use_fp32_reduce  // whether to use fp32 global reduce
+) {
+    int bidx = blockIdx.x;
+    __shared__ uint8_t smem_base[0x4000]; //4x16x256 = 16Kbytes
+    using LoadingManagerType = LoadingManager<scalar_t, w_type_id, THREADS, BLOCKS_M, BLOCKS_N, BLOCKS_K, HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED>;
+    LoadingManagerType loading_manager;
+    A += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_k / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    C += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    #ifdef BF16_HIGH_PRECISION
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        C_tmp += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(float));
+    }
+    #endif
+    loading_manager.set_address(A, B, C, C_tmp, scales_ptr, zp_ptr);
+    //loading_manager.init_address(prob_m, prob_n, prob_k, bidx, max_iters, smem_base);
+    loading_manager.init_address_pre(std::min<int>(MAX_BLOCKS_M*SLICE_M, prob_m - blockIdx.y * (MAX_BLOCKS_M * SLICE_M)), prob_n, prob_k, quant_group_power2, bidx, max_iters, smem_base);
+    loading_manager.clear_c();
+
+    while (max_iters > 0) {
+        loading_manager.init_bsm_addr(); //reset all bsm address for current tile
+        loading_manager.ldg_scales(); //Load all scales to bsm
+        loading_manager.ldg_zp();
+        loading_manager.ldg_b(0);    //load b0 and b64, two gvm
+        loading_manager.ldg_a(0);    //Load first k0~31 and all m, one ldg_b128, heavy load
+        loading_manager.sts_scales();
+        loading_manager.sts_zeros();
+        barrier_bsm;
+        loading_manager.lds_scales(); //load scale0 and scale64
+        loading_manager.pack_scales(); //pack scales into two v2f structure
+
+        int k_idx = 0;
+        if constexpr(BLOCKS_K > 1) {
+            #pragma unroll BLOCKS_K - 1
+            for (; k_idx < BLOCKS_K - 1; k_idx++) {
+                int m_idx = 0;
+                loading_manager.template on_dequant<false>(k_idx);
+                //Loop for 3 times so that we can add some loading instructions before matmul
+                if constexpr(BLOCKS_M > 1) {
+                    #pragma unroll BLOCKS_M - 1
+                    for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                        loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                        loading_manager.matmul(m_idx); //do matmul
+                    }
+                }
+                barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+                loading_manager.next_k(); //modify gvm/bsm address of a and b
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        int m_idx = 0;
+        loading_manager.template on_dequant<true>(k_idx);
+        if constexpr(BLOCKS_M > 1) {
+            #pragma unroll BLOCKS_M - 1
+            for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        loading_manager.next_tile_pre();
+
+        loading_manager.matmul(m_idx); //do matmul
+        max_iters--;
+
+        if (loading_manager.tile_manager.need_save_data_pre()) {
+            loading_manager.write_c_pre(); // reduce and write back
+            loading_manager.clear_c();
+        }
+
+        barrier_bsm;
+    }
+}
+
+} //end of namespace __hgemm_singular_blocks_k
+
+namespace __hgemm_even_blocks_k {
+template<typename scalar_t, const vllm::ScalarTypeId w_type_id, int THREADS, int BLOCKS_M, int BLOCKS_N, int BLOCKS_K, bool HAS_ACT, bool HAS_ZP, bool HAS_M_PRED, bool HAS_NK_PRED>
+struct LoadingManager {
+    constexpr static int FragACount = 4;
+    using FragA = PackTypeInt2;
+    constexpr static int FragBCount = 1;
+    using FragB = PackType;
+    constexpr static int FragCCount = 4;
+    #ifdef BF16_HIGH_PRECISION
+    using FragC = scalar_t;
+    #else
+    //Directly use half as the final atomic type:
+    //1. Half precision and data range satisfies need of deepseek gemm
+    //2. C500 has no atomic instructions for bfloat16, we cannot atomic a bfloat16 memory
+    //3. The perfect precision type of atomic should be fp32, but the cost is too high to allocate a temp memory for float atomic
+    using atomic_type = half;
+    using FragC = atomic_type;
+    #endif
+    const FragA* A;
+    const FragA* A_loading;
+    const FragB* B;
+    const FragB* B_loading;
+    FragC* C;
+    float* C_temp;
+    using FragScaleLoading = half2;
+    using FragZeroLoading = uint32_t;
+    const FragScaleLoading* scales;
+    const FragScaleLoading* scales_loading;
+    const FragZeroLoading* zeros;
+    const FragZeroLoading* zeros_loading;
+
+    constexpr static int DOUBLE_SLICE_K = SLICE_K * 2;
+    constexpr static int DOUBLE_PAD_SLICE_K = SLICE_K * 2 + sizeof(PackTypeInt4) / sizeof(scalar_t);
+
+    int m;
+    int n;
+    int k;
+    int quant_group_power2;
+    uint8_t* smem_base;
+    int bidx;
+
+    PackTypeInt4* bsm_a_ptr;
+    scalar_t* bsm_scales_ptr;
+    float* bsm_zeros_ptr;
+    //float* remaining_bsm_ptr;
+
+    PackTypeInt2 local_a[BLOCKS_M][2];
+    PackType local_b[N_ITERS];
+    PackType local_b_cache[N_ITERS];
+    scalar_t local_dequanted_b[N_ITERS][PACK_RATIO_4BITS];
+    scalar_t local_dequanted_b_8bits[N_ITERS][2][PACK_RATIO_8BITS];
+    v2f local_scales[N_ITERS];
+    v2f local_zeros[N_ITERS];
+    FragScaleLoading temp_scales;
+    PackType temp_zeros;
+    float output[BLOCKS_M][N_ITERS][4];
+    FragA temp_a[LOADING_A_LOOP];
+
+    TileManager<BLOCKS_M, BLOCKS_N, BLOCKS_K> tile_manager;
+    ThreadView tv;
+
+    __device__ __forceinline__ void set_address(const PackTypeInt4* a,
+        const PackTypeInt4* b,
+        PackTypeInt4* c,
+        PackTypeInt4* c_temp,
+        const PackTypeInt4* scale_ptr,
+        const PackTypeInt4* zp_ptr = nullptr) {
+            A = (const FragA*)a;
+            B = (const FragB*)b;
+            C = (FragC*)c;
+            C_temp = (float*)c_temp;
+            scales = (const FragScaleLoading*)scale_ptr;
+            if constexpr(w_type_id == vllm::kU4.id()) {
+                zeros = (const FragZeroLoading*)zp_ptr;
+            }
+    }
+
+    __device__ __forceinline__ bool debug() {
+        #ifdef DEBUG
+        bool do_print = tv.wave_idx == 1 && tv.slot_idx == 0 && tv.slot_tid == 0;
+        return do_print;
+        #else
+        return false;
+        #endif
+    }
+
+    __device__ __forceinline__ void next_k0() {
+        //reset bsm a to base
+        bsm_a_ptr = (PackTypeInt4*)smem_base;
+        bsm_a_ptr += tv.slot_tid * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+    }
+
+    __device__ __forceinline__ void next_k1() {
+        //Update only bsm_a_ptr
+        bsm_a_ptr = (PackTypeInt4*)smem_base;
+        bsm_a_ptr += tv.slot_tid * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx + WAVE_SLOTS;
+        //load k32~k63
+        //bsm_a_ptr += 4;
+    }
+
+    __device__ __forceinline__ void next_k0_pre() {
+        //A_loading += SLICE_K / FragACount;
+        //B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading += SLICE_K / PACK_RATIO_8BITS * n;
+        }
+    }
+
+    __device__ __forceinline__ void next_k1_pre() {
+        A_loading += DOUBLE_SLICE_K / FragACount;
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading += SLICE_K / PACK_RATIO_4BITS * n;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading += SLICE_K / PACK_RATIO_8BITS * n;
+        }
+    }
+
+    __device__ __forceinline__ void ldg_a(int k_idx) {
+        //32x64/2/256 = 16 / 4 = 4
+        int t = tv.tid;
+        int k_broad = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (DOUBLE_SLICE_K / FragACount);
+            int reading_k = t % (DOUBLE_SLICE_K / FragACount);
+            int gvm_offset = reading_m * k / FragACount + reading_k;
+            FragA* gvm_addr = (FragA*)A_loading + gvm_offset;
+            //FIXME: we cannot do slice k pad as ldg_b32_bsm_async seems does not support padding
+            if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                bool pred = reading_m < m;
+                bool pred_k = k_broad + reading_k * FragACount < k;
+                pred = pred && pred_k && tile_manager.global_pred;
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_M_PRED) {
+                bool pred = reading_m < m && tile_manager.global_pred;
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, pred, true);
+            } else if constexpr(HAS_NK_PRED) {
+                bool pred_k = k_broad + reading_k * FragACount < k && tile_manager.global_pred;
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, pred_k, true);
+            } else {
+                ldg_b64_reg_noasync(temp_a[i], gvm_addr, tile_manager.global_pred, true);
+            }
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void sts_a() {
+        FragA* to_bsm_a_ptr = (FragA*)smem_base;
+        int t = tv.tid;
+        #pragma unroll LOADING_A_LOOP
+        for (int i = 0; i < LOADING_A_LOOP; i++)  {
+            int reading_m = t / (DOUBLE_SLICE_K / FragACount);
+            int reading_k = t % (DOUBLE_SLICE_K / FragACount);
+            int bsm_offset = reading_m * (DOUBLE_PAD_SLICE_K / FragACount) + reading_k;
+            *(to_bsm_a_ptr + bsm_offset) = temp_a[i];
+            t += THREADS;
+        }
+    }
+
+    __device__ __forceinline__ void lds_a(int midx) {
+        *((PackTypeInt4*)local_a[midx]) = *bsm_a_ptr;
+        bsm_a_ptr += SLOT * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t)));
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    //korder used gptq_8bits, ldg_b will load two times in one SLICE_K
+    //For example, t0 loads packed_k0, packed_k1, and packed_k0 represents a packed 4 ks in first line of B,
+    //and packed_k1 represents a packed 4 ks in second line of B
+    __device__ __forceinline__ void ldg_b(int k_idx, int korder = 0) {
+        if constexpr(HAS_NK_PRED) {
+            bool pred_k = tile_manager.tile_start_row * TILE_K + k_idx * SLICE_K + tv.slot_idx * PACK_RATIO_4BITS + korder < k;
+            bool pred_n = tile_manager.tile_start_col * TILE_N + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS < n;
+            bool pred = pred_n && pred_k && tile_manager.global_pred;
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), pred, true);
+            }
+        } else {
+            FragB* addr =  (FragB*)B_loading + korder * n;
+            if constexpr(N_ITERS == 2) {
+                ldg_b64_reg_noasync(*((PackTypeInt2*)local_b_cache), ((PackTypeInt2*)addr), tile_manager.global_pred, true);
+            } else if constexpr(N_ITERS == 4) {
+                ldg_b128_reg_noasync(*((PackTypeInt4*)local_b_cache), ((PackTypeInt4*)addr), tile_manager.global_pred, true);
+            }
+        }
+    }
+
+    __device__ __forceinline__ void swap_b_cache(int i) {
+        local_b[i] = local_b_cache[i];
+    }
+
+    __device__ __forceinline__ void ldg_scales() {
+        bool pred = tv.tid < TILE_N / (sizeof(FragScaleLoading) / sizeof(scalar_t)) && tile_manager.global_pred;
+        if constexpr(HAS_NK_PRED) {
+            pred = pred && tv.tid < (n - tile_manager.tile_start_col * TILE_N) / (sizeof(FragScaleLoading) / sizeof(scalar_t));
+        }
+        //FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x2000) + tv.tid;
+        FragScaleLoading *gvm_addr = (FragScaleLoading*)scales_loading + tv.tid;
+        //ldg_b32_bsm_async(scale_bsm, gvm_addr, pred, false);
+        ldg_b32_reg_noasync(*((PackType*)&temp_scales), gvm_addr, pred, true);
+    }
+
+    __device__ __forceinline__ void ldg_zp() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if constexpr(HAS_NK_PRED) {
+                pred = pred && tv.tid < ((n - tile_manager.tile_start_col * TILE_N) / PACK_RATIO_4BITS);
+            }
+            FragZeroLoading *gvm_addr = (FragZeroLoading*)zeros_loading + tv.tid;
+            ldg_b32_reg_noasync(*((PackType*)&temp_zeros), gvm_addr, pred, true);
+        }
+    }
+
+    __device__ __forceinline__ void sts_scales() {
+        FragScaleLoading *scale_bsm = (FragScaleLoading*)(smem_base + 0x3000) + tv.tid;
+        *scale_bsm = temp_scales;
+    }
+
+    __device__ __forceinline__ void sts_zeros() {
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bool pred = (tv.tid < TILE_N / PACK_RATIO_4BITS) && tile_manager.global_pred;
+            if (pred) {
+                float temp[PACK_RATIO_4BITS];
+                decompress_zero_4bits(temp_zeros, temp);
+                float *zeros_bsm = (float*)(smem_base + 0x3400) + tv.tid * PACK_RATIO_4BITS;
+                for (int i = 0; i < PACK_RATIO_4BITS; i++) {
+                    *(zeros_bsm + i) = temp[i];
+                }
+            }
+        }
+    }
+
+    //TODO: implement when N_ITERS==1 or N_ITERS==3
+    __device__ __forceinline__ void lds_scales() {
+        if constexpr(N_ITERS==2) {
+            *((half2*)local_dequanted_b[0]) = *((half2*)bsm_scales_ptr);
+        } else if constexpr(N_ITERS==4) {
+            *((PackTypeInt2*)local_dequanted_b[0]) = *((PackTypeInt2*)bsm_scales_ptr);
+        }
+    }
+
+    __device__ __forceinline__ void pack_scales() {
+        if constexpr(w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -8 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU4.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s;
+                if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+                    s = __bfloat162float(local_dequanted_b[0][i]);
+                } else {
+                    s = local_dequanted_b[0][i];
+                }
+                float z = *(bsm_zeros_ptr + i);
+                z = z * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                float s = local_dequanted_b[0][i];
+                float z = -128 * s;
+                local_scales[i] = {s, s};
+                local_zeros[i] = {z, z};
+            }
+        } else if constexpr(w_type_id == vllm::kU8.id()) {
+            // should apply zeros
+        }
+    }
+
+    __device__ __forceinline__ void dequant(int kdx, int korder = 0) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            dequant_gptq_4bits<scalar_t>(local_b[kdx], local_dequanted_b[kdx], local_scales[kdx], local_zeros[kdx]);
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            dequant_gptq_8bits<scalar_t>(local_b[kdx], local_dequanted_b_8bits[kdx][korder], local_scales[kdx], local_zeros[kdx]);
+        }
+    }
+
+    __device__ __forceinline__ void matmul(int mdx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b[i]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b[i] + 1), *((PackTypeInt4*)output[mdx][i]));
+            }
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            #pragma unroll
+            for (int i = 0; i < N_ITERS; i++) {
+                mma_16x16x16<scalar_t>(local_a[mdx][0], *((PackTypeInt2*)local_dequanted_b_8bits[i][0]), *((PackTypeInt4*)output[mdx][i]));
+                mma_16x16x16<scalar_t>(local_a[mdx][1], *((PackTypeInt2*)local_dequanted_b_8bits[i][1]), *((PackTypeInt4*)output[mdx][i]));
+            }
+        }
+    }
+
+    __device__ __forceinline__ void clear_c() {
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    output[miter][niter][miter2] = 0;
+                }
+            }
+        }
+    }
+
+    //functions for preloading next tile data
+    __device__ __forceinline__ void init_address_pre(int _m, int _n, int _k, int _quant_group_power2, int _bidx, int _iters, uint8_t *_smem_base) {
+        tv.init();
+        m = _m;
+        n = _n;
+        k = _k;
+        quant_group_power2 = _quant_group_power2;
+        bidx = _bidx;
+        smem_base = _smem_base;
+        tile_manager.init(m, n, k, bidx, _iters);
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+     __device__ __forceinline__ void init_tile_pre(int col, int row) {
+        //Initialize start slice address and set them to A_loading and B_loading
+        int offset_n = col * TILE_N;
+        int offset_k = row * TILE_K;
+        //A_loading address will always be valid
+        A_loading = A + offset_k / (FragACount);
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_4BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            B_loading = B + (offset_k / PACK_RATIO_8BITS * n + offset_n) / (FragBCount) + tv.slot_idx * n * 2 + (tv.wave_idx * SLOT + tv.slot_tid)*N_ITERS;
+        }
+        scales_loading = scales + ((offset_k >> quant_group_power2) * n + offset_n) / (sizeof(FragScaleLoading)/sizeof(scalar_t));
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            zeros_loading = zeros + ((offset_k >> quant_group_power2) * n + offset_n) / PACK_RATIO_4BITS;
+        }
+    }
+
+    __device__ __forceinline__ void next_tile_pre() {
+        tile_manager.next_tile_pre();
+        init_tile_pre(tile_manager.tile_start_col, tile_manager.tile_start_row);
+    }
+
+    __device__ __forceinline__ void init_bsm_addr() {
+        bsm_a_ptr = (PackTypeInt4*)smem_base;           //use 8k bytes, will load at most 32x128*sizeof(half), either m32k128 or m128k32
+        //remaining_bsm_ptr = (float*)(smem_base + 0x2000 + 0x1000); //3k bytes
+        bsm_a_ptr += tv.slot_tid * (DOUBLE_PAD_SLICE_K / (sizeof(PackTypeInt4) / sizeof(scalar_t))) + tv.slot_idx;
+        bsm_scales_ptr = (scalar_t*)(smem_base + 0x3000);      //use 128xsizeof(float)*2 = 1k
+        bsm_scales_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        if constexpr(w_type_id == vllm::kU4.id()) {
+            bsm_zeros_ptr = (float*)(smem_base + 0x3400);      //use 128xsizeof(float)*2 = 1k
+            bsm_zeros_ptr += (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS;
+        }
+    }
+
+    __device__ __forceinline__ void write_c(int offset, const float& v) {
+        if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+            #ifdef BF16_HIGH_PRECISION
+	        atomicAdd(C_temp + offset, v);
+            #else
+            atomicAdd(C + offset, (atomic_type)v);
+            #endif
+        } else {
+            atomicAdd(C + offset, (scalar_t)v);
+        }
+    }
+
+    //atomic write to c
+    __device__ __forceinline__ void write_c_pre() {
+        int k_broad = tv.slot_idx * 4;
+        int n_broad = (tv.wave_idx * SLOT + tv.slot_tid) * N_ITERS + tile_manager.tile_start_col_cache * TILE_N;
+        #pragma unroll
+        for (int miter = 0; miter < BLOCKS_M; miter++) {
+            #pragma unroll
+            for (int niter = 0; niter < N_ITERS; niter++) {
+                int store_n = n_broad + niter;
+                #pragma unroll
+                for (int miter2 = 0; miter2 < 4; miter2++) {
+                    int store_m = k_broad + miter * SLICE_M + miter2;
+                    if constexpr(HAS_M_PRED && HAS_NK_PRED) {
+                        if (store_m < m && store_n < n) {
+			                write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                        }
+                    } else if constexpr(HAS_M_PRED) {
+                        if (store_m < m) {
+			                write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                        }
+                    } else if constexpr(HAS_NK_PRED) {
+                        if (store_n < n) {
+			                write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                        }
+                    } else {
+			            write_c(store_m * n + store_n, output[miter][niter][miter2]);
+                    }
+                }
+            }
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_sts_a() {
+        if constexpr(K == 0) {
+            sts_a(); // reorder data_a from reg(gvm resouce) to bsm
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_preload(int k_idx) {
+        if constexpr(!KTAIL) {
+            if constexpr(K == 0) {
+                next_k0_pre(); // preload gvm a/b
+            } else {
+                next_k1_pre(); // preload gvm a/b
+            }
+            ldg_b(k_idx + K + 1); //preload b for next k
+            if constexpr(K == 1) {
+                ldg_a(k_idx + K + 1); //preload a for next k
+            }
+        } else {
+            next_tile_pre();
+            ldg_scales();
+            ldg_zp();
+            ldg_b(0);
+            ldg_a(0);
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters1(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            on_sts_a<K,KTAIL>(); // reorder data_a from reg(gvm resouce) to bsm
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            ldg_b(0, 1);
+            dequant(0, 0);
+            on_sts_a<K,KTAIL>(); // reorder data_a from reg(gvm resouce) to bsm
+            swap_b_cache(0);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters2(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(1);   //dequant b64
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            ldg_b(0, 1);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            dequant(1);
+            swap_b_cache(0);
+            swap_b_cache(1);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1);   //dequant b64
+            dequant(1, 1);   //dequant b64
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters3(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            dequant(1);
+            swap_b_cache(2);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(2); //dequant b0
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            ldg_b(0, 1);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            dequant(1);
+            dequant(2);
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+            dequant(1, 1); //dequant b0
+            dequant(2, 1); //dequant b0
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant_niters4(int k_idx) {
+        if constexpr(w_type_id == vllm::kU4.id() || w_type_id == vllm::kU4B8.id()) {
+            swap_b_cache(0);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            swap_b_cache(1);
+            dequant(1); //dequant b1
+            swap_b_cache(2);
+            swap_b_cache(3);
+            dequant(2); //dequant b2
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(3); //dequant b3
+        } else if constexpr(w_type_id == vllm::kU8.id() || w_type_id == vllm::kU8B128.id()) {
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            ldg_b(0, 1);
+            dequant(0); //dequant b0
+            on_sts_a<K,KTAIL>();
+            dequant(1); //dequant b1
+            dequant(2); //dequant b2
+            dequant(3); //dequant b2
+            swap_b_cache(0);
+            swap_b_cache(1);
+            swap_b_cache(2);
+            swap_b_cache(3);
+            on_preload<K,KTAIL>(k_idx);
+            barrier_bsm; //sync threads for sts_a
+            lds_a(0); //load first 16 batches into registers
+            dequant(0, 1); //dequant b0
+            dequant(1, 1); //dequant b1
+            dequant(2, 1); //dequant b2
+            dequant(3, 1); //dequant b3
+        }
+    }
+
+    template<int K, bool KTAIL>
+    __device__ __forceinline__ void on_dequant(int kdx) {
+        if constexpr(N_ITERS == 1) on_dequant_niters1<K,KTAIL>(kdx);
+        else if constexpr(N_ITERS == 2) on_dequant_niters2<K,KTAIL>(kdx);
+        else if constexpr(N_ITERS == 3) on_dequant_niters3<K,KTAIL>(kdx);
+        else if constexpr(N_ITERS == 4) on_dequant_niters4<K,KTAIL>(kdx);
+    }
+};
+
+template<typename scalar_t,
+    const vllm::ScalarTypeId w_type_id,
+    const int THREADS,          // number of threads in a threadblock
+    const int BLOCKS_M,         // number of 16x16 blocks in the m
+                                // dimension (batchsize) of the
+                                // threadblock
+    const int BLOCKS_N,         // same for n dimension (output)
+    const int BLOCKS_K,         // same for k dimension (reduction)
+    const bool HAS_ACT_ORDER,   // whether act_order is enabled
+    const bool HAS_ZP,          // whether zero-points are enabled
+    const bool HAS_M_PRED = true,  //If we should use predictors to load m from gvm
+    const bool HAS_NK_PRED = true  //If we should use predictors to load nk from gvm
+    >
+__global__ void hgemm_gptq(
+    const PackTypeInt4* __restrict__ A,  // fp16 input matrix of shape mxk
+    const PackTypeInt4* __restrict__ B,  // 4bit quantized weight matrix of shape kxn
+    PackTypeInt4* __restrict__ C,        // fp16 output buffer of shape mxn
+    PackTypeInt4* __restrict__ C_tmp,    // fp32 tmp output buffer (for reduce)
+    const PackTypeInt4* __restrict__ scales_ptr,  // fp16 quantization scales of shape
+                                          // (k/groupsize)xn
+    const PackTypeInt4* __restrict__ zp_ptr,      // 4bit packed zero-points of shape
+                                          // (k/groupsize)x(n/pack_factor)
+    const int* __restrict__ g_idx,        // int32 group indices of shape k
+    int prob_m,           // batch dimension m
+    int prob_n,           // output dimension n
+    int prob_k,           // reduction dimension k
+    int quant_group_power2,
+    int max_iters,        // max tile iterations for one block
+    int* locks,           // extra global storage for barrier synchronization
+    bool use_fp32_reduce  // whether to use fp32 global reduce
+) {
+    int bidx = blockIdx.x;
+    __shared__ uint8_t smem_base[0x4000]; //4x16x256 = 16Kbytes
+    using LoadingManagerType = LoadingManager<scalar_t, w_type_id, THREADS, BLOCKS_M, BLOCKS_N, BLOCKS_K, HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED>;
+    LoadingManagerType loading_manager;
+    A += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_k / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    C += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(scalar_t));
+    #ifdef BF16_HIGH_PRECISION
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        C_tmp += blockIdx.y * (MAX_BLOCKS_M * SLICE_M) * prob_n / (sizeof(PackTypeInt4) / sizeof(float));
+    }
+    #endif
+    loading_manager.set_address(A, B, C, C_tmp, scales_ptr, zp_ptr);
+    //loading_manager.init_address(prob_m, prob_n, prob_k, bidx, max_iters, smem_base);
+    loading_manager.init_address_pre(std::min<int>(MAX_BLOCKS_M*SLICE_M, prob_m - blockIdx.y * (MAX_BLOCKS_M * SLICE_M)), prob_n, prob_k, quant_group_power2, bidx, max_iters, smem_base);
+
+    loading_manager.ldg_scales(); //Load all scales to bsm
+    loading_manager.ldg_zp();
+    loading_manager.ldg_b(0);    //load b in k0~31
+    loading_manager.ldg_a(0);    //Load first k0~63 and all m
+    loading_manager.clear_c();
+
+    while (max_iters > 0) {
+        loading_manager.init_bsm_addr(); //reset all bsm address for current tile
+        loading_manager.sts_scales();
+        loading_manager.sts_zeros();
+        barrier_bsm;
+        loading_manager.lds_scales(); //load scale0 and scale64
+        loading_manager.pack_scales(); //pack scales into two v2f structure
+
+        int k_idx = 0;
+        if constexpr(BLOCKS_K / 2 - 1 > 0) {
+            #pragma unroll BLOCKS_K / 2 - 1
+            for (int kloop = 0; kloop < BLOCKS_K / 2 - 1; kloop++) {
+                int m_idx = 0;
+                loading_manager.template on_dequant<0, false>(k_idx);
+                if constexpr(BLOCKS_M > 1) {
+                    #pragma unroll BLOCKS_M - 1
+                    for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                        loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                        loading_manager.matmul(m_idx); //do matmul
+                    }
+                }
+                barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+                loading_manager.next_k1(); //modify gvm/bsm address of a and b
+                loading_manager.matmul(m_idx); //do matmul
+                m_idx = 0;
+                loading_manager.template on_dequant<1, false>(k_idx);
+                if constexpr(BLOCKS_M > 1) {
+                    #pragma unroll BLOCKS_M - 1
+                    for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                        loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                        loading_manager.matmul(m_idx); //do matmul
+                    }
+                }
+                barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+                loading_manager.next_k0(); //modify gvm/bsm address of a and b
+                loading_manager.matmul(m_idx); //do matmul
+                k_idx += 2;
+            }
+        }
+
+        int m_idx = 0;
+        loading_manager.template on_dequant<0, false>(k_idx);
+        if constexpr(BLOCKS_M > 1) {
+            #pragma unroll BLOCKS_M - 1
+            for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+        loading_manager.next_k1(); //modify gvm/bsm address of a and b
+        loading_manager.matmul(m_idx); //do matmul
+        m_idx = 0;
+        loading_manager.template on_dequant<1, true>(k_idx);
+        if constexpr(BLOCKS_M > 1) {
+            #pragma unroll BLOCKS_M - 1
+            for (; m_idx < BLOCKS_M - 1; m_idx++) {
+                loading_manager.lds_a(m_idx+1); //pre load next 16 batches
+                loading_manager.matmul(m_idx); //do matmul
+            }
+        }
+        barrier_bsm; //wait for the last 16 batches ready, becasue of k_idx will be change
+        //loading_manager.next_tile_pre(); should move into on_dequant ?
+        loading_manager.matmul(m_idx); //do matmul
+
+        max_iters--;
+
+        if (loading_manager.tile_manager.need_save_data_pre()) {
+            loading_manager.write_c_pre(); // reduce and write back
+            loading_manager.clear_c();
+        }
+
+        barrier_bsm;
+    }
+}
+} //end of namespace __hgemm_even_blocks_k
+
+template<typename scalar_t,
+    const vllm::ScalarTypeId w_type_id,
+    int THREADS,
+    int BLOCKS_M,
+    int BLOCKS_N,
+    int BLOCKS_K,
+    bool HAS_ACT_ORDER,
+    bool HAS_ZP,
+    bool HAS_M_PRED,
+    bool HAS_NK_PRED>
+bool launch_gemm_gptq_kernel(const PackTypeInt4* A,
+    const PackTypeInt4* B,
+    PackTypeInt4* C,
+    PackTypeInt4* C_temp,
+    const PackTypeInt4* scales,
+    const PackTypeInt4* zeros,
+    int* g_idx, int m, int n, int k, int quant_group, int chunks, cudaStream_t stream = nullptr) {
+    int tiles_m = div_ceil(m, TILE_M);
+    int tiles_n = div_ceil(n, TILE_N);
+    int tiles_k = div_ceil(k, TILE_K);
+    if (TILE_K > quant_group && TILE_K % quant_group != 0) {
+        printf("Invalid TILE_K %d that can not be dived by QUANT_GROUP %d\n", TILE_K, quant_group);
+        return false;
+    }
+
+    int total_tiles = tiles_n * tiles_k;
+    int blocks = PEUS;
+    int iters = div_ceil(total_tiles, PEUS);
+    if (total_tiles < PEUS) {
+        if (TILE_K < quant_group) {
+            iters = quant_group / TILE_K;
+            blocks = div_ceil(total_tiles, iters);
+        } else {
+            iters = 1;
+            blocks = total_tiles;
+        }
+    } else {
+        if (TILE_K < quant_group) {
+            iters = div_ceil(iters, quant_group / TILE_K) * quant_group / TILE_K;
+            blocks = div_ceil(total_tiles, iters);
+        }
+    }
+    while (iters * blocks - total_tiles >= iters) {
+        blocks -= 1;
+    }
+
+    if (total_tiles < blocks) {
+        printf("total slice %d < blocks %d, Invalid configure\n", total_tiles, blocks);
+        return false;
+    }
+    // printf("Launching hgemm_gptq_4bits THREADS=%d, BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_K=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d, tiles_n=%d, tiles_k = %d, total_tiles = %d, iters = %d, blocks = %d, chunks = %d, TILE=m%dn%dk%d\n",
+    // THREADS, BLOCKS_M, BLOCKS_N, BLOCKS_K, HAS_M_PRED, HAS_NK_PRED, tiles_n, tiles_k, total_tiles, iters, blocks, chunks, BLOCKS_M*SLICE_M, BLOCKS_N*SLICE_N, BLOCKS_K*SLICE_K
+    // );
+
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        size_t num_elem = size_t(m) * size_t(n);
+        size_t clean_blocks = std::max(size_t(1), (num_elem + clean_kernel_thread_num * clean_kernel_pack_num - 1)/ (clean_kernel_thread_num * clean_kernel_pack_num));
+        clean_zero<clean_kernel_thread_num, clean_kernel_pack_num><<<clean_blocks, clean_kernel_thread_num, 0, stream>>>((float*)C_temp, num_elem);
+    }
+
+
+    //It is better to do perm before launch kernel
+    if constexpr(BLOCKS_K % 2 == 1) {
+        __hgemm_singular_blocks_k::hgemm_gptq<scalar_t,
+            w_type_id,
+            THREADS,
+            BLOCKS_M, BLOCKS_N, BLOCKS_K,
+            HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED
+            ><<<dim3(std::max(blocks,1), std::max(chunks,1), 1), THREADS, 0, stream>>>(A, B, C, C_temp, scales, zeros, g_idx, m, n, k, get_power2(quant_group), iters, nullptr, false);
+    } else {
+        __hgemm_even_blocks_k::hgemm_gptq<scalar_t,
+            w_type_id,
+            THREADS,
+            BLOCKS_M, BLOCKS_N, BLOCKS_K,
+            HAS_ACT_ORDER, HAS_ZP, HAS_M_PRED, HAS_NK_PRED
+            ><<<dim3(std::max(blocks,1), std::max(chunks,1), 1), THREADS, 0, stream>>>(A, B, C, C_temp, scales, zeros, g_idx, m, n, k, get_power2(quant_group), iters, nullptr, false);
+    }
+
+    if constexpr(std::is_same_v<scalar_t, __maca_bfloat16>) {
+        size_t num_elem = size_t(m) * size_t(n);
+        size_t reduce_blocks = std::max(size_t(1), (num_elem  + reduce_kernel_thread_num * reduce_kernel_pack_num - 1) / (reduce_kernel_thread_num * reduce_kernel_pack_num));
+        all_reduce<reduce_kernel_thread_num, reduce_kernel_pack_num, false><<<reduce_blocks, reduce_kernel_thread_num, 0, stream>>>((float*)C_temp, (maca_bfloat16*)C, num_elem);
+    }
+
+    return true;
+}
+
+}
+
diff --git a/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp b/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
new file mode 100644
index 000000000..6e0fdb0e3
--- /dev/null
+++ b/csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
@@ -0,0 +1,633 @@
+// 2024 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#pragma once
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include "Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+typedef __NATIVE_VECTOR__(2, float) v2f;
+template <int ThreadBlock, int BlockDimX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    if (splitKOffset + k_block > k) k = k - splitKOffset;
+    else k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements * 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr, *bsm_scales_ptr, *smem;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += BlockDimX) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            *((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS) + 1;
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS) + 1;
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[x*2] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[x*2+1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[x*2] = s1;
+            bsm_scales_ptr[x*2+1] = s2;
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        v2f local_scales_d16[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = bsm_scales_ptr[m_index + c];
+            float s_d16 = s / 16;
+            local_scales[c] = {s, s};
+            local_scales_d16[c] = {s_d16, s_d16};
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c],bsm_zeros_ptr[m_index + c]};
+
+        const int shuffled_dequant_index[PACK_RATIO] = {0, 4, 1, 5, 2, 6, 3, 7};
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            if constexpr (N <= 4) {
+                v2f local_b[PACK_RATIO/2*N];
+                for (int y = 0; y < N; y++) {
+                    for (int x = 0; x < PACK_RATIO / 2; x++) {
+                        local_b[x+PACK_RATIO/2*y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                        local_b[x+PACK_RATIO/2*y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                    }
+                }
+                #pragma unroll m_per_thread
+                for (int c = 0; c < m_per_thread; c++) {
+                    uint32_t p0 = A[c] & 0x0f0f0f0f;
+                    uint32_t p1 = A[c] & 0xf0f0f0f0;
+                    float o1,o2,o3,o4,o5,o6,o7,o8;
+                    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+                    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(p0));
+                    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+                    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(p0));
+                    asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o5):"r"(p1));
+                    asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o6):"r"(p1));
+                    asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o7):"r"(p1));
+                    asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o8):"r"(p1));
+                    v2f a0 = {o1, o3};
+                    v2f a1 = {o5, o7};
+                    v2f a2 = {o2, o4};
+                    v2f a3 = {o6, o8};
+
+                    a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+                    a1 = __builtin_mxc_pk_fma_f32(a1, local_scales_d16[c], local_zeros[c]);
+                    a2 = __builtin_mxc_pk_fma_f32(a2, local_scales[c], local_zeros[c]);
+                    a3 = __builtin_mxc_pk_fma_f32(a3, local_scales_d16[c], local_zeros[c]);
+
+                    #pragma unroll N
+                    for (int y = 0; y < N; y++) {
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[0+PACK_RATIO/2*y], c_splited[y][c]);
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[1+PACK_RATIO/2*y], c_splited[y][c]);
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a2, local_b[2+PACK_RATIO/2*y], c_splited[y][c]);
+                        c_splited[y][c] = __builtin_mxc_pk_fma_f32(a3, local_b[3+PACK_RATIO/2*y], c_splited[y][c]);
+                    }
+                }
+            } else {
+                v2f local_b;
+                for (int x = 0; x < PACK_RATIO / 2; x++) {
+                    for (int y = 0; y < N; y++) {
+                        local_b.x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                        local_b.y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                        for (int c = 0; c < m_per_thread; c++) {
+                            float a1 = (float)__builtin_mxc_ubfe(A[c], QBITS * shuffled_dequant_index[x*2], QBITS);
+                            float a2 = (float)__builtin_mxc_ubfe(A[c], QBITS * shuffled_dequant_index[x*2+1], QBITS);
+                            v2f a = {a1, a2};
+                            a = __builtin_mxc_pk_fma_f32(a, local_scales[c], local_zeros[c]);
+                            c_splited[y][c] = __builtin_mxc_pk_fma_f32(a, local_b, c_splited[y][c]);
+                        }
+                    }
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        for (int i = 0; i < m_per_thread; i++) {
+            smem[tidCol + (tidRow * m_per_thread + i) * ThreadBlock / BlockDimX] = c_splited[y][i].x + c_splited[y][i].y;
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < m_per_thread * BlockDimX; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
+
+
+template<int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq_tb256_bx256_kb128(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int ThreadBlock = 256;
+    constexpr int BlockDimX = 256;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    constexpr int k_block = QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr;
+    half *bsm_scales_ptr;
+
+    __shared__ float bsm_ptr[2048];  //128*N+256*4+256*4/2 = 8Kblock 256PEUAP8block
+    bsm_b_ptr = bsm_ptr;
+    bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+    bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+    //smem = bsm_ptr;
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            *((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS) + 1;
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS) + 1;
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[x*2] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[x*2+1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[x*2] = temp_scales[0];
+            bsm_scales_ptr[x*2+1] = temp_scales[1];
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        v2f local_scales_d16[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = (float)bsm_scales_ptr[m_index + c];
+            float s_d16 = s / 16;
+            local_scales[c] = {s, s};
+            local_scales_d16[c] = {s_d16, s_d16};
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c],bsm_zeros_ptr[m_index + c]};
+
+        const int shuffled_dequant_index[PACK_RATIO] = {0, 4, 1, 5, 2, 6, 3, 7};
+
+        #pragma unroll 16
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            v2f local_b[PACK_RATIO/2*N];
+            for (int y = 0; y < N; y++) {
+                for (int x = 0; x < PACK_RATIO / 2; x++) {
+                    local_b[x+PACK_RATIO/2*y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                    local_b[x+PACK_RATIO/2*y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                }
+            }
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                uint32_t p0 = A[c] & 0x0f0f0f0f;
+                uint32_t p1 = A[c] & 0xf0f0f0f0;
+                float o1,o2,o3,o4,o5,o6,o7,o8;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(p0));
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(p0));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(p0));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(p0));
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o5):"r"(p1));
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o6):"r"(p1));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o7):"r"(p1));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o8):"r"(p1));
+                v2f a0 = {o1, o3};
+                v2f a1 = {o5, o7};
+                v2f a2 = {o2, o4};
+                v2f a3 = {o6, o8};
+
+                a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+                a1 = __builtin_mxc_pk_fma_f32(a1, local_scales_d16[c], local_zeros[c]);
+                a2 = __builtin_mxc_pk_fma_f32(a2, local_scales[c], local_zeros[c]);
+                a3 = __builtin_mxc_pk_fma_f32(a3, local_scales_d16[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[0+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[1+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a2, local_b[2+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a3, local_b[3+PACK_RATIO/2*y], c_splited[y][c]);
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    // directly do atomic add, may cause partial write?
+    for (int y = 0; y < N; y++) {
+        for (int c = 0; c < m_per_thread; c++) {
+            atomicAdd(dst + tidRow * m_per_thread + c + y * dstStride, (half)(c_splited[y][c].x + c_splited[y][c].y));
+        }
+    }
+}
+
+
+template<>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq_tb256_bx256_kb128<4>(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm) {
+    constexpr int QBITS = 4;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int ThreadBlock = 256;
+    constexpr int BlockDimX = 256;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = 4;
+    const int k_stride = k;
+    constexpr int k_block = QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements + group_elements / 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr;
+    half *bsm_scales_ptr;
+
+    __shared__ float bsm_ptr[2048];  //128*N+256*4+256*4/2 = 8Kblock 256PEUAP8block
+    bsm_b_ptr = bsm_ptr;
+    bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+    bsm_scales_ptr = (half*)(bsm_zeros_ptr + group_elements);
+    //smem = bsm_ptr;
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 2;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales[2];
+            *((float*)temp_scales) = ((float*)(scales + quant_group * scales_stride + m_offset_scales))[x];
+            uint32_t z = temp_zeros;
+            uint32_t z0 = __builtin_mxc_ubfe(z, 0, QBITS) + 1;
+            uint32_t z1 = __builtin_mxc_ubfe(z, 4, QBITS) + 1;
+            float s1 = (float)(temp_scales[0]);
+            float s2 = (float)(temp_scales[1]);
+            //Store to shared memory
+            bsm_zeros_ptr[x*2] = (float)z0 * s1 * -1.0f;
+            bsm_zeros_ptr[x*2+1] = (float)z1 * s2 * -1.0f;
+            bsm_scales_ptr[x*2] = temp_scales[0];
+            bsm_scales_ptr[x*2+1] = temp_scales[1];
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = (float)bsm_scales_ptr[m_index + c];
+            local_scales[c] = {s,s};
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) local_zeros[c] = {bsm_zeros_ptr[m_index + c], bsm_zeros_ptr[m_index + c]};
+
+        //const int shuffled_dequant_index[PACK_RATIO] = {0, 4, 1, 5, 2, 6, 3, 7};
+
+        #pragma unroll 16
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            //Split dequant and w*b into 4 parts so we can reduce registers usage from 114 to 76, and each peu will run 6 waves
+            v2f local_b[N];
+            uint32_t Aq[m_per_thread];
+            for (int y = 0; y < N; y++) {
+                int x = 0;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                Aq[c] = A[c] & 0x0f0f0f0f;
+                float o1,o3;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(Aq[c]));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(Aq[c]));
+                v2f a0 = {o1, o3};
+
+                a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            for (int y = 0; y < N; y++) {
+                int x = 2;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                float o2,o4;
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(Aq[c]));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(Aq[c]));
+                v2f a2 = {o2, o4};
+
+                a2 = __builtin_mxc_pk_fma_f32(a2, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a2, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            for (int y = 0; y < N; y++) {
+                int x = 1;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+
+            for (int c = 0; c < m_per_thread; c++) {
+                Aq[c] = (A[c] >> 4) & 0x0f0f0f0f;
+                float o5,o7;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o5):"r"(Aq[c]));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o7):"r"(Aq[c]));
+                v2f a1 = {o5, o7};
+                a1 = __builtin_mxc_pk_fma_f32(a1, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            for (int y = 0; y < N; y++) {
+                int x = 3;
+                local_b[y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                local_b[y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+            }
+
+            for (int c = 0; c < m_per_thread; c++) {
+                float o6,o8;
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o6):"r"(Aq[c]));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o8):"r"(Aq[c]));
+                v2f a3 = {o6, o8};
+                a3 = __builtin_mxc_pk_fma_f32(a3, local_scales[c], local_zeros[c]);
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a3, local_b[y], c_splited[y][c]);
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    // directly do atomic add, may cause partial write?
+    for (int y = 0; y < N; y++) {
+        for (int c = 0; c < m_per_thread; c++) {
+            atomicAdd(dst + tidRow * m_per_thread + c + y * dstStride, (half)(c_splited[y][c].x + c_splited[y][c].y));
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp b/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
new file mode 100644
index 000000000..2a178d362
--- /dev/null
+++ b/csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
@@ -0,0 +1,215 @@
+#pragma once
+
+#include <mc_runtime.h>
+#include <maca_fp16.h>
+#include "Hgemm_common.cuh"
+#include "dequant.cuh"
+#define quant_packed_type uint32_t
+typedef __NATIVE_VECTOR__(2, float) v2f;
+template <int ThreadBlock, int BlockDimX, int BATCH>
+__global__ __launch_bounds__(256) void hgemv_nn_splitk_gptq_int8(const half* __restrict__ srcB,
+                                                        const quant_packed_type* __restrict__ srcA,
+                                                        const quant_packed_type* __restrict__ zeros,
+                                                        const half* __restrict__ scales,
+                                                        half *dst,
+                                                        int m,
+                                                        int n,
+                                                        int k,
+                                                        int srcAStride,
+                                                        int dstStride,
+                                                        int k_div_sk,
+                                                        const int* __restrict__ b_perm = nullptr) {
+    constexpr int QBITS = 8;
+    constexpr int PACK_RATIO = (32 / QBITS);
+    constexpr int QUANT_GROUP = 128;
+    constexpr int LoopNum = QUANT_GROUP / (ThreadBlock / BlockDimX);
+    constexpr int N = BATCH;
+    const int k_stride = k;
+    const int k_block = (k_div_sk + QUANT_GROUP - 1) / QUANT_GROUP * QUANT_GROUP;
+    const int splitKOffset = blockIdx.y * k_block;
+    if (splitKOffset + k_block > k) k = k - splitKOffset;
+    else k = k_block;
+    srcA += splitKOffset * srcAStride / PACK_RATIO;
+    //srcB += splitKOffset;
+
+    constexpr int quant_groups = 1;
+    constexpr int m_per_thread = sizeof(float4)/sizeof(quant_packed_type);
+    constexpr int thread_groups = ThreadBlock / BlockDimX;
+    constexpr int group_elements = BlockDimX * m_per_thread;
+    constexpr int reduce_size = ThreadBlock * m_per_thread;
+    constexpr int data_cache_size = LoopNum * thread_groups * N + group_elements * 2;
+    float *bsm_b_ptr, *bsm_zeros_ptr, *bsm_scales_ptr, *smem;
+    if constexpr(reduce_size > data_cache_size) {
+        __shared__ float bsm_ptr[reduce_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    } else {
+        __shared__ float bsm_ptr[data_cache_size];
+        bsm_b_ptr = bsm_ptr;
+        bsm_zeros_ptr = bsm_ptr + LoopNum * thread_groups * N;
+        bsm_scales_ptr = bsm_zeros_ptr + group_elements;
+        smem = bsm_ptr;
+    }
+
+    const int zeros_stride = srcAStride / PACK_RATIO;
+    const int scales_stride = srcAStride;
+    zeros += splitKOffset * zeros_stride / QUANT_GROUP;
+    scales += splitKOffset * scales_stride / QUANT_GROUP;
+
+    dst += group_elements * blockIdx.x;
+    const int m_offset_a = group_elements * blockIdx.x;
+    const int m_offset_zeros = group_elements / PACK_RATIO * blockIdx.x;
+    const int m_offset_scales = group_elements * blockIdx.x;
+
+    //store splited fma results
+    v2f c_splited[N][m_per_thread];
+    for (int i = 0; i < N; i++) {
+        for (int j = 0; j < m_per_thread; j++) c_splited[i][j] = {0, 0};
+    }
+    
+    int tid = threadIdx.x;
+    int tidCol = tid / BlockDimX;
+    int tidRow = tid % BlockDimX;
+
+    for (int i = 0; i < k; i += LoopNum * thread_groups) {
+        int quant_group = i / QUANT_GROUP;
+        constexpr int loading_pack = 1;
+        constexpr int loading_count = group_elements / loading_pack;
+        //Load needed zeros, scales
+        for (int x = tid; x < loading_count; x += ThreadBlock) {
+            uint8_t temp_zeros = 0;
+            temp_zeros = ((uint8_t*)(zeros + quant_group * zeros_stride + m_offset_zeros))[x];
+            half temp_scales;
+            temp_scales = (scales + quant_group * scales_stride + m_offset_scales)[x];
+            uint32_t z = temp_zeros;
+            float z0;
+            asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(z0):"r"(z));
+            float s1 = (float)(temp_scales);
+            //Store to shared memory
+            //bsm_zeros_ptr[x] = (float)z0 * s1 * -1.0f;    //modify 11.19
+	    bsm_zeros_ptr[x] = (float)(z0+1) * s1 * -1.0f;
+            bsm_scales_ptr[x] = s1;
+            // if (i == 0 && blockIdx.x == 0) {
+            //     printf("tid %d, x = %d, temp_zero=%u, temp_scale=%f,  z0 = %f, s1 = %f\n", tid, x, (uint32_t)temp_zeros, (float)temp_scales, z0, s1);
+            // }
+        }
+
+        int loop_index = 0;
+        quant_packed_type A[m_per_thread];
+        //Load A
+        *((float4*)A) = *(float4*)(srcA + (loop_index + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+
+        //Load B and transform to float
+        if (b_perm != nullptr) {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[b_perm[splitKOffset + i + tidCol * LoopNum + x] + y * k_stride];
+                }
+            }
+        } else {
+            for (int y = 0; y < N; y++) {
+                for (int x = tidRow; x < LoopNum; x += BlockDimX) {
+                    bsm_b_ptr[x + tidCol * LoopNum + y * LoopNum * thread_groups] = srcB[splitKOffset + i + tidCol * LoopNum + x + y * k_stride];
+                }
+            }
+        }
+
+        __syncthreads();
+
+        //Load zero and scale from bsm
+        int m_index = tidRow * m_per_thread;
+
+        v2f local_scales[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            float s = bsm_scales_ptr[m_index + c];
+            local_scales[c] = {s, s};
+            // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+            //     printf("get scale %f from index %d\n", s, m_index+c);
+            // }
+        }
+        v2f local_zeros[m_per_thread];
+        for (int c = 0; c < m_per_thread; c++) {
+            local_zeros[c] = {bsm_zeros_ptr[m_index + c],bsm_zeros_ptr[m_index + c]};
+            // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+            //     printf("get zero %f from index %d\n", local_zeros[c].x, m_index+c);
+            // }
+        }
+
+        for (; loop_index < LoopNum; loop_index += PACK_RATIO) {
+            v2f local_b[PACK_RATIO/2*N];
+            for (int y = 0; y < N; y++) {
+                for (int x = 0; x < PACK_RATIO / 2; x++) {
+                    local_b[x+PACK_RATIO/2*y].x = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+y*thread_groups*LoopNum];
+                    local_b[x+PACK_RATIO/2*y].y = bsm_b_ptr[tidCol*LoopNum+loop_index+x*2+1+y*thread_groups*LoopNum];
+                }
+            }
+            #pragma unroll m_per_thread
+            for (int c = 0; c < m_per_thread; c++) {
+                float o1,o2,o3,o4;
+                asm volatile("cvt_b0tof32 %0,%1;\n":"=r"(o1):"r"(A[c]));
+                asm volatile("cvt_b1tof32 %0,%1;\n":"=r"(o2):"r"(A[c]));
+                asm volatile("cvt_b2tof32 %0,%1;\n":"=r"(o3):"r"(A[c]));
+                asm volatile("cvt_b3tof32 %0,%1;\n":"=r"(o4):"r"(A[c]));
+                v2f a0 = {o1, o2};
+                v2f a1 = {o3, o4};
+
+                // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+                //     printf("GPU int8 a0=%f,%f, a1=%f,%f,scale=%f,zero=%f\n",
+                //     a0.x, a0.y, a1.x, a1.y, local_scales[c].x, local_zeros[c].x);
+                // }
+
+                a0 = __builtin_mxc_pk_fma_f32(a0, local_scales[c], local_zeros[c]);
+                a1 = __builtin_mxc_pk_fma_f32(a1, local_scales[c], local_zeros[c]);
+
+                // if (tid == 0 && c == 0 && blockIdx.x == 0 && blockIdx.y == 0) {
+                //     printf("GPU a=%x, a0=%f,%f,a1=%f,%f,b=%f,%f,%f,%f\n",
+                //         A[c], a0.x, a0.y, a1.x, a1.y, local_b[0].x, local_b[0].y, local_b[1].x, local_b[1].y
+                //     );
+                // }
+
+                #pragma unroll N
+                for (int y = 0; y < N; y++) {
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a0, local_b[0+PACK_RATIO/2*y], c_splited[y][c]);
+                    c_splited[y][c] = __builtin_mxc_pk_fma_f32(a1, local_b[1+PACK_RATIO/2*y], c_splited[y][c]);
+                }
+            }
+
+            if constexpr(thread_groups != QUANT_GROUP / PACK_RATIO) {
+                if (loop_index + PACK_RATIO < LoopNum) {
+                    *((float4*)A) = *(float4*)(srcA + (loop_index + PACK_RATIO + i + tidCol * LoopNum) / PACK_RATIO * srcAStride + m_offset_a + m_per_thread * tidRow);
+                }
+            }
+        }
+        __syncthreads();
+    }
+
+    #pragma unroll N
+    for (int y = 0; y < N; y++) {
+        for (int i = 0; i < m_per_thread; i++) {
+            smem[tidCol + (tidRow * m_per_thread + i) * ThreadBlock / BlockDimX] = c_splited[y][i].x + c_splited[y][i].y;
+        }
+        __syncthreads();
+        constexpr int stride = ThreadBlock / BlockDimX;
+        int data_size = ThreadBlock * m_per_thread;
+        #pragma unroll
+        for (int i = ThreadBlock / BlockDimX / 2; i > 0; i /= 2) {
+            for (int j = tid; j < data_size / 2; j += ThreadBlock) {
+                int reduce_group = j / i;
+                int reduce_index = j % i;
+                smem[reduce_index + reduce_group * stride] += smem[reduce_index + reduce_group * stride + i];
+            }
+            __syncthreads();
+            data_size /= 2;
+        }
+        for (int i = tid; i < m_per_thread * BlockDimX; i += ThreadBlock) {
+            atomicAdd(dst + i + y * dstStride, (half)smem[i*stride]);
+        }
+        if constexpr(N > 1) {
+            if (y + 1 < N) {
+                __syncthreads();
+            }
+        }
+    }
+}
diff --git a/csrc/quantization/gptq/hgemv_selector.hpp b/csrc/quantization/gptq/hgemv_selector.hpp
new file mode 100644
index 000000000..b9d12a4e1
--- /dev/null
+++ b/csrc/quantization/gptq/hgemv_selector.hpp
@@ -0,0 +1,287 @@
+#include <memory>
+#include <vector>
+#include <algorithm>
+#include "mc_runtime.h"
+#include "maca_fp16.h"
+
+struct KernelEventRecorder {
+    mcEvent_t _start;
+    mcEvent_t _stop;
+    float _eventMs = -1.f;
+
+    KernelEventRecorder() {
+        mcEventCreate(&_start);
+        mcEventCreate(&_stop);
+    }
+
+    ~KernelEventRecorder() {
+        mcEventDestroy(_start);
+        mcEventDestroy(_stop);
+    }
+
+    void start() {
+        mcEventRecord(_start, NULL);
+    }
+
+    float stop() {
+        mcEventRecord(_stop, NULL);
+        mcEventSynchronize(_stop);
+        mcEventElapsedTime(&_eventMs, _start, _stop);
+        return _eventMs;
+    }
+};
+namespace hgemv_selector {
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+struct GemvParamAutoSelector {
+    int m;
+    int k;
+    int best_block_x = 0;
+    int best_split_k = 0;
+
+    std::pair<int,int> block_x_range;
+    std::pair<int,int> split_k_range;
+    bool _valid = false;
+
+private:
+    std::vector<std::pair<int, int>> param_candidates;
+    int warmup_iters = 0;
+    int current_block_x = 0;
+    int current_split_k = 0;
+    int current_perf_iter = 0;
+    std::vector<float> perf_times;
+    float kernel_best_time_ms_ave = 99999999.0f;
+    float best_band_width;
+    float data_size_gb;
+    std::shared_ptr<KernelEventRecorder> _r;
+    bool _selected = false;
+    const static int MAX_PERF_COUNT = 20;
+
+public:
+    GemvParamAutoSelector(int _m, int _n, int _k, bool allow_imcomplete_bx = false) : m(_m), k(_k)
+    {
+        if (!prepare_parameters(_m, _n, _k, block_x_range.second, split_k_range.second, allow_imcomplete_bx)) return;
+        block_x_range.first = block_x_range.second;
+        split_k_range.first = split_k_range.second;
+        for (int i = 0; i < 3; i++) if (block_x_range.first / 2 >= 16) block_x_range.first /= 2;
+        for (int i = 0; i < 3; i++) if (split_k_range.first % 2 == 0 && split_k_range.first / 2 > 0) split_k_range.first /= 2;
+        for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+            for (int j = split_k_range.first; j <= split_k_range.second; j *= 2) {
+                param_candidates.emplace_back(i, j);
+            }
+        }
+        if (split_k_range.second * quant_group != k) {
+            int max_split_k = k / quant_group;
+            if (max_split_k < 256) {
+                for (int i = block_x_range.first; i <= block_x_range.second; i *= 2) {
+                    param_candidates.emplace_back(i, max_split_k);
+                }
+            }
+        }
+
+        current_block_x = block_x_range.second;
+        current_split_k = split_k_range.second;
+        uint64_t data_size_in_bytes = _n * _k * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group * sizeof(half);
+        data_size_in_bytes += (uint64_t)_m * _k / quant_group / pack_ratio * sizeof(uint32_t);
+        data_size_in_bytes += (uint64_t)_n * _m * sizeof(half);
+        data_size_gb = float(data_size_in_bytes) / 1024 / 1024 / 1024;
+        warmup_iters = 4;
+        _valid = true;
+    }
+
+    void select_in_warmup(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (_selected) {
+            f(best_block_x, best_split_k);
+            return;
+        };
+        //Warmup
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            for (int i = 0; i < 5; i++) {
+                auto &p = *iter;
+                f(p.first, p.second);
+            }
+        }
+        _r.reset(new KernelEventRecorder());
+        kernel_best_time_ms_ave = 9999999.0f;
+        mcDeviceSynchronize();
+
+        for (auto iter = param_candidates.begin(); iter != param_candidates.end(); iter++) {
+            auto &p = *iter;
+            auto &bx = p.first;
+            auto &sk = p.second;
+            mcDeviceSynchronize();
+            _r->start();
+            bool launched = false;
+            for (int i = 0; i < MAX_PERF_COUNT; i++) {
+                launched = f(bx, sk);
+            }
+            auto ms = _r->stop();
+            if (launched && ms / MAX_PERF_COUNT < kernel_best_time_ms_ave) {
+                best_block_x = bx;
+                best_split_k = sk;
+                kernel_best_time_ms_ave = ms / MAX_PERF_COUNT;
+            }
+        }
+
+        _r.reset();
+        _selected = true;
+        warmup_iters = 0;
+    }
+
+    void run(const std::function<bool(int,int)> &f) {
+        if (!valid()) {
+            printf("Cannot run this selector!\n");
+            return;
+        }
+        if (warmup_iters > 0) {
+            f(current_block_x, current_split_k);
+            //printf("Calling warmup current_block_x = %d, current_split_k = %d\n", current_block_x, current_split_k);
+            if (current_block_x > block_x_range.first) current_block_x /= 2;
+            else if (current_split_k > split_k_range.first) current_split_k /= 2;
+            if (current_block_x == block_x_range.first && current_split_k >= split_k_range.first) {
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+            }
+            warmup_iters--;
+            mcDeviceSynchronize();
+            return;
+        }
+
+        if (_selected) {
+            f(best_block_x, best_split_k);
+        } else {
+            if (!_r) {
+                _r = std::shared_ptr<KernelEventRecorder>(new KernelEventRecorder());
+                current_block_x = block_x_range.second;
+                current_split_k = split_k_range.second;
+                current_perf_iter = MAX_PERF_COUNT;
+            }
+            _r->start();
+            auto launched = f(current_block_x, current_split_k);
+            auto ms = _r->stop();
+            //printf("Launched with bx=%d,sk=%d,iter=%d,ms=%fms\n", current_block_x, current_split_k, current_perf_iter, ms);
+            if (!launched) {
+                printf("ERROR: Cannot run gemv with bx = %d, sk = %d\n", current_block_x, current_split_k);
+                return;
+            }
+            if (current_perf_iter-- > 0) {
+                perf_times.emplace_back(ms);
+                return;
+            }
+
+            std::sort(perf_times.begin(), perf_times.end());
+            float total_tm = 0;
+            for (size_t i = 1; i < perf_times.size() - 1; i++) {
+                total_tm += perf_times[i];
+            }
+
+            ms = total_tm /= (MAX_PERF_COUNT - 2);
+            perf_times.clear();
+            current_perf_iter = MAX_PERF_COUNT;
+            //printf("get ave time %fms\n", ms);
+
+            if (ms < kernel_best_time_ms_ave) {
+                best_block_x = current_block_x;
+                best_split_k = current_split_k;
+                kernel_best_time_ms_ave = ms;
+            }
+
+            if (current_split_k > split_k_range.first) {
+                current_split_k /= 2;
+            } else if (current_block_x > block_x_range.first){
+                current_split_k = split_k_range.second;
+                current_block_x /= 2;
+            } else if (current_block_x == block_x_range.first && current_split_k == split_k_range.first) {
+                _selected = true;
+                _r.reset();
+            } else {
+                printf("Error: all parameters are tried! current_block_x = %d, current_split_k = %d, block_x_range = %d,%d, split_k_range = %d,%d, best_block_x = %d, best_split_k = %d\n",
+                    current_block_x, current_split_k,
+                    block_x_range.first, block_x_range.second,
+                    split_k_range.first, split_k_range.second,
+                    best_block_x, best_split_k
+                );
+            }
+        }
+    }
+
+    bool valid() const { return _valid; }
+    bool selected() const {return _selected; }
+
+    float gemv_ave_time_us_cost() {
+        return kernel_best_time_ms_ave;
+    }
+
+    float gemv_bandwidth() {
+        return data_size_gb / (kernel_best_time_ms_ave / 1000.0f);
+    }
+
+    static bool prepare_parameters(int m, int n, int k, int &bx, int &sk, bool allow_imcomplete_bx = false) {
+        if (n > 4) return false;
+        if (k % quant_group != 0) return false;
+        if (m < 16 * m_per_thread) return false;
+        int max_split_k = k / quant_group;
+        int proper_splitk = 1;
+        while (proper_splitk*2 <= 256 && proper_splitk*2 <= max_split_k) proper_splitk *= 2;
+
+        int thread_block = 256;  //A thread block of 512 will wait for more warps when doing syncthreads
+
+        int proper_bx = 16;
+        if (m % (proper_bx * m_per_thread) != 0) return false;
+        //proper_bx * m_per_threads corresponds to group_elements, this value should not be larger than 2048 as limit of shared memory
+        while (proper_bx * 2 * m_per_thread && (m % (proper_bx*2*m_per_thread) == 0) && proper_bx * 2 <= 256 && proper_bx * 2 * m_per_thread <= 2048) proper_bx*=2;
+        if (allow_imcomplete_bx) {
+            int may_proper_bx = proper_bx * 2;
+            if (may_proper_bx <= 256 && may_proper_bx * m_per_thread <= 2048 && (m % (may_proper_bx*m_per_thread)) > may_proper_bx * m_per_thread / 4) {
+                proper_bx = may_proper_bx;
+            }
+        }
+
+        bx = proper_bx;
+        sk = proper_splitk;
+        //printf("m=%d,n=%d,k=%d,get bx=%d,sk=%d,allow_imcomplete_bx=%d\n", m, n, k, bx, sk, allow_imcomplete_bx);
+        return true;
+    }
+};
+
+template<int quant_group=128, int pack_ratio=8, int m_per_thread=4>
+class GemvSelectorHolder {
+private:
+    std::vector<GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>> _selectors;
+    static std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> _holder;
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> _invalid_selector;
+
+public:
+    static GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& selector(int m, int n, int k, bool allow_imcomplete_bx = false) {
+        if (!GemvSelectorHolder::_holder) {
+            GemvSelectorHolder::_holder.reset(new GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>());
+        }
+        int bx, sk;
+        if (!GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>::prepare_parameters(m, n, k, bx, sk, allow_imcomplete_bx)) {
+            return _invalid_selector;
+        }
+        auto iter = std::find_if(_holder->_selectors.begin(), _holder->_selectors.end(),
+            [&](const GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread>& p) -> bool {
+                return p.m == m && p.k == k;
+            });
+        if (iter != _holder->_selectors.end()) return *iter;
+        GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> sl(m, n, k, allow_imcomplete_bx);
+        if (!sl.valid()) {
+            return _invalid_selector;
+        }
+        _holder->_selectors.emplace_back(sl);
+        return _holder->_selectors.back();
+    }
+};
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+std::shared_ptr<GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_holder;
+
+template<int quant_group, int pack_ratio, int m_per_thread>
+GemvParamAutoSelector<quant_group,pack_ratio,m_per_thread> GemvSelectorHolder<quant_group,pack_ratio,m_per_thread>::_invalid_selector(0,8,0);
+}
\ No newline at end of file
diff --git a/csrc/quantization/gptq/q_gemm.cu b/csrc/quantization/gptq/q_gemm.cu
index 6fad16e19..7a2c5eb6a 100644
--- a/csrc/quantization/gptq/q_gemm.cu
+++ b/csrc/quantization/gptq/q_gemm.cu
@@ -19,6 +19,17 @@ https://github.com/qwopqwop200/GPTQ-for-LLaMa
 #include "qdq_4.cuh"
 #include "qdq_8.cuh"
 
+#ifdef USE_MACA
+#include "hgemm_gptq.h"
+#include "scalar_type.hpp"
+
+#include "hgemv_nn_splitk_gptq.hpp"
+#include "hgemv_nn_splitk_gptq_int8.hpp"
+#include "hgemv_selector.hpp"
+#include "Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp"
+#include "Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp"
+#endif // USE_MACA
+
 namespace vllm {
 namespace gptq {
 
@@ -31,6 +42,7 @@ namespace gptq {
 #define THREADS_X 32
 #define THREADS_Y 32
 #define DIVIDE(x, size) (((x) + (size) - 1) / (size))
+#define QUANT_GROUP 128
 
 #if defined(USE_ROCM)
   #include <hipblas/hipblas.h>
@@ -734,26 +746,309 @@ fp_gemm_half_q_half_gptq_kernel pick_gemm_half_q_half_gptq_kernel(
   return NULL;
 }
 
+template <typename T>
+__global__ void blasMemset(T *data, size_t cnt, T init) {
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (cnt + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
+        data[loop * threads + tid] = init;
+    }
+}
+
+template <typename dstT, typename srcT, typename scalarT>
+__global__ void blasMemcpy(dstT *dst, const srcT *src, size_t cnt, scalarT beta) {
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (cnt + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && loop * threads + tid < cnt; ++loop) {
+        dst[loop * threads + tid] =
+            static_cast<double>(beta) * static_cast<double>(src[loop * threads + tid]);
+    }
+}
+
+template <typename reducT, typename outputT, typename scalarT>
+__global__ void blasReduc(outputT *dC_out, outputT *dC_in, reducT *d_acc, int count, int segs, scalarT beta)
+{
+    using accT = float;
+    size_t threads = gridDim.x * blockDim.x;
+    size_t itemsPerThread = (count + threads - 1) / threads;
+    int tid = blockIdx.x * blockDim.x + threadIdx.x;
+    for (size_t loop = 0; loop < itemsPerThread && (loop * threads + tid) < count; ++loop) {
+        accT acc = static_cast<accT>(beta) * static_cast<accT>(dC_in[loop * threads + tid]);
+        for (size_t SEG=0; SEG < segs; ++SEG)
+        {
+            acc += static_cast<accT>(d_acc[SEG * count + loop * threads + tid]);
+        }
+        dC_out[loop * threads + tid] = static_cast<outputT>(acc);
+    }
+}
+
+template<typename T_ACC, typename T_ACC_PACK, typename T, typename T_PACK>
+__global__ void split_reduce(const T_ACC* src, const int row, const int splitk, T* dest) {
+    constexpr int ELEMS = sizeof(T_ACC_PACK)/sizeof(T_ACC);
+    for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < row*sizeof(T_ACC)/sizeof(T_ACC_PACK); i += blockDim.x*gridDim.x) {
+        T_ACC_PACK p0 = ((T_ACC_PACK*)src)[i];
+        T_ACC p0_a[ELEMS];
+        for (int j = 0; j < ELEMS; j++) p0_a[j] = ((T_ACC*)&p0)[j];
+        for (int k = 1; k < splitk; k++) {
+            p0 = ((T_ACC_PACK*)src)[i + row / ELEMS * k];
+            for (int j = 0; j < ELEMS; j++) {
+                p0_a[j] += ((T_ACC*)&p0)[j];
+            }
+        }
+        T dest_pack[ELEMS];
+        for (int j = 0; j < ELEMS; j++) dest_pack[j] = p0_a[j];
+        ((T_PACK*)dest)[i] = *(T_PACK*)dest_pack;
+    }
+}
+
+template <int tileK, int tileN>
+__global__ void perm_b(half *output, const half *input, const int *idx, int k, int n, int ldb) {
+    int tid = threadIdx.x;
+    int row = blockIdx.x * tileK + tid;
+    if (row < k) {
+        int index = idx[row];
+        int col_offset = blockIdx.y * tileN;
+#pragma unroll 1
+        for (int i = 0; (i < tileN) && ((col_offset + i) < n); ++i) {
+            int col = col_offset + i;
+            output[row + ldb * col] = input[index + ldb * col];
+        }
+    }
+}
+
+#define SWITCH_CASE_BATCH(BlockDimX, SplitK, BATCH) \
+    case BATCH: {                                   \
+        CALL_GEMM(BlockDimX, SplitK, BATCH)         \
+        break;                                      \
+    }
+
+#define APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH) \
+    switch(BATCH) {                                 \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 1)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 2)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 3)           \
+        SWITCH_CASE_BATCH(BlockDimX, SplitK, 4)           \
+        default: {                                          \
+            launched = false;                               \
+            printf("ERROR: Unsupported BATCH %d\n", BATCH); \
+            break;                                          \
+        }                                                   \
+    }
+
+#define SWITCH_CASE_BlockDimX(BlockDimX, SplitK, BATCH) \
+    case BlockDimX: {                                   \
+        APPLY_HGEMM_BATCH(BlockDimX, SplitK, BATCH)    \
+        break;                                          \
+    }
+
+#define APPLY_HGEMM(BlockDimX, SplitK, BATCH)           \
+    switch (BlockDimX) {                                \
+        SWITCH_CASE_BlockDimX(16, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(32, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(64, SplitK, BATCH)        \
+        SWITCH_CASE_BlockDimX(128, SplitK, BATCH)       \
+        SWITCH_CASE_BlockDimX(256, SplitK, BATCH)       \
+        default: {                                                  \
+            launched = false;                                       \
+            printf("ERROR: Unsupported BlockDimX %d\n", BlockDimX); \
+            break;                                                  \
+        }                                                           \
+    }
+
+bool call_kernel(const half *srcB,
+    const quant_packed_type *srcA,
+    quant_packed_type *zeros, half *scales,
+    half* dst_D,
+    int m, int n, int k, int srcStride, int dstStride,
+    int block_x, int split_k, int bit,
+    const int* b_perm_D = nullptr) {
+    constexpr int ThreadBlock = 256;
+    const dim3 threadBlock = {static_cast<unsigned int>(ThreadBlock)};
+    const dim3 gridBlock = {static_cast<unsigned int>(m / (block_x * sizeof(float4) / sizeof(quant_packed_type))), static_cast<unsigned int>(split_k)};
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    if (split_k * QUANT_GROUP > k || k % QUANT_GROUP != 0) return false;
+    if (block_x < 16 || n > 4) return false;
+    bool launched = true;
+    #define CALL_GEMM(BX, SK, N) \
+    if (bit == 4) { \
+        if (QUANT_GROUP*SK == k && BX == 256) { \
+            hgemv_nn_splitk_gptq_tb256_bx256_kb128<N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+        } else {    \
+            hgemv_nn_splitk_gptq<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+                srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D);  \
+        } \
+    } \
+    if (bit == 8) { \
+        hgemv_nn_splitk_gptq_int8<256, BX, N><<<gridBlock, threadBlock, 0, stream>>>( \
+            srcB, srcA, zeros, scales, dst_D, m, n, k, srcStride, dstStride, k/SK, b_perm_D); \
+    }
+    APPLY_HGEMM(block_x, split_k, n);
+    return launched;
+}
+
 void gemm_half_q_half_cuda_part(const half* a, const uint32_t* b_q_weight,
                                 const uint32_t* b_gptq_qzeros,
                                 const half* b_gptq_scales, const int* b_q_perm,
                                 half* c, int size_m, int size_n, int size_k,
-                                int m_count, int groups, int bit) {
-  dim3 blockDim, gridDim;
-  blockDim.x = BLOCK_KN_SIZE;
-  blockDim.y = 1;
-  blockDim.z = 1;
-  gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
-  gridDim.y = DIVIDE(size_m, m_count);
-  gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
+                                int m_count, int groups, int bit, bool m_sign, bool v_sign) {
+  if ((bit == 4 || bit == 8) && m_sign && !v_sign){
+        const int threads_n = 256;
+        const int tileM = 128;
+        const int tileN = 32;
+        const int tileK = 128;
+        int lda = size_n;
+        int ldb = size_k;
+        int ldc = size_n;
+
+        int splitk_iters = 3;
+        bool isSplitk = splitk_iters > 1;
+
+        uint32_t gridx = (size_n - 1) / tileM + 1;
+        uint32_t gridy = (size_m - 1) / tileN + 1;
+        uint32_t gridz = splitk_iters;
+
+        uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+        half* scales = const_cast<half*>(b_gptq_scales);
+
+        dim3 dimBlock(threads_n, 1, 1);
+        dim3 dimGrid(gridx, gridy, gridz);
+        float alpha = 1.0, beta = 0.0;
+        const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+        bool isBetaZero = (beta == 0.0);
+        Operation_t trans_a = Operation_t(0);
+        Operation_t trans_b = Operation_t(0);
+
+        if (trans_a == OP_N && trans_b == OP_N && size_n % 8 == 0 && size_k % 8 == 0) {
+             half *dB_perm;
+             if (b_q_perm != nullptr) {
+                 mcMallocAsync((void **)&dB_perm, ldb * size_m * sizeof(input_type), stream);
+                 const int threads_n1 = 128;
+                 const int tileK1 = 128;
+                 const int tileN1 = 8;
+                 uint32_t gridx1 = (size_k - 1) / tileK1 + 1;
+                 uint32_t gridy1 = (size_m - 1) / tileN1 + 1;
+                 dim3 dimBlock1(threads_n1, 1, 1);
+                 dim3 dimGrid1(gridx1, gridy1, 1);
+                 perm_b<tileK1, tileN1><<<dimGrid1, dimBlock1, 0, stream>>>(dB_perm, a, b_q_perm, size_k, size_m, ldb);
+             }
+             const half *dB_actual = (b_q_perm != nullptr ? dB_perm : a);
+	     if (bit == 4) {
+                 if (!isSplitk) {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales);
+                     } else {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, false, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales);
+                     }
+                 } else {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, true, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, c);
+                     }
+                     else {
+                         acc_type *d_acc;
+                         mcMalloc(reinterpret_cast<void **>(&d_acc), size_n * size_m * sizeof(acc_type));
+                         blasMemcpy<<<104, 512, 0, stream>>>(d_acc, c, size_n * size_m, beta);
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_4bit<OP_N, OP_N, false, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, d_acc);
+                         blasMemcpy<<<104, 512, 0, stream>>>(c, d_acc, size_n * size_m, 1);
+                         mcFree(d_acc);
+                     }
+                 }
+             }
+	     else if (bit == 8){
+                 if (!isSplitk) {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, true, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales, 1, nullptr, nullptr);
+                     } else {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, false, tileM, tileN, tileK, true, false>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c,
+                                                                c, ldc, zeros, scales);
+                     }
+                 } else {
+                     if (isBetaZero) {
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, true, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, c);
+                     } else {
+                         acc_type *d_acc;
+                         mcMallocAsync(reinterpret_cast<void **>(&d_acc), size_n * size_m * sizeof(acc_type), stream);
+                         blasMemcpy<<<104, 512, 0, stream>>>(d_acc, c, size_n * size_m, beta);
+                         Hgemm_nn_128x32x128_8m1n8k_gptq_8bit<OP_N, OP_N, false, tileM, tileN, tileK, true, true>
+                             <<<dimGrid, dimBlock, 0, stream>>>(size_n, size_m, size_k, alpha, beta, b_q_weight, lda, dB_actual, ldb, c, c,
+                                                                ldc, zeros, scales, splitk_iters, d_acc);
+                         blasMemcpy<<<104, 512, 0, stream>>>(c, d_acc, size_n * size_m, 1);
+                         mcFreeAsync(d_acc, stream);
+                     }
+                 }
+             }
+             if (b_q_perm != nullptr) {
+                 mcFreeAsync(dB_perm, stream);
+             }
+        } else {
+            printf("Parameters not supported!\n");
+            return;
+        }
+  }
+  else if((bit == 4 || bit == 8) && v_sign){
+         constexpr int m_per_thread = 4;
+         uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+         half* scales = const_cast<half*>(b_gptq_scales);
+         auto kernel_testing = [&](int bx, int sk) -> bool {
+             return call_kernel(a, b_q_weight, zeros, scales, c, size_n, size_m, size_k, size_n, size_n, bx, sk, bit, b_q_perm);
+         };
+         if (bit == 4){
+             auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,8,m_per_thread>::selector(size_n, size_m, size_k);
+             if (sl_warmup.valid()) {
+                 if (!sl_warmup.selected()) {
+                     sl_warmup.select_in_warmup(kernel_testing);
+                     mcMemset(c, 0, size_n * size_m * sizeof(half));
+                     sl_warmup.run(kernel_testing);
+                 } else {
+                     sl_warmup.run(kernel_testing);
+                 }
+             }
+         }
+         else {
+             auto& sl_warmup = hgemv_selector::GemvSelectorHolder<QUANT_GROUP,4,m_per_thread>::selector(size_n, size_m, size_k);
+             if (sl_warmup.valid()) {
+                 if (!sl_warmup.selected()) {
+                     sl_warmup.select_in_warmup(kernel_testing);
+                     mcMemset(c, 0, size_n * size_m * sizeof(half));
+                     sl_warmup.run(kernel_testing);
+                 } else {
+                     sl_warmup.run(kernel_testing);
+                 }
+             }
+         }
+  }
+  else {
+    dim3 blockDim, gridDim;
+    blockDim.x = BLOCK_KN_SIZE;
+    blockDim.y = 1;
+    blockDim.z = 1;
+    gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
+    gridDim.y = DIVIDE(size_m, m_count);
+    gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
 
-  fp_gemm_half_q_half_gptq_kernel kernel =
-      pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
+    fp_gemm_half_q_half_gptq_kernel kernel =
+        pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);
 
-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
-  kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
-                                           b_gptq_scales, c, size_m, size_n,
-                                           size_k, groups, b_q_perm);
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,
+                                            b_gptq_scales, c, size_m, size_n,
+                                            size_k, groups, b_q_perm);
+  }
 }
 
 __global__ void reconstruct_exllama_8bit_kernel(
@@ -1487,56 +1782,339 @@ void reconstruct_gptq(const uint32_t* b_q_weight, const uint32_t* b_gptq_qzeros,
                                            width, groups, out);
 }
 
+
+template <int tileK, int tileM, typename dtype>
+__global__ void perm_a(dtype *output, const dtype *input, const int *idx, int k, int m, int lda) {
+    int tid = threadIdx.x;
+    int row = blockIdx.x * tileK + tid;
+    int col_st = blockIdx.y * tileM;
+    if (row < k) {
+        int index = idx[row];
+        #pragma unroll tileM
+        for (int i = 0; i < tileM; ++i) {
+            int col = col_st + i;
+            if (col < m) {
+                output[row + lda * col] = input[index + lda * col];
+            }
+        }
+    }
+}
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm_gptq(int m,
+                      int n,
+                      int k,
+                      int quant_group,
+                      const input_tp *dA,
+                      int lda,
+                      const quant_packed_tp *dB,
+                      int ldb,
+                      output_tp *dC,
+		                  float *dC_temp,
+                      int ldc,
+                      quant_packed_tp *d_zeros,
+                      input_tp *d_scales,
+                      const cudaStream_t stream,
+                      int chunks = 1) {
+    using namespace hgemm_marlin_gptq;
+    if(n % 16 != 0) {
+        printf("n %% 16 != 0, n = %d\n", n);
+        return false;
+    }
+    if(k % 32 != 0) {
+        printf("k %% 32 != 0, k = %d\n", k);
+        return false;
+    }
+    //const vllm::ScalarTypeId w_type_id = vllm::kU4B8.id();
+    const int THREADS = 256;
+    int BLOCKS_M = div_ceil(m, SLICE_M);
+    if(BLOCKS_M >= MAX_BLOCKS_M && BLOCKS_M % MAX_BLOCKS_M != 0) {
+        printf("Error: input m is error, m = %d, blocks_m = %d\n", m, BLOCKS_M);
+        return false;
+    }
+    if (BLOCKS_M > MAX_BLOCKS_M) BLOCKS_M = MAX_BLOCKS_M;
+    int BLOCKS_N = 8;
+    //int BLOCKS_K = 4;
+    //It is better let TILE_K = quant_group
+    //But if quant_group is too large, a quant_group can be divided into two parts
+    int BLOCKS_K = quant_group / SLICE_K;
+    if (quant_group > 128) BLOCKS_K = 128 / SLICE_K;
+
+    if (BLOCKS_M == 1 || BLOCKS_M == 2) {
+        BLOCKS_N = 16;
+    }
+    const bool HAS_ACT_ORDER = false;
+    //const bool HAS_ZP = false;
+    const bool HAS_ZP = (w_type_id == vllm::kU4.id()) || (w_type_id == vllm::kU8.id());
+    int *g_idx = nullptr;
+    bool HAS_NK_PRED = true;
+    bool HAS_M_PRED = true;
+    //int TILE_N = BLOCKS_N * SLICE_N;
+    //int TILE_K = BLOCKS_K * SLICE_K;
+    //int TILE_M = BLOCKS_M * SLICE_M;
+    if (n % TILE_N == 0 && k % TILE_K == 0) {
+        HAS_NK_PRED = false;
+    }
+    if (m % TILE_M == 0) {
+        HAS_M_PRED = false;
+    }
+
+#define LAUNCH_GPTQ(threads, bm, bn, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    else if (THREADS == threads && BLOCKS_M == bm && BLOCKS_N == bn \
+        && BLOCKS_K == bk  && HAS_ACT_ORDER == has_act_order \
+        && HAS_ZP == has_zp \
+        && HAS_M_PRED == has_m_pred && HAS_NK_PRED == has_nk_pred) { \
+            launch_gemm_gptq_kernel<input_tp, w_type_id, \
+                    threads, bm, bn, bk, has_act_order, has_zp, has_m_pred, has_nk_pred>( \
+                    (const PackTypeInt4*)dA, \
+                    (const PackTypeInt4*)dB, \
+                    (PackTypeInt4*)dC, \
+                    (PackTypeInt4*)dC_temp, \
+                    (const PackTypeInt4*)d_scales, \
+                    (const PackTypeInt4*)d_zeros, \
+                    nullptr, m, n, k, quant_group, chunks,\
+                    stream); \
+    }
+
+#define LAUNCH_GPTQ_K(bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 1, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 2, 16, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 3, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ(256, 4, 8, bk, has_act_order, has_zp, has_nk_pred, has_m_pred)
+
+#define LAUNCH_GPTQ_ZP(has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(1, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(2, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(4, false, has_zp, has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_K(8, false, has_zp, has_nk_pred, has_m_pred)
+
+#define LAUNCH_GPTQ_PRED(has_nk_pred, has_m_pred) \
+    LAUNCH_GPTQ_ZP(false, has_nk_pred, has_m_pred) 
+    //LAUNCH_GPTQ_ZP(true, has_nk_pred, has_m_pred)
+
+    if (false) {
+
+    }
+    LAUNCH_GPTQ_PRED(true, true)
+    LAUNCH_GPTQ_PRED(true, false)
+    LAUNCH_GPTQ_PRED(false, true)
+    LAUNCH_GPTQ_PRED(false, false)
+    else {
+        printf("BLOCKS_M=%d, BLOCKS_N=%d, BLOCKS_k=%d, THREADS=%d, HAS_ACT_ORDER=%d, HAS_ZP=%d, quant_group=%d, HAS_M_PRED=%d, HAS_NK_PRED=%d is not supported\n",
+        BLOCKS_M, BLOCKS_N, BLOCKS_K, THREADS, HAS_ACT_ORDER, HAS_ZP, quant_group, HAS_M_PRED, HAS_NK_PRED);
+        return false;
+    }
+
+    return true;
+}
+
+#ifdef BF16_HIGH_PRECISION
+__global__ void vectorized_elementwise_fp32tobf16(float* input, __maca_bfloat16* output, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        // printf("tid = %d, input = %f, output = %f\n", tid, input[tid], (float)(__maca_bfloat16)input[tid]);
+        *(__maca_bfloat16*)(output+tid) = (__maca_bfloat16)input[tid];
+    }
+}
+#else
+__global__ void vectorized_elementwise_fp16tobf16(__maca_bfloat16* input, int N) {
+    uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;
+    if (tid < N) {
+        input[tid] = (float)(*(half*)(input+tid));
+    }
+}
+#endif
+
+
+template <typename input_tp, const vllm::ScalarTypeId w_type_id, typename output_tp, typename quant_packed_tp>
+bool launch_gemm(int m,
+                int n,
+                int k,
+                int quant_group,
+                const input_tp *dA,
+                int lda,
+                const quant_packed_tp *dB,
+                int ldb,
+                output_tp *dC,
+                float *dC_temp,
+                int ldc,
+                quant_packed_tp *d_zeros,
+                input_tp *d_scales,
+                const int* g_idx,
+                input_tp *perm_space,
+                bool is_gptq = true) {
+    using namespace hgemm_marlin_gptq;
+    //constexpr int max_blocks_m = 4;
+    int total_m_blocks = div_ceil(m, SLICE_M);
+    int chunks = total_m_blocks / MAX_BLOCKS_M;
+    int rest_blocks_m = total_m_blocks % MAX_BLOCKS_M;
+    // printf("m=%d,n=%d,k=%d,lda=%d,ldb=%d,ldc=%d,total_m_blocks=%d,chunks=%d,rest_blocks_m=%d\n",
+    //     m, n, k, lda, ldb, ldc, total_m_blocks, chunks, rest_blocks_m
+    // );
+    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+    //input_tp *dA_perm;
+    if (g_idx != nullptr) {
+        //mcMalloc(reinterpret_cast<void **>(&dA_perm), k * m * sizeof(input_tp));
+        //mcMallocAsync(reinterpret_cast<void **>(&dA_perm), k * m * sizeof(input_tp), stream);
+        const int threads = 256;
+        const int tileK1 = 256;
+        const int tileM1 = 16;
+        uint32_t gridx1 = (k + tileK1 - 1) / tileK1;
+        uint32_t gridy1 = (m + tileM1 - 1) / tileM1;
+        dim3 dimBlock1(threads, 1, 1);
+        dim3 dimGrid1(gridx1, gridy1, 1);
+        perm_a<tileK1, tileM1, input_tp><<<dimGrid1, dimBlock1, 0, stream>>>(perm_space, dA, g_idx, k, m, k);
+    }
+    const input_tp *dA_actual = (g_idx != nullptr ? perm_space : dA);
+    bool ret = true;
+    if (chunks > 0) {
+        int real_m = m > chunks * MAX_BLOCKS_M * SLICE_M ? chunks * MAX_BLOCKS_M * SLICE_M : m;
+        if (is_gptq) {
+            ret = launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(
+                    real_m, n, k, quant_group, 
+                    dA_actual, lda, 
+                    dB, ldb, 
+                    dC, dC_temp, ldc, 
+                    d_zeros, d_scales, stream, chunks);
+        }
+    }
+    if (rest_blocks_m > 0) {
+        int m_offset = chunks * MAX_BLOCKS_M * SLICE_M;
+        if (is_gptq) {
+            ret = ret && launch_gemm_gptq<input_tp, w_type_id, output_tp, quant_packed_tp>(
+                            m - m_offset, n, k, quant_group, 
+                            dA_actual + lda * m_offset, lda, 
+                            dB, ldb, 
+                            dC + ldc * m_offset, dC_temp, ldc, 
+                            d_zeros, d_scales, stream, 1);
+        }
+    }
+
+//#if 0
+    if constexpr(std::is_same_v<input_tp, __maca_bfloat16>) {
+        uint64_t size = m * n;
+        uint64_t block = 512;
+        uint64_t grid = div_ceil(size, block);
+	    vectorized_elementwise_fp32tobf16<<<grid, block, 0, stream>>>((float*)dC_temp, (input_tp*)dC, size);
+    }
+#if 0
+    #ifdef BF16_HIGH_PRECISION
+	  vectorized_elementwise_fp32tobf16<<<grid, block, 0, stream>>>((float*)dC_temp, (input_tp*)dC, size);
+    #else
+          vectorized_elementwise_fp16tobf16<<<grid, block, 0, stream>>>((input_tp*)dC, size);
+    #endif
+#endif
+
+    return ret;
+}
+
+void gemm_bf16_q_bf16_cuda(const __maca_bfloat16* a,
+		           const uint32_t* b_q_weight,
+			   const uint32_t* b_gptq_qzeros,
+			   const __maca_bfloat16* b_gptq_scales, const int* b_g_idx,
+			   __maca_bfloat16* c, float* temp_space, int size_m, int size_n, int size_k,
+			   int bit, int group_size, __maca_bfloat16* perm_space) {
+  bool opt = ((group_size == 128) || (group_size == 64));
+  using scalar_t = __maca_bfloat16;
+  if ((bit == 4) && opt) {
+	  uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+	  scalar_t* scales = const_cast<scalar_t*>(b_gptq_scales);
+	  launch_gemm<scalar_t, vllm::kU4B8.id(), scalar_t, quant_packed_type>(size_m, size_n, size_k,
+			  group_size, a, size_k, b_q_weight, size_n, c, temp_space, size_n, zeros, scales,
+			  b_g_idx, perm_space, true);
+  } else if ((bit == 8) && opt) {
+          uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+          scalar_t* scales = const_cast<scalar_t*>(b_gptq_scales);
+          launch_gemm<scalar_t, vllm::kU8B128.id(), scalar_t, quant_packed_type>(size_m, size_n, size_k,
+                          group_size, a, size_k, b_q_weight, size_n, c, temp_space, size_n, zeros, scales,
+                          b_g_idx, perm_space, true);
+  } else {
+	  printf("Only supported bit-4 , bit-8 of block_size 128 or 64 now!\n");
+  }
+
+}
 void gemm_half_q_half_cuda(cublasHandle_t cublas_handle, const half* a,
                            const uint32_t* b_q_weight,
                            const uint32_t* b_gptq_qzeros,
                            const half* b_gptq_scales, const int* b_g_idx,
                            half* c, half* temp_dq, int size_m, int size_n,
-                           int size_k, int groups, bool use_exllama, int bit) {
+                           int size_k, int groups, bool use_exllama, int bit,
+                           int group_size, half* perm_space) {
   bool use_reconstruct;
-  if (use_exllama) {
-    use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
-                       (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
+  bool opt = ((group_size == 128) || (group_size == 64));
+  if ((bit == 4) && opt) {
+          if ((size_m <= 2) && (group_size == 128)) {
+                  gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
+                                             b_g_idx, c, size_m, size_n, size_k,
+                                             BLOCK_M_SIZE_MAX, groups, bit, true, true);
+          } else {
+                  uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+                  half* scales = const_cast<half*>(b_gptq_scales);
+                  launch_gemm<input_type, vllm::kU4B8.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
+                                  group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
+                                  b_g_idx, perm_space, true);
+          }
+  } else if ((bit == 8) && opt) {
+          uint32_t* zeros = const_cast<uint32_t*>(b_gptq_qzeros);
+          half* scales = const_cast<half*>(b_gptq_scales);
+          launch_gemm<input_type, vllm::kU8B128.id(), output_type, quant_packed_type>(size_m, size_n, size_k,
+                          group_size, a, size_k, b_q_weight, size_n, c, nullptr, size_n, zeros, scales,
+                          b_g_idx, perm_space, true);
   } else {
-    // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
-    // we disabled them for now.
-    use_reconstruct = (bit < 4 || size_m > MAX_ALT_GEMM_ROWS);
-  }
-  if (use_reconstruct) {
-    // Reconstruct FP16 matrix, then cuBLAS
     if (use_exllama) {
-      reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                          temp_dq, size_k, size_n, groups, bit);
+      use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||
+                        (bit != 8 && size_m > MAX_Q_GEMM_ROWS));
     } else {
-      reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                       temp_dq, size_k, size_n, groups, bit);
+      // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so
+      // we disabled them for now.
+      use_reconstruct = (bit < 4 || size_m > 0);
     }
+    if (use_reconstruct) {
+      // Reconstruct FP16 matrix, then cuBLAS
+      if (use_exllama) {
+        reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+                            temp_dq, size_k, size_n, groups, bit);
+      } else {
+        reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+                        temp_dq, size_k, size_n, groups, bit);
+      }
 
-    const half alpha = __float2half(1.0f);
-    const half beta = __float2half(0.0f);
-    cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
-                &alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);
-  } else if (use_exllama) {
-    // Quantized matmul
-    int max_chunks = size_m / BLOCK_M_SIZE_MAX;
-    int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
-    int last_chunk_size = size_m - last_chunk;
-
-    if (max_chunks) {
-      gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
-                                 b_g_idx, c, last_chunk, size_n, size_k,
-                                 BLOCK_M_SIZE_MAX, groups, bit);
-    }
+      const half alpha = __float2half(1.0f);
+      const half beta = __float2half(0.0f);
+      cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,
+                  &alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);
+    } else if (use_exllama) {
+      // Quantized matmul
+      int max_chunks = size_m / BLOCK_M_SIZE_MAX;
+      int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
+      int last_chunk_size = size_m - last_chunk;
+
+      bool m_sign;
+            bool v_sign;
+            if (group_size == 128) {
+                    m_sign = size_m <= 50;
+                    v_sign = size_m <= 4;
+            } else {
+                    m_sign = false;
+                    v_sign = false;
+            }
+
+      if (max_chunks) {
+        gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
+                                  b_g_idx, c, last_chunk, size_n, size_k,
+                                  BLOCK_M_SIZE_MAX, groups, bit, m_sign, v_sign);
+      }
 
-    if (last_chunk_size) {
-      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
-                                 b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                                 c + last_chunk * size_n, last_chunk_size,
-                                 size_n, size_k, last_chunk_size, groups, bit);
+      if (last_chunk_size) {
+        gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,
+                                  b_gptq_qzeros, b_gptq_scales, b_g_idx,
+                                  c + last_chunk * size_n, last_chunk_size,
+                                  size_n, size_k, last_chunk_size, groups, bit,  m_sign, v_sign);
+      }
+    } else {
+      gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
+                          c, size_m, size_n, size_k, bit);
     }
-  } else {
-    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,
-                         c, size_m, size_n, size_k, bit);
   }
 }
 
@@ -1823,25 +2401,45 @@ void shuffle_exllama_weight(uint32_t* q_weight, int* q_perm, int height,
 torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,
                         torch::Tensor b_gptq_qzeros,
                         torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,
-                        bool use_exllama, int64_t bit) {
+                        bool use_exllama, int64_t bit, int64_t group_size,
+                        torch::Tensor perm_space, torch::Tensor temp_space,
+                        bool dtype_bf16) {
   const at::cuda::OptionalCUDAGuard device_guard(device_of(a));
   auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());
-  at::Tensor c = torch::empty({a.size(0), b_q_weight.size(1)}, options);
+  at::Tensor c = torch::zeros({a.size(0), b_q_weight.size(1)}, options);
   at::Tensor temp_dq = torch::empty(
       {b_q_weight.size(0) * 32 / bit, b_q_weight.size(1)}, options);
 
-  vllm::gptq::gemm_half_q_half_cuda(
-      at::cuda::getCurrentCUDABlasHandle(), (const half*)a.data_ptr(),
-      (const uint32_t*)b_q_weight.data_ptr(),
-      (const uint32_t*)b_gptq_qzeros.data_ptr(),
-      (const half*)b_gptq_scales.data_ptr(),
-      b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),
-      (half*)c.data_ptr(), (half*)temp_dq.data_ptr(),
-      c.size(0),              // m
-      c.size(1),              // n
-      a.size(1),              // k
-      b_gptq_qzeros.size(0),  // group number
-      use_exllama, bit);
+  if (dtype_bf16) {
+      vllm::gptq::gemm_bf16_q_bf16_cuda(
+        (const __maca_bfloat16*)a.data_ptr(),
+        (const uint32_t*)b_q_weight.data_ptr(),
+        (const uint32_t*)b_gptq_qzeros.data_ptr(),
+        (const __maca_bfloat16*)b_gptq_scales.data_ptr(),
+        b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),
+        (__maca_bfloat16*)c.data_ptr(),
+	(float*)temp_space.data_ptr(),
+        c.size(0),              // m
+        c.size(1),              // n
+        a.size(1),              // k
+        bit, group_size,
+        (__maca_bfloat16*)perm_space.data_ptr());
+  } else {
+    vllm::gptq::gemm_half_q_half_cuda(
+        at::cuda::getCurrentCUDABlasHandle(), (const half*)a.data_ptr(),
+        (const uint32_t*)b_q_weight.data_ptr(),
+        (const uint32_t*)b_gptq_qzeros.data_ptr(),
+        (const half*)b_gptq_scales.data_ptr(),
+        b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),
+        (half*)c.data_ptr(), (half*)temp_dq.data_ptr(),
+        c.size(0),              // m
+        c.size(1),              // n
+        a.size(1),              // k
+        b_gptq_qzeros.size(0),  // group number
+        use_exllama, bit, group_size,
+        (half*)perm_space.data_ptr());
+  }
+
   return c;
 }
 
diff --git a/csrc/quantization/gptq/qdq_4.cuh b/csrc/quantization/gptq/qdq_4.cuh
index 7f65d2d28..6462584bc 100644
--- a/csrc/quantization/gptq/qdq_4.cuh
+++ b/csrc/quantization/gptq/qdq_4.cuh
@@ -85,7 +85,7 @@ __forceinline__ __device__ void dequant_4bit_8_prep_zero(const uint32_t zero,
   y1y16[0] = __half2half2(y1);
   y1y16[1] = __half2half2(y16);
 }
-
+typedef __NATIVE_VECTOR__(2, float) v2f;
 __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
                                                     half2 (&dq)[4],
                                                     half2 (&z1z16)[2],
@@ -112,12 +112,50 @@ __forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,
     dq[2] = __hfma2(q2.as_half2, y1y16[0], z1z16[0]);
     dq[3] = __hfma2(q3.as_half2, y1y16[1], z1z16[1]);
   } else {
+#ifndef USE_MACA
     dq[0] = __hadd2(q0.as_half2, z1z16[0]);  // half2( q[0] - z, q[1] - z )
     dq[1] = __hfma2(q1.as_half2, y1y16[1],
                     z1z16[1]);               // half2( q[2] - z, q[3] - z )
     dq[2] = __hadd2(q2.as_half2, z1z16[0]);  // half2( q[4] - z, q[5] - z )
     dq[3] = __hfma2(q3.as_half2, y1y16[1],
                     z1z16[1]);  // half2( q[6] - z, q[7] - z )
+#endif // USE_MACA
+#if 1
+    dq[0] = __hadd2(q0.as_half2, z1z16[0]);
+    dq[2] = __hadd2(q2.as_half2, z1z16[0]);
+    v2f q1_float2, q3_float2, y1y16_1_float2, z1z16_1_float2;
+    q1_float2[0] = __half2float(__low2half(q1.as_half2));
+    q1_float2[1] = __half2float(__high2half(q1.as_half2));
+
+    q3_float2[0] = __half2float(__low2half(q3.as_half2));
+    q3_float2[1] = __half2float(__high2half(q3.as_half2));
+
+    y1y16_1_float2[0] = __half2float(__low2half(y1y16[1]));
+    y1y16_1_float2[1] = __half2float(__high2half(y1y16[1]));
+
+    z1z16_1_float2[0] = __half2float(__low2half(z1z16[1]));
+    z1z16_1_float2[1] = __half2float(__high2half(z1z16[1]));
+    v2f result1, result3;
+    result1 = __builtin_mxc_pk_fma_f32(q1_float2, y1y16_1_float2, z1z16_1_float2);
+    result3 = __builtin_mxc_pk_fma_f32(q3_float2, y1y16_1_float2, z1z16_1_float2);
+    dq[1] = __floats2half2_rn(result1[0], result1[1]);
+    dq[3] = __floats2half2_rn(result3[0], result3[1]);
+#else
+    dq[0] = __hadd2(q0.as_half2, z1z16[0]);
+    dq[2] = __hadd2(q2.as_half2, z1z16[0]);
+
+    v2f q1_float2, q3_float2, y1y16_1_float2, z1z16_1_float2;
+    q1_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&q1.as_half2));
+    q3_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&q3.as_half2));
+    y1y16_1_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&y1y16[1]));
+    z1z16_1_float2 = __builtin_mxc_cvt_pk_f16tof32(*reinterpret_cast<const float*>(&z1z16[1]));
+
+    v2f result1 = __builtin_mxc_pk_fma_f32(q1_float2, y1y16_1_float2, z1z16_1_float2);
+    v2f result3 = __builtin_mxc_pk_fma_f32(q3_float2, y1y16_1_float2, z1z16_1_float2);
+
+    dq[1] = __floats2half2_rn(result1[0], result1[1]);
+    dq[3] = __floats2half2_rn(result3[0], result3[1]);
+#endif
   }
 }
 }  // namespace gptq
diff --git a/csrc/quantization/gptq/scalar_type.hpp b/csrc/quantization/gptq/scalar_type.hpp
new file mode 100644
index 000000000..46596ff49
--- /dev/null
+++ b/csrc/quantization/gptq/scalar_type.hpp
@@ -0,0 +1,551 @@
+#pragma once
+
+#include <variant>
+#include <tuple>
+//#include <torch/custom_class.h>
+
+namespace vllm {
+
+//
+//  ScalarType can represent a wide range of floating point and integer types,
+//  in particular it can be used to represent sub-byte data types (something
+//  that torch.dtype currently does not support).
+//
+//  ScalarTypeTorch is a subclass of ScalarType that is compatible with
+//  TORCH_LIBRARY, making it accessible from Python as well meaning this class
+//  can be used as a argument for custom operators, helping to simplify these
+//  interfaces.
+//
+//  The type definitions on the Python side can be found in: vllm/_core_ext.pyi
+//  these type definitions should be kept up to date with any Python API changes
+//  here.
+//
+class ScalarType {
+ public:
+  enum NanRepr : uint8_t {
+    NAN_NONE = 0,                // nans are not supported
+    NAN_IEEE_754 = 1,            // nans are: exp all 1s, mantissa not all 0s
+    NAN_EXTD_RANGE_MAX_MIN = 2,  // nans are: exp all 1s, mantissa all 1s
+
+    NAN_REPR_ID_MAX
+  };
+
+  constexpr ScalarType(uint8_t exponent, uint8_t mantissa, bool signed_,
+                       int32_t bias, bool finite_values_only = false,
+                       NanRepr nan_repr = NAN_IEEE_754)
+      : exponent(exponent),
+        mantissa(mantissa),
+        signed_(signed_),
+        bias(bias),
+        finite_values_only(finite_values_only),
+        nan_repr(nan_repr){};
+
+  static constexpr ScalarType int_(uint8_t size_bits, int32_t bias = 0) {
+    return ScalarType(0, size_bits - 1, true, bias);
+  }
+
+  static constexpr ScalarType uint(uint8_t size_bits, int32_t bias = 0) {
+    return ScalarType(0, size_bits, false, bias);
+  }
+
+  // IEEE 754 compliant floating point type
+  static constexpr ScalarType float_IEEE754(uint8_t exponent,
+                                            uint8_t mantissa) {
+    //TORCH_CHECK(mantissa > 0 && exponent > 0);
+    return ScalarType(exponent, mantissa, true, 0, false, NAN_IEEE_754);
+  }
+
+  // IEEE 754 non-compliant floating point type
+  static constexpr ScalarType float_(uint8_t exponent, uint8_t mantissa,
+                                     bool finite_values_only,
+                                     NanRepr nan_repr) {
+    //TORCH_CHECK(nan_repr < NAN_REPR_ID_MAX, "Invalid NanRepr");
+    //TORCH_CHECK(mantissa > 0 && exponent > 0);
+    //TORCH_CHECK(nan_repr != NAN_IEEE_754,
+    //            "use `float_IEEE754` constructor for floating point types that "
+    //            "follow IEEE 754 conventions");
+    return ScalarType(exponent, mantissa, true, 0, finite_values_only,
+                      nan_repr);
+  }
+
+  uint8_t const exponent;  // size of the exponent field (0 for integer types)
+  uint8_t const mantissa;  // size of the mantissa field (size of the integer
+                           // excluding the sign bit for integer types)
+  bool const signed_;  // flag if the type supports negative numbers (i.e. has a
+                       // sign bit)
+  int32_t const bias;  // stored values equal value + bias,
+                       // used for quantized type
+
+  // Extra Floating point info
+  bool const finite_values_only;  // i.e. no +/-inf if true
+  NanRepr const nan_repr;         // how NaNs are represented
+                                  // (not applicable for integer types)
+
+  using Id = int64_t;
+
+ private:
+  // Field size in id
+  template <typename T_>
+  static constexpr size_t member_id_field_width() {
+    using T = std::decay_t<T_>;
+    return std::is_same_v<T, bool> ? 1 : sizeof(T) * 8;
+  }
+
+  template <typename Fn, typename Init, typename Member, typename... Rest>
+  static constexpr auto reduce_members_helper(Fn f, Init val, Member member,
+                                              Rest... rest) {
+    auto new_val = f(val, member);
+    if constexpr (sizeof...(rest) > 0) {
+      return reduce_members_helper(f, new_val, rest...);
+    } else {
+      return new_val;
+    };
+  }
+
+  template <typename Fn, typename Init>
+  constexpr auto reduce_members(Fn f, Init init) const {
+    // Should be in constructor order for `from_id`
+    return reduce_members_helper(f, init, exponent, mantissa, signed_, bias,
+                                 finite_values_only, nan_repr);
+  };
+
+  template <typename Fn, typename Init>
+  static constexpr auto reduce_member_types(Fn f, Init init) {
+    constexpr auto dummy_type = ScalarType(0, 0, false, 0, false, NAN_NONE);
+    return dummy_type.reduce_members(f, init);
+  };
+
+  static constexpr auto id_size_bits() {
+    return reduce_member_types(
+        [](int acc, auto member) -> int {
+          return acc + member_id_field_width<decltype(member)>();
+        },
+        0);
+  }
+
+ public:
+  // unique id for this scalar type that can be computed at compile time for
+  //  c++17 template specialization this is not needed once we migrate to
+  //  c++20 and can pass literal classes as template parameters
+  constexpr Id id() const {
+    static_assert(id_size_bits() <= sizeof(Id) * 8,
+                  "ScalarType id is too large to be stored");
+
+    auto or_and_advance = [](std::pair<Id, uint32_t> result,
+                             auto member) -> std::pair<Id, uint32_t> {
+      auto [id, bit_offset] = result;
+      auto constexpr bits = member_id_field_width<decltype(member)>();
+      return {id | (int64_t(member) & ((uint64_t(1) << bits) - 1))
+                       << bit_offset,
+              bit_offset + bits};
+    };
+    return reduce_members(or_and_advance, std::pair<Id, uint32_t>{}).first;
+  }
+
+  // create a ScalarType from an id, for c++17 template specialization,
+  //  this is not needed once we migrate to c++20 and can pass literal
+  //  classes as template parameters
+  static constexpr ScalarType from_id(Id id) {
+    auto extract_and_advance = [id](auto result, auto member) {
+      using T = decltype(member);
+      auto [tuple, bit_offset] = result;
+      auto constexpr bits = member_id_field_width<T>();
+      auto extracted_val = static_cast<T>((int64_t(id) >> bit_offset) &
+                                          ((uint64_t(1) << bits) - 1));
+      auto new_tuple = std::tuple_cat(tuple, std::make_tuple(extracted_val));
+      return std::pair<decltype(new_tuple), int>{new_tuple, bit_offset + bits};
+    };
+
+    auto [tuple_args, _] = reduce_member_types(extract_and_advance,
+                                               std::pair<std::tuple<>, int>{});
+    return std::apply([](auto... args) { return ScalarType(args...); },
+                      tuple_args);
+  }
+
+  constexpr int64_t size_bits() const {
+    return mantissa + exponent + is_signed();
+  }
+  constexpr bool is_signed() const { return signed_; }
+  constexpr bool is_integer() const { return exponent == 0; }
+  constexpr bool is_floating_point() const { return exponent > 0; }
+  constexpr bool is_ieee_754() const {
+    return is_floating_point() && finite_values_only == false &&
+           nan_repr == NAN_IEEE_754;
+  }
+  constexpr bool has_nans() const {
+    return is_floating_point() && nan_repr != NAN_NONE;
+  }
+  constexpr bool has_infs() const {
+    return is_floating_point() && finite_values_only == false;
+  }
+  constexpr bool has_bias() const { return bias != 0; }
+
+ private:
+  double _floating_point_max() const {
+    //TORCH_CHECK(mantissa <= 52 && exponent <= 11,
+    //            "Cannot represent max/min as a double for type ", str());
+
+    uint64_t max_mantissa = (uint64_t(1) << mantissa) - 1;
+    if (nan_repr == NAN_EXTD_RANGE_MAX_MIN) {
+      max_mantissa -= 1;
+    }
+
+    uint64_t max_exponent = (uint64_t(1) << exponent) - 2;
+    if (nan_repr == NAN_EXTD_RANGE_MAX_MIN || nan_repr == NAN_NONE) {
+      //TORCH_CHECK(exponent < 11,
+      //            "Cannot represent max/min as a double for type ", str());
+      max_exponent += 1;
+    }
+
+    // adjust the exponent to match that of a double
+    //  for now we assume the exponent bias is the standard 2^(e-1) -1, (where e
+    //  is the exponent bits), there is some precedent for non-standard biases,
+    //  example `float8_e4m3b11fnuz` here: https://github.com/jax-ml/ml_dtypes
+    //  but to avoid premature over complication we are just assuming the
+    //  standard exponent bias until there is a need to support non-standard
+    //  biases
+    uint64_t exponent_bias = (uint64_t(1) << (exponent - 1)) - 1;
+    uint64_t exponent_bias_double = (uint64_t(1) << 10) - 1;  // double e = 11
+
+    uint64_t max_exponent_double =
+        max_exponent - exponent_bias + exponent_bias_double;
+
+    // shift the mantissa into the position for a double and
+    // the exponent
+    uint64_t double_raw =
+        (max_mantissa << (52 - mantissa)) | (max_exponent_double << 52);
+
+    return *reinterpret_cast<double*>(&double_raw);
+  }
+
+  constexpr std::variant<int64_t, double> _raw_max() const {
+    if (is_floating_point()) {
+      return {_floating_point_max()};
+    } else {
+      //TORCH_CHECK(size_bits() < 64 || size_bits() == 64 && is_signed(),
+      //            "Cannot represent max as a int64_t");
+      return {(int64_t(1) << mantissa) - 1};
+    }
+  }
+
+  constexpr std::variant<int64_t, double> _raw_min() const {
+    if (is_floating_point()) {
+      //TORCH_CHECK(is_signed(),
+      //            "We currently assume all floating point types are signed");
+      constexpr uint64_t sign_bit_double = (uint64_t(1) << 63);
+
+      double max = _floating_point_max();
+      uint64_t max_raw = *reinterpret_cast<uint64_t*>(&max);
+      uint64_t min_raw = max_raw | sign_bit_double;
+      return {*reinterpret_cast<double*>(&min_raw)};
+    } else {
+      //TORCH_CHECK(!is_signed() || size_bits() <= 64,
+      //            "Cannot represent min as a int64_t");
+      if (is_signed()) {
+        // set the top bit to 1 (i.e. INT64_MIN) and the rest to 0
+        // then perform an arithmetic shift right to set all the bits above
+        // (size_bits() - 1) to 1
+        return {INT64_MIN >> (64 - size_bits())};
+      } else {
+        return {int64_t(0)};
+      }
+    }
+  }
+
+ public:
+  // Max representable value for this scalar type.
+  // (accounting for bias if there is one)
+  constexpr std::variant<int64_t, double> max() const {
+    return std::visit(
+        [this](auto x) -> std::variant<int64_t, double> { return {x - bias}; },
+        _raw_max());
+  }
+
+  // Min representable value for this scalar type.
+  // (accounting for bias if there is one)
+  constexpr std::variant<int64_t, double> min() const {
+    return std::visit(
+        [this](auto x) -> std::variant<int64_t, double> { return {x - bias}; },
+        _raw_min());
+  }
+
+  std::string str() const {
+    /* naming generally follows: https://github.com/jax-ml/ml_dtypes
+     * for floating point types (leading f) the scheme is:
+     *  `float<size_bits>_e<exponent_bits>m<mantissa_bits>[flags]`
+     *  flags:
+     *  - no-flags: means it follows IEEE 754 conventions
+     *  - f: means finite values only (no infinities)
+     *  - n: means nans are supported (non-standard encoding)
+     * for integer types the scheme is:
+     *  `[u]int<size_bits>[b<bias>]`
+     *  - if bias is not present it means its zero
+     */
+    if (is_floating_point()) {
+      auto ret = "float" + std::to_string(size_bits()) + "_e" +
+                 std::to_string(exponent) + "m" + std::to_string(mantissa);
+      if (!is_ieee_754()) {
+        if (finite_values_only) {
+          ret += "f";
+        }
+        if (nan_repr != NAN_NONE) {
+          ret += "n";
+        }
+      }
+      return ret;
+    } else {
+      auto ret = ((is_signed()) ? "int" : "uint") + std::to_string(size_bits());
+      if (has_bias()) {
+        ret += "b" + std::to_string(bias);
+      }
+      return ret;
+    }
+  }
+
+  constexpr bool operator==(ScalarType const& other) const {
+    return mantissa == other.mantissa && exponent == other.exponent &&
+           bias == other.bias && signed_ == other.signed_ &&
+           finite_values_only == other.finite_values_only &&
+           nan_repr == other.nan_repr;
+  }
+};
+#if 0
+// Create a TORCH_LIBRARY compatible version of ScalarType (i.e. inherit from
+//  torch::CustomClassHolder), we use multiple inheritance here since we cannot
+//  have ScalarType inherit from torch::CustomClassHolder and have a constexpr
+//  constructor at the same time (torch::CustomClassHolder does not have a
+//  constexpr destructor)
+// See also:
+// https://docs.google.com/document/d/18fBMPuOJ0fY5ZQ6YyrHUppw9FA332CpNtgB6SOIgyuA
+class ScalarTypeTorch : public torch::CustomClassHolder, public ScalarType {
+ public:
+  ScalarTypeTorch(int64_t exponent, int64_t mantissa, int64_t bias,
+                  bool _signed)
+      : ScalarType(exponent, mantissa, bias, _signed){};
+
+  ScalarTypeTorch(ScalarType type) : ScalarType(type){};
+
+  using Base = ScalarType;
+  using Self = ScalarTypeTorch;
+  using SelfPtr = c10::intrusive_ptr<Self>;
+
+  static void check_size_bits(int64_t size_bits, bool signed_) {
+    TORCH_CHECK(
+        size_bits <=
+            std::numeric_limits<decltype(std::declval<Self>().mantissa)>::max(),
+        "size_bits bit width is too large to be represented");
+  }
+
+  static void check_bias(int64_t bias) {
+    using Bias = decltype(std::declval<Self>().bias);
+    TORCH_CHECK(bias <= std::numeric_limits<Bias>::max() &&
+                    bias >= std::numeric_limits<Bias>::min(),
+                "bias too large or small to be represented");
+  }
+
+  static void check_exponent(int64_t exponent) {
+    TORCH_CHECK(
+        exponent <=
+            std::numeric_limits<decltype(std::declval<Self>().exponent)>::max(),
+        "exponent bit width is too large to be represented");
+  }
+
+  static void check_mantissa(int64_t mantissa) {
+    TORCH_CHECK(
+        mantissa <=
+            std::numeric_limits<decltype(std::declval<Self>().mantissa)>::max(),
+        "mantissa bit width is too large to be represented");
+  }
+
+  static SelfPtr int_(int64_t size_bits, c10::optional<int64_t> bias) {
+    check_size_bits(size_bits, true);
+    check_bias(bias.value_or(0));
+    return c10::make_intrusive<Self>(
+        ScalarType::int_(size_bits, bias.value_or(0)));
+  }
+
+  static SelfPtr uint(int64_t size_bits, c10::optional<int64_t> bias) {
+    check_size_bits(size_bits, true);
+    check_bias(bias.value_or(0));
+    return c10::make_intrusive<Self>(
+        ScalarType::uint(size_bits, bias.value_or(0)));
+  }
+
+  static SelfPtr float_IEEE754(int64_t exponent, int64_t mantissa) {
+    check_mantissa(mantissa);
+    check_exponent(exponent);
+    return c10::make_intrusive<Self>(
+        ScalarType::float_IEEE754(exponent, mantissa));
+  }
+
+  static SelfPtr float_(int64_t exponent, int64_t mantissa,
+                        bool finite_values_only, int64_t nan_repr) {
+    check_mantissa(mantissa);
+    check_exponent(exponent);
+    return c10::make_intrusive<Self>(ScalarType::float_(
+        exponent, mantissa, finite_values_only, NanRepr(nan_repr)));
+  }
+
+  // This needs to be implemented and throw a TypeError in order for
+  // PyTorch's opcheck to work on ops that use ScalarTypes.
+  int64_t len() const {
+    throw c10::TypeError({__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
+                         "__len__ not implemented");
+    return 0;
+  }
+
+  // Serialize a ScalarType into a tuple of pairs.  Where each pair
+  // is a (fieldname, value).
+  // For simplicity, we are just going to convert to a ScalarTypeId.
+  std::tuple<std::tuple<std::string, int64_t>> obj_flatten() const {
+    return {{"ScalarType", id()}};
+  }
+
+  // Deserialize a scalar type that has been serialized by obj_flatten,
+  // ostensibly from a tuple of (member name, value) pairs, but in reality
+  // just a ScalarTypeId.
+  static SelfPtr obj_unflatten(
+      std::tuple<std::tuple<std::string, int64_t>> const& flat_type) {
+    return c10::make_intrusive<Self>(
+        from_id(std::get<1>(std::get<0>(flat_type))));
+  }
+
+  template <typename T>
+  static void bind_readonly_property(torch::class_<Self>& cls,
+                                     std::string const& name, T Base::*field) {
+    auto getter_func_helper = [field = std::move(field)](SelfPtr const& self) {
+      if constexpr (std::is_member_function_pointer_v<decltype(field)>) {
+        return (self.get()->*field)();
+      } else {
+        return self.get()->*field;
+      }
+    };
+
+    auto getter_func = [field = std::move(field),
+                        getter_func_helper = std::move(getter_func_helper)](
+                           SelfPtr const& self) {
+      auto val = getter_func_helper(self);
+      // upconvert uint8_t, int32_t etc. to int64_t for python
+      if constexpr (std::is_integral_v<T>) {
+        return static_cast<int64_t>(val);
+      } else {
+        return val;
+      }
+    };
+
+    cls.def_property(name, getter_func);
+  }
+
+  template <typename MemberFunc, typename Cls>
+  static void bind_function(torch::class_<Self>& cls, const std::string& name,
+                            MemberFunc Cls::*member) {
+    cls.def(name, [member = std::move(member)](SelfPtr const& self) {
+      return (self.get()->*member)();
+    });
+  }
+
+  template <typename Func>
+  static void bind_function(torch::class_<Self>& cls, const std::string& name,
+                            Func func) {
+    cls.def(name, func);
+  }
+
+  template <typename Func>
+  static void bind_static_function(torch::class_<Self>& cls,
+                                   const std::string& name, Func func) {
+    cls.def_static(name, func);
+  }
+
+  static void bind_class(torch::Library& lib) {
+    auto cls = lib.class_<ScalarTypeTorch>("ScalarType")
+                   .def(torch::init<int64_t, int64_t, int64_t, bool>());
+
+    // Bind Properties
+    bind_readonly_property(cls, "mantissa", &Base::mantissa);
+    bind_readonly_property(cls, "exponent", &Base::exponent);
+    bind_readonly_property(cls, "bias", &Base::bias);
+    bind_readonly_property(cls, "signed", &Base::is_signed);
+    bind_readonly_property(cls, "size_bits", &Base::size_bits);
+
+    // Bind member functions
+    bind_function(cls, "is_signed", &Base::is_signed);
+    bind_function(cls, "is_integer", &Base::is_integer);
+    bind_function(cls, "is_floating_point", &Base::is_floating_point);
+    bind_function(cls, "is_ieee_754", &Base::is_ieee_754);
+    bind_function(cls, "has_nans", &Base::has_nans);
+    bind_function(cls, "has_infs", &Base::has_infs);
+    bind_function(cls, "has_bias", &Base::has_bias);
+
+    bind_function(cls, "max", [](SelfPtr const& self) {
+      return std::visit([](auto arg) { return c10::IValue(arg); },
+                        self.get()->max());
+    });
+    bind_function(cls, "min", [](SelfPtr const& self) {
+      return std::visit([](auto arg) { return c10::IValue(arg); },
+                        self.get()->min());
+    });
+
+    bind_function(cls, "__len__", &ScalarTypeTorch::len);
+    bind_function(cls, "__str__", &Base::str);
+    bind_function(cls, "__eq__", [](SelfPtr const& self, SelfPtr const& other) {
+      return *self == *other;
+    });
+    bind_function(cls, "__repr__", [](SelfPtr const& self) {
+      return "ScalarType." + self.get()->str();
+    });
+
+    bind_function(cls, "__obj_flatten__", &ScalarTypeTorch::obj_flatten);
+    bind_static_function(cls, "__obj_unflatten__",
+                         &ScalarTypeTorch::obj_unflatten);
+
+    // Bind static functions (convenience constructors)
+    bind_static_function(cls, "int_", &ScalarTypeTorch::int_);
+    bind_static_function(cls, "uint", &ScalarTypeTorch::uint);
+    bind_static_function(cls, "float_IEEE754", &ScalarTypeTorch::float_IEEE754);
+    bind_static_function(cls, "float_", &ScalarTypeTorch::float_);
+  }
+};
+#endif
+using ScalarTypeId = int64_t;
+//using ScalarTypeTorchPtr = c10::intrusive_ptr<ScalarTypeTorch>;
+
+// "rust style" names generally following:
+//   https://github.com/pytorch/pytorch/blob/6d9f74f0af54751311f0dd71f7e5c01a93260ab3/torch/csrc/api/include/torch/types.h#L60-L70
+static inline constexpr auto kS4 = ScalarType::int_(4);
+static inline constexpr auto kU4 = ScalarType::uint(4);
+static inline constexpr auto kU4B8 = ScalarType::uint(4, 8);
+static inline constexpr auto kS8 = ScalarType::int_(8);
+static inline constexpr auto kU8 = ScalarType::uint(8);
+static inline constexpr auto kU8B128 = ScalarType::uint(8, 128);
+
+static inline constexpr auto kFE3M2f =
+    ScalarType::float_(3, 2, true, ScalarType::NAN_NONE);
+static inline constexpr auto kFE4M3fn =
+    ScalarType::float_(4, 3, true, ScalarType::NAN_EXTD_RANGE_MAX_MIN);
+static inline constexpr auto kFE5M2 = ScalarType::float_IEEE754(5, 2);
+static inline constexpr auto kFE8M7 = ScalarType::float_IEEE754(8, 7);
+static inline constexpr auto kFE5M10 = ScalarType::float_IEEE754(5, 10);
+#if 0
+// Fixed width style names, generally following:
+//  https://github.com/pytorch/pytorch/blob/6d9f74f0af54751311f0dd71f7e5c01a93260ab3/torch/csrc/api/include/torch/types.h#L47-L57
+static inline constexpr auto kInt4 = kS4;
+static inline constexpr auto kUint4 = kU4;
+static inline constexpr auto kUint4b8 = kU4B8;
+static inline constexpr auto kInt8 = kS8;
+static inline constexpr auto kUint8 = kU8;
+static inline constexpr auto kUint8b128 = kU8B128;
+
+static inline constexpr auto kFloat6_e3m2f = kFE3M2f;
+static inline constexpr auto kFloat8_e4m3fn = kFE4M3fn;
+static inline constexpr auto kFloat8_e5m2 = kFE5M2;
+static inline constexpr auto kFloat16_e8m7 = kFE8M7;
+static inline constexpr auto kFloat16_e5m10 = kFE5M10;
+
+// colloquial names
+static inline constexpr auto kHalf = kFE5M10;
+static inline constexpr auto kFloat16 = kHalf;
+static inline constexpr auto kBFloat16 = kFE8M7;
+
+static inline constexpr auto kFloat16Id = kFloat16.id();
+#endif
+};  // namespace vllm
diff --git a/csrc/torch_bindings.cpp b/csrc/torch_bindings.cpp
index c9a120976..cb84306e5 100644
--- a/csrc/torch_bindings.cpp
+++ b/csrc/torch_bindings.cpp
@@ -54,16 +54,25 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   ops.def(
       "paged_attention_v2("
       "    Tensor! out, Tensor! exp_sums, Tensor! max_logits,"
-      "    Tensor! tmp_out, Tensor query, Tensor key_cache,"
+      "    Tensor! tmp_out, Tensor block_count, Tensor query, Tensor key_cache,"
       "    Tensor value_cache, int num_kv_heads, float scale,"
       "    Tensor block_tables, Tensor seq_lens, int block_size,"
       "    int max_seq_len, Tensor? alibi_slopes,"
       "    str kv_cache_dtype, Tensor k_scale, Tensor v_scale,"
       "    int tp_rank, int blocksparse_local_blocks,"
       "    int blocksparse_vert_stride, int blocksparse_block_size,"
-      "    int blocksparse_head_sliding_step) -> ()");
+      "    int blocksparse_head_sliding_step, bool count_init_once) -> ()");
   ops.impl("paged_attention_v2", torch::kCUDA, &paged_attention_v2);
 
+  ops.def(
+      "page_reshape_kv_cache("
+      "    Tensor! key_cache, Tensor! value_cache,"
+      "    Tensor! key_cache_new_layer, Tensor! value_cache_new_layer,"
+      "    int num_seqs, int num_heads, int head_size, int num_kv_heads, int block_size,"
+      "           str! kv_cache_dtype) -> ()");
+  //ops.def("page_reshape_kv_cache", &page_reshape_kv_cache);
+  ops.impl("page_reshape_kv_cache", torch::kCUDA, &page_reshape_kv_cache);
+
 #ifndef USE_ROCM
   // Merge attn states
   // Implements section 2.2 of https://www.arxiv.org/pdf/2501.01005
@@ -195,6 +204,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
 
   // Quantization ops
 #ifndef USE_ROCM
+#ifndef USE_MACA
   // Quantized GEMM for AQLM.
   ops.def(
       "aqlm_gemm(Tensor input, Tensor codes, Tensor codebooks, "
@@ -209,21 +219,26 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "int[] codebook_partition_sizes) -> Tensor",
       {stride_tag});
   ops.impl("aqlm_dequant", torch::kCUDA, &aqlm_dequant);
+#endif // USE_MACA
 
   // Quantized GEMM for AWQ.
   ops.def(
       "awq_gemm(Tensor _in_feats, Tensor _kernel, Tensor _scaling_factors, "
-      "Tensor _zeros, SymInt split_k_iters) -> Tensor",
-      {stride_tag});
+      "Tensor _zeros, SymInt split_k_iters, Tensor _temp_space, bool dtype_bf16) -> Tensor");
   ops.impl("awq_gemm", torch::kCUDA, &awq_gemm);
 
   // Dequantization for AWQ.
   ops.def(
       "awq_dequantize(Tensor _kernel, Tensor _scaling_factors, "
-      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor",
-      {stride_tag});
+      "Tensor _zeros, SymInt split_k_iters, int thx, int thy) -> Tensor");
   ops.impl("awq_dequantize", torch::kCUDA, &awq_dequantize);
 
+  // Convert AWQ to GPTQ
+  ops.def(
+      "awq_to_gptq_4bit(Tensor qweight) -> Tensor");
+  ops.impl("awq_to_gptq_4bit", torch::kCUDA, &awq_to_gptq_4bit);
+
+#ifndef USE_MACA
   // Note about marlin kernel 'workspace' arguments:
   // Technically these should be mutable since they are modified by the kernel.
   // But since they are set back to zero once the kernel is finished we can
@@ -254,7 +269,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "SymInt size_m, SymInt size_n, SymInt size_k) -> Tensor",
       {stride_tag});
   //  conditionally compiled so impl in source file
-
+#endif // USE_MACA
   // Machete (Dense) Optimized Mixed Precision GEMM for Hopper.
   ops.def(
       "machete_supported_schedules("
@@ -292,6 +307,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   ops.def("permute_cols(Tensor A, Tensor perm) -> Tensor");
   ops.impl("permute_cols", torch::kCUDA, &permute_cols);
 
+#ifndef USE_MACA
   // gptq_marlin Optimized Quantized GEMM for GPTQ.
   ops.def(
       "gptq_marlin_gemm(Tensor a, Tensor b_q_weight, Tensor b_scales, "
@@ -342,8 +358,10 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   ops.impl("ggml_moe_a8", torch::kCUDA, &ggml_moe_a8);
 
   ops.def("ggml_moe_get_block_size", &ggml_moe_get_block_size);
+#endif // USE_MACA
 
 #ifndef USE_ROCM
+#ifndef USE_MACA
   // fp8_marlin Optimized Quantized GEMM for FP8 weight-only.
   ops.def(
       "fp8_marlin_gemm(Tensor a, Tensor b_q_weight, Tensor b_scales, "
@@ -368,6 +386,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "                      Tensor alpha) -> ()",
       {stride_tag});
   ops.impl("cutlass_scaled_fp4_mm", torch::kCUDA, &cutlass_scaled_fp4_mm);
+#endif // USE_MACA
 
   // CUTLASS w8a8 GEMM, supporting symmetric per-tensor or per-row/column
   // quantization, as well as bias
@@ -388,6 +407,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       {stride_tag});
   ops.impl("cutlass_scaled_mm_azp", torch::kCUDA, &cutlass_scaled_mm_azp);
 
+#ifndef USE_MACA
   // Check if cutlass scaled_mm is supported for CUDA devices of the given
   // capability
   ops.def("cutlass_scaled_mm_supports_fp8(int cuda_device_capability) -> bool");
@@ -449,6 +469,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   // CUTLASS sparse matrix compressor
   ops.def("cutlass_sparse_compress(Tensor a) -> Tensor[]");
   ops.impl("cutlass_sparse_compress", &cutlass_sparse_compress);
+#endif // USE_MACA
 
   // Mamba selective scan kernel
   ops.def(
@@ -485,6 +506,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
       "int pad_slot_id) -> ()");
   ops.impl("causal_conv1d_fwd", torch::kCUDA, &causal_conv1d_fwd);
 
+#ifndef USE_MACA
   // Compute NVFP4 block quantized tensor.
   ops.def(
       "scaled_fp4_quant(Tensor! output, Tensor input,"
@@ -495,6 +517,7 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   // of the given capability
   ops.def("cutlass_scaled_mm_supports_fp4(int cuda_device_capability) -> bool");
   ops.impl("cutlass_scaled_mm_supports_fp4", &cutlass_scaled_mm_supports_fp4);
+#endif // USE_MACA
 #endif
 
   // Quantized GEMM for GPTQ.
@@ -502,9 +525,8 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
   // to prevent the meta function registry.
   ops.def(
       "gptq_gemm(Tensor a, Tensor b_q_weight, Tensor b_gptq_qzeros, "
-      "Tensor b_gptq_scales, Tensor b_g_idx, bool use_exllama, int bit) "
-      "-> Tensor",
-      {stride_tag});
+      "Tensor b_gptq_scales, Tensor b_g_idx, bool use_exllama, int bit, int group_size, Tensor perm_space, "
+      "Tensor temp_space, bool dtype_bf16)-> Tensor");
   ops.impl("gptq_gemm", torch::kCUDA, &gptq_gemm);
 
   // Post processing for GPTQ.
@@ -591,6 +613,15 @@ TORCH_LIBRARY_EXPAND(CONCAT(TORCH_EXTENSION_NAME, _cache_ops), cache_ops) {
       "                  Tensor k_scale, Tensor v_scale) -> ()");
   cache_ops.impl("reshape_and_cache", torch::kCUDA, &reshape_and_cache);
 
+cache_ops.def(
+      "reshape_and_cache_new(Tensor key, Tensor value,"
+      "                  Tensor! key_cache, Tensor! value_cache,"
+      "                  Tensor slot_mapping,"
+      "                  str kv_cache_dtype,"
+      "                  float kv_scale,"
+      "                  float v_scale) -> ()");
+  cache_ops.impl("reshape_and_cache_new", torch::kCUDA, &reshape_and_cache_new);
+
   // Reshape the key and value tensors and cache them.
   cache_ops.def(
       "reshape_and_cache_flash(Tensor key, Tensor value,"
diff --git a/docs/source/models/supported_models.md b/docs/source/models/supported_models.md
index 98b7d7631..127001438 100644
--- a/docs/source/models/supported_models.md
+++ b/docs/source/models/supported_models.md
@@ -585,6 +585,11 @@ See [this page](#generative-models) for more information on how to use generativ
   * `Zyphra/Zamba2-7B-instruct`, `Zyphra/Zamba2-2.7B-instruct`, `Zyphra/Zamba2-1.2B-instruct`, etc.
   *
   *
+- * `MiMoForCausalLM`
+  * MiMo
+  * `XiaomiMiMo/MiMo-7B-RL`, etc.
+  *
+  *
 :::
 
 :::{note}
diff --git a/env.sh b/env.sh
new file mode 100644
index 000000000..e95190521
--- /dev/null
+++ b/env.sh
@@ -0,0 +1,10 @@
+DEFAULT_DIR="/opt/maca"
+export MACA_PATH=${1:-$DEFAULT_DIR}
+export CUDA_PATH=/usr/local/cuda
+export CUCC_PATH=${MACA_PATH}/tools/cu-bridge
+export PATH=${CUDA_PATH}/bin:${MACA_PATH}/mxgpu_llvm/bin:${MACA_PATH}/bin:${CUCC_PATH}/tools:${CUCC_PATH}/bin:${PATH}
+export LD_LIBRARY_PATH=${MACA_PATH}/lib:${MACA_PATH}/ompi/lib:${MACA_PATH}/mxgpu_llvm/lib:${LD_LIBRARY_PATH}
+
+export VLLM_INSTALL_PUNICA_KERNELS=1
+
+echo "MACA PATH: ${MACA_PATH} Compile Code"
diff --git a/pyproject.toml b/pyproject.toml
index b5f1039b4..daafffe05 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -15,8 +15,7 @@ build-backend = "setuptools.build_meta"
 [project]
 name = "vllm"
 authors = [{name = "vLLM Team"}]
-license = "Apache-2.0"
-license-files = ["LICENSE"]
+license = { "file"= "LICENSE" }
 readme = "README.md"
 description = "A high-throughput and memory-efficient inference and serving engine for LLMs"
 classifiers = [
diff --git a/requirements/common.txt b/requirements/common.txt
index 33c4c3219..9bc7ff2f9 100644
--- a/requirements/common.txt
+++ b/requirements/common.txt
@@ -6,9 +6,13 @@ requests >= 2.26.0
 tqdm
 blake3
 py-cpuinfo
-transformers >= 4.51.1
+# [PAE-8915] Fix transformers version to avoid mixtral models loading error (config["head_dim"] == null)
+# transformers >= 4.51.1
+transformers == 4.51.3
 huggingface-hub[hf_xet] >= 0.30.0  # Required for Xet downloads.
-tokenizers >= 0.21.1  # Required for fast incremental detokenization.
+# Work-around for Invalid prefix encountered
+# tokenizers >= 0.21.1  # Required for fast incremental detokenization.
+tokenizers == 0.21.0
 protobuf # Required by LlamaTokenizer.
 fastapi[standard] >= 0.115.0 # Required by FastAPI's form models in the OpenAI API server's audio transcriptions endpoint.
 aiohttp
diff --git a/requirements/cuda.txt b/requirements/cuda.txt
index cdc6ee75a..92c546cdb 100644
--- a/requirements/cuda.txt
+++ b/requirements/cuda.txt
@@ -5,9 +5,9 @@ numba == 0.60.0; python_version == '3.9' # v0.61 doesn't support Python 3.9. Req
 numba == 0.61.2; python_version > '3.9'
 
 # Dependencies for NVIDIA GPUs
-ray[cgraph]>=2.43.0, !=2.44.* # Ray Compiled Graph, required for pipeline parallelism in V1.
+# ray[cgraph]>=2.43.0, !=2.44.* # Ray Compiled Graph, required for pipeline parallelism in V1.
 torch==2.6.0
-torchaudio==2.6.0
+# torchaudio==2.6.0
 # These must be updated alongside torch
-torchvision==0.21.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
-xformers==0.0.29.post2; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.6.0
+# torchvision==0.21.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version
+# xformers==0.0.29.post2; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.6.0
diff --git a/setup.py b/setup.py
index a1867960e..1832d489b 100755
--- a/setup.py
+++ b/setup.py
@@ -15,9 +15,11 @@ import torch
 from packaging.version import Version, parse
 from setuptools import Extension, setup
 from setuptools.command.build_ext import build_ext
-from setuptools_scm import get_version
+# from setuptools_scm import get_version
 from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME
 
+USE_MACA = True
+CMAKE_EXECUTABLE = 'cmake' if not USE_MACA else 'cmake_maca'
 
 def load_module_from_path(module_name, path):
     spec = importlib.util.spec_from_file_location(module_name, path)
@@ -143,11 +145,20 @@ class cmake_build_ext(build_ext):
         # Note: optimization level + debug info are set by the build type
         default_cfg = "Debug" if self.debug else "RelWithDebInfo"
         cfg = envs.CMAKE_BUILD_TYPE or default_cfg
+        
+        maca_version = get_maca_version_list()
 
         cmake_args = [
             '-DCMAKE_BUILD_TYPE={}'.format(cfg),
             '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
         ]
+        if USE_MACA:
+            maca_args_ext = ['-DUSE_MACA=ON',
+                '-DMACA_VERSION_MAJOR={}'.format(maca_version[0]),
+                '-DMACA_VERSION_MINOR={}'.format(maca_version[1]),
+                '-DMACA_VERSION_PATCH={}'.format(maca_version[2]),
+                '-DMACA_VERSION_BUILD={}'.format(maca_version[3]),]
+            cmake_args.extend(maca_args_ext)
 
         verbose = envs.VERBOSE
         if verbose:
@@ -202,16 +213,16 @@ class cmake_build_ext(build_ext):
             # Default build tool to whatever cmake picks.
             build_tool = []
         # Make sure we use the nvcc from CUDA_HOME
-        if _is_cuda():
-            cmake_args += [f'-DCMAKE_CUDA_COMPILER={CUDA_HOME}/bin/nvcc']
+        # if _is_cuda():
+        #     cmake_args += [f'-DCMAKE_CUDA_COMPILER={CUDA_HOME}/bin/nvcc']
         subprocess.check_call(
-            ['cmake', ext.cmake_lists_dir, *build_tool, *cmake_args],
+            [CMAKE_EXECUTABLE, ext.cmake_lists_dir, *build_tool, *cmake_args],
             cwd=self.build_temp)
 
     def build_extensions(self) -> None:
         # Ensure that CMake is present and working
         try:
-            subprocess.check_output(['cmake', '--version'])
+            subprocess.check_output([CMAKE_EXECUTABLE, '--version'])
         except OSError as e:
             raise RuntimeError('Cannot find CMake executable') from e
 
@@ -238,7 +249,7 @@ class cmake_build_ext(build_ext):
             *[f"--target={name}" for name in targets],
         ]
 
-        subprocess.check_call(["cmake", *build_args], cwd=self.build_temp)
+        subprocess.check_call([CMAKE_EXECUTABLE, *build_args], cwd=self.build_temp)
 
         # Install the libraries
         for ext in self.extensions:
@@ -260,7 +271,7 @@ class cmake_build_ext(build_ext):
 
             # prefix here should actually be the same for all components
             install_args = [
-                "cmake", "--install", ".", "--prefix", prefix, "--component",
+                CMAKE_EXECUTABLE, "--install", ".", "--prefix", prefix, "--component",
                 target_name(ext.name)
             ]
             subprocess.check_call(install_args, cwd=self.build_temp)
@@ -376,9 +387,11 @@ class repackage_wheel(build_ext):
             files_to_copy = [
                 "vllm/_C.abi3.so",
                 "vllm/_moe_C.abi3.so",
-                "vllm/_flashmla_C.abi3.so",
-                "vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
-                "vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
+                # "vllm/_flashmla_C.abi3.so",
+                # "vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so",
+                # "vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so",
+                # "vllm/vllm_flash_attn/flash_attn_interface.py",
+                # "vllm/vllm_flash_attn/__init__.py",
                 "vllm/cumem_allocator.abi3.so",
                 # "vllm/_version.py", # not available in nightly wheels yet
             ]
@@ -547,9 +560,52 @@ def get_gaudi_sw_version():
             " ", "").split(":")[1][:-1].split("-")[0]
     return "0.0.0"  # when hl-smi is not available
 
+def get_maca_version():
+    """
+    Returns the MACA SDK Version
+    """
+    maca_path = str(os.getenv('MACA_PATH'))
+    if not os.path.exists(maca_path):
+        return None
+    file_full_path = os.path.join(maca_path, 'Version.txt')
+    if not os.path.isfile(file_full_path):
+        return None
+    
+    with open(file_full_path, 'r', encoding='utf-8') as file:
+        first_line = file.readline().strip()
+    return first_line.split(":")[-1]
+
+def get_maca_version_list():
+    version_str = get_maca_version()
+    version_list = list(map(int, (version_str or "0.0.0.0").split('.')))
+    version_list.extend([0] * (4 - len(version_list)))
+    return version_list
+    
+def get_git_commit():
+    curdir = os.path.dirname(__file__)
+    default_gitdir = os.path.normpath(os.path.join(curdir, ".git"))
+    print(default_gitdir)
+    try:
+        subprocess.check_output(["git", "--git-dir", default_gitdir, "config", "--global", "--add", "safe.directory", '*'])
+        commit_id = subprocess.check_output(["git", "--git-dir", default_gitdir, "rev-parse", "HEAD"]).decode("utf-8").strip()
+        return commit_id
+    except Exception as e:
+        print(f"Error: {e}")
+        return "git error"
+
+def write_to_file(file_path, content):
+    try:
+        with open(file_path, "w") as file:
+            file.write(content)
+        print(f"Content written to {file_path} successfully.")
+    except Exception as e:
+        print(f"Error writing to file: {e}")
 
 def get_vllm_version() -> str:
-    version = get_version(write_to="vllm/_version.py")
+    # version = get_version(write_to="vllm/_version.py")
+    commit_id = get_git_commit()
+    write_to_file("vllm/_release_info.txt", commit_id)
+    version = "0.8.5"
     sep = "+" if "+" not in version else "."  # dev versions might contain +
 
     if _no_device():
@@ -559,12 +615,24 @@ def get_vllm_version() -> str:
         if envs.VLLM_USE_PRECOMPILED:
             version += f"{sep}precompiled"
         else:
-            cuda_version = str(get_nvcc_cuda_version())
-            if cuda_version != MAIN_CUDA_VERSION:
-                cuda_version_str = cuda_version.replace(".", "")[:3]
-                # skip this for source tarball, required for pypi
-                if "sdist" not in sys.argv:
-                    version += f"{sep}cu{cuda_version_str}"
+            # cuda_version = str(get_nvcc_cuda_version())
+            # if cuda_version != MAIN_CUDA_VERSION:
+            #     cuda_version_str = cuda_version.replace(".", "")[:3]
+            #     # skip this for source tarball, required for pypi
+            #     if "sdist" not in sys.argv:
+            #         version += f"{sep}cu{cuda_version_str}"
+            if not USE_MACA:
+                cuda_version = str(get_nvcc_cuda_version())
+                if cuda_version != MAIN_CUDA_VERSION:
+                    cuda_version_str = cuda_version.replace(".", "")[:3]
+                    # skip this for source tarball, required for pypi
+                    if "sdist" not in sys.argv:
+                        version += f"{sep}cu{cuda_version_str}"
+            else:
+                maca_version_str = get_maca_version()
+                torch_version = torch.__version__
+                major_minor_version = ".".join(torch_version.split(".")[:2])
+                version += f"{sep}maca{maca_version_str}torch{major_minor_version}"
     elif _is_hip():
         # Get the Rocm Version
         rocm_version = get_rocm_version() or torch.version.hip
@@ -651,7 +719,7 @@ if _is_cuda() or _is_hip():
 if _is_hip():
     ext_modules.append(CMakeExtension(name="vllm._rocm_C"))
 
-if _is_cuda():
+if _is_cuda() and False:
     ext_modules.append(CMakeExtension(name="vllm.vllm_flash_attn._vllm_fa2_C"))
     if envs.VLLM_USE_PRECOMPILED or get_nvcc_cuda_version() >= Version("12.3"):
         # FA3 requires CUDA 12.3 or later
@@ -661,9 +729,11 @@ if _is_cuda():
         # not targeting a hopper system
         ext_modules.append(
             CMakeExtension(name="vllm._flashmla_C", optional=True))
+
+if _is_cuda():
     ext_modules.append(CMakeExtension(name="vllm.cumem_allocator"))
 
-if _build_custom_ops():
+if _build_custom_ops() or True:
     ext_modules.append(CMakeExtension(name="vllm._C"))
 
 package_data = {
@@ -671,6 +741,8 @@ package_data = {
         "py.typed",
         "model_executor/layers/fused_moe/configs/*.json",
         "model_executor/layers/quantization/utils/configs/*.json",
+        "attention/backends/configs/*.json",
+        "_release_info.txt",
     ]
 }
 
diff --git a/tests/models/registry.py b/tests/models/registry.py
index a08924639..1b9af14ba 100644
--- a/tests/models/registry.py
+++ b/tests/models/registry.py
@@ -242,6 +242,8 @@ _TEXT_GENERATION_EXAMPLE_MODELS = {
                                          is_available_online=False,
                                          trust_remote_code=True),
     "Zamba2ForCausalLM": _HfExamplesInfo("Zyphra/Zamba2-7B-instruct"),
+    "MiMoForCausalLM": _HfExamplesInfo("XiaomiMiMo/MiMo-7B-RL",
+                                        trust_remote_code=True),
     # [Encoder-decoder]
     "BartModel": _HfExamplesInfo("facebook/bart-base"),
     "BartForConditionalGeneration": _HfExamplesInfo("facebook/bart-large-cnn"),
@@ -398,6 +400,9 @@ _SPECULATIVE_DECODING_EXAMPLE_MODELS = {
                                             trust_remote_code=True,
                                             speculative_model="yuhuili/EAGLE3-LLaMA3.1-Instruct-8B",
                                             tokenizer="meta-llama/Llama-3.1-8B-Instruct"),
+    "MiMoMTPModel": _HfExamplesInfo("XiaomiMiMo/MiMo-7B-RL",
+                                    trust_remote_code=True,
+                                    speculative_model="XiaomiMiMo/MiMo-7B-RL")
 }
 
 _TRANSFORMERS_MODELS = {
diff --git a/tests/v1/core/test_scheduler.py b/tests/v1/core/test_scheduler.py
index ee4e95856..27d0117be 100644
--- a/tests/v1/core/test_scheduler.py
+++ b/tests/v1/core/test_scheduler.py
@@ -21,7 +21,7 @@ EOS_TOKEN_ID = 50256
 
 
 def create_scheduler(
-    model: str = "facebook/opt-125m",
+    model: str = "/pde_ai/models/llm/OPT/opt-125/",
     max_num_seqs: int = 16,
     max_num_batched_tokens: int = 8192,
     enable_prefix_caching: Optional[bool] = None,
@@ -362,7 +362,7 @@ def test_schedule_concurrent_partial_requests(enable_prefix_caching: bool):
 
     """
     scheduler = create_scheduler(
-        model="facebook/opt-125m",
+        model="/pde_ai/models/llm/OPT/opt-125/",
         max_num_batched_tokens=1024,
         long_prefill_token_threshold=400,
         enable_prefix_caching=enable_prefix_caching,
@@ -1165,3 +1165,80 @@ def test_kv_connector_handles_preemption():
     # All memory should be freed since nothing is running.
     assert scheduler.kv_cache_manager.block_pool.get_num_free_blocks() \
         == NUM_BLOCKS - 1
+
+
+def make_output(scheduler: Scheduler):
+    return ModelRunnerOutput(
+        req_ids=[req.request_id for req in scheduler.running],
+        req_id_to_index={
+            req.request_id: i
+            for i, req in enumerate(scheduler.running)
+        },
+        sampled_token_ids=[[1000]] * len(scheduler.running),
+        spec_token_ids=None,
+        logprobs=None,
+        prompt_logprobs_dict={},
+    )
+
+
+def assert_scheduler_empty(scheduler: Scheduler):
+    """Confirm the scheduler is "empty" - i.e. no leaks."""
+    # Scheduler Metadata.
+    assert len(scheduler.requests) == 0
+    assert len(scheduler.waiting) == 0
+    assert len(scheduler.running) == 0
+    assert len(scheduler.finished_req_ids) == 0
+    assert len(scheduler._cached_reqs_data) == 0
+
+    # EncoderCacheManager.
+    assert len(scheduler.encoder_cache_manager.freed) == 0
+    assert len(scheduler.encoder_cache_manager.cached) == 0
+
+    # KVCache Manager.
+    assert len(scheduler.kv_cache_manager.req_to_blocks) == 0
+    assert len(scheduler.kv_cache_manager.req_to_block_hashes) == 0
+    assert len(scheduler.kv_cache_manager.num_cached_block) == 0
+    num_free_blocks = (
+        scheduler.kv_cache_manager.block_pool.free_block_queue.num_free_blocks)
+    assert num_free_blocks == (
+        scheduler.kv_cache_manager.block_pool.num_gpu_blocks - 1)
+
+    # NOTE(rob): just the ref count on blocks will be 0. The hash
+    # value, etc will remain since we lazily evict for prefix cache.
+    for block in scheduler.kv_cache_manager.block_pool.blocks:
+        assert block.ref_cnt == 0
+        # assert block._block_hash is None
+    # assert (
+    #     len(scheduler.kv_cache_manager.block_pool.cached_block_hash_to_block
+    #           ) == 0)
+
+
+def test_memory_leak():
+    """Test that we do not have a memory leak."""
+
+    scheduler = create_scheduler(enable_prefix_caching=True)
+
+    NUM_REQUESTS = 5
+    NUM_TOKENS = 10
+    MAX_TOKENS = 10
+    requests = create_requests(num_requests=NUM_REQUESTS,
+                               num_tokens=NUM_TOKENS,
+                               max_tokens=MAX_TOKENS)
+
+    # Add each request.
+    for request in requests:
+        scheduler.add_request(request)
+        scheduler_output = scheduler.schedule()
+        model_runner_output = make_output(scheduler)
+        scheduler.update_from_output(scheduler_output, model_runner_output)
+
+    # Iterate until done.
+    while True:
+        scheduler_output = scheduler.schedule()
+        if len(scheduler.running) == 0:
+            break
+        model_runner_output = make_output(scheduler)
+        scheduler.update_from_output(scheduler_output, model_runner_output)
+
+    # Confirm no memory leak.
+    assert_scheduler_empty(scheduler)
\ No newline at end of file
diff --git a/tests/v1/e2e/test_cascade_attention.py b/tests/v1/e2e/test_cascade_attention.py
index 48c265560..11a8760db 100644
--- a/tests/v1/e2e/test_cascade_attention.py
+++ b/tests/v1/e2e/test_cascade_attention.py
@@ -8,8 +8,8 @@ from ...utils import fork_new_process_for_each_test
 
 
 @fork_new_process_for_each_test
-@pytest.mark.parametrize("attn_backend",
-                         ["FLASH_ATTN_VLLM_V1", "FLASHINFER_VLLM_V1"])
+@pytest.mark.parametrize("attn_backend", ["FLASH_ATTN_VLLM_V1"])
+                         #["FLASH_ATTN_VLLM_V1", "FLASHINFER_VLLM_V1"])
 def test_cascade_attention(example_system_message, monkeypatch, attn_backend):
     prompt = "\n<User>: Implement fibonacci sequence in Python.\n<Claude>:"
 
@@ -17,7 +17,7 @@ def test_cascade_attention(example_system_message, monkeypatch, attn_backend):
         m.setenv("VLLM_USE_V1", "1")
         m.setenv("VLLM_ATTENTION_BACKEND", attn_backend)
 
-        llm = LLM(model="Qwen/Qwen2-1.5B-Instruct")
+        llm = LLM(model="/pde_ai/models/llm/MiMo-7B-RL-0530/",trust_remote_code=True,disable_cascade_attn=True)
         sampling_params = SamplingParams(temperature=0.0, max_tokens=100)
 
         # No cascade attention.
diff --git a/tests/v1/engine/test_engine_core_client.py b/tests/v1/engine/test_engine_core_client.py
index 8cc36fa16..1533257eb 100644
--- a/tests/v1/engine/test_engine_core_client.py
+++ b/tests/v1/engine/test_engine_core_client.py
@@ -26,7 +26,7 @@ if not current_platform.is_cuda():
     pytest.skip(reason="V1 currently only supported on CUDA.",
                 allow_module_level=True)
 
-MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
+MODEL_NAME = "/pde_ai/models/llm/Llama//Llama-3.2-1B-Instruct/"
 TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)
 PROMPT = "Hello my name is Robert and I love quantization kernels"
 PROMPT_TOKENS = TOKENIZER(PROMPT).input_ids
diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index 4c577c1c4..a72eb39a0 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -24,7 +24,6 @@ supports_moe_ops = False
 with contextlib.suppress(ImportError):
     import vllm._moe_C  # noqa: F401
     supports_moe_ops = True
-
 if TYPE_CHECKING:
 
     def register_fake(fn):
@@ -35,7 +34,6 @@ else:
     except ImportError:
         from torch.library import impl_abstract as register_fake
 
-
 # page attention ops
 def paged_attention_v1(
     out: torch.Tensor,
@@ -71,6 +69,7 @@ def paged_attention_v2(
     exp_sum: torch.Tensor,
     max_logits: torch.Tensor,
     tmp_out: torch.Tensor,
+    block_count: torch.Tensor,
     query: torch.Tensor,
     key_cache: torch.Tensor,
     value_cache: torch.Tensor,
@@ -89,13 +88,14 @@ def paged_attention_v2(
     blocksparse_vert_stride: int = 0,
     blocksparse_block_size: int = 64,
     blocksparse_head_sliding_step: int = 0,
+    count_init_once: bool = False,
 ) -> None:
     torch.ops._C.paged_attention_v2(
-        out, exp_sum, max_logits, tmp_out, query, key_cache, value_cache,
+        out, exp_sum, max_logits, tmp_out, block_count, query, key_cache, value_cache,
         num_kv_heads, scale, block_tables, seq_lens, block_size, max_seq_len,
         alibi_slopes, kv_cache_dtype, k_scale, v_scale, tp_rank,
         blocksparse_local_blocks, blocksparse_vert_stride,
-        blocksparse_block_size, blocksparse_head_sliding_step)
+        blocksparse_block_size, blocksparse_head_sliding_step, count_init_once)
 
 
 def paged_attention_rocm(
@@ -171,6 +171,21 @@ def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
                                           cos_sin_cache, is_neox, rot_dim,
                                           cos_sin_cache_offsets)
 
+def page_reshape_kv_cache(
+    key_cache: torch.Tensor,
+    value_cache: torch.Tensor,
+    key_cache_new_layer: torch.Tensor,
+    value_cache_new_layer: torch.Tensor,
+    num_seqs: int,
+    num_heads: int,
+    head_size: int,
+    num_kv_heads: int,
+    block_size: int,
+    kv_cache_dtype: str,
+  )->None:
+    torch.ops._C.page_reshape_kv_cache(key_cache, value_cache, key_cache_new_layer,
+            value_cache_new_layer, num_seqs, num_heads, head_size, num_kv_heads, block_size, kv_cache_dtype)
+
 
 # layer norm ops
 def rms_norm(out: torch.Tensor, input: torch.Tensor, weight: torch.Tensor,
@@ -246,25 +261,59 @@ def awq_dequantize(qweight: torch.Tensor, scales: torch.Tensor,
         return awq_dequantize_triton(qweight, scales, zeros)
     return torch.ops._C.awq_dequantize(qweight, scales, zeros, split_k_iters,
                                        thx, thy)
+if hasattr(torch.ops._C, "awq_dequantize"):
 
+    @register_fake("_C::awq_dequantize")
+    def _awq_dequantize_fake(qweight: torch.Tensor, scales: torch.Tensor,
+                             zeros: torch.Tensor, split_k_iters: torch.SymInt,
+                             thx: int, thy: int) -> torch.Tensor:
+        in_c = qweight.size(0)
+        qout_c = qweight.size(1)
+        out_c = qout_c * 8
+        return torch.empty((in_c, out_c),
+                           dtype=scales.dtype,
+                           device=scales.device)
+    
 
 def awq_gemm(input: torch.Tensor, qweight: torch.Tensor, qzeros: torch.Tensor,
-             scales: torch.Tensor, split_k_iters: int) -> torch.Tensor:
+             scales: torch.Tensor, split_k_iters: int, temp_space: torch.Tensor, 
+             dtype_bf16: bool) -> torch.Tensor:
     if envs.VLLM_USE_TRITON_AWQ:
         from vllm.model_executor.layers.quantization.awq_triton import (
             awq_gemm_triton)
-        return awq_gemm_triton(input, qweight, qzeros, scales, split_k_iters)
-    return torch.ops._C.awq_gemm(input, qweight, qzeros, scales, split_k_iters)
+        return awq_gemm_triton(input, qweight, scales, qzeros, split_k_iters)
+    return torch.ops._C.awq_gemm(input, qweight, scales, qzeros, split_k_iters,
+                                temp_space, dtype_bf16)
+if hasattr(torch.ops._C, "awq_gemm"):
+
+    @register_fake("_C::awq_gemm")
+    def _awq_gemm_fake(input: torch.Tensor, qweight: torch.Tensor,
+                       qzeros: torch.Tensor, scales: torch.Tensor,
+                       split_k_iters: torch.SymInt, temp_space: torch.Tensor, 
+                       dtype_bf16: bool) -> torch.Tensor:
+        x = input.reshape(-1, input.shape[-1])
+        num_in_feats = x.size(0)
+        num_out_channel = qweight.size(0)
+
+        return torch.empty((num_in_feats, num_out_channel),
+                           dtype=input.dtype,
+                           device=input.device).sum(0)
 
+# awq to gptq 4bit conversion
+def awq_to_gptq_4bit(qweight: torch.Tensor) -> torch.Tensor:
+    if envs.VLLM_USE_TRITON_AWQ:
+        return qweight
+    return torch.ops._C.awq_to_gptq_4bit(qweight)
 
 # gptq
 def gptq_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
               b_gptq_qzeros: torch.Tensor, b_gptq_scales: torch.Tensor,
               b_g_idx: torch.Tensor, use_exllama: bool,
-              bit: int) -> torch.Tensor:
+              bit: int, group_size: int, perm_space: torch.Tensor,
+              temp_space: torch.Tensor, dtype_bf16: bool) -> torch.Tensor:
     return torch.ops._C.gptq_gemm(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,
-                                  b_g_idx, use_exllama, bit)
-
+                                  b_g_idx, use_exllama, bit, group_size, 
+                                  perm_space, temp_space, dtype_bf16)
 
 if hasattr(torch.ops._C, "gptq_gemm"):
 
@@ -272,12 +321,13 @@ if hasattr(torch.ops._C, "gptq_gemm"):
     def _gptq_gemm_fake(a: torch.Tensor, b_q_weight: torch.Tensor,
                         b_gptq_qzeros: torch.Tensor,
                         b_gptq_scales: torch.Tensor, b_g_idx: torch.Tensor,
-                        use_exllama: bool, bit: int) -> torch.Tensor:
+                        use_exllama: bool, bit: int, 
+                        group_size: int, perm_space: torch.Tensor,
+                        temp_space: torch.Tensor, dtype_bf16: bool) -> torch.Tensor:
         return torch.empty((a.size(0), b_q_weight.size(1)),
                            dtype=a.dtype,
                            device=a.device)
 
-
 def gptq_shuffle(q_weight: torch.Tensor, q_perm: torch.Tensor,
                  bit: int) -> None:
     torch.ops._C.gptq_shuffle(q_weight, q_perm, bit)
@@ -300,7 +350,7 @@ def gptq_marlin_24_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
                                             workspace, b_q_type.id, size_m,
                                             size_n, size_k)
 
-
+"""
 if hasattr(torch.ops._C, "gptq_marlin_24_gemm"):
 
     @register_fake("_C::gptq_marlin_24_gemm")
@@ -492,12 +542,12 @@ if hasattr(torch.ops._C, "ggml_dequantize"):
         return torch.empty((tokens * top_k, row),
                            dtype=torch.float16,
                            device=W.device)
-
+"""
 
 # cutlass
 def cutlass_scaled_mm_supports_fp4(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_fp4(cuda_device_capability)
-
+    # return torch.ops._C.cutlass_scaled_mm_supports_fp4(cuda_device_capability)
+    return False
 
 def cutlass_scaled_fp4_mm(a: torch.Tensor, b: torch.Tensor,
                           block_scale_a: torch.Tensor,
@@ -510,15 +560,24 @@ def cutlass_scaled_fp4_mm(a: torch.Tensor, b: torch.Tensor,
                                        alpha)
     return out
 
-
 def cutlass_scaled_mm_supports_fp8(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
+    # return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
+    return True
 
 
 def cutlass_scaled_mm_supports_block_fp8(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
-        cuda_device_capability)
-
+    # return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
+    #     cuda_device_capability)
+    return True
+
+# Batch gemm in vllm, support w8a8 int8 quantization
+def cutlass_scaled_batch_mm(a: torch.Tensor, b: torch.Tensor,
+                            scale_a: torch.Tensor, scale_b: torch.Tensor,
+                            out_dtype: torch.dtype, bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+    assert (a.shape[0] == b.shape[0] and a.shape[2] == b.shape[1])
+    out = torch.empty((a.shape[0], a.shape[1], b.shape[2]), device = a.device, dtype = out_dtype)
+    torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
+    return out
 
 def cutlass_scaled_mm(a: torch.Tensor,
                       b: torch.Tensor,
@@ -872,7 +931,6 @@ def machete_prepack_B(
     return torch.ops._C.machete_prepack_B(b_q_weight, a_type, b_type.id,
                                           group_scales_type)
 
-
 if hasattr(torch.ops._C, "permute_cols"):
 
     @register_fake("_C::permute_cols")
@@ -1264,6 +1322,15 @@ def topk_softmax(topk_weights: torch.Tensor, topk_ids: torch.Tensor,
     torch.ops._moe_C.topk_softmax(topk_weights, topk_ids,
                                   token_expert_indicies, gating_output)
 
+def fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor,
+                    topk_weights: torch.Tensor, topk_ids: torch.Tensor,
+                    sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor,
+                    num_tokens_post_padded: torch.Tensor, mul_routed_weight: bool, top_k: int, tileConfig: int) -> None:
+    torch.ops._moe_C.fused_moe_kernel(A, B, C,
+                    topk_weights, topk_ids,
+                    sorted_token_ids, expert_ids,
+                    num_tokens_post_padded, mul_routed_weight, top_k, tileConfig)
+
 
 def moe_wna16_marlin_gemm(input: torch.Tensor, output: Optional[torch.Tensor],
                           b_qweight: torch.Tensor, b_scales: torch.Tensor,
@@ -1344,6 +1411,20 @@ def reshape_and_cache(
                                              value_cache, slot_mapping,
                                              kv_cache_dtype, k_scale, v_scale)
 
+def reshape_and_cache_new(
+    key: torch.Tensor,
+    value: torch.Tensor,
+    key_cache: torch.Tensor,
+    value_cache: torch.Tensor,
+    slot_mapping: torch.Tensor,
+    kv_cache_dtype: str,
+    kv_scale: float,
+    v_scale: float,
+) -> None:
+    torch.ops._C_cache_ops.reshape_and_cache_new(key, value, key_cache,
+                                             value_cache, slot_mapping,
+                                             kv_cache_dtype, kv_scale, v_scale)
+
 
 def reshape_and_cache_flash(
     key: torch.Tensor,
diff --git a/vllm/attention/backends/configs/tp8_merge.json b/vllm/attention/backends/configs/tp8_merge.json
new file mode 100644
index 000000000..773051b85
--- /dev/null
+++ b/vllm/attention/backends/configs/tp8_merge.json
@@ -0,0 +1,986 @@
+[
+    {
+        "BS": 1,
+        "L": 2,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 4,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 8,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 16,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 32,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 64,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 128,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 256,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 512,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 1024,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 2048,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 4096,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 8192,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 16384,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 32768,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 1,
+        "L": 65536,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 2,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 4,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 8,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 16,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 32,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 64,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 128,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 256,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 512,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 1024,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 2048,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 4096,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 8192,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 16384,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 32768,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 2,
+        "L": 65536,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 2,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 4,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 8,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 16,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 32,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 64,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 128,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 256,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 512,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 1024,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 2048,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 4096,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 8192,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 16384,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 4,
+        "L": 32768,
+        "num_kv_splits": 16,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 2,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 8,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 16,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 32,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 64,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 128,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 256,
+        "num_kv_splits": 8,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 512,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 1024,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 2048,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 4096,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 8192,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 8,
+        "L": 16384,
+        "num_kv_splits": 13,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 32,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 64,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 128,
+        "num_kv_splits": 4,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 256,
+        "num_kv_splits": 6,
+        "num_stages": 2
+    },
+    {
+        "BS": 16,
+        "L": 512,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 1024,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 2048,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 4096,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 16,
+        "L": 8192,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 128,
+        "num_kv_splits": 3,
+        "num_stages": 2
+    },
+    {
+        "BS": 32,
+        "L": 256,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 512,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 1024,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 2048,
+        "num_kv_splits": 6,
+        "num_stages": 1
+    },
+    {
+        "BS": 32,
+        "L": 4096,
+        "num_kv_splits": 13,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 2,
+        "num_kv_splits": 4,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 128,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 256,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 512,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 1024,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 2048,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 64,
+        "L": 2048,
+        "num_kv_splits": 8,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 256,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 512,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 96,
+        "L": 1024,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 256,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 512,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 128,
+        "L": 1024,
+        "num_kv_splits": 3,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 256,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 256,
+        "L": 512,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 512,
+        "L": 256,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1024,
+        "L": 128,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 1536,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 2048,
+        "L": 64,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 3072,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 2,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 4,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 8,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 16,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    },
+    {
+        "BS": 4096,
+        "L": 32,
+        "num_kv_splits": 2,
+        "num_stages": 1
+    }
+]
\ No newline at end of file
diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py
index 7f8f720ee..93a8cdc65 100755
--- a/vllm/attention/backends/flash_attn.py
+++ b/vllm/attention/backends/flash_attn.py
@@ -27,8 +27,12 @@ from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
 from vllm.logger import init_logger
 from vllm.multimodal import MultiModalPlaceholderMap
 from vllm.utils import async_tensor_h2d, make_tensor_with_pad
-from vllm.vllm_flash_attn import (flash_attn_varlen_func,
+from flash_attn import (flash_attn_varlen_func,
                                   flash_attn_with_kvcache)
+# from vllm.vllm_flash_attn.fa_utils import (flash_attn_supports_fp8,
+#                                            get_flash_attn_version)
+def flash_attn_supports_fp8() -> bool:
+    return False
 
 if TYPE_CHECKING:
     from vllm.worker.model_runner import (ModelInputForGPUBuilder,
@@ -636,8 +640,8 @@ class FlashAttentionImpl(AttentionImpl):
         self.sliding_window = ((sliding_window - 1,
                                 0) if sliding_window is not None else (-1, -1))
         self.kv_cache_dtype = kv_cache_dtype
-        self.vllm_flash_attn_version = get_flash_attn_version(
-            requires_alibi=self.alibi_slopes is not None)
+        # self.vllm_flash_attn_version = get_flash_attn_version(
+        #     requires_alibi=self.alibi_slopes is not None)
         if is_quantized_kv_cache(self.kv_cache_dtype) and (
                 not self.kv_cache_dtype.startswith("fp8")
                 or not flash_attn_supports_fp8()):
@@ -805,7 +809,7 @@ class FlashAttentionImpl(AttentionImpl):
                         (num_kv_tokens, num_kv_heads, head_size))
 
                 descale_shape = (q_seq_start_loc.shape[0] - 1, key.shape[1])
-                flash_attn_varlen_func(
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
                     q=query,
                     k=key,
                     v=value,
@@ -818,11 +822,11 @@ class FlashAttentionImpl(AttentionImpl):
                     window_size=window_size,
                     alibi_slopes=alibi_slopes,
                     softcap=logits_soft_cap,
-                    out=prefill_output,
-                    fa_version=self.vllm_flash_attn_version,
-                    q_descale=layer._q_scale.expand(descale_shape),
-                    k_descale=layer._k_scale.expand(descale_shape),
-                    v_descale=layer._v_scale.expand(descale_shape),
+                    # out=prefill_output,
+                    # fa_version=self.vllm_flash_attn_version,
+                    # q_descale=layer._q_scale.expand(descale_shape),
+                    # k_descale=layer._k_scale.expand(descale_shape),
+                    # v_descale=layer._v_scale.expand(descale_shape),
                 )
             else:
                 # prefix-enabled attention
@@ -833,13 +837,13 @@ class FlashAttentionImpl(AttentionImpl):
                 max_seq_len = max(prefill_meta.seq_lens)
                 descale_shape = (prefill_meta.query_start_loc.shape[0] - 1,
                                  key.shape[1])
-                flash_attn_varlen_func(  # noqa
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(  # noqa
                     q=query,
                     k=key_cache,
                     v=value_cache,
                     cu_seqlens_q=prefill_meta.query_start_loc,
                     max_seqlen_q=prefill_meta.max_query_len,
-                    seqused_k=prefill_meta.seq_lens_tensor,
+                    cu_seqlens_k=prefill_meta.seq_start_loc,
                     max_seqlen_k=max_seq_len,
                     softmax_scale=softmax_scale,
                     causal=True,
@@ -847,11 +851,11 @@ class FlashAttentionImpl(AttentionImpl):
                     alibi_slopes=alibi_slopes,
                     block_table=prefill_meta.block_tables,
                     softcap=logits_soft_cap,
-                    out=prefill_output,
-                    fa_version=self.vllm_flash_attn_version,
-                    q_descale=layer._q_scale.expand(descale_shape),
-                    k_descale=layer._k_scale.expand(descale_shape),
-                    v_descale=layer._v_scale.expand(descale_shape),
+                    # out=prefill_output,
+                    # fa_version=self.vllm_flash_attn_version,
+                    # q_descale=layer._q_scale.expand(descale_shape),
+                    # k_descale=layer._k_scale.expand(descale_shape),
+                    # v_descale=layer._v_scale.expand(descale_shape),
                 )
 
         if decode_meta := attn_metadata.decode_metadata:
@@ -868,13 +872,13 @@ class FlashAttentionImpl(AttentionImpl):
                 assert decode_meta.query_start_loc is not None
                 descale_shape = (decode_meta.query_start_loc.shape[0] - 1,
                                  key.shape[1])
-                flash_attn_varlen_func(
+                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
                     q=decode_query,
                     k=key_cache,
                     v=value_cache,
                     cu_seqlens_q=decode_meta.query_start_loc,
                     max_seqlen_q=decode_meta.max_decode_query_len,
-                    seqused_k=decode_meta.seq_lens_tensor,
+                    cu_seqlens_k=decode_meta.seq_start_loc,
                     max_seqlen_k=decode_meta.max_decode_seq_len,
                     softmax_scale=softmax_scale,
                     causal=True,
@@ -882,11 +886,11 @@ class FlashAttentionImpl(AttentionImpl):
                     alibi_slopes=alibi_slopes,
                     softcap=logits_soft_cap,
                     block_table=decode_meta.block_tables,
-                    out=decode_output,
-                    fa_version=self.vllm_flash_attn_version,
-                    q_descale=layer._q_scale.expand(descale_shape),
-                    k_descale=layer._k_scale.expand(descale_shape),
-                    v_descale=layer._v_scale.expand(descale_shape),
+                    # out=decode_output,
+                    # fa_version=self.vllm_flash_attn_version,
+                    # q_descale=layer._q_scale.expand(descale_shape),
+                    # k_descale=layer._k_scale.expand(descale_shape),
+                    # v_descale=layer._v_scale.expand(descale_shape),
                 )
             else:
                 # Use flash_attn_with_kvcache for normal decoding.
@@ -896,7 +900,7 @@ class FlashAttentionImpl(AttentionImpl):
                     block_tables_arg,
                 ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
                 descale_shape = (seq_lens_arg.shape[0], key_cache.shape[-2])
-                flash_attn_with_kvcache(
+                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
                     q=decode_query.unsqueeze(1),
                     k_cache=key_cache,
                     v_cache=value_cache,
@@ -907,12 +911,10 @@ class FlashAttentionImpl(AttentionImpl):
                     window_size=window_size,
                     alibi_slopes=alibi_slopes,
                     softcap=logits_soft_cap,
-                    out=decode_output.unsqueeze(1),
-                    fa_version=self.vllm_flash_attn_version,
-                    q_descale=layer._q_scale.expand(descale_shape),
-                    k_descale=layer._k_scale.expand(descale_shape),
-                    v_descale=layer._v_scale.expand(descale_shape),
-                )
+                    # q_descale=layer._q_scale.expand(descale_shape),
+                    # k_descale=layer._k_scale.expand(descale_shape),
+                    # v_descale=layer._v_scale.expand(descale_shape),
+                ).squeeze(1)
         return output
 
 
diff --git a/vllm/attention/backends/flash_attn_pg.py b/vllm/attention/backends/flash_attn_pg.py
new file mode 100644
index 000000000..c376d1049
--- /dev/null
+++ b/vllm/attention/backends/flash_attn_pg.py
@@ -0,0 +1,1032 @@
+# SPDX-License-Identifier: Apache-2.0
+"""Attention layer with FlashAttention."""
+from collections import defaultdict
+from dataclasses import dataclass
+from itertools import accumulate
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type
+
+import torch
+
+from vllm import _custom_ops as ops
+# yapf conflicts with isort for this block
+# yapf: disable
+from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+                                              AttentionLayer,
+                                              AttentionMetadata,
+                                              AttentionMetadataBuilder,
+                                              AttentionType)
+from vllm.attention.backends.utils import (
+    PAD_SLOT_ID, CommonAttentionState, compute_slot_mapping,
+    compute_slot_mapping_start_idx, get_num_prefill_decode_query_kv_tokens,
+    get_seq_len_block_table_args, is_all_cross_attn_metadata_set,
+    is_all_encoder_attn_metadata_set, is_block_tables_empty)
+from vllm.envs import VLLM_FLASH_ATTN_VERSION
+from vllm.logger import init_logger
+from vllm.multimodal import MultiModalPlaceholderMap
+from vllm.platforms import current_platform
+from vllm.utils import async_tensor_h2d, make_tensor_with_pad
+#from vllm.vllm_flash_attn import (fa_version_unsupported_reason,
+#                                  flash_attn_varlen_func,
+#                                  flash_attn_with_kvcache,
+#                                  is_fa_version_supported)
+from vllm.attention.ops.paged_attn import (PagedAttention,
+                                           PagedAttentionMetadata)
+
+from flash_attn import (flash_attn_varlen_func,
+                        flash_attn_with_kvcache)
+
+def flash_attn_supports_fp8() -> bool:
+    return False
+
+if TYPE_CHECKING:
+    from vllm.worker.model_runner import (ModelInputForGPUBuilder,
+                                          ModelInputForGPUWithSamplingMetadata)
+
+logger = init_logger(__name__)
+
+
+class FlashAttentionBackend(AttentionBackend):
+
+    accept_output_buffer: bool = True
+
+    @staticmethod
+    def get_supported_head_sizes() -> List[int]:
+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
+
+    @staticmethod
+    def get_name() -> str:
+        return "FLASH_ATTN"
+
+    @staticmethod
+    def get_impl_cls() -> Type["FlashAttentionImpl"]:
+        return FlashAttentionImpl
+
+    @staticmethod
+    def get_metadata_cls() -> Type["AttentionMetadata"]:
+        return FlashAttentionMetadata
+
+    @staticmethod
+    def get_builder_cls() -> Type["FlashAttentionMetadataBuilder"]:
+        return FlashAttentionMetadataBuilder
+
+    @staticmethod
+    def get_state_cls() -> Type["CommonAttentionState"]:
+        return CommonAttentionState
+
+    @staticmethod
+    def get_kv_cache_shape(
+        num_blocks: int,
+        block_size: int,
+        num_kv_heads: int,
+        head_size: int,
+    ) -> Tuple[int, ...]:
+        """
+        if block_size % 16 != 0:
+            raise ValueError("Block size must be a multiple of 16.")
+        return (2, num_blocks, block_size, num_kv_heads, head_size)
+        """
+        return PagedAttention.get_kv_cache_shape(num_blocks, block_size,
+                                                 num_kv_heads, head_size)
+
+    @staticmethod
+    def swap_blocks(
+        src_kv_cache: torch.Tensor,
+        dst_kv_cache: torch.Tensor,
+        src_to_dst: torch.Tensor,
+    ) -> None:
+        """
+        src_key_cache = src_kv_cache[0]
+        dst_key_cache = dst_kv_cache[0]
+        ops.swap_blocks(src_key_cache, dst_key_cache, src_to_dst)
+        src_value_cache = src_kv_cache[1]
+        dst_value_cache = dst_kv_cache[1]
+        ops.swap_blocks(src_value_cache, dst_value_cache, src_to_dst)
+        """
+        PagedAttention.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)
+
+    @staticmethod
+    def copy_blocks(
+        kv_caches: List[torch.Tensor],
+        src_to_dists: torch.Tensor,
+    ) -> None:
+        """
+        key_caches = [kv_cache[0] for kv_cache in kv_caches]
+        value_caches = [kv_cache[1] for kv_cache in kv_caches]
+
+        ops.copy_blocks(key_caches, value_caches, src_to_dists)
+        """
+        PagedAttention.copy_blocks(kv_caches, src_to_dists)
+
+
+@dataclass
+#class FlashAttentionMetadata(AttentionMetadata):
+class FlashAttentionMetadata(AttentionMetadata, PagedAttentionMetadata):
+    """Metadata for FlashAttentionBackend.
+
+    NOTE: Any python object stored here is not updated when it is
+    cuda-graph replayed. If you have values that need to be changed
+    dynamically, it should be stored in tensor. The tensor has to be
+    updated from `CUDAGraphRunner.forward` API.
+    """
+    # (batch_size,). The sequence length per sequence. Sequence length means
+    # the computed tokens + new tokens None if it is a decoding.
+    seq_lens: Optional[List[int]]
+    # seq_lens stored as a tensor.
+    seq_lens_tensor: Optional[torch.Tensor]
+
+    # NOTE(sang): Definition of context_len, query_len, and seq_len.
+    # |---------- N-1 iteration --------|
+    # |---------------- N iteration ---------------------|
+    # |- tokenA -|......................|-- newTokens ---|
+    # |---------- context_len ----------|
+    # |-------------------- seq_len ---------------------|
+    #                                   |-- query_len ---|
+
+    # Maximum sequence length among prefill batch. 0 if there are decoding
+    # requests only.
+    max_prefill_seq_len: int
+    # Maximum sequence length among decode batch. 0 if there are prefill
+    # requests only.
+    max_decode_seq_len: int
+    # (batch_size,) A tensor of context lengths (tokens that are computed
+    # so far).
+    context_lens_tensor: Optional[torch.Tensor]
+
+    # (batch_size, max_blocks_per_seq).
+    # Block addresses per sequence. (Seq id -> list of physical block)
+    # E.g., [0, 1, 2] means tokens are stored in 0th, 1st, and 2nd blocks
+    # in the kv cache. Each block can contain up to block_size tokens.
+    # 2nd dimensions are padded up to max_blocks_per_seq if it is cuda-graph
+    # captured.
+    block_tables: Optional[torch.Tensor]
+
+    # Whether or not if cuda graph is enabled.
+    # Cuda-graph is currently enabled for decoding only.
+    # TODO(woosuk): Move `use_cuda_graph` out since it's unrelated to attention.
+
+    use_cuda_graph: bool
+
+    # Maximum query length in the batch.
+    max_query_len: Optional[int] = None
+
+    # Max number of query tokens among request in the batch.
+    max_decode_query_len: Optional[int] = None
+
+    # (batch_size + 1,). The cumulative subquery lengths of the sequences in
+    # the batch, used to index into subquery. E.g., if the subquery length
+    # is [4, 6], it is [0, 4, 10].
+    query_start_loc: Optional[torch.Tensor] = None
+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
+    # the batch, used to index into sequence. E.g., if the sequence length is
+    # [4, 6], it is [0, 4, 10].
+    seq_start_loc: Optional[torch.Tensor] = None
+
+    _cached_prefill_metadata: Optional["FlashAttentionMetadata"] = None
+    _cached_decode_metadata: Optional["FlashAttentionMetadata"] = None
+
+    # Begin encoder attn & enc/dec cross-attn fields...
+
+    # Encoder sequence lengths representation
+    encoder_seq_lens: Optional[List[int]] = None
+    encoder_seq_lens_tensor: Optional[torch.Tensor] = None
+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
+    # the batch, used to index into sequence. E.g., if the sequence length is
+    # [4, 6], it is [0, 4, 10].
+    encoder_seq_start_loc: Optional[torch.Tensor] = None
+    # Maximum sequence length among encoder sequences
+    max_encoder_seq_len: Optional[int] = None
+    # Number of tokens input to encoder
+    num_encoder_tokens: Optional[int] = None
+
+    # Cross-attention memory-mapping data structures: slot mapping
+    # and block tables
+    cross_slot_mapping: Optional[torch.Tensor] = None
+    cross_block_tables: Optional[torch.Tensor] = None
+
+    @property
+    def is_all_encoder_attn_metadata_set(self):
+        '''
+        All attention metadata required for encoder attention is set.
+        '''
+        return is_all_encoder_attn_metadata_set(self)
+
+    @property
+    def is_all_cross_attn_metadata_set(self):
+        '''
+        All attention metadata required for enc/dec cross-attention is set.
+
+        Superset of encoder attention required metadata.
+        '''
+        return is_all_cross_attn_metadata_set(self)
+
+    @property
+    def prefill_metadata(self) -> Optional["FlashAttentionMetadata"]:
+        if self.num_prefills == 0:
+            return None
+
+        if self._cached_prefill_metadata is not None:
+            return self._cached_prefill_metadata
+
+        assert ((self.seq_lens is not None)
+                or (self.encoder_seq_lens is not None))
+        assert ((self.seq_lens_tensor is not None)
+                or (self.encoder_seq_lens_tensor is not None))
+
+        # Compute some attn_metadata fields which default to None
+        query_start_loc = (None if self.query_start_loc is None else
+                           self.query_start_loc[:self.num_prefills + 1])
+        slot_mapping = (None if self.slot_mapping is None else
+                        self.slot_mapping[:self.num_prefill_tokens])
+        seq_lens = (None if self.seq_lens is None else
+                    self.seq_lens[:self.num_prefills])
+        seq_lens_tensor = (None if self.seq_lens_tensor is None else
+                           self.seq_lens_tensor[:self.num_prefills])
+        seq_start_loc = (None if self.seq_start_loc is None else
+                         self.seq_start_loc[:self.num_prefills + 1])
+        context_lens_tensor = (None if self.context_lens_tensor is None else
+                               self.context_lens_tensor[:self.num_prefills])
+        block_tables = (None if self.block_tables is None else
+                        self.block_tables[:self.num_prefills])
+
+        self._cached_prefill_metadata = FlashAttentionMetadata(
+            num_prefills=self.num_prefills,
+            num_prefill_tokens=self.num_prefill_tokens,
+            num_decode_tokens=0,
+            slot_mapping=slot_mapping,
+            multi_modal_placeholder_index_maps=self.
+            multi_modal_placeholder_index_maps,
+            enable_kv_scales_calculation=self.enable_kv_scales_calculation,
+            seq_lens=seq_lens,
+            seq_lens_tensor=seq_lens_tensor,
+            max_query_len=self.max_query_len,
+            max_prefill_seq_len=self.max_prefill_seq_len,
+            max_decode_query_len=0,
+            max_decode_seq_len=0,
+            query_start_loc=query_start_loc,
+            seq_start_loc=seq_start_loc,
+            context_lens_tensor=context_lens_tensor,
+            block_tables=block_tables,
+            use_cuda_graph=False,
+            # Begin encoder & cross attn fields below...
+            encoder_seq_lens=self.encoder_seq_lens,
+            encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
+            encoder_seq_start_loc=self.encoder_seq_start_loc,
+            max_encoder_seq_len=self.max_encoder_seq_len,
+            cross_slot_mapping=self.cross_slot_mapping,
+            cross_block_tables=self.cross_block_tables)
+        return self._cached_prefill_metadata
+
+    @property
+    def decode_metadata(self) -> Optional["FlashAttentionMetadata"]:
+        if self.num_decode_tokens == 0:
+            return None
+
+        if self._cached_decode_metadata is not None:
+            return self._cached_decode_metadata
+        assert ((self.seq_lens_tensor is not None)
+                or (self.encoder_seq_lens_tensor is not None))
+
+        # Compute some attn_metadata fields which default to None
+        slot_mapping = (None if self.slot_mapping is None else
+                        self.slot_mapping[self.num_prefill_tokens:])
+        seq_lens_tensor = (None if self.seq_lens_tensor is None else
+                           self.seq_lens_tensor[self.num_prefills:])
+        block_tables = (None if self.block_tables is None else
+                        self.block_tables[self.num_prefills:])
+
+        self._cached_decode_metadata = FlashAttentionMetadata(
+            num_prefills=0,
+            num_prefill_tokens=0,
+            num_decode_tokens=self.num_decode_tokens,
+            slot_mapping=slot_mapping,
+            multi_modal_placeholder_index_maps=None,
+            enable_kv_scales_calculation=True,
+            seq_lens=None,
+            seq_lens_tensor=seq_lens_tensor,
+            max_decode_query_len=self.max_decode_query_len,
+            max_query_len=self.max_query_len,
+            max_prefill_seq_len=0,
+            max_decode_seq_len=self.max_decode_seq_len,
+            # Batch may be composed of prefill|decodes, adjust query start
+            # indices to refer to the start of decodes. E.g.
+            # in tokens:[3 prefills|6 decodes], query_start_loc=[3,9] => [0,6].
+            query_start_loc=(self.query_start_loc[self.num_prefills:] -
+                             self.query_start_loc[self.num_prefills])
+            if self.query_start_loc is not None else None,
+            seq_start_loc=self.seq_start_loc[self.num_prefills:]
+            if self.seq_start_loc is not None else None,
+            context_lens_tensor=None,
+            block_tables=block_tables,
+            use_cuda_graph=self.use_cuda_graph,
+            # Begin encoder & cross attn fields below...
+            encoder_seq_lens=self.encoder_seq_lens,
+            encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
+            encoder_seq_start_loc=self.encoder_seq_start_loc,
+            max_encoder_seq_len=self.max_encoder_seq_len,
+            cross_slot_mapping=self.cross_slot_mapping,
+            cross_block_tables=self.cross_block_tables)
+        return self._cached_decode_metadata
+
+    def advance_step(self,
+                     model_input: "ModelInputForGPUWithSamplingMetadata",
+                     sampled_token_ids: Optional[torch.Tensor],
+                     block_size: int,
+                     num_seqs: int,
+                     num_queries: int,
+                     turn_prefills_into_decodes: bool = False):
+        """
+        Update metadata in-place to advance one decode step.
+        """
+        # When using cudagraph, the num_seqs is padded to the next captured
+        # batch sized, but num_queries tracks the actual number of requests in
+        # the batch. For --enforce-eager mode, num_seqs == num_queries
+        if num_seqs != num_queries:
+            assert num_seqs > num_queries
+            assert self.use_cuda_graph
+
+        if turn_prefills_into_decodes:
+            # When Mutli-Step is enabled with Chunked-Prefill, prefills and
+            # decodes are scheduled together. In the first step, all the
+            # prefills turn into decodes. This update reflects that
+            # conversion.
+            assert self.num_decode_tokens + self.num_prefills == num_seqs
+            self.num_decode_tokens += self.num_prefills
+            self.num_prefills = 0
+            self.num_prefill_tokens = 0
+            self.max_prefill_seq_len = 0
+            self.max_query_len = 1
+
+            self.slot_mapping = self.slot_mapping[:num_seqs]
+        else:
+            assert self.seq_lens is not None
+            assert self.max_decode_seq_len == max(self.seq_lens)
+
+        assert self.num_prefills == 0
+        assert self.num_prefill_tokens == 0
+        assert self.num_decode_tokens == num_seqs
+        assert self.slot_mapping.shape == (num_seqs, )
+
+        assert self.seq_lens is not None
+        assert len(self.seq_lens) == num_seqs
+        assert self.seq_lens_tensor is not None
+        assert self.seq_lens_tensor.shape == (num_seqs, )
+        assert self.max_query_len == 1
+        assert self.max_prefill_seq_len == 0
+
+        assert self.query_start_loc is not None
+        assert self.query_start_loc.shape == (num_queries + 1, )
+        assert self.seq_start_loc is not None
+        assert self.seq_start_loc.shape == (num_seqs + 1, )
+
+        assert self.context_lens_tensor is not None
+        assert self.context_lens_tensor.shape == (num_queries, )
+
+        assert self.block_tables is not None
+        assert self.block_tables.shape[0] == num_seqs
+
+        # Update query lengths. Note that we update only queries and not seqs,
+        # since tensors may be padded due to captured cuda graph batch size
+        for i in range(num_queries):
+            self.seq_lens[i] += 1
+        self.max_decode_seq_len = max(self.seq_lens)
+
+        ops.advance_step_flashattn(num_seqs=num_seqs,
+                                   num_queries=num_queries,
+                                   block_size=block_size,
+                                   input_tokens=model_input.input_tokens,
+                                   sampled_token_ids=sampled_token_ids,
+                                   input_positions=model_input.input_positions,
+                                   seq_lens=self.seq_lens_tensor,
+                                   slot_mapping=self.slot_mapping,
+                                   block_tables=self.block_tables)
+
+
+class FlashAttentionMetadataBuilder(
+        AttentionMetadataBuilder[FlashAttentionMetadata]):
+
+    def __init__(self, input_builder: "ModelInputForGPUBuilder"):
+        self.input_builder = input_builder
+        self.runner = input_builder.runner
+        self.sliding_window = input_builder.sliding_window
+        self.block_size = input_builder.block_size
+
+    def prepare(self):
+        self.slot_mapping: List[int] = []
+        self.prefill_seq_lens: List[int] = []
+        self.context_lens: List[int] = []
+        self.block_tables: List[List[int]] = []
+        self.curr_seq_lens: List[int] = []
+        self.multimodal_placeholder_maps: Dict[
+            str,
+            MultiModalPlaceholderMap] = defaultdict(MultiModalPlaceholderMap)
+        self.num_prefills = 0
+        self.num_prefill_tokens = 0
+        self.num_decode_tokens = 0
+        self.has_prefix_cache_hit = False
+
+    def _add_seq_group(
+            self, inter_data: "ModelInputForGPUBuilder.InterDataForSeqGroup",
+            chunked_prefill_enabled: bool, prefix_cache_hit: bool):
+        """Add a sequence group to the metadata. Specifically update/append
+        1. context length.
+        2. block table.
+        3. slot mapping.
+        """
+        is_prompt = inter_data.is_prompt
+        block_tables = inter_data.block_tables
+
+        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,
+             curr_sliding_window_block) in zip(
+                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],
+                 inter_data.orig_seq_lens, inter_data.seq_lens,
+                 inter_data.query_lens, inter_data.context_lens,
+                 inter_data.curr_sliding_window_blocks):
+            self.context_lens.append(context_len)
+
+            if is_prompt:
+                mm_maps = inter_data.multi_modal_placeholder_maps
+                if mm_maps:
+                    for modality, placeholders in mm_maps.items():
+                        self.multimodal_placeholder_maps[modality].extend(
+                            placeholders)
+
+                self.num_prefills += 1
+                self.num_prefill_tokens += token_len
+                self.prefill_seq_lens.append(seq_len)
+            else:
+                self.num_decode_tokens += query_len
+                self.curr_seq_lens.append(curr_seq_len)
+
+            # Compute block table.
+            # TODO(sang): Combine chunked prefill and prefix caching by
+            # only allowing multiple of block_size chunk size.
+            # NOTE: This only works for oooooooxxx style attention.
+            block_table = []
+            if prefix_cache_hit:
+                # NOTE(woosuk): For flash-attn, the block table should
+                # include the entries for the incoming prefill tokens.
+                block_table = block_tables[seq_id]
+            elif ((chunked_prefill_enabled or not is_prompt)
+                  and block_tables is not None):
+                if curr_sliding_window_block == 0:
+                    block_table = block_tables[seq_id]
+                else:
+                    block_table = block_tables[seq_id][
+                        -curr_sliding_window_block:]
+            self.block_tables.append(block_table)
+
+            # Compute slot mapping.
+            is_profile_run = is_block_tables_empty(block_tables)
+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,
+                                                       context_len,
+                                                       self.sliding_window)
+            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,
+                                 seq_len, context_len, start_idx,
+                                 self.block_size, inter_data.block_tables)
+
+    def _get_graph_runner_block_tables(
+            self, num_seqs: int,
+            block_tables: List[List[int]]) -> torch.Tensor:
+        # The shape of graph_block_tables is
+        # [max batch size, max context len // block size].
+        max_batch_size, max_blocks = self.runner.graph_block_tables.shape
+        assert max_batch_size >= num_seqs
+
+        graph_block_tables = self.runner.graph_block_tables[:num_seqs]
+        for i, block_table in enumerate(block_tables):
+            if block_table:
+                num_blocks = len(block_table)
+                if num_blocks <= max_blocks:
+                    graph_block_tables[i, :num_blocks] = block_table
+                else:
+                    # It may be possible to have more blocks allocated due
+                    # to lookahead slots of multi-step, however, they are
+                    # not used anyway, so can be safely ignored.
+                    graph_block_tables[
+                        i, :max_blocks] = block_table[:max_blocks]
+
+        return torch.from_numpy(graph_block_tables).to(
+            device=self.runner.device, non_blocking=True)
+
+    def build(self, seq_lens: List[int], query_lens: List[int],
+              cuda_graph_pad_size: int, batch_size: int):
+        """Build attention metadata with on-device tensors.
+
+        Args:
+            seq_lens: The maybe padded sequence lengths of the input sequences.
+            query_lens: The query lengths of the input sequences.
+            cuda_graph_pad_size: The padding size for cuda graph.
+                                 -1 if cuda graph is not used.
+            batch_size: The maybe padded batch size.
+        """
+        prefix_cache_hit = any([
+            inter_data.prefix_cache_hit
+            for inter_data in self.input_builder.inter_data_list
+        ])
+        for inter_data in self.input_builder.inter_data_list:
+            self._add_seq_group(inter_data,
+                                self.input_builder.chunked_prefill_enabled,
+                                prefix_cache_hit)
+
+        device = self.runner.device
+        use_captured_graph = cuda_graph_pad_size != -1
+
+        max_query_len = max(query_lens)
+        decode_query_lens = query_lens[self.num_prefills:]
+        if len(decode_query_lens) > 0:
+            max_decode_query_len = max(decode_query_lens)
+        else:
+            max_decode_query_len = 1
+        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)
+        max_decode_seq_len = max(self.curr_seq_lens, default=0)
+        num_decode_tokens = self.num_decode_tokens
+        query_start_loc = list(accumulate(query_lens, initial=0))
+        seq_start_loc = list(accumulate(seq_lens, initial=0))
+
+        num_seqs = len(seq_lens)
+        if use_captured_graph:
+            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)
+            self.block_tables.extend([] * cuda_graph_pad_size)
+            num_decode_tokens = batch_size - self.num_prefill_tokens
+            block_tables = self._get_graph_runner_block_tables(
+                num_seqs, self.block_tables)
+        else:
+            block_tables = make_tensor_with_pad(
+                self.block_tables,
+                pad=0,
+                dtype=torch.int,
+                device=device,
+            )
+        assert max_query_len > 0, ("query_lens: {}".format(query_lens))
+
+        assert device is not None
+        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,
+                                               device, self.runner.pin_memory)
+        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,
+                                           self.runner.pin_memory)
+        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
+                                               device, self.runner.pin_memory)
+        query_start_loc_tensor = async_tensor_h2d(query_start_loc, torch.int32,
+                                                  device,
+                                                  self.runner.pin_memory)
+        seq_start_loc_tensor = async_tensor_h2d(seq_start_loc, torch.int32,
+                                                device, self.runner.pin_memory)
+        placeholder_index_maps = {
+            modality: placeholder_map.index_map()
+            for modality, placeholder_map in
+            self.multimodal_placeholder_maps.items()
+        }
+
+        return FlashAttentionMetadata(
+            num_prefills=self.num_prefills,
+            slot_mapping=slot_mapping_tensor,
+            num_prefill_tokens=self.num_prefill_tokens,
+            num_decode_tokens=num_decode_tokens,
+            seq_lens=seq_lens,
+            multi_modal_placeholder_index_maps=placeholder_index_maps,
+            enable_kv_scales_calculation=True,
+            seq_lens_tensor=seq_lens_tensor,
+            max_query_len=max_query_len,
+            max_decode_query_len=max_decode_query_len,
+            max_prefill_seq_len=max_prefill_seq_len,
+            max_decode_seq_len=max_decode_seq_len,
+            query_start_loc=query_start_loc_tensor,
+            seq_start_loc=seq_start_loc_tensor,
+            context_lens_tensor=context_lens_tensor,
+            block_tables=block_tables,
+            use_cuda_graph=use_captured_graph,
+        )
+
+
+class FlashAttentionImpl(AttentionImpl):
+    """
+    If the input tensors contain prompt tokens, the layout is as follows:
+    |<--------------- num_prefill_tokens ----------------->|	
+    |<--prefill_0-->|<--prefill_1-->|...|<--prefill_N-1--->|
+
+    Otherwise, the layout is as follows:	
+    |<----------------- num_decode_tokens ------------------>|	
+    |<--decode_0-->|..........|<--decode_M-1-->|<--padding-->|
+
+    Generation tokens can contain padding when cuda-graph is used.
+    Currently, prompt tokens don't contain any padding.
+
+    The prompts might have different lengths, while the generation tokens
+    always have length 1.
+
+    If chunked prefill is enabled, prefill tokens and decode tokens can be
+    batched together in a flattened 1D query.
+
+    |<----- num_prefill_tokens ---->|<------- num_decode_tokens --------->|
+    |<-prefill_0->|...|<-prefill_N-1->|<--decode_0-->|...|<--decode_M-1-->|
+
+    Currently, cuda graph is disabled for chunked prefill, meaning there's no
+    padding between prefill and decode tokens.
+    """
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: int,
+        alibi_slopes: Optional[List[float]],
+        sliding_window: Optional[int],
+        kv_cache_dtype: str,
+        blocksparse_params: Optional[Dict[str, Any]] = None,
+        logits_soft_cap: Optional[float] = None,
+        attn_type: str = AttentionType.DECODER,
+    ) -> None:
+        if blocksparse_params is not None:
+            raise ValueError(
+                "FlashAttention does not support block-sparse attention.")
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = float(scale)
+        self.num_kv_heads = num_kv_heads
+        if alibi_slopes is not None:
+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)
+        self.alibi_slopes = alibi_slopes
+        self.sliding_window = ((sliding_window - 1,
+                                0) if sliding_window is not None else (-1, -1))
+        self.kv_cache_dtype = kv_cache_dtype
+        if logits_soft_cap is None:
+            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.
+            logits_soft_cap = 0
+        self.logits_soft_cap = logits_soft_cap
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+        support_head_sizes = FlashAttentionBackend.get_supported_head_sizes()
+        if head_size not in support_head_sizes:
+            raise ValueError(
+                f"Head size {head_size} is not supported by FlashAttention. "
+                f"Supported head sizes are: {support_head_sizes}.")
+        self.attn_type = attn_type
+
+        # if hopper default to FA3, otherwise stick to FA2 for now
+        # TODO(lucas): profile FA3 on ampere to see if it makes sense to
+        #  use FA3 as default for both
+        #if current_platform.get_device_capability()[0] >= 9:
+        #    self.fa_version = 3 if is_fa_version_supported(3) else 2
+        #else:
+        #    self.fa_version = 2
+
+        #if VLLM_FLASH_ATTN_VERSION is not None:
+        #    assert VLLM_FLASH_ATTN_VERSION in [2, 3]
+        #    self.fa_version = VLLM_FLASH_ATTN_VERSION
+
+        #if not is_fa_version_supported(self.fa_version):
+        #    logger.error("Cannot use FA version %d is not supported due to %s",
+        #                 self.fa_version,
+        #                 fa_version_unsupported_reason(self.fa_version))
+
+        #assert is_fa_version_supported(self.fa_version)
+
+    def forward(
+        self,
+        layer: AttentionLayer,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: FlashAttentionMetadata,
+        output: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """Forward pass with FlashAttention.
+
+        Args:
+            query: shape = [num_tokens, num_heads, head_size]
+            key: shape = [num_tokens, num_kv_heads, head_size]
+            value: shape = [num_tokens, num_kv_heads, head_size]
+            output: shape = [num_tokens, num_heads, head_size]
+            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
+                NOTE: kv_cache will be an empty tensor with shape [0]
+                for profiling run.
+            attn_metadata: Metadata for attention.
+        NOTE: It in-place updates the output tensor.
+        """
+        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.
+        assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0, (
+            "key/v_scale is not supported in FlashAttention.")
+
+        assert output is not None, "Output tensor must be provided."
+        k_scale = layer._k_scale
+        v_scale = layer._v_scale
+
+        attn_type = self.attn_type
+        if (attn_type == AttentionType.ENCODER
+                and (not attn_metadata.is_all_encoder_attn_metadata_set)):
+            raise AttributeError("Encoder attention requires setting "
+                                 "encoder metadata attributes.")
+        elif (attn_type == AttentionType.ENCODER_DECODER
+              and (not attn_metadata.is_all_cross_attn_metadata_set)):
+            raise AttributeError("Encoder/decoder cross-attention "
+                                 "requires setting cross-attention "
+                                 "metadata attributes.")
+
+        kv_cache_dtype: str = self.kv_cache_dtype
+        softmax_scale: float = self.scale
+        window_size = self.sliding_window
+        alibi_slopes: Optional[torch.Tensor] = self.alibi_slopes
+        logits_soft_cap: Optional[float] = self.logits_soft_cap
+
+        if kv_cache.numel() > 0:
+            #key_cache = kv_cache[0]
+            #value_cache = kv_cache[1]
+            key_cache, value_cache = PagedAttention.split_kv_cache(
+                kv_cache, self.num_kv_heads, self.head_size)
+            # We skip updating the KV cache under two conditions:
+            #  a. When the Attention Type is ENCODER. In this phase, we compute
+            #     only the encoder attention without updating the cache.
+            #  b. When both Key and Value are None. This occurs during
+            #     cross-attention computation in the decoding phase, where the
+            #     KV cache is already populated with the cross-attention
+            #     tensor. Thus, we skip cache updates during this time.
+            if (attn_type != AttentionType.ENCODER) and (key is not None) and (
+                    value is not None):
+                if attn_type == AttentionType.ENCODER_DECODER:
+                    # Update cross-attention KV cache (prefill-only)
+                    updated_slot_mapping = attn_metadata.cross_slot_mapping
+                else:
+                    # Update self-attention KV cache (prefill/decode)
+                    updated_slot_mapping = attn_metadata.slot_mapping
+
+                # Reshape the input keys and values and store them in the cache.
+                # If kv_cache is not provided, the new key and value tensors are
+                # not cached. This happens during the initial memory
+                # profiling run.
+                #torch.ops._C_cache_ops.reshape_and_cache_flash(
+                #    key,
+                #    value,
+                #    kv_cache[0],
+                #    kv_cache[1],
+                #    updated_slot_mapping.flatten(),  # type: ignore[union-attr]
+                #    kv_cache_dtype,
+                #    layer._k_scale,
+                #    layer._v_scale,
+                #)
+                PagedAttention.write_to_paged_cache(key, value, key_cache,
+                                                value_cache,
+                                                attn_metadata.slot_mapping,
+                                                self.kv_cache_dtype, k_scale, v_scale)
+
+        (num_prefill_query_tokens, num_prefill_kv_tokens,
+        num_decode_query_tokens) = \
+            get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)
+        decode_query = query[num_prefill_query_tokens:]
+        decode_output = output[num_prefill_query_tokens:]
+        # QKV for prefill.
+        query = query[:num_prefill_query_tokens]
+        prefill_output = output[:num_prefill_query_tokens]
+        assert query.shape[0] == num_prefill_query_tokens
+        assert decode_query.shape[0] == num_decode_query_tokens
+
+        if prefill_meta := attn_metadata.prefill_metadata:
+            # Prompt run.
+            if (kv_cache.numel() == 0 or prefill_meta.block_tables is None
+                    or prefill_meta.block_tables.numel() == 0):
+                # normal attention
+                # When block_tables are not filled, it means q and k are the
+                # prompt, and they have the same length.
+                q_seq_start_loc, q_seq_len, k_seq_start_loc, k_seq_len = \
+                    _get_query_key_seq_metadata(prefill_meta, True, attn_type)
+
+                key = key[:num_prefill_kv_tokens]
+                value = value[:num_prefill_kv_tokens]
+
+                #flash_attn_varlen_func(
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
+                    q=query,
+                    k=key,
+                    v=value,
+                    cu_seqlens_q=q_seq_start_loc,
+                    cu_seqlens_k=k_seq_start_loc,
+                    max_seqlen_q=q_seq_len,
+                    max_seqlen_k=k_seq_len,
+                    softmax_scale=softmax_scale,
+                    causal=_get_causal_option(attn_type),
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    softcap=logits_soft_cap,
+                    #out=prefill_output,
+                    #fa_version=self.fa_version,
+                )
+            else:
+                # prefix-enabled attention
+                assert attn_type == AttentionType.DECODER, (
+                    "Only decoder-only models support prefix caching")
+                assert prefill_meta.seq_lens is not None
+                max_seq_len = max(prefill_meta.seq_lens)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads, self.head_size//(32 // kv_cache.element_size()), -1, 2, 16 // kv_cache.element_size())
+                key_cache = key_cache.permute(0,1,2,4,3,5)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads,self.head_size//(16 // kv_cache.element_size()),-1,16 // kv_cache.element_size())
+                value_cache =  value_cache.permute(0,1,3,2)
+                #flash_attn_varlen_func(  # noqa
+                """
+                output[:num_prefill_query_tokens] = flash_attn_varlen_func(
+                    q=query,
+                    k=key_cache,
+                    v=value_cache,
+                    cu_seqlens_q=prefill_meta.query_start_loc,
+                    max_seqlen_q=prefill_meta.max_query_len,
+                    seqused_k=prefill_meta.seq_lens_tensor,
+                    max_seqlen_k=max_seq_len,
+                    softmax_scale=softmax_scale,
+                    causal=True,
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    block_table=prefill_meta.block_tables,
+                    softcap=logits_soft_cap,
+                    #out=prefill_output,
+                    #fa_version=self.fa_version,
+                )
+                """
+                output[:num_prefill_query_tokens] = PagedAttention.forward_prefix(
+                    query,
+                    key,
+                    value,
+                    self.kv_cache_dtype,
+                    key_cache,
+                    value_cache,
+                    attn_metadata.block_tables,
+                    attn_metadata.query_start_loc,
+                    attn_metadata.seq_lens_tensor,
+                    attn_metadata.context_lens_tensor,
+                    attn_metadata.max_query_len,
+                    self.alibi_slopes,
+                    self.logits_soft_cap,
+                    k_scale,
+                    v_scale,
+                )
+                value_cache =  value_cache.permute(0,1,3,2)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads,self.head_size//(32 // kv_cache.element_size()),2,-1,16 // kv_cache.element_size())
+                key_cache = key_cache.permute(0,1,2,4,3,5)
+                key_cache = key_cache.reshape(kv_cache.shape[1],self.num_kv_heads, self.head_size//(32 // kv_cache.element_size()), -1, 32 // kv_cache.element_size())
+
+        if decode_meta := attn_metadata.decode_metadata:
+            # Decoding run.
+            # Use flash_attn_varlen_func kernel for speculative decoding
+            # because different queries might have different lengths.
+
+            assert decode_meta.max_decode_query_len is not None
+            # use only for actual varlen decoding
+            if decode_meta.max_decode_query_len > 1:
+                assert attn_type == AttentionType.DECODER, (
+                    "Only decoder-only models support max_decode_query_len > 1"
+                )
+                """
+                output[num_prefill_query_tokens:] = flash_attn_varlen_func(
+                    q=decode_query,
+                    k=key_cache,
+                    v=value_cache,
+                    cu_seqlens_q=decode_meta.query_start_loc,
+                    max_seqlen_q=decode_meta.max_decode_query_len,
+                    seqused_k=decode_meta.seq_lens_tensor,
+                    max_seqlen_k=decode_meta.max_decode_seq_len,
+                    softmax_scale=softmax_scale,
+                    causal=True,
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    softcap=logits_soft_cap,
+                    block_table=decode_meta.block_tables,
+                    #out=decode_output,
+                    #fa_version=self.fa_version,
+                )
+                """
+                output[num_prefill_query_tokens:] = PagedAttention.forward_decode(
+                    decode_query,
+                    key_cache,
+                    value_cache,
+                    block_tables=attn_metadata.block_tables,
+                    seq_lens=attn_metadata.seq_lens_tensor,    #attn_metadata.context_lens
+                    max_seq_len=attn_metadata.max_decode_seq_len,  #attn_metadata.max_context_len
+                    kv_cache_dtype='auto',
+                    num_kv_heads=self.num_kv_heads,
+                    scale=self.scale,
+                    alibi_slopes=self.alibi_slopes,
+                    k_scale=k_scale,
+                    v_scale=v_scale,
+                )
+            else:
+                # Use flash_attn_with_kvcache for normal decoding.
+                (
+                    seq_lens_arg,
+                    _,
+                    block_tables_arg,
+                ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
+                """
+                output[num_prefill_query_tokens:] = flash_attn_with_kvcache(
+                    q=decode_query.unsqueeze(1),
+                    k_cache=key_cache,
+                    v_cache=value_cache,
+                    block_table=block_tables_arg,
+                    cache_seqlens=seq_lens_arg,
+                    softmax_scale=softmax_scale,
+                    causal=True,
+                    window_size=window_size,
+                    alibi_slopes=alibi_slopes,
+                    softcap=logits_soft_cap,
+                    #out=decode_output.unsqueeze(1),
+                    #fa_version=self.fa_version,
+                ).squeeze(1)
+                """
+                output[num_prefill_query_tokens:] = PagedAttention.forward_decode(
+                    decode_query,
+                    key_cache,
+                    value_cache,
+                    block_tables=attn_metadata.block_tables,
+                    seq_lens=attn_metadata.seq_lens_tensor,    #attn_metadata.context_lens
+                    max_seq_len=attn_metadata.max_decode_seq_len,  #attn_metadata.max_context_len
+                    kv_cache_dtype='auto',
+                    num_kv_heads=self.num_kv_heads,
+                    scale=self.scale,
+                    alibi_slopes=self.alibi_slopes,
+                    k_scale=k_scale,
+                    v_scale=v_scale,
+                )
+        return output
+
+
+def _get_query_key_seq_metadata(
+    attn_metadata,
+    is_prompt: bool,
+    attn_type: str,
+) -> tuple:
+    """
+    Returns sequence metadata for key and query based on the specified 
+    attention type and whether input is a prompt.
+
+    This function computes the starting locations and maximum sequence lengths 
+    for key and query sequences for different attention types.
+
+    Args:
+        attn_metadata: The attention metadata object
+        is_prompt (bool): A flag indicating if the input is a prompt
+        attn_type (AttentionType): The type of attention being used.
+
+    Returns:
+        tuple: A tuple containing four integers:
+            - Starting location for the query sequence.
+            - Maximum sequence length for the query sequence.
+            - Starting location for the key sequence.
+            - Maximum sequence length for the key sequence.
+
+    Raises:
+        AttributeError: If an invalid attention type is provided.
+    """
+    if attn_type == AttentionType.DECODER:
+        # Decoder self-attention
+        # Choose max_seq_len based on whether we are in prompt_run
+        if is_prompt:
+            max_seq_len = attn_metadata.max_prefill_seq_len
+        else:
+            max_seq_len = attn_metadata.max_decode_seq_len
+        return (attn_metadata.seq_start_loc, max_seq_len,
+                attn_metadata.seq_start_loc, max_seq_len)
+
+    elif attn_type == AttentionType.ENCODER_DECODER:
+        # This is cross attention between the where the key
+        # is the precomputed encoder attention and query
+        # is the input sequence.
+        # Choose query max length based on whether it is prompt
+        # or not.
+        if is_prompt:
+            max_seq_len = attn_metadata.max_prefill_seq_len
+        else:
+            max_seq_len = attn_metadata.max_decode_seq_len
+        return (attn_metadata.seq_start_loc, max_seq_len,
+                attn_metadata.encoder_seq_start_loc,
+                attn_metadata.max_encoder_seq_len)
+    elif attn_type == AttentionType.ENCODER:
+        # For encoder attention both the query and the key are same i.e the
+        # encoder sequence.
+        return (attn_metadata.encoder_seq_start_loc,
+                attn_metadata.max_encoder_seq_len,
+                attn_metadata.encoder_seq_start_loc,
+                attn_metadata.max_encoder_seq_len)
+    elif attn_type == AttentionType.ENCODER_ONLY:
+        assert is_prompt, "Should not have decode for encoder only model."
+        return (attn_metadata.seq_start_loc, attn_metadata.max_prefill_seq_len,
+                attn_metadata.seq_start_loc, attn_metadata.max_prefill_seq_len)
+    else:
+        raise AttributeError(f"Invalid attention type {str(attn_type)}")
+
+
+def _get_causal_option(attn_type: str) -> bool:
+    """
+    Determine whether the given attention type is suitable for causal 
+    attention mechanisms.
+
+    Args:
+        attn_type (AttentionType): The type of attention being evaluated
+
+    Returns:
+        bool: Returns `True` if the attention type is suitable for causal 
+        attention (i.e., not encoder, encoder-only, or encoder-decoder), 
+        otherwise returns `False`.
+    """
+    return not (attn_type == AttentionType.ENCODER
+                or attn_type == AttentionType.ENCODER_ONLY
+                or attn_type == AttentionType.ENCODER_DECODER)
diff --git a/vllm/attention/backends/flashinfer.py b/vllm/attention/backends/flashinfer.py
index d92177d58..c25f45f74 100644
--- a/vllm/attention/backends/flashinfer.py
+++ b/vllm/attention/backends/flashinfer.py
@@ -14,7 +14,8 @@ try:
     from flashinfer.decode import CUDAGraphBatchDecodeWithPagedKVCacheWrapper
     from flashinfer.prefill import BatchPrefillWithPagedKVCacheWrapper
 
-    from vllm.vllm_flash_attn import flash_attn_varlen_func
+    # from vllm.vllm_flash_attn import flash_attn_varlen_func
+    from flash_attn import flash_attn_varlen_func
     FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024
 except ImportError:
     # Avoid turning these types into variables during type checking
diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py
index 382a9a6d4..5ae7989b1 100644
--- a/vllm/attention/backends/mla/common.py
+++ b/vllm/attention/backends/mla/common.py
@@ -910,7 +910,12 @@ class MLACommonMetadataBuilder(AttentionMetadataBuilder[T], Generic[T]):
                                                device, self.runner.pin_memory)
         seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,
                                            self.runner.pin_memory)
-        input_positions = async_tensor_h2d(self.input_positions, torch.long,
+        # aligned according to batch_size for advance_step_flashattn used
+        input_positions_list = self.input_positions
+        for _ in range(len(self.input_positions), batch_size):
+            input_positions_list.append(0)        
+
+        input_positions = async_tensor_h2d(input_positions_list, torch.long,
                                            device, self.runner.pin_memory)
         slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
                                                device, self.runner.pin_memory)
@@ -1064,19 +1069,22 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
         # and the one from vllm_flash_attn. The former is used on RoCM and the
         # latter has an additional parameter to control FA2 vs FA3
         self.flash_attn_varlen_func = flash_attn_varlen_func
+        """
         self.vllm_flash_attn_version = get_flash_attn_version()
         if self.vllm_flash_attn_version is not None:
             self.flash_attn_varlen_func = \
                 functools.partial(flash_attn_varlen_func,
                                   fa_version=self.vllm_flash_attn_version)
+        """
 
         # For MLA the v head dim is smaller than qk head dim so we pad out
         # v with 0s to match the qk head dim for attention backends that do
         # not support different headdims
         # We don't need to pad V if we are on a hopper system with FA3
-        self._pad_v = self.vllm_flash_attn_version is None or not (
-            self.vllm_flash_attn_version == 3
-            and current_platform.get_device_capability()[0] == 9)
+        self._pad_v = True 
+        # self.vllm_flash_attn_version is None or not (
+            # self.vllm_flash_attn_version == 3
+            # and current_platform.get_device_capability()[0] == 9)
 
     def _flash_attn_varlen_diff_headdims(self, q, k, v, softmax_scale,
                                          return_softmax_lse, **kwargs):
@@ -1186,7 +1194,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
                 del eye
                 # standardize to (output, input)
                 return dequant_weights.T
-            return layer.weight
+            return layer.weight if not envs.MACA_VLLM_USE_TN_2_NN else layer.weight.T
 
         # we currently do not have quantized bmm's which are needed for
         # `W_UV` and `W_UK_T`, we we just store fp16/bf16 copies and perform
diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
index 61e5c76d9..4f9f71785 100644
--- a/vllm/attention/backends/triton_mla.py
+++ b/vllm/attention/backends/triton_mla.py
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0
 
+from dataclasses import dataclass
 from typing import Any, Dict, List, Optional, Type
 
 import torch
@@ -11,6 +12,43 @@ from vllm.attention.backends.mla.common import (MLACommonBackend,
                                                 MLACommonMetadata)
 from vllm.attention.ops.triton_decode_attention import decode_attention_fwd
 
+import json
+import os
+
+# TODO: Configure environment variables temporarily. New versions do not need to be configured
+os.environ['TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP'] = '1'
+os.environ['TRITON_ENABLE_MACA_CHAIN_DOT_OPT'] = '1'
+
+def load_config():
+    # Load JSON data from the file
+    json_path = config_file_path = os.path.join(
+        os.path.dirname(os.path.realpath(__file__)), "configs", "tp8_merge.json")
+    with open(json_path, 'r') as file:
+        data = json.load(file)
+    return data
+
+JSON_DATA = load_config()
+
+def find_best_mla_para(json_data, batch_size, input_len, tp_size):
+    best_match = None
+    best_batch_size_diff = float('inf')
+    best_input_len_diff = float('inf')
+    
+    for entry in json_data:
+        if entry["BS"] == batch_size and entry["L"] == input_len:
+            return entry["num_kv_splits"], entry['num_stages']
+        batch_size_diff = abs(entry["BS"] - batch_size)
+        input_len_diff = abs(entry["L"] - input_len)
+        
+        # Check if this is a better match than the current best match
+        if batch_size_diff < best_batch_size_diff or (batch_size_diff == best_batch_size_diff and input_len_diff < best_input_len_diff):
+            best_match = entry
+            best_batch_size_diff = batch_size_diff
+            best_input_len_diff = input_len_diff
+    
+    # If a match was found, return the best_kv_splits, otherwise return None
+    return best_match["num_kv_splits"],best_match["num_stages"]
+
 
 class TritonMLABackend(MLACommonBackend):
 
@@ -21,9 +59,38 @@ class TritonMLABackend(MLACommonBackend):
     @staticmethod
     def get_impl_cls() -> Type["TritonMLAImpl"]:
         return TritonMLAImpl
-
-
-class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
+    
+    @staticmethod
+    def get_metadata_cls() -> Type["TritonMLAMetadata"]:
+        return TritonMLAMetadata
+
+@dataclass
+class TritonMLAMetadata(MLACommonMetadata):
+    num_kv_splits: int = 4  # TODO: heuristic
+    num_stages: int = 1
+
+    @property
+    def decode_metadata(self):
+        if self.num_decode_tokens == 0:
+            return None
+        if self._cached_decode_metadata is not None:
+            return self._cached_decode_metadata
+        
+        decode_metadata = super().decode_metadata
+        
+        if decode_metadata is not None:
+            if decode_metadata.seq_lens_tensor is not None:
+                batch = decode_metadata.seq_lens_tensor.shape[0]
+                max_seq_len = int(decode_metadata.seq_lens_tensor.max())
+                num_kv_splits, num_stages = find_best_mla_para(JSON_DATA, batch, max_seq_len, 8)
+            else:
+                num_kv_splits = self.num_kv_splits
+                num_stages = self.num_stages
+            decode_metadata.num_kv_splits = num_kv_splits
+            decode_metadata.num_stages = num_stages
+        return decode_metadata
+
+class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
 
     def __init__(
             self,
@@ -68,7 +135,7 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
         q_nope: torch.Tensor,
         q_pe: torch.Tensor,
         kv_c_and_k_pe_cache: torch.Tensor,
-        attn_metadata: MLACommonMetadata,
+        attn_metadata: TritonMLAMetadata,
     ) -> torch.Tensor:
         assert kv_c_and_k_pe_cache.numel() > 0
 
@@ -83,14 +150,12 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
                         dtype=q.dtype,
                         device=q.device)
 
-        num_kv_splits = 4  # TODO: heuristic
-
         # TODO(lucas) Allocate ahead of time
         attn_logits = torch.empty(
             (
                 B,
                 self.num_heads,
-                num_kv_splits,
+                decode_meta.num_kv_splits,
                 # NOTE(lucas) idk why the +1 is here but sglang has it so we
                 # just mirror that
                 self.kv_lora_rank + 1,
@@ -108,6 +173,6 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
         decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,
                              decode_meta.block_tables,
                              decode_meta.seq_lens_tensor, attn_logits,
-                             num_kv_splits, self.scale, PAGE_SIZE)
+                             decode_meta.num_kv_splits, decode_meta.num_stages, self.scale, PAGE_SIZE)
 
         return self._v_up_proj_and_o_proj(o)
diff --git a/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py b/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
index 71caf3cba..ac23b18c3 100644
--- a/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
+++ b/vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
@@ -117,7 +117,7 @@ def blocksparse_flash_attn_varlen_fwd(
                          q_block_size),  # smaller for decoding
         EVEN_D=block_d == head_size,
         num_warps=1 if decoding_only else 4,
-        num_stages=3)
+        num_stages=1)
 
     return out
 
diff --git a/vllm/attention/ops/blocksparse_attention/interface.py b/vllm/attention/ops/blocksparse_attention/interface.py
index 6ab69ea5b..e0453729e 100644
--- a/vllm/attention/ops/blocksparse_attention/interface.py
+++ b/vllm/attention/ops/blocksparse_attention/interface.py
@@ -11,9 +11,9 @@ from .utils import (dense_to_crow_col, get_head_sliding_step,
 
 IS_COMPUTE_8_OR_ABOVE = current_platform.has_device_capability(80)
 
-if IS_COMPUTE_8_OR_ABOVE:
-    from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
-
+# if IS_COMPUTE_8_OR_ABOVE:
+#     from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
+from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
 
 class LocalStridedBlockSparseAttn(torch.nn.Module):
 
@@ -34,8 +34,7 @@ class LocalStridedBlockSparseAttn(torch.nn.Module):
         super().__init__()
         if use_spda is None:
             use_spda = current_platform.is_rocm() or \
-                        current_platform.is_cpu() or not \
-                        IS_COMPUTE_8_OR_ABOVE
+                        current_platform.is_cpu()
         device = device or (torch.cuda.current_device()
                             if current_platform.is_cuda_alike() else "cpu")
         device = torch.device(device)
@@ -123,10 +122,10 @@ class LocalStridedBlockSparseAttn(torch.nn.Module):
 
         return: tensor of shape as q.
         """
-        assert (
-            IS_COMPUTE_8_OR_ABOVE
-        ), "Requires compute capability of 8 or above (Ampere or newer) to use \
-            Triton kernel."
+        # assert (
+        #     IS_COMPUTE_8_OR_ABOVE
+        # ), "Requires compute capability of 8 or above (Ampere or newer) to use \
+        #     Triton kernel."
 
         sm_scale = sm_scale or 1.0 / math.sqrt(q.size(-1))
 
diff --git a/vllm/attention/ops/flashmla.py b/vllm/attention/ops/flashmla.py
index 18b69a6b3..9ed45826f 100644
--- a/vllm/attention/ops/flashmla.py
+++ b/vllm/attention/ops/flashmla.py
@@ -9,29 +9,37 @@ from vllm.platforms import current_platform
 
 logger = init_logger(__name__)
 
-if current_platform.is_cuda():
-    try:
-        import vllm._flashmla_C  # noqa: F401
-        _flashmla_C_AVAILABLE = True
-    except ImportError:
-        _flashmla_C_AVAILABLE = False
-else:
-    _flashmla_C_AVAILABLE = False
+# if current_platform.is_cuda():
+#     try:
+#         import vllm._flashmla_C  # noqa: F401
+#         _flashmla_C_AVAILABLE = True
+#     except ImportError:
+#         _flashmla_C_AVAILABLE = False
+# else:
+#     _flashmla_C_AVAILABLE = False
+try :
+    import flash_mla
+    _flashmla_AVAILABLE = True
+except ImportError as e:
+    logger.warning("Failed to import from flash_mla with %r on MACA Platform", e)
+    _flashmla_AVAILABLE = False
 
 
 def is_flashmla_supported() -> Tuple[bool, Optional[str]]:
     """
     Return: is_supported_flag, unsupported_reason (optional).
     """
-    if not current_platform.is_cuda():
-        return False, "FlashMLA is only supported on CUDA devices."
-    if current_platform.get_device_capability()[0] != 9:
-        return False, "FlashMLA is only supported on Hopper devices."
-    if not _flashmla_C_AVAILABLE:
-        return False, "vllm._flashmla_C is not available, likely was not "\
-            "compiled due to insufficient nvcc version or a supported arch "\
-            "(only sm90a currently) was not in the list of target arches to "\
-            "compile for."
+    # if not current_platform.is_cuda():
+    #     return False, "FlashMLA is only supported on CUDA devices."
+    # if current_platform.get_device_capability()[0] != 9:
+    #     return False, "FlashMLA is only supported on Hopper devices."
+    # if not _flashmla_C_AVAILABLE:
+    #     return False, "vllm._flashmla_C is not available, likely was not "\
+    #         "compiled due to insufficient nvcc version or a supported arch "\
+    #         "(only sm90a currently) was not in the list of target arches to "\
+    #         "compile for."
+    if not _flashmla_AVAILABLE:
+        return False, "flash_mla is not available"
     return True, None
 
 
@@ -51,7 +59,10 @@ def get_mla_metadata(
                                  dtype torch.int32.
         num_splits: (batch_size + 1), dtype torch.int32.
     """
-    return torch.ops._flashmla_C.get_mla_metadata(cache_seqlens,
+    # return torch.ops._flashmla_C.get_mla_metadata(cache_seqlens,
+    #                                               num_heads_per_head_k,
+    #                                               num_heads_k)
+    return flash_mla.flash_mla_interface.get_mla_metadata(cache_seqlens,
                                                   num_heads_per_head_k,
                                                   num_heads_k)
 
@@ -85,19 +96,30 @@ def flash_mla_with_kvcache(
         out: (batch_size, seq_len_q, num_heads_q, head_dim_v).
         softmax_lse: (batch_size, num_heads_q, seq_len_q), torch.float32.
     """
-    if softmax_scale is None:
-        softmax_scale = q.shape[-1]**(-0.5)
-    out, softmax_lse = torch.ops._flashmla_C.fwd_kvcache_mla(
+    # if softmax_scale is None:
+    #     softmax_scale = q.shape[-1]**(-0.5)
+    # out, softmax_lse = torch.ops._flashmla_C.fwd_kvcache_mla(
+    #     q,
+    #     k_cache,
+    #     None,
+    #     head_dim_v,
+    #     cache_seqlens,
+    #     block_table,
+    #     softmax_scale,
+    #     causal,
+    #     tile_scheduler_metadata,
+    #     num_splits,
+    # )
+    out, softmax_lse = flash_mla.flash_mla_interface.flash_mla_with_kvcache(
         q,
         k_cache,
-        None,
-        head_dim_v,
-        cache_seqlens,
         block_table,
-        softmax_scale,
-        causal,
+        cache_seqlens,
+        head_dim_v,
         tile_scheduler_metadata,
         num_splits,
+        softmax_scale,
+        causal,
     )
     return out, softmax_lse
 
diff --git a/vllm/attention/ops/paged_attn.py b/vllm/attention/ops/paged_attn.py
index 827c3041a..db3a7c3cd 100644
--- a/vllm/attention/ops/paged_attn.py
+++ b/vllm/attention/ops/paged_attn.py
@@ -53,14 +53,16 @@ class PagedAttention:
         num_kv_heads: int,
         head_size: int,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        x = 16 // kv_cache.element_size()
+        # Support page attention backend
+        x = 32 // kv_cache.element_size()
         num_blocks = kv_cache.shape[1]
 
         key_cache = kv_cache[0]
         key_cache = key_cache.view(num_blocks, num_kv_heads, head_size // x,
                                    -1, x)
         value_cache = kv_cache[1]
-        value_cache = value_cache.view(num_blocks, num_kv_heads, head_size, -1)
+        # Support page attention backend
+        value_cache = value_cache.view(num_blocks, num_kv_heads, -1, head_size)
         return key_cache, value_cache
 
     @staticmethod
@@ -74,7 +76,8 @@ class PagedAttention:
         k_scale: torch.Tensor,
         v_scale: torch.Tensor,
     ) -> None:
-        ops.reshape_and_cache(
+        # Support page attention backend
+        ops.reshape_and_cache_new(
             key,
             value,
             key_cache,
@@ -107,14 +110,14 @@ class PagedAttention:
     ) -> torch.Tensor:
         if blocksparse_vert_stride is not None and blocksparse_vert_stride > 1:
             # use blocksparse paged attention
-            block_size = value_cache.size(-1)
+            block_size = value_cache.shape[2] # # Support page attention backend
             assert (blocksparse_block_size > 0 and
                     blocksparse_block_size % block_size == 0), \
                 (f"{blocksparse_block_size=} needs to be a multiple of"
                  f"{block_size=} used in block_tables.")
 
         output = torch.empty_like(query)
-        block_size = value_cache.shape[3]
+        block_size = value_cache.shape[2]
         num_seqs, num_heads, head_size = query.shape
         max_num_partitions = ((max_seq_len + _PARTITION_SIZE - 1) //
                               _PARTITION_SIZE)
@@ -159,17 +162,24 @@ class PagedAttention:
                 dtype=output.dtype,
                 device=output.device,
             )
+            block_count = torch.zeros(
+                size=(num_seqs, num_heads),
+                dtype=torch.int,
+                device=output.device,
+            )
             exp_sums = torch.empty(
                 size=(num_seqs, num_heads, max_num_partitions),
                 dtype=torch.float32,
                 device=output.device,
             )
             max_logits = torch.empty_like(exp_sums)
+            block_count_init_once = False
             ops.paged_attention_v2(
                 output,
                 exp_sums,
                 max_logits,
                 tmp_output,
+                block_count,
                 query,
                 key_cache,
                 value_cache,
@@ -188,6 +198,7 @@ class PagedAttention:
                 blocksparse_vert_stride,
                 blocksparse_block_size,
                 blocksparse_head_sliding_step,
+                block_count_init_once
             )
         return output
 
diff --git a/vllm/attention/ops/triton_decode_attention.py b/vllm/attention/ops/triton_decode_attention.py
index 35ee0835f..ada602439 100644
--- a/vllm/attention/ops/triton_decode_attention.py
+++ b/vllm/attention/ops/triton_decode_attention.py
@@ -30,6 +30,7 @@ It supports page size >= 1.
 
 import logging
 
+import torch
 import triton
 import triton.language as tl
 
@@ -387,17 +388,18 @@ def _decode_grouped_att_m_fwd(
     Req_to_tokens,
     B_Seqlen,
     num_kv_splits,
+    num_stages,
     sm_scale,
     page_size,
     logit_cap,
 ):
-    BLOCK = 32
+    BLOCK = 16
     Lk = k_buffer.shape[-1]
     Lv = v_buffer.shape[-1]
 
     # [TODO] work around shmem limit on MI3xx
-    if is_hip_ and Lk >= 576:
-        BLOCK = 16
+    # if is_hip_ and Lk >= 576:
+    #     BLOCK = 16
 
     if Lk == 576:
         BLOCK_DMODEL = 512
@@ -421,8 +423,15 @@ def _decode_grouped_att_m_fwd(
         NUM_KV_SPLITS,
     )
 
-    extra_kargs = {}
+    if num_stages == 1:
+        extra_kargs = {"scenario":"mla"}
+    elif num_stages == 2:
+        extra_kargs = {"scenario" : "mla", "pipeline" : "cpasync"}
+    else:
+        KeyError("num_stages should be 1 or 2")  
+    """
     num_stages = 2
+    extra_kargs = {}
     if is_hip_:
         # https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/workload.html#mi300x-triton-kernel-performance-optimization
         # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
@@ -432,6 +441,7 @@ def _decode_grouped_att_m_fwd(
             "kpack": 2
         }
         num_stages = 1
+    """
 
     _fwd_grouped_kernel_stage1[grid](
         q,
@@ -541,6 +551,7 @@ def _decode_softmax_reducev_fwd(
     NUM_KV_SPLITS = num_kv_splits
 
     extra_kargs = {}
+    """
     if is_hip_:
         # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html
         # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py
@@ -549,6 +560,7 @@ def _decode_softmax_reducev_fwd(
             "matrix_instr_nonkdim": 16,
             "kpack": 2
         }
+    """
 
     grid = (batch, head_num)
     _fwd_kernel_stage2[grid](
@@ -607,6 +619,7 @@ def decode_attention_fwd_grouped(
     b_seq_len,
     attn_logits,
     num_kv_splits,
+    num_stages,
     sm_scale,
     page_size,
     logit_cap=0.0,
@@ -619,6 +632,7 @@ def decode_attention_fwd_grouped(
         req_to_token,
         b_seq_len,
         num_kv_splits,
+        num_stages,
         sm_scale,
         page_size,
         logit_cap,
@@ -636,6 +650,7 @@ def decode_attention_fwd(
     b_seq_len,
     attn_logits,
     num_kv_splits,
+    num_stages,
     sm_scale,
     page_size=1,
     logit_cap=0.0,
@@ -669,6 +684,7 @@ def decode_attention_fwd(
             b_seq_len,
             attn_logits,
             num_kv_splits,
+            num_stages,
             sm_scale,
             page_size,
             logit_cap,
diff --git a/vllm/config.py b/vllm/config.py
index e64510355..419598523 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -798,13 +798,13 @@ class ModelConfig:
                     # Raise error if the override is not custom (custom would
                     # be in QUANTIZATION_METHODS but not QuantizationMethods)
                     # and hasn't been added to the overrides list.
-                    if (name in get_args(QuantizationMethods)
-                            and name not in overrides):
-                        raise ValueError(
-                            f"Quantization method {name} is an override but "
-                            "is has not been added to the `overrides` list "
-                            "above. This is necessary to ensure that the "
-                            "overrides are checked in order of preference.")
+                    # if (name in get_args(QuantizationMethods)
+                    #         and name not in overrides):
+                    #     raise ValueError(
+                    #         f"Quantization method {name} is an override but "
+                    #         "is has not been added to the `overrides` list "
+                    #         "above. This is necessary to ensure that the "
+                    #         "overrides are checked in order of preference.")
                     quant_method = quantization_override
                     self.quantization = quantization_override
                     break
@@ -1094,7 +1094,8 @@ class ModelConfig:
     def get_layers_start_end_indices(
             self, parallel_config: "ParallelConfig") -> tuple[int, int]:
         from vllm.distributed.utils import get_pp_indices
-        if self.hf_text_config.model_type == "deepseek_mtp":
+        if (self.hf_text_config.model_type == "deepseek_mtp"
+                or self.hf_config.model_type == "mimo_mtp"):
             total_num_hidden_layers = getattr(self.hf_text_config,
                                               "num_nextn_predict_layers", 0)
         else:
@@ -1606,7 +1607,7 @@ class ParallelConfig:
     sequentially in multiple batches. To avoid RAM OOM when using tensor
     parallel and large models."""
 
-    disable_custom_all_reduce: bool = False
+    disable_custom_all_reduce: bool = True
     """Disable the custom all-reduce kernel and fall back to NCCL."""
 
     tokenizer_pool_config: Optional[TokenizerPoolConfig] = None
@@ -2277,6 +2278,17 @@ class SpeculativeConfig:
                 "n_predict": n_predict,
                 "architectures": ["DeepSeekMTPModel"]
             })
+
+        if hf_config.architectures[0] == "MiMoForCausalLM":
+            hf_config.model_type = "mimo_mtp"
+            n_predict = getattr(hf_config, "num_nextn_predict_layers", None)
+            hf_config.update({
+                "num_hidden_layers": 0,
+                "n_predict": n_predict,
+                "architectures": ["MiMoMTPModel"]
+            })
+            return hf_config
+
         return hf_config
 
     def __post_init__(self):
@@ -2293,8 +2305,10 @@ class SpeculativeConfig:
             # TODO(Shangming): Refactor mtp configuration logic when supporting
             # mtp acceleration for more models besides deepseek_v3
             if self.target_model_config and \
-                self.target_model_config.hf_text_config.model_type \
-                        == "deepseek_v3":
+                (self.target_model_config.hf_text_config.model_type \
+                        == "deepseek_v3" or
+                    self.target_model_config.hf_text_config.model_type \
+                        == "mimo"):
                 # use the draft model from the same model:
                 self.model = self.target_model_config.model
             elif self.method in ("ngram", "[ngram]"):
@@ -3801,12 +3815,14 @@ class VllmConfig:
 
             if capability_tuple is not None:
                 capability = capability_tuple.to_int()
+                """
                 if capability < quant_config.get_min_capability():
                     raise ValueError(
                         f"The quantization method {model_config.quantization} "
                         "is not supported for the current GPU. Minimum "
                         f"capability: {quant_config.get_min_capability()}. "
                         f"Current capability: {capability}.")
+                """
             supported_dtypes = quant_config.get_supported_act_dtypes()
             if model_config.dtype not in supported_dtypes:
                 raise ValueError(
@@ -3888,8 +3904,7 @@ class VllmConfig:
             # FIXME(woosuk): Disable inductor to reduce the compilation time
             # and avoid any potential issues with the inductor.
             # FIXME(rob): Add function to set all of these.
-            if not self.compilation_config.custom_ops:
-                self.compilation_config.custom_ops = ["none"]
+            self.compilation_config.custom_ops = ["none"]
             self.compilation_config.use_cudagraph = True
             self.compilation_config.use_inductor = True
             self.compilation_config.cudagraph_num_of_warmups = 1
@@ -4033,7 +4048,7 @@ class VllmConfig:
             if self.model_config is not None and \
                 not self.model_config.enforce_eager:
                 batch_size_capture_list = [1, 2, 4
-                                           ] + [i for i in range(8, 513, 8)]
+                                           ] + [i for i in range(8, 257, 8)]
                 if self.parallel_config.tensor_parallel_size > 1 and \
                     self.compilation_config.pass_config.enable_sequence_parallelism:
                     batch_size_capture_list = \
diff --git a/vllm/distributed/device_communicators/cuda_wrapper.py b/vllm/distributed/device_communicators/cuda_wrapper.py
index 1d53b1c5b..55b73dd00 100644
--- a/vllm/distributed/device_communicators/cuda_wrapper.py
+++ b/vllm/distributed/device_communicators/cuda_wrapper.py
@@ -64,33 +64,33 @@ def find_loaded_library(lib_name) -> Optional[str]:
 class CudaRTLibrary:
     exported_functions = [
         # cudaError_t cudaSetDevice ( int  device )
-        Function("cudaSetDevice", cudaError_t, [ctypes.c_int]),
+        Function("mcSetDevice", cudaError_t, [ctypes.c_int]),
         # cudaError_t 	cudaDeviceSynchronize ( void )
-        Function("cudaDeviceSynchronize", cudaError_t, []),
+        Function("mcDeviceSynchronize", cudaError_t, []),
         # cudaError_t cudaDeviceReset ( void )
-        Function("cudaDeviceReset", cudaError_t, []),
+        Function("mcDeviceReset", cudaError_t, []),
 
         # const char* 	cudaGetErrorString ( cudaError_t error )
-        Function("cudaGetErrorString", ctypes.c_char_p, [cudaError_t]),
+        Function("mcGetErrorString", ctypes.c_char_p, [cudaError_t]),
 
         # cudaError_t 	cudaMalloc ( void** devPtr, size_t size )
-        Function("cudaMalloc", cudaError_t,
+        Function("mcMalloc", cudaError_t,
                  [ctypes.POINTER(ctypes.c_void_p), ctypes.c_size_t]),
         # cudaError_t 	cudaFree ( void* devPtr )
-        Function("cudaFree", cudaError_t, [ctypes.c_void_p]),
+        Function("mcFree", cudaError_t, [ctypes.c_void_p]),
         # cudaError_t cudaMemset ( void* devPtr, int  value, size_t count )
-        Function("cudaMemset", cudaError_t,
+        Function("mcMemset", cudaError_t,
                  [ctypes.c_void_p, ctypes.c_int, ctypes.c_size_t]),
         # cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind ) # noqa
-        Function("cudaMemcpy", cudaError_t, [
+        Function("mcMemcpy", cudaError_t, [
             ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t, cudaMemcpyKind
         ]),
 
         # cudaError_t cudaIpcGetMemHandle ( cudaIpcMemHandle_t* handle, void* devPtr ) # noqa
-        Function("cudaIpcGetMemHandle", cudaError_t,
+        Function("mcIpcGetMemHandle", cudaError_t,
                  [ctypes.POINTER(cudaIpcMemHandle_t), ctypes.c_void_p]),
         # cudaError_t cudaIpcOpenMemHandle ( void** devPtr, cudaIpcMemHandle_t handle, unsigned int  flags ) # noqa
-        Function("cudaIpcOpenMemHandle", cudaError_t, [
+        Function("mcIpcOpenMemHandle", cudaError_t, [
             ctypes.POINTER(ctypes.c_void_p), cudaIpcMemHandle_t, ctypes.c_uint
         ]),
     ]
@@ -105,7 +105,7 @@ class CudaRTLibrary:
 
     def __init__(self, so_file: Optional[str] = None):
         if so_file is None:
-            so_file = find_loaded_library("libcudart")
+            so_file = find_loaded_library("libmcruntime")
             if so_file is None:
                 so_file = envs.VLLM_CUDART_SO_PATH  # fallback to env var
             assert so_file is not None, \
@@ -134,39 +134,39 @@ class CudaRTLibrary:
             raise RuntimeError(f"CUDART error: {error_str}")
 
     def cudaGetErrorString(self, error: cudaError_t) -> str:
-        return self.funcs["cudaGetErrorString"](error).decode("utf-8")
+        return self.funcs["mcGetErrorString"](error).decode("utf-8")
 
     def cudaSetDevice(self, device: int) -> None:
-        self.CUDART_CHECK(self.funcs["cudaSetDevice"](device))
+        self.CUDART_CHECK(self.funcs["mcSetDevice"](device))
 
     def cudaDeviceSynchronize(self) -> None:
-        self.CUDART_CHECK(self.funcs["cudaDeviceSynchronize"]())
+        self.CUDART_CHECK(self.funcs["mcDeviceSynchronize"]())
 
     def cudaDeviceReset(self) -> None:
-        self.CUDART_CHECK(self.funcs["cudaDeviceReset"]())
+        self.CUDART_CHECK(self.funcs["mcDeviceReset"]())
 
     def cudaMalloc(self, size: int) -> ctypes.c_void_p:
         devPtr = ctypes.c_void_p()
-        self.CUDART_CHECK(self.funcs["cudaMalloc"](ctypes.byref(devPtr), size))
+        self.CUDART_CHECK(self.funcs["mcMalloc"](ctypes.byref(devPtr), size))
         return devPtr
 
     def cudaFree(self, devPtr: ctypes.c_void_p) -> None:
-        self.CUDART_CHECK(self.funcs["cudaFree"](devPtr))
+        self.CUDART_CHECK(self.funcs["mcFree"](devPtr))
 
     def cudaMemset(self, devPtr: ctypes.c_void_p, value: int,
                    count: int) -> None:
-        self.CUDART_CHECK(self.funcs["cudaMemset"](devPtr, value, count))
+        self.CUDART_CHECK(self.funcs["mcMemset"](devPtr, value, count))
 
     def cudaMemcpy(self, dst: ctypes.c_void_p, src: ctypes.c_void_p,
                    count: int) -> None:
         cudaMemcpyDefault = 4
         kind = cudaMemcpyDefault
-        self.CUDART_CHECK(self.funcs["cudaMemcpy"](dst, src, count, kind))
+        self.CUDART_CHECK(self.funcs["mcMemcpy"](dst, src, count, kind))
 
     def cudaIpcGetMemHandle(self,
                             devPtr: ctypes.c_void_p) -> cudaIpcMemHandle_t:
         handle = cudaIpcMemHandle_t()
-        self.CUDART_CHECK(self.funcs["cudaIpcGetMemHandle"](
+        self.CUDART_CHECK(self.funcs["mcIpcGetMemHandle"](
             ctypes.byref(handle), devPtr))
         return handle
 
@@ -174,6 +174,6 @@ class CudaRTLibrary:
                              handle: cudaIpcMemHandle_t) -> ctypes.c_void_p:
         cudaIpcMemLazyEnablePeerAccess = 1
         devPtr = ctypes.c_void_p()
-        self.CUDART_CHECK(self.funcs["cudaIpcOpenMemHandle"](
+        self.CUDART_CHECK(self.funcs["mcIpcOpenMemHandle"](
             ctypes.byref(devPtr), handle, cudaIpcMemLazyEnablePeerAccess))
         return devPtr
diff --git a/vllm/distributed/device_communicators/pynccl_wrapper.py b/vllm/distributed/device_communicators/pynccl_wrapper.py
index 4f04899e9..ba4071de5 100644
--- a/vllm/distributed/device_communicators/pynccl_wrapper.py
+++ b/vllm/distributed/device_communicators/pynccl_wrapper.py
@@ -127,18 +127,18 @@ class Function:
 class NCCLLibrary:
     exported_functions = [
         # const char* ncclGetErrorString(ncclResult_t result)
-        Function("ncclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
+        Function("mcclGetErrorString", ctypes.c_char_p, [ncclResult_t]),
         # ncclResult_t  ncclGetVersion(int *version);
-        Function("ncclGetVersion", ncclResult_t,
+        Function("mcclGetVersion", ncclResult_t,
                  [ctypes.POINTER(ctypes.c_int)]),
         # ncclResult_t ncclGetUniqueId(ncclUniqueId* uniqueId);
-        Function("ncclGetUniqueId", ncclResult_t,
+        Function("mcclGetUniqueId", ncclResult_t,
                  [ctypes.POINTER(ncclUniqueId)]),
         # ncclResult_t  ncclCommInitRank(
         #   ncclComm_t* comm, int nranks, ncclUniqueId commId, int rank);
         # note that ncclComm_t is a pointer type, so the first argument
         # is a pointer to a pointer
-        Function("ncclCommInitRank", ncclResult_t, [
+        Function("mcclCommInitRank", ncclResult_t, [
             ctypes.POINTER(ncclComm_t), ctypes.c_int, ncclUniqueId,
             ctypes.c_int
         ]),
@@ -148,7 +148,7 @@ class NCCLLibrary:
         #   cudaStream_t stream);
         # note that cudaStream_t is a pointer type, so the last argument
         # is a pointer
-        Function("ncclAllReduce", ncclResult_t, [
+        Function("mcclAllReduce", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ncclRedOp_t, ncclComm_t, cudaStream_t
         ]),
@@ -159,7 +159,7 @@ class NCCLLibrary:
         #   cudaStream_t stream);
         # note that cudaStream_t is a pointer type, so the last argument
         # is a pointer
-        Function("ncclAllGather", ncclResult_t, [
+        Function("mcclAllGather", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ncclComm_t, cudaStream_t
         ]),
@@ -170,7 +170,7 @@ class NCCLLibrary:
         #   cudaStream_t stream);
         # note that cudaStream_t is a pointer type, so the last argument
         # is a pointer
-        Function("ncclReduceScatter", ncclResult_t, [
+        Function("mcclReduceScatter", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ncclRedOp_t, ncclComm_t, cudaStream_t
         ]),
@@ -178,7 +178,7 @@ class NCCLLibrary:
         # ncclResult_t  ncclSend(
         #   const void* sendbuff, size_t count, ncclDataType_t datatype,
         #   int dest, ncclComm_t comm, cudaStream_t stream);
-        Function("ncclSend", ncclResult_t, [
+        Function("mcclSend", ncclResult_t, [
             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
             ncclComm_t, cudaStream_t
         ]),
@@ -186,7 +186,7 @@ class NCCLLibrary:
         # ncclResult_t  ncclRecv(
         #   void* recvbuff, size_t count, ncclDataType_t datatype,
         #   int src, ncclComm_t comm, cudaStream_t stream);
-        Function("ncclRecv", ncclResult_t, [
+        Function("mcclRecv", ncclResult_t, [
             buffer_type, ctypes.c_size_t, ncclDataType_t, ctypes.c_int,
             ncclComm_t, cudaStream_t
         ]),
@@ -195,7 +195,7 @@ class NCCLLibrary:
         #   const void* sendbuff, void* recvbuff, size_t count,
         #   ncclDataType_t datatype, int root, ncclComm_t comm,
         #   cudaStream_t stream);
-        Function("ncclBroadcast", ncclResult_t, [
+        Function("mcclBroadcast", ncclResult_t, [
             buffer_type, buffer_type, ctypes.c_size_t, ncclDataType_t,
             ctypes.c_int, ncclComm_t, cudaStream_t
         ]),
@@ -205,7 +205,7 @@ class NCCLLibrary:
         # because Python object destruction can happen in random order,
         # it is better not to call it at all.
         # ncclResult_t  ncclCommDestroy(ncclComm_t comm);
-        Function("ncclCommDestroy", ncclResult_t, [ncclComm_t]),
+        Function("mcclCommDestroy", ncclResult_t, [ncclComm_t]),
     ]
 
     # class attribute to store the mapping from the path to the library
@@ -248,7 +248,7 @@ class NCCLLibrary:
         self._funcs = NCCLLibrary.path_to_dict_mapping[so_file]
 
     def ncclGetErrorString(self, result: ncclResult_t) -> str:
-        return self._funcs["ncclGetErrorString"](result).decode("utf-8")
+        return self._funcs["mcclGetErrorString"](result).decode("utf-8")
 
     def NCCL_CHECK(self, result: ncclResult_t) -> None:
         if result != 0:
@@ -257,7 +257,7 @@ class NCCLLibrary:
 
     def ncclGetVersion(self) -> str:
         version = ctypes.c_int()
-        self.NCCL_CHECK(self._funcs["ncclGetVersion"](ctypes.byref(version)))
+        self.NCCL_CHECK(self._funcs["mcclGetVersion"](ctypes.byref(version)))
         version_str = str(version.value)
         # something like 21903 --> "2.19.3"
         major = version_str[0].lstrip("0")
@@ -267,14 +267,14 @@ class NCCLLibrary:
 
     def ncclGetUniqueId(self) -> ncclUniqueId:
         unique_id = ncclUniqueId()
-        self.NCCL_CHECK(self._funcs["ncclGetUniqueId"](
+        self.NCCL_CHECK(self._funcs["mcclGetUniqueId"](
             ctypes.byref(unique_id)))
         return unique_id
 
     def ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId,
                          rank: int) -> ncclComm_t:
         comm = ncclComm_t()
-        self.NCCL_CHECK(self._funcs["ncclCommInitRank"](ctypes.byref(comm),
+        self.NCCL_CHECK(self._funcs["mcclCommInitRank"](ctypes.byref(comm),
                                                         world_size, unique_id,
                                                         rank))
         return comm
@@ -287,7 +287,7 @@ class NCCLLibrary:
         # both are aliases of `ctypes.c_int`
         # when we pass int to a function, it will be converted to `ctypes.c_int`
         # by ctypes automatically
-        self.NCCL_CHECK(self._funcs["ncclAllReduce"](sendbuff, recvbuff, count,
+        self.NCCL_CHECK(self._funcs["mcclAllReduce"](sendbuff, recvbuff, count,
                                                      datatype, op, comm,
                                                      stream))
 
@@ -299,7 +299,7 @@ class NCCLLibrary:
         # both are aliases of `ctypes.c_int`
         # when we pass int to a function, it will be converted to `ctypes.c_int`
         # by ctypes automatically
-        self.NCCL_CHECK(self._funcs["ncclReduceScatter"](sendbuff, recvbuff,
+        self.NCCL_CHECK(self._funcs["mcclReduceScatter"](sendbuff, recvbuff,
                                                          count, datatype, op,
                                                          comm, stream))
 
@@ -310,28 +310,28 @@ class NCCLLibrary:
         # which is an aliases of `ctypes.c_int`
         # when we pass int to a function, it will be converted to `ctypes.c_int`
         # by ctypes automatically
-        self.NCCL_CHECK(self._funcs["ncclAllGather"](sendbuff, recvbuff, count,
+        self.NCCL_CHECK(self._funcs["mcclAllGather"](sendbuff, recvbuff, count,
                                                      datatype, comm, stream))
 
     def ncclSend(self, sendbuff: buffer_type, count: int, datatype: int,
                  dest: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclSend"](sendbuff, count, datatype,
+        self.NCCL_CHECK(self._funcs["mcclSend"](sendbuff, count, datatype,
                                                 dest, comm, stream))
 
     def ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int,
                  src: int, comm: ncclComm_t, stream: cudaStream_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclRecv"](recvbuff, count, datatype, src,
+        self.NCCL_CHECK(self._funcs["mcclRecv"](recvbuff, count, datatype, src,
                                                 comm, stream))
 
     def ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type,
                       count: int, datatype: int, root: int, comm: ncclComm_t,
                       stream: cudaStream_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclBroadcast"](sendbuff, recvbuff, count,
+        self.NCCL_CHECK(self._funcs["mcclBroadcast"](sendbuff, recvbuff, count,
                                                      datatype, root, comm,
                                                      stream))
 
     def ncclCommDestroy(self, comm: ncclComm_t) -> None:
-        self.NCCL_CHECK(self._funcs["ncclCommDestroy"](comm))
+        self.NCCL_CHECK(self._funcs["mcclCommDestroy"](comm))
 
 
 __all__ = [
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index cb9658ce1..073366331 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -1117,12 +1117,13 @@ def cleanup_dist_env_and_memory(shutdown_ray: bool = False):
     from vllm.platforms import current_platform
     if not current_platform.is_cpu():
         torch.cuda.empty_cache()
+    """
     try:
         torch._C._host_emptyCache()
     except AttributeError:
         logger.warning(
             "torch._C._host_emptyCache() only available in Pytorch >=2.5")
-
+    """
 
 def in_the_same_node_as(pg: Union[ProcessGroup, StatelessProcessGroup],
                         source_rank: int = 0) -> List[bool]:
diff --git a/vllm/distributed/utils.py b/vllm/distributed/utils.py
index e4d4008cd..4dbc7e828 100644
--- a/vllm/distributed/utils.py
+++ b/vllm/distributed/utils.py
@@ -20,12 +20,19 @@ from torch.distributed.distributed_c10d import (Backend, PrefixStore,
                                                 is_nccl_available)
 from torch.distributed.rendezvous import rendezvous
 
+from packaging import version
+from packaging.version import Version
+
 import vllm.envs as envs
 from vllm.logger import init_logger
 
 logger = init_logger(__name__)
 
 
+def supports_pg_options() -> bool:
+    base_torch_version = Version(Version(torch.__version__).base_version)
+    return base_torch_version < Version("2.5.0")
+
 def ensure_divisibility(numerator, denominator):
     """Ensure that numerator is divisible by the denominator."""
     assert numerator % denominator == 0, "{} is not divisible by {}".format(
@@ -318,11 +325,20 @@ def stateless_init_torch_distributed_process_group(
     # different systems (e.g. RPC) in case the store is multi-tenant.
     prefix_store = PrefixStore(init_method, store)
 
-    pg: ProcessGroup = ProcessGroup(
-        prefix_store,
-        group_rank,
-        group_size,
-    )
+    if not supports_pg_options():
+        pg: ProcessGroup = ProcessGroup(
+            prefix_store,
+            group_rank,
+            group_size,
+        )
+    else:
+        pg_options = ProcessGroup.Options(backend=backend, timeout=timeout)
+        pg: ProcessGroup = ProcessGroup(
+            prefix_store,
+            group_rank,
+            group_size,
+            pg_options
+        )
 
     if backend == "gloo":
         from torch.distributed.distributed_c10d import ProcessGroupGloo
@@ -346,7 +362,8 @@ def stateless_init_torch_distributed_process_group(
     else:
         raise RuntimeError(f"Unsupported torch distributed backend: {backend}")
 
-    pg._set_default_backend(backend_type)
+    if not supports_pg_options():
+        pg._set_default_backend(backend_type)
     backend_class._set_sequence_number_for_group()
 
     pg._register_backend(device, backend_type, backend_class)
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 5d735103f..b4f1339b7 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -237,7 +237,7 @@ class EngineArgs:
     prefix_caching_hash_algo: PrefixCachingHashAlgo = \
         CacheConfig.prefix_caching_hash_algo
     disable_sliding_window: bool = False
-    disable_cascade_attn: bool = False
+    disable_cascade_attn: bool = True
     use_v2_block_manager: bool = True
     swap_space: float = CacheConfig.swap_space
     cpu_offload_gb: float = CacheConfig.cpu_offload_gb
@@ -261,7 +261,7 @@ class EngineArgs:
     quantization: Optional[str] = None
     enforce_eager: Optional[bool] = None
     max_seq_len_to_capture: int = 8192
-    disable_custom_all_reduce: bool = ParallelConfig.disable_custom_all_reduce
+    disable_custom_all_reduce: bool = True #ParallelConfig.disable_custom_all_reduce
     # The following three fields are deprecated and will be removed in a future
     # release. Setting them will have no effect. Please remove them from your
     # configurations.
diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 653e61a11..36fed94d1 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -177,7 +177,7 @@ class LLM:
         cpu_offload_gb: float = 0,
         enforce_eager: Optional[bool] = None,
         max_seq_len_to_capture: int = 8192,
-        disable_custom_all_reduce: bool = False,
+        disable_custom_all_reduce: bool = True,
         disable_async_output_proc: bool = False,
         hf_token: Optional[Union[bool, str]] = None,
         hf_overrides: Optional[HfOverrides] = None,
diff --git a/vllm/env_override.py b/vllm/env_override.py
index 71f031d1e..9c084aa0a 100644
--- a/vllm/env_override.py
+++ b/vllm/env_override.py
@@ -31,4 +31,4 @@ os.environ['PYTORCH_NVML_BASED_CUDA_CHECK'] = '1'
 # see https://github.com/vllm-project/vllm/issues/10480
 os.environ['TORCHINDUCTOR_COMPILE_THREADS'] = '1'
 # see https://github.com/vllm-project/vllm/issues/10619
-torch._inductor.config.compile_threads = 1
+# torch._inductor.config.compile_threads = 1
diff --git a/vllm/envs.py b/vllm/envs.py
index ea40bfff1..862f9c97c 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -110,7 +110,8 @@ if TYPE_CHECKING:
     VLLM_USE_DEEP_GEMM: bool = False
     VLLM_XGRAMMAR_CACHE_MB: int = 0
     VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256
-
+    MACA_VLLM_USE_TN_2_NN: bool = True
+    MACA_VLLM_PG_OPT: bool = True
 
 def get_default_cache_root():
     return os.getenv(
@@ -345,9 +346,10 @@ environment_variables: dict[str, Callable[[], Any]] = {
     lambda: os.getenv("VLLM_ATTENTION_BACKEND", None),
 
     # If set, vllm will use flashinfer sampler
+    # FIXME: some models hang, change VLLM_USE_FLASHINFER_SAMPLER default to False, V1 does not use flashinfo top-k top-p sampling.
     "VLLM_USE_FLASHINFER_SAMPLER":
     lambda: bool(int(os.environ["VLLM_USE_FLASHINFER_SAMPLER"]))
-    if "VLLM_USE_FLASHINFER_SAMPLER" in os.environ else None,
+    if "VLLM_USE_FLASHINFER_SAMPLER" in os.environ else False,
 
     # If set, vllm will force flashinfer to use tensor cores;
     # otherwise will use heuristic based on model architecture.
@@ -727,6 +729,14 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # limit will actually be zero-copy decoded.
     "VLLM_MSGPACK_ZERO_COPY_THRESHOLD":
     lambda: int(os.getenv("VLLM_MSGPACK_ZERO_COPY_THRESHOLD", "256")),
+    
+    # if set, enable loading weight by transpose
+    "MACA_VLLM_USE_TN_2_NN":
+    lambda: os.environ.get("MACA_VLLM_USE_TN_2_NN", "1") == "1",
+    
+    # if set, enable page attention backend
+    "MACA_VLLM_PG_OPT":
+    lambda: os.environ.get("MACA_VLLM_PG_OPT", "0") == "1",
 }
 
 # end-env-vars-definition
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index 9b0b98731..a3dc9dff6 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -556,8 +556,16 @@ class RayDistributedExecutor(DistributedExecutorBase):
     def _compiled_ray_dag(self, enable_asyncio: bool):
         assert self.parallel_config.use_ray
         self._check_ray_cgraph_installation()
+        # Enlarge the default value of "RAY_CGRAPH_get_timeout" to 300 seconds
+        # (it is 10 seconds by default). This is a Ray environment variable to
+        # control the timeout of getting result from a compiled graph execution,
+        # i.e., the distributed execution that includes model forward runs and
+        # intermediate tensor communications, in the case of vllm.
+        os.environ.setdefault("RAY_CGRAPH_get_timeout", "300")  # noqa: SIM112
         from ray.dag import InputNode, MultiOutputNode
 
+        logger.info("RAY_CGRAPH_get_timeout is set to %s",
+                    os.environ["RAY_CGRAPH_get_timeout"])  # noqa: SIM112
         logger.info("VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = %s",
                     envs.VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE)
         logger.info("VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = %s",
@@ -569,14 +577,6 @@ class RayDistributedExecutor(DistributedExecutorBase):
                 "Invalid value for VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE: "
                 f"{channel_type}. Valid values are: 'auto', 'nccl', or 'shm'.")
 
-        # Enlarge the default value of "RAY_CGRAPH_get_timeout" to 300 seconds
-        # (it is 10 seconds by default). This is a Ray environment variable to
-        # control the timeout of getting result from a compiled graph execution,
-        # i.e., the distributed execution that includes model forward runs and
-        # intermediate tensor communications, in the case of vllm.
-        os.environ.setdefault("RAY_CGRAPH_get_timeout", "300")  # noqa: SIM112
-        logger.info("RAY_CGRAPH_get_timeout is set to %s",
-                    os.environ["RAY_CGRAPH_get_timeout"])  # noqa: SIM112
 
         with InputNode() as input_data:
             # Example DAG: PP=2, TP=4
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index a209715ed..4f0315c5b 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -23,6 +23,8 @@ from vllm.model_executor.layers.quantization.utils.int8_utils import (
 from vllm.platforms import current_platform
 from vllm.utils import direct_register_custom_op
 
+import math
+
 from .rocm_aiter_fused_moe import is_rocm_aiter_moe_enabled
 
 logger = init_logger(__name__)
@@ -81,6 +83,8 @@ def fused_moe_kernel_gptq_awq(
         BLOCK_SIZE_N: tl.constexpr,
         BLOCK_SIZE_K: tl.constexpr,
         GROUP_SIZE_M: tl.constexpr,
+        SPLIT_K: tl.constexpr,
+        ACCF32: tl.constexpr,
         MUL_ROUTED_WEIGHT: tl.constexpr,
         top_k: tl.constexpr,
         compute_type: tl.constexpr,
@@ -249,55 +253,77 @@ def fused_moe_kernel_gptq_awq(
     c_mask = token_mask[:, None] & (offs_cn[None, :] < N)
     tl.store(c_ptrs, accumulator, mask=c_mask)
 
-
+@triton.heuristics(
+    {
+        "UPGRADE": lambda args: math.ceil((args["EM"] * args["N"]) / (args["BLOCK_SIZE_M"] * args["BLOCK_SIZE_N"])).bit_length() > 31,
+    }
+)
+@triton.heuristics(
+    {
+        "UPGRADE_A_OFFS": lambda args: (args["num_valid_tokens"] // args["top_k"] * args["stride_am"] + args["BLOCK_SIZE_K"] * args["stride_ak"]).bit_length() > 31,
+    }
+)
+@triton.heuristics(
+    {
+        "UPGRADE_B_OFFS": lambda args: ((args["E"]-1) * args["stride_be"] + (args["N"]-1) * args["stride_bn"]+(args["K"]-1) * args["stride_bk"]).bit_length() > 31,
+    }
+)
 @triton.jit
 def fused_moe_kernel(
-    # Pointers to matrices
-    a_ptr,
-    b_ptr,
-    c_ptr,
-    a_scale_ptr,
-    b_scale_ptr,
-    topk_weights_ptr,
-    sorted_token_ids_ptr,
-    expert_ids_ptr,
-    num_tokens_post_padded_ptr,
-    # Matrix dimensions
-    N,
-    K,
-    EM,
-    num_valid_tokens,
-    # The stride variables represent how much to increase the ptr by when
-    # moving by 1 element in a particular dimension. E.g. `stride_am` is
-    # how much to increase `a_ptr` by to get the element one row down
-    # (A has M rows).
-    stride_am,
-    stride_ak,
-    stride_be,
-    stride_bk,
-    stride_bn,
-    stride_cm,
-    stride_cn,
-    stride_asm,
-    stride_ask,
-    stride_bse,
-    stride_bsk,
-    stride_bsn,
-    # Block size for block-wise quantization
-    group_n: tl.constexpr,
-    group_k: tl.constexpr,
-    # Meta-parameters
-    BLOCK_SIZE_M: tl.constexpr,
-    BLOCK_SIZE_N: tl.constexpr,
-    BLOCK_SIZE_K: tl.constexpr,
-    GROUP_SIZE_M: tl.constexpr,
-    MUL_ROUTED_WEIGHT: tl.constexpr,
-    top_k: tl.constexpr,
-    compute_type: tl.constexpr,
-    use_fp8_w8a8: tl.constexpr,
-    use_int8_w8a8: tl.constexpr,
-    use_int8_w8a16: tl.constexpr,
-    per_channel_quant: tl.constexpr,
+        # Pointers to matrices
+        a_ptr,
+        b_ptr,
+        c_ptr,
+        a_scale_ptr,
+        b_scale_ptr,
+        topk_weights_ptr,
+        sorted_token_ids_ptr,
+        expert_ids_ptr,
+        num_tokens_post_padded_ptr,
+        # Matrix dimensions
+        E,          # B.shape[0]
+        N,
+        K,
+        EM,
+        num_valid_tokens,
+        # The stride variables represent how much to increase the ptr by when
+        # moving by 1 element in a particular dimension. E.g. `stride_am` is
+        # how much to increase `a_ptr` by to get the element one row down
+        # (A has M rows).
+        stride_am,
+        stride_ak,
+        stride_be,
+        stride_bk,
+        stride_bn,
+        stride_cm,
+        stride_cn,
+        stride_asm,
+        stride_ask,
+        stride_bse,
+        stride_bsk,
+        stride_bsn,
+        # Block size for block-wise quantization
+        group_n: tl.constexpr,
+        group_k: tl.constexpr,
+        # Meta-parameters
+        BLOCK_SIZE_M: tl.constexpr,
+        BLOCK_SIZE_N: tl.constexpr,
+        BLOCK_SIZE_K: tl.constexpr,
+        GROUP_SIZE_M: tl.constexpr,
+        SPLIT_K: tl.constexpr,
+        ACCF32: tl.constexpr,
+        MUL_ROUTED_WEIGHT: tl.constexpr,
+        top_k: tl.constexpr,
+        # experts_num: tl.constexpr,
+        compute_type: tl.constexpr,
+        use_fp8_w8a8: tl.constexpr,
+        use_int8_w8a8: tl.constexpr,
+        use_int8_w8a16: tl.constexpr,
+        per_channel_quant: tl.constexpr,
+        UPGRADE: tl.constexpr,
+        UPGRADE_A_OFFS: tl.constexpr,
+        UPGRADE_B_OFFS: tl.constexpr,
+        FAST_F32_TO_BF16: tl.constexpr
 ):
     """
     Implements the fused computation for a Mixture of Experts (MOE) using
@@ -328,7 +354,12 @@ def fused_moe_kernel(
     # -----------------------------------------------------------
     # Map program ids `pid` to the block of C it should compute.
     # This is done in a grouped ordering to promote L2 data reuse.
-    pid = tl.program_id(axis=0)
+    if UPGRADE:
+        pid = tl.program_id(axis=0).to(tl.int64)
+        pid_z = tl.program_id(axis=1).to(tl.int64)
+    else:
+        pid = tl.program_id(axis=0)
+        pid_z = tl.program_id(axis=1)
     num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)
     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
     num_pid_in_group = GROUP_SIZE_M * num_pid_n
@@ -352,7 +383,13 @@ def fused_moe_kernel(
     offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)
     token_mask = offs_token < num_valid_tokens
 
-    off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
+    if UPGRADE_B_OFFS:
+        off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)
+    else:
+        off_experts = tl.load(expert_ids_ptr + pid_m)
+        
+    if UPGRADE_A_OFFS:
+        offs_token = offs_token.to(tl.int64)
     if off_experts == -1:
         # -----------------------------------------------------------
         # Write back zeros to the output when the expert is not
@@ -362,9 +399,16 @@ def fused_moe_kernel(
                               BLOCK_SIZE_N, compute_type)
         return
 
-    offs_bn = (pid_n * BLOCK_SIZE_N +
-               tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N
-    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    if UPGRADE_B_OFFS:
+        offs_bn = (pid_n * BLOCK_SIZE_N +
+                   tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N
+    else:
+        offs_bn = (pid_n * BLOCK_SIZE_N +
+                   tl.arange(0, BLOCK_SIZE_N)) % N
+        
+    # offs_k = tl.arange(0, BLOCK_SIZE_K)
+    offs_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)
+    
     a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +
                       offs_k[None, :] * stride_ak)
 
@@ -374,9 +418,14 @@ def fused_moe_kernel(
         b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + offs_bn[
             None, :] * stride_bsn
         b_scale = tl.load(b_scale_ptrs)
+            
+    if use_int8_w8a8:  
+        a_scale = tl.load(a_scale_ptr+(offs_token[:, None] // top_k * stride_asm),mask=token_mask[:, None],other=0.0)
+        b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + offs_bn[
+            None, :] * stride_bsn
+        b_scale = tl.load(b_scale_ptrs)
 
-    if use_fp8_w8a8 or use_int8_w8a8:
-        # block-wise
+    if use_fp8_w8a8:
         if group_k > 0 and group_n > 0:
             a_scale_ptrs = a_scale_ptr + (offs_token // top_k) * stride_asm
             offs_bsn = offs_bn // group_n
@@ -401,23 +450,29 @@ def fused_moe_kernel(
     # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
     # of fp32 values for higher accuracy.
     # `accumulator` will be converted back to fp16 after the loop.
-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
-    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
+            
+    # accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32 if use_int8_w8a8 else tl.float32)
+
+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):
         # Load the next block of A and B, generate a mask by checking the
         # K dimension.
         a = tl.load(a_ptrs,
                     mask=token_mask[:, None] &
-                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),
+                   (offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K),
                     other=0.0)
         b = tl.load(b_ptrs,
-                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,
+                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K,
                     other=0.0)
         # We accumulate along the K dimension.
         if use_int8_w8a16:
             accumulator = tl.dot(a, b.to(compute_type), acc=accumulator)
-        elif use_fp8_w8a8 or use_int8_w8a8:
+        elif use_int8_w8a8:
+            a = a.to(tl.int8)
+            accumulator += tl.dot(a, b,out_dtype=accumulator.dtype)
+        elif use_fp8_w8a8:
             if group_k > 0 and group_n > 0:
-                k_start = k * BLOCK_SIZE_K
+                k_start = k * BLOCK_SIZE_K * SPLIT_K
                 offs_ks = k_start // group_k
                 a_scale = tl.load(a_scale_ptrs + offs_ks * stride_ask,
                                   mask=token_mask,
@@ -435,8 +490,8 @@ def fused_moe_kernel(
         else:
             accumulator += tl.dot(a, b)
         # Advance the ptrs to the next K block.
-        a_ptrs += BLOCK_SIZE_K * stride_ak
-        b_ptrs += BLOCK_SIZE_K * stride_bk
+        a_ptrs += BLOCK_SIZE_K * stride_ak * SPLIT_K
+        b_ptrs += BLOCK_SIZE_K * stride_bk* SPLIT_K
 
     if MUL_ROUTED_WEIGHT:
         moe_weight = tl.load(topk_weights_ptr + offs_token,
@@ -445,7 +500,15 @@ def fused_moe_kernel(
         accumulator = accumulator * moe_weight[:, None]
     if use_int8_w8a16:
         accumulator = (accumulator * b_scale).to(compute_type)
-    elif use_fp8_w8a8 or use_int8_w8a8:
+    elif use_int8_w8a8:
+        accumulator = accumulator.to(tl.float32)
+        accumulator = (accumulator * a_scale * b_scale)
+        if not ACCF32:
+            if FAST_F32_TO_BF16:
+                accumulator = accumulator.to(compute_type, "rtne_no_nan")
+            else:
+                accumulator = accumulator.to(compute_type)
+    elif use_fp8_w8a8:
         if group_k > 0 and group_n > 0:
             accumulator = accumulator.to(compute_type)
         else:
@@ -458,8 +521,10 @@ def fused_moe_kernel(
     c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[
         None, :]
     c_mask = token_mask[:, None] & (offs_cn[None, :] < N)
-    tl.store(c_ptrs, accumulator, mask=c_mask)
-
+    if SPLIT_K == 1:
+        tl.store(c_ptrs, accumulator, mask=c_mask)
+    else:
+        tl.atomic_add(c_ptrs, accumulator, mask=c_mask)
 
 def invoke_fused_moe_kernel(A: torch.Tensor,
                             B: torch.Tensor,
@@ -468,6 +533,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
                             B_scale: Optional[torch.Tensor],
                             B_zp: Optional[torch.Tensor],
                             topk_weights: Optional[torch.Tensor],
+                            topk_ids: torch.Tensor,
                             sorted_token_ids: torch.Tensor,
                             expert_ids: torch.Tensor,
                             num_tokens_post_padded: torch.Tensor,
@@ -479,6 +545,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
                             use_int8_w8a8: bool,
                             use_int8_w8a16: bool,
                             use_int4_w4a16: bool,
+                            orig_acc_dtype: torch.dtype,
                             per_channel_quant: bool,
                             block_shape: Optional[List[int]] = None) -> None:
     assert topk_weights is not None or not mul_routed_weight
@@ -497,7 +564,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
         EM = min(sorted_token_ids.shape[0],
                  A.shape[0] * top_k * config['BLOCK_SIZE_M'])
     grid = lambda META: (triton.cdiv(EM, META['BLOCK_SIZE_M']) * triton.cdiv(
-        B.shape[1], META['BLOCK_SIZE_N']), )
+        B.shape[1], META['BLOCK_SIZE_N']), META['SPLIT_K'] )
 
     if (use_int8_w8a16 or use_int4_w4a16) and \
             block_shape is not None and block_shape[1] > 0:
@@ -509,26 +576,28 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             group_size=block_shape[1],
             num_experts=B.shape[0],
             bit=4 if use_int4_w4a16 else 8)
-        config = config.copy()
-        config.update(
-            get_moe_wna16_block_config(config=config,
-                                       use_moe_wna16_cuda=use_moe_wna16_cuda,
-                                       num_valid_tokens=num_tokens,
-                                       size_k=A.shape[1],
-                                       size_n=B.shape[1],
-                                       num_experts=B.shape[1],
-                                       group_size=block_shape[1],
-                                       real_top_k=top_k,
-                                       block_size_m=config["BLOCK_SIZE_M"]))
-
-        if use_moe_wna16_cuda:
+
+        # TODO: update config for moe_wan16_gemm
+        # config = config.copy()
+        # config.update(
+        #     get_moe_wna16_block_config(config=config,
+        #                                use_moe_wna16_cuda=use_moe_wna16_cuda,
+        #                                num_valid_tokens=num_tokens,
+        #                                size_k=A.shape[1],
+        #                                size_n=B.shape[1],
+        #                                num_experts=B.shape[1],
+        #                                group_size=block_shape[1],
+        #                                real_top_k=top_k,
+        #                                block_size_m=config["BLOCK_SIZE_M"]))
+
+        if False and use_moe_wna16_cuda:
             bit = 4 if use_int4_w4a16 else 8
             ops.moe_wna16_gemm(A, C, B, B_scale, B_zp,
                                topk_weights if mul_routed_weight else None,
                                sorted_token_ids, expert_ids,
                                num_tokens_post_padded, top_k,
                                config["BLOCK_SIZE_M"], config["BLOCK_SIZE_N"],
-                               config["BLOCK_SIZE_K"], bit)
+                               config["BLOCK_SIZE_K"] * config["SPLIT_K"], bit)
             return
 
         fused_moe_kernel_gptq_awq[grid](
@@ -558,7 +627,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             B_zp.stride(0) if B_zp is not None else 0,
             B_zp.stride(2) if B_zp is not None else 0,
             B_zp.stride(1) if B_zp is not None else 0,
-            block_k_diviable=A.shape[1] % config["BLOCK_SIZE_K"] == 0,
+            block_k_diviable=A.shape[1] % config["BLOCK_SIZE_K"] * config["SPLIT_K"] == 0,
             group_size=block_shape[1],
             MUL_ROUTED_WEIGHT=mul_routed_weight,
             top_k=top_k,
@@ -584,6 +653,7 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             sorted_token_ids,
             expert_ids,
             num_tokens_post_padded,
+            B.shape[0],
             B.shape[1],
             B.shape[2],
             EM,
@@ -595,29 +665,28 @@ def invoke_fused_moe_kernel(A: torch.Tensor,
             B.stride(1),
             C.stride(1),
             C.stride(2),
-            A_scale.stride(0)
-            if A_scale is not None and A_scale.ndim == 2 else 0,
-            A_scale.stride(1)
-            if A_scale is not None and A_scale.ndim == 2 else 0,
-            B_scale.stride(0)
-            if B_scale is not None and B_scale.ndim >= 2 else 0,
-            B_scale.stride(2)
-            if B_scale is not None and B_scale.ndim == 3 else 0,
-            B_scale.stride(1)
-            if B_scale is not None and B_scale.ndim >= 2 else 0,
+            A_scale.stride(0) if A_scale is not None and A_scale.ndim == 2 else 0,
+            A_scale.stride(1) if A_scale is not None and A_scale.ndim == 2 else 0,
+            B_scale.stride(0) if B_scale is not None and B_scale.ndim >= 2 else 0,
+            B_scale.stride(2) if B_scale is not None and B_scale.ndim == 3 else 0,
+            B_scale.stride(1) if B_scale is not None and B_scale.ndim >= 2 else 0,
             0 if block_shape is None else block_shape[0],
             0 if block_shape is None else block_shape[1],
             MUL_ROUTED_WEIGHT=mul_routed_weight,
             top_k=top_k,
+            # experts_num=expert_ids.shape[0],
             compute_type=compute_type,
             use_fp8_w8a8=use_fp8_w8a8,
             use_int8_w8a8=use_int8_w8a8,
             use_int8_w8a16=use_int8_w8a16,
             per_channel_quant=per_channel_quant,
             BLOCK_SIZE_K=BLOCK_SIZE_K,
+            FAST_F32_TO_BF16 = True,
             **config,
         )
-
+    if config["ACCF32"]:
+       C = C.to(orig_acc_dtype)
+    return C
 
 # Adapted from: https://github.com/sgl-project/sglang/pull/2628
 def get_config_file_name(E: int,
@@ -625,6 +694,7 @@ def get_config_file_name(E: int,
                          dtype: Optional[str],
                          block_shape: Optional[List[int]] = None) -> str:
     device_name = current_platform.get_device_name().replace(" ", "_")
+    device_name = "Device_4000"
     dtype_selector = "" if not dtype else f",dtype={dtype}"
     block_shape_selector = ("" if not block_shape or not all(block_shape) else
                             f",block_shape={block_shape}").replace(" ", "")
@@ -639,6 +709,7 @@ def get_moe_configs(
     dtype: Optional[str],
     block_n: Optional[int] = None,
     block_k: Optional[int] = None,
+    H: int = 0,
 ) -> Optional[Dict[int, Any]]:
     """
     Return optimized configurations for the fused MoE kernel.
@@ -653,9 +724,17 @@ def get_moe_configs(
     # directory
     block_shape = [block_n, block_k] if block_n and block_k else None
     json_file_name = get_config_file_name(E, N, dtype, block_shape)
+    json_file_name_new = f"H={H},{json_file_name}"
 
     config_file_path = os.path.join(
         os.path.dirname(os.path.realpath(__file__)), "configs", json_file_name)
+    config_file_path_new = os.path.join(
+        os.path.dirname(os.path.realpath(__file__)), "configs", json_file_name_new)
+
+    # First find H, E, N config file
+    if os.path.exists(config_file_path_new):
+        config_file_path = config_file_path_new
+
     if os.path.exists(config_file_path):
         with open(config_file_path) as f:
             logger.info("Using configuration from %s for MoE layer.",
@@ -756,21 +835,22 @@ def get_default_config(
             "num_warps": 4,
             "num_stages": 3,
         }
-    elif dtype in ["int4_w4a16", "int8_w8a16"] and block_shape is not None:
-        # moe wna16 kernels
-        # only set BLOCK_SIZE_M
-        # BLOCK_SIZE_N and BLOCK_SIZE_K would be set later
-        bit = 4 if dtype == "int4_w4a16" else 8
-        use_moe_wna16_cuda = should_moe_wna16_use_cuda(M * topk,
-                                                       block_shape[1], E, bit)
-        if use_moe_wna16_cuda:
-            config = {"BLOCK_SIZE_M": min(16, M)}
-        elif M <= 20:
-            config = {"BLOCK_SIZE_M": 16, "GROUP_SIZE_M": 1}
-        elif M <= 40:
-            config = {"BLOCK_SIZE_M": 32, "GROUP_SIZE_M": 1}
-        else:
-            config = {"BLOCK_SIZE_M": 64, "GROUP_SIZE_M": 1}
+    # TODO: missing config for BLOCK_SIZE_K
+    # elif dtype in ["int4_w4a16", "int8_w8a16"] and block_shape is not None:
+    #     # moe wna16 kernels
+    #     # only set BLOCK_SIZE_M
+    #     # BLOCK_SIZE_N and BLOCK_SIZE_K would be set later
+    #     bit = 4 if dtype == "int4_w4a16" else 8
+    #     use_moe_wna16_cuda = should_moe_wna16_use_cuda(M * topk,
+    #                                                    block_shape[1], E, bit)
+    #     if use_moe_wna16_cuda:
+    #         config = {"BLOCK_SIZE_M": min(16, M)}
+    #     elif M <= 20:
+    #         config = {"BLOCK_SIZE_M": 16, "GROUP_SIZE_M": 1}
+    #     elif M <= 40:
+    #         config = {"BLOCK_SIZE_M": 32, "GROUP_SIZE_M": 1}
+    #     else:
+    #         config = {"BLOCK_SIZE_M": 64, "GROUP_SIZE_M": 1}
     elif is_marlin:
         for block_size_m in [8, 16, 32, 48, 64]:
             if M * topk / E / block_size_m < 0.9:
@@ -801,6 +881,7 @@ def try_get_optimal_moe_config(
     M: int,
     is_marlin: bool = False,
     block_shape: Optional[List[int]] = None,
+    H: int = 0,
 ):
     from vllm.model_executor.layers.fused_moe import get_config
     override_config = get_config()
@@ -809,11 +890,12 @@ def try_get_optimal_moe_config(
     else:
         # First try to load optimal config from the file
         E, _, N = w2_shape
-        if dtype == "int4_w4a16":
-            N = N * 2
+        # TODO: why we need N * 2
+        # if dtype == "int4_w4a16":
+        #     N = N * 2
         block_n = block_shape[0] if block_shape else 0
         block_k = block_shape[1] if block_shape else 0
-        configs = get_moe_configs(E, N, dtype, block_n, block_k)
+        configs = get_moe_configs(E, N, dtype, block_n, block_k, H)
 
         if configs:
             # If an optimal configuration map has been found, look up the
@@ -947,10 +1029,13 @@ def grouped_topk(
 def get_config_dtype_str(
         dtype: torch.dtype,
         use_int4_w4a16: Optional[bool] = False,
+        use_int8_w8a8: Optional[bool] = False,
         use_int8_w8a16: Optional[bool] = False,
         use_fp8_w8a8: Optional[bool] = False) -> Optional[str]:
     if use_fp8_w8a8:
         return "fp8_w8a8"
+    elif use_int8_w8a8:
+        return "int8_w8a8"
     elif use_int8_w8a16:
         return "int8_w8a16"
     elif use_int4_w4a16:
@@ -1062,7 +1147,7 @@ def outplace_fused_experts_fake(
         w2: torch.Tensor,
         topk_weights: torch.Tensor,
         topk_ids: torch.Tensor,
-        activation: str = "silu",
+        activation: str = None, # To compatible with torch 2.4, str default value need to be None
         use_fp8_w8a8: bool = False,
         use_int8_w8a8: bool = False,
         use_int8_w8a16: bool = False,
@@ -1208,7 +1293,7 @@ def moe_kernel_prepare_input(
             # activation channel-wise int8 quantization
             assert (per_channel_quant
                     ), "int8 quantization only supports block or channel-wise"
-            A, A_scale = per_token_quant_int8(A)
+            A, A_scale, _= ops.scaled_int8_quant(A, A_scale)
         else:
             # activation block-wise int8 quantization
             assert len(block_shape) == 2
@@ -1264,6 +1349,7 @@ def fused_experts_impl(hidden_states: torch.Tensor,
         torch.float32, torch.float16, torch.bfloat16
     ]
 
+    H = hidden_states.shape[-1]
     num_tokens, _ = hidden_states.shape
     E, N, _ = w1.shape
     K = w2.shape[1]
@@ -1275,6 +1361,7 @@ def fused_experts_impl(hidden_states: torch.Tensor,
     CHUNK_SIZE = envs.VLLM_FUSED_MOE_CHUNK_SIZE
     M = min(num_tokens, CHUNK_SIZE)
     config_dtype = get_config_dtype_str(use_fp8_w8a8=use_fp8_w8a8,
+                                        use_int8_w8a8=use_int8_w8a8, 
                                         use_int8_w8a16=use_int8_w8a16,
                                         use_int4_w4a16=use_int4_w4a16,
                                         dtype=hidden_states.dtype)
@@ -1286,18 +1373,57 @@ def fused_experts_impl(hidden_states: torch.Tensor,
         top_k_num,
         config_dtype,
         block_shape=block_shape,
+        H=H,
     )
 
     config = get_config_func(M)
 
-    # We can reuse the memory between these because by the time we need
+    # TODO: We can reuse the memory between these because by the time we need
     # cache3, we're done with cache1
-    cache13 = torch.empty(M * top_k_num * max(N, K),
-                          device=hidden_states.device,
-                          dtype=hidden_states.dtype)
-    intermediate_cache1 = cache13[:M * top_k_num * N].view(M, top_k_num, N)
-    intermediate_cache3 = cache13[:M * top_k_num * K].view(M, top_k_num, K)
+    # cache13 = torch.empty(M * top_k_num * max(N, K),
+    #                       device=hidden_states.device,
+    #                       dtype=hidden_states.dtype)
+    
+    stage1_config = config["stage1"] if "stage1" in config else config
+    stage2_config = config["stage2"] if "stage2" in config else config
+    
+    if 'ACCF32' not in stage1_config:
+        stage1_config['ACCF32'] = False
+    if 'ACCF32' not in stage2_config:
+        stage2_config['ACCF32'] = False
+    if 'SPLIT_K' not in stage1_config:
+        stage1_config['SPLIT_K'] = 1
+    if 'SPLIT_K' not in stage2_config:
+        stage2_config['SPLIT_K'] = 1    
+
+    if stage1_config['ACCF32']:
+       acc_type1 = torch.float32
+    else:
+       acc_type1 = hidden_states.dtype
+    if stage2_config['ACCF32']:
+       acc_type2 = torch.float32
+    else:
+       acc_type2 = hidden_states.dtype
+       
 
+    if stage1_config['SPLIT_K'] > 1:
+        intermediate_cache1 = torch.zeros((M, topk_ids.shape[1], N),
+                                          device=hidden_states.device,
+                                          dtype=acc_type1)
+    else:
+        intermediate_cache1 = torch.empty((M, topk_ids.shape[1], N),
+                                          device=hidden_states.device,
+                                          dtype=hidden_states.dtype)
+        
+    if stage2_config['SPLIT_K'] > 1:
+        intermediate_cache3 = torch.zeros((M, topk_ids.shape[1], w2.shape[1]),
+                                          device=hidden_states.device,
+                                          dtype=acc_type2)
+    else:
+        intermediate_cache3 = torch.empty((M, topk_ids.shape[1], w2.shape[1]),
+                                          device=hidden_states.device,
+                                          dtype=hidden_states.dtype)
+        
     # This needs separate memory since it's used concurrently with cache1
     intermediate_cache2 = torch.empty((M * top_k_num, N // 2),
                                       device=hidden_states.device,
@@ -1341,40 +1467,50 @@ def fused_experts_impl(hidden_states: torch.Tensor,
         curr_topk_ids = topk_ids[begin_chunk_idx:end_chunk_idx]
         curr_topk_weights = topk_weights[begin_chunk_idx:end_chunk_idx]
 
-        qcurr_hidden_states, qa1_scale = moe_kernel_prepare_input(
-            A=curr_hidden_states,
-            B=w1,
-            A_scale=a1_scale,
-            B_scale=w1_scale,
-            use_fp8_w8a8=use_fp8_w8a8,
-            use_int8_w8a8=use_int8_w8a8,
-            use_int8_w8a16=use_int8_w8a16,
-            use_int4_w4a16=use_int4_w4a16,
-            per_channel_quant=per_channel_quant,
-            block_shape=block_shape)
 
         sorted_token_ids, expert_ids, num_tokens_post_padded = (
-            moe_align_block_size(curr_topk_ids, config['BLOCK_SIZE_M'],
+            moe_align_block_size(curr_topk_ids, stage1_config['BLOCK_SIZE_M'],
                                  global_num_experts, expert_map))
-
-        invoke_fused_moe_kernel(qcurr_hidden_states,
+        if (stage1_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and (topk_ids.shape[1] == 1 or topk_ids.shape[1] == 2) and
+            (curr_hidden_states.dtype == torch.bfloat16 or curr_hidden_states.dtype == torch.float16) and
+            w1.shape[1] % 4 == 0 and w1.shape[2] % 8 == 0):
+            ops.fused_moe_kernel(curr_hidden_states, w1, intermediate_cache1,
+                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
+                                expert_ids, num_tokens_post_padded, False,
+                                topk_ids.shape[1], 0)
+        else:
+            qcurr_hidden_states, qa1_scale = moe_kernel_prepare_input(
+                                A=curr_hidden_states,
+                                B=w1,
+                                A_scale=a1_scale,
+                                B_scale=w1_scale,
+                                use_fp8_w8a8=use_fp8_w8a8,
+                                use_int8_w8a8=use_int8_w8a8,
+                                use_int8_w8a16=use_int8_w8a16,
+                                use_int4_w4a16=use_int4_w4a16,
+                                per_channel_quant=per_channel_quant,
+                                block_shape=block_shape)
+            
+            invoke_fused_moe_kernel(qcurr_hidden_states,
                                 w1,
                                 intermediate_cache1,
                                 qa1_scale,
                                 w1_scale,
                                 w1_zp,
                                 curr_topk_weights,
+                                curr_topk_ids,
                                 sorted_token_ids,
                                 expert_ids,
                                 num_tokens_post_padded,
                                 apply_router_weight_on_input,
                                 top_k_num,
-                                config,
+                                stage1_config,
                                 compute_type=compute_type,
                                 use_fp8_w8a8=use_fp8_w8a8,
                                 use_int8_w8a8=use_int8_w8a8,
                                 use_int8_w8a16=use_int8_w8a16,
                                 use_int4_w4a16=use_int4_w4a16,
+                                orig_acc_dtype=hidden_states.dtype,
                                 per_channel_quant=per_channel_quant,
                                 block_shape=block_shape)
 
@@ -1386,39 +1522,51 @@ def fused_experts_impl(hidden_states: torch.Tensor,
                                       intermediate_cache1.view(-1, N))
         else:
             raise ValueError(f"Unsupported FusedMoe activation: {activation}")
-
-        qintermediate_cache2, qa2_scale = moe_kernel_prepare_input(
-            A=intermediate_cache2,
-            B=w2,
-            A_scale=a2_scale,
-            B_scale=w2_scale,
-            use_fp8_w8a8=use_fp8_w8a8,
-            use_int8_w8a8=use_int8_w8a8,
-            use_int8_w8a16=use_int8_w8a16,
-            use_int4_w4a16=use_int4_w4a16,
-            per_channel_quant=per_channel_quant,
-            block_shape=block_shape)
-
-        invoke_fused_moe_kernel(qintermediate_cache2,
-                                w2,
-                                intermediate_cache3,
-                                qa2_scale,
-                                w2_scale,
-                                w2_zp,
-                                curr_topk_weights,
-                                sorted_token_ids,
-                                expert_ids,
-                                num_tokens_post_padded,
-                                not apply_router_weight_on_input,
-                                1,
-                                config,
-                                compute_type=compute_type,
-                                use_fp8_w8a8=use_fp8_w8a8,
-                                use_int8_w8a8=use_int8_w8a8,
-                                use_int8_w8a16=use_int8_w8a16,
-                                use_int4_w4a16=use_int4_w4a16,
-                                per_channel_quant=per_channel_quant,
-                                block_shape=block_shape)
+        
+        if (stage2_config['BLOCK_SIZE_M'] == 128 and use_int8_w8a8==False and w2.shape[1] % 4 == 0 and w2.shape[2] % 8 == 0 and
+            (hidden_states.dtype == torch.bfloat16 or hidden_states.dtype == torch.float16)):
+            ops.fused_moe_kernel(intermediate_cache2, w2, intermediate_cache3,
+                                curr_topk_weights, curr_topk_ids, sorted_token_ids,
+                                expert_ids, num_tokens_post_padded, True, 1, 0)
+        else:
+            if stage2_config['BLOCK_SIZE_M'] != stage1_config['BLOCK_SIZE_M']:
+                sorted_token_ids, expert_ids, num_tokens_post_padded = (
+                moe_align_block_size(curr_topk_ids, stage2_config['BLOCK_SIZE_M'], global_num_experts, expert_map))
+                
+            qintermediate_cache2, qa2_scale = moe_kernel_prepare_input(
+                                    A=intermediate_cache2,
+                                    B=w2,
+                                    A_scale=a2_scale,
+                                    B_scale=w2_scale,
+                                    use_fp8_w8a8=use_fp8_w8a8,
+                                    use_int8_w8a8=use_int8_w8a8,
+                                    use_int8_w8a16=use_int8_w8a16,
+                                    use_int4_w4a16=use_int4_w4a16,
+                                    per_channel_quant=per_channel_quant,
+                                    block_shape=block_shape)
+
+            invoke_fused_moe_kernel(qintermediate_cache2,
+                                    w2,
+                                    intermediate_cache3,
+                                    qa2_scale,
+                                    w2_scale,
+                                    w2_zp,
+                                    curr_topk_weights,
+                                    curr_topk_ids,
+                                    sorted_token_ids,
+                                    expert_ids,
+                                    num_tokens_post_padded,
+                                    True,
+                                    1,
+                                    stage2_config,
+                                    compute_type=compute_type,
+                                    use_fp8_w8a8=use_fp8_w8a8,
+                                    use_int8_w8a8=use_int8_w8a8,
+                                    use_int8_w8a16=use_int8_w8a16,
+                                    use_int4_w4a16=use_int4_w4a16,
+                                    orig_acc_dtype=hidden_states.dtype,
+                                    per_channel_quant=per_channel_quant,
+                                    block_shape=block_shape)
 
         ops.moe_sum(intermediate_cache3.view(*intermediate_cache3.shape),
                     out_hidden_states[begin_chunk_idx:end_chunk_idx])
@@ -1478,7 +1626,7 @@ def fused_moe(
         products for w1 and w2. Defaults to False.
     - use_int8_w8a8 (bool): If True, use int8 arithmetic to compute the inner
         products for w1 and w2. Defaults to False.
-    - use_int8_w8a16 (bool): If True, use matmul of int8 weight and bf16/fp16
+    - use_int8_w8a16 (bool): If True, use weight int8, activation int16 arithmetic to compute the inner
         activation to compute the inner products for w1 and w2.
         Defaults to False.
     - use_int4_w4a16 (bool): If True, use matmul of int4 weight and bf16/fp16
diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
index 794de4c38..c6b4306ad 100644
--- a/vllm/model_executor/layers/linear.py
+++ b/vllm/model_executor/layers/linear.py
@@ -27,6 +27,8 @@ from vllm.model_executor.parameter import (BasevLLMParameter,
 # yapf: enable
 from vllm.model_executor.utils import set_weight_attrs
 
+import vllm.envs as envs
+
 logger = init_logger(__name__)
 
 WEIGHT_LOADER_V2_SUPPORTED = [
@@ -104,7 +106,11 @@ def adjust_scalar_to_fused_array(param, loaded_weight, shard_id):
         assert loaded_weight.shape[0] == 1
         loaded_weight = loaded_weight[0]
 
-    return param[shard_id], loaded_weight
+    # Support gemm_tn->gemm_nn
+    if envs.MACA_VLLM_USE_TN_2_NN:
+        return param[shard_id], loaded_weight.t()
+    else:
+        return param[shard_id], loaded_weight
 
 
 # TODO(Isotr0py): We might need a more flexible structure to handle
@@ -186,10 +192,17 @@ class UnquantizedLinearMethod(LinearMethodBase):
                        output_partition_sizes: list[int], input_size: int,
                        output_size: int, params_dtype: torch.dtype,
                        **extra_weight_attrs):
-        weight = Parameter(torch.empty(sum(output_partition_sizes),
-                                       input_size_per_partition,
+        # Support gemm_tn->gemm_nn here
+        if envs.MACA_VLLM_USE_TN_2_NN:
+            weight = Parameter(torch.empty(input_size_per_partition,
+                                       sum(output_partition_sizes),
                                        dtype=params_dtype),
                            requires_grad=False)
+        else:
+            weight = Parameter(torch.empty(sum(output_partition_sizes),
+                                           input_size_per_partition,
+                                           dtype=params_dtype),
+                           requires_grad=False)
         set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
         layer.register_parameter("weight", weight)
         set_weight_attrs(weight, extra_weight_attrs)
@@ -198,8 +211,11 @@ class UnquantizedLinearMethod(LinearMethodBase):
               layer: torch.nn.Module,
               x: torch.Tensor,
               bias: Optional[torch.Tensor] = None) -> torch.Tensor:
-
-        return dispatch_unquantized_gemm()(x, layer.weight, bias)
+        # Support gemm_tn->gemm_nn here
+        if envs.MACA_VLLM_USE_TN_2_NN and x.shape[-1] == layer.weight.shape[0]:
+            return dispatch_unquantized_gemm()(x, layer.weight.t(), bias)
+        else:
+            return dispatch_unquantized_gemm()(x, layer.weight, bias)
 
 
 class LinearBase(torch.nn.Module):
@@ -319,6 +335,11 @@ class ReplicatedLinear(LinearBase):
         if len(loaded_weight.shape) == 0:
             loaded_weight = loaded_weight.reshape(1)
 
+        # Support gemm_tn->gemm_nn here
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
+            loaded_weight = loaded_weight.t()
+
         assert param.size() == loaded_weight.size(), (
             f"Tried to load weights of size {loaded_weight.size()}"
             f"to a parameter of size {param.size()}")
@@ -436,6 +457,8 @@ class ColumnParallelLinear(LinearBase):
         # bitsandbytes loads the weights of the specific portion
         # no need to narrow
         is_sharded_weight = is_sharded_weight or use_bitsandbytes_4bit
+        # Support gemm_tn->gemm_nn here
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
 
         # Special case for GGUF
         is_gguf_weight = getattr(param, "is_gguf_weight", False)
@@ -454,7 +477,13 @@ class ColumnParallelLinear(LinearBase):
 
         param_data = param.data
         if output_dim is not None and not is_sharded_weight:
-            shard_size = param_data.shape[output_dim]
+            
+            # Support gemm_tn->gemm_nn here
+            if not envs.MACA_VLLM_USE_TN_2_NN or len(param_data.shape)==1 or is_quantization:
+                shard_size = param_data.shape[output_dim] 
+            else:
+                shard_size = param_data.shape[int(not(output_dim))]
+                
             start_idx = tp_rank * shard_size
             loaded_weight = loaded_weight.narrow(output_dim, start_idx,
                                                  shard_size)
@@ -464,6 +493,10 @@ class ColumnParallelLinear(LinearBase):
         if len(loaded_weight.shape) == 0:
             loaded_weight = loaded_weight.reshape(1)
 
+        # Support gemm_tn->gemm_nn here
+        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
+            loaded_weight = loaded_weight.t()
+
         assert param_data.shape == loaded_weight.shape
         param_data.copy_(loaded_weight)
 
@@ -595,7 +628,10 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
         is_metadata = getattr(param, "is_metadata", False)
         # Special case for per-tensor scale to load scalar into fused array.
         needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
-
+        
+        # Support gemm_tn->gemm_nn here
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+        
         if loaded_shard_id is None:
             # Loaded weight is already fused on disk (mlp).
             # (e.g., Phi-3's gate_up_proj).
@@ -675,8 +711,12 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
                 shard_offset = loaded_weight.shape[output_dim] * \
                     loaded_shard_id
 
-            param_data = param_data.narrow(output_dim, shard_offset,
-                                           shard_size)
+            # Support gemm_tn->gemm_nn here
+            if not envs.MACA_VLLM_USE_TN_2_NN or is_quantization:
+                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
+            else:
+                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
+                
             start_idx = tp_rank * shard_size
             if not is_sharded_weight:
                 loaded_weight = loaded_weight.narrow(output_dim, start_idx,
@@ -700,7 +740,9 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
                     "Loading a weight without `output_dim` attribute in "
                     "MergedColumnParallelLinear, assume the weight is "
                     "the same for all partitions.")
-
+        # Support gemm_tn->gemm_nn here
+        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
+            loaded_weight = loaded_weight.t()
         assert param_data.shape == loaded_weight.shape
         param_data.copy_(loaded_weight)
 
@@ -990,7 +1032,10 @@ class QKVParallelLinear(ColumnParallelLinear):
 
         # Special case for per-tensor scales in fused case.
         needs_scalar_to_array = getattr(param, "needs_scalar_to_array", False)
-
+        
+        # Support gemm_tn->gemm_nn here
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+        
         if loaded_shard_id is None:
             # Loaded weight is already fused on disk (qkv).
             # (e.g., Phi-3's qkv_proj).
@@ -1097,8 +1142,12 @@ class QKVParallelLinear(ColumnParallelLinear):
                 shard_size, shard_offset = adjust_bitsandbytes_4bit_shard(
                     param, orig_qkv_offsets, loaded_shard_id)
 
-            param_data = param_data.narrow(output_dim, shard_offset,
-                                           shard_size)
+            # Support gemm_tn->gemm_nn here
+            if not envs.MACA_VLLM_USE_TN_2_NN or len(param_data.shape)==1 or is_quantization:
+                param_data = param_data.narrow(output_dim, shard_offset,shard_size)
+            else:
+                param_data = param_data.narrow(int(not(output_dim)), shard_offset,shard_size)
+            
             if loaded_shard_id == "q":
                 shard_id = tp_rank
             else:
@@ -1128,6 +1177,10 @@ class QKVParallelLinear(ColumnParallelLinear):
                     "QKVParallelLinear, assume the weight is the same "
                     "for all partitions.")
 
+        # Support gemm_tn->gemm_nn here
+        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
+            loaded_weight = loaded_weight.t()
+
         assert param_data.shape == loaded_weight.shape
         param_data.copy_(loaded_weight)
 
@@ -1239,8 +1292,15 @@ class RowParallelLinear(LinearBase):
             param.materialize(tuple(weight_shape), dtype=loaded_weight.dtype)
 
         param_data = param.data
+        # Support gemm_tn->gemm_nn here
+        is_quantization = not isinstance(self.quant_method, UnquantizedLinearMethod)
+        
         if input_dim is not None and not is_sharded_weight:
-            shard_size = param_data.shape[input_dim]
+            # Support gemm_tn->gemm_nn here
+            if not envs.MACA_VLLM_USE_TN_2_NN or is_quantization:
+                shard_size = param_data.shape[input_dim]
+            else:
+                shard_size = param_data.shape[int(not(input_dim))]
             start_idx = tp_rank * shard_size
             loaded_weight = loaded_weight.narrow(input_dim, start_idx,
                                                  shard_size)
@@ -1250,6 +1310,9 @@ class RowParallelLinear(LinearBase):
         if len(loaded_weight.shape) == 0:
             loaded_weight = loaded_weight.reshape(1)
 
+        # Support gemm_tn->gemm_nn here
+        if envs.MACA_VLLM_USE_TN_2_NN and not is_quantization:
+            loaded_weight = loaded_weight.t()
         assert param_data.shape == loaded_weight.shape
         param_data.copy_(loaded_weight)
 
diff --git a/vllm/model_executor/layers/quantization/__init__.py b/vllm/model_executor/layers/quantization/__init__.py
index 15e08220b..b4f3619ad 100644
--- a/vllm/model_executor/layers/quantization/__init__.py
+++ b/vllm/model_executor/layers/quantization/__init__.py
@@ -8,6 +8,7 @@ from vllm.model_executor.layers.quantization.base_config import (
 QuantizationMethods = Literal[
     "aqlm",
     "awq",
+    "gptq",
     "deepspeedfp",
     "tpu_int8",
     "fp8",
@@ -22,7 +23,7 @@ QuantizationMethods = Literal[
     "gptq_marlin",
     "gptq_bitblas",
     "awq_marlin",
-    "gptq",
+    # "gptq",
     "compressed-tensors",
     "bitsandbytes",
     "qqq",
@@ -123,9 +124,9 @@ def get_quantization_config(quantization: str) -> Type[QuantizationConfig]:
         "bitblas": BitBLASConfig,
         "gguf": GGUFConfig,
         "gptq_marlin_24": GPTQMarlin24Config,
-        "gptq_marlin": GPTQMarlinConfig,
+        "gptq_marlin": GPTQConfig,
         "gptq_bitblas": GPTQBitBLASConfig,
-        "awq_marlin": AWQMarlinConfig,
+        "awq_marlin": AWQConfig,
         "gptq": GPTQConfig,
         "compressed-tensors": CompressedTensorsConfig,
         "bitsandbytes": BitsAndBytesConfig,
diff --git a/vllm/model_executor/layers/quantization/awq.py b/vllm/model_executor/layers/quantization/awq.py
index 227be1497..75856ea38 100644
--- a/vllm/model_executor/layers/quantization/awq.py
+++ b/vllm/model_executor/layers/quantization/awq.py
@@ -11,7 +11,7 @@ from vllm.model_executor.layers.quantization.base_config import (
     QuantizationConfig)
 from vllm.model_executor.parameter import (GroupQuantScaleParameter,
                                            PackedvLLMParameter)
-
+from vllm.utils import direct_register_custom_op
 
 class AWQConfig(QuantizationConfig):
     """Config class for AWQ.
@@ -48,7 +48,7 @@ class AWQConfig(QuantizationConfig):
         return "awq"
 
     def get_supported_act_dtypes(self) -> List[torch.dtype]:
-        return [torch.half]
+        return [torch.half, torch.bfloat16]
 
     @classmethod
     def get_min_capability(cls) -> int:
@@ -85,6 +85,60 @@ def is_layer_skipped_awq(prefix: str, modules_to_not_convert: List[str]):
     return any(module_name in prefix for module_name in modules_to_not_convert)
 
 
+
+def _apply_awq_fake(x: torch.Tensor,
+                    qweight: torch.Tensor,
+                    scales: torch.Tensor,
+                    qzeros: torch.Tensor,
+                    bias: torch.Tensor,
+                    pack_factor: int,
+                    group_size: int) -> torch.Tensor:
+    out_shape = ()
+    if group_size % 32:
+        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
+    else:
+        out_shape = (x.shape[:-1] + (qweight.shape[0], ))
+    return torch.empty(out_shape, dtype=x.dtype, device=x.device)
+
+def _apply_awq(x: torch.Tensor,
+               qweight: torch.Tensor,
+               scales: torch.Tensor,
+               qzeros: torch.Tensor,
+               bias: torch.Tensor,
+               pack_factor: int,
+               group_size: int) -> torch.Tensor:
+    out_shape = ()
+    reshaped_x = x.reshape(-1, x.shape[-1])
+    out = torch.empty(0)          
+    # num_tokens >= threshold
+    FP16_MATMUL_HEURISTIC_CONDITION = x.shape[:-1].numel() >= 256
+    # if (FP16_MATMUL_HEURISTIC_CONDITION and reshaped_x.dtype == torch.half) or self.quant_config.group_size != 128:
+    if group_size % 32:
+        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
+        out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)
+        out = torch.matmul(reshaped_x, out)
+    else:
+        num_out_channel = qweight.shape[0]
+        out_shape = (x.shape[:-1] + (num_out_channel, ))
+        temp_space = torch.empty(0, dtype=torch.float32, device=x.device)
+        if reshaped_x.dtype == torch.bfloat16:
+            temp_space = torch.zeros(reshaped_x.shape[0], num_out_channel,
+                                        dtype=torch.float32, device=x.device)
+        out = ops.awq_gemm(reshaped_x, qweight, qzeros, scales,
+                            pack_factor, temp_space,
+                            True if reshaped_x.dtype == torch.bfloat16 else False)
+    if bias is not None:
+        out.add_(bias)
+    return out.reshape(out_shape)
+
+direct_register_custom_op(
+    op_name="_apply_awq",
+    op_func=_apply_awq,
+    mutates_args=[],
+    fake_impl=_apply_awq_fake,
+    tags=(torch.Tag.needs_fixed_stride_order, ),
+)
+
 class AWQLinearMethod(LinearMethodBase):
     """Linear method for AWQ.
 
@@ -158,6 +212,12 @@ class AWQLinearMethod(LinearMethodBase):
                                           requires_grad=False)
         layer.scales = torch.nn.Parameter(layer.scales.data,
                                           requires_grad=False)
+        # warmup
+        if self.quant_config.group_size % 32:
+            pass
+        else:
+            qweight = ops.awq_to_gptq_4bit(layer.qweight)
+            layer.qweight = torch.nn.Parameter(qweight, requires_grad=False)
 
     def apply(self,
               layer: torch.nn.Module,
@@ -167,18 +227,8 @@ class AWQLinearMethod(LinearMethodBase):
         scales = layer.scales
         qzeros = layer.qzeros
         pack_factor = self.quant_config.pack_factor
-        out_shape = (x.shape[:-1] + (qweight.shape[-1] * pack_factor, ))
-        reshaped_x = x.reshape(-1, x.shape[-1])
-
-        # num_tokens >= threshold
-        FP16_MATMUL_HEURISTIC_CONDITION = x.shape[:-1].numel() >= 256
-
-        if FP16_MATMUL_HEURISTIC_CONDITION:
-            out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)
-            out = torch.matmul(reshaped_x, out)
-        else:
-            out = ops.awq_gemm(reshaped_x, qweight, scales, qzeros,
-                               pack_factor)
-        if bias is not None:
-            out.add_(bias)
-        return out.reshape(out_shape)
+        group_size = self.quant_config.group_size
+        
+        return torch.ops.vllm._apply_awq(x, qweight, scales, qzeros, 
+                                        bias, pack_factor, group_size)
+        
\ No newline at end of file
diff --git a/vllm/model_executor/layers/quantization/base_config.py b/vllm/model_executor/layers/quantization/base_config.py
index 5ef11546f..5f3717c15 100644
--- a/vllm/model_executor/layers/quantization/base_config.py
+++ b/vllm/model_executor/layers/quantization/base_config.py
@@ -107,7 +107,11 @@ class QuantizationConfig(ABC):
            this method should only be overwritten by subclasses in exceptional 
            circumstances
         """
-        return None
+
+        if(user_quant != None):
+            return user_quant
+        else:
+            return hf_quant_cfg["quant_method"]   
 
     @staticmethod
     def get_from_keys(config: Dict[str, Any], keys: List[str]) -> Any:
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
index 7b0032572..0aca48781 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
@@ -194,7 +194,7 @@ class CompressedTensorsConfig(QuantizationConfig):
                                 error: bool = True,
                                 match_exact: bool = False) -> bool:
         capability_tuple = current_platform.get_device_capability()
-
+        """
         if capability_tuple is not None:
             capability = capability_tuple.to_int()
             if match_exact:
@@ -214,6 +214,8 @@ class CompressedTensorsConfig(QuantizationConfig):
             return supported
         else:
             return False
+        """
+        return False
 
     def _is_static_tensor_w8a8(self, weight_quant: BaseModel,
                                input_quant: BaseModel) -> bool:
@@ -271,6 +273,15 @@ class CompressedTensorsConfig(QuantizationConfig):
             input_quant.strategy == QuantizationStrategy.TENSOR)
         return is_symmetric_activation and is_per_tensor_activation
 
+    def _is_dynamic_token_int8_w8a8(self, weight_quant: BaseModel,
+                     input_quant: BaseModel) -> bool:
+        # Confirm weights and activations quantized.
+        if weight_quant is None or input_quant is None:
+            return False
+            
+        is_int8_w8a8 = (weight_quant.type == QuantizationType.INT and input_quant.type == QuantizationType.INT)
+        return is_int8_w8a8 and self._is_dynamic_token_w8a8(weight_quant, input_quant)  
+
     def _is_fp8_w8a8_sm90(self, weight_quant: BaseModel,
                           input_quant: BaseModel) -> bool:
         return (self._check_scheme_supported(90, error=False, match_exact=True)
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
index 721e36af2..1df6f382d 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
@@ -71,10 +71,142 @@ class CompressedTensorsMoEMethod(FusedMoEMethodBase):
             return CompressedTensorsW8A8Fp8MoECutlassMethod(quant_config)
         elif quant_config._is_fp8_w8a8(weight_quant, input_quant):
             return CompressedTensorsW8A8Fp8MoEMethod(quant_config)
+        elif quant_config._is_dynamic_token_int8_w8a8(weight_quant, input_quant):
+            return CompressedTensorsW8A8Int8MoEMethod(quant_config)
         else:
             raise RuntimeError(
                 f"Unsupported FusedMoe scheme: {weight_quant}, {input_quant}")
 
+class CompressedTensorsW8A8Int8MoEMethod(CompressedTensorsMoEMethod):
+
+    def __init__(
+            self,
+            quant_config: "CompressedTensorsConfig"  # type: ignore # noqa E501
+    ):
+        self.quant_config = quant_config
+        self.weight_quant = self.quant_config.target_scheme_map["Linear"].get(
+            "weights")
+        self.input_quant = self.quant_config.target_scheme_map["Linear"].get(
+            "input_activations")
+
+        per_channel = (
+            self.weight_quant.strategy == QuantizationStrategy.CHANNEL
+            and self.input_quant.strategy == QuantizationStrategy.TOKEN)
+        if not per_channel:
+            raise ValueError(
+                "For INT8 Fused MoE layers, we require channelwise, "
+                "dynamic per token quantization. Found "
+                f"{self.weight_quant}, {self.input_quant}")
+
+        self.static_input_scales = not self.input_quant.dynamic
+        if self.static_input_scales:
+            raise ValueError(
+                "For INT8 Fused MoE layers, we require channelwise, "
+                "dynamic per token quantization. Found static input scales.")
+
+    def create_weights(self, layer: torch.nn.Module, num_experts: int,
+                       hidden_size: int, intermediate_size_per_partition: int,
+                       params_dtype: torch.dtype, **extra_weight_attrs):
+
+        params_dtype = torch.int8
+
+        # WEIGHTS
+        w13_weight = torch.nn.Parameter(torch.empty(
+            num_experts,
+            2 * intermediate_size_per_partition,
+            hidden_size,
+            dtype=params_dtype),
+                                        requires_grad=False)
+        layer.register_parameter("w13_weight", w13_weight)
+        set_weight_attrs(w13_weight, extra_weight_attrs)
+
+        w2_weight = torch.nn.Parameter(torch.empty(
+            num_experts,
+            hidden_size,
+            intermediate_size_per_partition,
+            dtype=params_dtype),
+                                       requires_grad=False)
+        layer.register_parameter("w2_weight", w2_weight)
+        set_weight_attrs(w2_weight, extra_weight_attrs)
+
+        # WEIGHT_SCALES
+        assert self.weight_quant.strategy == QuantizationStrategy.CHANNEL
+        w13_weight_scale = torch.nn.Parameter(torch.ones(
+            num_experts,
+            2 * intermediate_size_per_partition,
+            1,
+            dtype=torch.float32),
+                                              requires_grad=False)
+        layer.register_parameter("w13_weight_scale", w13_weight_scale)
+        w2_weight_scale = torch.nn.Parameter(torch.ones(num_experts,
+                                                        hidden_size,
+                                                        1,
+                                                        dtype=torch.float32),
+                                             requires_grad=False)
+        layer.register_parameter("w2_weight_scale", w2_weight_scale)
+        # Add PER-CHANNEL quantization for FusedMoE.weight_loader.
+        extra_weight_attrs.update(
+            {"quant_method": FusedMoeWeightScaleSupported.CHANNEL.value})
+        set_weight_attrs(w13_weight_scale, extra_weight_attrs)
+        set_weight_attrs(w2_weight_scale, extra_weight_attrs)
+
+        # INPUT_SCALES
+        assert not self.static_input_scales
+        layer.w13_input_scale = None
+        layer.w2_input_scale = None
+
+    def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
+        pass
+
+    def apply(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        router_logits: torch.Tensor,
+        top_k: int,
+        renormalize: bool,
+        use_grouped_topk: bool = False,
+        topk_group: Optional[int] = None,
+        num_expert_group: Optional[int] = None,
+        global_num_experts: int = -1,
+        expert_map: Optional[torch.Tensor] = None,
+        custom_routing_function: Optional[Callable] = None,
+        scoring_func: str = "softmax",
+        e_score_correction_bias: Optional[torch.Tensor] = None,
+        apply_router_weight_on_input: bool = False,
+        activation: str = "silu",
+    ) -> torch.Tensor:
+        from vllm.model_executor.layers.fused_moe import fused_experts
+
+        topk_weights, topk_ids = FusedMoE.select_experts(
+            hidden_states=x,
+            router_logits=router_logits,
+            use_grouped_topk=use_grouped_topk,
+            top_k=top_k,
+            renormalize=renormalize,
+            topk_group=topk_group,
+            num_expert_group=num_expert_group,
+            custom_routing_function=custom_routing_function,
+            scoring_func=scoring_func,
+            e_score_correction_bias=e_score_correction_bias)
+
+        return fused_experts(
+            hidden_states=x,
+            w1=layer.w13_weight,
+            w2=layer.w2_weight,
+            topk_weights=topk_weights,
+            topk_ids=topk_ids,
+            inplace=True,
+            activation=activation,
+            apply_router_weight_on_input=apply_router_weight_on_input,
+            use_int8_w8a8=True,
+            per_channel_quant=True,
+            global_num_experts=global_num_experts,
+            expert_map=expert_map,
+            w1_scale=layer.w13_weight_scale,
+            w2_scale=layer.w2_weight_scale,
+            a1_scale=layer.w13_input_scale,
+            a2_scale=layer.w2_input_scale)
 
 class CompressedTensorsW8A8Fp8MoEMethod(CompressedTensorsMoEMethod):
 
diff --git a/vllm/model_executor/layers/quantization/gptq.py b/vllm/model_executor/layers/quantization/gptq.py
index 1c8d6cb1e..056102f9d 100644
--- a/vllm/model_executor/layers/quantization/gptq.py
+++ b/vllm/model_executor/layers/quantization/gptq.py
@@ -84,7 +84,7 @@ class GPTQConfig(QuantizationConfig):
 
     @classmethod
     def get_supported_act_dtypes(cls) -> List[torch.dtype]:
-        return [torch.half]
+        return [torch.half, torch.bfloat16]
 
     @classmethod
     # Need to figure it out
@@ -249,7 +249,7 @@ class GPTQLinearMethod(LinearMethodBase):
 
         # exllama needs to shuffle the weight after the weight is loaded
         # here we do the shuffle on first forward pass
-        if layer.exllama_state == ExllamaState.UNINITIALIZED:
+        if self.quant_config.group_size == 128 or self.quant_config.group_size == 64:
             if self.quant_config.desc_act:
                 layer.g_idx.data = torch.argsort(layer.g_idx).to(torch.int)
             else:
@@ -259,6 +259,63 @@ class GPTQLinearMethod(LinearMethodBase):
             layer.exllama_state = ExllamaState.READY
             ops.gptq_shuffle(layer.qweight, layer.g_idx,
                              self.quant_config.weight_bits)
+            
+            if layer.scales.dtype != torch.bfloat16:
+                perm_space = torch.empty(0)
+                temp_space = torch.empty(0)
+                if self.quant_config.weight_bits == 4:
+                    # warmup
+                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*8, dtype=layer.scales.dtype, device="cuda")
+                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
+                                   layer.scales, layer.g_idx,
+                                   layer.exllama_state == ExllamaState.READY,
+                                   self.quant_config.weight_bits,
+                                   self.quant_config.group_size,
+                                   perm_space, temp_space,
+                                   False)
+                if self.quant_config.weight_bits == 8:
+                    # warmup
+                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*4, dtype=layer.scales.dtype, device="cuda")
+                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
+                                   layer.scales, layer.g_idx,
+                                   layer.exllama_state == ExllamaState.READY,
+                                   self.quant_config.weight_bits,
+                                   self.quant_config.group_size,
+                                   perm_space, temp_space,
+                                   False)
+        else:
+            if layer.exllama_state == ExllamaState.UNINITIALIZED:
+                if self.quant_config.desc_act:
+                    layer.g_idx.data = torch.argsort(layer.g_idx).to(torch.int)
+                else:
+                    layer.g_idx.data = torch.empty((0, ),
+                                                   dtype=torch.int,
+                                                   device=layer.g_idx.device)
+                layer.exllama_state = ExllamaState.READY
+                ops.gptq_shuffle(layer.qweight, layer.g_idx,
+                                 self.quant_config.weight_bits)
+
+                """
+                perm_space = torch.empty(0)
+                if self.quant_config.weight_bits == 4:
+                    # warmup
+                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*8, dtype=layer.scales.dtype, device="cuda")
+                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
+                                   layer.scales, layer.g_idx,
+                                   layer.exllama_state == ExllamaState.READY,
+                                   self.quant_config.weight_bits,
+                                   self.quant_config.group_size,
+                                   perm_space)
+                if self.quant_config.weight_bits == 8:
+                    # warmup
+                    reshaped_x = torch.randn(1, layer.qweight.shape[0]*4, dtype=layer.scales.dtype, device="cuda")
+                    _ = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
+                                   layer.scales, layer.g_idx,
+                                   layer.exllama_state == ExllamaState.READY,
+                                   self.quant_config.weight_bits,
+                                   self.quant_config.group_size,
+                                   perm_space)
+                """
 
     def apply(self,
               layer: torch.nn.Module,
@@ -267,10 +324,25 @@ class GPTQLinearMethod(LinearMethodBase):
         out_shape = x.shape[:-1] + (layer.qweight.shape[-1], )
         reshaped_x = x.reshape(-1, x.shape[-1])
 
+        perm_space = torch.empty(0)
+        temp_space = torch.empty(0)
+        if self.quant_config.weight_bits == 4 or self.quant_config.weight_bits == 8:
+            if self.quant_config.group_size == 128 or self.quant_config.group_size == 64:
+                if self.quant_config.desc_act:
+                    perm_space = torch.empty(reshaped_x.shape[0], reshaped_x.shape[1],
+                                             dtype=torch.float16, device="cuda")
+                    
+                if reshaped_x.dtype == torch.bfloat16:
+                    temp_space = torch.zeros(reshaped_x.shape[0], layer.qweight.shape[1],
+                                             dtype=torch.float32, device="cuda")
+
         output = ops.gptq_gemm(reshaped_x, layer.qweight, layer.qzeros,
                                layer.scales, layer.g_idx,
                                layer.exllama_state == ExllamaState.READY,
-                               self.quant_config.weight_bits)
+                               self.quant_config.weight_bits,
+                               self.quant_config.group_size,
+                               perm_space, temp_space,
+                               True if reshaped_x.dtype == torch.bfloat16 else False)
         if bias is not None:
             output.add_(bias)
         return output.reshape(out_shape)
diff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py
index 00c4b661e..e0a3bd80e 100644
--- a/vllm/model_executor/layers/quantization/moe_wna16.py
+++ b/vllm/model_executor/layers/quantization/moe_wna16.py
@@ -39,6 +39,7 @@ class MoeWNA16Config(QuantizationConfig):
             AWQMarlinConfig)
         from vllm.model_executor.layers.quantization.gptq_marlin import (
             GPTQMarlinConfig)
+        """
         if self.linear_quant_method == "gptq":
             self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(
                 full_config)
@@ -57,7 +58,7 @@ class MoeWNA16Config(QuantizationConfig):
                 full_config)
         else:
             raise ValueError("moe_wna16 only support gptq and awq.")
-
+        """
         if modules_to_not_convert is None:
             self.modules_to_not_convert = []
         else:
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index c5970c71c..f5d5f1b7b 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -78,7 +78,7 @@ def _apply_rotary_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor,
             positional embeddings.
     """
     if current_platform.is_cuda_alike():
-        from vllm.vllm_flash_attn.layers.rotary import apply_rotary_emb
+        from flash_attn.layers.rotary import apply_rotary_emb
         return apply_rotary_emb(x.unsqueeze(0), cos, sin,
                                 not is_neox_style).squeeze(0)
     else:
diff --git a/vllm/model_executor/layers/vocab_parallel_embedding.py b/vllm/model_executor/layers/vocab_parallel_embedding.py
index d5eaeec1a..6337c4c90 100644
--- a/vllm/model_executor/layers/vocab_parallel_embedding.py
+++ b/vllm/model_executor/layers/vocab_parallel_embedding.py
@@ -136,13 +136,15 @@ class VocabParallelEmbeddingShardIndices:
         assert self.num_added_elements <= self.num_added_elements_padded
 
 
-@torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
+# @torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)
+@torch.jit.script
 def get_masked_input_and_mask(
         input_: torch.Tensor, org_vocab_start_index: int,
         org_vocab_end_index: int, num_org_vocab_padding: int,
         added_vocab_start_index: int,
         added_vocab_end_index: int) -> Tuple[torch.Tensor, torch.Tensor]:
     # torch.compile will fuse all of the pointwise ops below
+    # torch.jit.script will fuse all of the pointwise ops below
     # into a single kernel, making it very fast
     org_vocab_mask = (input_ >= org_vocab_start_index) & (
         input_ < org_vocab_end_index)
@@ -261,13 +263,22 @@ class VocabParallelEmbedding(torch.nn.Module):
             self.shard_indices.added_vocab_end_index -
             self.shard_indices.added_vocab_start_index)
 
-        self.quant_method.create_weights(self,
-                                         self.embedding_dim,
-                                         [self.num_embeddings_per_partition],
-                                         self.embedding_dim,
-                                         self.num_embeddings_padded,
-                                         params_dtype=params_dtype,
-                                         weight_loader=self.weight_loader)
+        if isinstance(self.quant_method, UnquantizedEmbeddingMethod):
+            self.quant_method.create_weights(self,
+                                            self.embedding_dim,
+                                            [self.num_embeddings_per_partition],
+                                            self.embedding_dim,
+                                            self.num_embeddings_padded,
+                                            params_dtype=params_dtype,
+                                            weight_loader=self.weight_loader)
+        else:
+            self.quant_method.create_weights(self,
+                                          self.num_embeddings_per_partition,
+                                          [self.embedding_dim],
+                                          self.embedding_dim,
+                                          self.num_embeddings_padded,
+                                          params_dtype=params_dtype,
+                                          weight_loader=self.weight_loader) 
 
     @classmethod
     def _get_indices(cls, vocab_size_padded: int, org_vocab_size_padded: int,
diff --git a/vllm/model_executor/models/deepseek.py b/vllm/model_executor/models/deepseek.py
index c6421143d..58ffa69d1 100644
--- a/vllm/model_executor/models/deepseek.py
+++ b/vllm/model_executor/models/deepseek.py
@@ -55,6 +55,7 @@ from .utils import (AutoWeightsLoader, extract_layer_index,
                     make_empty_intermediate_tensors_factory, make_layers,
                     maybe_prefix)
 
+import vllm.envs as envs
 
 class DeepseekMLP(nn.Module):
 
@@ -150,8 +151,17 @@ class DeepseekMoE(nn.Module):
         w2s = torch._utils._unflatten_dense_tensors(self.w2, w2)
         for data, param in zip(w2s, w2):
             param.data = data
-
         self.w2 = self.w2.view(len(w2), *w2s[0].shape)
+            
+        if envs.MACA_VLLM_USE_TN_2_NN:
+            self.w1 = self.w1.permute(0,2,1).contiguous()   
+            for expert, w in zip(self.experts, self.w1):
+                expert.gate_up_proj.weight.data = w.permute(1,0)
+                
+            self.w2 = self.w2.permute(0, 2, 1).contiguous()
+            for expert, w in zip(self.experts, self.w2):
+                expert.down_proj.weight.data = w.permute(1, 0)
+        
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         num_tokens, hidden_dim = hidden_states.shape
diff --git a/vllm/model_executor/models/mimo.py b/vllm/model_executor/models/mimo.py
new file mode 100644
index 000000000..b882aeebb
--- /dev/null
+++ b/vllm/model_executor/models/mimo.py
@@ -0,0 +1,190 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Adapted from
+# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/qwen2/modeling_qwen2.py
+# Copyright 2025 Xiaomi Corporation.
+# Copyright 2024 The Qwen team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only MiMo model compatible with HuggingFace weights."""
+from typing import Iterable, Optional, Set, Tuple, Union
+
+import torch
+import torch.nn as nn
+
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import VllmConfig
+from vllm.distributed import get_pp_group
+from vllm.logger import init_logger
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.sampler import get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.models.qwen2 import Qwen2ForCausalLM, Qwen2Model
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .utils import PPMissingLayer, is_pp_missing_parameter, maybe_prefix
+
+logger = init_logger(__name__)
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class MiMoModel(Qwen2Model):
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+        for layer in self.layers[self.start_layer:self.end_layer]:
+            hidden_states, residual = layer(
+                positions,
+                hidden_states,
+                residual,
+            )
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        hidden_states = hidden_states + residual
+        return hidden_states
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            if "mtp_layers" in name:
+                continue
+            if "rotary_emb.inv_freq" in name:
+                continue
+            if (self.quant_config is not None and
+                (scale_name := self.quant_config.get_cache_scale(name))):
+                # Loading kv cache quantization scales
+                param = params_dict[scale_name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
+                                 loaded_weight[0])
+                weight_loader(param, loaded_weight)
+                loaded_params.add(scale_name)
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                # Remapping the name of FP8 kv-scale.
+                name = maybe_remap_kv_scale_name(name, params_dict)
+                if name is None:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+class MiMoForCausalLM(Qwen2ForCausalLM, nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        nn.Module.__init__(self)
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+
+        self.model = MiMoModel(vllm_config=vllm_config,
+                               prefix=maybe_prefix(prefix, "model"))
+
+        if get_pp_group().is_last_rank:
+            if config.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        hidden_states = self.model.norm(hidden_states)
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
diff --git a/vllm/model_executor/models/mimo_mtp.py b/vllm/model_executor/models/mimo_mtp.py
new file mode 100644
index 000000000..c2f1cf411
--- /dev/null
+++ b/vllm/model_executor/models/mimo_mtp.py
@@ -0,0 +1,283 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Adapted from
+# https://github.com/vllm-project/vllm/blob/v0.7.3/vllm/model_executor/models/deepseek_mtp.py
+# Copyright 2025 Xiaomi Corporation.
+# Copyright 2023 The vLLM team.
+# Copyright 2024 DeepSeek-AI team.
+
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only MiMo-MTP model."""
+from typing import Iterable, Optional, Set, Tuple
+
+import torch
+import torch.nn as nn
+from transformers import PretrainedConfig
+
+from vllm.config import CacheConfig, ModelConfig, VllmConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.qwen2 import Qwen2DecoderLayer
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .utils import maybe_prefix
+
+
+class MiMoMultiTokenPredictorLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        model_config: ModelConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__()
+
+        self.token_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.hidden_layernorm = RMSNorm(config.hidden_size,
+                                        eps=config.rms_norm_eps)
+        self.input_proj = nn.Linear(config.hidden_size * 2,
+                                    config.hidden_size,
+                                    bias=False)
+        self.mtp_block = Qwen2DecoderLayer(config=config,
+                                           cache_config=cache_config,
+                                           quant_config=quant_config,
+                                           prefix=prefix)
+        self.final_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        inputs_embeds: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        spec_step_index: int = 0,
+    ) -> torch.Tensor:
+        assert inputs_embeds is not None
+        # masking inputs at position 0, as not needed by MTP
+        inputs_embeds[positions == 0] = 0
+        inputs_embeds = self.token_layernorm(inputs_embeds)
+        previous_hidden_states = self.hidden_layernorm(previous_hidden_states)
+
+        hidden_states = self.input_proj(
+            torch.cat([previous_hidden_states, inputs_embeds], dim=-1))
+
+        hidden_states, residual = self.mtp_block(positions=positions,
+                                                 hidden_states=hidden_states,
+                                                 residual=None)
+        hidden_states = residual + hidden_states
+        return self.final_layernorm(hidden_states)
+
+
+class MiMoMultiTokenPredictor(nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        self.mtp_start_layer_idx = config.num_hidden_layers
+        self.num_mtp_layers = config.num_nextn_predict_layers
+
+        self.embed_tokens = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+        )
+
+        self.mtp_layers = torch.nn.ModuleDict({
+            str(idx):
+            MiMoMultiTokenPredictorLayer(
+                config,
+                f"{prefix}.layers.{idx}",
+                model_config=vllm_config.model_config,
+                cache_config=vllm_config.cache_config,
+                quant_config=vllm_config.quant_config,
+            )
+            for idx in range(self.mtp_start_layer_idx,
+                             self.mtp_start_layer_idx + self.num_mtp_layers)
+        })
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_idx: int = 0,
+    ) -> torch.Tensor:
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+        return self.mtp_layers[str(self.mtp_start_layer_idx + spec_step_idx)](
+            inputs_embeds,
+            positions,
+            previous_hidden_states,
+            spec_step_idx,
+        )
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        lm_head: ParallelLMHead,
+        sampling_metadata: SamplingMetadata,
+        spec_step_idx: int = 0,
+    ) -> torch.Tensor:
+        self.mtp_layers[str(self.mtp_start_layer_idx + spec_step_idx)]
+        logits = self.logits_processor(lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+
+class MiMoMTP(nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        self.config = vllm_config.model_config.hf_config
+        self.model = MiMoMultiTokenPredictor(vllm_config=vllm_config,
+                                             prefix=maybe_prefix(
+                                                 prefix, "model"))
+        self.lm_head = ParallelLMHead(self.config.vocab_size,
+                                      self.config.hidden_size)
+
+        self.sampler = get_sampler()
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_idx: int = 0,
+    ) -> torch.Tensor:
+        assert spec_step_idx == 0, "mimo_mtp only support predict one token now"
+        hidden_states = self.model(input_ids, positions,
+                                   previous_hidden_states, inputs_embeds,
+                                   spec_step_idx)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+        spec_step_idx: int = 0,
+    ) -> Optional[torch.Tensor]:
+        return self.model.compute_logits(hidden_states, self.lm_head,
+                                         sampling_metadata, spec_step_idx)
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+
+            if "rotary_emb.inv_freq" in name:
+                continue
+            name = self.map_model_name_to_mtp_param_name(name)
+
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                if "mtp_layers" not in name:
+                    break
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                if (("mlp.experts." in name) and name not in params_dict):
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if "mtp_layers" not in name and ("embed_tokens" not in name
+                                                 and "lm_head" not in name):
+                    continue
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+    def map_model_name_to_mtp_param_name(self, name: str) -> str:
+        import re
+        name_without_prefix = [
+            "token_layernorm", "hidden_layernorm", "input_proj",
+            "final_layernorm"
+        ]
+        for sub_name in name_without_prefix:
+            if sub_name in name:
+                return name
+        pattern = r"model.mtp_layers.(\d+)."
+        group = re.match(pattern, name)
+        if group is not None:
+            name = name.replace(group.group(), group.group() + "mtp_block.")
+        return name
+
+    def _rewrite_spec_layer_name(self, spec_layer: int, name: str) -> str:
+        """
+        Rewrite the weight name to match the format of the original model.
+        Add .mtp_block for modules in transformer layer block for spec layer
+        """
+        spec_layer_weight_names = [
+            "embed_tokens", "enorm", "hnorm", "eh_proj", "shared_head"
+        ]
+        spec_layer_weight = False
+        for weight_name in spec_layer_weight_names:
+            if weight_name in name:
+                spec_layer_weight = True
+                break
+        if not spec_layer_weight:
+            # treat rest weights as weights for transformer layer block
+            name = name.replace(f"model.layers.{spec_layer}.",
+                                f"model.layers.{spec_layer}.mtp_block.")
+        return name
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index 33877829f..470a0c4db 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -88,6 +88,7 @@ _TEXT_GENERATION_MODELS = {
     # transformers's mpt class has lower case
     "MptForCausalLM": ("mpt", "MPTForCausalLM"),
     "MPTForCausalLM": ("mpt", "MPTForCausalLM"),
+    "MiMoForCausalLM": ("mimo", "MiMoForCausalLM"),
     "NemotronForCausalLM": ("nemotron", "NemotronForCausalLM"),
     "OlmoForCausalLM": ("olmo", "OlmoForCausalLM"),
     "Olmo2ForCausalLM": ("olmo2", "Olmo2ForCausalLM"),
@@ -213,6 +214,7 @@ _MULTIMODAL_MODELS = {
 }
 
 _SPECULATIVE_DECODING_MODELS = {
+    "MiMoMTPModel": ("mimo_mtp", "MiMoMTP"),
     "EAGLEModel": ("eagle", "EAGLE"),
     "EagleLlamaForCausalLM": ("llama_eagle", "EagleLlamaForCausalLM"),
     "Eagle3LlamaForCausalLM": ("llama_eagle3", "Eagle3LlamaForCausalLM"),
diff --git a/vllm/platforms/__init__.py b/vllm/platforms/__init__.py
index 0ed221043..dd7b75d9a 100644
--- a/vllm/platforms/__init__.py
+++ b/vllm/platforms/__init__.py
@@ -60,8 +60,12 @@ def cuda_platform_plugin() -> Optional[str]:
             # we need to check if vllm is built with cpu too.
             # Otherwise, vllm will always activate cuda plugin
             # on a GPU machine, even if in a cpu build.
+            
             is_cuda = (pynvml.nvmlDeviceGetCount() > 0
                        and not vllm_version_matches_substr("cpu"))
+            # import torch
+            # if torch.cuda.device_count() > 0:
+            #     is_cuda = True
             if pynvml.nvmlDeviceGetCount() <= 0:
                 logger.debug(
                     "CUDA platform is not available because no GPU is found.")
diff --git a/vllm/platforms/cuda.py b/vllm/platforms/cuda.py
index f82af426b..aa922d330 100644
--- a/vllm/platforms/cuda.py
+++ b/vllm/platforms/cuda.py
@@ -31,7 +31,7 @@ pynvml = import_pynvml()
 
 # pytorch 2.5 uses cudnn sdpa by default, which will cause crash on some models
 # see https://github.com/huggingface/diffusers/issues/9704 for details
-torch.backends.cuda.enable_cudnn_sdp(False)
+# torch.backends.cuda.enable_cudnn_sdp(False)
 
 
 def device_id_to_physical_device_id(device_id: int) -> int:
@@ -256,10 +256,15 @@ class CudaPlatformBase(Platform):
         # installed.
         if target_backend == _Backend.FLASH_ATTN:
             try:
-                import vllm.vllm_flash_attn  # noqa: F401
-                from vllm.attention.backends.flash_attn import (  # noqa: F401
-                    FlashAttentionBackend, flash_attn_supports_fp8)
-
+                import flash_attn  # noqa: F401
+                # Support page attention backend
+                if envs.MACA_VLLM_PG_OPT:
+                    from vllm.attention.backends.flash_attn_pg import (  # noqa: F401
+                        FlashAttentionBackend, flash_attn_supports_fp8)
+                else:
+                    from vllm.attention.backends.flash_attn import (  # noqa: F401
+                        FlashAttentionBackend, flash_attn_supports_fp8)
+                    
                 supported_sizes = \
                     FlashAttentionBackend.get_supported_head_sizes()
                 if head_size not in supported_sizes:
@@ -290,7 +295,12 @@ class CudaPlatformBase(Platform):
             return "vllm.attention.backends.xformers.XFormersBackend"
 
         logger.info("Using Flash Attention backend.")
-        return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
+
+        # Support page attention backend
+        if envs.MACA_VLLM_PG_OPT:
+            return "vllm.attention.backends.flash_attn_pg.FlashAttentionBackend"
+        else:
+            return "vllm.attention.backends.flash_attn.FlashAttentionBackend"
 
     @classmethod
     def get_punica_wrapper(cls) -> str:
diff --git a/vllm/third_party/pymcml.py b/vllm/third_party/pymcml.py
new file mode 100644
index 000000000..097007ea0
--- /dev/null
+++ b/vllm/third_party/pymcml.py
@@ -0,0 +1,5439 @@
+# 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
+#####
+# Copyright (c) 2011-2023, NVIDIA Corporation.  All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+#    * Redistributions of source code must retain the above copyright notice,
+#      this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#    * Neither the name of the NVIDIA Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived from
+#      this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+# THE POSSIBILITY OF SUCH DAMAGE.
+#####
+
+##
+# Python bindings for the MXSMLEX library
+##
+from ctypes import *
+from ctypes.util import find_library
+from functools import wraps
+import sys
+import os
+import threading
+import string
+
+## C Type mappings ##
+## Enums
+_nvmlEnableState_t = c_uint
+MXSMLEX_FEATURE_DISABLED    = 0
+MXSMLEX_FEATURE_ENABLED     = 1
+
+_nvmlBrandType_t = c_uint
+MXSMLEX_BRAND_UNKNOWN             = 0
+MXSMLEX_BRAND_QUADRO              = 1
+MXSMLEX_BRAND_TESLA               = 2
+MXSMLEX_BRAND_NVS                 = 3
+MXSMLEX_BRAND_GRID                = 4   # Deprecated from API reporting. Keeping definition for backward compatibility.
+MXSMLEX_BRAND_GEFORCE             = 5
+MXSMLEX_BRAND_TITAN               = 6
+MXSMLEX_BRAND_NVIDIA_VAPPS        = 7   # NVIDIA Virtual Applications
+MXSMLEX_BRAND_NVIDIA_VPC          = 8   # NVIDIA Virtual PC
+MXSMLEX_BRAND_NVIDIA_VCS          = 9   # NVIDIA Virtual Compute Server
+MXSMLEX_BRAND_NVIDIA_VWS          = 10  # NVIDIA RTX Virtual Workstation
+MXSMLEX_BRAND_NVIDIA_CLOUD_GAMING = 11  # NVIDIA Cloud Gaming
+MXSMLEX_BRAND_NVIDIA_VGAMING      = MXSMLEX_BRAND_NVIDIA_CLOUD_GAMING # Deprecated from API reporting. Keeping definition for backward compatibility.
+MXSMLEX_BRAND_QUADRO_RTX          = 12
+MXSMLEX_BRAND_NVIDIA_RTX          = 13
+MXSMLEX_BRAND_NVIDIA              = 14
+MXSMLEX_BRAND_GEFORCE_RTX         = 15  # Unused
+MXSMLEX_BRAND_TITAN_RTX           = 16  # Unused
+MXSMLEX_BRAND_COUNT               = 17
+
+_nvmlTemperatureThresholds_t = c_uint
+MXSMLEX_TEMPERATURE_THRESHOLD_SHUTDOWN      = 0
+MXSMLEX_TEMPERATURE_THRESHOLD_SLOWDOWN      = 1
+MXSMLEX_TEMPERATURE_THRESHOLD_MEM_MAX       = 2
+MXSMLEX_TEMPERATURE_THRESHOLD_GPU_MAX       = 3
+MXSMLEX_TEMPERATURE_THRESHOLD_ACOUSTIC_MIN  = 4
+MXSMLEX_TEMPERATURE_THRESHOLD_ACOUSTIC_CURR = 5
+MXSMLEX_TEMPERATURE_THRESHOLD_ACOUSTIC_MAX  = 6
+MXSMLEX_TEMPERATURE_THRESHOLD_COUNT         = 7
+
+_nvmlTemperatureSensors_t = c_uint
+MXSMLEX_TEMPERATURE_GPU     = 0
+MXSMLEX_TEMPERATURE_COUNT   = 1
+
+
+_nvmlComputeMode_t = c_uint
+MXSMLEX_COMPUTEMODE_DEFAULT           = 0
+MXSMLEX_COMPUTEMODE_EXCLUSIVE_THREAD  = 1  ## Support Removed
+MXSMLEX_COMPUTEMODE_PROHIBITED        = 2
+MXSMLEX_COMPUTEMODE_EXCLUSIVE_PROCESS = 3
+MXSMLEX_COMPUTEMODE_COUNT             = 4
+
+_nvmlMemoryLocation_t = c_uint
+MXSMLEX_MEMORY_LOCATION_L1_CACHE = 0
+MXSMLEX_MEMORY_LOCATION_L2_CACHE = 1
+MXSMLEX_MEMORY_LOCATION_DEVICE_MEMORY = 2
+MXSMLEX_MEMORY_LOCATION_DRAM = 2
+MXSMLEX_MEMORY_LOCATION_REGISTER_FILE = 3
+MXSMLEX_MEMORY_LOCATION_TEXTURE_MEMORY = 4
+MXSMLEX_MEMORY_LOCATION_TEXTURE_SHM = 5
+MXSMLEX_MEMORY_LOCATION_CBU = 6
+MXSMLEX_MEMORY_LOCATION_SRAM = 7
+MXSMLEX_MEMORY_LOCATION_COUNT = 8
+
+MXSMLEX_NVLINK_MAX_LINKS = 18
+
+# For backwards compatibility, maintain the incorrectly-named "LANES" define
+MXSMLEX_NVLINK_MAX_LANES = MXSMLEX_NVLINK_MAX_LINKS
+
+_nvmlNvLinkErrorCounter_t = c_uint
+MXSMLEX_NVLINK_ERROR_DL_REPLAY = 0
+MXSMLEX_NVLINK_ERROR_DL_RECOVERY = 1
+MXSMLEX_NVLINK_ERROR_DL_CRC_FLIT = 2
+MXSMLEX_NVLINK_ERROR_DL_CRC_DATA = 3
+MXSMLEX_NVLINK_ERROR_DL_ECC_DATA = 4
+MXSMLEX_NVLINK_ERROR_COUNT = 5
+
+_nvmlNvLinkEccLaneErrorCounter_t = c_uint
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE0 = 0
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE1 = 1
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE2 = 2
+MXSMLEX_NVLINK_ERROR_DL_ECC_LANE3 = 3
+MXSMLEX_NVLINK_ERROR_DL_ECC_COUNT = 5
+
+_nvmlNvLinkCapability_t = c_uint
+MXSMLEX_NVLINK_CAP_P2P_SUPPORTED = 0
+MXSMLEX_NVLINK_CAP_SYSMEM_ACCESS = 1
+MXSMLEX_NVLINK_CAP_P2P_ATOMICS   = 2
+MXSMLEX_NVLINK_CAP_SYSMEM_ATOMICS= 3
+MXSMLEX_NVLINK_CAP_SLI_BRIDGE    = 4
+MXSMLEX_NVLINK_CAP_VALID         = 5
+MXSMLEX_NVLINK_CAP_COUNT         = 6
+
+_nvmlNvLinkUtilizationCountPktTypes_t = c_uint
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_NOP        = 0x1
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_READ       = 0x2
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_WRITE      = 0x4
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_RATOM      = 0x8
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_NRATOM     = 0x10
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_FLUSH      = 0x20
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_RESPDATA   = 0x40
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_RESPNODATA = 0x80
+MXSMLEX_NVLINK_COUNTER_PKTFILTER_ALL        = 0xFF
+
+_nvmlNvLinkUtilizationCountUnits_t = c_uint
+MXSMLEX_NVLINK_COUNTER_UNIT_CYCLES   = 0
+MXSMLEX_NVLINK_COUNTER_UNIT_PACKETS  = 1
+MXSMLEX_NVLINK_COUNTER_UNIT_BYTES    = 2
+MXSMLEX_NVLINK_COUNTER_UNIT_RESERVED = 3
+MXSMLEX_NVLINK_COUNTER_UNIT_COUNT    = 4
+
+_nvmlNvLinkDeviceType_t = c_uint
+MXSMLEX_NVLINK_DEVICE_TYPE_GPU     = 0x00
+MXSMLEX_NVLINK_DEVICE_TYPE_IBMNPU  = 0x01
+MXSMLEX_NVLINK_DEVICE_TYPE_SWITCH  = 0x02
+MXSMLEX_NVLINK_DEVICE_TYPE_UNKNOWN = 0xFF
+
+# These are deprecated, instead use _nvmlMemoryErrorType_t
+_nvmlEccBitType_t = c_uint
+MXSMLEX_SINGLE_BIT_ECC    = 0
+MXSMLEX_DOUBLE_BIT_ECC    = 1
+MXSMLEX_ECC_ERROR_TYPE_COUNT = 2
+
+_nvmlEccCounterType_t = c_uint
+MXSMLEX_VOLATILE_ECC      = 0
+MXSMLEX_AGGREGATE_ECC     = 1
+MXSMLEX_ECC_COUNTER_TYPE_COUNT = 2
+
+_nvmlMemoryErrorType_t = c_uint
+MXSMLEX_MEMORY_ERROR_TYPE_CORRECTED   = 0
+MXSMLEX_MEMORY_ERROR_TYPE_UNCORRECTED = 1
+MXSMLEX_MEMORY_ERROR_TYPE_COUNT       = 2
+
+_nvmlClockType_t = c_uint
+MXSMLEX_CLOCK_GRAPHICS  = 0
+MXSMLEX_CLOCK_SM        = 1
+MXSMLEX_CLOCK_MEM       = 2
+MXSMLEX_CLOCK_VIDEO     = 3
+MXSMLEX_CLOCK_COUNT     = 4
+
+_nvmlClockId_t = c_uint
+MXSMLEX_CLOCK_ID_CURRENT            = 0
+MXSMLEX_CLOCK_ID_APP_CLOCK_TARGET   = 1
+MXSMLEX_CLOCK_ID_APP_CLOCK_DEFAULT  = 2
+MXSMLEX_CLOCK_ID_CUSTOMER_BOOST_MAX = 3
+MXSMLEX_CLOCK_ID_COUNT              = 4
+
+_nvmlDriverModel_t = c_uint
+MXSMLEX_DRIVER_WDDM       = 0
+MXSMLEX_DRIVER_WDM        = 1
+MXSMLEX_DRIVER_MCDM       = 2
+
+MXSMLEX_MAX_GPU_PERF_PSTATES = 16
+
+_nvmlPstates_t = c_uint
+MXSMLEX_PSTATE_0               = 0
+MXSMLEX_PSTATE_1               = 1
+MXSMLEX_PSTATE_2               = 2
+MXSMLEX_PSTATE_3               = 3
+MXSMLEX_PSTATE_4               = 4
+MXSMLEX_PSTATE_5               = 5
+MXSMLEX_PSTATE_6               = 6
+MXSMLEX_PSTATE_7               = 7
+MXSMLEX_PSTATE_8               = 8
+MXSMLEX_PSTATE_9               = 9
+MXSMLEX_PSTATE_10              = 10
+MXSMLEX_PSTATE_11              = 11
+MXSMLEX_PSTATE_12              = 12
+MXSMLEX_PSTATE_13              = 13
+MXSMLEX_PSTATE_14              = 14
+MXSMLEX_PSTATE_15              = 15
+MXSMLEX_PSTATE_UNKNOWN         = 32
+
+_nvmlInforomObject_t = c_uint
+MXSMLEX_INFOROM_OEM            = 0
+MXSMLEX_INFOROM_ECC            = 1
+MXSMLEX_INFOROM_POWER          = 2
+MXSMLEX_INFOROM_COUNT          = 3
+
+_nvmlReturn_t = c_uint
+MXSMLEX_SUCCESS                         = 0
+MXSMLEX_ERROR_UNINITIALIZED             = 1
+MXSMLEX_ERROR_INVALID_ARGUMENT          = 2
+MXSMLEX_ERROR_NOT_SUPPORTED             = 3
+MXSMLEX_ERROR_NO_PERMISSION             = 4
+MXSMLEX_ERROR_ALREADY_INITIALIZED       = 5
+MXSMLEX_ERROR_NOT_FOUND                 = 6
+MXSMLEX_ERROR_INSUFFICIENT_SIZE         = 7
+MXSMLEX_ERROR_INSUFFICIENT_POWER        = 8
+MXSMLEX_ERROR_DRIVER_NOT_LOADED         = 9
+MXSMLEX_ERROR_TIMEOUT                   = 10
+MXSMLEX_ERROR_IRQ_ISSUE                 = 11
+MXSMLEX_ERROR_LIBRARY_NOT_FOUND         = 12
+MXSMLEX_ERROR_FUNCTION_NOT_FOUND        = 13
+MXSMLEX_ERROR_CORRUPTED_INFOROM         = 14
+MXSMLEX_ERROR_GPU_IS_LOST               = 15
+MXSMLEX_ERROR_RESET_REQUIRED            = 16
+MXSMLEX_ERROR_OPERATING_SYSTEM          = 17
+MXSMLEX_ERROR_LIB_RM_VERSION_MISMATCH   = 18
+MXSMLEX_ERROR_IN_USE                    = 19
+MXSMLEX_ERROR_MEMORY                    = 20
+MXSMLEX_ERROR_NO_DATA                   = 21
+MXSMLEX_ERROR_VGPU_ECC_NOT_SUPPORTED    = 22
+MXSMLEX_ERROR_INSUFFICIENT_RESOURCES    = 23
+MXSMLEX_ERROR_FREQ_NOT_SUPPORTED        = 24
+MXSMLEX_ERROR_ARGUMENT_VERSION_MISMATCH = 25
+MXSMLEX_ERROR_DEPRECATED                = 26
+MXSMLEX_ERROR_NOT_READY                 = 27
+MXSMLEX_ERROR_GPU_NOT_FOUND             = 28
+MXSMLEX_ERROR_INVALID_STATE             = 29
+MXSMLEX_ERROR_UNKNOWN                   = 999
+
+_nvmlFanState_t = c_uint
+MXSMLEX_FAN_NORMAL             = 0
+MXSMLEX_FAN_FAILED             = 1
+
+_nvmlFanControlPolicy_t = c_uint
+MXSMLEX_FAN_POLICY_TEMPERATURE_CONTINOUS_SW = 0
+MXSMLEX_FAN_POLICY_MANUAL                   = 1
+
+_nvmlLedColor_t = c_uint
+MXSMLEX_LED_COLOR_GREEN        = 0
+MXSMLEX_LED_COLOR_AMBER        = 1
+
+_nvmlGpuOperationMode_t = c_uint
+MXSMLEX_GOM_ALL_ON                 = 0
+MXSMLEX_GOM_COMPUTE                = 1
+MXSMLEX_GOM_LOW_DP                 = 2
+
+_nvmlPageRetirementCause_t = c_uint
+MXSMLEX_PAGE_RETIREMENT_CAUSE_MULTIPLE_SINGLE_BIT_ECC_ERRORS = 0
+MXSMLEX_PAGE_RETIREMENT_CAUSE_DOUBLE_BIT_ECC_ERROR           = 1
+MXSMLEX_PAGE_RETIREMENT_CAUSE_COUNT                          = 2
+
+_nvmlRestrictedAPI_t = c_uint
+MXSMLEX_RESTRICTED_API_SET_APPLICATION_CLOCKS                = 0
+MXSMLEX_RESTRICTED_API_SET_AUTO_BOOSTED_CLOCKS               = 1
+MXSMLEX_RESTRICTED_API_COUNT                                 = 2
+
+_nvmlBridgeChipType_t = c_uint
+MXSMLEX_BRIDGE_CHIP_PLX = 0
+MXSMLEX_BRIDGE_CHIP_BRO4 = 1
+MXSMLEX_MAX_PHYSICAL_BRIDGE = 128
+
+_nvmlValueType_t = c_uint
+MXSMLEX_VALUE_TYPE_DOUBLE = 0
+MXSMLEX_VALUE_TYPE_UNSIGNED_INT = 1
+MXSMLEX_VALUE_TYPE_UNSIGNED_LONG = 2
+MXSMLEX_VALUE_TYPE_UNSIGNED_LONG_LONG = 3
+MXSMLEX_VALUE_TYPE_SIGNED_LONG_LONG = 4
+MXSMLEX_VALUE_TYPE_SIGNED_INT = 5
+MXSMLEX_VALUE_TYPE_COUNT = 6
+
+_nvmlPerfPolicyType_t = c_uint
+MXSMLEX_PERF_POLICY_POWER = 0
+MXSMLEX_PERF_POLICY_THERMAL = 1
+MXSMLEX_PERF_POLICY_SYNC_BOOST = 2
+MXSMLEX_PERF_POLICY_BOARD_LIMIT = 3
+MXSMLEX_PERF_POLICY_LOW_UTILIZATION = 4
+MXSMLEX_PERF_POLICY_RELIABILITY = 5
+MXSMLEX_PERF_POLICY_TOTAL_APP_CLOCKS = 10
+MXSMLEX_PERF_POLICY_TOTAL_BASE_CLOCKS = 11
+MXSMLEX_PERF_POLICY_COUNT = 12
+
+_nvmlEncoderQueryType_t = c_uint
+MXSMLEX_ENCODER_QUERY_H264 = 0
+MXSMLEX_ENCODER_QUERY_HEVC = 1
+MXSMLEX_ENCODER_QUERY_AV1 = 2
+MXSMLEX_ENCODER_QUERY_UNKNOWN = 255
+
+_nvmlFBCSessionType_t = c_uint
+MXSMLEX_FBC_SESSION_TYPE_UNKNOWN = 0
+MXSMLEX_FBC_SESSION_TYPE_TOSYS = 1
+MXSMLEX_FBC_SESSION_TYPE_CUDA = 2
+MXSMLEX_FBC_SESSION_TYPE_VID = 3
+MXSMLEX_FBC_SESSION_TYPE_HWENC = 4
+
+_nvmlDetachGpuState_t = c_uint
+MXSMLEX_DETACH_GPU_KEEP = 0
+MXSMLEX_DETACH_GPU_REMOVE = 1
+
+_nvmlPcieLinkState_t = c_uint
+MXSMLEX_PCIE_LINK_KEEP = 0
+MXSMLEX_PCIE_LINK_SHUT_DOWN = 1
+
+_nvmlSamplingType_t = c_uint
+MXSMLEX_TOTAL_POWER_SAMPLES = 0
+MXSMLEX_GPU_UTILIZATION_SAMPLES = 1
+MXSMLEX_MEMORY_UTILIZATION_SAMPLES = 2
+MXSMLEX_ENC_UTILIZATION_SAMPLES = 3
+MXSMLEX_DEC_UTILIZATION_SAMPLES = 4
+MXSMLEX_PROCESSOR_CLK_SAMPLES = 5
+MXSMLEX_MEMORY_CLK_SAMPLES = 6
+MXSMLEX_MODULE_POWER_SAMPLES = 7
+MXSMLEX_JPG_UTILIZATION_SAMPLES = 8
+MXSMLEX_OFA_UTILIZATION_SAMPLES = 9
+MXSMLEX_SAMPLINGTYPE_COUNT = 10
+
+_nvmlPcieUtilCounter_t = c_uint
+MXSMLEX_PCIE_UTIL_TX_BYTES = 0
+MXSMLEX_PCIE_UTIL_RX_BYTES = 1
+MXSMLEX_PCIE_UTIL_COUNT = 2
+
+_nvmlGpuTopologyLevel_t = c_uint
+MXSMLEX_TOPOLOGY_INTERNAL = 0
+MXSMLEX_TOPOLOGY_SINGLE = 10
+MXSMLEX_TOPOLOGY_MULTIPLE = 20
+MXSMLEX_TOPOLOGY_HOSTBRIDGE = 30
+MXSMLEX_TOPOLOGY_NODE = 40
+MXSMLEX_TOPOLOGY_CPU = MXSMLEX_TOPOLOGY_NODE
+MXSMLEX_TOPOLOGY_SYSTEM = 50
+
+_nvmlGpuP2PCapsIndex_t = c_uint
+MXSMLEX_P2P_CAPS_INDEX_READ = 0,
+MXSMLEX_P2P_CAPS_INDEX_WRITE = 1
+#MXSMLEX_P2P_CAPS_INDEX_MXLINK =2
+NVML_P2P_CAPS_INDEX_NVLINK = 2
+MXSMLEX_P2P_CAPS_INDEX_ATOMICS = 3
+#
+# MXSMLEX_P2P_CAPS_INDEX_PROP is deprecated.
+# Use MXSMLEX_P2P_CAPS_INDEX_PCI instead.
+#
+MXSMLEX_P2P_CAPS_INDEX_PROP = 4
+MXSMLEX_P2P_CAPS_INDEX_PCI = 4
+MXSMLEX_P2P_CAPS_INDEX_UNKNOWN = 5
+
+_nvmlGpuP2PStatus_t = c_uint
+#MXSMLEX_P2P_STATUS_OK     = 0
+NVML_P2P_STATUS_OK = 0
+MXSMLEX_P2P_STATUS_CHIPSET_NOT_SUPPORED = 1
+MXSMLEX_P2P_STATUS_CHIPSET_NOT_SUPPORTED = MXSMLEX_P2P_STATUS_CHIPSET_NOT_SUPPORED
+MXSMLEX_P2P_STATUS_GPU_NOT_SUPPORTED = 2
+MXSMLEX_P2P_STATUS_IOH_TOPOLOGY_NOT_SUPPORTED =3
+MXSMLEX_P2P_STATUS_DISABLED_BY_REGKEY =4
+MXSMLEX_P2P_STATUS_NOT_SUPPORTED =5
+MXSMLEX_P2P_STATUS_UNKNOWN =6
+
+_nvmlDeviceArchitecture_t = c_uint
+MXSMLEX_DEVICE_ARCH_KEPLER   = 2
+MXSMLEX_DEVICE_ARCH_MAXWELL  = 3
+MXSMLEX_DEVICE_ARCH_PASCAL   = 4
+MXSMLEX_DEVICE_ARCH_VOLTA    = 5
+MXSMLEX_DEVICE_ARCH_TURING   = 6
+MXSMLEX_DEVICE_ARCH_AMPERE   = 7
+MXSMLEX_DEVICE_ARCH_ADA      = 8
+MXSMLEX_DEVICE_ARCH_HOPPER   = 9
+MXSMLEX_DEVICE_ARCH_UNKNOWN  = 0xffffffff
+
+# PCI bus Types
+_nvmlBusType_t = c_uint
+MXSMLEX_BUS_TYPE_UNKNOWN = 0
+MXSMLEX_BUS_TYPE_PCI     = 1
+MXSMLEX_BUS_TYPE_PCIE    = 2
+MXSMLEX_BUS_TYPE_FPCI    = 3
+MXSMLEX_BUS_TYPE_AGP     = 4
+
+_nvmlPowerSource_t = c_uint
+MXSMLEX_POWER_SOURCE_AC         = 0x00000000
+MXSMLEX_POWER_SOURCE_BATTERY    = 0x00000001
+MXSMLEX_POWER_SOURCE_UNDERSIZED = 0x00000002
+
+_nvmlAdaptiveClockInfoStatus_t = c_uint
+MXSMLEX_ADAPTIVE_CLOCKING_INFO_STATUS_DISABLED = 0x00000000
+MXSMLEX_ADAPTIVE_CLOCKING_INFO_STATUS_ENABLED = 0x00000001
+
+_nvmlClockLimitId_t = c_uint
+MXSMLEX_CLOCK_LIMIT_ID_RANGE_START = 0xffffff00
+MXSMLEX_CLOCK_LIMIT_ID_TDP         = 0xffffff01
+MXSMLEX_CLOCK_LIMIT_ID_UNLIMITED   = 0xffffff02
+
+_nvmlPcieLinkMaxSpeed_t = c_uint
+MXSMLEX_PCIE_LINK_MAX_SPEED_INVALID   = 0x00000000
+MXSMLEX_PCIE_LINK_MAX_SPEED_2500MBPS  = 0x00000001
+MXSMLEX_PCIE_LINK_MAX_SPEED_5000MBPS  = 0x00000002
+MXSMLEX_PCIE_LINK_MAX_SPEED_8000MBPS  = 0x00000003
+MXSMLEX_PCIE_LINK_MAX_SPEED_16000MBPS = 0x00000004
+MXSMLEX_PCIE_LINK_MAX_SPEED_32000MBPS = 0x00000005
+MXSMLEX_PCIE_LINK_MAX_SPEED_64000MBPS = 0x00000006
+
+_nvmlAffinityScope_t = c_uint
+MXSMLEX_AFFINITY_SCOPE_NODE   = 0
+MXSMLEX_AFFINITY_SCOPE_SOCKET = 1
+
+# C preprocessor defined values
+nvmlFlagDefault             = 0
+nvmlFlagForce               = 1
+MXSMLEX_INIT_FLAG_NO_GPUS      = 1
+MXSMLEX_INIT_FLAG_NO_ATTACH    = 2
+
+MXSMLEX_MAX_GPC_COUNT          = 32
+
+# buffer size
+MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE      = 16
+MXSMLEX_DEVICE_UUID_BUFFER_SIZE                 = 80
+MXSMLEX_DEVICE_UUID_V2_BUFFER_SIZE              = 96
+MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE       = 80
+MXSMLEX_SYSTEM_MXSMLEX_VERSION_BUFFER_SIZE         = 80
+MXSMLEX_DEVICE_NAME_BUFFER_SIZE                 = 64
+MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE              = 96
+MXSMLEX_DEVICE_SERIAL_BUFFER_SIZE               = 30
+MXSMLEX_DEVICE_PART_NUMBER_BUFFER_SIZE          = 80
+MXSMLEX_DEVICE_GPU_PART_NUMBER_BUFFER_SIZE      = 80
+MXSMLEX_DEVICE_VBIOS_VERSION_BUFFER_SIZE        = 32
+MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE           = 32
+MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE        = 16
+MXSMLEX_GRID_LICENSE_BUFFER_SIZE                = 128
+MXSMLEX_VGPU_NAME_BUFFER_SIZE                   = 64
+MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT          = 3
+MXSMLEX_VGPU_METADATA_OPAQUE_DATA_SIZE          = sizeof(c_uint) + 256
+MXSMLEX_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE     = 256
+MXSMLEX_DEVICE_GPU_FRU_PART_NUMBER_BUFFER_SIZE  = 0x14 # NV2080_GPU_MAX_PRODUCT_PART_NUMBER_LENGTH
+
+# Format strings
+MXSMLEX_DEVICE_PCI_BUS_ID_LEGACY_FMT   = "%04X:%02X:%02X.0"
+MXSMLEX_DEVICE_PCI_BUS_ID_FMT          = "%08X:%02X:%02X.0"
+
+MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong = c_ulonglong(-1)
+MXSMLEX_VALUE_NOT_AVAILABLE_uint = c_uint(-1)
+
+'''
+ Field Identifiers.
+
+ All Identifiers pertain to a device. Each ID is only used once and is guaranteed never to change.
+'''
+MXSMLEX_FI_DEV_ECC_CURRENT          = 1   # Current ECC mode. 1=Active. 0=Inactive
+MXSMLEX_FI_DEV_ECC_PENDING          = 2   # Pending ECC mode. 1=Active. 0=Inactive
+
+#ECC Count Totals
+MXSMLEX_FI_DEV_ECC_SBE_VOL_TOTAL    = 3   # Total single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_TOTAL    = 4   # Total double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_TOTAL    = 5   # Total single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_TOTAL    = 6   # Total double bit aggregate (persistent) ECC errors
+#Individual ECC locations
+MXSMLEX_FI_DEV_ECC_SBE_VOL_L1       = 7   # L1 cache single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_L1       = 8   # L1 cache double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_L2       = 9   # L2 cache single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_L2       = 10  # L2 cache double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_DEV      = 11  # Device memory single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_DEV      = 12  # Device memory double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_REG      = 13  # Register file single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_REG      = 14  # Register file double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_VOL_TEX      = 15  # Texture memory single bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_TEX      = 16  # Texture memory double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_VOL_CBU      = 17  # CBU double bit volatile ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_L1       = 18  # L1 cache single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_L1       = 19  # L1 cache double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_L2       = 20  # L2 cache single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_L2       = 21  # L2 cache double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_DEV      = 22  # Device memory single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_DEV      = 23  # Device memory double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_REG      = 24  # Register File single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_REG      = 25  # Register File double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_SBE_AGG_TEX      = 26  # Texture memory single bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_TEX      = 27  # Texture memory double bit aggregate (persistent) ECC errors
+MXSMLEX_FI_DEV_ECC_DBE_AGG_CBU      = 28  # CBU double bit aggregate ECC errors
+
+# Page Retirement
+MXSMLEX_FI_DEV_RETIRED_SBE          = 29  # Number of retired pages because of single bit errors
+MXSMLEX_FI_DEV_RETIRED_DBE          = 30  # Number of retired pages because of double bit errors
+MXSMLEX_FI_DEV_RETIRED_PENDING      = 31  # If any pages are pending retirement. 1=yes. 0=no.
+
+# NvLink Flit Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L0   = 32 # NVLink flow control CRC  Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L1   = 33 # NVLink flow control CRC  Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L2   = 34 # NVLink flow control CRC  Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L3   = 35 # NVLink flow control CRC  Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L4   = 36 # NVLink flow control CRC  Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L5   = 37 # NVLink flow control CRC  Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_TOTAL = 38 # NVLink flow control CRC  Error Counter total for all Lanes
+
+# NvLink CRC Data Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L0   = 39 # NVLink data CRC Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L1   = 40 # NVLink data CRC Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L2   = 41 # NVLink data CRC Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L3   = 42 # NVLink data CRC Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L4   = 43 # NVLink data CRC Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L5   = 44 # NVLink data CRC Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_TOTAL = 45 # NvLink data CRC Error Counter total for all Lanes
+
+# NvLink Replay Error Counters
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L0     = 46 # NVLink Replay Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L1     = 47 # NVLink Replay Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L2     = 48 # NVLink Replay Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L3     = 49 # NVLink Replay Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L4     = 50 # NVLink Replay Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L5     = 51 # NVLink Replay Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_TOTAL  = 52 # NVLink Replay Error Counter total for all Lanes
+
+# NvLink Recovery Error Counters
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L0   = 53 # NVLink Recovery Error Counter for Lane 0
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L1   = 54 # NVLink Recovery Error Counter for Lane 1
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L2   = 55 # NVLink Recovery Error Counter for Lane 2
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L3   = 56 # NVLink Recovery Error Counter for Lane 3
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L4   = 57 # NVLink Recovery Error Counter for Lane 4
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L5   = 58 # NVLink Recovery Error Counter for Lane 5
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_TOTAL = 59 # NVLink Recovery Error Counter total for all Lanes
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L0    = 60 # NVLink Bandwidth Counter for Counter Set 0, Lane 0
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L1    = 61 # NVLink Bandwidth Counter for Counter Set 0, Lane 1
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L2    = 62 # NVLink Bandwidth Counter for Counter Set 0, Lane 2
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L3    = 63 # NVLink Bandwidth Counter for Counter Set 0, Lane 3
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L4    = 64 # NVLink Bandwidth Counter for Counter Set 0, Lane 4
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L5    = 65 # NVLink Bandwidth Counter for Counter Set 0, Lane 5
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_TOTAL = 66 # NVLink Bandwidth Counter Total for Counter Set 0, All Lanes
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L0    = 67 # NVLink Bandwidth Counter for Counter Set 1, Lane 0
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L1    = 68 # NVLink Bandwidth Counter for Counter Set 1, Lane 1
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L2    = 69 # NVLink Bandwidth Counter for Counter Set 1, Lane 2
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L3    = 70 # NVLink Bandwidth Counter for Counter Set 1, Lane 3
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L4    = 71 # NVLink Bandwidth Counter for Counter Set 1, Lane 4
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L5    = 72 # NVLink Bandwidth Counter for Counter Set 1, Lane 5
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_TOTAL = 73 # NVLink Bandwidth Counter Total for Counter Set 1, All Lanes
+
+# Perf Policy Counters
+MXSMLEX_FI_DEV_PERF_POLICY_POWER             = 74   # Perf Policy Counter for Power Policy
+MXSMLEX_FI_DEV_PERF_POLICY_THERMAL           = 75   # Perf Policy Counter for Thermal Policy
+MXSMLEX_FI_DEV_PERF_POLICY_SYNC_BOOST        = 76   # Perf Policy Counter for Sync boost Policy
+MXSMLEX_FI_DEV_PERF_POLICY_BOARD_LIMIT       = 77   # Perf Policy Counter for Board Limit
+MXSMLEX_FI_DEV_PERF_POLICY_LOW_UTILIZATION   = 78   # Perf Policy Counter for Low GPU Utilization Policy
+MXSMLEX_FI_DEV_PERF_POLICY_RELIABILITY       = 79   # Perf Policy Counter for Reliability Policy
+MXSMLEX_FI_DEV_PERF_POLICY_TOTAL_APP_CLOCKS  = 80   # Perf Policy Counter for Total App Clock Policy
+MXSMLEX_FI_DEV_PERF_POLICY_TOTAL_BASE_CLOCKS = 81   # Perf Policy Counter for Total Base Clocks Policy
+
+# Memory temperatures
+MXSMLEX_FI_DEV_MEMORY_TEMP  = 82 # Memory temperature for the device
+
+# Energy Counter
+MXSMLEX_FI_DEV_TOTAL_ENERGY_CONSUMPTION = 83 # Total energy consumption for the GPU in mJ since the driver was last reloaded
+
+# NVLink Speed
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L0     = 84
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L1     = 85
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L2     = 86
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L3     = 87
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L4     = 88
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L5     = 89
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_COMMON = 90
+
+# NVLink Link Count
+MXSMLEX_FI_DEV_NVLINK_LINK_COUNT = 91
+
+# Page Retirement pending fields
+MXSMLEX_FI_DEV_RETIRED_PENDING_SBE = 92
+MXSMLEX_FI_DEV_RETIRED_PENDING_DBE = 93
+
+# PCIe replay and replay rollover counters
+MXSMLEX_FI_DEV_PCIE_REPLAY_COUNTER = 94
+MXSMLEX_FI_DEV_PCIE_REPLAY_ROLLOVER_COUNTER = 95
+
+# NvLink Flit Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L6   = 96 # NVLink flow control CRC  Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L7   = 97 # NVLink flow control CRC  Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L8   = 98 # NVLink flow control CRC  Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L9   = 99 # NVLink flow control CRC  Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L10  = 100 # NVLink flow control CRC  Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L11  = 101 # NVLink flow control CRC  Error Counter for Lane 11
+
+# NvLink CRC Data Error Counters
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L6   = 102 # NVLink data CRC Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L7   = 103 # NVLink data CRC Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L8   = 104 # NVLink data CRC Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L9   = 105 # NVLink data CRC Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L10  = 106 # NVLink data CRC Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L11  = 107 # NVLink data CRC Error Counter for Lane 11
+
+# NvLink Replay Error Counters
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L6     = 108 # NVLink Replay Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L7     = 109 # NVLink Replay Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L8     = 110 # NVLink Replay Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L9     = 111 # NVLink Replay Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L10    = 112 # NVLink Replay Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L11    = 113 # NVLink Replay Error Counter for Lane 11
+
+# NvLink Recovery Error Counters
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L6   = 114 # NVLink Recovery Error Counter for Lane 6
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L7   = 115 # NVLink Recovery Error Counter for Lane 7
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L8   = 116 # NVLink Recovery Error Counter for Lane 8
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L9   = 117 # NVLink Recovery Error Counter for Lane 9
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L10  = 118 # NVLink Recovery Error Counter for Lane 10
+MXSMLEX_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L11  = 119 # NVLink Recovery Error Counter for Lane 11
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L6    = 120 # NVLink Bandwidth Counter for Counter Set 0, Lane 6
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L7    = 121 # NVLink Bandwidth Counter for Counter Set 0, Lane 7
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L8    = 122 # NVLink Bandwidth Counter for Counter Set 0, Lane 8
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L9    = 123 # NVLink Bandwidth Counter for Counter Set 0, Lane 9
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L10   = 124 # NVLink Bandwidth Counter for Counter Set 0, Lane 10
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C0_L11   = 125 # NVLink Bandwidth Counter for Counter Set 0, Lane 11
+
+# NvLink Bandwidth Counters
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L6    = 126 # NVLink Bandwidth Counter for Counter Set 1, Lane 6
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L7    = 127 # NVLink Bandwidth Counter for Counter Set 1, Lane 7
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L8    = 128 # NVLink Bandwidth Counter for Counter Set 1, Lane 8
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L9    = 129 # NVLink Bandwidth Counter for Counter Set 1, Lane 9
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L10   = 130 # NVLink Bandwidth Counter for Counter Set 1, Lane 10
+MXSMLEX_FI_DEV_NVLINK_BANDWIDTH_C1_L11   = 131 # NVLink Bandwidth Counter for Counter Set 1, Lane 11
+
+# NVLink Speed
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L6     = 132
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L7     = 133
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L8     = 134
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L9     = 135
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L10    = 136
+MXSMLEX_FI_DEV_NVLINK_SPEED_MBPS_L11    = 137
+
+# NVLink Throughput Counters
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_DATA_TX = 138 # NVLink TX Data throughput in KiB
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_DATA_RX = 139 # NVLink RX Data throughput in KiB
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_RAW_TX  = 140 # NVLink TX Data + protocol overhead in KiB
+MXSMLEX_FI_DEV_NVLINK_THROUGHPUT_RAW_RX  = 141 # NVLink RX Data + protocol overhead in KiB
+
+# Row Remapper
+MXSMLEX_FI_DEV_REMAPPED_COR        = 142
+MXSMLEX_FI_DEV_REMAPPED_UNC        = 143
+MXSMLEX_FI_DEV_REMAPPED_PENDING    = 144
+MXSMLEX_FI_DEV_REMAPPED_FAILURE    = 145
+
+#Remote device NVLink ID
+MXSMLEX_FI_DEV_NVLINK_REMOTE_NVLINK_ID = 146
+
+# Number of NVLinks connected to NVSwitch
+MXSMLEX_FI_DEV_NVSWITCH_CONNECTED_LINK_COUNT = 147
+
+# NvLink ECC Data Error Counters
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L0    = 148 #< NVLink data ECC Error Counter for Link 0
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L1    = 149 #< NVLink data ECC Error Counter for Link 1
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L2    = 150 #< NVLink data ECC Error Counter for Link 2
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L3    = 151 #< NVLink data ECC Error Counter for Link 3
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L4    = 152 #< NVLink data ECC Error Counter for Link 4
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L5    = 153 #< NVLink data ECC Error Counter for Link 5
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L6    = 154 #< NVLink data ECC Error Counter for Link 6
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L7    = 155 #< NVLink data ECC Error Counter for Link 7
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L8    = 156 #< NVLink data ECC Error Counter for Link 8
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L9    = 157 #< NVLink data ECC Error Counter for Link 9
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L10   = 158 #< NVLink data ECC Error Counter for Link 10
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L11   = 159 #< NVLink data ECC Error Counter for Link 11
+MXSMLEX_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_TOTAL = 160 #< NvLink data ECC Error Counter total for all Links
+
+MXSMLEX_FI_DEV_NVLINK_ERROR_DL_REPLAY            = 161
+MXSMLEX_FI_DEV_NVLINK_ERROR_DL_RECOVERY          = 162
+MXSMLEX_FI_DEV_NVLINK_ERROR_DL_CRC               = 163
+MXSMLEX_FI_DEV_NVLINK_GET_SPEED                  = 164
+MXSMLEX_FI_DEV_NVLINK_GET_STATE                  = 165
+MXSMLEX_FI_DEV_NVLINK_GET_VERSION                = 166
+
+MXSMLEX_FI_DEV_NVLINK_GET_POWER_STATE            = 167
+MXSMLEX_FI_DEV_NVLINK_GET_POWER_THRESHOLD        = 168
+
+MXSMLEX_FI_DEV_PCIE_L0_TO_RECOVERY_COUNTER       = 169
+
+MXSMLEX_FI_DEV_C2C_LINK_COUNT                    = 170
+MXSMLEX_FI_DEV_C2C_LINK_GET_STATUS               = 171
+MXSMLEX_FI_DEV_C2C_LINK_GET_MAX_BW               = 172
+
+MXSMLEX_FI_DEV_PCIE_COUNT_CORRECTABLE_ERRORS     = 173
+MXSMLEX_FI_DEV_PCIE_COUNT_NAKS_RECEIVED          = 174
+MXSMLEX_FI_DEV_PCIE_COUNT_RECEIVER_ERROR         = 175
+MXSMLEX_FI_DEV_PCIE_COUNT_BAD_TLP                = 176
+MXSMLEX_FI_DEV_PCIE_COUNT_NAKS_SENT              = 177
+MXSMLEX_FI_DEV_PCIE_COUNT_BAD_DLLP               = 178
+MXSMLEX_FI_DEV_PCIE_COUNT_NON_FATAL_ERROR        = 179
+MXSMLEX_FI_DEV_PCIE_COUNT_FATAL_ERROR            = 180
+MXSMLEX_FI_DEV_PCIE_COUNT_UNSUPPORTED_REQ        = 181
+MXSMLEX_FI_DEV_PCIE_COUNT_LCRC_ERROR             = 182
+MXSMLEX_FI_DEV_PCIE_COUNT_LANE_ERROR             = 183
+
+MXSMLEX_FI_DEV_IS_RESETLESS_MIG_SUPPORTED        = 184
+
+MXSMLEX_FI_DEV_POWER_AVERAGE                     = 185
+MXSMLEX_FI_DEV_POWER_INSTANT                     = 186
+MXSMLEX_FI_DEV_POWER_MIN_LIMIT                   = 187
+MXSMLEX_FI_DEV_POWER_MAX_LIMIT                   = 188
+MXSMLEX_FI_DEV_POWER_DEFAULT_LIMIT               = 189
+MXSMLEX_FI_DEV_POWER_CURRENT_LIMIT               = 190
+MXSMLEX_FI_DEV_ENERGY                            = 191
+MXSMLEX_FI_DEV_POWER_REQUESTED_LIMIT             = 192
+
+MXSMLEX_FI_DEV_TEMPERATURE_SHUTDOWN_TLIMIT       = 193
+MXSMLEX_FI_DEV_TEMPERATURE_SLOWDOWN_TLIMIT       = 194
+MXSMLEX_FI_DEV_TEMPERATURE_MEM_MAX_TLIMIT        = 195
+MXSMLEX_FI_DEV_TEMPERATURE_GPU_MAX_TLIMIT        = 196
+
+MXSMLEX_FI_DEV_IS_MIG_MODE_INDEPENDENT_MIG_QUERY_CAPABLE   = 199
+
+MXSMLEX_FI_MAX = 200 # One greater than the largest field ID defined above
+
+
+## Enums needed for the method nvmlDeviceGetVirtualizationMode and nvmlDeviceSetVirtualizationMode
+MXSMLEX_GPU_VIRTUALIZATION_MODE_NONE        = 0  # Represents Bare Metal GPU
+MXSMLEX_GPU_VIRTUALIZATION_MODE_PASSTHROUGH = 1  # Device is associated with GPU-Passthorugh
+MXSMLEX_GPU_VIRTUALIZATION_MODE_VGPU        = 2  # Device is associated with vGPU inside virtual machine.
+MXSMLEX_GPU_VIRTUALIZATION_MODE_HOST_VGPU   = 3  # Device is associated with VGX hypervisor in vGPU mode
+MXSMLEX_GPU_VIRTUALIZATION_MODE_HOST_VSGA   = 4  # Device is associated with VGX hypervisor in vSGA mode
+
+## Lib loading ##
+nvmlLib = None
+libLoadLock = threading.Lock()
+_nvmlLib_refcount = 0 # Incremented on each nvmlInit and decremented on nvmlShutdown
+
+## vGPU Management
+_nvmlVgpuTypeId_t   = c_uint
+_nvmlVgpuInstance_t = c_uint
+
+_nvmlVgpuVmIdType_t = c_uint
+MXSMLEX_VGPU_VM_ID_DOMAIN_ID    = 0
+MXSMLEX_VGPU_VM_ID_UUID         = 1
+
+_nvmlGridLicenseFeatureCode_t = c_uint
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_UNKNOWN      = 0
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_VGPU         = 1
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_NVIDIA_RTX   = 2
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_VWORKSTATION = 2 # deprecated, use MXSMLEX_GRID_LICENSE_FEATURE_CODE_NVIDIA_RTX.
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_GAMING       = 3
+MXSMLEX_GRID_LICENSE_FEATURE_CODE_COMPUTE      = 4
+
+_nvmlGridLicenseExpiryStatus_t = c_uint8
+MXSMLEX_GRID_LICENSE_EXPIRY_NOT_AVAILABLE    = 0,   # Expiry information not available
+MXSMLEX_GRID_LICENSE_EXPIRY_INVALID          = 1,   # Invalid expiry or error fetching expiry
+MXSMLEX_GRID_LICENSE_EXPIRY_VALID            = 2,   # Valid expiry
+MXSMLEX_GRID_LICENSE_EXPIRY_NOT_APPLICABLE   = 3,   # Expiry not applicable
+MXSMLEX_GRID_LICENSE_EXPIRY_PERMANENT        = 4,   # Permanent expiry
+
+_nvmlVgpuCapability_t = c_uint
+MXSMLEX_VGPU_CAP_NVLINK_P2P                    = 0  # vGPU P2P over NVLink is supported
+MXSMLEX_VGPU_CAP_GPUDIRECT                     = 1  # GPUDirect capability is supported
+MXSMLEX_VGPU_CAP_MULTI_VGPU_EXCLUSIVE          = 2  # vGPU profile cannot be mixed with other vGPU profiles in same VM
+MXSMLEX_VGPU_CAP_EXCLUSIVE_TYPE                = 3  # vGPU profile cannot run on a GPU alongside other profiles of different type
+MXSMLEX_VGPU_CAP_EXCLUSIVE_SIZE                = 4  # vGPU profile cannot run on a GPU alongside other profiles of different size
+MXSMLEX_VGPU_CAP_COUNT                         = 5
+
+_nvmlVgpuDriverCapability_t = c_uint
+MXSMLEX_VGPU_DRIVER_CAP_HETEROGENEOUS_MULTI_VGPU          = 0  # Supports mixing of different vGPU profiles within one guest VM
+MXSMLEX_VGPU_DRIVER_CAP_COUNT                             = 1
+
+_nvmlDeviceVgpuCapability_t = c_uint
+MXSMLEX_DEVICE_VGPU_CAP_FRACTIONAL_MULTI_VGPU             = 0  # Query if the fractional vGPU profiles on this GPU can be used in multi-vGPU configurations
+MXSMLEX_DEVICE_VGPU_CAP_HETEROGENEOUS_TIMESLICE_PROFILES  = 1  # Query if the GPU supports concurrent execution of timesliced vGPU profiles of differing types
+MXSMLEX_DEVICE_VGPU_CAP_HETEROGENEOUS_TIMESLICE_SIZES     = 2  # Query if the GPU supports concurrent execution of timesliced vGPU profiles of differing framebuffer sizes
+MXSMLEX_DEVICE_VGPU_CAP_READ_DEVICE_BUFFER_BW             = 3  # Query the GPU's read_device_buffer expected bandwidth capacity in megabytes per second
+MXSMLEX_DEVICE_VGPU_CAP_WRITE_DEVICE_BUFFER_BW            = 4  # Query the GPU's write_device_buffer expected bandwidth capacity in megabytes per second
+MXSMLEX_DEVICE_VGPU_CAP_DEVICE_STREAMING                  = 5  # Query if vGPU profiles on the GPU supports migration data streaming
+MXSMLEX_DEVICE_VGPU_CAP_MINI_QUARTER_GPU                  = 6  # Set/Get support of mini-quarter vGPU profiles
+MXSMLEX_DEVICE_VGPU_CAP_COMPUTE_MEDIA_ENGINE_GPU          = 7  # Set/Get support for compute media engine vGPU profiles
+MXSMLEX_DEVICE_VGPU_CAP_COUNT                             = 8
+
+_nvmlVgpuGuestInfoState_t = c_uint
+MXSMLEX_VGPU_INSTANCE_GUEST_INFO_STATE_UNINITIALIZED = 0
+MXSMLEX_VGPU_INSTANCE_GUEST_INFO_STATE_INITIALIZED   = 1
+
+_nvmlVgpuVmCompatibility_t = c_uint
+MXSMLEX_VGPU_VM_COMPATIBILITY_NONE         = 0x0
+MXSMLEX_VGPU_VM_COMPATIBILITY_COLD         = 0x1
+MXSMLEX_VGPU_VM_COMPATIBILITY_HIBERNATE    = 0x2
+MXSMLEX_VGPU_VM_COMPATIBILITY_SLEEP        = 0x4
+MXSMLEX_VGPU_VM_COMPATIBILITY_LIVE         = 0x8
+
+_nvmlVgpuPgpuCompatibilityLimitCode_t = c_uint
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_NONE          = 0x0
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_HOST_DRIVER   = 0x1
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_GUEST_DRIVER  = 0x2
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_GPU           = 0x4
+MXSMLEX_VGPU_COMPATIBILITY_LIMIT_OTHER         = 0x80000000
+
+_nvmlHostVgpuMode_t = c_uint
+MXSMLEX_HOST_VGPU_MODE_NON_SRIOV   = 0
+MXSMLEX_HOST_VGPU_MODE_SRIOV       = 1
+
+_nvmlConfComputeGpusReadyState_t = c_uint
+MXSMLEX_CC_ACCEPTING_CLIENT_REQUESTS_FALSE = 0
+MXSMLEX_CC_ACCEPTING_CLIENT_REQUESTS_TRUE = 1
+
+_nvmlConfComputeGpuCaps_t = c_uint
+MXSMLEX_CC_SYSTEM_GPUS_CC_NOT_CAPABLE = 0
+MXSMLEX_CC_SYSTEM_GPUS_CC_CAPABLE = 1
+
+_nvmlConfComputeCpuCaps_t = c_uint
+MXSMLEX_CC_SYSTEM_CPU_CAPS_NONE = 0
+MXSMLEX_CC_SYSTEM_CPU_CAPS_AMD_SEV = 1
+MXSMLEX_CC_SYSTEM_CPU_CAPS_INTEL_TDX = 2
+
+_nvmlConfComputeDevToolsMode_t = c_uint
+MXSMLEX_CC_SYSTEM_DEVTOOLS_MODE_OFF = 0
+MXSMLEX_CC_SYSTEM_DEVTOOLS_MODE_ON = 1
+ 
+MXSMLEX_CC_SYSTEM_ENVIRONMENT_UNAVAILABLE = 0
+MXSMLEX_CC_SYSTEM_ENVIRONMENT_SIM = 1
+MXSMLEX_CC_SYSTEM_ENVIRONMENT_PROD = 2
+ 
+_nvmlConfComputeCcFeature_t = c_uint
+MXSMLEX_CC_SYSTEM_FEATURE_DISABLED = 0
+MXSMLEX_CC_SYSTEM_FEATURE_ENABLED = 1
+
+_nvmlConfComputeCcKeyRotationThreshAttackerAdv_t = c_uint
+MXSMLEX_CC_KEY_ROTATION_THRESH_ATTACKER_ADVANTAGE_MIN = 50
+MXSMLEX_CC_KEY_ROTATION_THRESH_ATTACKER_ADVANTAGE_MAX = 75
+
+# GSP firmware
+MXSMLEX_GSP_FIRMWARE_VERSION_BUF_SIZE = 0x40
+
+class MXSMLEXLibraryMismatchError(Exception):
+    pass
+
+## Error Checking ##
+class NVMLError(Exception):
+    _valClassMapping = dict()
+    # List of currently known error codes
+    _errcode_to_string = {
+        MXSMLEX_ERROR_UNINITIALIZED:       "Uninitialized",
+        MXSMLEX_ERROR_INVALID_ARGUMENT:    "Invalid Argument",
+        MXSMLEX_ERROR_NOT_SUPPORTED:       "Not Supported",
+        MXSMLEX_ERROR_NO_PERMISSION:       "Insufficient Permissions",
+        MXSMLEX_ERROR_ALREADY_INITIALIZED: "Already Initialized",
+        MXSMLEX_ERROR_NOT_FOUND:           "Not Found",
+        MXSMLEX_ERROR_INSUFFICIENT_SIZE:   "Insufficient Size",
+        MXSMLEX_ERROR_INSUFFICIENT_POWER:  "Insufficient External Power",
+        MXSMLEX_ERROR_DRIVER_NOT_LOADED:   "Driver Not Loaded",
+        MXSMLEX_ERROR_TIMEOUT:             "Timeout",
+        MXSMLEX_ERROR_IRQ_ISSUE:           "Interrupt Request Issue",
+        MXSMLEX_ERROR_LIBRARY_NOT_FOUND:   "MXSMLEX Shared Library Not Found",
+        MXSMLEX_ERROR_FUNCTION_NOT_FOUND:  "Function Not Found",
+        MXSMLEX_ERROR_CORRUPTED_INFOROM:   "Corrupted infoROM",
+        MXSMLEX_ERROR_GPU_IS_LOST:         "GPU is lost",
+        MXSMLEX_ERROR_RESET_REQUIRED:      "GPU requires restart",
+        MXSMLEX_ERROR_OPERATING_SYSTEM:    "The operating system has blocked the request.",
+        MXSMLEX_ERROR_LIB_RM_VERSION_MISMATCH: "RM has detected an MXSMLEX/RM version mismatch.",
+        MXSMLEX_ERROR_MEMORY:              "Insufficient Memory",
+        MXSMLEX_ERROR_UNKNOWN:             "Unknown Error",
+        }
+    def __new__(typ, value):
+        '''
+        Maps value to a proper subclass of NVMLError.
+        See _extractNVMLErrorsAsClasses function for more details
+        '''
+        if typ == NVMLError:
+            typ = NVMLError._valClassMapping.get(value, typ)
+        obj = Exception.__new__(typ)
+        obj.value = value
+        return obj
+    def __str__(self):
+        try:
+            if self.value not in NVMLError._errcode_to_string:
+                NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value))
+            return NVMLError._errcode_to_string[self.value]
+        except NVMLError:
+            return "MXSMLEX Error with code %d" % self.value
+    def __eq__(self, other):
+        return self.value == other.value
+
+def nvmlExceptionClass(nvmlErrorCode):
+    if nvmlErrorCode not in NVMLError._valClassMapping:
+        raise ValueError('nvmlErrorCode %s is not valid' % nvmlErrorCode)
+    return NVMLError._valClassMapping[nvmlErrorCode]
+
+def _extractNVMLErrorsAsClasses():
+    '''
+    Generates a hierarchy of classes on top of NVMLError class.
+
+    Each MXSMLEX Error gets a new NVMLError subclass. This way try,except blocks can filter appropriate
+    exceptions more easily.
+
+    NVMLError is a parent class. Each MXSMLEX_ERROR_* gets it's own subclass.
+    e.g. MXSMLEX_ERROR_ALREADY_INITIALIZED will be turned into NVMLError_AlreadyInitialized
+    '''
+    this_module = sys.modules[__name__]
+    nvmlErrorsNames = [x for x in dir(this_module) if x.startswith("MXSMLEX_ERROR_")]
+    for err_name in nvmlErrorsNames:
+        # e.g. Turn MXSMLEX_ERROR_ALREADY_INITIALIZED into NVMLError_AlreadyInitialized
+        class_name = "NVMLError_" + string.capwords(err_name.replace("MXSMLEX_ERROR_", ""), "_").replace("_", "")
+        err_val = getattr(this_module, err_name)
+        def gen_new(val):
+            def new(typ):
+                obj = NVMLError.__new__(typ, val)
+                return obj
+            return new
+        new_error_class = type(class_name, (NVMLError,), {'__new__': gen_new(err_val)})
+        new_error_class.__module__ = __name__
+        setattr(this_module, class_name, new_error_class)
+        NVMLError._valClassMapping[err_val] = new_error_class
+_extractNVMLErrorsAsClasses()
+
+def _nvmlCheckReturn(ret):
+    if (ret != MXSMLEX_SUCCESS):
+        raise NVMLError(ret)
+    return ret
+
+## Function access ##
+_nvmlGetFunctionPointer_cache = dict() # function pointers are cached to prevent unnecessary libLoadLock locking
+def _nvmlGetFunctionPointer(name):
+    global nvmlLib
+
+    if name in _nvmlGetFunctionPointer_cache:
+        return _nvmlGetFunctionPointer_cache[name]
+
+    libLoadLock.acquire()
+    try:
+        # ensure library was loaded
+        if (nvmlLib == None):
+            raise NVMLError(MXSMLEX_ERROR_UNINITIALIZED)
+        try:
+            _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name)
+            return _nvmlGetFunctionPointer_cache[name]
+        except AttributeError:
+            print("nvml error")
+            #raise NVMLError(MXSMLEX_ERROR_FUNCTION_NOT_FOUND)
+    finally:
+        # lock is always freed
+        libLoadLock.release()
+
+## Alternative object
+# Allows the object to be printed
+# Allows mismatched types to be assigned
+#  - like None when the Structure variant requires c_uint
+class nvmlFriendlyObject(object):
+    def __init__(self, dictionary):
+        for x in dictionary:
+            setattr(self, x, dictionary[x])
+    def __str__(self):
+        return self.__dict__.__str__()
+
+def nvmlStructToFriendlyObject(struct):
+    d = {}
+    for x in struct._fields_:
+        key = x[0]
+        value = getattr(struct, key)
+        # only need to convert from bytes if bytes, no need to check python version.
+        d[key] = value.decode() if isinstance(value, bytes) else value
+    obj = nvmlFriendlyObject(d)
+    return obj
+
+# pack the object so it can be passed to the MXSMLEX library
+def nvmlFriendlyObjectToStruct(obj, model):
+    for x in model._fields_:
+        key = x[0]
+        value = obj.__dict__[key]
+        # any c_char_p in python3 needs to be bytes, default encoding works fine.
+        if sys.version_info >= (3,):
+            setattr(model, key, value.encode())
+        else:
+            setattr(model, key, value)
+    return model
+
+## Unit structures
+class struct_c_nvmlUnit_t(Structure):
+    pass # opaque handle
+c_nvmlUnit_t = POINTER(struct_c_nvmlUnit_t)
+
+class _PrintableStructure(Structure):
+    """
+    Abstract class that produces nicer __str__ output than ctypes.Structure.
+    e.g. instead of:
+      >>> print str(obj)
+      <class_name object at 0x7fdf82fef9e0>
+    this class will print
+      class_name(field_name: formatted_value, field_name: formatted_value)
+
+    _fmt_ dictionary of <str _field_ name> -> <str format>
+    e.g. class that has _field_ 'hex_value', c_uint could be formatted with
+      _fmt_ = {"hex_value" : "%08X"}
+    to produce nicer output.
+    Default fomratting string for all fields can be set with key "<default>" like:
+      _fmt_ = {"<default>" : "%d MHz"} # e.g all values are numbers in MHz.
+    If not set it's assumed to be just "%s"
+
+    Exact format of returned str from this class is subject to change in the future.
+    """
+    _fmt_ = {}
+    def __str__(self):
+        result = []
+        for x in self._fields_:
+            key = x[0]
+            value = getattr(self, key)
+            fmt = "%s"
+            if key in self._fmt_:
+                fmt = self._fmt_[key]
+            elif "<default>" in self._fmt_:
+                fmt = self._fmt_["<default>"]
+            result.append(("%s: " + fmt) % (key, value))
+        return self.__class__.__name__ + "(" +  ", ".join(result) + ")"
+
+    def __getattribute__(self, name):
+        res = super(_PrintableStructure, self).__getattribute__(name)
+        # need to convert bytes to unicode for python3 don't need to for python2
+        # Python 2 strings are of both str and bytes
+        # Python 3 strings are not of type bytes
+        # ctypes should convert everything to the correct values otherwise
+        if isinstance(res, bytes):
+            if isinstance(res, str):
+                return res
+            return res.decode()
+        return res
+
+    def __setattr__(self, name, value):
+        if isinstance(value, str):
+            # encoding a python2 string returns the same value, since python2 strings are bytes already
+            # bytes passed in python3 will be ignored.
+            value = value.encode()
+        super(_PrintableStructure, self).__setattr__(name, value)
+
+class c_nvmlUnitInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('name', c_char * 96),
+        ('id', c_char * 96),
+        ('serial', c_char * 96),
+        ('firmwareVersion', c_char * 96),
+    ]
+
+class c_nvmlC2cModeInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('isC2cEnabled', c_uint)
+    ]
+
+nvmlC2cModeInfo_v1 = 0x1000008;
+
+class c_nvmlLedState_t(_PrintableStructure):
+    _fields_ = [
+        ('cause', c_char * 256),
+        ('color', _nvmlLedColor_t),
+    ]
+
+class c_nvmlPSUInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('state', c_char * 256),
+        ('current', c_uint),
+        ('voltage', c_uint),
+        ('power', c_uint),
+    ]
+
+class c_nvmlUnitFanInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('speed', c_uint),
+        ('state', _nvmlFanState_t),
+    ]
+
+class c_nvmlUnitFanSpeeds_t(_PrintableStructure):
+    _fields_ = [
+        ('fans', c_nvmlUnitFanInfo_t * 24),
+        ('count', c_uint)
+    ]
+
+## Device structures
+class struct_c_nvmlDevice_t(Structure):
+    pass # opaque handle
+c_nvmlDevice_t = POINTER(struct_c_nvmlDevice_t)
+
+class nvmlPciInfoExt_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('domain', c_uint),
+        ('bus', c_uint),
+        ('device', c_uint),
+        ('pciDeviceId', c_uint),
+        ('pciSubSystemId', c_uint),
+        ('baseClass', c_uint),
+        ('subClass', c_uint),
+        ('busId', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE),
+    ]
+    _fmt_ = {
+            'version'        : "0x%04X",
+            'domain'         : "0x%04X",
+            'bus'            : "0x%02X",
+            'device'         : "0x%02X",
+            'pciDeviceId'    : "0x%08X",
+            'pciSubSystemId' : "0x%08X",
+            'baseClass'      : "0x%01X",
+            'subClass'       : "0x%01X",
+            }
+
+nvmlPciInfoExt_v1 = 0x1000040
+
+# Legacy pciInfo used for _v1 and _v2
+class nvmlPciInfo_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('busId', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE),
+        ('domain', c_uint),
+        ('bus', c_uint),
+        ('device', c_uint),
+        ('pciDeviceId', c_uint),
+
+        # Added in 2.285
+        ('pciSubSystemId', c_uint),
+        ('reserved0', c_uint),
+        ('reserved1', c_uint),
+        ('reserved2', c_uint),
+        ('reserved3', c_uint),
+    ]
+    _fmt_ = {
+            'domain'         : "0x%04X",
+            'bus'            : "0x%02X",
+            'device'         : "0x%02X",
+            'pciDeviceId'    : "0x%08X",
+            'pciSubSystemId' : "0x%08X",
+            }
+
+class nvmlPciInfo_t(_PrintableStructure):
+    _fields_ = [
+        # Moved to the new busId location below
+        ('busIdLegacy', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE),
+        ('domain', c_uint),
+        ('bus', c_uint),
+        ('device', c_uint),
+        ('pciDeviceId', c_uint),
+
+        # Added in 2.285
+        ('pciSubSystemId', c_uint),
+        # New busId replaced the long deprecated and reserved fields with a
+        # field of the same size in 9.0
+        ('busId', c_char * MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE),
+    ]
+    _fmt_ = {
+            'domain'         : "0x%08X",
+            'bus'            : "0x%02X",
+            'device'         : "0x%02X",
+            'pciDeviceId'    : "0x%08X",
+            'pciSubSystemId' : "0x%08X",
+            }
+
+class c_nvmlExcludedDeviceInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('pci', nvmlPciInfo_t),
+        ('uuid', c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    ]
+
+class nvmlNvLinkUtilizationControl_t(_PrintableStructure):
+    _fields_ = [
+        ('units', _nvmlNvLinkUtilizationCountUnits_t),
+        ('pktfilter', _nvmlNvLinkUtilizationCountPktTypes_t),
+    ]
+
+class c_nvmlMemory_t(_PrintableStructure):
+    _fields_ = [
+        ('total', c_ulonglong),
+        ('free', c_ulonglong),
+        ('used', c_ulonglong),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+class c_nvmlMemory_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('total', c_ulonglong),
+        ('reserved', c_ulonglong),
+        ('free', c_ulonglong),
+        ('used', c_ulonglong),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+nvmlMemory_v2 = 0x02000028
+
+class c_nvmlBAR1Memory_t(_PrintableStructure):
+    _fields_ = [
+        ('bar1Total', c_ulonglong),
+        ('bar1Free', c_ulonglong),
+        ('bar1Used', c_ulonglong),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+class nvmlClkMonFaultInfo_t(Structure):
+    _fields_ = [("clkApiDomain", c_uint),
+                ("clkDomainFaultMask", c_uint)
+    ]
+
+class nvmlClkMonStatus_t(Structure):
+    _fields_ = [("bGlobalStatus", c_uint),
+                ("clkMonListSize", c_uint),
+                ("clkMonList", nvmlClkMonFaultInfo_t)
+    ]
+
+# On Windows with the WDDM driver, usedGpuMemory is reported as None
+# Code that processes this structure should check for None, I.E.
+#
+# if (info.usedGpuMemory == None):
+#     # TODO handle the error
+#     pass
+# else:
+#    print("Using %d MiB of memory" % (info.usedGpuMemory / 1024 / 1024))
+# endif
+#
+# See MXSMLEX documentation for more information
+class c_nvmlProcessInfo_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('pid', c_uint),
+        ('usedGpuMemory', c_ulonglong),
+        ('gpuInstanceId', c_uint),
+        ('computeInstanceId', c_uint),
+    ]
+    _fmt_ = {'usedGpuMemory': "%d B"}
+
+c_nvmlProcessInfo_v3_t = c_nvmlProcessInfo_v2_t
+
+c_nvmlProcessInfo_t = c_nvmlProcessInfo_v3_t
+
+_nvmlProcessMode_t = c_uint
+MXSMLEX_PROCESS_MODE_COMPUTE  = 0
+MXSMLEX_PROCESS_MODE_GRAPHICS = 1
+MXSMLEX_PROCESS_MODE_MPS      = 2
+
+class c_nvmlProcessDetail_v1_t(Structure):
+    _fields_ = [
+        ('pid', c_uint),
+        ('usedGpuMemory', c_ulonglong),
+        ('gpuInstanceId', c_uint),
+        ('computeInstanceId', c_uint),
+        ('usedGpuCcProtectedMemory', c_ulonglong),
+    ]
+
+class c_nvmlProcessDetailList_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('mode', _nvmlProcessMode_t),
+        ('numProcArrayEntries', c_uint),
+        ('procArray', POINTER(c_nvmlProcessDetail_v1_t)),
+    ]
+    _fmt_ = {'numProcArrayEntries': "%d B"}
+
+c_nvmlProcessDetailList_t = c_nvmlProcessDetailList_v1_t
+
+nvmlProcessDetailList_v1 = 0x1000018
+
+class c_nvmlBridgeChipInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('type', _nvmlBridgeChipType_t),
+        ('fwVersion', c_uint),
+    ]
+
+class c_nvmlBridgeChipHierarchy_t(_PrintableStructure):
+    _fields_ = [
+        ('bridgeCount', c_uint),
+        ('bridgeChipInfo', c_nvmlBridgeChipInfo_t * 128),
+    ]
+
+class c_nvmlEccErrorCounts_t(_PrintableStructure):
+    _fields_ = [
+        ('l1Cache', c_ulonglong),
+        ('l2Cache', c_ulonglong),
+        ('deviceMemory', c_ulonglong),
+        ('registerFile', c_ulonglong),
+    ]
+
+class c_nvmlUtilization_t(_PrintableStructure):
+    _fields_ = [
+        ('gpu', c_uint),
+        ('memory', c_uint),
+    ]
+    _fmt_ = {'<default>': "%d %%"}
+
+# Added in 2.285
+class c_nvmlHwbcEntry_t(_PrintableStructure):
+    _fields_ = [
+        ('hwbcId', c_uint),
+        ('firmwareVersion', c_char * 32),
+    ]
+
+class c_nvmlValue_t(Union):
+    _fields_ = [
+        ('dVal', c_double),
+        ('uiVal', c_uint),
+        ('ulVal', c_ulong),
+        ('ullVal', c_ulonglong),
+        ('sllVal', c_longlong),
+        ('siVal', c_int),
+    ]
+
+class c_nvmlSample_t(_PrintableStructure):
+    _fields_ = [
+        ('timeStamp', c_ulonglong),
+        ('sampleValue', c_nvmlValue_t),
+    ]
+
+class c_nvmlViolationTime_t(_PrintableStructure):
+    _fields_ = [
+        ('referenceTime', c_ulonglong),
+        ('violationTime', c_ulonglong),
+    ]
+
+class c_nvmlFieldValue_t(_PrintableStructure):
+    _fields_ = [
+        ('fieldId', c_uint32),
+        ('scopeId', c_uint32),
+        ('timestamp', c_int64),
+        ('latencyUsec', c_int64),
+        ('valueType', _nvmlValueType_t),
+        ('nvmlReturn', _nvmlReturn_t),
+        ('value', c_nvmlValue_t)
+    ]
+
+class c_nvmlVgpuHeterogeneousMode_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('mode', c_uint),
+    ]
+
+VgpuHeterogeneousMode_v1 = 0x1000008
+
+class c_nvmlVgpuPlacementId_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('placementId', c_uint),
+    ]
+
+VgpuPlacementId_v1 = 0x1000008
+
+class c_nvmlVgpuPlacementList_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('count', c_uint),
+        ('placementSize', c_uint),
+        ('placementIds', POINTER(c_uint)),
+    ]
+
+VgpuPlacementList_v1 = 0x1000018
+
+class c_nvmlVgpuInstanceUtilizationSample_t(_PrintableStructure):
+    _fields_ = [
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('timeStamp', c_ulonglong),
+        ('smUtil', c_nvmlValue_t),
+        ('memUtil', c_nvmlValue_t),
+        ('encUtil', c_nvmlValue_t),
+        ('decUtil', c_nvmlValue_t),
+    ]
+
+class c_nvmlVgpuInstanceUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('timeStamp', c_ulonglong),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('smUtil', c_nvmlValue_t),
+        ('memUtil', c_nvmlValue_t),
+        ('encUtil', c_nvmlValue_t),
+        ('decUtil', c_nvmlValue_t),
+        ('jpgUtil', c_nvmlValue_t),
+        ('ofaUtil', c_nvmlValue_t),
+    ]
+
+class c_nvmlVgpuInstancesUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('sampleValType', _nvmlValueType_t),
+        ('vgpuInstanceCount', c_uint),
+        ('lastSeenTimeStamp', c_ulonglong),
+        ('vgpuUtilArray', POINTER(c_nvmlVgpuInstanceUtilizationInfo_v1_t)),
+    ]
+
+VgpuInstancesUtilizationInfo_v1 = 0x01000020
+
+class c_nvmlVgpuProcessUtilizationSample_t(_PrintableStructure):
+    _fields_ = [
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('pid', c_uint),
+        ('processName', c_char * MXSMLEX_VGPU_NAME_BUFFER_SIZE),
+        ('timeStamp', c_ulonglong),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+    ]
+
+class c_nvmlVgpuProcessUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('processName', c_char * MXSMLEX_VGPU_NAME_BUFFER_SIZE),
+        ('timeStamp', c_ulonglong),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('pid', c_uint),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+        ('jpgUtil', c_uint),
+        ('ofaUtil', c_uint),
+    ]
+
+class c_nvmlVgpuProcessesUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('vgpuProcessCount', c_uint),
+        ('lastSeenTimeStamp', c_ulonglong),
+        ('vgpuProcUtilArray', POINTER(c_nvmlVgpuProcessUtilizationInfo_v1_t)),
+    ]
+
+VgpuProcessesUtilizationInfo_v1 = 0x01000018
+
+class c_nvmlVgpuLicenseExpiry_t(_PrintableStructure):
+    _fields_ = [
+        ('year',    c_uint32),
+        ('month',   c_uint16),
+        ('day',     c_uint16),
+        ('hour',    c_uint16),
+        ('min',     c_uint16),
+        ('sec',     c_uint16),
+        ('status',  c_uint8),
+    ]
+
+MXSMLEX_GRID_LICENSE_STATE_UNKNOWN                 = 0
+MXSMLEX_GRID_LICENSE_STATE_UNINITIALIZED           = 1
+MXSMLEX_GRID_LICENSE_STATE_UNLICENSED_UNRESTRICTED = 2
+MXSMLEX_GRID_LICENSE_STATE_UNLICENSED_RESTRICTED   = 3
+MXSMLEX_GRID_LICENSE_STATE_UNLICENSED              = 4
+MXSMLEX_GRID_LICENSE_STATE_LICENSED                = 5
+
+class c_nvmlVgpuLicenseInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('isLicensed',      c_uint8),
+        ('licenseExpiry',   c_nvmlVgpuLicenseExpiry_t),
+        ('currentState',    c_uint),
+    ]
+
+class c_nvmlEncoderSession_t(_PrintableStructure):
+    _fields_ = [
+        ('sessionId', c_uint),
+        ('pid', c_uint),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('codecType', c_uint),
+        ('hResolution', c_uint),
+        ('vResolution', c_uint),
+        ('averageFps', c_uint),
+        ('encodeLatency', c_uint),
+    ]
+
+class c_nvmlProcessUtilizationSample_t(_PrintableStructure):
+    _fields_ = [
+        ('pid', c_uint),
+        ('timeStamp', c_ulonglong),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+    ]
+
+class c_nvmlProcessUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('timeStamp', c_ulonglong),
+        ('pid', c_uint),
+        ('smUtil', c_uint),
+        ('memUtil', c_uint),
+        ('encUtil', c_uint),
+        ('decUtil', c_uint),
+        ('jpgUtil', c_uint),
+        ('ofaUtil', c_uint),
+    ]
+
+class c_nvmlProcessesUtilizationInfo_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('processSamplesCount', c_uint),
+        ('lastSeenTimeStamp', c_ulonglong),
+        ('procUtilArray', POINTER(c_nvmlProcessUtilizationInfo_v1_t)),
+    ]
+
+ProcessesUtilizationInfo_v1 = 0x01000018
+
+class c_nvmlGridLicenseExpiry_t(_PrintableStructure):
+    _fields_ = [
+        ('year',    c_uint32),
+        ('month',   c_uint16),
+        ('day',     c_uint16),
+        ('hour',    c_uint16),
+        ('min',     c_uint16),
+        ('sec',     c_uint16),
+        ('status',  c_uint8),
+    ]
+
+class c_nvmlGridLicensableFeature_v4_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode',    _nvmlGridLicenseFeatureCode_t),
+        ('featureState',   c_uint),
+        ('licenseInfo',    c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('productName',    c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('featureEnabled', c_uint),
+        ('licenseExpiry',  c_nvmlGridLicenseExpiry_t),
+    ]
+
+class c_nvmlGridLicensableFeatures_v4_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported',  c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures',  c_nvmlGridLicensableFeature_v4_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+class c_nvmlGridLicensableFeature_v3_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode', _nvmlGridLicenseFeatureCode_t),
+        ('featureState', c_uint),
+        ('licenseInfo', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('productName', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('featureEnabled', c_uint),
+    ]
+
+class c_nvmlGridLicensableFeatures_v3_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported', c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_v3_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+class c_nvmlGridLicensableFeature_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode', _nvmlGridLicenseFeatureCode_t),
+        ('featureState', c_uint),
+        ('licenseInfo', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+        ('productName', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+    ]
+
+class c_nvmlGridLicensableFeatures_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported', c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_v2_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+class c_nvmlGridLicensableFeature_t(_PrintableStructure):
+    _fields_ = [
+        ('featureCode', _nvmlGridLicenseFeatureCode_t),
+        ('featureState', c_uint),
+        ('licenseInfo', c_char * MXSMLEX_GRID_LICENSE_BUFFER_SIZE),
+    ]
+
+class c_nvmlGridLicensableFeatures_t(_PrintableStructure):
+    _fields_ = [
+        ('isGridLicenseSupported', c_int),
+        ('licensableFeaturesCount', c_uint),
+        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_t * MXSMLEX_GRID_LICENSE_FEATURE_MAX_COUNT),
+    ]
+
+## Event structures
+class struct_c_nvmlEventSet_t(Structure):
+    pass # opaque handle
+c_nvmlEventSet_t = POINTER(struct_c_nvmlEventSet_t)
+
+nvmlEventTypeSingleBitEccError     = 0x0000000000000001
+nvmlEventTypeDoubleBitEccError     = 0x0000000000000002
+nvmlEventTypePState                = 0x0000000000000004
+nvmlEventTypeXidCriticalError      = 0x0000000000000008
+nvmlEventTypeClock                 = 0x0000000000000010
+nvmlEventTypePowerSourceChange     = 0x0000000000000080
+nvmlEventMigConfigChange           = 0x0000000000000100
+nvmlEventTypeNone                  = 0x0000000000000000
+nvmlEventTypeAll                   = (
+                                        nvmlEventTypeNone
+                                        | nvmlEventTypeSingleBitEccError
+                                        | nvmlEventTypeDoubleBitEccError
+                                        | nvmlEventTypePState
+                                        | nvmlEventTypeClock
+                                        | nvmlEventTypePowerSourceChange
+                                        | nvmlEventTypeXidCriticalError
+                                        | nvmlEventMigConfigChange
+                                     )
+
+## Clock Event Reasons defines
+nvmlClocksEventReasonGpuIdle              = 0x0000000000000001
+nvmlClocksEventReasonApplicationsClocksSetting = 0x0000000000000002
+nvmlClocksEventReasonUserDefinedClocks         = nvmlClocksEventReasonApplicationsClocksSetting # deprecated, use nvmlClocksEventReasonApplicationsClocksSetting
+nvmlClocksEventReasonSwPowerCap           = 0x0000000000000004
+nvmlClocksEventReasonHwSlowdown           = 0x0000000000000008
+nvmlClocksEventReasonSyncBoost            = 0x0000000000000010
+nvmlClocksEventReasonSwThermalSlowdown    = 0x0000000000000020
+nvmlClocksEventReasonHwThermalSlowdown    = 0x0000000000000040
+nvmlClocksEventReasonHwPowerBrakeSlowdown = 0x0000000000000080
+nvmlClocksEventReasonDisplayClockSetting  = 0x0000000000000100
+nvmlClocksEventReasonNone                 = 0x0000000000000000
+nvmlClocksEventReasonAll                  = (
+                                                  nvmlClocksEventReasonNone |
+                                                  nvmlClocksEventReasonGpuIdle |
+                                                  nvmlClocksEventReasonApplicationsClocksSetting |
+                                                  nvmlClocksEventReasonSwPowerCap |
+                                                  nvmlClocksEventReasonHwSlowdown |
+                                                  nvmlClocksEventReasonSyncBoost |
+                                                  nvmlClocksEventReasonSwThermalSlowdown |
+                                                  nvmlClocksEventReasonHwThermalSlowdown |
+                                                  nvmlClocksEventReasonHwPowerBrakeSlowdown |
+                                                  nvmlClocksEventReasonDisplayClockSetting
+                                               )
+
+## Following have been deprecated
+nvmlClocksThrottleReasonGpuIdle              = 0x0000000000000001
+nvmlClocksThrottleReasonApplicationsClocksSetting = 0x0000000000000002
+nvmlClocksThrottleReasonUserDefinedClocks         = nvmlClocksThrottleReasonApplicationsClocksSetting # deprecated, use nvmlClocksThrottleReasonApplicationsClocksSetting
+nvmlClocksThrottleReasonSwPowerCap           = 0x0000000000000004
+nvmlClocksThrottleReasonHwSlowdown           = 0x0000000000000008
+nvmlClocksThrottleReasonSyncBoost            = 0x0000000000000010
+nvmlClocksThrottleReasonSwThermalSlowdown    = 0x0000000000000020
+nvmlClocksThrottleReasonHwThermalSlowdown    = 0x0000000000000040
+nvmlClocksThrottleReasonHwPowerBrakeSlowdown = 0x0000000000000080
+nvmlClocksThrottleReasonDisplayClockSetting  = 0x0000000000000100
+nvmlClocksThrottleReasonNone                 = 0x0000000000000000
+nvmlClocksThrottleReasonAll                  = (
+                                                  nvmlClocksThrottleReasonNone |
+                                                  nvmlClocksThrottleReasonGpuIdle |
+                                                  nvmlClocksThrottleReasonApplicationsClocksSetting |
+                                                  nvmlClocksThrottleReasonSwPowerCap |
+                                                  nvmlClocksThrottleReasonHwSlowdown |
+                                                  nvmlClocksThrottleReasonSyncBoost |
+                                                  nvmlClocksThrottleReasonSwThermalSlowdown |
+                                                  nvmlClocksThrottleReasonHwThermalSlowdown |
+                                                  nvmlClocksThrottleReasonHwPowerBrakeSlowdown |
+                                                  nvmlClocksThrottleReasonDisplayClockSetting
+                                               )
+
+class c_nvmlEventData_t(_PrintableStructure):
+    _fields_ = [
+        ('device', c_nvmlDevice_t),
+        ('eventType', c_ulonglong),
+        ('eventData', c_ulonglong),
+        ('gpuInstanceId', c_uint),
+        ('computeInstanceId', c_uint)
+    ]
+    _fmt_ = {'eventType': "0x%08X"}
+
+class c_nvmlAccountingStats_t(_PrintableStructure):
+    _fields_ = [
+        ('gpuUtilization', c_uint),
+        ('memoryUtilization', c_uint),
+        ('maxMemoryUsage', c_ulonglong),
+        ('time', c_ulonglong),
+        ('startTime', c_ulonglong),
+        ('isRunning', c_uint),
+        ('reserved', c_uint * 5)
+    ]
+
+class c_nvmlVgpuVersion_t(Structure):
+    _fields_ = [("minVersion", c_uint),
+                ("maxVersion", c_uint)
+               ]
+
+class c_nvmlVgpuMetadata_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("revision", c_uint),
+                ("guestInfoState", _nvmlVgpuGuestInfoState_t),
+                ("guestDriverVersion", c_char * MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),
+                ("hostDriverVersion", c_char * MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),
+                ("reserved", c_uint * 6),
+                ("vgpuVirtualizationCaps", c_uint),
+                ("guestVgpuVersion", c_uint),
+                ("opaqueDataSize", c_uint),
+                ("opaqueData", c_char * MXSMLEX_VGPU_METADATA_OPAQUE_DATA_SIZE)
+               ]
+
+class c_nvmlVgpuPgpuMetadata_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("revision", c_uint),
+                ("hostDriverVersion", c_char * MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),
+                ("pgpuVirtualizationCaps", c_uint),
+                ("reserved", c_uint * 5),
+                ("hostSupportedVgpuRange", c_nvmlVgpuVersion_t),
+                ("opaqueDataSize", c_uint),
+                ("opaqueData", c_char * MXSMLEX_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE)
+               ]
+
+class c_nvmlVgpuPgpuCompatibility_t(Structure):
+    _fields_ = [("vgpuVmCompatibility", _nvmlVgpuVmCompatibility_t),
+                ("compatibilityLimitCode", _nvmlVgpuPgpuCompatibilityLimitCode_t)
+               ]
+
+## vGPU scheduler policy defines
+MXSMLEX_VGPU_SCHEDULER_POLICY_UNKNOWN      = 0
+MXSMLEX_VGPU_SCHEDULER_POLICY_BEST_EFFORT  = 1
+MXSMLEX_VGPU_SCHEDULER_POLICY_EQUAL_SHARE  = 2
+MXSMLEX_VGPU_SCHEDULER_POLICY_FIXED_SHARE  = 3
+
+## Supported vGPU scheduler policy count
+MXSMLEX_SUPPORTED_VGPU_SCHEDULER_POLICY_COUNT  = 3
+
+MXSMLEX_SCHEDULER_SW_MAX_LOG_ENTRIES           = 200
+
+MXSMLEX_VGPU_SCHEDULER_ARR_DEFAULT   = 0
+MXSMLEX_VGPU_SCHEDULER_ARR_DISABLE   = 1
+MXSMLEX_VGPU_SCHEDULER_ARR_ENABLE    = 2
+
+class c_nvmlVgpuSchedDataWithARR_t(_PrintableStructure):
+    _fields_ = [
+        ('avgFactor',   c_uint),
+        ('timeslice',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedData_t(_PrintableStructure):
+    _fields_ = [
+        ('timeslice',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedulerParams_t(Union):
+    _fields_ = [
+        ('vgpuSchedDataWithARR', c_nvmlVgpuSchedDataWithARR_t),
+        ('vgpuSchedData',        c_nvmlVgpuSchedData_t),
+    ]
+
+class c_nvmlVgpuSchedulerLogEntry_t(_PrintableStructure):
+    _fields_ = [
+        ('timestamp',                   c_ulonglong),
+        ('timeRunTotal',                c_ulonglong),
+        ('timeRun',                     c_ulonglong),
+        ('swRunlistId',                 c_uint),
+        ('targetTimeSlice',             c_ulonglong),
+        ('cumulativePreemptionTime',    c_ulonglong),
+    ]
+
+class c_nvmlVgpuSchedulerLog_t(_PrintableStructure):
+    _fields_ = [
+        ('engineId',        c_uint),
+        ('schedulerPolicy', c_uint),
+        ('arrMode',         c_uint),
+        ('schedulerParams', c_nvmlVgpuSchedulerParams_t),
+        ('entriesCount',    c_uint),
+        ('logEntries',      c_nvmlVgpuSchedulerLogEntry_t * MXSMLEX_SCHEDULER_SW_MAX_LOG_ENTRIES),
+    ]
+
+class c_nvmlVgpuSchedulerGetState_t(_PrintableStructure):
+    _fields_ = [
+        ('schedulerPolicy', c_uint),
+        ('arrMode',         c_uint),
+        ('schedulerParams', c_nvmlVgpuSchedulerParams_t),
+    ]
+
+class c_nvmlVgpuSchedSetDataWithARR_t(_PrintableStructure):
+    _fields_ = [
+        ('avgFactor',   c_uint),
+        ('frequency',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedSetData_t(_PrintableStructure):
+    _fields_ = [
+        ('timeslice',   c_uint),
+    ]
+
+class c_nvmlVgpuSchedulerSetParams_t(Union):
+    _fields_ = [
+        ('vgpuSchedDataWithARR', c_nvmlVgpuSchedSetDataWithARR_t),
+        ('vgpuSchedData',        c_nvmlVgpuSchedSetData_t),
+    ]
+
+class c_nvmlVgpuSchedulerSetState_t(_PrintableStructure):
+    _fields_ = [
+        ('schedulerPolicy', c_uint),
+        ('enableARRMode',   c_uint),
+        ('schedulerParams', c_nvmlVgpuSchedulerSetParams_t),
+    ]
+
+class c_nvmlVgpuSchedulerCapabilities_t(_PrintableStructure):
+    _fields_ = [
+        ('supportedSchedulers', c_uint * MXSMLEX_SUPPORTED_VGPU_SCHEDULER_POLICY_COUNT),
+        ('maxTimeslice',        c_uint),
+        ('minTimeslice',        c_uint),
+        ('isArrModeSupported',  c_uint),
+        ('maxFrequencyForARR',  c_uint),
+        ('minFrequencyForARR',  c_uint),
+        ('maxAvgFactorForARR',  c_uint),
+        ('minAvgFactorForARR',  c_uint),
+    ]
+
+class c_nvmlFBCStats_t(Structure):
+    _fields_ = [("sessionsCount", c_uint),
+                ("averageFPS", c_uint),
+                ("averageLatency", c_uint)
+               ]
+
+class c_nvmlFBCSession_t(_PrintableStructure):
+    _fields_ = [
+        ('sessionId', c_uint),
+        ('pid', c_uint),
+        ('vgpuInstance', _nvmlVgpuInstance_t),
+        ('displayOrdinal', c_uint),
+        ('sessionType', c_uint),
+        ('sessionFlags', c_uint),
+        ('hMaxResolution', c_uint),
+        ('vMaxResolution', c_uint),
+        ('hResolution', c_uint),
+        ('vResolution', c_uint),
+        ('averageFPS', c_uint),
+        ('averageLatency', c_uint),
+    ]
+
+MXSMLEX_DEVICE_MIG_DISABLE = 0x0
+MXSMLEX_DEVICE_MIG_ENABLE  = 0x1
+
+MXSMLEX_GPU_INSTANCE_PROFILE_1_SLICE      = 0x0
+MXSMLEX_GPU_INSTANCE_PROFILE_2_SLICE      = 0x1
+MXSMLEX_GPU_INSTANCE_PROFILE_3_SLICE      = 0x2
+MXSMLEX_GPU_INSTANCE_PROFILE_4_SLICE      = 0x3
+MXSMLEX_GPU_INSTANCE_PROFILE_7_SLICE      = 0x4
+MXSMLEX_GPU_INSTANCE_PROFILE_8_SLICE      = 0x5
+MXSMLEX_GPU_INSTANCE_PROFILE_6_SLICE      = 0x6
+MXSMLEX_GPU_INSTANCE_PROFILE_1_SLICE_REV1 = 0x7
+MXSMLEX_GPU_INSTANCE_PROFILE_2_SLICE_REV1 = 0x8
+MXSMLEX_GPU_INSTANCE_PROFILE_1_SLICE_REV2 = 0x9
+MXSMLEX_GPU_INSTANCE_PROFILE_COUNT        = 0xA
+
+class c_nvmlGpuInstancePlacement_t(Structure):
+    _fields_ = [("start", c_uint),
+                ("size", c_uint)
+               ]
+
+class c_nvmlGpuInstanceProfileInfo_t(Structure):
+    _fields_ = [("id", c_uint),
+                ("isP2pSupported", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("copyEngineCount", c_uint),
+                ("decoderCount", c_uint),
+                ("encoderCount", c_uint),
+                ("jpegCount", c_uint),
+                ("ofaCount", c_uint),
+                ("memorySizeMB", c_ulonglong),
+               ]
+
+nvmlGpuInstanceProfileInfo_v2 = 0x02000098
+
+class c_nvmlGpuInstanceProfileInfo_v2_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("id", c_uint),
+                ("isP2pSupported", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("copyEngineCount", c_uint),
+                ("decoderCount", c_uint),
+                ("encoderCount", c_uint),
+                ("jpegCount", c_uint),
+                ("ofaCount", c_uint),
+                ("memorySizeMB", c_ulonglong),
+                ("name", c_char * MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE)
+               ]
+    
+    def __init__(self):
+        super(c_nvmlGpuInstanceProfileInfo_v2_t, self).__init__(version=nvmlGpuInstanceProfileInfo_v2)
+
+class c_nvmlGpuInstanceInfo_t(Structure):
+    _fields_ = [("device", c_nvmlDevice_t),
+                ("id", c_uint),
+                ("profileId", c_uint),
+                ("placement", c_nvmlGpuInstancePlacement_t)
+               ]
+
+class struct_c_nvmlGpuInstance_t(Structure):
+    pass # opaque handle
+c_nvmlGpuInstance_t = POINTER(struct_c_nvmlGpuInstance_t)
+
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_1_SLICE      = 0x0
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_2_SLICE      = 0x1
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_3_SLICE      = 0x2
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_4_SLICE      = 0x3
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_7_SLICE      = 0x4
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_8_SLICE      = 0x5
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_6_SLICE      = 0x6
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_1_SLICE_REV1 = 0x7
+MXSMLEX_COMPUTE_INSTANCE_PROFILE_COUNT        = 0x8
+
+MXSMLEX_COMPUTE_INSTANCE_ENGINE_PROFILE_SHARED = 0x0
+MXSMLEX_COMPUTE_INSTANCE_ENGINE_PROFILE_COUNT = 0x1
+
+class c_nvmlComputeInstancePlacement_t(Structure):
+    _fields_ = [("start", c_uint),
+                ("size", c_uint)
+               ]
+
+class c_nvmlComputeInstanceProfileInfo_t(Structure):
+    _fields_ = [("id", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("sharedCopyEngineCount", c_uint),
+                ("sharedDecoderCount", c_uint),
+                ("sharedEncoderCount", c_uint),
+                ("sharedJpegCount", c_uint),
+                ("sharedOfaCount", c_uint)
+               ]
+
+nvmlComputeInstanceProfileInfo_v2 = 0x02000088
+
+class c_nvmlComputeInstanceProfileInfo_v2_t(_PrintableStructure):
+    _fields_ = [("version", c_uint),
+                ("id", c_uint),
+                ("sliceCount", c_uint),
+                ("instanceCount", c_uint),
+                ("multiprocessorCount", c_uint),
+                ("sharedCopyEngineCount", c_uint),
+                ("sharedDecoderCount", c_uint),
+                ("sharedEncoderCount", c_uint),
+                ("sharedJpegCount", c_uint),
+                ("sharedOfaCount", c_uint),
+                ("name", c_char * MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE)
+               ]
+
+    def __init__(self):
+        super(c_nvmlComputeInstanceProfileInfo_v2_t, self).__init__(version=nvmlComputeInstanceProfileInfo_v2)
+
+class c_nvmlComputeInstanceInfo_t(Structure):
+    _fields_ = [("device", c_nvmlDevice_t),
+                ("gpuInstance", c_nvmlGpuInstance_t),
+                ("id", c_uint),
+                ("profileId", c_uint),
+                ("placement", c_nvmlComputeInstancePlacement_t)
+               ]
+
+MXSMLEX_MAX_GPU_UTILIZATIONS = 8
+MXSMLEX_GPU_UTILIZATION_DOMAIN_GPU    = 0
+MXSMLEX_GPU_UTILIZATION_DOMAIN_FB     = 1
+MXSMLEX_GPU_UTILIZATION_DOMAIN_VID    = 2
+MXSMLEX_GPU_UTILIZATION_DOMAIN_BUS    = 3
+class c_nvmlGpuDynamicPstatesUtilization_t(Structure):
+    _fields_ = [("bIsPresent", c_uint, 1),
+                ("percentage", c_uint),
+                ("incThreshold", c_uint),
+                ("decThreshold", c_uint)]
+class c_nvmlGpuDynamicPstatesInfo_t(Structure):
+    _fields_ = [("flags", c_uint),
+                ("utilization", c_nvmlGpuDynamicPstatesUtilization_t * MXSMLEX_MAX_GPU_UTILIZATIONS)]
+
+MXSMLEX_MAX_THERMAL_SENSORS_PER_GPU = 3
+
+MXSMLEX_THERMAL_TARGET_NONE          = 0
+MXSMLEX_THERMAL_TARGET_GPU           = 1
+MXSMLEX_THERMAL_TARGET_MEMORY        = 2
+MXSMLEX_THERMAL_TARGET_POWER_SUPPLY  = 4
+MXSMLEX_THERMAL_TARGET_BOARD         = 8
+MXSMLEX_THERMAL_TARGET_VCD_BOARD     = 9
+MXSMLEX_THERMAL_TARGET_VCD_INLET     = 10
+MXSMLEX_THERMAL_TARGET_VCD_OUTLET    = 11
+MXSMLEX_THERMAL_TARGET_ALL           = 15
+MXSMLEX_THERMAL_TARGET_UNKNOWN       = -1
+
+MXSMLEX_THERMAL_CONTROLLER_NONE            = 0
+MXSMLEX_THERMAL_CONTROLLER_GPU_INTERNAL    = 1
+MXSMLEX_THERMAL_CONTROLLER_ADM1032         = 2
+MXSMLEX_THERMAL_CONTROLLER_ADT7461         = 3
+MXSMLEX_THERMAL_CONTROLLER_MAX6649         = 4
+MXSMLEX_THERMAL_CONTROLLER_MAX1617         = 5
+MXSMLEX_THERMAL_CONTROLLER_LM99            = 6
+MXSMLEX_THERMAL_CONTROLLER_LM89            = 7
+MXSMLEX_THERMAL_CONTROLLER_LM64            = 8
+MXSMLEX_THERMAL_CONTROLLER_G781            = 9
+MXSMLEX_THERMAL_CONTROLLER_ADT7473         = 10
+MXSMLEX_THERMAL_CONTROLLER_SBMAX6649       = 11
+MXSMLEX_THERMAL_CONTROLLER_VBIOSEVT        = 12
+MXSMLEX_THERMAL_CONTROLLER_OS              = 13
+MXSMLEX_THERMAL_CONTROLLER_NVSYSCON_CANOAS = 14
+MXSMLEX_THERMAL_CONTROLLER_NVSYSCON_E551   = 15
+MXSMLEX_THERMAL_CONTROLLER_MAX6649R        = 16
+MXSMLEX_THERMAL_CONTROLLER_ADT7473S        = 17
+MXSMLEX_THERMAL_CONTROLLER_UNKNOWN         = -1
+
+class c_nvmlGpuThermalSensor_t(Structure):
+    _fields_ = [("controller", c_int),
+                ("defaultMinTemp", c_int),
+                ("defaultMaxTemp", c_int),
+                ("currentTemp", c_int),
+                ("target", c_int)]
+class c_nvmlGpuThermalSettings_t(Structure):
+    _fields_ = [("count", c_uint),
+                ("sensor", c_nvmlGpuThermalSensor_t * MXSMLEX_MAX_THERMAL_SENSORS_PER_GPU)]
+
+class struct_c_nvmlComputeInstance_t(Structure):
+    pass # opaque handle
+c_nvmlComputeInstance_t = POINTER(struct_c_nvmlComputeInstance_t)
+
+class c_nvmlDeviceAttributes(Structure):
+    _fields_ = [("multiprocessorCount", c_uint),
+                ("sharedCopyEngineCount", c_uint),
+                ("sharedDecoderCount", c_uint),
+                ("sharedEncoderCount", c_uint),
+                ("sharedJpegCount", c_uint),
+                ("sharedOfaCount", c_uint),
+                ("gpuInstanceSliceCount", c_uint),
+                ("computeInstanceSliceCount", c_uint),
+                ("memorySizeMB", c_ulonglong),
+               ]
+
+class c_nvmlRowRemapperHistogramValues(Structure):
+    _fields_ = [("max", c_uint),
+                ("high", c_uint),
+                ("partial", c_uint),
+                ("low", c_uint),
+                ("none", c_uint)
+               ]
+
+MXSMLEX_GPU_CERT_CHAIN_SIZE                = 0x1000
+MXSMLEX_GPU_ATTESTATION_CERT_CHAIN_SIZE    = 0x1400
+MXSMLEX_CC_GPU_CEC_NONCE_SIZE              = 0x20
+MXSMLEX_CC_GPU_ATTESTATION_REPORT_SIZE     = 0x2000
+MXSMLEX_CC_GPU_CEC_ATTESTATION_REPORT_SIZE = 0x1000
+MXSMLEX_CC_CEC_ATTESTATION_REPORT_NOT_PRESENT = 0
+MXSMLEX_CC_CEC_ATTESTATION_REPORT_PRESENT     = 1
+
+class c_nvmlConfComputeSystemState_t(Structure):
+    _fields_ = [('environment', c_uint),
+                ('ccFeature', c_uint),
+                ('devToolsMode', c_uint),
+               ]
+
+nvmlSystemConfComputeSettings_v1 = 0x1000014
+
+class c_nvmlSystemConfComputeSettings_v1_t(Structure):
+    _fields_ = [('version', c_uint),
+                ('environment', c_uint),
+                ('ccFeature', c_uint),
+                ('devToolsMode', c_uint),
+                ('multiGpuMode', c_uint),
+               ]
+    def __init__(self):
+        super(c_nvmlSystemConfComputeSettings_v1_t, self).__init__(version=nvmlSystemConfComputeSettings_v1)
+
+class c_nvmlConfComputeSystemCaps_t(Structure):
+    _fields_ = [('cpuCaps', c_uint),
+                ('gpusCaps', c_uint),
+               ]
+
+class c_nvmlConfComputeMemSizeInfo_t(Structure):
+    _fields_ = [('protectedMemSizeKib', c_ulonglong),
+                ('unprotectedMemSizeKib', c_ulonglong),
+               ]
+
+class c_nvmlConfComputeGpuCertificate_t(Structure):
+    _fields_ = [('certChainSize', c_uint),
+                ('attestationCertChainSize', c_uint),
+                ('certChain', c_uint8 * MXSMLEX_GPU_CERT_CHAIN_SIZE),
+                ('attestationCertChain', c_uint8 * MXSMLEX_GPU_ATTESTATION_CERT_CHAIN_SIZE),
+               ]
+
+class c_nvmlConfComputeGpuAttestationReport_t(Structure):
+    _fields_ = [('isCecAttestationReportPresent', c_uint),
+                ('attestationReportSize', c_uint),
+                ('cecAttestationReportSize', c_uint),
+                ('nonce', c_uint8 * MXSMLEX_CC_GPU_CEC_NONCE_SIZE),
+                ('attestationReport', c_uint8 * MXSMLEX_CC_GPU_ATTESTATION_REPORT_SIZE),
+                ('cecAttestationReport', c_uint8 * MXSMLEX_CC_GPU_CEC_ATTESTATION_REPORT_SIZE),
+               ]
+
+class c_nvmlConfComputeSetKeyRotationThresholdInfo_t(Structure):
+    _fields_ = [('version', c_uint),
+                ('maxAttackerAdvantage', c_ulong),
+               ]
+ConfComputeSetKeyRotationThresholdInfo_v1 = 0x1000010
+
+class c_nvmlConfComputeGetKeyRotationThresholdInfo_t(Structure):
+    _fields_ = [('version', c_uint),
+                ('attackerAdvantage', c_ulong),
+               ]
+ConfComputeGetKeyRotationThresholdInfo_v1 = 0x1000010
+
+
+## string/bytes conversion for ease of use
+def convertStrBytes(func):
+    '''
+    In python 3, strings are unicode instead of bytes, and need to be converted for ctypes
+    Args from caller: (1, 'string', <__main__.c_nvmlDevice_t at 0xFFFFFFFF>)
+    Args passed to function: (1, b'string', <__main__.c_nvmlDevice_t at 0xFFFFFFFF)>
+    ----
+    Returned from function: b'returned string'
+    Returned to caller: 'returned string'
+    '''
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        # encoding a str returns bytes in python 2 and 3
+        args = [arg.encode() if isinstance(arg, str) else arg for arg in args]
+        res = func(*args, **kwargs)
+        # In python 2, str and bytes are the same
+        # In python 3, str is unicode and should be decoded.
+        # Ctypes handles most conversions, this only effects c_char and char arrays.
+        if isinstance(res, bytes):
+            if isinstance(res, str):
+                return res
+            return res.decode()
+        return res
+
+    if sys.version_info >= (3,):
+        return wrapper
+    return func
+
+def throwOnVersionMismatch(func):
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        try:
+            return func(*args, **kwargs)
+        except NVMLError_FunctionNotFound:
+            raise MXSMLEXLibraryMismatchError("Unversioned function called and the "
+                                           "pyMXSMLEX version does not match the MXSMLEX lib version. "
+                                           "Either use matching pyMXSMLEX and MXSMLEX lib versions or "
+                                           "use a versioned function such as " + func.__name__ + "_v2")
+    return wrapper
+
+## C function wrappers ##
+def nvmlInitWithFlags(flags):
+    _LoadNvmlLibrary()
+
+    #
+    # Initialize the library
+    #
+    fn = _nvmlGetFunctionPointer("mxSmlExInit")
+    ret = fn(flags)
+    _nvmlCheckReturn(ret)
+
+    # Atomically update refcount
+    global _nvmlLib_refcount
+    libLoadLock.acquire()
+    _nvmlLib_refcount += 1
+    libLoadLock.release()
+    return None
+
+def nvmlInit():
+    nvmlInitWithFlags(0)
+    return None
+
+def _LoadNvmlLibrary():
+    '''
+    Load the library if it isn't loaded already
+    '''
+    global nvmlLib
+
+    if (nvmlLib == None):
+        # lock to ensure only one caller loads the library
+        libLoadLock.acquire()
+
+        try:
+            # ensure the library still isn't loaded
+            if (nvmlLib == None):
+                try:
+                    if (sys.platform[:3] == "win"):
+                        # cdecl calling convention
+                        try:
+                            # Check for nvml.dll in System32 first for DCH drivers
+                            nvmlLib = CDLL(os.path.join(os.getenv("WINDIR", "C:/Windows"), "System32/nvml.dll"))
+                        except OSError as ose:
+                            # If nvml.dll is not found in System32, it should be in ProgramFiles
+                            # load nvml.dll from %ProgramFiles%/NVIDIA Corporation/NVSMI/nvml.dll
+                            nvmlLib = CDLL(os.path.join(os.getenv("ProgramFiles", "C:/Program Files"), "NVIDIA Corporation/NVSMI/nvml.dll"))
+                    else:
+                        # assume linux
+                        nvmlLib = CDLL("libmxsml.so")
+                except OSError as ose:
+                    _nvmlCheckReturn(MXSMLEX_ERROR_LIBRARY_NOT_FOUND)
+                if (nvmlLib == None):
+                    _nvmlCheckReturn(MXSMLEX_ERROR_LIBRARY_NOT_FOUND)
+        finally:
+            # lock is always freed
+            libLoadLock.release()
+
+def nvmlShutdown():
+    #
+    # Leave the library loaded, but shutdown the interface
+    #
+    fn = _nvmlGetFunctionPointer("mxSmlExShutdown")
+    ret = fn()
+    _nvmlCheckReturn(ret)
+
+    # Atomically update refcount
+    global _nvmlLib_refcount
+    libLoadLock.acquire()
+    if (0 < _nvmlLib_refcount):
+        _nvmlLib_refcount -= 1
+    libLoadLock.release()
+    return None
+
+# Added in 2.285
+@convertStrBytes
+def nvmlErrorString(result):
+    fn = _nvmlGetFunctionPointer("mxSmlExErrorString")
+    fn.restype = c_char_p # otherwise return is an int
+    ret = fn(result)
+    return ret
+
+# Added in 2.285
+@convertStrBytes
+def nvmlSystemGetMXSMLEXVersion():
+    c_version = create_string_buffer(MXSMLEX_SYSTEM_MXSMLEX_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetMXSMLEXVersion")
+    ret = fn(c_version, c_uint(MXSMLEX_SYSTEM_MXSMLEX_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+def nvmlSystemGetCudaDriverVersion():
+    c_cuda_version = c_int()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetCudaDriverVersion")
+    ret = fn(byref(c_cuda_version))
+    _nvmlCheckReturn(ret)
+    return c_cuda_version.value
+
+def nvmlSystemGetCudaDriverVersion_v2():
+    c_cuda_version = c_int()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetCudaDriverVersion_v2")
+    ret = fn(byref(c_cuda_version))
+    _nvmlCheckReturn(ret)
+    return c_cuda_version.value
+
+# Added in 2.285
+@convertStrBytes
+def nvmlSystemGetProcessName(pid):
+    c_name = create_string_buffer(1024)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetProcessName")
+    ret = fn(c_uint(pid), c_name, c_uint(1024))
+    _nvmlCheckReturn(ret)
+    return c_name.value
+
+@convertStrBytes
+def nvmlSystemGetDriverVersion():
+    c_version = create_string_buffer(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetDriverVersion")
+    ret = fn(c_version, c_uint(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 2.285
+def nvmlSystemGetHicVersion():
+    c_count = c_uint(0)
+    hics = None
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetHicVersion")
+
+    # get the count
+    ret = fn(byref(c_count), None)
+
+    # this should only fail with insufficient size
+    if ((ret != MXSMLEX_SUCCESS) and
+        (ret != MXSMLEX_ERROR_INSUFFICIENT_SIZE)):
+        raise NVMLError(ret)
+
+    # If there are no hics
+    if (c_count.value == 0):
+        return []
+
+    hic_array = c_nvmlHwbcEntry_t * c_count.value
+    hics = hic_array()
+    ret = fn(byref(c_count), hics)
+    _nvmlCheckReturn(ret)
+    return hics
+
+## Unit get functions
+def nvmlUnitGetCount():
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetCount")
+    ret = fn(byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlUnitGetHandleByIndex(index):
+    c_index = c_uint(index)
+    unit = c_nvmlUnit_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetHandleByIndex")
+    ret = fn(c_index, byref(unit))
+    _nvmlCheckReturn(ret)
+    return unit
+
+def nvmlUnitGetUnitInfo(unit):
+    c_info = c_nvmlUnitInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetUnitInfo")
+    ret = fn(unit, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlUnitGetLedState(unit):
+    c_state =  c_nvmlLedState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetLedState")
+    ret = fn(unit, byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state
+
+def nvmlUnitGetPsuInfo(unit):
+    c_info = c_nvmlPSUInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetPsuInfo")
+    ret = fn(unit, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlUnitGetTemperature(unit, type):
+    c_temp = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetTemperature")
+    ret = fn(unit, c_uint(type), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return c_temp.value
+
+def nvmlUnitGetFanSpeedInfo(unit):
+    c_speeds = c_nvmlUnitFanSpeeds_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetFanSpeedInfo")
+    ret = fn(unit, byref(c_speeds))
+    _nvmlCheckReturn(ret)
+    return c_speeds
+
+# added to API
+def nvmlUnitGetDeviceCount(unit):
+    c_count = c_uint(0)
+    # query the unit to determine device count
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetDevices")
+    ret = fn(unit, byref(c_count), None)
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = MXSMLEX_SUCCESS
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlUnitGetDevices(unit):
+    c_count = c_uint(nvmlUnitGetDeviceCount(unit))
+    device_array = c_nvmlDevice_t * c_count.value
+    c_devices = device_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitGetDevices")
+    ret = fn(unit, byref(c_count), c_devices)
+    _nvmlCheckReturn(ret)
+    return c_devices
+
+## Device get functions
+def nvmlDeviceGetCount():
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCount")
+    ret = fn(byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetHandleByIndex(index):
+    c_index = c_uint(index)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetDeviceHandleByIndex")
+    ret = fn(c_index, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetHandleBySerial(serial):
+    c_serial = c_char_p(serial)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHandleBySerial")
+    ret = fn(c_serial, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetHandleByUUID(uuid):
+    c_uuid = c_char_p(uuid)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHandleByUUID")
+    ret = fn(c_uuid, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetHandleByPciBusId(pciBusId):
+    c_busId = c_char_p(pciBusId)
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHandleByPciBusId_v2")
+    ret = fn(c_busId, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+@convertStrBytes
+def nvmlDeviceGetName(handle):
+    c_name = create_string_buffer(MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetName")
+    ret = fn(handle, c_name, c_uint(MXSMLEX_DEVICE_NAME_V2_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_name.value
+
+def nvmlDeviceGetBoardId(handle):
+    c_id = c_uint();
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBoardId")
+    ret = fn(handle, byref(c_id))
+    _nvmlCheckReturn(ret)
+    return c_id.value
+
+def nvmlDeviceGetMultiGpuBoard(handle):
+    c_multiGpu = c_uint();
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMultiGpuBoard")
+    ret = fn(handle, byref(c_multiGpu))
+    _nvmlCheckReturn(ret)
+    return c_multiGpu.value
+
+def nvmlDeviceGetBrand(handle):
+    c_type = _nvmlBrandType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBrand")
+    ret = fn(handle, byref(c_type))
+    _nvmlCheckReturn(ret)
+    return c_type.value
+
+def nvmlDeviceGetC2cModeInfoV1(handle):
+    c_info = c_nvmlC2cModeInfo_v1_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetC2cModeInfoV")
+    ret = fn(handle, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlDeviceGetC2cModeInfoV(handle):
+    return nvmlDeviceGetC2cModeInfoV1(handle)
+
+@convertStrBytes
+def nvmlDeviceGetBoardPartNumber(handle):
+    c_part_number = create_string_buffer(MXSMLEX_DEVICE_PART_NUMBER_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBoardPartNumber")
+    ret = fn(handle, c_part_number, c_uint(MXSMLEX_DEVICE_PART_NUMBER_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_part_number.value
+
+@convertStrBytes
+def nvmlDeviceGetSerial(handle):
+    c_serial = create_string_buffer(MXSMLEX_DEVICE_SERIAL_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSerial")
+    ret = fn(handle, c_serial, c_uint(MXSMLEX_DEVICE_SERIAL_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_serial.value
+
+def nvmlDeviceGetModuleId(handle, moduleId):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetModuleId")
+    ret = fn(handle, moduleId)
+    return ret
+
+def nvmlDeviceGetMemoryAffinity(handle, nodeSetSize, scope):
+    affinity_array = c_ulonglong * nodeSetSize
+    c_affinity = affinity_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryAffinity")
+    ret = fn(handle, nodeSetSize, byref(c_affinity), _nvmlAffinityScope_t(scope))
+    _nvmlCheckReturn(ret)
+    return c_affinity
+
+def nvmlDeviceGetCpuAffinityWithinScope(handle, cpuSetSize, scope):
+    affinity_array = c_ulonglong * cpuSetSize
+    c_affinity = affinity_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCpuAffinityWithinScope")
+    ret = fn(handle, cpuSetSize, byref(c_affinity), _nvmlAffinityScope_t(scope))
+    _nvmlCheckReturn(ret)
+    return c_affinity
+
+def nvmlDeviceGetCpuAffinity(handle, cpuSetSize):
+    affinity_array = c_ulonglong * cpuSetSize
+    c_affinity = affinity_array()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCpuAffinity")
+    ret = fn(handle, cpuSetSize, byref(c_affinity))
+    _nvmlCheckReturn(ret)
+    return c_affinity
+
+def nvmlDeviceSetCpuAffinity(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetCpuAffinity")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceClearCpuAffinity(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearCpuAffinity")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetNumaNodeId(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNumaNodeId")
+    node = c_int()
+    ret = fn(handle, byref(node))
+    _nvmlCheckReturn(ret)
+    return node.value
+
+def nvmlDeviceGetMinorNumber(handle):
+    c_minor_number = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMinorNumber")
+    ret = fn(handle, byref(c_minor_number))
+    _nvmlCheckReturn(ret)
+    return c_minor_number.value
+
+@convertStrBytes
+def nvmlDeviceGetUUID(handle):
+    c_uuid = create_string_buffer(MXSMLEX_DEVICE_UUID_V2_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetUUID")
+    ret = fn(handle, c_uuid, c_uint(MXSMLEX_DEVICE_UUID_V2_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_uuid.value
+
+@convertStrBytes
+def nvmlDeviceGetInforomVersion(handle, infoRomObject):
+    c_version = create_string_buffer(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetInforomVersion")
+    ret = fn(handle, _nvmlInforomObject_t(infoRomObject),
+                 c_version, c_uint(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 4.304
+@convertStrBytes
+def nvmlDeviceGetInforomImageVersion(handle):
+    c_version = create_string_buffer(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetInforomImageVersion")
+    ret = fn(handle, c_version, c_uint(MXSMLEX_DEVICE_INFOROM_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 4.304
+def nvmlDeviceGetInforomConfigurationChecksum(handle):
+    c_checksum = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetInforomConfigurationChecksum")
+    ret = fn(handle, byref(c_checksum))
+    _nvmlCheckReturn(ret)
+    return c_checksum.value
+
+# Added in 4.304
+def nvmlDeviceValidateInforom(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceValidateInforom")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetLastBBXFlushTime(handle):
+    c_timestamp = c_ulonglong()
+    c_durationUs = c_ulong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetLastBBXFlushTime")
+    ret = fn(handle, byref(c_timestamp), byref(c_durationUs))
+    _nvmlCheckReturn(ret)
+    return [c_timestamp.value, c_durationUs.value]
+
+def nvmlDeviceGetDisplayMode(handle):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDisplayMode")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlDeviceGetDisplayActive(handle):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDisplayActive")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+
+def nvmlDeviceGetPersistenceMode(handle):
+    c_state = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPersistenceMode")
+    ret = fn(handle, byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state.value
+
+def nvmlDeviceGetPciInfoExt(handle, c_info):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPciInfoExt")
+    ret = fn(handle, c_info)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetPciInfo_v3(handle):
+    c_info = nvmlPciInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPciInfo_v3")
+    ret = fn(handle, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlDeviceGetPciInfo(handle):
+    return nvmlDeviceGetPciInfo_v3(handle)
+
+def nvmlDeviceGetClockInfo(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetClockInfo")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 2.285
+def nvmlDeviceGetMaxClockInfo(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxClockInfo")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 4.304
+def nvmlDeviceGetApplicationsClock(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetApplicationsClock")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+def nvmlDeviceGetMaxCustomerBoostClock(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxCustomerBoostClock")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+def nvmlDeviceGetClock(handle, type, id):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetClock")
+    ret = fn(handle, _nvmlClockType_t(type), _nvmlClockId_t(id), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 5.319
+def nvmlDeviceGetDefaultApplicationsClock(handle, type):
+    c_clock = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDefaultApplicationsClock")
+    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))
+    _nvmlCheckReturn(ret)
+    return c_clock.value
+
+# Added in 4.304
+def nvmlDeviceGetSupportedMemoryClocks(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedMemoryClocks")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no clocks
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        clocks_array = c_uint * c_count.value
+        c_clocks = clocks_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_clocks)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            procs.append(c_clocks[i])
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+# Added in 4.304
+def nvmlDeviceGetSupportedGraphicsClocks(handle, memoryClockMHz):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedGraphicsClocks")
+    ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no clocks
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        clocks_array = c_uint * c_count.value
+        c_clocks = clocks_array()
+
+        # make the call again
+        ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), c_clocks)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            procs.append(c_clocks[i])
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetFanSpeed(handle):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFanSpeed")
+    ret = fn(handle, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetFanSpeed_v2(handle, fan):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFanSpeed_v2")
+    ret = fn(handle, fan, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetTargetFanSpeed(handle, fan):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTargetFanSpeed")
+    ret = fn(handle, fan, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetNumFans(device):
+    c_numFans = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNumFans")
+    ret = fn(device, byref(c_numFans))
+    _nvmlCheckReturn(ret)
+    return c_numFans.value
+
+def nvmlDeviceSetDefaultFanSpeed_v2(handle, index):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetDefaultFanSpeed_v2");
+    ret = fn(handle, index)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMinMaxFanSpeed(handle, minSpeed, maxSpeed):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMinMaxFanSpeed")
+    ret = fn(handle, minSpeed, maxSpeed)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetFanControlPolicy_v2(handle, fan, fanControlPolicy):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFanControlPolicy_v2")
+    ret = fn(handle, fan, fanControlPolicy)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceSetFanControlPolicy(handle, fan, fanControlPolicy):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetFanControlPolicy")
+    ret = fn(handle, fan, _nvmlFanControlPolicy_t(fanControlPolicy))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetTemperature(handle, sensor):
+    c_temp = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTemperature")
+    ret = fn(handle, _nvmlTemperatureSensors_t(sensor), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return c_temp.value
+
+def nvmlDeviceGetTemperatureThreshold(handle, threshold):
+    c_temp = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTemperatureThreshold")
+    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return c_temp.value
+
+def nvmlDeviceSetTemperatureThreshold(handle, threshold, temp):
+    c_temp = c_uint()
+    c_temp.value = temp
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetTemperatureThreshold")
+    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))
+    _nvmlCheckReturn(ret)
+    return None
+
+# DEPRECATED use nvmlDeviceGetPerformanceState
+def nvmlDeviceGetPowerState(handle):
+    c_pstate = _nvmlPstates_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerState")
+    ret = fn(handle, byref(c_pstate))
+    _nvmlCheckReturn(ret)
+    return c_pstate.value
+
+def nvmlDeviceGetPerformanceState(handle):
+    c_pstate = _nvmlPstates_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPerformanceState")
+    ret = fn(handle, byref(c_pstate))
+    _nvmlCheckReturn(ret)
+    return c_pstate.value
+
+def nvmlDeviceGetPowerManagementMode(handle):
+    c_pcapMode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementMode")
+    ret = fn(handle, byref(c_pcapMode))
+    _nvmlCheckReturn(ret)
+    return c_pcapMode.value
+
+def nvmlDeviceGetPowerManagementLimit(handle):
+    c_limit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementLimit")
+    ret = fn(handle, byref(c_limit))
+    _nvmlCheckReturn(ret)
+    return c_limit.value
+
+# Added in 4.304
+def nvmlDeviceGetPowerManagementLimitConstraints(handle):
+    c_minLimit = c_uint()
+    c_maxLimit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementLimitConstraints")
+    ret = fn(handle, byref(c_minLimit), byref(c_maxLimit))
+    _nvmlCheckReturn(ret)
+    return [c_minLimit.value, c_maxLimit.value]
+
+# Added in 4.304
+def nvmlDeviceGetPowerManagementDefaultLimit(handle):
+    c_limit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerManagementDefaultLimit")
+    ret = fn(handle, byref(c_limit))
+    _nvmlCheckReturn(ret)
+    return c_limit.value
+
+
+# Added in 331
+def nvmlDeviceGetEnforcedPowerLimit(handle):
+    c_limit = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEnforcedPowerLimit")
+    ret = fn(handle, byref(c_limit))
+    _nvmlCheckReturn(ret)
+    return c_limit.value
+
+def nvmlDeviceGetPowerUsage(handle):
+    c_watts = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerUsage")
+    ret = fn(handle, byref(c_watts))
+    _nvmlCheckReturn(ret)
+    return c_watts.value
+
+def nvmlDeviceGetTotalEnergyConsumption(handle):
+    c_millijoules = c_uint64()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTotalEnergyConsumption")
+    ret = fn(handle, byref(c_millijoules))
+    _nvmlCheckReturn(ret)
+    return c_millijoules.value
+
+# Added in 4.304
+def nvmlDeviceGetGpuOperationMode(handle):
+    c_currState = _nvmlGpuOperationMode_t()
+    c_pendingState = _nvmlGpuOperationMode_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuOperationMode")
+    ret = fn(handle, byref(c_currState), byref(c_pendingState))
+    _nvmlCheckReturn(ret)
+    return [c_currState.value, c_pendingState.value]
+
+# Added in 4.304
+def nvmlDeviceGetCurrentGpuOperationMode(handle):
+    return nvmlDeviceGetGpuOperationMode(handle)[0]
+
+# Added in 4.304
+def nvmlDeviceGetPendingGpuOperationMode(handle):
+    return nvmlDeviceGetGpuOperationMode(handle)[1]
+
+def nvmlDeviceGetMemoryInfo(handle, version=None):
+    if not version:
+        c_memory = c_nvmlMemory_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryInfo")
+    else:
+        c_memory = c_nvmlMemory_v2_t()
+        c_memory.version = version
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryInfo_v2")
+    ret = fn(handle, byref(c_memory))
+    _nvmlCheckReturn(ret)
+    return c_memory
+
+def nvmlDeviceGetBAR1MemoryInfo(handle):
+    c_bar1_memory = c_nvmlBAR1Memory_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBAR1MemoryInfo")
+    ret = fn(handle, byref(c_bar1_memory))
+    _nvmlCheckReturn(ret)
+    return c_bar1_memory
+
+def nvmlDeviceGetComputeMode(handle):
+    c_mode = _nvmlComputeMode_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeMode")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlDeviceGetCudaComputeCapability(handle):
+    c_major = c_int()
+    c_minor = c_int()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeCapability")
+    ret = fn(handle, byref(c_major), byref(c_minor))
+    _nvmlCheckReturn(ret)
+    return (c_major.value, c_minor.value)
+
+def nvmlDeviceGetEccMode(handle):
+    c_currState = _nvmlEnableState_t()
+    c_pendingState = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEccMode")
+    ret = fn(handle, byref(c_currState), byref(c_pendingState))
+    _nvmlCheckReturn(ret)
+    return [c_currState.value, c_pendingState.value]
+
+# added to API
+def nvmlDeviceGetCurrentEccMode(handle):
+    return nvmlDeviceGetEccMode(handle)[0]
+
+# added to API
+def nvmlDeviceGetPendingEccMode(handle):
+    return nvmlDeviceGetEccMode(handle)[1]
+
+def nvmlDeviceGetDefaultEccMode(handle):
+    c_defaultState = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDefaultEccMode")
+    ret = fn(handle, byref(c_defaultState))
+    _nvmlCheckReturn(ret)
+    return [c_defaultState.value]
+
+def nvmlDeviceGetTotalEccErrors(handle, errorType, counterType):
+    c_count = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTotalEccErrors")
+    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),
+                 _nvmlEccCounterType_t(counterType), byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+# This is deprecated, instead use nvmlDeviceGetMemoryErrorCounter
+def nvmlDeviceGetDetailedEccErrors(handle, errorType, counterType):
+    c_counts = c_nvmlEccErrorCounts_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDetailedEccErrors")
+    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),
+                 _nvmlEccCounterType_t(counterType), byref(c_counts))
+    _nvmlCheckReturn(ret)
+    return c_counts
+
+# Added in 4.304
+def nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType, locationType):
+    c_count = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryErrorCounter")
+    ret = fn(handle,
+             _nvmlMemoryErrorType_t(errorType),
+             _nvmlEccCounterType_t(counterType),
+             _nvmlMemoryLocation_t(locationType),
+             byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetUtilizationRates(handle):
+    c_util = c_nvmlUtilization_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetUtilizationRates")
+    ret = fn(handle, byref(c_util))
+    _nvmlCheckReturn(ret)
+    return c_util
+
+def nvmlDeviceGetEncoderUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetDecoderUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDecoderUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetJpgUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetJpgUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetOfaUtilization(handle):
+    c_util = c_uint()
+    c_samplingPeriod = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetOfaUtilization")
+    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))
+    _nvmlCheckReturn(ret)
+    return [c_util.value, c_samplingPeriod.value]
+
+def nvmlDeviceGetPcieReplayCounter(handle):
+    c_replay = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieReplayCounter")
+    ret = fn(handle, byref(c_replay))
+    _nvmlCheckReturn(ret)
+    return c_replay.value
+
+def nvmlDeviceGetDriverModel(handle):
+    c_currModel = _nvmlDriverModel_t()
+    c_pendingModel = _nvmlDriverModel_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDriverModel")
+    ret = fn(handle, byref(c_currModel), byref(c_pendingModel))
+    _nvmlCheckReturn(ret)
+    return [c_currModel.value, c_pendingModel.value]
+
+# added to API
+def nvmlDeviceGetCurrentDriverModel(handle):
+    return nvmlDeviceGetDriverModel(handle)[0]
+
+# added to API
+def nvmlDeviceGetPendingDriverModel(handle):
+    return nvmlDeviceGetDriverModel(handle)[1]
+
+# Added in 2.285
+@convertStrBytes
+def nvmlDeviceGetVbiosVersion(handle):
+    c_version = create_string_buffer(MXSMLEX_DEVICE_VBIOS_VERSION_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVbiosVersion")
+    ret = fn(handle, c_version, c_uint(MXSMLEX_DEVICE_VBIOS_VERSION_BUFFER_SIZE))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+# Added in 2.285
+def nvmlDeviceGetComputeRunningProcesses_v2(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeRunningProcesses_v2")
+    ret = fn(handle, byref(c_count), None)
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v2_t * c_count.value
+        c_procs = proc_array()
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+# Added in 2.285
+def nvmlDeviceGetComputeRunningProcesses_v3(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeRunningProcesses_v3")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v3_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+@throwOnVersionMismatch
+def nvmlDeviceGetComputeRunningProcesses(handle):
+    return nvmlDeviceGetComputeRunningProcesses_v3(handle)
+
+def nvmlDeviceGetGraphicsRunningProcesses_v2(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGraphicsRunningProcesses_v2")
+    ret = fn(handle, byref(c_count), None)
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v2_t * c_count.value
+        c_procs = proc_array()
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetGraphicsRunningProcesses_v3(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGraphicsRunningProcesses_v3")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v3_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+@throwOnVersionMismatch
+def nvmlDeviceGetGraphicsRunningProcesses(handle):
+    return nvmlDeviceGetGraphicsRunningProcesses_v3(handle)
+
+@throwOnVersionMismatch
+def nvmlDeviceGetMPSComputeRunningProcesses(handle):
+    return nvmlDeviceGetMPSComputeRunningProcesses_v3(handle)
+
+def nvmlDeviceGetMPSComputeRunningProcesses_v2(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMPSComputeRunningProcesses_v2")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v2_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetMPSComputeRunningProcesses_v3(handle):
+    # first call to get the size
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMPSComputeRunningProcesses_v3")
+    ret = fn(handle, byref(c_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        # oversize the array incase more processes are created
+        c_count.value = c_count.value * 2 + 5
+        proc_array = c_nvmlProcessInfo_v3_t * c_count.value
+        c_procs = proc_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_count), c_procs)
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_count.value):
+            # use an alternative struct for this object
+            obj = nvmlStructToFriendlyObject(c_procs[i])
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                # special case for WDDM on Windows, see comment above
+                obj.usedGpuMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetRunningProcessDetailList(handle, version, mode):
+    c_processDetailList = c_nvmlProcessDetailList_t()
+    c_processDetailList.version = version
+    c_processDetailList.mode = mode
+
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRunningProcessDetailList")
+
+    # first call to get the size
+    ret = fn(handle, byref(c_processDetailList))
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no running processes
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        c_procs = c_nvmlProcessDetail_v1_t * c_processDetailList.numProcArrayEntries
+        c_processDetailList.procArray = cast((c_procs)(), POINTER(c_nvmlProcessDetail_v1_t))
+
+        # make the call again
+        ret = fn(handle, byref(c_processDetailList))
+        _nvmlCheckReturn(ret)
+
+        procs = []
+        for i in range(c_processDetailList.numProcArrayEntries):
+            # use an alternative struct for this object
+            obj = c_processDetailList.procArray[i]
+            if (obj.usedGpuMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                obj.usedGpuMemory = None
+            if (obj.usedGpuCcProtectedMemory == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+                obj.usedGpuCcProtectedMemory = None
+            procs.append(obj)
+
+        return procs
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetAutoBoostedClocksEnabled(handle):
+    c_isEnabled = _nvmlEnableState_t()
+    c_defaultIsEnabled = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAutoBoostedClocksEnabled")
+    ret = fn(handle, byref(c_isEnabled), byref(c_defaultIsEnabled))
+    _nvmlCheckReturn(ret)
+    return [c_isEnabled.value, c_defaultIsEnabled.value]
+    #Throws MXSMLEX_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks
+
+## Set functions
+def nvmlUnitSetLedState(unit, color):
+    fn = _nvmlGetFunctionPointer("mxSmlExUnitSetLedState")
+    ret = fn(unit, _nvmlLedColor_t(color))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetPersistenceMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPersistenceMode")
+    ret = fn(handle, _nvmlEnableState_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetComputeMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetComputeMode")
+    ret = fn(handle, _nvmlComputeMode_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetEccMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetEccMode")
+    ret = fn(handle, _nvmlEnableState_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceClearEccErrorCounts(handle, counterType):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearEccErrorCounts")
+    ret = fn(handle, _nvmlEccCounterType_t(counterType))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetDriverModel(handle, model):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetDriverModel")
+    ret = fn(handle, _nvmlDriverModel_t(model))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetAutoBoostedClocksEnabled(handle, enabled):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetAutoBoostedClocksEnabled")
+    ret = fn(handle, _nvmlEnableState_t(enabled))
+    _nvmlCheckReturn(ret)
+    return None
+    #Throws MXSMLEX_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks
+
+def nvmlDeviceSetDefaultAutoBoostedClocksEnabled(handle, enabled, flags):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetDefaultAutoBoostedClocksEnabled")
+    ret = fn(handle, _nvmlEnableState_t(enabled), c_uint(flags))
+    _nvmlCheckReturn(ret)
+    return None
+    #Throws MXSMLEX_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks
+
+def nvmlDeviceSetGpuLockedClocks(handle, minGpuClockMHz, maxGpuClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetGpuLockedClocks")
+    ret = fn(handle, c_uint(minGpuClockMHz), c_uint(maxGpuClockMHz))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceResetGpuLockedClocks(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetGpuLockedClocks")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetMemoryLockedClocks(handle, minMemClockMHz, maxMemClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetMemoryLockedClocks")
+    ret = fn(handle, c_uint(minMemClockMHz), c_uint(maxMemClockMHz))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceResetMemoryLockedClocks(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetMemoryLockedClocks")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetClkMonStatus(handle, c_clkMonInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetClkMonStatus")
+    ret = fn(handle, c_clkMonInfo)
+    return ret
+
+# Added in 4.304
+def nvmlDeviceSetApplicationsClocks(handle, maxMemClockMHz, maxGraphicsClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetApplicationsClocks")
+    ret = fn(handle, c_uint(maxMemClockMHz), c_uint(maxGraphicsClockMHz))
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 4.304
+def nvmlDeviceResetApplicationsClocks(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetApplicationsClocks")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 4.304
+def nvmlDeviceSetPowerManagementLimit(handle, limit):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPowerManagementLimit")
+    ret = fn(handle, c_uint(limit))
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 4.304
+def nvmlDeviceSetGpuOperationMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetGpuOperationMode")
+    ret = fn(handle, _nvmlGpuOperationMode_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 2.285
+def nvmlEventSetCreate():
+    fn = _nvmlGetFunctionPointer("mxSmlExEventSetCreate")
+    eventSet = c_nvmlEventSet_t()
+    ret = fn(byref(eventSet))
+    _nvmlCheckReturn(ret)
+    return eventSet
+
+# Added in 2.285
+def nvmlDeviceRegisterEvents(handle, eventTypes, eventSet):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceRegisterEvents")
+    ret = fn(handle, c_ulonglong(eventTypes), eventSet)
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 2.285
+def nvmlDeviceGetSupportedEventTypes(handle):
+    c_eventTypes = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedEventTypes")
+    ret = fn(handle, byref(c_eventTypes))
+    _nvmlCheckReturn(ret)
+    return c_eventTypes.value
+
+# raises MXSMLEX_ERROR_TIMEOUT exception on timeout
+def nvmlEventSetWait_v2(eventSet, timeoutms):
+    fn = _nvmlGetFunctionPointer("mxSmlExEventSetWait_v2")
+    data = c_nvmlEventData_t()
+    ret = fn(eventSet, byref(data), c_uint(timeoutms))
+    _nvmlCheckReturn(ret)
+    return data
+
+def nvmlEventSetWait(eventSet, timeoutms):
+    return nvmlEventSetWait_v2(eventSet, timeoutms)
+
+# Added in 2.285
+def nvmlEventSetFree(eventSet):
+    fn = _nvmlGetFunctionPointer("mxSmlExEventSetFree")
+    ret = fn(eventSet)
+    _nvmlCheckReturn(ret)
+    return None
+
+# Added in 3.295
+def nvmlDeviceOnSameBoard(handle1, handle2):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceOnSameBoard")
+    onSameBoard = c_int()
+    ret = fn(handle1, handle2, byref(onSameBoard))
+    _nvmlCheckReturn(ret)
+    return (onSameBoard.value != 0)
+
+# Added in 3.295
+def nvmlDeviceGetCurrPcieLinkGeneration(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrPcieLinkGeneration")
+    gen = c_uint()
+    ret = fn(handle, byref(gen))
+    _nvmlCheckReturn(ret)
+    return gen.value
+
+# Added in 3.295
+def nvmlDeviceGetMaxPcieLinkGeneration(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxPcieLinkGeneration")
+    gen = c_uint()
+    ret = fn(handle, byref(gen))
+    _nvmlCheckReturn(ret)
+    return gen.value
+
+# Added in 3.295
+def nvmlDeviceGetCurrPcieLinkWidth(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrPcieLinkWidth")
+    width = c_uint()
+    ret = fn(handle, byref(width))
+    _nvmlCheckReturn(ret)
+    return width.value
+
+# Added in 3.295
+def nvmlDeviceGetMaxPcieLinkWidth(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxPcieLinkWidth")
+    width = c_uint()
+    ret = fn(handle, byref(width))
+    _nvmlCheckReturn(ret)
+    return width.value
+
+def nvmlDeviceGetGpuMaxPcieLinkGeneration(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuMaxPcieLinkGeneration")
+    gen = c_uint()
+    ret = fn(handle, byref(gen))
+    _nvmlCheckReturn(ret)
+    return gen.value
+
+# Added in 4.304
+def nvmlDeviceGetSupportedClocksThrottleReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedClocksThrottleReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+def nvmlDeviceGetSupportedClocksEventReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedClocksEventReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+# Added in 4.304
+def nvmlDeviceGetCurrentClocksThrottleReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrentClocksThrottleReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+def nvmlDeviceGetCurrentClocksEventReasons(handle):
+    c_reasons= c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetCurrentClocksEventReasons")
+    ret = fn(handle, byref(c_reasons))
+    _nvmlCheckReturn(ret)
+    return c_reasons.value
+
+# Added in 5.319
+def nvmlDeviceGetIndex(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetIndex")
+    c_index = c_uint()
+    ret = fn(handle, byref(c_index))
+    _nvmlCheckReturn(ret)
+    return c_index.value
+
+# Added in 5.319
+def nvmlDeviceGetAccountingMode(handle):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingMode")
+    ret = fn(handle, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlDeviceSetAccountingMode(handle, mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetAccountingMode")
+    ret = fn(handle, _nvmlEnableState_t(mode))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceClearAccountingPids(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearAccountingPids")
+    ret = fn(handle)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetAccountingStats(handle, pid):
+    stats = c_nvmlAccountingStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingStats")
+    ret = fn(handle, c_uint(pid), byref(stats))
+    _nvmlCheckReturn(ret)
+    if (stats.maxMemoryUsage == MXSMLEX_VALUE_NOT_AVAILABLE_ulonglong.value):
+        # special case for WDDM on Windows, see comment above
+        stats.maxMemoryUsage = None
+    return stats
+
+def nvmlDeviceGetAccountingPids(handle):
+    count = c_uint(nvmlDeviceGetAccountingBufferSize(handle))
+    pids = (c_uint * count.value)()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingPids")
+    ret = fn(handle, byref(count), pids)
+    _nvmlCheckReturn(ret)
+    return list(map(int, pids[0:count.value]))
+
+def nvmlDeviceGetAccountingBufferSize(handle):
+    bufferSize = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAccountingBufferSize")
+    ret = fn(handle, byref(bufferSize))
+    _nvmlCheckReturn(ret)
+    return int(bufferSize.value)
+
+def nvmlDeviceGetRetiredPages(device, sourceFilter):
+    c_source = _nvmlPageRetirementCause_t(sourceFilter)
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRetiredPages")
+
+    # First call will get the size
+    ret = fn(device, c_source, byref(c_count), None)
+
+    # this should only fail with insufficient size
+    if ((ret != MXSMLEX_SUCCESS) and
+        (ret != MXSMLEX_ERROR_INSUFFICIENT_SIZE)):
+        raise NVMLError(ret)
+
+    # call again with a buffer
+    # oversize the array for the rare cases where additional pages
+    # are retired between MXSMLEX calls
+    c_count.value = c_count.value * 2 + 5
+    page_array = c_ulonglong * c_count.value
+    c_pages = page_array()
+    ret = fn(device, c_source, byref(c_count), c_pages)
+    _nvmlCheckReturn(ret)
+    return list(map(int, c_pages[0:c_count.value]))
+
+def nvmlDeviceGetRetiredPages_v2(device, sourceFilter):
+    c_source = _nvmlPageRetirementCause_t(sourceFilter)
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRetiredPages_v2")
+
+    # First call will get the size
+    ret = fn(device, c_source, byref(c_count), None)
+
+    # this should only fail with insufficient size
+    if ((ret != MXSMLEX_SUCCESS) and
+        (ret != MXSMLEX_ERROR_INSUFFICIENT_SIZE)):
+        raise NVMLError(ret)
+
+    # call again with a buffer
+    # oversize the array for the rare cases where additional pages
+    # are retired between MXSMLEX calls
+    c_count.value = c_count.value * 2 + 5
+    page_array = c_ulonglong * c_count.value
+    c_pages = page_array()
+    times_array = c_ulonglong * c_count.value
+    c_times = times_array()
+    ret = fn(device, c_source, byref(c_count), c_pages, c_times)
+    _nvmlCheckReturn(ret)
+    return [ { 'address': int(c_pages[i]), 'timestamp': int(c_times[i]) } for i in range(c_count.value) ];
+
+def nvmlDeviceGetRetiredPagesPendingStatus(device):
+    c_pending = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRetiredPagesPendingStatus")
+    ret = fn(device, byref(c_pending))
+    _nvmlCheckReturn(ret)
+    return int(c_pending.value)
+
+def nvmlDeviceGetAPIRestriction(device, apiType):
+    c_permission = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAPIRestriction")
+    ret = fn(device, _nvmlRestrictedAPI_t(apiType), byref(c_permission))
+    _nvmlCheckReturn(ret)
+    return int(c_permission.value)
+
+def nvmlDeviceSetAPIRestriction(handle, apiType, isRestricted):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetAPIRestriction")
+    ret = fn(handle, _nvmlRestrictedAPI_t(apiType), _nvmlEnableState_t(isRestricted))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetBridgeChipInfo(handle):
+    bridgeHierarchy = c_nvmlBridgeChipHierarchy_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBridgeChipInfo")
+    ret = fn(handle, byref(bridgeHierarchy))
+    _nvmlCheckReturn(ret)
+    return bridgeHierarchy
+
+def nvmlDeviceGetSamples(device, sampling_type, timeStamp):
+    c_sampling_type = _nvmlSamplingType_t(sampling_type)
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_sample_count = c_uint(0)
+    c_sample_value_type = _nvmlValueType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSamples")
+
+    ## First Call gets the size
+    ret = fn(device, c_sampling_type, c_time_stamp, byref(c_sample_value_type), byref(c_sample_count), None)
+
+    # Stop if this fails
+    if (ret != MXSMLEX_SUCCESS):
+        raise NVMLError(ret)
+
+    sampleArray = c_sample_count.value * c_nvmlSample_t
+    c_samples = sampleArray()
+    ret = fn(device, c_sampling_type, c_time_stamp,  byref(c_sample_value_type), byref(c_sample_count), c_samples)
+    _nvmlCheckReturn(ret)
+    return (c_sample_value_type.value, c_samples[0:c_sample_count.value])
+
+def nvmlDeviceGetViolationStatus(device, perfPolicyType):
+    c_perfPolicy_type = _nvmlPerfPolicyType_t(perfPolicyType)
+    c_violTime = c_nvmlViolationTime_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetViolationStatus")
+
+    ## Invoke the method to get violation time
+    ret = fn(device, c_perfPolicy_type, byref(c_violTime))
+    _nvmlCheckReturn(ret)
+    return c_violTime
+
+def nvmlDeviceGetPcieThroughput(device, counter):
+    c_util = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieThroughput")
+    ret = fn(device, _nvmlPcieUtilCounter_t(counter), byref(c_util))
+    _nvmlCheckReturn(ret)
+    return c_util.value
+
+def nvmlSystemGetTopologyGpuSet(cpuNumber):
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetTopologyGpuSet")
+
+    # First call will get the size
+    ret = fn(cpuNumber, byref(c_count), None)
+
+    if ret != MXSMLEX_SUCCESS:
+        raise NVMLError(ret)
+    # call again with a buffer
+    device_array = c_nvmlDevice_t * c_count.value
+    c_devices = device_array()
+    ret = fn(cpuNumber, byref(c_count), c_devices)
+    _nvmlCheckReturn(ret)
+    return list(c_devices[0:c_count.value])
+
+def nvmlDeviceGetTopologyNearestGpus(device, level):
+    c_count = c_uint(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTopologyNearestGpus")
+
+    # First call will get the size
+    ret = fn(device, level, byref(c_count), None)
+
+    if ret != MXSMLEX_SUCCESS:
+        raise NVMLError(ret)
+
+    # call again with a buffer
+    device_array = c_nvmlDevice_t * c_count.value
+    c_devices = device_array()
+    ret = fn(device, level, byref(c_count), c_devices)
+    _nvmlCheckReturn(ret)
+    return list(c_devices[0:c_count.value])
+
+def nvmlDeviceGetTopologyCommonAncestor(device1, device2):
+    c_level = _nvmlGpuTopologyLevel_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetTopologyCommonAncestor")
+    ret = fn(device1, device2, byref(c_level))
+    _nvmlCheckReturn(ret)
+    return c_level.value
+
+def nvmlDeviceGetNvLinkUtilizationCounter(device, link, counter):
+    c_rxcounter = c_ulonglong()
+    c_txcounter = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkUtilizationCounter")
+    ret = fn(device, link, counter, byref(c_rxcounter), byref(c_txcounter))
+    _nvmlCheckReturn(ret)
+    return (c_rxcounter.value, c_txcounter.value)
+
+def nvmlDeviceFreezeNvLinkUtilizationCounter(device, link, counter, freeze):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceFreezeNvLinkUtilizationCounter")
+    ret = fn(device, link, counter, freeze)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceResetNvLinkUtilizationCounter(device, link, counter):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetNvLinkUtilizationCounter")
+    ret = fn(device, link, counter)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceSetNvLinkUtilizationControl(device, link, counter, control, reset):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetNvLinkUtilizationControl")
+    ret = fn(device, link, counter, byref(control), reset)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetNvLinkUtilizationControl(device, link, counter):
+    c_control = nvmlNvLinkUtilizationControl_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkUtilizationControl")
+    ret = fn(device, link, counter, byref(c_control))
+    _nvmlCheckReturn(ret)
+    return c_control
+
+def nvmlDeviceGetNvLinkCapability(device, link, capability):
+    c_capResult = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkCapability")
+    ret = fn(device, link, capability, byref(c_capResult))
+    _nvmlCheckReturn(ret)
+    return c_capResult.value
+
+def nvmlDeviceGetNvLinkErrorCounter(device, link, counter):
+    c_result = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkErrorCounter")
+    ret = fn(device, link, counter, byref(c_result))
+    _nvmlCheckReturn(ret)
+    return c_result.value
+
+def nvmlDeviceResetNvLinkErrorCounters(device, link):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceResetNvLinkErrorCounters")
+    ret = fn(device, link)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetNvLinkRemotePciInfo(device, link):
+    c_pci = nvmlPciInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkRemotePciInfo_v2")
+    ret = fn(device, link, byref(c_pci))
+    _nvmlCheckReturn(ret)
+    return c_pci
+
+def nvmlDeviceGetNvLinkRemoteDeviceType(handle, link):
+    c_type = _nvmlNvLinkDeviceType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkRemoteDeviceType")
+    ret = fn(handle, link, byref(c_type))
+    _nvmlCheckReturn(ret)
+    return c_type.value
+
+def nvmlDeviceGetNvLinkState(device, link):
+    c_isActive = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkState")
+    ret = fn(device, link, byref(c_isActive))
+    _nvmlCheckReturn(ret)
+    return c_isActive.value
+
+def nvmlDeviceGetNvLinkVersion(device, link):
+    c_version = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNvLinkVersion")
+    ret = fn(device, link, byref(c_version))
+    _nvmlCheckReturn(ret)
+    return c_version.value
+
+def nvmlDeviceModifyDrainState(pciInfo, newState):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceModifyDrainState")
+    ret = fn(pointer(pciInfo), newState)
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceQueryDrainState(pciInfo):
+    c_newState = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceQueryDrainState")
+    ret = fn(pointer(pciInfo), byref(c_newState))
+    _nvmlCheckReturn(ret)
+    return c_newState.value
+
+def nvmlDeviceRemoveGpu(pciInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceRemoveGpu")
+    ret = fn(pointer(pciInfo))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceDiscoverGpus(pciInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceDiscoverGpus")
+    ret = fn(pointer(pciInfo))
+    _nvmlCheckReturn(ret)
+    return None
+
+def nvmlDeviceGetFieldValues(handle, fieldIds):
+    values_arr = c_nvmlFieldValue_t * len(fieldIds)
+    values = values_arr()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFieldValues")
+
+    for i, fieldId in enumerate(fieldIds):
+        try:
+            (values[i].fieldId, values[i].scopeId) = fieldId
+        except TypeError:
+            values[i].fieldId = fieldId
+
+    ret = fn(handle, c_int32(len(fieldIds)), byref(values))
+    _nvmlCheckReturn(ret)
+    return values
+
+def nvmlDeviceClearFieldValues(handle, fieldIds):
+    values_arr = c_nvmlFieldValue_t * len(fieldIds)
+    values = values_arr()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceClearFieldValues")
+
+    for i, fieldId in enumerate(fieldIds):
+        try:
+            (values[i].fieldId, values[i].scopeId) = fieldId
+        except TypeError:
+            values[i].fieldId = fieldId
+
+    ret = fn(handle, c_int32(len(fieldIds)), byref(values))
+    _nvmlCheckReturn(ret)
+    return values
+
+def nvmlDeviceGetVirtualizationMode(handle):
+    c_virtualization_mode = c_ulonglong()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVirtualizationMode")
+    ret = fn(handle, byref(c_virtualization_mode))
+    _nvmlCheckReturn(ret)
+    return c_virtualization_mode.value
+
+def nvmlDeviceSetVirtualizationMode(handle, virtualization_mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVirtualizationMode")
+    return fn(handle, virtualization_mode)
+
+def nvmlDeviceGetVgpuHeterogeneousMode(handle):
+    c_vgpuHeterogeneousMode = c_nvmlVgpuHeterogeneousMode_v1_t(0)
+    c_vgpuHeterogeneousMode.version = VgpuHeterogeneousMode_v1
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuHeterogeneousMode")
+    ret = fn(handle, byref(c_vgpuHeterogeneousMode))
+    _nvmlCheckReturn(ret)
+    return c_vgpuHeterogeneousMode.mode
+
+def nvmlDeviceSetVgpuHeterogeneousMode(handle, heterogeneous_mode):
+    c_vgpuHeterogeneousMode = c_nvmlVgpuHeterogeneousMode_v1_t(0)
+    c_vgpuHeterogeneousMode.version = VgpuHeterogeneousMode_v1
+    c_vgpuHeterogeneousMode.mode = heterogeneous_mode
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVgpuHeterogeneousMode")
+    ret = fn(handle, byref(c_vgpuHeterogeneousMode))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlVgpuInstanceGetPlacementId(vgpuInstance):
+    c_placement = c_nvmlVgpuPlacementId_v1_t(0)
+    c_placement.version = VgpuPlacementId_v1
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetPlacementId")
+    ret = fn(vgpuInstance, byref(c_placement))
+    _nvmlCheckReturn(ret)
+    return c_placement.placementId
+
+def nvmlDeviceGetVgpuTypeSupportedPlacements(handle, vgpuTypeId):
+    c_vgpu_placements = c_nvmlVgpuPlacementList_v1_t()
+    c_max_instances = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstances")
+    ret = fn(handle, vgpuTypeId, byref(c_max_instances))
+    _nvmlCheckReturn(ret)
+    c_placements = c_uint * c_max_instances.value
+    c_vgpu_placements.version = VgpuPlacementList_v1
+    c_vgpu_placements.placementIds = c_placements()
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuTypeSupportedPlacements")
+    ret = fn(handle, vgpuTypeId, byref(c_vgpu_placements))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_placements
+
+def nvmlDeviceGetVgpuTypeCreatablePlacements(handle, vgpuTypeId):
+    c_vgpu_placements = c_nvmlVgpuPlacementList_v1_t()
+    c_max_instances = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstances")
+    ret = fn(handle, vgpuTypeId, byref(c_max_instances))
+    _nvmlCheckReturn(ret)
+    c_placements = c_uint * c_max_instances.value
+    c_vgpu_placements.version = VgpuPlacementList_v1
+    c_vgpu_placements.placementIds = c_placements()
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuTypeCreatablePlacements")
+    ret = fn(handle, vgpuTypeId, byref(c_vgpu_placements))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_placements
+
+def nvmlGetVgpuDriverCapabilities(capability):
+    c_capResult = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetVgpuDriverCapabilities")
+    ret = fn(_nvmlVgpuDriverCapability_t(capability), byref(c_capResult))
+    _nvmlCheckReturn(ret)
+    return c_capResult.value
+
+def nvmlDeviceGetVgpuCapabilities(handle, capability):
+    c_capResult = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuCapabilities")
+    ret = fn(handle, _nvmlDeviceVgpuCapability_t(capability), byref(c_capResult))
+    _nvmlCheckReturn(ret)
+    return c_capResult.value
+
+def nvmlDeviceSetVgpuCapabilities(handle, capability, state):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVgpuCapabilities")
+    ret = fn(handle, _nvmlDeviceVgpuCapability_t(capability), state)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetSupportedVgpus(handle):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+
+    fn =  _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedVgpus")
+    ret = fn(handle, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no supported vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        vgpu_type_ids_array = _nvmlVgpuTypeId_t * c_vgpu_count.value
+        c_vgpu_type_ids = vgpu_type_ids_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpu_count), c_vgpu_type_ids)
+        _nvmlCheckReturn(ret)
+        vgpus = []
+        for i in range(c_vgpu_count.value):
+            vgpus.append(c_vgpu_type_ids[i])
+        return vgpus
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetCreatableVgpus(handle):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+
+    fn =  _nvmlGetFunctionPointer("mxSmlExDeviceGetCreatableVgpus")
+    ret = fn(handle, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no supported vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        vgpu_type_ids_array = _nvmlVgpuTypeId_t * c_vgpu_count.value
+        c_vgpu_type_ids = vgpu_type_ids_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpu_count), c_vgpu_type_ids)
+        _nvmlCheckReturn(ret)
+        vgpus = []
+        for i in range(c_vgpu_count.value):
+            vgpus.append(c_vgpu_type_ids[i])
+        return vgpus
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuTypeGetGpuInstanceProfileId(vgpuTypeId):
+    c_profile_id = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetGpuInstanceProfileId")
+    ret = fn(vgpuTypeId, byref(c_profile_id))
+    _nvmlCheckReturn(ret)
+    return (c_profile_id.value)
+
+@convertStrBytes
+def nvmlVgpuTypeGetClass(vgpuTypeId):
+    c_class = create_string_buffer(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetClass")
+    ret = fn(vgpuTypeId, c_class, byref(c_buffer_size))
+    _nvmlCheckReturn(ret)
+    return c_class.value
+
+@convertStrBytes
+def nvmlVgpuTypeGetName(vgpuTypeId):
+    c_name = create_string_buffer(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_NAME_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetName")
+    ret = fn(vgpuTypeId, c_name, byref(c_buffer_size))
+    _nvmlCheckReturn(ret)
+    return c_name.value
+
+def nvmlVgpuTypeGetDeviceID(vgpuTypeId):
+    c_device_id    = c_ulonglong(0)
+    c_subsystem_id = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetDeviceID")
+    ret = fn(vgpuTypeId, byref(c_device_id), byref(c_subsystem_id))
+    _nvmlCheckReturn(ret)
+    return (c_device_id.value, c_subsystem_id.value)
+
+def nvmlVgpuTypeGetFramebufferSize(vgpuTypeId):
+    c_fb_size = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetFramebufferSize")
+    ret = fn(vgpuTypeId, byref(c_fb_size))
+    _nvmlCheckReturn(ret)
+    return c_fb_size.value
+
+def nvmlVgpuTypeGetNumDisplayHeads(vgpuTypeId):
+    c_num_heads = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetNumDisplayHeads")
+    ret = fn(vgpuTypeId, byref(c_num_heads))
+    _nvmlCheckReturn(ret)
+    return c_num_heads.value
+
+def nvmlVgpuTypeGetResolution(vgpuTypeId):
+    c_xdim = c_uint(0)
+    c_ydim = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetResolution")
+    ret = fn(vgpuTypeId, 0, byref(c_xdim), byref(c_ydim))
+    _nvmlCheckReturn(ret)
+    return (c_xdim.value, c_ydim.value)
+
+@convertStrBytes
+def nvmlVgpuTypeGetLicense(vgpuTypeId):
+    c_license = create_string_buffer(MXSMLEX_GRID_LICENSE_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_GRID_LICENSE_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetLicense")
+    ret = fn(vgpuTypeId, c_license, c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_license.value
+
+def nvmlVgpuTypeGetFrameRateLimit(vgpuTypeId):
+    c_frl_config = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetFrameRateLimit")
+    ret = fn(vgpuTypeId, byref(c_frl_config))
+    _nvmlCheckReturn(ret)
+    return c_frl_config.value
+
+def nvmlVgpuTypeGetGspHeapSize(vgpuTypeId):
+    c_gsp_heap = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetGspHeapSize")
+    ret = fn(vgpuTypeId, byref(c_gsp_heap))
+    _nvmlCheckReturn(ret)
+    return c_gsp_heap.value
+
+def nvmlVgpuTypeGetFbReservation(vgpuTypeId):
+    c_fb_reservation = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetFbReservation")
+    ret = fn(vgpuTypeId, byref(c_fb_reservation))
+    _nvmlCheckReturn(ret)
+    return c_fb_reservation.value
+
+def nvmlVgpuTypeGetMaxInstances(handle, vgpuTypeId):
+    c_max_instances = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstances")
+    ret = fn(handle, vgpuTypeId, byref(c_max_instances))
+    _nvmlCheckReturn(ret)
+    return c_max_instances.value
+
+def nvmlVgpuTypeGetMaxInstancesPerVm(vgpuTypeId):
+    c_max_instances_per_vm = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetMaxInstancesPerVm")
+    ret = fn(vgpuTypeId, byref(c_max_instances_per_vm))
+    _nvmlCheckReturn(ret)
+    return c_max_instances_per_vm.value
+
+def nvmlDeviceGetActiveVgpus(handle):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetActiveVgpus")
+    ret = fn(handle, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        vgpu_instance_array = _nvmlVgpuInstance_t * c_vgpu_count.value
+        c_vgpu_instances = vgpu_instance_array()
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpu_count), c_vgpu_instances)
+        _nvmlCheckReturn(ret)
+        vgpus = []
+        for i in range(c_vgpu_count.value):
+            vgpus.append(c_vgpu_instances[i])
+        return vgpus
+    else:
+        # error case
+        raise NVMLError(ret)
+
+@convertStrBytes
+def nvmlVgpuInstanceGetVmID(vgpuInstance):
+    c_vm_id = create_string_buffer(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_GRID_LICENSE_BUFFER_SIZE)
+    c_vm_id_type  = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetVmID")
+    ret = fn(vgpuInstance, byref(c_vm_id), c_buffer_size, byref(c_vm_id_type))
+    _nvmlCheckReturn(ret)
+    return (c_vm_id.value, c_vm_id_type.value)
+
+@convertStrBytes
+def nvmlVgpuInstanceGetUUID(vgpuInstance):
+    c_uuid = create_string_buffer(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetUUID")
+    ret = fn(vgpuInstance, byref(c_uuid), c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_uuid.value
+
+@convertStrBytes
+def nvmlVgpuInstanceGetMdevUUID(vgpuInstance):
+    c_uuid = create_string_buffer(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_DEVICE_UUID_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetMdevUUID")
+    ret = fn(vgpuInstance, byref(c_uuid), c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_uuid.value
+
+@convertStrBytes
+def nvmlVgpuInstanceGetVmDriverVersion(vgpuInstance):
+    c_driver_version = create_string_buffer(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)
+    c_buffer_size = c_uint(MXSMLEX_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetVmDriverVersion")
+    ret = fn(vgpuInstance, byref(c_driver_version), c_buffer_size)
+    _nvmlCheckReturn(ret)
+    return c_driver_version.value
+
+def nvmlVgpuInstanceGetLicenseStatus(vgpuInstance):
+    c_license_status = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetLicenseStatus")
+    ret = fn(vgpuInstance, byref(c_license_status))
+    _nvmlCheckReturn(ret)
+    return c_license_status.value
+
+def nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance):
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetLicenseInfo_v2")
+    c_license_info = c_nvmlVgpuLicenseInfo_t()
+    ret = fn(vgpuInstance, byref(c_license_info))
+    _nvmlCheckReturn(ret)
+    return c_license_info
+
+def nvmlVgpuInstanceGetLicenseInfo(vgpuInstance):
+    return nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance)
+
+def nvmlVgpuInstanceGetFrameRateLimit(vgpuInstance):
+    c_frl = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFrameRateLimit")
+    ret = fn(vgpuInstance, byref(c_frl))
+    _nvmlCheckReturn(ret)
+    return c_frl.value
+
+def nvmlVgpuInstanceGetEccMode(vgpuInstance):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEccMode")
+    ret = fn(vgpuInstance, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlVgpuInstanceGetType(vgpuInstance):
+    c_vgpu_type = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetType")
+    ret = fn(vgpuInstance, byref(c_vgpu_type))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_type.value
+
+def nvmlVgpuInstanceGetEncoderCapacity(vgpuInstance):
+    c_encoder_capacity = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEncoderCapacity")
+    ret = fn(vgpuInstance, byref(c_encoder_capacity))
+    _nvmlCheckReturn(ret)
+    return c_encoder_capacity.value
+
+def nvmlVgpuInstanceSetEncoderCapacity(vgpuInstance, encoder_capacity):
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceSetEncoderCapacity")
+    return fn(vgpuInstance, encoder_capacity)
+
+def nvmlVgpuInstanceGetFbUsage(vgpuInstance):
+    c_fb_usage = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFbUsage")
+    ret = fn(vgpuInstance, byref(c_fb_usage))
+    _nvmlCheckReturn(ret)
+    return c_fb_usage.value
+
+def nvmlVgpuTypeGetCapabilities(vgpuTypeId, capability):
+    c_cap_result = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuTypeGetCapabilities")
+    ret = fn(vgpuTypeId, _nvmlVgpuCapability_t(capability), byref(c_cap_result))
+    _nvmlCheckReturn(ret)
+    return (c_cap_result.value)
+
+def nvmlVgpuInstanceGetGpuInstanceId(vgpuInstance):
+    c_id = c_uint(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetGpuInstanceId")
+    ret = fn(vgpuInstance, byref(c_id))
+    _nvmlCheckReturn(ret)
+    return (c_id.value)
+
+@convertStrBytes
+def nvmlVgpuInstanceGetGpuPciId(vgpuInstance):
+    c_vgpuPciId = create_string_buffer(MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE)
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetGpuPciId")
+    ret = fn(vgpuInstance, c_vgpuPciId, byref(c_uint(MXSMLEX_DEVICE_PCI_BUS_ID_BUFFER_SIZE)))
+    _nvmlCheckReturn(ret)
+    return c_vgpuPciId.value
+
+def nvmlDeviceGetVgpuUtilization(handle, timeStamp):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_sample_value_type = _nvmlValueType_t()
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuUtilization")
+    ret = fn(handle, c_time_stamp, byref(c_sample_value_type), byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpu_count.value * c_nvmlVgpuInstanceUtilizationSample_t
+        c_samples = sampleArray()
+
+        # make the call again
+        ret = fn(handle, c_time_stamp, byref(c_sample_value_type), byref(c_vgpu_count), c_samples)
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpu_count.value]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetVgpuInstancesUtilizationInfo(handle, timeStamp):
+    # first call to get the size
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_vgpuUtilInfo = c_nvmlVgpuInstancesUtilizationInfo_v1_t(0)
+    c_vgpuUtilInfo.version = VgpuInstancesUtilizationInfo_v1
+    c_vgpuUtilInfo.sampleValType = _nvmlValueType_t()
+    c_vgpuUtilInfo.vgpuInstanceCount = c_uint(0)
+    c_vgpuUtilInfo.lastSeenTimeStamp = c_time_stamp
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuInstancesUtilizationInfo")
+    ret = fn(handle, byref(c_vgpuUtilInfo))
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpuUtilInfo.vgpuInstanceCount * c_nvmlVgpuInstanceUtilizationInfo_v1_t
+        c_samples = sampleArray()
+        c_vgpuUtilInfo.vgpuUtilArray = c_samples
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpuUtilInfo))
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpuUtilInfo.vgpuInstanceCount]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetP2PStatus(device1, device2, p2pIndex):
+    c_p2pstatus = _nvmlGpuP2PStatus_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetP2PStatus")
+    ret = fn(device1, device2,p2pIndex, byref(c_p2pstatus))
+    _nvmlCheckReturn(ret)
+    return c_p2pstatus.value
+
+def nvmlDeviceGetGridLicensableFeatures_v4(handle):
+    c_get_grid_licensable_features = c_nvmlGridLicensableFeatures_v4_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGridLicensableFeatures_v4")
+    ret = fn(handle, byref(c_get_grid_licensable_features))
+    _nvmlCheckReturn(ret)
+
+    return (c_get_grid_licensable_features)
+
+def nvmlDeviceGetGridLicensableFeatures(handle):
+    return nvmlDeviceGetGridLicensableFeatures_v4(handle)
+
+def nvmlDeviceGetGspFirmwareVersion(handle, version):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGspFirmwareVersion")
+    ret = fn(handle, version)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGspFirmwareMode(handle, isEnabled, defaultMode):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGspFirmwareMode")
+    ret = fn(handle, isEnabled, defaultMode)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetEncoderCapacity(handle, encoderQueryType):
+    c_encoder_capacity = c_ulonglong(0)
+    c_encoderQuery_type = _nvmlEncoderQueryType_t(encoderQueryType)
+
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderCapacity")
+    ret = fn(handle, c_encoderQuery_type, byref(c_encoder_capacity))
+    _nvmlCheckReturn(ret)
+    return c_encoder_capacity.value
+
+def nvmlDeviceGetVgpuProcessUtilization(handle, timeStamp):
+    # first call to get the size
+    c_vgpu_count = c_uint(0)
+    c_time_stamp = c_ulonglong(timeStamp)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuProcessUtilization")
+    ret = fn(handle, c_time_stamp, byref(c_vgpu_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpu_count.value * c_nvmlVgpuProcessUtilizationSample_t
+        c_samples = sampleArray()
+
+        # make the call again
+        ret = fn(handle, c_time_stamp, byref(c_vgpu_count), c_samples)
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpu_count.value]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetVgpuProcessesUtilizationInfo(handle, timeStamp):
+    # first call to get the size
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_vgpuProcUtilInfo = c_nvmlVgpuProcessesUtilizationInfo_v1_t(0)
+    c_vgpuProcUtilInfo.version = VgpuProcessesUtilizationInfo_v1
+    c_vgpuProcUtilInfo.vgpuProcessCount = c_uint(0)
+    c_vgpuProcUtilInfo.lastSeenTimeStamp = c_time_stamp
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuProcessesUtilizationInfo")
+    ret = fn(handle, byref(c_vgpuProcUtilInfo))
+
+    if (ret == MXSMLEX_SUCCESS):
+        # special case, no active vGPUs
+        return []
+    elif (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_vgpuProcUtilInfo.vgpuProcessCount * c_nvmlVgpuProcessUtilizationInfo_v1_t
+        c_samples = sampleArray()
+        c_vgpuProcUtilInfo.vgpuProcUtilArray = c_samples
+
+        # make the call again
+        ret = fn(handle, byref(c_vgpuProcUtilInfo))
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_vgpuProcUtilInfo.vgpuProcessCount]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetEncoderStats(handle):
+    c_encoderCount = c_ulonglong(0)
+    c_encodeFps = c_ulonglong(0)
+    c_encoderLatency = c_ulonglong(0)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderStats")
+    ret = fn(handle, byref(c_encoderCount), byref(c_encodeFps), byref(c_encoderLatency))
+    _nvmlCheckReturn(ret)
+    return (c_encoderCount.value, c_encodeFps.value, c_encoderLatency.value)
+
+def nvmlDeviceGetEncoderSessions(handle):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetEncoderSessions")
+    ret = fn(handle, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlEncoderSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(handle, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetFBCStats(handle):
+    c_fbcStats = c_nvmlFBCStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetFBCStats")
+    ret = fn(handle, byref(c_fbcStats))
+    _nvmlCheckReturn(ret)
+    return c_fbcStats
+
+def nvmlDeviceGetFBCSessions(handle):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetFBCSessions")
+    ret = fn(handle, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlFBCSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(handle, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuInstanceGetEncoderStats(vgpuInstance):
+    c_encoderCount    = c_ulonglong(0)
+    c_encodeFps       = c_ulonglong(0)
+    c_encoderLatency  = c_ulonglong(0)
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEncoderStats")
+    ret = fn(vgpuInstance, byref(c_encoderCount), byref(c_encodeFps), byref(c_encoderLatency))
+    _nvmlCheckReturn(ret)
+    return (c_encoderCount.value, c_encodeFps.value, c_encoderLatency.value)
+
+def nvmlVgpuInstanceGetEncoderSessions(vgpuInstance):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetEncoderSessions")
+    ret = fn(vgpuInstance, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlEncoderSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(vgpuInstance, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuInstanceGetFBCStats(vgpuInstance):
+    c_fbcStats = c_nvmlFBCStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFBCStats")
+    ret = fn(vgpuInstance, byref(c_fbcStats))
+    _nvmlCheckReturn(ret)
+    return c_fbcStats
+
+def nvmlVgpuInstanceGetFBCSessions(vgpuInstance):
+    # first call to get the size
+    c_session_count = c_uint(0)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetFBCSessions")
+    ret = fn(vgpuInstance, byref(c_session_count), None)
+
+    if (ret == MXSMLEX_SUCCESS):
+        if (c_session_count.value != 0):
+            # typical case
+            session_array = c_nvmlFBCSession_t * c_session_count.value
+            c_sessions = session_array()
+
+            # make the call again
+            ret = fn(vgpuInstance, byref(c_session_count), c_sessions)
+            _nvmlCheckReturn(ret)
+            sessions = []
+            for i in range(c_session_count.value):
+                sessions.append(c_sessions[i])
+            return sessions
+        else:
+            return []  # no active sessions
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetProcessUtilization(handle, timeStamp):
+    # first call to get the size
+    c_count = c_uint(0)
+    c_time_stamp = c_ulonglong(timeStamp)
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetProcessUtilization")
+    ret = fn(handle, None, byref(c_count), c_time_stamp)
+
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_count.value * c_nvmlProcessUtilizationSample_t
+        c_samples = sampleArray()
+
+        # make the call again
+        ret = fn(handle, c_samples, byref(c_count), c_time_stamp)
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_count.value]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlDeviceGetProcessesUtilizationInfo(handle, timeStamp):
+    # first call to get the size
+    c_time_stamp = c_ulonglong(timeStamp)
+    c_processesUtilInfo = c_nvmlProcessesUtilizationInfo_v1_t(0)
+    c_processesUtilInfo.version = ProcessesUtilizationInfo_v1
+    c_processesUtilInfo.processSamplesCount = c_uint(0)
+    c_processesUtilInfo.lastSeenTimeStamp = c_time_stamp
+
+    fn  = _nvmlGetFunctionPointer("mxSmlExDeviceGetProcessesUtilizationInfo")
+    ret = fn(handle, byref(c_processesUtilInfo))
+
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        # typical case
+        sampleArray = c_processesUtilInfo.processSamplesCount * c_nvmlProcessUtilizationInfo_v1_t
+        c_samples = sampleArray()
+        c_processesUtilInfo.procUtilArray = c_samples
+
+        # make the call again
+        ret = fn(handle, byref(c_processesUtilInfo))
+        _nvmlCheckReturn(ret)
+
+        return c_samples[0:c_processesUtilInfo.processSamplesCount]
+    else:
+        # error case
+        raise NVMLError(ret)
+
+def nvmlVgpuInstanceGetMetadata(vgpuInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetMetadata")
+    c_vgpuMetadata = c_nvmlVgpuMetadata_t()
+    c_bufferSize = c_uint(0)
+    # Make the first MXSMLEX API call to get the c_bufferSize value.
+    # We have already allocated required buffer above.
+    ret = fn(vgpuInstance, byref(c_vgpuMetadata), byref(c_bufferSize))
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = fn(vgpuInstance, byref(c_vgpuMetadata), byref(c_bufferSize))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return c_vgpuMetadata
+
+def nvmlDeviceGetVgpuMetadata(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuMetadata")
+    c_vgpuPgpuMetadata = c_nvmlVgpuPgpuMetadata_t()
+    c_bufferSize = c_uint(0)
+    # Make the first MXSMLEX API call to get the c_bufferSize value.
+    # We have already allocated required buffer above.
+    ret = fn(handle, byref(c_vgpuPgpuMetadata), byref(c_bufferSize))
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = fn(handle, byref(c_vgpuPgpuMetadata), byref(c_bufferSize))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return c_vgpuPgpuMetadata
+
+def nvmlGetVgpuCompatibility(vgpuMetadata, pgpuMetadata):
+    fn = _nvmlGetFunctionPointer("mxSmlExGetVgpuCompatibility")
+    c_vgpuPgpuCompatibility = c_nvmlVgpuPgpuCompatibility_t()
+    ret = fn(byref(vgpuMetadata), byref(pgpuMetadata), byref(c_vgpuPgpuCompatibility))
+    _nvmlCheckReturn(ret)
+    return c_vgpuPgpuCompatibility
+
+@convertStrBytes
+def nvmlDeviceGetPgpuMetadataString(handle):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPgpuMetadataString")
+    c_pgpuMetadata = create_string_buffer(MXSMLEX_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE)
+    c_bufferSize = c_uint(0)
+    # Make the first MXSMLEX API call to get the c_bufferSize value.
+    # We have already allocated required buffer above.
+    ret = fn(handle, byref(c_pgpuMetadata), byref(c_bufferSize))
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        ret = fn(handle, byref(c_pgpuMetadata), byref(c_bufferSize))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return (c_pgpuMetadata.value, c_bufferSize.value)
+
+def nvmlDeviceGetVgpuSchedulerLog(handle):
+    c_vgpu_sched_log = c_nvmlVgpuSchedulerLog_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuSchedulerLog")
+    ret = fn(handle, byref(c_vgpu_sched_log))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_sched_log
+
+def nvmlDeviceGetVgpuSchedulerState(handle):
+    c_vgpu_sched_state = c_nvmlVgpuSchedulerGetState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuSchedulerState")
+    ret = fn(handle, byref(c_vgpu_sched_state))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_sched_state
+
+def nvmlDeviceGetVgpuSchedulerCapabilities(handle):
+    c_vgpu_sched_caps = c_nvmlVgpuSchedulerCapabilities_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetVgpuSchedulerCapabilities")
+    ret = fn(handle, byref(c_vgpu_sched_caps))
+    _nvmlCheckReturn(ret)
+    return c_vgpu_sched_caps
+
+def nvmlDeviceSetVgpuSchedulerState(handle, sched_state):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetVgpuSchedulerState")
+    ret = fn(handle, byref(sched_state))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSetVgpuVersion(vgpuVersion):
+    fn = _nvmlGetFunctionPointer("mxSmlExSetVgpuVersion")
+    ret = fn(byref(vgpuVersion))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGetVgpuVersion(supported, current):
+    fn = _nvmlGetFunctionPointer("mxSmlExGetVgpuVersion")
+    ret = fn(byref(supported), byref(current))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlVgpuInstanceGetAccountingMode(vgpuInstance):
+    c_mode = _nvmlEnableState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetAccountingMode")
+    ret = fn(vgpuInstance, byref(c_mode))
+    _nvmlCheckReturn(ret)
+    return c_mode.value
+
+def nvmlVgpuInstanceGetAccountingPids(vgpuInstance):
+    c_pidCount = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetAccountingPids")
+    ret = fn(vgpuInstance, byref(c_pidCount), None)
+    if (ret == MXSMLEX_ERROR_INSUFFICIENT_SIZE):
+        sampleArray = c_pidCount.value * c_uint
+        c_pidArray = sampleArray()
+        ret = fn(vgpuInstance, byref(c_pidCount), byref(c_pidArray))
+        _nvmlCheckReturn(ret)
+    else:
+        raise NVMLError(ret)
+    return (c_pidCount, c_pidArray)
+
+def nvmlVgpuInstanceGetAccountingStats(vgpuInstance, pid):
+    c_accountingStats = c_nvmlAccountingStats_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceGetAccountingStats")
+    ret = fn(vgpuInstance, pid, byref(c_accountingStats))
+    _nvmlCheckReturn(ret)
+    return c_accountingStats
+
+def nvmlVgpuInstanceClearAccountingPids(vgpuInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExVgpuInstanceClearAccountingPids")
+    ret = fn(vgpuInstance)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGetExcludedDeviceCount():
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetExcludedDeviceCount")
+    ret = fn(byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlGetExcludedDeviceInfoByIndex(index):
+    c_index = c_uint(index)
+    info = c_nvmlExcludedDeviceInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGetExcludedDeviceInfoByIndex")
+    ret = fn(c_index, byref(info))
+    _nvmlCheckReturn(ret)
+    return info
+
+def nvmlDeviceGetHostVgpuMode(handle):
+    c_host_vgpu_mode = _nvmlHostVgpuMode_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetHostVgpuMode")
+    ret = fn(handle, byref(c_host_vgpu_mode))
+    _nvmlCheckReturn(ret)
+    return c_host_vgpu_mode.value
+
+def nvmlDeviceSetMigMode(device, mode):
+    c_activationStatus = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetMigMode")
+    ret = fn(device, mode, byref(c_activationStatus))
+    _nvmlCheckReturn(ret)
+    return c_activationStatus.value
+
+def nvmlDeviceGetMigMode(device):
+    c_currentMode = c_uint()
+    c_pendingMode = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMigMode")
+    ret = fn(device, byref(c_currentMode), byref(c_pendingMode))
+    _nvmlCheckReturn(ret)
+    return [c_currentMode.value, c_pendingMode.value]
+
+def nvmlDeviceGetGpuInstanceProfileInfo(device, profile, version=2):
+    if version == 2:
+        c_info = c_nvmlGpuInstanceProfileInfo_v2_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceProfileInfoV")
+    elif version == 1:
+        c_info = c_nvmlGpuInstanceProfileInfo_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceProfileInfo")
+    else:
+        raise NVMLError(MXSMLEX_ERROR_FUNCTION_NOT_FOUND)
+    ret = fn(device, profile, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+# Define function alias for the API exposed by MXSMLEX
+nvmlDeviceGetGpuInstanceProfileInfoV = nvmlDeviceGetGpuInstanceProfileInfo
+
+def nvmlDeviceGetGpuInstanceRemainingCapacity(device, profileId):
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceRemainingCapacity")
+    ret = fn(device, profileId, byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetGpuInstancePossiblePlacements(device, profileId, placementsRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstancePossiblePlacements_v2")
+    ret = fn(device, profileId, placementsRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceCreateGpuInstance(device, profileId):
+    c_instance = c_nvmlGpuInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceCreateGpuInstance")
+    ret = fn(device, profileId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlDeviceCreateGpuInstanceWithPlacement(device, profileId, placement):
+    c_instance = c_nvmlGpuInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceCreateGpuInstanceWithPlacement")
+    ret = fn(device, profileId, placement, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlGpuInstanceDestroy(gpuInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceDestroy")
+    ret = fn(gpuInstance)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpuInstances(device, profileId, gpuInstancesRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstances")
+    ret = fn(device, profileId, gpuInstancesRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpuInstanceById(device, gpuInstanceId):
+    c_instance = c_nvmlGpuInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceById")
+    ret = fn(device, gpuInstanceId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlGpuInstanceGetInfo(gpuInstance):
+    c_info = c_nvmlGpuInstanceInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetInfo")
+    ret = fn(gpuInstance, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlGpuInstanceGetComputeInstanceProfileInfo(device, profile, engProfile, version=2):
+    if version == 2:
+        c_info = c_nvmlComputeInstanceProfileInfo_v2_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceProfileInfoV")
+    elif version == 1:
+        c_info = c_nvmlComputeInstanceProfileInfo_t()
+        fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceProfileInfo")
+    else:
+        raise NVMLError(MXSMLEX_ERROR_FUNCTION_NOT_FOUND) 
+    ret = fn(device, profile, engProfile, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+# Define function alias for the API exposed by MXSMLEX
+nvmlGpuInstanceGetComputeInstanceProfileInfoV = nvmlGpuInstanceGetComputeInstanceProfileInfo
+
+def nvmlGpuInstanceGetComputeInstanceRemainingCapacity(gpuInstance, profileId):
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceRemainingCapacity")
+    ret = fn(gpuInstance, profileId, byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlGpuInstanceGetComputeInstancePossiblePlacements(gpuInstance, profileId, placementsRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstancePossiblePlacements")
+    ret = fn(gpuInstance, profileId, placementsRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpuInstanceCreateComputeInstance(gpuInstance, profileId):
+    c_instance = c_nvmlComputeInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceCreateComputeInstance")
+    ret = fn(gpuInstance, profileId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlGpuInstanceCreateComputeInstanceWithPlacement(gpuInstance, profileId, placement):
+    c_instance = c_nvmlComputeInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceCreateComputeInstanceWithPlacement")
+    ret = fn(gpuInstance, profileId, placement, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlComputeInstanceDestroy(computeInstance):
+    fn = _nvmlGetFunctionPointer("mxSmlExComputeInstanceDestroy")
+    ret = fn(computeInstance)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpuInstanceGetComputeInstances(gpuInstance, profileId, computeInstancesRef, countRef):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstances")
+    ret = fn(gpuInstance, profileId, computeInstancesRef, countRef)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpuInstanceGetComputeInstanceById(gpuInstance, computeInstanceId):
+    c_instance = c_nvmlComputeInstance_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpuInstanceGetComputeInstanceById")
+    ret = fn(gpuInstance, computeInstanceId, byref(c_instance))
+    _nvmlCheckReturn(ret)
+    return c_instance
+
+def nvmlComputeInstanceGetInfo_v2(computeInstance):
+    c_info = c_nvmlComputeInstanceInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExComputeInstanceGetInfo_v2")
+    ret = fn(computeInstance, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return c_info
+
+def nvmlComputeInstanceGetInfo(computeInstance):
+    return nvmlComputeInstanceGetInfo_v2(computeInstance)
+
+def nvmlDeviceIsMigDeviceHandle(device):
+    c_isMigDevice = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceIsMigDeviceHandle")
+    ret = fn(device, byref(c_isMigDevice))
+    _nvmlCheckReturn(ret)
+    return c_isMigDevice
+
+def nvmlDeviceGetGpuInstanceId(device):
+    c_gpuInstanceId = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuInstanceId")
+    ret = fn(device, byref(c_gpuInstanceId))
+    _nvmlCheckReturn(ret)
+    return c_gpuInstanceId.value
+
+def nvmlDeviceGetComputeInstanceId(device):
+    c_computeInstanceId = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetComputeInstanceId")
+    ret = fn(device, byref(c_computeInstanceId))
+    _nvmlCheckReturn(ret)
+    return c_computeInstanceId.value
+
+def nvmlDeviceGetMaxMigDeviceCount(device):
+    c_count = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMaxMigDeviceCount")
+    ret = fn(device, byref(c_count))
+    _nvmlCheckReturn(ret)
+    return c_count.value
+
+def nvmlDeviceGetMigDeviceHandleByIndex(device, index):
+    c_index = c_uint(index)
+    migDevice = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMigDeviceHandleByIndex")
+    ret = fn(device, c_index, byref(migDevice))
+    _nvmlCheckReturn(ret)
+    return migDevice
+
+def nvmlDeviceGetDeviceHandleFromMigDeviceHandle(migDevice):
+    device = c_nvmlDevice_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDeviceHandleFromMigDeviceHandle")
+    ret = fn(migDevice, byref(device))
+    _nvmlCheckReturn(ret)
+    return device
+
+def nvmlDeviceGetAttributes_v2(device):
+    c_attrs = c_nvmlDeviceAttributes()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAttributes_v2")
+    ret = fn(device, byref(c_attrs))
+    _nvmlCheckReturn(ret)
+    return c_attrs
+
+def nvmlDeviceGetAttributes(device):
+    return nvmlDeviceGetAttributes_v2(device)
+
+def nvmlDeviceGetRemappedRows(device):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRemappedRows")
+    c_corr = c_uint()
+    c_unc = c_uint()
+    c_bpending = c_uint()
+    c_bfailure = c_uint()
+    ret = fn(device, byref(c_corr), byref(c_unc), byref(c_bpending), byref(c_bfailure))
+    _nvmlCheckReturn(ret)
+    return (c_corr.value, c_unc.value, c_bpending.value, c_bfailure.value)
+
+def nvmlDeviceGetRowRemapperHistogram(device):
+    c_vals = c_nvmlRowRemapperHistogramValues()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetRowRemapperHistogram")
+    ret = fn(device, byref(c_vals))
+    _nvmlCheckReturn(ret)
+    return c_vals
+
+def nvmlDeviceGetArchitecture(device):
+    arch = _nvmlDeviceArchitecture_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetArchitecture")
+    ret = fn(device, byref(arch))
+    _nvmlCheckReturn(ret)
+    return arch.value
+
+def nvmlDeviceGetBusType(device):
+    c_busType = _nvmlBusType_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetBusType")
+    ret = fn(device, byref(c_busType))
+    _nvmlCheckReturn(ret)
+    return c_busType.value
+
+def nvmlDeviceGetIrqNum(device):
+    c_irqNum = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetIrqNum")
+    ret = fn(device, byref(c_irqNum))
+    _nvmlCheckReturn(ret)
+    return c_irqNum.value
+
+def nvmlDeviceGetNumGpuCores(device):
+    c_numCores = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetNumGpuCores")
+    ret = fn(device, byref(c_numCores))
+    _nvmlCheckReturn(ret)
+    return c_numCores.value
+
+def nvmlDeviceGetPowerSource(device):
+    c_powerSource = _nvmlPowerSource_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPowerSource")
+    ret = fn(device, byref(c_powerSource))
+    _nvmlCheckReturn(ret)
+    return c_powerSource.value
+
+def nvmlDeviceGetMemoryBusWidth(device):
+    c_memBusWidth = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemoryBusWidth")
+    ret = fn(device, byref(c_memBusWidth))
+    _nvmlCheckReturn(ret)
+    return c_memBusWidth.value
+
+def nvmlDeviceGetPcieLinkMaxSpeed(device):
+    c_speed = _nvmlPcieLinkMaxSpeed_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieLinkMaxSpeed")
+    ret = fn(device, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetAdaptiveClockInfoStatus(device):
+    c_adaptiveClockInfoStatus = _nvmlAdaptiveClockInfoStatus_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetAdaptiveClockInfoStatus")
+    ret = fn(device, byref(c_adaptiveClockInfoStatus))
+    _nvmlCheckReturn(ret)
+    return c_adaptiveClockInfoStatus.value
+
+def nvmlDeviceGetPcieSpeed(device):
+    c_speed = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetPcieSpeed")
+    ret = fn(device, byref(c_speed))
+    _nvmlCheckReturn(ret)
+    return c_speed.value
+
+def nvmlDeviceGetDynamicPstatesInfo(device, c_dynamicpstatesinfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetDynamicPstatesInfo");
+    ret = fn(device, c_dynamicpstatesinfo)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceSetFanSpeed_v2(handle, index, speed):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetFanSpeed_v2");
+    ret = fn(handle, index, speed)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetThermalSettings(device, sensorindex, c_thermalsettings):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetThermalSettings");
+    ret = fn(device, sensorindex, c_thermalsettings)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMinMaxClockOfPState(device, type, pstate, minClockMHz, maxClockMHz):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMinMaxClockOfPState");
+    ret = fn(device, _nvmlClockType_t(type), _nvmlClockType_t(pstate), minClockMHz, maxClockMHz)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetSupportedPerformanceStates(device):
+    pstates = []
+    c_count = c_uint(MXSMLEX_MAX_GPU_PERF_PSTATES)
+    c_size = sizeof(c_uint)*c_count.value
+
+    # NOTE: use 'c_uint' to represent the size of the nvmlPstate_t enumeration.
+    pstates_array = _nvmlPstates_t * c_count.value
+    c_pstates = pstates_array()
+
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSupportedPerformanceStates")
+    ret = fn(device, c_pstates, c_size)
+    _nvmlCheckReturn(ret)
+
+    for value in c_pstates:
+        if value != MXSMLEX_PSTATE_UNKNOWN:
+            pstates.append(value)
+
+    return pstates
+
+def nvmlDeviceGetGpcClkVfOffset(device):
+    offset = c_int32()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpcClkVfOffset")
+    ret = fn(device, byref(offset))
+    _nvmlCheckReturn(ret)
+    return offset.value
+
+def nvmlDeviceSetGpcClkVfOffset(device, offset):
+    c_offset = c_int32(offset)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetGpcClkVfOffset")
+    ret = fn(device, c_offset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpcClkMinMaxVfOffset(device, minOffset, maxOffset):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpcClkMinMaxVfOffset")
+    ret = fn(device, minOffset, maxOffset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMemClkVfOffset(device):
+    offset = c_int32()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemClkVfOffset")
+    ret = fn(device, byref(offset))
+    _nvmlCheckReturn(ret)
+    return offset.value
+
+def nvmlDeviceSetMemClkVfOffset(device, offset):
+    c_offset = c_int32(offset)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetMemClkVfOffset")
+    ret = fn(device, c_offset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetMemClkMinMaxVfOffset(device, minOffset, maxOffset):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetMemClkMinMaxVfOffset")
+    ret = fn(device, minOffset, maxOffset)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemSetConfComputeGpusReadyState(state):
+    c_state = c_uint(state)
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetConfComputeGpusReadyState")
+    ret = fn(c_state)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemGetConfComputeGpusReadyState():
+    c_state = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeGpusReadyState")
+    ret = fn(byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state.value
+
+def nvmlSystemGetConfComputeCapabilities():
+    c_ccSysCaps = c_nvmlConfComputeSystemCaps_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeCapabilities")
+    ret = fn(byref(c_ccSysCaps))
+    _nvmlCheckReturn(ret)
+    return c_ccSysCaps
+
+def nvmlSystemGetConfComputeState():
+    c_state = c_nvmlConfComputeSystemState_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeState")
+    ret = fn(byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state
+
+def nvmlSystemGetConfComputeSettings(settings):
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeSettings")
+    return fn(settings)
+
+def nvmlDeviceSetConfComputeUnprotectedMemSize(device, c_ccMemSize):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetConfComputeUnprotectedMemSize")
+    ret = fn(device, c_ccMemSize)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetConfComputeMemSizeInfo(device):
+    c_ccMemSize = c_nvmlConfComputeMemSizeInfo_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeMemSizeInfo")
+    ret = fn(device, byref(c_ccMemSize))
+    _nvmlCheckReturn(ret)
+    return c_ccMemSize
+
+def nvmlDeviceGetConfComputeProtectedMemoryUsage(device):
+    c_memory = c_nvmlMemory_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeProtectedMemoryUsage")
+    ret = fn(device, byref(c_memory))
+    _nvmlCheckReturn(ret)
+    return c_memory
+
+def nvmlDeviceGetConfComputeGpuCertificate(device):
+    c_cert = c_nvmlConfComputeGpuCertificate_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeGpuCertificate")
+    ret = fn(device, byref(c_cert))
+    _nvmlCheckReturn(ret)
+    return c_cert
+
+def nvmlDeviceGetConfComputeGpuAttestationReport(device, c_nonce):
+    c_attestReport = c_nvmlConfComputeGpuAttestationReport_t()
+    c_nonce_arr = (c_uint8 * len(c_nonce))(*(c_nonce))
+    setattr(c_attestReport, 'nonce', c_nonce_arr)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetConfComputeGpuAttestationReport")
+    ret = fn(device, byref(c_attestReport))
+    _nvmlCheckReturn(ret)
+    return c_attestReport
+
+def nvmlSystemSetConfComputeKeyRotationThresholdInfo(max_atk_adv):
+    c_keyRotationThrInfo = c_nvmlConfComputeSetKeyRotationThresholdInfo_t(0)
+    c_keyRotationThrInfo.version = ConfComputeSetKeyRotationThresholdInfo_v1
+    c_keyRotationThrInfo.maxAttackerAdvantage = max_atk_adv
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetConfComputeKeyRotationThresholdInfo")
+    ret = fn(byref(c_keyRotationThrInfo))
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemGetConfComputeKeyRotationThresholdInfo():
+    c_keyRotationThrInfo = c_nvmlConfComputeGetKeyRotationThresholdInfo_t(0)
+    c_keyRotationThrInfo.version = ConfComputeGetKeyRotationThresholdInfo_v1
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetConfComputeKeyRotationThresholdInfo")
+    ret = fn(byref(c_keyRotationThrInfo))
+    _nvmlCheckReturn(ret)
+    return c_keyRotationThrInfo
+
+## GPM ##
+#########
+
+## Enums/defines
+
+#### GPM Metric Identifiers
+MXSMLEX_GPM_METRIC_GRAPHICS_UTIL           = 1 # Percentage of time any compute/graphics app was active on the GPU. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_SM_UTIL                 = 2 # Percentage of SMs that were busy. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_SM_OCCUPANCY            = 3 # Percentage of warps that were active vs theoretical maximum. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_INTEGER_UTIL            = 4 # Percentage of time the GPU's SMs were doing integer operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_ANY_TENSOR_UTIL         = 5 # Percentage of time the GPU's SMs were doing ANY tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_DFMA_TENSOR_UTIL        = 6 # Percentage of time the GPU's SMs were doing DFMA tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_HMMA_TENSOR_UTIL        = 7 # Percentage of time the GPU's SMs were doing HMMA tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_IMMA_TENSOR_UTIL        = 9 # Percentage of time the GPU's SMs were doing IMMA tensor operations. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_DRAM_BW_UTIL            = 10 # Percentage of DRAM bw used vs theoretical maximum. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_FP64_UTIL               = 11 # Percentage of time the GPU's SMs were doing non-tensor FP64 math. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_FP32_UTIL               = 12 # Percentage of time the GPU's SMs were doing non-tensor FP32 math. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_FP16_UTIL               = 13 # Percentage of time the GPU's SMs were doing non-tensor FP16 math. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_PCIE_TX_PER_SEC         = 20 # PCIe traffic from this GPU in MiB/sec
+MXSMLEX_GPM_METRIC_PCIE_RX_PER_SEC         = 21 # PCIe traffic to this GPU in MiB/sec
+MXSMLEX_GPM_METRIC_NVDEC_0_UTIL            = 30 # Percent utilization of NVDEC 0. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_1_UTIL            = 31 # Percent utilization of NVDEC 1. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_2_UTIL            = 32 # Percent utilization of NVDEC 2. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_3_UTIL            = 33 # Percent utilization of NVDEC 3. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_4_UTIL            = 34 # Percent utilization of NVDEC 4. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_5_UTIL            = 35 # Percent utilization of NVDEC 5. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_6_UTIL            = 36 # Percent utilization of NVDEC 6. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVDEC_7_UTIL            = 37 # Percent utilization of NVDEC 7. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_0_UTIL            = 40 # Percent utilization of NVJPG 0. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_1_UTIL            = 41 # Percent utilization of NVJPG 1. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_2_UTIL            = 42 # Percent utilization of NVJPG 2. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_3_UTIL            = 43 # Percent utilization of NVJPG 3. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_4_UTIL            = 44 # Percent utilization of NVJPG 4. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_5_UTIL            = 45 # Percent utilization of NVJPG 5. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_6_UTIL            = 46 # Percent utilization of NVJPG 6. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVJPG_7_UTIL            = 47 # Percent utilization of NVJPG 7. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVOFA_0_UTIL            = 50 # Percent utilization of NVOFA 0. 0.0 - 100.0
+MXSMLEX_GPM_METRIC_NVLINK_TOTAL_RX_PER_SEC = 60 # NvLink read bandwidth for all links in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_TOTAL_TX_PER_SEC = 61 # NvLink write bandwidth for all links in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L0_RX_PER_SEC    = 62 # NvLink read bandwidth for link 0 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L0_TX_PER_SEC    = 63 # NvLink write bandwidth for link 0 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L1_RX_PER_SEC    = 64 # NvLink read bandwidth for link 1 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L1_TX_PER_SEC    = 65 # NvLink write bandwidth for link 1 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L2_RX_PER_SEC    = 66 # NvLink read bandwidth for link 2 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L2_TX_PER_SEC    = 67 # NvLink write bandwidth for link 2 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L3_RX_PER_SEC    = 68 # NvLink read bandwidth for link 3 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L3_TX_PER_SEC    = 69 # NvLink write bandwidth for link 3 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L4_RX_PER_SEC    = 70 # NvLink read bandwidth for link 4 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L4_TX_PER_SEC    = 71 # NvLink write bandwidth for link 4 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L5_RX_PER_SEC    = 72 # NvLink read bandwidth for link 5 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L5_TX_PER_SEC    = 73 # NvLink write bandwidth for link 5 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L6_RX_PER_SEC    = 74 # NvLink read bandwidth for link 6 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L6_TX_PER_SEC    = 75 # NvLink write bandwidth for link 6 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L7_RX_PER_SEC    = 76 # NvLink read bandwidth for link 7 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L7_TX_PER_SEC    = 77 # NvLink write bandwidth for link 7 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L8_RX_PER_SEC    = 78 # NvLink read bandwidth for link 8 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L8_TX_PER_SEC    = 79 # NvLink write bandwidth for link 8 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L9_RX_PER_SEC    = 80 # NvLink read bandwidth for link 9 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L9_TX_PER_SEC    = 81 # NvLink write bandwidth for link 9 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L10_RX_PER_SEC   = 82 # NvLink read bandwidth for link 10 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L10_TX_PER_SEC   = 83 # NvLink write bandwidth for link 10 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L11_RX_PER_SEC   = 84 # NvLink read bandwidth for link 11 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L11_TX_PER_SEC   = 85 # NvLink write bandwidth for link 11 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L12_RX_PER_SEC   = 86 # NvLink read bandwidth for link 12 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L12_TX_PER_SEC   = 87 # NvLink write bandwidth for link 12 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L13_RX_PER_SEC   = 88 # NvLink read bandwidth for link 13 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L13_TX_PER_SEC   = 89 # NvLink write bandwidth for link 13 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L14_RX_PER_SEC   = 90 # NvLink read bandwidth for link 14 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L14_TX_PER_SEC   = 91 # NvLink write bandwidth for link 14 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L15_RX_PER_SEC   = 92 # NvLink read bandwidth for link 15 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L15_TX_PER_SEC   = 93 # NvLink write bandwidth for link 15 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L16_RX_PER_SEC   = 94 # NvLink read bandwidth for link 16 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L16_TX_PER_SEC   = 95 # NvLink write bandwidth for link 16 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L17_RX_PER_SEC   = 96 # NvLink read bandwidth for link 17 in MiB/sec
+MXSMLEX_GPM_METRIC_NVLINK_L17_TX_PER_SEC   = 97 # NvLink write bandwidth for link 17 in MiB/sec
+MXSMLEX_GPM_METRIC_MAX                     = 98
+
+## Structs
+
+class c_nvmlUnitInfo_t(_PrintableStructure):
+    _fields_ = [
+        ('name', c_char * 96),
+        ('id', c_char * 96),
+        ('serial', c_char * 96),
+        ('firmwareVersion', c_char * 96),
+    ]
+
+class struct_c_nvmlGpmSample_t(Structure):
+    pass # opaque handle
+c_nvmlGpmSample_t = POINTER(struct_c_nvmlGpmSample_t)
+
+class c_metricInfo_t(Structure):
+    _fields_ = [
+        ("shortName", c_char_p),
+        ("longName", c_char_p),
+        ("unit", c_char_p),
+    ]
+
+class c_nvmlGpmMetric_t(_PrintableStructure):
+    _fields_ = [
+        ('metricId', c_uint),
+        ('nvmlReturn', _nvmlReturn_t),
+        ('value', c_double),
+        ('metricInfo', c_metricInfo_t)
+    ]
+
+class c_nvmlGpmMetricsGet_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('numMetrics', c_uint),
+        ('sample1', c_nvmlGpmSample_t),
+        ('sample2', c_nvmlGpmSample_t),
+        ('metrics', c_nvmlGpmMetric_t * MXSMLEX_GPM_METRIC_MAX)
+    ]
+
+MXSMLEX_GPM_METRICS_GET_VERSION = 1
+
+class c_nvmlGpmSupport_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('isSupportedDevice', c_uint),
+    ]
+
+MXSMLEX_GPM_SUPPORT_VERSION = 1
+
+## Functions
+
+def nvmlGpmMetricsGet(metricsGet):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmMetricsGet")
+    ret = fn(byref(metricsGet))
+    _nvmlCheckReturn(ret)
+    return metricsGet
+
+def nvmlGpmSampleFree(gpmSample):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSampleFree")
+    ret = fn(gpmSample)
+    _nvmlCheckReturn(ret)
+    return
+
+def nvmlGpmSampleAlloc():
+    gpmSample = c_nvmlGpmSample_t()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSampleAlloc")
+    ret = fn(byref(gpmSample))
+    _nvmlCheckReturn(ret)
+    return gpmSample
+
+def nvmlGpmSampleGet(device, gpmSample):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSampleGet")
+    ret = fn(device, gpmSample)
+    _nvmlCheckReturn(ret)
+    return gpmSample
+
+def nvmlGpmMigSampleGet(device, gpuInstanceId, gpmSample):
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmMigSampleGet")
+    ret = fn(device, gpuInstanceId, gpmSample)
+    _nvmlCheckReturn(ret)
+    return gpmSample
+
+def nvmlGpmQueryDeviceSupport(device):
+    gpmSupport = c_nvmlGpmSupport_t()
+    gpmSupport.version = MXSMLEX_GPM_SUPPORT_VERSION
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmQueryDeviceSupport")
+    ret = fn(device, byref(gpmSupport))
+    _nvmlCheckReturn(ret)
+    return gpmSupport
+
+def nvmlGpmSetStreamingEnabled(device, state):
+    c_state = c_uint(state)
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmSetStreamingEnabled")
+    ret = fn(device, c_state)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlGpmQueryIfStreamingEnabled(device):
+    c_state = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExGpmQueryIfStreamingEnabled")
+    ret = fn(device, byref(c_state))
+    _nvmlCheckReturn(ret)
+    return c_state.value
+
+# Low Power Structure and Function
+
+MXSMLEX_NVLINK_POWER_STATE_HIGH_SPEED    = 0x0
+MXSMLEX_NVLINK_POWER_STATE_LOW           = 0x1
+
+MXSMLEX_NVLINK_LOW_POWER_THRESHOLD_MIN   = 0x1
+MXSMLEX_NVLINK_LOW_POWER_THRESHOLD_MAX   = 0x1FFF
+MXSMLEX_NVLINK_LOW_POWER_THRESHOLD_RESET = 0xFFFFFFFF
+
+class c_nvmlNvLinkPowerThres_t(Structure):
+    _fields_ = [
+        ("lowPwrThreshold", c_uint),
+    ]
+
+def nvmlDeviceSetNvLinkDeviceLowPowerThreshold(device, l1threshold):
+    c_info = c_nvmlNvLinkPowerThres_t()
+    c_info.lowPwrThreshold = l1threshold
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetNvLinkDeviceLowPowerThreshold")
+    ret = fn(device, byref(c_info))
+    _nvmlCheckReturn(ret)
+    return ret 
+
+_nvmlGpuFabricState_t = c_uint
+MXSMLEX_GPU_FABRIC_STATE_NOT_SUPPORTED = 0
+MXSMLEX_GPU_FABRIC_STATE_NOT_STARTED   = 1
+MXSMLEX_GPU_FABRIC_STATE_IN_PROGRESS   = 2
+MXSMLEX_GPU_FABRIC_STATE_COMPLETED     = 3
+
+class c_nvmlGpuFabricInfo_t(_PrintableStructure):
+    _fields_ = [
+        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
+        ("status", _nvmlReturn_t),
+        ("cliqueId", c_uint32),
+        ("state", _nvmlGpuFabricState_t)
+    ]
+
+nvmlGpuFabricInfo_v2 = 0x02000024
+
+class c_nvmlGpuFabricInfoV_t(_PrintableStructure):
+    _fields_ = [
+        ("version", c_uint),
+        ("clusterUuid", c_char * MXSMLEX_DEVICE_UUID_BUFFER_SIZE),
+        ("status", _nvmlReturn_t),
+        ("cliqueId", c_uint32),
+        ("state", _nvmlGpuFabricState_t),
+        ("healthMask", c_uint32)
+    ]
+
+    def __init__(self):
+        super(c_nvmlGpuFabricInfoV_t, self).__init__(version=nvmlGpuFabricInfo_v2)
+
+def nvmlDeviceGetGpuFabricInfo(device, gpuFabricInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfo");
+    ret = fn(device, gpuFabricInfo)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlDeviceGetGpuFabricInfoV(device, gpuFabricInfo):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetGpuFabricInfoV");
+    ret = fn(device, gpuFabricInfo)
+    _nvmlCheckReturn(ret)
+    return ret
+
+######################
+## Enums/defines
+#### MXSMLEX GPU NVLINK BW MODE
+MXSMLEX_GPU_NVLINK_BW_MODE_FULL      = 0x0
+MXSMLEX_GPU_NVLINK_BW_MODE_OFF       = 0x1
+MXSMLEX_GPU_NVLINK_BW_MODE_MIN       = 0x2
+MXSMLEX_GPU_NVLINK_BW_MODE_HALF      = 0x3
+MXSMLEX_GPU_NVLINK_BW_MODE_3QUARTER  = 0x4
+MXSMLEX_GPU_NVLINK_BW_MODE_COUNT     = 0x5
+
+def nvmlSystemSetNvlinkBwMode(mode):
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemSetNvlinkBwMode")
+    ret = fn(mode)
+    _nvmlCheckReturn(ret)
+    return ret
+
+def nvmlSystemGetNvlinkBwMode():
+    mode = c_uint()
+    fn = _nvmlGetFunctionPointer("mxSmlExSystemGetNvlinkBwMode")
+    ret = fn(byref(mode))
+    _nvmlCheckReturn(ret)
+    return mode.value
+
+_nvmlPowerScopeType_t = c_uint
+MXSMLEX_POWER_SCOPE_GPU     = 0
+MXSMLEX_POWER_SCOPE_MODULE  = 1
+MXSMLEX_POWER_SCOPE_MEMORY  = 2
+
+class c_nvmlPowerValue_v2_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('powerScope', _nvmlPowerScopeType_t),
+        ('powerValueMw', c_uint),
+    ]
+    _fmt_ = {'<default>': "%d B"}
+
+nvmlPowerValue_v2 = 0x0200000C
+
+def nvmlDeviceSetPowerManagementLimit_v2(device, powerScope, powerLimit, version=nvmlPowerValue_v2):
+    c_powerScope = _nvmlPowerScopeType_t(powerScope)
+    c_powerValue = c_nvmlPowerValue_v2_t()
+    c_powerValue.version = c_uint(version)
+    c_powerValue.powerScope = c_powerScope
+    c_powerValue.powerValueMw = c_uint(powerLimit)
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceSetPowerManagementLimit_v2")
+    ret = fn(device, byref(c_powerValue))
+    return ret
+
+class c_nvmlEccSramErrorStatus_v1_t(_PrintableStructure):
+    _fields_ = [
+        ('version', c_uint),
+        ('aggregateUncParity', c_ulonglong),
+        ('aggregateUncSecDed', c_ulonglong),
+        ('aggregateCor', c_ulonglong),
+        ('volatileUncParity', c_ulonglong),
+        ('volatileUncSecDed', c_ulonglong),
+        ('volatileCor', c_ulonglong),
+        ('aggregateUncBucketL2', c_ulonglong),
+        ('aggregateUncBucketSm', c_ulonglong),
+        ('aggregateUncBucketPcie', c_ulonglong),
+        ('aggregateUncBucketMcu', c_ulonglong),
+        ('aggregateUncBucketOther', c_ulonglong),
+        ('bThresholdExceeded', c_uint)
+    ]
+
+    def __init__(self):
+        super(c_nvmlEccSramErrorStatus_v1_t, self).__init__(version=nvmlEccSramErrorStatus_v1)
+
+nvmlEccSramErrorStatus_v1 = 0x1000068
+
+def nvmlDeviceGetSramEccErrorStatus(device, status):
+    fn = _nvmlGetFunctionPointer("mxSmlExDeviceGetSramEccErrorStatus")
+    ret = fn(device, status)
+    _nvmlCheckReturn(ret)
+    return ret
+
diff --git a/vllm/utils.py b/vllm/utils.py
index 73726bb9a..01fdda9a4 100644
--- a/vllm/utils.py
+++ b/vllm/utils.py
@@ -809,6 +809,7 @@ def create_kv_caches_with_random(
     model_dtype: Optional[Union[str, torch.dtype]] = None,
     seed: Optional[int] = None,
     device: Optional[str] = "cuda",
+    new_layerout:Optional[bool] = False,
 ) -> tuple[list[torch.Tensor], list[torch.Tensor]]:
 
     if cache_dtype == "fp8" and head_size % 16:
@@ -836,7 +837,12 @@ def create_kv_caches_with_random(
             raise ValueError(
                 f"Does not support key cache of type {cache_dtype}")
         key_caches.append(key_cache)
-
+        if new_layerout:
+            key_cache_new = torch.empty(size=key_cache_shape,
+                                        dtype=torch_dtype,
+                                        device=device)
+            key_caches.append(key_cache_new)
+            
     value_cache_shape = (num_blocks, num_heads, head_size, block_size)
     value_caches: list[torch.Tensor] = []
     for _ in range(num_layers):
@@ -851,6 +857,11 @@ def create_kv_caches_with_random(
             raise ValueError(
                 f"Does not support value cache of type {cache_dtype}")
         value_caches.append(value_cache)
+        if new_layerout:
+            key_cache_new = torch.empty(size=key_cache_shape,
+                                        dtype=torch_dtype,
+                                        device=device)
+            key_caches.append(key_cache_new)
     return key_caches, value_caches
 
 
@@ -1047,7 +1058,7 @@ def find_nccl_library() -> str:
             so_file)
     else:
         if torch.version.cuda is not None:
-            so_file = "libnccl.so.2"
+            so_file = "libmccl.so"
         elif torch.version.hip is not None:
             so_file = "librccl.so.1"
         else:
@@ -2056,12 +2067,12 @@ def direct_register_custom_op(
 
     if not supports_custom_op():
         from vllm.platforms import current_platform
-        assert not current_platform.is_cuda_alike(), (
-            "cuda platform needs torch>=2.4 to support custom op, "
-            "chances are you are using an old version of pytorch "
-            "or a custom build of pytorch. It is recommended to "
-            "use vLLM in a fresh new environment and let it install "
-            "the required dependencies.")
+        # assert not current_platform.is_cuda_alike(), (
+        #     "cuda platform needs torch>=2.4 to support custom op, "
+        #     "chances are you are using an old version of pytorch "
+        #     "or a custom build of pytorch. It is recommended to "
+        #     "use vLLM in a fresh new environment and let it install "
+        #     "the required dependencies.")
         return
 
     import torch.library
@@ -2483,7 +2494,7 @@ def import_pynvml():
     After all the troubles, we decide to copy the official `pynvml`
     module to our codebase, and use it directly.
     """
-    import vllm.third_party.pynvml as pynvml
+    import vllm.third_party.pymcml as pynvml
     return pynvml
 
 
diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 41bb9aba2..16ef6abd9 100755
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -10,9 +10,12 @@ from vllm import _custom_ops as ops
 from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
                                               AttentionMetadata, AttentionType,
                                               is_quantized_kv_cache)
+from vllm.attention.layer import Attention
 from vllm.attention.ops.merge_attn_states import merge_attn_states
-from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
-                                           get_flash_attn_version)
+# from vllm.attention.utils.fa_utils import (flash_attn_supports_fp8,
+#                                        get_flash_attn_version)
+
+from vllm.config import VllmConfig, get_layers_from_vllm_config
 from vllm.logger import init_logger
 from vllm.platforms import current_platform
 from vllm.utils import cdiv
@@ -23,11 +26,15 @@ if TYPE_CHECKING:
     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
 
 if current_platform.is_cuda():
-    from vllm.vllm_flash_attn import (flash_attn_varlen_func,
-                                      get_scheduler_metadata)
+    from flash_attn import (flash_attn_varlen_func, flash_attn_with_kvcache)
 
 logger = init_logger(__name__)
 
+def flash_attn_supports_fp8() -> bool:
+    return False
+
+def get_flash_attn_version():
+    return None
 
 class FlashAttentionBackend(AttentionBackend):
 
@@ -35,7 +42,7 @@ class FlashAttentionBackend(AttentionBackend):
 
     @staticmethod
     def get_supported_head_sizes() -> list[int]:
-        return [32, 64, 96, 128, 160, 192, 224, 256]
+        return [32, 64, 80, 96, 112, 128, 160, 192, 224, 256]
 
     @staticmethod
     def get_name() -> str:
@@ -76,10 +83,28 @@ class FlashAttentionMetadata:
     #                                   |-- query_len ---|
 
     num_actual_tokens: int  # Number of tokens excluding padding.
+    block_table: torch.Tensor
+    
     max_query_len: int
     query_start_loc: torch.Tensor
     max_seq_len: int
     seq_lens: torch.Tensor
+
+    # For handling prefill decode split
+    num_decodes: int
+    num_decode_tokens: int
+    decode_query_start_loc: torch.Tensor
+    decode_max_seq_len: int
+    decode_seq_lens: torch.Tensor
+    decode_block_table: torch.Tensor
+
+    num_prefills: int
+    num_prefill_tokens: int
+    prefill_query_start_loc: torch.Tensor
+    prefill_max_seq_len: int
+    prefill_seq_lens: torch.Tensor
+    prefill_block_table: torch.Tensor
+
     block_table: torch.Tensor
     slot_mapping: torch.Tensor
 
@@ -276,6 +301,17 @@ def make_local_attention_virtual_batches(
         block_table_local
 
 
+def _get_sliding_window_configs(
+        vllm_config: VllmConfig) -> set[Optional[tuple[int, int]]]:
+    """Get the set of all sliding window configs used in the model."""
+    sliding_window_configs: set[Optional[tuple[int, int]]] = set()
+    layers = get_layers_from_vllm_config(vllm_config, Attention)
+    for layer in layers.values():
+        assert isinstance(layer.impl, FlashAttentionImpl)
+        sliding_window_configs.add(layer.impl.sliding_window)
+    return sliding_window_configs
+
+
 class FlashAttentionMetadataBuilder:
 
     def __init__(self, runner: "GPUModelRunner"):
@@ -290,38 +326,151 @@ class FlashAttentionMetadataBuilder:
         self.headdim = model_config.get_head_size()
         self.page_size = self.runner.block_size
 
+        # Sliding window size to be used with the AOT scheduler will be
+        # populated on first build() call.
+        self.aot_sliding_window: Optional[tuple[int, int]] = None
+
     def reorder_batch(self, input_batch: "InputBatch",
                       scheduler_output: "SchedulerOutput") -> bool:
-        return False
+        # We now want to reorder the batch so that the "decode" requests are and
+        # the front and the "prefill" requests are at the using the least amount
+        # swaps possible. (NOTE for now we loosely use "decode" to mean requests
+        # where attention is likely memory-bound and "prefill" to mean requests
+        # where attention is likely compute-bound, TODO(lucas): figure out a
+        # better naming here)
+        decodes = []
+        prefills = []
+        num_decode_tokens = 0
+        num_prefill_tokens = 0
+
+        for i, req_id in enumerate(input_batch.req_ids):
+            num_tokens = scheduler_output.num_scheduled_tokens[req_id]
+            # for now treat 1 scheduled token as "decode" even if its not,
+            # we should update this to something like < 8 in the future but
+            # currently the decode run only supports num_tokens = 1
+            if num_tokens == 1:
+                decodes.append(i)
+                num_decode_tokens += num_tokens
+            else:
+                prefills.append(i)
+                num_prefill_tokens += num_tokens
+
+        # We hope that this is fairly minimal since decodes
+        # should be around for a number of iterations so hopefully they are
+        # relatively stationary (and new request are generally appended to the
+        # persistent batch so already should be at the back)
+        # To achieve this we loop over the decodes in descending order and
+        # the prefills in ascending order. We swap decodes from the  "back"
+        # i.e. past where the last decode should be in the reodorered with
+        # prefills from the front of the batch.
+        # `decodes` and `prefills` are already in ascending order just based on
+        # the above loop
+        num_decodes = len(decodes)
+        num_prefills = len(prefills)
+        modified_batch = False
+
+        for i in range(1, min(num_decodes, num_prefills) + 1):
+            # If the decode is at the "back" of the batch, i, we can swap it
+            # with the prefill closest to the front of the batch
+            decode_idx = decodes[num_decodes - i]
+            if decode_idx < num_decodes:
+                break
+
+            input_batch.swap_states(prefills[i - 1], decode_idx)
+            modified_batch = True
+
+        # Save for next `build` call
+        # TODO(lucas): this is a bit of a hack, we should probably have a
+        # better way of doing this
+        self._num_decodes = num_decodes
+        self._num_prefills = num_prefills
+        self._num_decode_tokens = num_decode_tokens
+        self._num_prefill_tokens = num_prefill_tokens
+
+        return modified_batch
 
     def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
               common_prefix_len: int):
+        assert self._num_decodes + self._num_prefills == num_reqs
+        assert (self._num_decode_tokens +
+                self._num_prefill_tokens == num_actual_tokens)
+        
         max_seq_len = self.runner.seq_lens_np[:num_reqs].max()
         query_start_loc_cpu = self.runner.query_start_loc_cpu[:num_reqs + 1]
         query_start_loc = query_start_loc_cpu.to(self.runner.device,
                                                  non_blocking=True)
         seq_lens_cpu = self.runner.seq_lens_cpu[:num_reqs]
         seq_lens = seq_lens_cpu.to(self.runner.device, non_blocking=True)
+
+        # For handling prefill decode split
+        if self._num_decodes > 0:
+            decode_max_seq_len = self.runner.seq_lens_np[:self._num_decodes].max()
+            decode_query_start_loc_cpu = self.runner.query_start_loc_cpu[:self._num_decodes + 1]
+            decode_query_start_loc = decode_query_start_loc_cpu.to(self.runner.device,
+                                                    non_blocking=True)
+            decode_seq_lens_cpu = self.runner.seq_lens_cpu[:self._num_decodes]
+            decode_seq_lens = decode_seq_lens_cpu.to(self.runner.device, non_blocking=True)
+            decode_block_table = (
+                self.runner.input_batch.block_table.get_device_tensor()[:self._num_decodes])
+        else:
+            decode_max_seq_len = 0
+            decode_query_start_loc = None
+            decode_seq_lens = None
+            decode_block_table = None
+
+        if self._num_prefills > 0:
+            prefill_max_seq_len = self.runner.seq_lens_np[self._num_decodes:num_reqs].max()
+            prefill_query_start_loc_cpu = (self.runner.query_start_loc_cpu[self._num_decodes:num_reqs+1] - 
+                                            self.runner.query_start_loc_cpu[self._num_decodes])
+            prefill_query_start_loc = prefill_query_start_loc_cpu.to(self.runner.device,
+                                                    non_blocking=True)
+            prefill_seq_lens_cpu = self.runner.seq_lens_cpu[self._num_decodes:num_reqs]
+            prefill_seq_lens = prefill_seq_lens_cpu.to(self.runner.device, non_blocking=True)
+            prefill_block_table = (
+                self.runner.input_batch.block_table.get_device_tensor()[self._num_decodes:num_reqs])
+        else:
+            prefill_max_seq_len = 0
+            prefill_query_start_loc = None
+            prefill_seq_lens = None
+            prefill_block_table = None
+
         block_table = (
             self.runner.input_batch.block_table.get_device_tensor()[:num_reqs])
         slot_mapping = self.runner.slot_mapping_cpu[:num_actual_tokens].to(
             self.runner.device, non_blocking=True).long()
+        
+        if self.aot_sliding_window is None:
+            self.aot_sliding_window = (-1, -1)
+            # For the AOT scheduler we need the sliding window value to be
+            # constant for all layers to. We have to populate this on the first
+            # build() call so the layers are constructed (cannot populate)
+            # in __init__.
+            if self.aot_schedule:
+                sliding_window_configs = _get_sliding_window_configs(
+                    self.runner.vllm_config)
+                if len(sliding_window_configs) == 1:
+                    sliding_window_config = sliding_window_configs.pop()
+                    if sliding_window_config is not None:
+                        self.aot_sliding_window = sliding_window_config
+                elif len(sliding_window_configs) > 1:
+                    self.aot_schedule = False
 
         def schedule(batch_size, cu_query_lens, max_query_len, seqlens,
                      max_seq_len, causal):
-            if self.aot_schedule:
-                return get_scheduler_metadata(
-                    batch_size=batch_size,
-                    max_seqlen_q=max_query_len,
-                    max_seqlen_k=max_seq_len,
-                    cache_seqlens=seqlens,
-                    num_heads_q=self.num_heads_q,
-                    num_heads_kv=self.num_heads_kv,
-                    headdim=self.headdim,
-                    page_size=self.page_size,
-                    cu_seqlens_q=cu_query_lens,
-                    causal=causal,
-                )
+            # if self.aot_schedule:
+            #     return get_scheduler_metadata(
+            #         batch_size=batch_size,
+            #         max_seqlen_q=max_query_len,
+            #         max_seqlen_k=max_seq_len,
+            #         cache_seqlens=seqlens,
+            #         num_heads_q=self.num_heads_q,
+            #         num_heads_kv=self.num_heads_kv,
+            #         headdim=self.headdim,
+            #         page_size=self.page_size,
+            #         cu_seqlens_q=cu_query_lens,
+            #         causal=causal,
+            #         window_size=self.aot_sliding_window,
+            #     )
             return None
 
         # for local attention
@@ -403,6 +552,19 @@ class FlashAttentionMetadataBuilder:
             query_start_loc=query_start_loc,
             max_seq_len=max_seq_len,
             seq_lens=seq_lens,
+            # For handling prefill decode split
+            num_decodes=self._num_decodes,
+            num_decode_tokens=self._num_decode_tokens,
+            decode_query_start_loc=decode_query_start_loc,
+            decode_max_seq_len=decode_max_seq_len,
+            decode_seq_lens=decode_seq_lens,
+            decode_block_table=decode_block_table,
+            num_prefills=self._num_prefills,
+            num_prefill_tokens=self._num_prefill_tokens,
+            prefill_query_start_loc=prefill_query_start_loc,
+            prefill_max_seq_len=prefill_max_seq_len,
+            prefill_seq_lens=prefill_seq_lens,
+            prefill_block_table=prefill_block_table,
             block_table=block_table,
             slot_mapping=slot_mapping,
             use_cascade=use_cascade,
@@ -548,6 +710,43 @@ class FlashAttentionImpl(AttentionImpl):
         # Compute attention and update output up to `num_actual_tokens`.
         use_local_attn = \
             (self.use_irope and attn_metadata.local_attn_metadata is not None)
+        
+        # For handling prefill decode split
+        if not attn_metadata.use_cascade and not use_local_attn:
+            num_decode_tokens = attn_metadata.num_decode_tokens
+            if attn_metadata.num_prefills > 0:
+                cu_prefix_kv_lens = torch.tensor([0] + attn_metadata.prefill_seq_lens.tolist(), device=attn_metadata.prefill_seq_lens.device, dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
+                output[num_decode_tokens:num_actual_tokens] = flash_attn_varlen_func(
+                    q=query[num_decode_tokens:num_actual_tokens],
+                    k=key_cache,
+                    v=value_cache,
+                    block_table=attn_metadata.prefill_block_table,
+                    cu_seqlens_q=attn_metadata.prefill_query_start_loc,
+                    cu_seqlens_k=cu_prefix_kv_lens,
+                    max_seqlen_q=attn_metadata.max_query_len,
+                    max_seqlen_k=attn_metadata.prefill_max_seq_len,
+                    softmax_scale=self.scale,
+                    causal=True,
+                    window_size=self.sliding_window,
+                    alibi_slopes=self.alibi_slopes,
+                    softcap=self.logits_soft_cap,
+                )
+            if attn_metadata.num_decodes > 0:
+                # Use flash_attn_with_kvcache for normal decoding.
+                decode_query = query[:num_decode_tokens]
+                output[:num_decode_tokens] = flash_attn_with_kvcache(
+                    q=decode_query.unsqueeze(1),
+                    k_cache=key_cache,
+                    v_cache=value_cache,
+                    block_table=attn_metadata.decode_block_table,
+                    cache_seqlens=attn_metadata.decode_seq_lens,
+                    softmax_scale=self.scale,
+                    causal=True,
+                    window_size=self.sliding_window,
+                    alibi_slopes=self.alibi_slopes,
+                    softcap=self.logits_soft_cap,
+                ).squeeze(1)
+            return output
 
         if not attn_metadata.use_cascade or use_local_attn:
             if use_local_attn:
@@ -567,16 +766,17 @@ class FlashAttentionImpl(AttentionImpl):
                 block_table = attn_metadata.block_table
                 scheduler_metadata = attn_metadata.scheduler_metadata
 
-            descale_shape = (cu_seqlens_q.shape[0] - 1, key.shape[1])
+            cu_prefix_kv_lens = torch.tensor([0] + attn_metadata.seq_lens.tolist(), device=attn_metadata.seq_lens.device, dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
 
-            flash_attn_varlen_func(
+            output[:num_actual_tokens] = flash_attn_varlen_func(
                 q=query[:num_actual_tokens],
                 k=key_cache,
                 v=value_cache,
-                out=output[:num_actual_tokens],
+                # out=output[:num_actual_tokens],
                 cu_seqlens_q=cu_seqlens_q,
                 max_seqlen_q=max_seqlen_q,
-                seqused_k=seqused_k,
+                # seqused_k=seqused_k,
+                cu_seqlens_k=cu_prefix_kv_lens,
                 max_seqlen_k=max_seqlen_k,
                 softmax_scale=self.scale,
                 causal=True,
@@ -584,11 +784,11 @@ class FlashAttentionImpl(AttentionImpl):
                 window_size=self.sliding_window,
                 block_table=block_table,
                 softcap=self.logits_soft_cap,
-                scheduler_metadata=scheduler_metadata,
-                fa_version=self.vllm_flash_attn_version,
-                q_descale=layer._q_scale.expand(descale_shape),
-                k_descale=layer._k_scale.expand(descale_shape),
-                v_descale=layer._v_scale.expand(descale_shape),
+                # scheduler_metadata=scheduler_metadata,
+                # fa_version=self.vllm_flash_attn_version,
+                # q_descale=layer._q_scale.expand(descale_shape),
+                # k_descale=layer._k_scale.expand(descale_shape),
+                # v_descale=layer._v_scale.expand(descale_shape),
             )
             return output
 
@@ -727,11 +927,13 @@ def cascade_attention(
     descale_shape = (cu_prefix_query_lens.shape[0] - 1, key_cache.shape[-2])
 
     # Process shared prefix.
+    cu_prefix_kv_lens = torch.tensor([0] + prefix_kv_lens.tolist(), device=prefix_kv_lens.device, dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
     prefix_output, prefix_lse = flash_attn_varlen_func(
         q=query,
         k=key_cache,
         v=value_cache,
         cu_seqlens_q=cu_prefix_query_lens,
+        cu_seqlens_k=cu_prefix_kv_lens,
         seqused_k=prefix_kv_lens,
         max_seqlen_q=num_tokens,
         max_seqlen_k=common_prefix_len,
@@ -741,25 +943,27 @@ def cascade_attention(
         block_table=block_table[:1],
         softcap=logits_soft_cap,
         return_softmax_lse=True,
-        scheduler_metadata=prefix_scheduler_metadata,
-        fa_version=fa_version,
-        q_descale=q_descale.expand(descale_shape)
-        if q_descale is not None else None,
-        k_descale=k_descale.expand(descale_shape)
-        if k_descale is not None else None,
-        v_descale=v_descale.expand(descale_shape)
-        if v_descale is not None else None,
+        # scheduler_metadata=prefix_scheduler_metadata,
+        # fa_version=fa_version,
+        # q_descale=q_descale.expand(descale_shape)
+        # if q_descale is not None else None,
+        # k_descale=k_descale.expand(descale_shape)
+        # if k_descale is not None else None,
+        # v_descale=v_descale.expand(descale_shape)
+        # if v_descale is not None else None,
     )
 
     descale_shape = (cu_query_lens.shape[0] - 1, key_cache.shape[-2])
 
     # Process suffix per query.
+    cu_suffix_kv_lens = torch.tensor([0] + suffix_kv_lens.tolist(), device=suffix_kv_lens.device, dtype=torch.int32).cumsum(dim=0, dtype=torch.int32)
     suffix_output, suffix_lse = flash_attn_varlen_func(
         q=query,
         k=key_cache,
         v=value_cache,
         cu_seqlens_q=cu_query_lens,
-        seqused_k=suffix_kv_lens,
+        # seqused_k=suffix_kv_lens,
+        cu_seqlens_k=cu_suffix_kv_lens,
         max_seqlen_q=max_query_len,
         max_seqlen_k=max_kv_len - common_prefix_len,
         softmax_scale=softmax_scale,
@@ -769,13 +973,13 @@ def cascade_attention(
         softcap=logits_soft_cap,
         return_softmax_lse=True,
         scheduler_metadata=suffix_scheduler_metadata,
-        fa_version=fa_version,
-        q_descale=q_descale.expand(descale_shape)
-        if q_descale is not None else None,
-        k_descale=k_descale.expand(descale_shape)
-        if k_descale is not None else None,
-        v_descale=v_descale.expand(descale_shape)
-        if v_descale is not None else None,
+        # fa_version=fa_version,
+        # q_descale=q_descale.expand(descale_shape)
+        # if q_descale is not None else None,
+        # k_descale=k_descale.expand(descale_shape)
+        # if k_descale is not None else None,
+        # v_descale=v_descale.expand(descale_shape)
+        # if v_descale is not None else None,
     )
 
     # Merge prefix and suffix outputs, and store the result in output.
diff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py
index bce446bd2..1016b974b 100755
--- a/vllm/v1/attention/backends/flashinfer.py
+++ b/vllm/v1/attention/backends/flashinfer.py
@@ -7,8 +7,8 @@ from typing import TYPE_CHECKING, Any, Optional
 
 import torch
 from flashinfer import (BatchDecodeWithPagedKVCacheWrapper,
-                        BatchPrefillWithPagedKVCacheWrapper,
-                        MultiLevelCascadeAttentionWrapper)
+                        BatchPrefillWithPagedKVCacheWrapper)
+                        #MultiLevelCascadeAttentionWrapper
 
 import vllm.envs as envs
 from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
@@ -181,7 +181,7 @@ class FlashInferMetadata:
 
     prefill_wrapper: Optional[BatchPrefillWithPagedKVCacheWrapper] = None
     decode_wrapper: Optional[BatchDecodeWithPagedKVCacheWrapper] = None
-    cascade_wrapper: Optional[MultiLevelCascadeAttentionWrapper] = None
+    # cascade_wrapper: Optional[MultiLevelCascadeAttentionWrapper] = None
 
     # For logging.
     num_input_tokens: int = 0  # Number of tokens including padding.
@@ -303,17 +303,17 @@ class FlashInferMetadataBuilder:
                 use_tensor_cores=use_tensor_cores)
         return self._decode_wrapper
 
-    def _get_cascade_wrapper(self):
-        if self._cascade_wrapper is None:
-            self._cascade_wrapper = MultiLevelCascadeAttentionWrapper(
-                2, self._get_workspace_buffer(), "NHD")
-        return self._cascade_wrapper
+    # def _get_cascade_wrapper(self):
+    #     if self._cascade_wrapper is None:
+    #         self._cascade_wrapper = MultiLevelCascadeAttentionWrapper(
+    #             2, self._get_workspace_buffer(), "NHD")
+    #     return self._cascade_wrapper
 
     def _plan(self, attn_metadata: FlashInferMetadata):
         if self.global_hyperparameters is None:
             self.global_hyperparameters = infer_global_hyperparameters(
                 get_per_layer_parameters(self.vllm_config))
-        if attn_metadata.use_cascade:
+        if attn_metadata.use_cascade and False: # not supported
             attn_metadata.cascade_wrapper = self._get_cascade_wrapper()
             attn_metadata.cascade_wrapper.plan(
                 [attn_metadata.shared_qo_indptr, attn_metadata.qo_indptr],
@@ -484,6 +484,7 @@ class FlashInferMetadataBuilder:
         return attn_metadata
 
     def use_cascade_attention(self, *args, **kwargs) -> bool:
+        return False # flashinfer not support 
         if self.runner.kv_cache_dtype != self.runner.model_config.dtype:
             # TODO: The cascade wrapper currently does not support setting
             # kv cache dtype to something different from query dtype.
@@ -590,11 +591,11 @@ class FlashInferImpl(AttentionImpl):
         output_padded = output
         output = output[:num_actual_tokens]
 
-        if attn_metadata.use_cascade:
-            # Cascade attention (rare case).
-            assert attn_metadata.cascade_wrapper is not None
-            output.copy_(attn_metadata.cascade_wrapper.run(query, kv_cache))
-            return output
+        # if attn_metadata.use_cascade:
+        #     # Cascade attention (rare case).
+        #     assert attn_metadata.cascade_wrapper is not None
+        #     output.copy_(attn_metadata.cascade_wrapper.run(query, kv_cache))
+        #     return output
 
         num_decode_tokens = attn_metadata.num_decode_tokens
         num_prefill_tokens = attn_metadata.num_prefill_tokens
diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py
index e6e483bae..31b0857ff 100644
--- a/vllm/v1/attention/backends/mla/common.py
+++ b/vllm/v1/attention/backends/mla/common.py
@@ -197,7 +197,6 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionLayer,
                                               MLAAttentionImpl)
 from vllm.attention.backends.utils import get_mla_dims
 from vllm.attention.ops.merge_attn_states import merge_attn_states
-from vllm.attention.utils.fa_utils import get_flash_attn_version
 from vllm.logger import init_logger
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
                                                LinearBase, RowParallelLinear,
@@ -218,6 +217,8 @@ if TYPE_CHECKING:
     from vllm.v1.core.sched.output import SchedulerOutput
     from vllm.v1.worker.gpu_input_batch import InputBatch
     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
+    
+from vllm import envs
 
 logger = init_logger(__name__)
 
@@ -352,7 +353,7 @@ class MLACommonMetadataBuilder(Generic[M]):
         self.num_heads = model_config.get_num_attention_heads(
             runner.parallel_config)
         self.mla_dims = get_mla_dims(model_config)
-        self.aot_schedule = is_vllm_fa and (get_flash_attn_version() == 3)
+        self.aot_schedule = False # is_vllm_fa and (get_flash_attn_version() == 3)
 
         # Dont try to access the runner on AMD
         if self.aot_schedule:
@@ -497,11 +498,12 @@ class MLACommonMetadataBuilder(Generic[M]):
                 max_context_chunk = (self.chunked_prefill_workspace_size //
                                      num_prefills_with_context_cpu)
 
-                # align max_context_chunk to page_size by rounding down,
-                # currently the `gather_cache` kernel cannot handle
-                # `context_chunk_starts` that are not aligned to page_size
-                max_context_chunk = round_down(max_context_chunk,
-                                               self.page_size)
+                if self.aot_schedule:
+                    # align max_context_chunk to page_size by rounding down,
+                    # currently the `gather_cache` kernel cannot handle
+                    # `context_chunk_starts` that are not aligned to page_size
+                    max_context_chunk = round_down(max_context_chunk,
+                                                   self.page_size)
 
                 assert max_context_chunk > 0
                 num_chunks = cdiv(max_context_len_cpu, max_context_chunk)
@@ -631,25 +633,25 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
         self.q_proj = q_proj
         self.kv_b_proj = kv_b_proj
         self.o_proj = o_proj
-        self.vllm_flash_attn_version = get_flash_attn_version()
+        # self.vllm_flash_attn_version = get_flash_attn_version()
 
         # Handle the differences between the flash_attn_varlen from flash_attn
         # and the one from vllm_flash_attn. The former is used on RoCM and the
         # latter has an additional parameter to control FA2 vs FA3
         self.flash_attn_varlen_func = flash_attn_varlen_func
-        self.vllm_flash_attn_version = get_flash_attn_version()
-        if self.vllm_flash_attn_version is not None:
-            self.flash_attn_varlen_func = \
-                functools.partial(flash_attn_varlen_func,
-                                  fa_version=self.vllm_flash_attn_version)
+        # if self.vllm_flash_attn_version is not None:
+        #     self.flash_attn_varlen_func = \
+        #         functools.partial(flash_attn_varlen_func,
+        #                           fa_version=self.vllm_flash_attn_version)
 
         # For MLA the v head dim is smaller than qk head dim so we pad out
         # v with 0s to match the qk head dim for attention backends that do
         # not support different headdims
         # We don't need to pad V if we are on a hopper system with FA3
-        self._pad_v = self.vllm_flash_attn_version is None or not (
-            self.vllm_flash_attn_version == 3
-            and current_platform.get_device_capability()[0] == 9)
+        self._pad_v = True 
+        # self.vllm_flash_attn_version is None or not (
+        #     self.vllm_flash_attn_version == 3
+        #     and current_platform.get_device_capability()[0] == 9)
 
     def _flash_attn_varlen_diff_headdims(self,
                                          q,
@@ -667,7 +669,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
             q=q,
             k=k,
             v=maybe_padded_v,
-            return_softmax_lse=return_softmax_lse,
+            return_attn_probs=return_softmax_lse,
             softmax_scale=softmax_scale,
             **kwargs,
         )
@@ -679,7 +681,8 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
 
         # unpad if necessary
         if self._pad_v:
-            attn_out = attn_out[..., :v.shape[-1]]
+            attn_out = attn_out.view(-1, self.num_heads, q.shape[-1])[..., :v.shape[-1]]\
+                .reshape(-1, self.num_heads * v.shape[-1])
 
         # Remain consistent with old `flash_attn_varlen_func` where there
         # is only one output tensor if `return_softmax_lse` is False.
@@ -732,7 +735,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
                 del eye
                 # standardize to (output, input)
                 return dequant_weights.T
-            return layer.weight
+            return layer.weight if not envs.MACA_VLLM_USE_TN_2_NN else layer.weight.T
 
         # we currently do not have quantized bmm's which are needed for
         # `W_UV` and `W_UK_T`, we we just store fp16/bf16 copies and perform
@@ -860,10 +863,10 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
             max_seqlen_k=attn_metadata.prefill.max_query_len,
             softmax_scale=self.scale,
             causal=True,
-            return_softmax_lse=has_context,
+            # return_softmax_lse=has_context,
         )
 
-        if has_context:
+        if False and has_context:
             suffix_output, suffix_lse = output
             context_output, context_lse = self._compute_prefill_context( \
                 q, kv_c_and_k_pe_cache, attn_metadata)
@@ -877,7 +880,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
                 suffix_lse=suffix_lse,
             )
 
-        return self.o_proj(output.flatten(start_dim=-2))[0]
+        return self.o_proj(output)[0]
 
     @abstractmethod
     def _forward_decode(
diff --git a/vllm/v1/attention/backends/mla/triton_mla.py b/vllm/v1/attention/backends/mla/triton_mla.py
index 8e7e4f10b..ed431d68c 100644
--- a/vllm/v1/attention/backends/mla/triton_mla.py
+++ b/vllm/v1/attention/backends/mla/triton_mla.py
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0
 
+from dataclasses import dataclass
 from typing import Any, Optional
 
 import torch
@@ -9,11 +10,21 @@ from vllm.attention.backends.abstract import (AttentionType,
 from vllm.attention.ops.triton_decode_attention import decode_attention_fwd
 from vllm.logger import init_logger
 from vllm.v1.attention.backends.mla.common import (MLACommonBackend,
+                                                   MLACommonDecodeMetadata,
                                                    MLACommonImpl,
-                                                   MLACommonMetadata)
+                                                   MLACommonMetadata,
+                                                   MLACommonMetadataBuilder)
+from vllm.attention.backends.triton_mla import (load_config,
+                                                find_best_mla_para)
 
 logger = init_logger(__name__)
 
+import os
+# TODO: Configure environment variables temporarily. New versions do not need to be configured
+os.environ['TRITON_ENABLE_MACA_OPT_MOVE_DOT_OPERANDS_OUT_LOOP'] = '1'
+os.environ['TRITON_ENABLE_MACA_CHAIN_DOT_OPT'] = '1'
+
+JSON_DATA = load_config()
 
 class TritonMLABackend(MLACommonBackend):
 
@@ -21,10 +32,46 @@ class TritonMLABackend(MLACommonBackend):
     def get_name() -> str:
         return "TRITON_MLA_VLLM_V1"
 
+    @staticmethod
+    def get_metadata_cls() -> type["TritonMLAMetadata"]:
+        return TritonMLAMetadata
+
+    @staticmethod
+    def get_builder_cls() -> type["TritonMLAMetadataBuilder"]:
+        return TritonMLAMetadataBuilder
+
     @staticmethod
     def get_impl_cls() -> type["TritonMLAImpl"]:
         return TritonMLAImpl
 
+@dataclass
+class TritonMLADecodeMetadata(MLACommonDecodeMetadata):
+    num_kv_splits: int
+    num_stages: int
+
+@dataclass
+class TritonMLAMetadata(MLACommonMetadata[TritonMLADecodeMetadata]):
+    pass
+
+class TritonMLAMetadataBuilder(MLACommonMetadataBuilder[TritonMLAMetadata]):
+    def _build_decode(self, input_positions: torch.Tensor,
+                      block_table: torch.Tensor,
+                      seq_lens: torch.Tensor) -> TritonMLADecodeMetadata:
+        if seq_lens is not None:
+            batch = seq_lens.shape[0]
+            max_seq_len = int(seq_lens.max())
+            num_kv_splits, num_stages = find_best_mla_para(JSON_DATA, batch, max_seq_len, 8)
+        else:
+            num_kv_splits = 4
+            num_stages = 1
+
+        return TritonMLADecodeMetadata(
+            input_positions=input_positions,
+            block_table=block_table,
+            seq_lens=seq_lens,
+            num_kv_splits=num_kv_splits,
+            num_stages=num_stages,
+        )
 
 class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
 
@@ -88,14 +135,12 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
                         dtype=q.dtype,
                         device=q.device)
 
-        num_kv_splits = 4  # TODO: heuristic
-
         # TODO(lucas) Allocate ahead of time
         attn_logits = torch.empty(
             (
                 B,
                 self.num_heads,
-                num_kv_splits,
+                attn_metadata.decode.num_kv_splits,
                 # NOTE(lucas) idk why the +1 is here but sglang has it so we
                 # just mirror that
                 self.kv_lora_rank + 1,
@@ -113,6 +158,8 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):
         decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,
                              attn_metadata.decode.block_table,
                              attn_metadata.decode.seq_lens, attn_logits,
-                             num_kv_splits, self.scale, PAGE_SIZE)
+                             attn_metadata.decode.num_kv_splits,
+                             attn_metadata.decode.num_stages,
+                             self.scale, PAGE_SIZE)
 
         return self._v_up_proj_and_o_proj(o)
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 21711c929..60c6a0f00 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -739,7 +739,10 @@ class Scheduler(SchedulerInterface):
 
         # Return the cached request data to the queue so they can be reused.
         for req_data in scheduler_output.scheduled_cached_reqs:
-            self._cached_reqs_data[req_data.req_id].append(req_data)
+            # NOTE(rob): since we free stopped reqs above, adding stopped reqs
+            # to _cached_reqs_data will cause a memory leak.
+            if req_data.req_id not in self.finished_req_ids:
+                self._cached_reqs_data[req_data.req_id].append(req_data)
 
         self.running = new_running
         engine_core_outputs = EngineCoreOutputs(
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 80807665e..fd8d1773e 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -599,6 +599,7 @@ class DPEngineCoreProc(EngineCoreProc):
                 str(device_id_to_physical_device_id(i))
                 for i in range(local_dp_rank * tp_size, (local_dp_rank + 1) *
                                tp_size))
+            os.environ["MACA_VISIBLE_DEVICES"] = os.environ["CUDA_VISIBLE_DEVICES"]
 
         self.local_dp_rank = local_dp_rank
         self.dp_group = vllm_config.parallel_config.stateless_init_dp_group()
diff --git a/vllm/version.py b/vllm/version.py
index 8329d7bec..904ee45b7 100644
--- a/vllm/version.py
+++ b/vllm/version.py
@@ -5,11 +5,11 @@ try:
 except Exception as e:
     import warnings
 
-    warnings.warn(f"Failed to read commit hash:\n{e}",
-                  RuntimeWarning,
-                  stacklevel=2)
+    # warnings.warn(f"Failed to read commit hash:\n{e}",
+    #               RuntimeWarning,
+    #               stacklevel=2)
 
-    __version__ = "dev"
+    __version__ = "0.8.5"
     __version_tuple__ = (0, 0, __version__)
 
 
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 73e0eff9a..ff51a5be1 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -73,8 +73,21 @@ TModelInputForGPU = TypeVar('TModelInputForGPU', bound="ModelInputForGPU")
 
 # For now, bump up cache limits for recompilations during CUDA graph warmups.
 torch._dynamo.config.cache_size_limit = 128
-torch._dynamo.config.accumulated_cache_size_limit = 128
-
+# torch._dynamo.config.accumulated_cache_size_limit = 128
+
+# --- FLAGSCALE MODIFICATION BEG ---
+# Know more about FlagGems: https://github.com/FlagOpen/FlagGems
+import os
+if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
+    try:
+        import flag_gems
+        flag_gems.enable()
+        logger.info("Successfully enabled flag_gems as default ops implementation.")
+    except ImportError:
+        logger.warning("Failed to import 'flag_gems'. Falling back to default implementation.")
+    except Exception as e:
+        logger.warning(f"Failed to enable 'flag_gems': {e}. Falling back to default implementation.")
+# --- FLAGSCALE MODIFICATION END ---
 
 @dataclass(frozen=True)
 class ModelInputForGPU(ModelRunnerInputBase):
diff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py
index a6f5ec825..74705a50f 100644
--- a/vllm/worker/multi_step_model_runner.py
+++ b/vllm/worker/multi_step_model_runner.py
@@ -32,7 +32,7 @@ if TYPE_CHECKING:
 logger = init_logger(__name__)
 
 MULTI_STEP_ATTENTION_BACKENDS = [
-    "FLASH_ATTN", "ROCM_FLASH", "FLASHINFER", "NO_ATTENTION"
+    "FLASH_ATTN", "ROCM_FLASH", "FLASHINFER", "NO_ATTENTION", "TRITON_MLA"
 ]
 MULTI_STEP_CHUNKED_PREFILL_ATTENTION_BACKENDS = ["FLASH_ATTN", "FLASHINFER"]
 
diff --git a/vllm/worker/worker.py b/vllm/worker/worker.py
index 78ea990de..3ef1a0033 100644
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -71,7 +71,11 @@ class Worker(LocalOrDistributedWorkerBase):
             or (speculative_config.draft_model_config.hf_config.model_type ==
                 model_config.hf_config.model_type) \
             or (speculative_config.draft_model_config.hf_config.model_type
-                not in ("medusa", "mlp_speculator", "eagle", "deepseek_mtp")) \
+                not in ("medusa",
+                        "mlp_speculator",
+                        "eagle",
+                        "deepseek_mtp",
+                         "mimo_mtp")) \
                     else {"return_hidden_states": True}
 
         ModelRunnerClass: Type[GPUModelRunnerBase] = ModelRunner
