diff --git a/examples/deepseek_r1/conf/hostfile.txt b/examples/deepseek_r1/conf/hostfile.txt
index 0d8b1e05..a5852477 100644
--- a/examples/deepseek_r1/conf/hostfile.txt
+++ b/examples/deepseek_r1/conf/hostfile.txt
@@ -3,3 +3,5 @@
 x.x.x.x slots=8 type=gpu
 # worker nodes
 x.x.x.x slots=8 type=gpu
+x.x.x.x slots=8 type=gpu
+x.x.x.x slots=8 type=gpu
\ No newline at end of file
diff --git a/examples/deepseek_r1/conf/serve.yaml b/examples/deepseek_r1/conf/serve.yaml
index f63820f4..76eccee8 100644
--- a/examples/deepseek_r1/conf/serve.yaml
+++ b/examples/deepseek_r1/conf/serve.yaml
@@ -10,13 +10,11 @@ experiment:
     use_fs_serve: false
   runner:
     hostfile: examples/deepseek_r1/conf/hostfile.txt
-    docker: flagrelease_nv
+    docker: flagrelease_metax
     ssh_port: 22
-  envs:
-    CUDA_DEVICE_MAX_CONNECTIONS: 1
   cmds:
-    before_start: source /root/miniconda3/bin/activate flagscale-inference && export GLOO_SOCKET_IFNAME=bond0 # replace "bond0" with your own network card
+    before_start: export VLLM_PP_LAYER_PARTITION=16,15,15,15
 action: run
 hydra:
   run:
-    dir: ${experiment.exp_dir}/hydra
+    dir: ${experiment.exp_dir}/hydra
\ No newline at end of file
diff --git a/examples/deepseek_r1/conf/serve/671b.yaml b/examples/deepseek_r1/conf/serve/671b.yaml
index 719a6726..bb2a28fd 100644
--- a/examples/deepseek_r1/conf/serve/671b.yaml
+++ b/examples/deepseek_r1/conf/serve/671b.yaml
@@ -1,12 +1,12 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /models/deepseek_r1 # path of weight of deepseek r1
+    model: /nfs/deepseek_r1_BF16 # path of weight of deepseek r1
     tensor_parallel_size: 8
     pipeline_parallel_size: 4
     gpu_memory_utilization: 0.9
-    max_model_len: 32768
-    max_num_seqs: 256
-    enforce_eager: true
+    swap_space: 16
+    dtype: bfloat16
+    max_model_len: 4096
     trust_remote_code: true
-    enable_chunked_prefill: true
+    distributed_executor_backend: ray
\ No newline at end of file
diff --git a/examples/deepseek_r1_w8a8/conf/hostfile.txt b/examples/deepseek_r1_w8a8/conf/hostfile.txt
new file mode 100644
index 00000000..1410176e
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/hostfile.txt
@@ -0,0 +1,5 @@
+# ip slots type=x.x.x.x
+# master node
+x.x.x.x slots=8 type=gpu
+# worker nodes
+x.x.x.x slots=8 type=gpu
diff --git a/examples/deepseek_r1_w8a8/conf/serve.yaml b/examples/deepseek_r1_w8a8/conf/serve.yaml
new file mode 100644
index 00000000..370bc3ca
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/serve.yaml
@@ -0,0 +1,20 @@
+defaults:
+  - _self_
+  - serve: 671b_w8a8
+experiment:
+  exp_name: deepseek_r1_w8a8
+  exp_dir: outputs/${experiment.exp_name}
+  task:
+    type: serve
+  deploy:
+    use_fs_serve: false
+  runner:
+    hostfile: examples/deepseek_r1_w8a8/conf/hostfile.txt
+    docker: ds_flagscale3
+    ssh_port: 22
+  cmds:
+    before_start:
+action: run
+hydra:
+  run:
+    dir: ${experiment.exp_dir}/hydra
diff --git a/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
new file mode 100644
index 00000000..ee3171e8
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
@@ -0,0 +1,14 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /share/project/zhangyu.d/deepseek_r1_bf16_w8a8/vllm_quant_model # path of weight of deepseek r1
+    tensor_parallel_size: 8
+    pipeline_parallel_size: 2
+    gpu_memory_utilization: 0.95
+    max_model_len: 32768
+    max_seq_len_to_capture: 32768
+    max_num_batched_tokens: 32768
+    swap_space: 16
+    dtype: bfloat16
+    trust_remote_code: true
+    distributed_executor_backend: ray
diff --git a/examples/qwen3/conf/serve.yaml b/examples/qwen3/conf/serve.yaml
index f3e09dca..7859ad53 100644
--- a/examples/qwen3/conf/serve.yaml
+++ b/examples/qwen3/conf/serve.yaml
@@ -1,19 +1,16 @@
 defaults:
 - _self_
-- serve: 0_6b
+- serve: 32b
 
 experiment:
-  exp_name: qwen3_0.6b
+  exp_name: qwen3_32b
   exp_dir: outputs/${experiment.exp_name}
   task:
     type: serve
+  deploy:
+    use_fs_serve: false
   runner:
     hostfile: null
-    deploy:
-      use_fs_serve: false
-  envs:
-    CUDA_VISIBLE_DEVICES: 0
-    CUDA_DEVICE_MAX_CONNECTIONS: 1
 
 action: run
 
diff --git a/examples/qwen3/conf/serve/32b.yaml b/examples/qwen3/conf/serve/32b.yaml
new file mode 100644
index 00000000..991ac232
--- /dev/null
+++ b/examples/qwen3/conf/serve/32b.yaml
@@ -0,0 +1,16 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /nfs/models/Qwen3-32B/
+    host: 0.0.0.0
+    max_model_len: 4096
+    uvicorn_log_level: warning
+  engine_args_specific:
+    vllm:
+      tensor_parallel_size: 4
+      pipeline_parallel_size: 1
+      gpu_memory_utilization: 0.95
+      swap_space: 16
+      dtype: bfloat16
+      trust_remote_code: true
+      distributed_executor_backend: ray
\ No newline at end of file
diff --git a/examples/qwen3/conf/serve_atmb.yaml b/examples/qwen3/conf/serve_atmb.yaml
index 34683dae..11c82b4e 100644
--- a/examples/qwen3/conf/serve_atmb.yaml
+++ b/examples/qwen3/conf/serve_atmb.yaml
@@ -8,12 +8,12 @@ experiment:
   exp_dir: outputs/${experiment.exp_name}
   task:
     type: serve
+  deploy:
+    port: 6701
+    use_fs_serve: false
   runner:
     nnodes: 1
     nproc_per_node: 4
-    deploy:
-      port: 6701
-      use_fs_serve: false
   envs:
     CUDA_VISIBLE_DEVICES: 0,1,2,3
     CUDA_DEVICE_MAX_CONNECTIONS: 1
diff --git a/examples/qwen3/conf/train.yaml b/examples/qwen3/conf/train.yaml
index fb0ec63b..f741f52f 100644
--- a/examples/qwen3/conf/train.yaml
+++ b/examples/qwen3/conf/train.yaml
@@ -1,15 +1,13 @@
 defaults:
   - _self_
-  # - train: 30b_a3b
-  - train: 32b
+  - train: 30b_a3b
 
 experiment:
-  # exp_name: Qwen3-30b-a3b-Train
-  exp_name: Qwen3-32b-Train
+  exp_name: Qwen3-Test
   seed: 42
   save_steps: 10000
-  load: null
-  exp_dir: xxx
+  load: None
+  exp_dir: /xxx
   ckpt_format: torch
   task:
     type: train
@@ -21,7 +19,7 @@ experiment:
     rdzv_backend: static
     hostfile: null
   cmds:
-    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
+    before_start: "ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale"
   envs:
     LOGLEVEL: "INFO"
     CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
diff --git a/examples/qwen3/conf/train/30b_a3b.yaml b/examples/qwen3/conf/train/30b_a3b.yaml
index c7fdf487..642c2cb4 100644
--- a/examples/qwen3/conf/train/30b_a3b.yaml
+++ b/examples/qwen3/conf/train/30b_a3b.yaml
@@ -1,14 +1,19 @@
 system:
   no_shared_fs: ${experiment.runner.no_shared_fs}
-  num_workers: 2
-  tensor_model_parallel_size: 1
+  num_workers: 16
+  tensor_model_parallel_size: 2
   pipeline_model_parallel_size: 2
-  expert_model_parallel_size: 4
+  expert_model_parallel_size: 2
   context_parallel_size: 1
+  disable_bias_linear: true
+  reset_position_ids: True
+  reset_attention_mask: True
+  qk_layernorm: true
   sequence_parallel: true
   use_distributed_optimizer: true
-  # overlap_grad_reduce: true
-  # overlap_param_gather: true
+  overlap_grad_reduce: true
+  overlap_param_gather: true
+  finetune: false
   precision:
     bf16: true
     attention_softmax_in_fp32: true
@@ -33,9 +38,8 @@ model:
   transformer_impl: transformer_engine
   num_layers: 48
   hidden_size: 2048
-  num_attention_heads: 32
   kv_channels: 128
-  group_query_attention: true
+  num_attention_heads: 32
   num_query_groups: 4 # num_key_value_heads
   seq_length: 4096
   max_position_embeddings: 40960
@@ -44,14 +48,14 @@ model:
   rotary_base: 1000000
   swiglu: true
   normalization: RMSNorm
-  qk_layernorm: true
   init_method_std: 0.02
   attention_dropout: 0.0
   hidden_dropout: 0.0
+  clip_grad: 1.0
+  position_embedding_type: rope
   untie_embeddings_and_output_weights: true
   no_position_embedding: true
   no_rope_fusion: true
-  disable_bias_linear: true
 
   # moe args ===================
   ffn_hidden_size: 6144
@@ -67,26 +71,24 @@ model:
   seed: ${experiment.seed}
   # finetune: false
   micro_batch_size: 1
-  global_batch_size: 128 #2048
+  global_batch_size: 2048
   eval_iters: 0
-  train_iters: 102400
+  train_samples: 244141056 #1T #29297664 #120B tokens
 
   optimizer:
-    clip_grad: 1.0
     weight_decay: 0.1
     adam_beta1: 0.9
     adam_beta2: 0.95
     lr_scheduler:
       lr: 3.0e-3
       min_lr: 3.0e-4
-      lr_warmup_fraction: 0.01
+      lr_warmup_samples: 2048000
       lr_decay_style: WSD
       lr_wsd_decay_style: cosine
-      lr_wsd_decay_iters: 10
+      lr_wsd_decay_samples: 2048
+
 
 data:
-  reset_position_ids: True
-  reset_attention_mask: True
   data_path: /path
   split: 1
   no_mmap_bin_files: true
