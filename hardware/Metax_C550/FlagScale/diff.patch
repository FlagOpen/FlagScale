diff --git a/examples/deepseek_r1/conf/hostfile.txt b/examples/deepseek_r1/conf/hostfile.txt
index 0d8b1e05..a5852477 100644
--- a/examples/deepseek_r1/conf/hostfile.txt
+++ b/examples/deepseek_r1/conf/hostfile.txt
@@ -3,3 +3,5 @@
 x.x.x.x slots=8 type=gpu
 # worker nodes
 x.x.x.x slots=8 type=gpu
+x.x.x.x slots=8 type=gpu
+x.x.x.x slots=8 type=gpu
\ No newline at end of file
diff --git a/examples/deepseek_r1/conf/serve.yaml b/examples/deepseek_r1/conf/serve.yaml
index f63820f4..76eccee8 100644
--- a/examples/deepseek_r1/conf/serve.yaml
+++ b/examples/deepseek_r1/conf/serve.yaml
@@ -10,13 +10,11 @@ experiment:
     use_fs_serve: false
   runner:
     hostfile: examples/deepseek_r1/conf/hostfile.txt
-    docker: flagrelease_nv
+    docker: flagrelease_metax
     ssh_port: 22
-  envs:
-    CUDA_DEVICE_MAX_CONNECTIONS: 1
   cmds:
-    before_start: source /root/miniconda3/bin/activate flagscale-inference && export GLOO_SOCKET_IFNAME=bond0 # replace "bond0" with your own network card
+    before_start: export VLLM_PP_LAYER_PARTITION=16,15,15,15
 action: run
 hydra:
   run:
-    dir: ${experiment.exp_dir}/hydra
+    dir: ${experiment.exp_dir}/hydra
\ No newline at end of file
diff --git a/examples/deepseek_r1/conf/serve/671b.yaml b/examples/deepseek_r1/conf/serve/671b.yaml
index 719a6726..bb2a28fd 100644
--- a/examples/deepseek_r1/conf/serve/671b.yaml
+++ b/examples/deepseek_r1/conf/serve/671b.yaml
@@ -1,12 +1,12 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /models/deepseek_r1 # path of weight of deepseek r1
+    model: /nfs/deepseek_r1_BF16 # path of weight of deepseek r1
     tensor_parallel_size: 8
     pipeline_parallel_size: 4
     gpu_memory_utilization: 0.9
-    max_model_len: 32768
-    max_num_seqs: 256
-    enforce_eager: true
+    swap_space: 16
+    dtype: bfloat16
+    max_model_len: 4096
     trust_remote_code: true
-    enable_chunked_prefill: true
+    distributed_executor_backend: ray
\ No newline at end of file
diff --git a/examples/deepseek_r1_distill_qwen/conf/serve/32b.yaml b/examples/deepseek_r1_distill_qwen/conf/serve/32b.yaml
index ef0e04b9..fc2b4ced 100644
--- a/examples/deepseek_r1_distill_qwen/conf/serve/32b.yaml
+++ b/examples/deepseek_r1_distill_qwen/conf/serve/32b.yaml
@@ -1,14 +1,18 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /share/DeepSeek-R1-Distill-Qwen-32B # should be customized
-    served_model_name: deepseek-r1-distill-qwen-32b-flagos
+    model: /share/project/deepseek_distill/deepseek-ai/Deepseek-R1-Distill-Qwen-32B # should be customized
+      #served_model_name: deepseek-r1-distill-qwen-32b-flagos
     tensor_parallel_size: 8
     max_model_len: 32768
+    max_num_batched_tokens: 32768
+    max_seq_len_to_capture: 32768
+    swap_space: 16
     pipeline_parallel_size: 1
-    max_num_seqs: 8 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
-    gpu_memory_utilization: 0.9
-    port: 9010
+    max_num_seqs: 256 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
+    gpu_memory_utilization: 0.95
+    port: 8000
     trust_remote_code: true
-    enforce_eager: true
-    enable_chunked_prefill: true
+    enforce_eager: false
+    enable_chunked_prefill: false
+    distributed_executor_backend: ray
diff --git a/examples/deepseek_r1_w8a8/conf/hostfile.txt b/examples/deepseek_r1_w8a8/conf/hostfile.txt
new file mode 100644
index 00000000..1410176e
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/hostfile.txt
@@ -0,0 +1,5 @@
+# ip slots type=x.x.x.x
+# master node
+x.x.x.x slots=8 type=gpu
+# worker nodes
+x.x.x.x slots=8 type=gpu
diff --git a/examples/deepseek_r1_w8a8/conf/serve.yaml b/examples/deepseek_r1_w8a8/conf/serve.yaml
new file mode 100644
index 00000000..370bc3ca
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/serve.yaml
@@ -0,0 +1,20 @@
+defaults:
+  - _self_
+  - serve: 671b_w8a8
+experiment:
+  exp_name: deepseek_r1_w8a8
+  exp_dir: outputs/${experiment.exp_name}
+  task:
+    type: serve
+  deploy:
+    use_fs_serve: false
+  runner:
+    hostfile: examples/deepseek_r1_w8a8/conf/hostfile.txt
+    docker: ds_flagscale3
+    ssh_port: 22
+  cmds:
+    before_start:
+action: run
+hydra:
+  run:
+    dir: ${experiment.exp_dir}/hydra
diff --git a/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
new file mode 100644
index 00000000..ee3171e8
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
@@ -0,0 +1,14 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /share/project/zhangyu.d/deepseek_r1_bf16_w8a8/vllm_quant_model # path of weight of deepseek r1
+    tensor_parallel_size: 8
+    pipeline_parallel_size: 2
+    gpu_memory_utilization: 0.95
+    max_model_len: 32768
+    max_seq_len_to_capture: 32768
+    max_num_batched_tokens: 32768
+    swap_space: 16
+    dtype: bfloat16
+    trust_remote_code: true
+    distributed_executor_backend: ray
diff --git a/examples/qwen3/conf/serve.yaml b/examples/qwen3/conf/serve.yaml
index d8006ef8..7859ad53 100644
--- a/examples/qwen3/conf/serve.yaml
+++ b/examples/qwen3/conf/serve.yaml
@@ -1,9 +1,9 @@
 defaults:
 - _self_
-- serve: 0_6b
+- serve: 32b
 
 experiment:
-  exp_name: qwen3_0.6b
+  exp_name: qwen3_32b
   exp_dir: outputs/${experiment.exp_name}
   task:
     type: serve
@@ -11,9 +11,6 @@ experiment:
     use_fs_serve: false
   runner:
     hostfile: null
-  envs:
-    CUDA_VISIBLE_DEVICES: 0
-    CUDA_DEVICE_MAX_CONNECTIONS: 1
 
 action: run
 
diff --git a/examples/qwen3/conf/serve/32b.yaml b/examples/qwen3/conf/serve/32b.yaml
new file mode 100644
index 00000000..991ac232
--- /dev/null
+++ b/examples/qwen3/conf/serve/32b.yaml
@@ -0,0 +1,16 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /nfs/models/Qwen3-32B/
+    host: 0.0.0.0
+    max_model_len: 4096
+    uvicorn_log_level: warning
+  engine_args_specific:
+    vllm:
+      tensor_parallel_size: 4
+      pipeline_parallel_size: 1
+      gpu_memory_utilization: 0.95
+      swap_space: 16
+      dtype: bfloat16
+      trust_remote_code: true
+      distributed_executor_backend: ray
\ No newline at end of file
