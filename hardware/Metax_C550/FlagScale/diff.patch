diff --git a/examples/deepseek_r1_distill_qwen/conf/32b.yaml b/examples/deepseek_r1_distill_qwen/conf/32b.yaml
new file mode 100644
index 00000000..fc2b4ced
--- /dev/null
+++ b/examples/deepseek_r1_distill_qwen/conf/32b.yaml
@@ -0,0 +1,18 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /share/project/deepseek_distill/deepseek-ai/Deepseek-R1-Distill-Qwen-32B # should be customized
+      #served_model_name: deepseek-r1-distill-qwen-32b-flagos
+    tensor_parallel_size: 8
+    max_model_len: 32768
+    max_num_batched_tokens: 32768
+    max_seq_len_to_capture: 32768
+    swap_space: 16
+    pipeline_parallel_size: 1
+    max_num_seqs: 256 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
+    gpu_memory_utilization: 0.95
+    port: 8000
+    trust_remote_code: true
+    enforce_eager: false
+    enable_chunked_prefill: false
+    distributed_executor_backend: ray
diff --git a/examples/deepseek_r1_w8a8/conf/hostfile.txt b/examples/deepseek_r1_w8a8/conf/hostfile.txt
new file mode 100644
index 00000000..1410176e
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/hostfile.txt
@@ -0,0 +1,5 @@
+# ip slots type=x.x.x.x
+# master node
+x.x.x.x slots=8 type=gpu
+# worker nodes
+x.x.x.x slots=8 type=gpu
diff --git a/examples/deepseek_r1_w8a8/conf/serve.yaml b/examples/deepseek_r1_w8a8/conf/serve.yaml
new file mode 100644
index 00000000..370bc3ca
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/serve.yaml
@@ -0,0 +1,20 @@
+defaults:
+  - _self_
+  - serve: 671b_w8a8
+experiment:
+  exp_name: deepseek_r1_w8a8
+  exp_dir: outputs/${experiment.exp_name}
+  task:
+    type: serve
+  deploy:
+    use_fs_serve: false
+  runner:
+    hostfile: examples/deepseek_r1_w8a8/conf/hostfile.txt
+    docker: ds_flagscale3
+    ssh_port: 22
+  cmds:
+    before_start:
+action: run
+hydra:
+  run:
+    dir: ${experiment.exp_dir}/hydra
diff --git a/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
new file mode 100644
index 00000000..ee3171e8
--- /dev/null
+++ b/examples/deepseek_r1_w8a8/conf/serve/671b_w8a8.yaml
@@ -0,0 +1,14 @@
+- serve_id: vllm_model
+  engine: vllm
+  engine_args:
+    model: /share/project/zhangyu.d/deepseek_r1_bf16_w8a8/vllm_quant_model # path of weight of deepseek r1
+    tensor_parallel_size: 8
+    pipeline_parallel_size: 2
+    gpu_memory_utilization: 0.95
+    max_model_len: 32768
+    max_seq_len_to_capture: 32768
+    max_num_batched_tokens: 32768
+    swap_space: 16
+    dtype: bfloat16
+    trust_remote_code: true
+    distributed_executor_backend: ray
