diff --git a/examples/deepseek_v3/conf/train.yaml b/examples/deepseek_v3/conf/train.yaml
index e030db9e..d377ab11 100644
--- a/examples/deepseek_v3/conf/train.yaml
+++ b/examples/deepseek_v3/conf/train.yaml
@@ -4,27 +4,49 @@ defaults:
   - train: 16b_a3b
 
 experiment:
-  exp_name: DeepSeek-16b-a3b
+  exp_name: train_deepseek_v3_16b_a3b
   seed: 42
-  save_steps: 10000
-  load: null
-  exp_dir: xxx
-  ckpt_format: torch
+  save_steps: 1000
+  exp_dir: ./${experiment.exp_name}
   task:
     type: train
     backend: megatron
     entrypoint: flagscale/train/train_gpt.py
   runner:
-    per_node_task: false
-    no_shared_fs: false
+    backend: torchrun
+    nnodes: 4
+    nproc_per_node: 8
+    hostfile: /share/project/hostfile
+    master_port: xxxxx
+    ssh_port: xxxxxx
+    master_addr: xx.x.xx.xxx
     rdzv_backend: static
-    hostfile: null
-  cmds:
-    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
   envs:
-    LOGLEVEL: "INFO"
-    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
+    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
     CUDA_DEVICE_MAX_CONNECTIONS: 1
+    NVTE_APPLY_QK_LAYER_SCALING: 0
+    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
+    NCCL_NET_GDR_LEVEL: 7
+    NCCL_NET_GDR_READ: 1
+    NCCL_MAX_NCHANNELS: 32
+    NCCL_MIN_NCHANNELS: 32
+    RCCL_SDMA_COPY_ENABLE: 0
+    NCCL_ALGO: Ring
+    HSA_FORCE_FINE_GRAIN_PCIE: 1
+    OMP_NUM_THREADS: 1
+    NCCL_SOCKET_IFNAME: enp225s0f0np0
+    GLOO_SOCKET_IFNAME: enp225s0f0np0
+    NCCL_IB_HCA: mlx5_0:1,mlx5_6:1
+    ALLREDUCE_STREAM_WITH_COMPUTE: 1
+    SENDRECV_STREAM_WITH_COMPUTE: 1
+    cache_size_limit: 64
+    GPU_MAX_HW_QUEUES: 20
+  cmds:
+    before_start: |-
+      source /opt/dtk/env.sh
+      export LD_LIBRARY_PATH=/root/xxx01/lib:$LD_LIBRARY_PATH
+      export LD_LIBRARY_PATH=/root/xxx02/lib:$LD_LIBRARY_PATH
+    #before_start: "ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train"
 
 action: run
 
diff --git a/examples/deepseek_v3/conf/train/16b_a3b.yaml b/examples/deepseek_v3/conf/train/16b_a3b.yaml
index 532d9621..e261ae08 100644
--- a/examples/deepseek_v3/conf/train/16b_a3b.yaml
+++ b/examples/deepseek_v3/conf/train/16b_a3b.yaml
@@ -1,15 +1,20 @@
 system:
-  no_shared_fs: ${experiment.runner.no_shared_fs}
-  num_workers: 2
+  num_workers: 16
+  deterministic_mode: true
   tensor_model_parallel_size: 1
   pipeline_model_parallel_size: 2
   decoder_first_pipeline_num_layers: 13
-  expert_model_parallel_size: 4
+  expert_model_parallel_size: 2
   context_parallel_size: 1
+  disable_bias_linear: true
+  reset_position_ids: True
+  reset_attention_mask: True
+  qk_layernorm: true
   sequence_parallel: true
   use_distributed_optimizer: true
   overlap_grad_reduce: true
   overlap_param_gather: true
+  finetune: true
   precision:
     bf16: true
     attention_softmax_in_fp32: true
@@ -26,9 +31,9 @@ system:
     log_num_zeros_in_grad: true
     log_memory_to_tensorboard: true
   checkpoint:
-    save_interval: ${experiment.save_steps}
-    load: ${experiment.load}
-    ckpt_format: ${experiment.ckpt_format}
+    save_interval: 1500
+    load: /share/projset_public/perf_logs/XLC_2025_openseek/flagscale_ckpt_tp1_pp2_ep2
+    ckpt_format: torch
 
 model:
   transformer_impl: transformer_engine
@@ -43,15 +48,14 @@ model:
   rotary_base: 1000000
   swiglu: true
   normalization: RMSNorm
-  qk_layernorm: true
   init_method_std: 0.02
   attention_dropout: 0.0
   hidden_dropout: 0.0
+  clip_grad: 1.0
   position_embedding_type: rope
   untie_embeddings_and_output_weights: true
   no_position_embedding: true
   no_rope_fusion: true
-  disable_bias_linear: true
 
   # mla args ==================
   multi_latent_attention: true
@@ -72,46 +76,39 @@ model:
   moe_router_bias_update_rate: 0.001
   moe_aux_loss_coeff: 0.02
   moe_layer_freq: "[0]+[1]*26"
-  # node limited routing
   moe_router_num_groups: 1
   moe_router_group_topk: 1
   moe_router_topk: 6
   moe_router_topk_scaling_factor: 2.446
   moe_token_dispatcher_type: "alltoall"
 
-  # mtp args ====================
-  mtp_num_layers: 1
-  mtp_loss_scaling_factor: 0.3
-
   # training
   seed: ${experiment.seed}
-  finetune: false
+  # finetune: false
   micro_batch_size: 1
-  global_batch_size: 128 #2048
+  global_batch_size: 256
   eval_iters: 0
-  train_iters: 102400
+  train_samples: 768000
 
   optimizer:
-    clip_grad: 1.0
     weight_decay: 0.1
     adam_beta1: 0.9
     adam_beta2: 0.95
     lr_scheduler:
       lr: 3.0e-3
       min_lr: 3.0e-4
-      lr_warmup_fraction: 0.01
+      lr_warmup_samples: 76800
       lr_decay_style: WSD
       lr_wsd_decay_style: cosine
-      lr_wsd_decay_iters: 10
+      lr_wsd_decay_samples: 768
+
 
 data:
-  reset_position_ids: True
-  reset_attention_mask: True
-  data_path: /path
+  data_path: /share/project/DNOT_MOVE/XLC_2025_openseek/dataset/cosmopedia-v2-full_text_document
   split: 1
   no_mmap_bin_files: true
   tokenizer:
     tokenizer_type: QwenTokenizerFS
-    tokenizer_path: examples/aquila/qwentokenizer
+    tokenizer_path: /share/projset_public/perf_logs/XLC_2025_openseek/hf_ckpt
     vocab_size: 151851
     make_vocab_size_divisible_by: 64
diff --git a/examples/llama3/conf/train.yaml b/examples/llama3/conf/train.yaml
index 8b3acdfd..33c12e22 100644
--- a/examples/llama3/conf/train.yaml
+++ b/examples/llama3/conf/train.yaml
@@ -1,24 +1,49 @@
 defaults:
-  - train: 70b
   - _self_
+  - train: 70b_finetune
 
 experiment:
-  exp_name: llama3
-  exp_dir: ./outputs_llama3_70b
+  exp_name: train_llama3_70b
+  seed: 42
+  save_steps: 1000
+  exp_dir: ./${experiment.exp_name}
   task:
     type: train
     backend: megatron
-    entrypoint: ./flagscale/train/train_gpt.py
+    entrypoint: flagscale/train/train_gpt.py
   runner:
     backend: torchrun
     nnodes: 4
     nproc_per_node: 8
-    hostfile: ${hostfile??}
+    hostfile: /share/project/hostfile
+    master_port: xxxx
+    ssh_port: xxxx
+    master_addr: xx.x.xx.xx
+    rdzv_backend: static
   envs:
     CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
     CUDA_DEVICE_MAX_CONNECTIONS: 1
     NVTE_APPLY_QK_LAYER_SCALING: 0
     NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
+    NCCL_NET_GDR_LEVEL: 7
+    NCCL_NET_GDR_READ: 1
+    NCCL_MAX_NCHANNELS: 32
+    NCCL_MIN_NCHANNELS: 32
+    RCCL_SDMA_COPY_ENABLE: 0
+    NCCL_ALGO: Ring
+    HSA_FORCE_FINE_GRAIN_PCIE: 1
+    OMP_NUM_THREADS: 1
+    NCCL_SOCKET_IFNAME: enp225s0f0np0
+    NCCL_IB_HCA: mlx5_0:1,mlx5_6:1
+    ALLREDUCE_STREAM_WITH_COMPUTE: 1
+    SENDRECV_STREAM_WITH_COMPUTE: 1
+    cache_size_limit: 64
+    GPU_MAX_HW_QUEUES: 20
+  cmds:
+    before_start: |-
+      source /opt/dtk-25.04.1-rc1/env.sh
+      export LD_LIBRARY_PATH=/xxx/xx01/lib:$LD_LIBRARY_PATH
+
 action: run
 
 hydra:
diff --git a/examples/llama3/conf/train/70b_finetune.yaml b/examples/llama3/conf/train/70b_finetune.yaml
index aca7f56b..55d1a8ef 100644
--- a/examples/llama3/conf/train/70b_finetune.yaml
+++ b/examples/llama3/conf/train/70b_finetune.yaml
@@ -18,9 +18,9 @@ system:
     wandb_project: "train-llama3-70B"
     wandb_exp_name: "train-llama3-70B"
   checkpoint:
-    load: ${ckpt_path:??}
+    load: /share/projset_public/perf_logs/XLC_2025_llama3/flagscale_ckpt_tp8pp4
     ckpt_format: torch
-    save_interval: 100
+    save_interval: 250
     finetune: True
 
 model:
@@ -48,11 +48,12 @@ model:
   hidden_dropout: 0.0
   clip_grad: 1.0
 
-  train_samples: 6160066
+  train_samples: 256000
   micro_batch_size: 1
-  global_batch_size: 1024
+  global_batch_size: 512
   seed: 42
 
+
   optimizer:
     start_weight_decay: 0
     end_weight_decay: 5e-7
@@ -62,13 +63,13 @@ model:
     lr_scheduler:
       lr: 5e-6
       min_lr: 0
-      lr_warmup_samples: 2048000
+      lr_warmup_samples: 25600
       lr_decay_style: cosine
 
 data:
-  data_path: ${data_path:??}
+  data_path: /share/project/DNOT_MOVE/dataset/dedup-md5-pile-pile-cc_text_document
   split: 1
   tokenizer:
     tokenizer_type: Llama3TokenizerFS
-    tokenizer_path: ${tokenizer_path:??}
+    tokenizer_path:  /share/project/DNOT_MOVE/tokenizer
     vocab_size: 128256
diff --git a/examples/qwen2_5_vl/conf/train.yaml b/examples/qwen2_5_vl/conf/train.yaml
index 7159e1b1..bab349c8 100644
--- a/examples/qwen2_5_vl/conf/train.yaml
+++ b/examples/qwen2_5_vl/conf/train.yaml
@@ -11,18 +11,38 @@ experiment:
     entrypoint: ./flagscale/train/train_qwen2_5_vl.py
   runner:
     backend: torchrun
-    nnodes: 1
+    nnodes: 4
     nproc_per_node: 8
+    hostfile: /share/project/hostfile
+    master_port: xxxxx
+    ssh_port: xxxx
+    master_addr: xx.x.xx.xxx
     rdzv_backend: static
-  cmds:  
-    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
+  cmds:
+    before_start: |-
+      source /opt/dtk/env.sh
+      export LD_LIBRARY_PATH=/xxx/xx01/lib:$LD_LIBRARY_PATH
+      export LD_LIBRARY_PATH=/xxx/xx02/lib:$LD_LIBRARY_PATH
   envs:
-    # NCCL_DEBUG: INFO
-    # NCCL_DEBUG_SUBSYSTEM: ALL
     CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
     CUDA_DEVICE_MAX_CONNECTIONS: 1
     NVTE_APPLY_QK_LAYER_SCALING: 0
     NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
+    NCCL_NET_GDR_LEVEL: 7
+    NCCL_NET_GDR_READ: 1
+    NCCL_MAX_NCHANNELS: 32
+    NCCL_MIN_NCHANNELS: 32
+    RCCL_SDMA_COPY_ENABLE: 0
+    NCCL_ALGO: Ring
+    HSA_FORCE_FINE_GRAIN_PCIE: 1
+    OMP_NUM_THREADS: 1
+    NCCL_SOCKET_IFNAME: enp225s0f0np0
+    GLOO_SOCKET_IFNAME: enp225s0f0np0
+    NCCL_IB_HCA: mlx5_0:1,mlx5_6:1
+    ALLREDUCE_STREAM_WITH_COMPUTE: 1
+    SENDRECV_STREAM_WITH_COMPUTE: 1
+    cache_size_limit: 64
+    GPU_MAX_HW_QUEUES: 20
 
 action: run
 
diff --git a/examples/qwen2_5_vl/conf/train/7b.yaml b/examples/qwen2_5_vl/conf/train/7b.yaml
index 62b2e072..5d0a7710 100644
--- a/examples/qwen2_5_vl/conf/train/7b.yaml
+++ b/examples/qwen2_5_vl/conf/train/7b.yaml
@@ -1,22 +1,20 @@
 system:
-  num_workers: 1
-  calculate_per_token_loss: true
   tensor_model_parallel_size: 2
   pipeline_model_parallel_size: 1
   context_parallel_size: 1
-  # decoder_first_pipeline_num_layers: 12
+  # decoder_first_pipeline_num_layers: 10
   disable_bias_linear: True
   use_flash_attn: True
   use_distributed_optimizer: True
   sequence_parallel: True
   tp_comm_overlap: False
-  overlap_grad_reduce: False # if has text-only must be false
-  overlap_param_gather: False # if has text-only must be false
+  overlap_grad_reduce: True
+  overlap_param_gather: True
   use_mcore_models: True
   transformer_impl: transformer_engine
-  recompute_method: "uniform"
-  recompute_granularity: "full"
-  recompute_num_layers: 1
+  # recompute_method: "uniform"
+  # recompute_granularity: "full"
+  # recompute_num_layers: 1
   use_te: True
   precision:
     bf16: True
@@ -30,15 +28,14 @@ system:
     log_params_norm: True
     log_num_zeros_in_grad: True
   checkpoint:
-    save_interval: 1000
-    pretrained_checkpoint: xxxx
+    save_interval: 2500
+    pretrained_checkpoint: /share/projset_public/perf_logs/XLC_2025_qwen2.5_vl/flagscale_ckpt_tp2pp1
     dataloader_save: ${experiment.exp_dir}/checkpoints/dataloader
     use_dist_ckpt: False
     ckpt_format: torch
     async_save: False
 
 model:
-  attention_backend: flash # don't use "auto(nvte_flash_attn)"
   disable_bias_linear: True
   add_qkv_bias: True
   num_layers: 28
@@ -46,22 +43,20 @@ model:
   ffn_hidden_size: 18944
   num_attention_heads: 28
   num_query_groups: 4
-  seq_length: 2048
-  max_padding_length: 2048 # (cutoff_len)max 32768, change according the dataset
-  # especial for qwen2.5-vl
-  enable_variable_seq_lengths: True
+  seq_length: 4096 # origin LLM 32768
+  max_padding_length: 4096 # real seq_length
   max_position_embeddings: 128000 # only useful for additional position embedding
   swiglu: True
   normalization: RMSNorm
   norm_epsilon: 1e-6
-  init_method_std: 0.02
+  init_method_std: 0.014
   attention_dropout: 0.0
   hidden_dropout: 0.0
   clip_grad: 1.0
-  train_iters: 62
-  eval_iters: 0 # no valid
-  micro_batch_size: 1
-  global_batch_size: 16
+  train_iters: 5000
+  eval_iters: 0
+  micro_batch_size: 2
+  global_batch_size: 256
   allow_missing_vision_projection_checkpoint: False
   apply_layernorm_1p: False
   group_query_attention: True
@@ -84,24 +79,22 @@ model:
   seed: 42
 
   optimizer:
-    weight_decay: 0.1
+    weight_decay: 0.0
     adam_beta1: 0.9
-    adam_beta2: 0.999
+    adam_beta2: 0.95
     lr_scheduler:
       lr: 1.0e-5
       min_lr: 1.0e-6
-      # lr_warmup_fraction: .03
-      lr_warmup_iters: 10
+      lr_warmup_fraction: .03
       lr_decay_style: cosine
 
 data:
-  data_path: xxxx
-  vision_root: xxxx
+  data_path: /share/project/DNOT_MOVE/XLC_2025_qwen2.5_vl/dataset
   dataloader_type: external
   split: 100,0,0
   tokenizer:
     tokenizer_type: Qwen2VLTokenizer
-    tokenizer_path: xxxx
+    tokenizer_path: /share/project/DNOT_MOVE/XLC_2025_qwen2.5_vl/flagscale_ckpt_tp2pp1
     vocab_size: 152064 # 7b
     extra_vocab_size: 421
     make_vocab_size_divisible_by: 64
diff --git a/flagscale/train/models/qwen2_5_vl/QuickStart.md b/flagscale/train/models/qwen2_5_vl/QuickStart.md
deleted file mode 100644
index 8c2fa82d..00000000
--- a/flagscale/train/models/qwen2_5_vl/QuickStart.md
+++ /dev/null
@@ -1,111 +0,0 @@
-
-# 1. Install the FlagScale
-
-## 1.1. Downlowd the source code 
-
-```bash
-git clone https://github.com/FlagOpen/FlagScale.git
-cd FlagScale
-```
-
-## 1.2. Apply the submodule patch code
-
-```bash
-python ./tools/patch/unpatch.py --backend=Megatron-LM
-python ./tools/patch/unpatch.py --backend=Megatron-Energon
-cd ./third_party/Megatron-Energon/
-pip install -e .
-cp -r src/megatron/energon/ ../Megatron-LM/megatron/
-```
-
-You can also refered the readme in `https://github.com/FlagOpen/FlagScale.git`
-# 2. Prepare checkpoint
-
-Reference [convert.md](../../../../tools/checkpoint/qwen2_5_vl/convert.md)
-```bash
-mkdir -p /mnt/qwen2.5-vl-ckpts
-cd /mnt/qwen2.5-vl-ckpts
-git clone https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
-cd Qwen2.5-VL-7B-Instruct
-git lfs pull
-
-cd ./tools/checkpoint/qwen2_5_vl/
-bash hf2mcore_qwen2.5_vl_convertor.sh 7B \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-tp2 \
-2 1 false bf16  \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct
-```
-
-# 3. Preprocess dataset
-
-Reference [dataset_preparation.md](../../../../tools/datasets/qwenvl/dataset_preparation.md)
-
-```bash
-cd /mnt # custom your path
-
-mkdir llava-datasets
-cd llava-datasets
-git clone https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain
-cd LLaVA-Pretrain
-unzip images.zip
-
-#convert to webdataset format
-cd ./tools/datasets/qwenvl/
-export PYTHONPATH=$PYTHONPATH:../../../../third_party/Megatron-LM/
-
-python convert_custom_dataset_to_wds_chatml_str.py \
-    --dataset-root=/mnt/LLaVA-Pretrain \
-    --output-root=/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/ \
-    --json=blip_laion_cc_sbu_558k.json \
-    --train-split 1 \
-    --val-split 0 \
-    --images-key=image \
-    --videos-key=video \
-    --vision-root=/mnt/LLaVA-Pretrain \
-    --dp-size 1 \
-    --num-workers 20
-```
-The preprocessed dataset will be stored at the output-root path `/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1`.
-The configuration of `data-path` is `/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1` and the configuration of `vision-path` is `/mnt/LLaVA-Pretrain` in the step 4.
-
-# 4. Add your configuration
-
-Add the data path and checkpoint path in ./examples/qwen2_5_vl/conf/train/7b.yaml as shown below:
-
-```bash
-# dataset
-data_path: /mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1
-vision_root: /mnt/LLaVA-Pretrain
-
-# ckpt
-pretrained_checkpoint: /mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-tp2
-tokenizer_path: /mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-tp2
-```
-
-Start training.
-```bash
-python run.py --config-path ./examples/qwen2_5_vl/conf  --config-name train action=run
-```
-
-Stop training.
-```bash
-python run.py --config-path ./examples/qwen2_5_vl/conf  --config-name train action=stop
-```
-
-# 5. Convert the checkpoint to HuggingFace
-
-Reference [convert.md](../../../../tools/checkpoint/qwen2_5_vl/convert.md)
-
-``` bash
-cd ./tools/checkpoint/qwen2_5_vl/
-bash hf2mcore_qwen2.5_vl_convertor.sh 7B \
-./train_qwen2_5_vl_7b/checkpoints \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-fs2hf-tp2 \
-2 1 true bf16  \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct
-```
-The converved checkpoint is stored in `/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-fs2hf-tp2`
-
-# PS
-The path `./` represents the path of `FlagScale` that you download.
\ No newline at end of file
diff --git a/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py b/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py
index 5f0ce50e..93154cdf 100644
--- a/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py
+++ b/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py
@@ -199,7 +199,6 @@ class Qwen2_5VLModel(MegatronModule):
         Returns:
             output (torch.Tensor): Loss of shape [b, s] if labels are provided, otherwise logits of shape [b, s, vocab_size].
         """
-
         use_inference_kv_cache = (
             inference_params is not None
             and "image_tokens_count" in inference_params.key_value_memory_dict
@@ -210,11 +209,6 @@ class Qwen2_5VLModel(MegatronModule):
         if self.pre_process:
             vision_embeds = None
             if vision_grid_thw.shape[0] > 0:
-                # NOTE(lizhiyu): Reference https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1612
-                if self.config.bf16:
-                    vision_data = vision_data.to(torch.bfloat16)
-                elif self.config.fp16:
-                    vision_data = vision_data.to(torch.float16)
                 vision_embeds = self.vision_model(
                     vision_data=vision_data, # If None, vision model should use intermediate outputs (EPP > 1)
                     grid_thw=vision_grid_thw # should provided in each EPP stage
@@ -268,6 +262,7 @@ class Qwen2_5VLModel(MegatronModule):
                 )  # [text_seq_len, b, h_language]
         else:
             combined_embeddings = None
+
         output = self.language_model(
             input_ids=None,
             position_ids=position_ids,              # None in encoder
diff --git a/flagscale/train/models/qwen2_5_vl/transformer_config.py b/flagscale/train/models/qwen2_5_vl/transformer_config.py
index c138dbef..3dc620f2 100644
--- a/flagscale/train/models/qwen2_5_vl/transformer_config.py
+++ b/flagscale/train/models/qwen2_5_vl/transformer_config.py
@@ -24,9 +24,9 @@ def get_vision_model_config(args, config):
     # mlp: hidden_size -> intermediate_size -> embed_dim, silu
     # NOTE: here we provide a workaround to solve the wrong layer amount when VPP of decoder is on
     if config.num_layers in[28, 36]:
-        config.ffn_hidden_size = 3420 # 7B 72B
+        config.ffn_hidden_size = 3420 # 7B num_layers: 28
     else:
-        config.ffn_hidden_size = 3456 # 32B
+        config.ffn_hidden_size = 3456
 
     if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
         config.num_layers = 32 * parallel_state.get_virtual_pipeline_model_parallel_world_size() # depth
@@ -67,10 +67,6 @@ def get_vision_model_config(args, config):
     config.first_pipeline_num_layers = None
     config.num_layers_in_first_pipeline_stage = None
     config.num_layers_in_last_pipeline_stage = None
-    if args.vision_recompute_layer_steps != 0:
-        config.recompute_method="uniform"
-        config.recompute_granularity="full"
-        config.recompute_num_layers=args.vision_recompute_layer_steps # 16 for 32B
     return config
 
 
diff --git a/flagscale/train/models/qwen2_5_vl/vision_attention.py b/flagscale/train/models/qwen2_5_vl/vision_attention.py
index f92a97b0..8975383e 100644
--- a/flagscale/train/models/qwen2_5_vl/vision_attention.py
+++ b/flagscale/train/models/qwen2_5_vl/vision_attention.py
@@ -96,7 +96,7 @@ def _apply_rotary_pos_emb_bshd_vision(
     """
     rot_dim = freqs.shape[-1]
     input_dtype = t.dtype
-    t = t.float()
+    t = t.to(freqs.dtype)
     # ideally t_pass is empty so rotary pos embedding is applied to all tensor t
     t, t_pass = t[..., :rot_dim], t[..., rot_dim:]
 
@@ -107,8 +107,8 @@ def _apply_rotary_pos_emb_bshd_vision(
 
     # first part is cosine component
     # second part is sine component, need to change signs with _rotate_half method
-    cos_ = torch.cos(freqs).float()
-    sin_ = torch.sin(freqs).float()
+    cos_ = (torch.cos(freqs) * mscale)
+    sin_ = (torch.sin(freqs) * mscale)
     t = (t * cos_) + (_rotate_half(t, rotary_interleaved) * sin_)
     return torch.cat((t, t_pass), dim=-1).to(input_dtype)
 
diff --git a/flagscale/train/models/qwen2_5_vl/vit_model.py b/flagscale/train/models/qwen2_5_vl/vit_model.py
index 2a21fe14..5de99a6a 100644
--- a/flagscale/train/models/qwen2_5_vl/vit_model.py
+++ b/flagscale/train/models/qwen2_5_vl/vit_model.py
@@ -62,15 +62,14 @@ class VisionRotaryEmbedding(nn.Module):
     """
     def __init__(self, dim: int, theta: float = 10000.0) -> None:
         super().__init__()
-        # NOTE(lizhiyu): print inv_freq to check it.
-        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.bfloat16) / dim))
+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
         self.register_buffer("inv_freq", inv_freq, persistent=False)
 
     def forward(self, seqlen: int) -> torch.Tensor:
         seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
         # freqs [seq_len, dim // 2]
         freqs = torch.outer(seq, self.inv_freq)
-        return freqs
+        return freqs.float()
 
 # reference from https://github.com/huggingface/transformers/blob/0ad3710d4767d4ac7ee95f33f8554373e59efade/src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py#L243
 class Qwen2_5VisionModel(VisionModule):
diff --git a/flagscale/train/train.py b/flagscale/train/train.py
index 7039c1a2..5da202d0 100644
--- a/flagscale/train/train.py
+++ b/flagscale/train/train.py
@@ -14,20 +14,16 @@ from typing import List
 
 import torch.distributed
 from megatron.training.log_handler import CustomHandler
-
 # Make default logging level INFO, but filter out all log messages not from MCore.
 logging.basicConfig(handlers=[CustomHandler()], level=logging.INFO)
 from megatron.training.theoretical_memory_usage import report_theoretical_memory
 import time
-
 # The earliest we can measure the start time.
 _TRAIN_START_TIME = time.time()
 import torch
 
 try:
-    from megatron.post_training.algos.distillation import (
-        get_tensor_shapes_adjust_fn_for_distillation,
-    )
+    from megatron.post_training.algos.distillation import get_tensor_shapes_adjust_fn_for_distillation
 
     has_nvidia_modelopt = True
 except ImportError:
@@ -45,10 +41,9 @@ from megatron.training.checkpointing import load_checkpoint
 from megatron.training.checkpointing import save_checkpoint
 from megatron.training.checkpointing import checkpoint_exists
 from megatron.core.transformer.module import Float16Module
-from megatron.core.distributed import DistributedDataParallelConfig, TorchFullyShardedDataParallelConfig
+from megatron.core.distributed import DistributedDataParallelConfig
 from megatron.core.distributed import DistributedDataParallel as DDP
 from megatron.core.distributed.custom_fsdp import FullyShardedDataParallel as custom_FSDP
-
 try:
     from megatron.core.distributed import TorchFullyShardedDataParallel as torch_FSDP
 
@@ -68,7 +63,10 @@ from megatron.core.rerun_state_machine import (
 from megatron.training.initialize import initialize_megatron
 from megatron.training.initialize import write_args_to_tensorboard
 from megatron.training.initialize import set_jit_fusion_options
-from megatron.training.utils import get_batch_on_this_cp_rank, get_batch_on_this_tp_rank
+from megatron.training.utils import (
+    get_batch_on_this_cp_rank,
+    get_batch_on_this_tp_rank,
+)
 from megatron.legacy.data.data_samplers import build_pretraining_data_loader
 from megatron.core.optimizer_param_scheduler import OptimizerParamScheduler
 from megatron.core.transformer.moe import upcycling_utils
@@ -91,8 +89,7 @@ from megatron.core.num_microbatches_calculator import (
     get_current_global_batch_size,
     get_current_running_global_batch_size,
     get_num_microbatches,
-    update_num_microbatches,
-)
+    update_num_microbatches)
 
 from megatron.training.async_utils import maybe_finalize_async_save
 from megatron.training.utils import (
@@ -164,22 +161,15 @@ def num_floating_point_operations(args, batch_size):
     def mlp_layer_flops(batch_size, seq_len, hidden_size, expansion=4.0, swiglu=False):
         """Calculate FLOPs for an MLP layer."""
         scale_factor = 3.0 / 2.0 if swiglu else 1.0
-        return 4 * expansion * scale_factor * batch_size * seq_len * hidden_size**2
+        return 4 * expansion * scale_factor * batch_size * seq_len * hidden_size ** 2
 
-    def attn_layer_flops(
-        batch_size, seq_len, hidden_size, num_heads, gqa=True, gqa_groups=8, kv_channels=None
-    ):
+    def attn_layer_flops(batch_size, seq_len, hidden_size, num_heads, gqa=True,
+                         gqa_groups=8, kv_channels=None):
         """Calculate FLOPs for an attention layer."""
         p = (kv_channels * num_heads / hidden_size) if kv_channels else 1
         g = gqa_groups if gqa else num_heads
-        return (
-            4
-            * batch_size
-            * seq_len
-            * hidden_size
-            * p
-            * (hidden_size + (hidden_size * (g / num_heads)) + (seq_len / 2))
-        )
+        return 4 * batch_size * seq_len * hidden_size * p * (
+                hidden_size + (hidden_size * (g / num_heads)) + (seq_len / 2 ))
 
     def mamba_layer_flops(batch_size, seq_len, hidden_size, state_dim=16,
                           head_dim=64, num_groups=1, num_heads=128):
@@ -192,15 +182,10 @@ def num_floating_point_operations(args, batch_size):
         else:
             nheads = d_in // head_dim
         return (
-            (
-                2
-                * batch_size
-                * seq_len
-                * hidden_size
-                * (2 * d_in + 2 * num_groups * state_dim + nheads)
-            )  # in_proj
-            + (7 * batch_size * seq_len * d_in * state_dim)  # scan
-            + (2 * batch_size * seq_len * d_in * hidden_size)  # out_proj
+                (2 * batch_size * seq_len * hidden_size * (
+                        2 * d_in + 2 * num_groups * state_dim + nheads)) +  # in_proj
+                (7 * batch_size * seq_len * d_in * state_dim) +  # scan
+                (2 * batch_size * seq_len * d_in * hidden_size)  # out_proj
         )
 
     def hybrid_flops(batch_size, seq_len, hidden_size,
@@ -269,11 +254,7 @@ def num_floating_point_operations(args, batch_size):
             mtp_num_layers = 0
             num_layers = args.num_layers
 
-        moe_ffn_hidden_size = (
-            args.moe_ffn_hidden_size
-            if args.moe_ffn_hidden_size is not None
-            else args.ffn_hidden_size
-        )
+        moe_ffn_hidden_size = args.moe_ffn_hidden_size if args.moe_ffn_hidden_size is not None else args.ffn_hidden_size
         shared_expert_ffn_hidden_size = (
             0
             if args.moe_shared_expert_intermediate_size is None
@@ -309,38 +290,26 @@ def num_floating_point_operations(args, batch_size):
             '''
             ## MLA
             if args.q_lora_rank is None:
-                q_term = (
-                    args.hidden_size
-                    * args.num_attention_heads
-                    * (args.qk_head_dim + args.qk_pos_emb_head_dim)
-                )
+                q_term = args.hidden_size * args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim)
             else:
-                q_term = args.q_lora_rank * (
-                    args.hidden_size
-                    + args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim)
-                    + 1
-                )
+                q_term = args.q_lora_rank * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim) + 1)
             self_attn_term = (
-                3
-                * 2  # fwd(1) + bwd(2) *FMA
+                3*2 # fwd(1) + bwd(2) *FMA
                 * num_layers
                 * (
                     ## q lora + rope + q norm
                     q_term
+
                     ## kv lora + rope + kv norm
                     + args.kv_lora_rank
-                    * (
-                        args.hidden_size
-                        + args.num_attention_heads * (args.qk_head_dim + args.v_head_dim)
-                        + 1
-                    )
+                    * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.v_head_dim) + 1)
                     + args.hidden_size * args.qk_pos_emb_head_dim
+
                     ## o proj
                     + (args.num_attention_heads * args.v_head_dim) * args.hidden_size
+
                     ## core attn
-                    + args.seq_length
-                    * (args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim))
-                    / 2
+                    + args.seq_length * (args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim)) / 2
                     + args.seq_length * args.num_attention_heads * args.v_head_dim / 2
                 )
             )
@@ -358,48 +327,53 @@ def num_floating_point_operations(args, batch_size):
                         + (args.num_query_groups / args.num_attention_heads)
                         # # Only half of the attention matrix is non-zero and needs to be multiplied with V.
                         + (args.seq_length / args.hidden_size / 2)
-                    )
-                    * query_projection_to_hidden_size_ratio
+                    ) * query_projection_to_hidden_size_ratio
                 )
             )
 
-        total_floating_point_operations = (
-            batch_size
-            * args.seq_length
+        total_floating_point_operations = batch_size * args.seq_length * (
+            # MLP
+            expansion_factor
+            * num_layers
+            * args.hidden_size
             * (
-                # MLP
-                expansion_factor
-                * num_layers
-                * args.hidden_size
-                * (
-                    # dense layer (deepseek v2, v3 style)
-                    (args.ffn_hidden_size * gated_linear_multiplier)
-                    * (num_dense_layers / num_layers)
-                    # routed experts
-                    + (moe_ffn_hidden_size * num_experts_routed_to * gated_linear_multiplier)
-                    * (num_moe_layers / num_layers)
-                    # Shared Experts.
-                    + (shared_expert_ffn_hidden_size * gated_linear_multiplier)
-                    * (num_moe_layers / num_layers)
-                )
-                # Self Attention
-                + self_attn_term
-                # MTP norms and proj
-                + 3
-                * 2
-                * mtp_num_layers
-                * (
-                    # MTP eh norm + final nrom
-                    3 * args.hidden_size
-                    # MTH eh proj
-                    + 2 * args.hidden_size * args.hidden_size
-                )
-                # Logit.
-                + 3 * 2 * args.hidden_size * args.padded_vocab_size * (mtp_num_layers + 1)
+                # dense layer (deepseek v2, v3 style)
+                (
+                    args.ffn_hidden_size
+                    * gated_linear_multiplier
+                ) * (num_dense_layers/num_layers)
+                # routed experts
+                + (
+                    moe_ffn_hidden_size
+                    * num_experts_routed_to
+                    * gated_linear_multiplier
+                ) * (num_moe_layers/num_layers)
+                # Shared Experts.
+                + (
+                    shared_expert_ffn_hidden_size
+                    * gated_linear_multiplier
+                ) * (num_moe_layers/num_layers)
             )
+            # Self Attention
+            + self_attn_term
+            # MTP norms and proj
+            + 3*2
+            * mtp_num_layers
+            * (
+                # MTP eh norm + final nrom
+                3 * args.hidden_size
+                # MTH eh proj
+                + 2 * args.hidden_size * args.hidden_size
+            )
+            # Logit.
+            + 3*2
+            * args.hidden_size
+            * args.padded_vocab_size
+            * (mtp_num_layers + 1)
         )
         return total_floating_point_operations
 
+
     # Main entrypoint for FLOPs calculation.
     if args.is_hybrid_model:
         # Calculate the number of each type of layer.
@@ -423,7 +397,7 @@ def num_floating_point_operations(args, batch_size):
             kv_channels=args.kv_channels,
             mlp_expansion=args.ffn_hidden_size / args.hidden_size,
             swiglu=args.swiglu,
-            vocab_size=args.padded_vocab_size,
+            vocab_size=args.padded_vocab_size
         )
     else:
         # Compute standard Transformer model FLOPs.
@@ -656,7 +630,8 @@ def get_start_time_from_progress_log():
             line_tokens = line.split('\t')
             world_size_in_line = _get_field(line_tokens[2], int)
             if line_tokens[3] == "Saved checkpoint":
-                latest_num_floating_point_operations = _get_field(line_tokens[7], float)
+                latest_num_floating_point_operations = \
+                    _get_field(line_tokens[7], float)
             if world_size_in_line != args.world_size:
                 # Re-start search if we see a different world size.
                 start_time = None
@@ -665,16 +640,16 @@ def get_start_time_from_progress_log():
             if line_tokens[3] == "Starting job":
                 if start_time is None:
                     start_time = line_tokens[0]
-                    start_num_floating_point_operations = latest_num_floating_point_operations
-    assert (
-        start_time is not None and start_num_floating_point_operations is not None
-    ), "Should have seen at least one 'Starting job' entry with same world_size"
-    return datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S'), start_num_floating_point_operations
+                    start_num_floating_point_operations = \
+                        latest_num_floating_point_operations
+    assert start_time is not None and start_num_floating_point_operations is not None, \
+        "Should have seen at least one 'Starting job' entry with same world_size"
+    return datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S'), \
+        start_num_floating_point_operations
 
 
 def preprocess_common_state_dict(common_state_dict):
     import copy
-
     # Convert args key of type namespace to dictionary
     preprocessed_common_state_dict = copy.deepcopy(common_state_dict)
     preprocessed_common_state_dict['args'] = vars(preprocessed_common_state_dict['args'])
@@ -914,7 +889,7 @@ def pretrain(
         extra_args_provider=extra_args_provider,
         args_defaults=args_defaults,
         get_embedding_ranks=get_embedding_ranks,
-        get_position_embedding_ranks=get_position_embedding_ranks,
+        get_position_embedding_ranks=get_position_embedding_ranks
     )
 
     args = get_args()
@@ -937,19 +912,24 @@ def pretrain(
     # image ... launches.
     global _TRAIN_START_TIME
     if "cpu:gloo" == torch.distributed.get_backend():
-        start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device='cpu')
+        start_time_tensor = torch.tensor([_TRAIN_START_TIME],
+                                         dtype=torch.double,
+                                         device='cpu')
     else:
-        start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device='cuda')
-    torch.distributed.all_reduce(start_time_tensor, op=torch.distributed.ReduceOp.MIN)
+        start_time_tensor = torch.tensor([_TRAIN_START_TIME],
+                                         dtype=torch.double,
+                                         device='cuda')
+
+    torch.distributed.all_reduce(start_time_tensor,
+                                 op=torch.distributed.ReduceOp.MIN)
     _TRAIN_START_TIME = start_time_tensor.item()
 
     app_metrics = {}
     app_metrics['app_start_time'] = round(_TRAIN_START_TIME * 1000.0)
     app_metrics['app_model_init_start_time'] = round(_TRAIN_START_TIME * 1000.0)
 
-    print_rank_0(
-        'time to initialize megatron (seconds): {:.3f}'.format(time.time() - _TRAIN_START_TIME)
-    )
+    print_rank_0('time to initialize megatron (seconds): {:.3f}'.format(
+        time.time() - _TRAIN_START_TIME))
     print_datetime('after megatron is initialized')
     app_metrics['app_model_init_finish_time'] = one_logger_utils.get_timestamp_in_ms()
 
@@ -959,33 +939,28 @@ def pretrain(
     # Context used for persisting some state between checkpoint saves.
     if args.non_persistent_ckpt_type == 'local':
         try:
-            from nvidia_resiliency_ext.checkpointing.local.ckpt_managers.local_manager import (
-                LocalCheckpointManager,
-            )
-            from nvidia_resiliency_ext.checkpointing.local.replication.group_utils import (
-                parse_group_sequence,
-                GroupWrapper,
-            )
-            from nvidia_resiliency_ext.checkpointing.local.replication.strategies import (
-                CliqueReplicationStrategy,
-            )
+            from nvidia_resiliency_ext.checkpointing.local.ckpt_managers.local_manager import \
+                LocalCheckpointManager
+            from nvidia_resiliency_ext.checkpointing.local.replication.group_utils import \
+                parse_group_sequence, GroupWrapper
+            from nvidia_resiliency_ext.checkpointing.local.replication.strategies import \
+                CliqueReplicationStrategy
         except ModuleNotFoundError:
-            raise RuntimeError(
-                "The 'nvidia_resiliency_ext' module is required for local "
-                "checkpointing but was not found. Please ensure it is installed."
-            )
+            raise RuntimeError("The 'nvidia_resiliency_ext' module is required for local "
+                               "checkpointing but was not found. Please ensure it is installed.")
 
         if args.replication:
             repl_strategy = CliqueReplicationStrategy.from_replication_params(
-                args.replication_jump, args.replication_factor
+                args.replication_jump,
+                args.replication_factor
             )
         else:
             repl_strategy = None
 
         checkpointing_context = {
-            'local_checkpoint_manager': LocalCheckpointManager(
-                args.non_persistent_local_ckpt_dir, repl_strategy=repl_strategy
-            )
+            'local_checkpoint_manager': LocalCheckpointManager(args.non_persistent_local_ckpt_dir,
+                                                               repl_strategy=repl_strategy
+                                                               )
         }
     else:
         checkpointing_context = {}
@@ -999,50 +974,46 @@ def pretrain(
     timers('model-and-optimizer-setup', log_level=0).start(barrier=True)
     app_metrics['app_build_optimizer_start_time'] = one_logger_utils.get_timestamp_in_ms()
     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
-        model_provider, model_type, checkpointing_context=checkpointing_context
-    )
+        model_provider, model_type, checkpointing_context=checkpointing_context)
 
     timers('model-and-optimizer-setup').stop()
-    print_datetime('after model, optimizer, and learning rate ' 'scheduler are built')
+    print_datetime('after model, optimizer, and learning rate '
+                   'scheduler are built')
     app_metrics['app_build_optimizer_finish_time'] = one_logger_utils.get_timestamp_in_ms()
     config = get_model_config(model[0])
 
     # Data stuff.
     app_metrics['app_build_dataiters_start_time'] = one_logger_utils.get_timestamp_in_ms()
-    timers('train/valid/test-data-iterators-setup', log_level=0).start(barrier=True)
+    timers('train/valid/test-data-iterators-setup', log_level=0).start(
+        barrier=True)
     if args.virtual_pipeline_model_parallel_size is not None:
         train_data_iterator = []
         valid_data_iterator = []
         test_data_iterator = []
         for i in range(len(model)):
             mpu.set_virtual_pipeline_model_parallel_rank(i)
-            iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
+            iterators = build_train_valid_test_data_iterators(
+                train_valid_test_dataset_provider)
             train_data_iterator.append(iterators[0])
             valid_data_iterator.append(iterators[1])
             test_data_iterator.append(iterators[2])
     else:
-        train_data_iterator, valid_data_iterator, test_data_iterator = (
-            build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
-        )
+        train_data_iterator, valid_data_iterator, test_data_iterator \
+            = build_train_valid_test_data_iterators(
+                train_valid_test_dataset_provider)
     timers('train/valid/test-data-iterators-setup').stop()
     print_datetime('after dataloaders are built')
     app_metrics['app_build_dataiters_finish_time'] = one_logger_utils.get_timestamp_in_ms()
 
     # Track if training is enabled. Can only be done once args.do_train is assigned after dataloader is built.
-    one_logger_utils.track_config_flags(
-        args.train_iters,
-        args.skip_train,
-        args.do_train,
-        args.do_valid,
-        args.do_test,
-        args.dataloader_type,
-        args.retro_project_dir,
-        args.retro_cyclic_train_iters,
-    )
+    one_logger_utils.track_config_flags(args.train_iters, args.skip_train, args.do_train,
+                                        args.do_valid, args.do_test, args.dataloader_type,
+                                        args.retro_project_dir, args.retro_cyclic_train_iters)
 
     # Print setup timing.
     print_rank_0('done with setup ...')
-    timers.log(['model-and-optimizer-setup', 'train/valid/test-data-iterators-setup'], barrier=True)
+    timers.log(['model-and-optimizer-setup',
+                'train/valid/test-data-iterators-setup'], barrier=True)
 
     one_logger = get_one_logger()
     one_logger and one_logger.log_metrics(app_metrics)
@@ -1059,36 +1030,23 @@ def pretrain(
         if args.do_train and args.train_iters > 0:
             iteration, num_floating_point_operations_so_far = train(
                 forward_step_func,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                train_data_iterator,
-                valid_data_iterator,
-                process_non_loss_data_func,
-                config,
-                checkpointing_context,
-                non_loss_data_func,
-                extra_valid_dataset_provider,
-            )
+                model, optimizer, opt_param_scheduler,
+                train_data_iterator, valid_data_iterator,
+                process_non_loss_data_func, config, checkpointing_context,
+                non_loss_data_func, extra_valid_dataset_provider)
 
         print_datetime('after training is done')
 
         if not args.auto_tune:
             if args.save and iteration != 0 and iteration % args.save_interval != 0:
-                save_checkpoint(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                    preprocess_common_state_dict_fn=preprocess_common_state_dict,
-                )
+                save_checkpoint(iteration, model, optimizer, opt_param_scheduler,
+                                num_floating_point_operations_so_far, checkpointing_context,
+                                train_data_iterator=train_data_iterator,
+                                preprocess_common_state_dict_fn=preprocess_common_state_dict)
 
-        one_logger and one_logger.log_metrics(
-            {'app_train_loop_finish_time': one_logger_utils.get_timestamp_in_ms()}
-        )
+        one_logger and one_logger.log_metrics({
+            'app_train_loop_finish_time': one_logger_utils.get_timestamp_in_ms()
+        })
 
     else:
         print_rank_0('skipping training (--skip-train is on) ...')
@@ -1097,33 +1055,19 @@ def pretrain(
 
     if args.do_valid:
         prefix = f'iteration {iteration} on validation set'
-        evaluate_and_print_results(
-            prefix,
-            forward_step_func,
-            valid_data_iterator,
-            model,
-            iteration,
-            process_non_loss_data_func,
-            config,
-            verbose=True,
-            write_to_tensorboard=not args.skip_train,
-            non_loss_data_func=non_loss_data_func,
-        )
+        evaluate_and_print_results(prefix, forward_step_func,
+                                   valid_data_iterator, model,
+                                   iteration, process_non_loss_data_func, config,
+                                   verbose=True, write_to_tensorboard=not args.skip_train,
+                                   non_loss_data_func=non_loss_data_func)
 
     if args.do_test:
         prefix = f'iteration {iteration} on test set'
-        evaluate_and_print_results(
-            prefix,
-            forward_step_func,
-            test_data_iterator,
-            model,
-            iteration,
-            process_non_loss_data_func,
-            config,
-            verbose=True,
-            write_to_tensorboard=not args.skip_train,
-            non_loss_data_func=non_loss_data_func,
-        )
+        evaluate_and_print_results(prefix, forward_step_func,
+                                   test_data_iterator, model,
+                                   iteration, process_non_loss_data_func, config,
+                                   verbose=True, write_to_tensorboard=not args.skip_train,
+                                   non_loss_data_func=non_loss_data_func)
 
     if extra_valid_dataset_provider is not None:
         # NOTE(zhaoyinglia): Must rebuild the dataloaders for extra validation here,
@@ -1133,28 +1077,20 @@ def pretrain(
             extra_valid_data_iterator = []
             for i in range(len(model)):
                 mpu.set_virtual_pipeline_model_parallel_rank(i)
-                extra_iterators = build_extra_valid_data_iterators(extra_valid_dataset_provider)
+                extra_iterators = build_extra_valid_data_iterators(
+                    extra_valid_dataset_provider)
                 extra_valid_data_iterator.append(extra_iterators)
         else:
-            extra_valid_data_iterator = (
-                build_extra_valid_data_iterators(extra_valid_dataset_provider)
-            )
+            extra_valid_data_iterator = build_extra_valid_data_iterators(
+                extra_valid_dataset_provider)
         if getattr(args, "do_extra_valid", False):
             prefix = f'iteration {iteration} on extra validation set'
             for extra_valid_index, extra_valid_data_itr in enumerate(extra_valid_data_iterator):
-                extra_evaluate_and_print_results(
-                    extra_valid_index,
-                    prefix,
-                    forward_step_func,
-                    extra_valid_data_itr,
-                    model,
-                    iteration,
-                    process_non_loss_data_func,
-                    config,
-                    verbose=True,
-                    write_to_tensorboard=not args.skip_train,
-                    non_loss_data_func=non_loss_data_func
-                )
+                extra_evaluate_and_print_results(extra_valid_index, prefix, forward_step_func,
+                                                 extra_valid_data_itr, model,
+                                                 iteration, process_non_loss_data_func, config,
+                                                 verbose=True, write_to_tensorboard=not args.skip_train,
+                                                 non_loss_data_func=non_loss_data_func)
 
     wandb_writer = get_wandb_writer()
     if wandb_writer:
@@ -1164,9 +1100,9 @@ def pretrain(
     maybe_finalize_async_save(blocking=True, terminate=True)
     ft_integration.on_checkpointing_end(is_async_finalization=True)
 
-    one_logger and one_logger.log_metrics(
-        {'app_finish_time': one_logger_utils.get_timestamp_in_ms()}
-    )
+    one_logger and one_logger.log_metrics({
+        'app_finish_time': one_logger_utils.get_timestamp_in_ms()
+    })
 
     ft_integration.shutdown()
     one_logger_utils.finish()
@@ -1187,10 +1123,7 @@ def update_train_iters(args):
         iterations = 0
         consumed_samples = 0
         # Rampup phase.
-        while (
-            consumed_samples <= int(args.rampup_batch_size[2])
-            and consumed_samples <= args.train_samples
-        ):
+        while consumed_samples <= int(args.rampup_batch_size[2]) and consumed_samples <= args.train_samples:
             update_num_microbatches(consumed_samples, consistency_check=False)
             consumed_samples += get_current_global_batch_size()
             iterations += 1
@@ -1199,7 +1132,8 @@ def update_train_iters(args):
         # Constant phase
         # Note that we throw away any partial last batch.
         if args.train_samples > consumed_samples:
-            iterations += (args.train_samples - consumed_samples) // args.global_batch_size
+            iterations += (args.train_samples - consumed_samples) // \
+                          args.global_batch_size
         args.train_iters = iterations
 
     print_rank_0(f'setting training iterations to {args.train_iters}')
@@ -1212,28 +1146,26 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
 
     # Build model.
     def build_model():
-        if (
-            mpu.get_pipeline_model_parallel_world_size() > 1
-            and args.virtual_pipeline_model_parallel_size is not None
-        ):
+        if mpu.get_pipeline_model_parallel_world_size() > 1 and \
+        args.virtual_pipeline_model_parallel_size is not None:
             if model_type == ModelType.encoder_and_decoder:
-                assert (
-                    args.encoder_pipeline_model_parallel_size == 0
-                ), "Interleaved schedule not supported for model with encoder on separate PP rank"
+                assert args.encoder_pipeline_model_parallel_size == 0, \
+                    "Interleaved schedule not supported for model with encoder on separate PP rank"
             model = []
             for i in range(args.virtual_pipeline_model_parallel_size):
                 mpu.set_virtual_pipeline_model_parallel_rank(i)
                 # Set pre_process and post_process only after virtual rank is set.
-                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False)
-                post_process = mpu.is_pipeline_last_stage(ignore_virtual=False)
+                pre_process = mpu.is_pipeline_first_stage()
+                post_process = mpu.is_pipeline_last_stage()
                 this_model = model_provider_func(
-                    pre_process=pre_process, post_process=post_process, vp_stage=i)
+                    pre_process=pre_process,
+                    post_process=post_process
+                )
                 this_model.model_type = model_type
-                this_model.vp_stage = i
                 model.append(this_model)
         else:
-            pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False)
-            post_process = mpu.is_pipeline_last_stage(ignore_virtual=False)
+            pre_process = mpu.is_pipeline_first_stage()
+            post_process = mpu.is_pipeline_last_stage()
             add_encoder = True
             add_decoder = True
             if model_type == ModelType.encoder_and_decoder:
@@ -1249,13 +1181,14 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
                     pre_process=pre_process,
                     post_process=post_process,
                     add_encoder=add_encoder,
-                    add_decoder=add_decoder,
-                )
+                    add_decoder=add_decoder)
             else:
-                model = model_provider_func(pre_process=pre_process, post_process=post_process)
+                model = model_provider_func(
+                    pre_process=pre_process,
+                    post_process=post_process
+                )
             model.model_type = model_type
         return model
-
     if args.init_model_with_meta_device:
         with torch.device('meta'):
             model = build_model()
@@ -1275,26 +1208,20 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
 
     # Print number of parameters.
     num_parameters = sum(
-        [sum([p.nelement() for p in model_module.parameters()]) for model_module in model]
+        [sum([p.nelement() for p in model_module.parameters()])
+         for model_module in model]
     )
     if mpu.get_data_parallel_rank() == 0:
-        print(
-            ' > number of parameters on (tensor, pipeline) '
-            'model parallel rank ({}, {}): {}'.format(
-                mpu.get_tensor_model_parallel_rank(),
-                mpu.get_pipeline_model_parallel_rank(),
-                num_parameters,
-            ),
-            flush=True,
-        )
+        print(' > number of parameters on (tensor, pipeline) '
+              'model parallel rank ({}, {}): {}'.format(
+            mpu.get_tensor_model_parallel_rank(),
+            mpu.get_pipeline_model_parallel_rank(),
+            num_parameters), flush=True)
 
     # GPU allocation.
     # For FSDP2, we don't allocate GPU memory here. We allocate GPU memory
     # in the fully_shard function of FSDP2 instead.
-    if (
-        not (args.use_torch_fsdp2 and args.use_cpu_initialization)
-        and not args.init_model_with_meta_device
-    ):
+    if not (args.use_torch_fsdp2 and args.use_cpu_initialization) and not args.init_model_with_meta_device:
         for model_module in model:
             model_module.cuda(torch.cuda.current_device())
 
@@ -1321,32 +1248,30 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
 
         config = get_model_config(model[0])
 
-        if getattr(args, "use_torch_fsdp2", False):
-            reshard_after_forward = getattr(args, "torch_fsdp2_reshard_after_forward", True)
-            ddp_config = TorchFullyShardedDataParallelConfig(reshard_after_forward=reshard_after_forward)
+        kwargs = {}
+        for f in dataclasses.fields(DistributedDataParallelConfig):
+            if hasattr(args, f.name):
+                kwargs[f.name] = getattr(args, f.name)
+        kwargs['grad_reduce_in_fp32'] = args.accumulate_allreduce_grads_in_fp32
+        kwargs['check_for_nan_in_grad'] = args.check_for_nan_in_loss_and_grad
+        kwargs['check_for_large_grads'] = args.check_for_large_grads
+        if args.ddp_num_buckets is not None:
+            assert args.ddp_bucket_size is None, \
+                "Cannot specify both --ddp-num-buckets and --ddp-bucket-size"
+            assert args.ddp_num_buckets > 0, \
+                "--ddp-num-buckets must be greater than 0"
+            kwargs['bucket_size'] = num_parameters // args.ddp_num_buckets
         else:
-            kwargs = {}
-            for f in dataclasses.fields(DistributedDataParallelConfig):
-                if hasattr(args, f.name):
-                    kwargs[f.name] = getattr(args, f.name)
-            kwargs['grad_reduce_in_fp32'] = args.accumulate_allreduce_grads_in_fp32
-            kwargs['check_for_nan_in_grad'] = args.check_for_nan_in_loss_and_grad
-            kwargs['check_for_large_grads'] = args.check_for_large_grads
-            if args.ddp_num_buckets is not None:
-                assert args.ddp_bucket_size is None, \
-                    "Cannot specify both --ddp-num-buckets and --ddp-bucket-size"
-                assert args.ddp_num_buckets > 0, \
-                    "--ddp-num-buckets must be greater than 0"
-                kwargs['bucket_size'] = num_parameters // args.ddp_num_buckets
-            else:
-                kwargs['bucket_size'] = args.ddp_bucket_size
-            kwargs['pad_buckets_for_high_nccl_busbw'] = args.ddp_pad_buckets_for_high_nccl_busbw
-            kwargs['average_in_collective'] = args.ddp_average_in_collective
-            if args.use_custom_fsdp and args.use_precision_aware_optimizer:
-                kwargs["preserve_fp32_weights"] = False
-            ddp_config = DistributedDataParallelConfig(**kwargs)
-
+            kwargs['bucket_size'] = args.ddp_bucket_size
+        kwargs['pad_buckets_for_high_nccl_busbw'] = args.ddp_pad_buckets_for_high_nccl_busbw
+        kwargs['average_in_collective'] = args.ddp_average_in_collective
+        if args.use_custom_fsdp and args.use_precision_aware_optimizer:
+            kwargs["preserve_fp32_weights"] = False
+        ddp_config = DistributedDataParallelConfig(**kwargs)
+
+        if not getattr(args, "use_torch_fsdp2", False):
             # In the custom FSDP and DDP use path, we need to initialize the bucket size.
+
             # If bucket_size is not provided as an input, use sane default.
             # If using very large dp_sizes, make buckets larger to ensure that chunks used in NCCL
             # ring-reduce implementations are large enough to remain bandwidth-bound rather than
@@ -1359,18 +1284,13 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
             if not ddp_config.overlap_grad_reduce:
                 ddp_config.bucket_size = None
 
-        model = [
-            DP(
-                config=config,
-                ddp_config=ddp_config,
-                module=model_chunk,
-                # Turn off bucketing for model_chunk 2 onwards, since communication for these
-                # model chunks is overlapped with compute anyway.
-                disable_bucketing=(model_chunk_idx > 0)
-                or args.overlap_param_gather_with_optimizer_step,
-            )
-            for (model_chunk_idx, model_chunk) in enumerate(model)
-        ]
+        model = [DP(config=config,
+                     ddp_config=ddp_config,
+                     module=model_chunk,
+                     # Turn off bucketing for model_chunk 2 onwards, since communication for these
+                     # model chunks is overlapped with compute anyway.
+                     disable_bucketing=(model_chunk_idx > 0) or args.overlap_param_gather_with_optimizer_step)
+                 for (model_chunk_idx, model_chunk) in enumerate(model)]
 
         # Broadcast params from data parallel src rank to other data parallel ranks.
         if args.data_parallel_random_init:
@@ -1413,7 +1333,8 @@ def get_optimizer_param_scheduler(optimizer):
         else:
             lr_warmup_steps = args.lr_warmup_samples
     else:
-        raise Exception('either train-iters or train-samples should be provided.')
+        raise Exception(
+            'either train-iters or train-samples should be provided.')
 
     stablelm2_scheduler_config = None
     if args.lr_decay_style == 'stablelm2-scheduler':
@@ -1443,20 +1364,17 @@ def get_optimizer_param_scheduler(optimizer):
         override_opt_param_scheduler=args.override_opt_param_scheduler,
         wsd_decay_steps=wsd_decay_steps,
         lr_wsd_decay_style=args.lr_wsd_decay_style,
-        stablelm2_scheduler_config=stablelm2_scheduler_config,
-    )
+        stablelm2_scheduler_config=stablelm2_scheduler_config)
 
     return opt_param_scheduler
 
 
-def setup_model_and_optimizer(
-    model_provider_func,
-    model_type,
-    no_wd_decay_cond=None,
-    scale_lr_cond=None,
-    lr_mult=1.0,
-    checkpointing_context=None,
-):
+def setup_model_and_optimizer(model_provider_func,
+                              model_type,
+                              no_wd_decay_cond=None,
+                              scale_lr_cond=None,
+                              lr_mult=1.0,
+                              checkpointing_context=None):
     """Setup model and optimizer."""
     args = get_args()
     timers = get_timers()
@@ -1477,24 +1395,19 @@ def setup_model_and_optimizer(
                 kwargs[f.name] = getattr(args, f.name)
         config = OptimizerConfig(**kwargs)
     config.timers = timers
-    optimizer = get_megatron_optimizer(
-        config,
-        model,
-        no_wd_decay_cond,
-        scale_lr_cond,
-        lr_mult,
-        use_gloo_process_groups=args.enable_gloo_process_groups,
-    )
+    optimizer = get_megatron_optimizer(config, model, no_wd_decay_cond,
+                                       scale_lr_cond, lr_mult,
+                                       use_gloo_process_groups=args.enable_gloo_process_groups)
     opt_param_scheduler = get_optimizer_param_scheduler(optimizer)
 
     if args.moe_use_upcycling:
         torch.distributed.barrier()
-        assert not checkpoint_exists(args.save), (
-            "The upcycling destination directory already exists. "
+        assert not checkpoint_exists(
+            args.save
+        ), ("The upcycling destination directory already exists. "
             "Please check if --moe-use-upcycling is mistakenly enabled. "
             "Upcycling should only be set for the first run when converting the dense model. "
-            "All subsequent runs should remove this flag. "
-        )
+            "All subsequent runs should remove this flag. ")
         num_experts = args.num_experts
         args.num_experts = None
         expert_model_parallel_size = args.expert_model_parallel_size
@@ -1506,57 +1419,38 @@ def setup_model_and_optimizer(
             load_checkpoint,
             unwrapped_model,
             dense_model_for_upcycling,
-            load_kwargs={
-                'model': dense_model_for_upcycling,
-                'optimizer': None,
-                'opt_param_scheduler': None,
-            },
+            load_kwargs = {'model': dense_model_for_upcycling, 'optimizer': None, 'opt_param_scheduler': None}
         )
         args.iteration = 1
-        save_checkpoint(
-            args.iteration, model, None, None, args.num_floating_point_operations_so_far
-        )
+        save_checkpoint(args.iteration, model, None, None, args.num_floating_point_operations_so_far)
         torch.distributed.barrier()
         del dense_model_for_upcycling
         if (args.fp16 or args.bf16) and optimizer is not None:
             optimizer.reload_model_params()
         print_rank_0(f'Upcycled checkpoint saved to {args.save}')
 
-    if (
-        args.load is not None or args.pretrained_checkpoint is not None
-    ) and not args.moe_use_upcycling:
-        one_logger and one_logger.log_metrics(
-            {'load_checkpoint_start_time': one_logger_utils.get_timestamp_in_ms()}
-        )
+    if (args.load is not None or args.pretrained_checkpoint is not None) and not args.moe_use_upcycling:
+        one_logger and one_logger.log_metrics({
+            'load_checkpoint_start_time': one_logger_utils.get_timestamp_in_ms()
+        })
         timers('load-checkpoint', log_level=0).start(barrier=True)
 
         args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(
-            model,
-            optimizer,
-            opt_param_scheduler,
-            checkpointing_context=checkpointing_context,
-            skip_load_to_model_and_opt=HAVE_FSDP2
-            and getattr(args, "use_torch_fsdp2", False)
-            and args.ckpt_format == "torch_dist",
-        )
+                model, optimizer, opt_param_scheduler, checkpointing_context=checkpointing_context,
+                skip_load_to_model_and_opt=HAVE_FSDP2 and getattr(args, "use_torch_fsdp2", False) and args.ckpt_format == "torch_dist")
         timers('load-checkpoint').stop(barrier=True)
         timers.log(['load-checkpoint'])
-        one_logger and one_logger.log_metrics(
-            {
-                'load_checkpoint_finish_time': one_logger_utils.get_timestamp_in_ms(),
-                'load_checkpoint_time': timers('load-checkpoint').active_time(),
-            }
-        )
+        one_logger and one_logger.log_metrics({
+            'load_checkpoint_finish_time': one_logger_utils.get_timestamp_in_ms(),
+            'load_checkpoint_time': timers('load-checkpoint').active_time()
+        })
     else:
         args.iteration = 0
         args.num_floating_point_operations_so_far = 0
 
     # get model without FP16 and/or DDP wrappers
-    if (
-        args.iteration == 0
-        and len(unwrapped_model) == 1
-        and hasattr(unwrapped_model[0], 'init_state_dict_from_bert')
-    ):
+    if args.iteration == 0 and len(unwrapped_model) == 1 \
+        and hasattr(unwrapped_model[0], 'init_state_dict_from_bert'):
         print_rank_0("Initializing ICT from pretrained BERT model")
         unwrapped_model[0].init_state_dict_from_bert()
         if args.fp16:
@@ -1569,14 +1463,9 @@ def setup_model_and_optimizer(
         args.save = os.path.join(args.ckpt_convert_save, args.ckpt_convert_format)
         update_use_dist_ckpt(args)
 
-        save_checkpoint(
-            args.iteration,
-            model,
-            optimizer,
-            opt_param_scheduler,
-            args.num_floating_point_operations_so_far,
-            preprocess_common_state_dict_fn=preprocess_common_state_dict,
-        )
+        save_checkpoint(args.iteration, model, optimizer, opt_param_scheduler,
+                        args.num_floating_point_operations_so_far,
+                        preprocess_common_state_dict_fn=preprocess_common_state_dict)
 
         print_rank_0("> converted checkpoint: %s -> %s." % (load_ckpt_format, args.ckpt_format))
         torch.distributed.barrier()
@@ -1594,7 +1483,8 @@ def dummy_train_step(data_iterator):
         batch = get_batch_on_this_cp_rank(batch)
 
 
-def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config):
+def train_step(forward_step_func, data_iterator,
+               model, optimizer, opt_param_scheduler, config):
     """Single training step."""
     args = get_args()
     timers = get_timers()
@@ -1638,8 +1528,7 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
             micro_batch_size=args.micro_batch_size,
             decoder_seq_length=args.decoder_seq_length,
             forward_only=False,
-            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,
-        )
+            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn)
     should_checkpoint, should_exit, exit_code = rerun_state_machine.should_checkpoint_and_exit()
     if should_exit:
         return {}, True, should_checkpoint, should_exit, exit_code, None, None
@@ -1687,7 +1576,9 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
 
     # Update learning rate.
     if update_successful:
-        increment = get_num_microbatches() * args.micro_batch_size * args.data_parallel_size
+        increment = get_num_microbatches() * \
+                    args.micro_batch_size * \
+                    args.data_parallel_size
         opt_param_scheduler.step(increment=increment)
         skipped_iter = 0
     else:
@@ -1706,47 +1597,28 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
         # Average loss across microbatches.
         loss_reduced = {}
         for key in losses_reduced[0].keys():
-            val = [x[key].view(-1) for x in losses_reduced]
-            if val[0].numel() == 2:
+            numerator = 0
+            denominator = 0
+            for x in losses_reduced:
+                val = x[key]
                 # there is one dict per microbatch. in new reporting, we average
                 # over the total number of tokens across the global batch.
-                val = torch.vstack(val).sum(dim=0)
-                torch.distributed.all_reduce(
-                    val,
-                    group=mpu.get_data_parallel_group(with_context_parallel=True)
-                )
-                loss_reduced[key] = val[0] / val[1]
-            elif val[0].numel() == 1:
-                # legacy behavior, we average over the number of microbatches
-                val = torch.cat(val).mean()
-                loss_reduced[key] = val
-            else:
-                raise ValueError(f"Invalid value shape: {val[0].shape} for key {key}")
-        return (
-            loss_reduced,
-            skipped_iter,
-            should_checkpoint,
-            should_exit,
-            exit_code,
-            grad_norm,
-            num_zeros_in_grad,
-        )
+                if isinstance(val, tuple) or isinstance(val, list):
+                    numerator += val[0]
+                    denominator += val[1]
+                else:
+                    # legacy behavior. we average over the number of microbatches,
+                    # and so the denominator is 1.
+                    numerator += val
+                    denominator += 1
+            loss_reduced[key] = numerator / denominator
+        return loss_reduced, skipped_iter, should_checkpoint, should_exit, exit_code, grad_norm, num_zeros_in_grad
     return {}, skipped_iter, should_checkpoint, should_exit, exit_code, grad_norm, num_zeros_in_grad
 
 
-def training_log(
-    loss_dict,
-    total_loss_dict,
-    learning_rate,
-    decoupled_learning_rate,
-    iteration,
-    loss_scale,
-    report_memory_flag,
-    skipped_iter,
-    grad_norm,
-    params_norm,
-    num_zeros_in_grad,
-):
+def training_log(loss_dict, total_loss_dict, learning_rate, decoupled_learning_rate, iteration,
+                 loss_scale, report_memory_flag, skipped_iter,
+                 grad_norm, params_norm, num_zeros_in_grad):
     """Log training information such as losses, timing, ...."""
     args = get_args()
     timers = get_timers()
@@ -1760,25 +1632,28 @@ def training_log(
     nan_iters_key = 'nan iterations'
     # Advanced iterations.
     if not skipped_iter:
-        total_loss_dict[advanced_iters_key] = total_loss_dict.get(advanced_iters_key, 0) + 1
+        total_loss_dict[advanced_iters_key] = total_loss_dict.get(
+            advanced_iters_key, 0) + 1
     else:
         if advanced_iters_key not in total_loss_dict:
             total_loss_dict[advanced_iters_key] = 0
     # Skipped iterations.
-    total_loss_dict[skipped_iters_key] = total_loss_dict.get(skipped_iters_key, 0) + skipped_iter
+    total_loss_dict[skipped_iters_key] = total_loss_dict.get(
+        skipped_iters_key, 0) + skipped_iter
     # Update losses and set nan iterations
     got_nan = False
     for key in loss_dict:
         if not skipped_iter:
-            total_loss_dict[key] = (
-                total_loss_dict.get(key, torch.tensor([0.0], dtype=torch.float, device='cuda'))
-                + loss_dict[key]
-            )
+            total_loss_dict[key] = total_loss_dict.get(
+                key, torch.tensor([0.0], dtype=torch.float, device='cuda')) + loss_dict[key]
         else:
             value = loss_dict[key].float().sum().item()
-            is_nan = value == float('inf') or value == -float('inf') or value != value
+            is_nan = value == float('inf') or \
+                     value == -float('inf') or \
+                     value != value
             got_nan = got_nan or is_nan
-    total_loss_dict[nan_iters_key] = total_loss_dict.get(nan_iters_key, 0) + int(got_nan)
+    total_loss_dict[nan_iters_key] = total_loss_dict.get(
+        nan_iters_key, 0) + int(got_nan)
 
     # Logging.
     timers_to_log = [
@@ -1805,30 +1680,35 @@ def training_log(
         'optimizer-count-zeros',
         'optimizer-inner-step',
         'optimizer-copy-main-to-model-params',
-        'optimizer',
-    ]
+        'optimizer']
 
     # Calculate batch size.
-    batch_size = args.micro_batch_size * args.data_parallel_size * get_num_microbatches()
+    batch_size = args.micro_batch_size * args.data_parallel_size * \
+        get_num_microbatches()
 
     # Track app tag & app tag ID
     one_logger_utils.track_app_tag(batch_size, args.world_size, args.seq_length)
 
-    total_iterations = total_loss_dict[advanced_iters_key] + total_loss_dict[skipped_iters_key]
+    total_iterations = total_loss_dict[advanced_iters_key] + \
+                       total_loss_dict[skipped_iters_key]
 
     # learning rate will be None on ranks without trainable params, so we must gather across mp ranks
     learning_rate = reduce_max_stat_across_model_parallel_group(learning_rate)
     # Tensorboard values.
     # Timer requires all the ranks to call.
-    if args.log_timers_to_tensorboard and (iteration % args.tensorboard_log_interval == 0):
-        timers.write(timers_to_log, writer, iteration, normalizer=total_iterations)
+    if args.log_timers_to_tensorboard and \
+       (iteration % args.tensorboard_log_interval == 0):
+        timers.write(timers_to_log, writer, iteration,
+                     normalizer=total_iterations)
     if is_last_rank() and (iteration % args.tensorboard_log_interval == 0):
         if wandb_writer:
-            wandb_writer.log({'samples vs steps': args.consumed_train_samples}, iteration)
+            wandb_writer.log({'samples vs steps': args.consumed_train_samples},
+                             iteration)
             wandb_writer.log({'consumed-tokens': args.consumed_train_samples * args.seq_length / 1000. / 1000 / 1000}, iteration)
         if writer:
             writer.add_scalar('learning-rate', learning_rate, iteration)
-            writer.add_scalar('learning-rate vs samples', learning_rate, args.consumed_train_samples)
+            writer.add_scalar('learning-rate vs samples', learning_rate,
+                                args.consumed_train_samples)
         if wandb_writer:
             wandb_writer.log({'learning-rate': learning_rate}, iteration)
         if args.decoupled_lr is not None:
@@ -1841,69 +1721,87 @@ def training_log(
                 wandb_writer.log({'skipped-train-samples': args.skipped_train_samples}, iteration)
         if writer:
             writer.add_scalar('batch-size', batch_size, iteration)
-            writer.add_scalar('batch-size vs samples', batch_size, args.consumed_train_samples)
+            writer.add_scalar('batch-size vs samples', batch_size,
+                          args.consumed_train_samples)
         if wandb_writer:
             wandb_writer.log({'batch-size': batch_size}, iteration)
         for key in loss_dict:
             if writer:
-                writer.add_scalar(key, loss_dict[key], iteration)
-                writer.add_scalar(key + ' vs samples', loss_dict[key], args.consumed_train_samples)
+                writer.add_scalar(key , loss_dict[key], iteration)
+                writer.add_scalar(key + ' vs samples', loss_dict[key],
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({key: loss_dict[key]}, iteration)
         if args.log_loss_scale_to_tensorboard:
             if writer:
                 writer.add_scalar('loss-scale', loss_scale, iteration)
-                writer.add_scalar('loss-scale vs samples', loss_scale, args.consumed_train_samples)
+                writer.add_scalar('loss-scale vs samples', loss_scale,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'loss-scale': loss_scale}, iteration)
         if args.log_world_size_to_tensorboard:
             if writer:
                 writer.add_scalar('world-size', args.world_size, iteration)
-                writer.add_scalar('world-size vs samples', args.world_size, args.consumed_train_samples)
+                writer.add_scalar('world-size vs samples', args.world_size,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'world-size': args.world_size}, iteration)
         if grad_norm is not None:
             if writer:
                 writer.add_scalar('grad-norm', grad_norm, iteration)
-                writer.add_scalar('grad-norm vs samples', grad_norm, args.consumed_train_samples)
+                writer.add_scalar('grad-norm vs samples', grad_norm,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'grad-norm': grad_norm}, iteration)
         if num_zeros_in_grad is not None:
             if writer:
                 writer.add_scalar('num-zeros', num_zeros_in_grad, iteration)
-                writer.add_scalar(
-                    'num-zeros vs samples', num_zeros_in_grad, args.consumed_train_samples
-                )
+                writer.add_scalar('num-zeros vs samples', num_zeros_in_grad,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'num-zeros': num_zeros_in_grad}, iteration)
         if params_norm is not None:
             if writer:
                 writer.add_scalar('params-norm', params_norm, iteration)
-                writer.add_scalar('params-norm vs samples', params_norm, args.consumed_train_samples)
+                writer.add_scalar('params-norm vs samples', params_norm,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'params-norm': params_norm}, iteration)
         if args.log_memory_to_tensorboard:
             mem_stats = torch.cuda.memory_stats()
             if writer:
                 writer.add_scalar(
-                    "mem-reserved-bytes", mem_stats["reserved_bytes.all.current"], iteration
+                    "mem-reserved-bytes",
+                    mem_stats["reserved_bytes.all.current"],
+                    iteration,
+                )
+                writer.add_scalar(
+                    "mem-allocated-bytes",
+                    mem_stats["allocated_bytes.all.current"],
+                    iteration,
                 )
                 writer.add_scalar(
-                    "mem-allocated-bytes", mem_stats["allocated_bytes.all.current"], iteration
+                    "mem-max-allocated-bytes",
+                    mem_stats["allocated_bytes.all.peak"],
+                    iteration,
                 )
                 writer.add_scalar(
-                    "mem-max-allocated-bytes", mem_stats["allocated_bytes.all.peak"], iteration
+                    "mem-allocated-count",
+                    mem_stats["allocation.all.current"],
+                    iteration,
                 )
-                writer.add_scalar("mem-allocated-count", mem_stats["allocation.all.current"], iteration)
             if wandb_writer:
                 wandb_writer.log(
-                    {"mem-reserved-bytes": mem_stats["reserved_bytes.all.current"]}, iteration
+                    {"mem-reserved-bytes": mem_stats["reserved_bytes.all.current"]},
+                    iteration,
                 )
                 wandb_writer.log(
-                    {"mem-allocated-bytes": mem_stats["allocated_bytes.all.current"]}, iteration
+                    {"mem-allocated-bytes": mem_stats["allocated_bytes.all.current"]},
+                    iteration,
                 )
                 wandb_writer.log(
-                    {"mem-allocated-count": mem_stats["allocation.all.current"]}, iteration
+                    {"mem-allocated-count": mem_stats["allocation.all.current"]},
+                    iteration,
                 )
 
     if args.num_experts is not None:
@@ -1923,18 +1821,17 @@ def training_log(
             force_initialize=True,
             track_names=track_names,
             num_layers=args.num_layers,
-            moe_layer_freq=args.moe_layer_freq,
+            moe_layer_freq=args.moe_layer_freq
         )
     if args.mtp_num_layers is not None:
         mtp_loss_scale = 1 / get_num_microbatches()
         MTPLossLoggingHelper.track_mtp_metrics(
             mtp_loss_scale, iteration, writer, wandb_writer, total_loss_dict
-        )
+            )
     if iteration % args.log_interval == 0:
         if args.record_memory_history and is_last_rank():
             snapshot = torch.cuda.memory._snapshot()
             from pickle import dump
-
             with open(args.memory_snapshot_path, 'wb') as f:
                 dump(snapshot, f)
 
@@ -1942,24 +1839,27 @@ def training_log(
         elapsed_time_per_iteration = elapsed_time / total_iterations
 
         throughput = num_floating_point_operations(args, batch_size) / (
-            elapsed_time_per_iteration * 10**12 * args.world_size
-        )
+            elapsed_time_per_iteration * 10**12 * args.world_size)
 
         one_logger_utils.track_e2e_metrics(args.log_throughput, throughput)
 
         if args.log_timers_to_tensorboard:
             if writer:
-                writer.add_scalar('iteration-time', elapsed_time_per_iteration, iteration)
+                writer.add_scalar('iteration-time',
+                                  elapsed_time_per_iteration, iteration)
             if wandb_writer:
-                wandb_writer.log({'iteration-time': elapsed_time_per_iteration}, iteration)
+                wandb_writer.log({'iteration-time': elapsed_time_per_iteration},
+                                 iteration)
         log_string = f" [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]"
-        log_string += ' iteration {:8d}/{:8d} |'.format(iteration, args.train_iters)
-        log_string += ' consumed samples: {:12d} |'.format(args.consumed_train_samples)
+        log_string += ' iteration {:8d}/{:8d} |'.format(
+            iteration, args.train_iters)
+        log_string += ' consumed samples: {:12d} |'.format(
+            args.consumed_train_samples)
         if args.skipped_train_samples > 0:
-            log_string += ' skipped samples: {:12d} |'.format(args.skipped_train_samples)
+            log_string += ' skipped samples: {:12d} |'.format(
+                args.skipped_train_samples)
         log_string += ' elapsed time per iteration (ms): {:.1f} |'.format(
-            elapsed_time_per_iteration * 1000.0
-        )
+            elapsed_time_per_iteration * 1000.0)
         if args.log_throughput:
             log_string += f' throughput per GPU (TFLOP/s/GPU): {throughput:.1f} |'
             if args.log_timers_to_tensorboard:
@@ -1969,20 +1869,18 @@ def training_log(
                     wandb_writer.log({'throughput': throughput}, iteration)
         # Decoupled_learning_rate should be not None only on first and last pipeline stage.
         log_string += f' learning rate: {learning_rate:.6E} |'
-        if args.decoupled_lr is not None and (
-            mpu.is_pipeline_first_stage(ignore_virtual=True)
-            or mpu.is_pipeline_last_stage(ignore_virtual=True)
-        ):
+        if args.decoupled_lr is not None and (mpu.is_pipeline_first_stage(ignore_virtual=True) or
+                                              mpu.is_pipeline_last_stage(ignore_virtual=True)):
             assert decoupled_learning_rate is not None
             log_string += f' decoupled learning rate: {decoupled_learning_rate:.6E} |'
         else:
             assert decoupled_learning_rate is None
         log_string += f' global batch size: {batch_size:5d} |'
         for key in total_loss_dict:
-            if key not in [advanced_iters_key, skipped_iters_key, nan_iters_key]:
-                avg = total_loss_dict[key].item() / float(
-                    max(1, total_loss_dict[advanced_iters_key])
-                )
+            if key not in [advanced_iters_key, skipped_iters_key,
+                           nan_iters_key]:
+                avg = total_loss_dict[key].item() / \
+                      float(max(1, total_loss_dict[advanced_iters_key]))
                 if avg > 0.0:
                     log_string += ' {}: {:.6E} |'.format(key, avg)
                 total_loss_dict[key] = torch.tensor([0.0], dtype=torch.float, device='cuda')
@@ -1994,9 +1892,9 @@ def training_log(
         if params_norm is not None:
             log_string += f' params norm: {params_norm:.3f} |'
         log_string += ' number of skipped iterations: {:3d} |'.format(
-            total_loss_dict[skipped_iters_key]
-        )
-        log_string += ' number of nan iterations: {:3d} |'.format(total_loss_dict[nan_iters_key])
+            total_loss_dict[skipped_iters_key])
+        log_string += ' number of nan iterations: {:3d} |'.format(
+            total_loss_dict[nan_iters_key])
         total_loss_dict[advanced_iters_key] = 0
         total_loss_dict[skipped_iters_key] = 0
         total_loss_dict[nan_iters_key] = 0
@@ -2019,7 +1917,8 @@ def training_log(
     return report_memory_flag
 
 
-def compute_throughputs_and_append_to_progress_log(iteration, num_floating_point_operations_so_far):
+def compute_throughputs_and_append_to_progress_log(iteration,
+                                                   num_floating_point_operations_so_far):
     args = get_args()
     if args.save is None:
         return
@@ -2028,28 +1927,28 @@ def compute_throughputs_and_append_to_progress_log(iteration, num_floating_point
     # args.num_floating_point_operations_so_far keeps track of floating-point operations
     # completed at the start of job.
     global _TRAIN_START_TIME
-    job_throughput = (
-        num_floating_point_operations_so_far - args.num_floating_point_operations_so_far
-    ) / ((time.time() - _TRAIN_START_TIME) * 10**12 * args.world_size)
+    job_throughput = \
+        (num_floating_point_operations_so_far -
+         args.num_floating_point_operations_so_far) / (
+            (time.time() - _TRAIN_START_TIME) * 10**12 * args.world_size)
 
     # Compute cumulative throughput since jobs of this world size were launched.
     # `get_start_time_from_progress_log` returns start time and number of floating-point
     # operations of first job of this world size.
     start_time, start_num_floating_point_operations = get_start_time_from_progress_log()
     elapsed_time = (datetime.now() - start_time).total_seconds()
-    cumulative_throughput = (
-        num_floating_point_operations_so_far - start_num_floating_point_operations
-    ) / (elapsed_time * 10**12 * args.world_size)
+    cumulative_throughput = \
+        (num_floating_point_operations_so_far -
+         start_num_floating_point_operations) / (
+            elapsed_time * 10**12 * args.world_size)
 
     tokens_so_far = args.consumed_train_samples * args.seq_length
     saved_ckpt_prefix = 'Saving async checkpoint' if args.async_save else 'Saved checkpoint'
-    append_to_progress_log(
-        f"{saved_ckpt_prefix}\tIteration: {iteration}\t"
-        f"Job throughput: {job_throughput:.1f} TFLOP/s/GPU\t"
-        f"Cumulative throughput: {cumulative_throughput:.1f} TFLOP/s/GPU\t"
-        f"Floating-point operations: {num_floating_point_operations_so_far:.2e}\t"
-        f"Tokens (in billions): {tokens_so_far / 10**9:.2f}"
-    )
+    append_to_progress_log(f"{saved_ckpt_prefix}\tIteration: {iteration}\t"
+                           f"Job throughput: {job_throughput:.1f} TFLOP/s/GPU\t"
+                           f"Cumulative throughput: {cumulative_throughput:.1f} TFLOP/s/GPU\t"
+                           f"Floating-point operations: {num_floating_point_operations_so_far:.2e}\t"
+                           f"Tokens (in billions): {tokens_so_far / 10**9:.2f}")
 
 
 def enable_forward_pre_hook(model_chunks):
@@ -2064,16 +1963,9 @@ def disable_forward_pre_hook(model_chunks, param_sync=True):
         model_chunk.disable_forward_pre_hook(param_sync=param_sync)
 
 
-def save_checkpoint_and_time(
-    iteration,
-    model,
-    optimizer,
-    opt_param_scheduler,
-    num_floating_point_operations_so_far,
-    checkpointing_context,
-    non_persistent_ckpt=False,
-    train_data_iterator=None,
-):
+def save_checkpoint_and_time(iteration, model, optimizer, opt_param_scheduler,
+                             num_floating_point_operations_so_far, checkpointing_context,
+                             non_persistent_ckpt=False, train_data_iterator=None):
     args = get_args()
     timers = get_timers()
 
@@ -2087,17 +1979,10 @@ def save_checkpoint_and_time(
     one_logger_utils.track_e2e_metrics()
     if should_disable_forward_pre_hook(args):
         disable_forward_pre_hook(model)
-    save_checkpoint(
-        iteration,
-        model,
-        optimizer,
-        opt_param_scheduler,
-        num_floating_point_operations_so_far,
-        checkpointing_context,
-        non_persistent_ckpt=non_persistent_ckpt,
-        train_data_iterator=train_data_iterator,
-        preprocess_common_state_dict_fn=preprocess_common_state_dict,
-    )
+    save_checkpoint(iteration, model, optimizer, opt_param_scheduler,
+                    num_floating_point_operations_so_far, checkpointing_context,
+                    non_persistent_ckpt=non_persistent_ckpt, train_data_iterator=train_data_iterator,
+                    preprocess_common_state_dict_fn=preprocess_common_state_dict)
     if should_disable_forward_pre_hook(args):
         enable_forward_pre_hook(model)
     timers(timer_key).stop(barrier=True)
@@ -2109,22 +1994,15 @@ def save_checkpoint_and_time(
     one_logger_utils.on_save_checkpoint_end(save_checkpoint_duration, iteration, args.async_save)
 
     if args.log_progress and not non_persistent_ckpt:
-        compute_throughputs_and_append_to_progress_log(
-            iteration, num_floating_point_operations_so_far
-        )
+        compute_throughputs_and_append_to_progress_log(iteration,
+                                                       num_floating_point_operations_so_far)
 
     # Recover timing
     timers('interval-time', log_level=0).start(barrier=True)
 
 
-def post_training_step_callbacks(
-    model,
-    optimizer,
-    opt_param_scheduler,
-    iteration,
-    prof,
-    num_floating_point_operations_since_last_log_event,
-):
+def post_training_step_callbacks(model, optimizer, opt_param_scheduler, iteration, prof,
+                                 num_floating_point_operations_since_last_log_event):
     """Run all post-training-step functions (e.g., FT heartbeats, GC)."""
     args = get_args()
 
@@ -2138,31 +2016,27 @@ def post_training_step_callbacks(
         num_floating_point_operations_since_last_log_event = 0.0
 
     # Check weight hash across DP replicas.
-    if (
-        args.check_weight_hash_across_dp_replicas_interval is not None
-        and iteration % args.check_weight_hash_across_dp_replicas_interval == 0
-    ):
+    if args.check_weight_hash_across_dp_replicas_interval is not None and \
+            iteration % args.check_weight_hash_across_dp_replicas_interval == 0:
         if should_disable_forward_pre_hook(args):
             disable_forward_pre_hook(model)
-        assert check_param_hashes_across_dp_replicas(
-            model, cross_check=True
-        ), "Parameter hashes not matching across DP replicas"
+        assert check_param_hashes_across_dp_replicas(model, cross_check=True), \
+            "Parameter hashes not matching across DP replicas"
         torch.distributed.barrier()
         print_rank_0(f">>> Weight hashes match after {iteration} iterations...")
         if should_disable_forward_pre_hook(args):
             enable_forward_pre_hook(model)
 
     # Autoresume.
-    if args.adlr_autoresume and (iteration % args.adlr_autoresume_interval == 0):
-        check_adlr_autoresume_termination(iteration, model, optimizer, opt_param_scheduler)
+    if args.adlr_autoresume and \
+        (iteration % args.adlr_autoresume_interval == 0):
+        check_adlr_autoresume_termination(iteration, model, optimizer,
+                                          opt_param_scheduler)
 
     # Profiling.
-    torch.cuda.nvtx.range_pop() # for iteratrion
-    if (
-        args.profile
-        and iteration == args.profile_step_end
-        and torch.distributed.get_rank() in args.profile_ranks
-    ):
+    if args.profile and \
+        iteration == args.profile_step_end and \
+        torch.distributed.get_rank() in args.profile_ranks:
         if args.use_pytorch_profiler:
             assert prof is not None
             prof.stop()
@@ -2175,15 +2049,9 @@ def post_training_step_callbacks(
             gc.collect()
 
 
-def checkpoint_and_decide_exit(
-    model,
-    optimizer,
-    opt_param_scheduler,
-    iteration,
-    num_floating_point_operations_so_far,
-    checkpointing_context,
-    train_data_iterator,
-):
+def checkpoint_and_decide_exit(model, optimizer, opt_param_scheduler, iteration,
+                               num_floating_point_operations_so_far, checkpointing_context,
+                               train_data_iterator):
     """Save checkpoint and decide whether to exit based on arguments (e.g., if
     --exit-duration-in-mins is set). Actual exit happens in main training loop
     based on the return value of this function."""
@@ -2196,68 +2064,47 @@ def checkpoint_and_decide_exit(
         signal_handler = get_signal_handler()
         if any(signal_handler.signals_received()):
             if args.save:
-                save_checkpoint_and_time(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                )
+                save_checkpoint_and_time(iteration, model, optimizer,
+                                         opt_param_scheduler,
+                                         num_floating_point_operations_so_far,
+                                         checkpointing_context, train_data_iterator=train_data_iterator)
             print_datetime('exiting program after receiving SIGTERM.')
 
             return True
 
     # Regular save (persistent and non-persistent).
-    if args.save and args.save_interval and iteration % args.save_interval == 0:
-        save_checkpoint_and_time(
-            iteration,
-            model,
-            optimizer,
-            opt_param_scheduler,
-            num_floating_point_operations_so_far,
-            checkpointing_context,
-            train_data_iterator=train_data_iterator,
-        )
+    if args.save and args.save_interval and \
+        iteration % args.save_interval == 0:
+        save_checkpoint_and_time(iteration, model, optimizer,
+                                 opt_param_scheduler,
+                                 num_floating_point_operations_so_far,
+                                 checkpointing_context, train_data_iterator=train_data_iterator)
         saved_checkpoint = True
 
-    elif (
-        args.save
-        and args.non_persistent_save_interval
-        and iteration % args.non_persistent_save_interval == 0
-    ):
-        save_checkpoint_and_time(
-            iteration,
-            model,
-            optimizer,
-            opt_param_scheduler,
-            num_floating_point_operations_so_far,
-            checkpointing_context,
-            non_persistent_ckpt=True,
-            train_data_iterator=train_data_iterator,
-        )
+    elif args.save and args.non_persistent_save_interval and \
+        iteration % args.non_persistent_save_interval == 0:
+        save_checkpoint_and_time(iteration, model, optimizer,
+                                 opt_param_scheduler,
+                                 num_floating_point_operations_so_far,
+                                 checkpointing_context,
+                                 non_persistent_ckpt=True, train_data_iterator=train_data_iterator)
         saved_checkpoint = True
 
     # Exit based on duration.
     if args.exit_duration_in_mins:
         train_time = (time.time() - _TRAIN_START_TIME) / 60.0
         done_cuda = torch.tensor(
-            [train_time > args.exit_duration_in_mins], dtype=torch.int, device='cuda'
-        )
-        torch.distributed.all_reduce(done_cuda, op=torch.distributed.ReduceOp.MAX)
+            [train_time > args.exit_duration_in_mins],
+            dtype=torch.int, device='cuda')
+        torch.distributed.all_reduce(
+            done_cuda, op=torch.distributed.ReduceOp.MAX)
         done = done_cuda.item()
         if done:
             if args.save and not saved_checkpoint:
-                save_checkpoint_and_time(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                )
+                save_checkpoint_and_time(iteration, model, optimizer,
+                                         opt_param_scheduler,
+                                         num_floating_point_operations_so_far,
+                                         checkpointing_context, train_data_iterator=train_data_iterator)
             print_datetime(f'exiting program after {train_time} minutes')
 
             return True
@@ -2265,15 +2112,10 @@ def checkpoint_and_decide_exit(
     # Exit based on iterations.
     if args.exit_interval and iteration % args.exit_interval == 0:
         if args.save and not saved_checkpoint:
-            save_checkpoint_and_time(
-                iteration,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                num_floating_point_operations_so_far,
-                checkpointing_context,
-                train_data_iterator=train_data_iterator,
-            )
+            save_checkpoint_and_time(iteration, model, optimizer,
+                                     opt_param_scheduler,
+                                     num_floating_point_operations_so_far,
+                                     checkpointing_context, train_data_iterator=train_data_iterator)
         torch.distributed.barrier()
         print_datetime(f'exiting program at iteration {iteration}')
 
@@ -2282,19 +2124,10 @@ def checkpoint_and_decide_exit(
     return False
 
 
-def train(
-    forward_step_func,
-    model,
-    optimizer,
-    opt_param_scheduler,
-    train_data_iterator,
-    valid_data_iterator,
-    process_non_loss_data_func,
-    config,
-    checkpointing_context,
-    non_loss_data_func,
-    extra_valid_dataset_provider=None,
-):
+def train(forward_step_func, model, optimizer, opt_param_scheduler,
+          train_data_iterator, valid_data_iterator,
+          process_non_loss_data_func, config, checkpointing_context, non_loss_data_func,
+          extra_valid_dataset_provider=None):
     """Training function: run train_step desired number of times, run validation, checkpoint."""
     args = get_args()
     timers = get_timers()
@@ -2304,10 +2137,7 @@ def train(
         try:
             from workload_inspector.utils.webserver import run_server
             import threading
-
-            threading.Thread(
-                target=run_server, daemon=True, args=(torch.distributed.get_rank(),)
-            ).start()
+            threading.Thread(target=run_server, daemon=True, args=(torch.distributed.get_rank(), )).start()
         except ModuleNotFoundError:
             print_rank_0("workload inspector module not found.")
 
@@ -2330,17 +2160,11 @@ def train(
         rerun_state_machine.current_iteration = iteration
 
     # Track E2E metrics at the start of training.
-    one_logger_utils.on_train_start(
-        iteration=iteration,
-        consumed_train_samples=args.consumed_train_samples,
-        train_samples=args.train_samples,
-        seq_length=args.seq_length,
-        train_iters=args.train_iters,
-        save=args.save,
-        async_save=args.async_save,
-        log_throughput=args.log_throughput,
-        num_floating_point_operations_so_far=args.num_floating_point_operations_so_far,
-    )
+    one_logger_utils.on_train_start(iteration=iteration, consumed_train_samples=args.consumed_train_samples,
+                                    train_samples=args.train_samples, seq_length=args.seq_length,
+                                    train_iters=args.train_iters, save=args.save, async_save=args.async_save,
+                                    log_throughput=args.log_throughput,
+                                    num_floating_point_operations_so_far=args.num_floating_point_operations_so_far)
 
     num_floating_point_operations_so_far = args.num_floating_point_operations_so_far
 
@@ -2348,10 +2172,9 @@ def train(
     config.grad_scale_func = optimizer.scale_loss
     config.timers = timers
     if isinstance(model[0], (custom_FSDP, DDP)) and args.overlap_grad_reduce:
-        assert config.no_sync_func is None, (
-            'When overlap_grad_reduce is True, config.no_sync_func must be None; '
-            'a custom no_sync_func is not supported when overlapping grad-reduce'
-        )
+        assert config.no_sync_func is None, \
+            ('When overlap_grad_reduce is True, config.no_sync_func must be None; '
+             'a custom no_sync_func is not supported when overlapping grad-reduce')
         config.no_sync_func = [model_chunk.no_sync for model_chunk in model]
         if len(model) == 1:
             config.no_sync_func = config.no_sync_func[0]
@@ -2375,9 +2198,8 @@ def train(
     if args.manual_gc:
         # Disable the default garbage collector and perform the collection manually.
         # This is to align the timing of garbage collection across ranks.
-        assert (
-            args.manual_gc_interval >= 0
-        ), 'Manual garbage collection interval should be larger than or equal to 0'
+        assert args.manual_gc_interval >= 0, \
+            'Manual garbage collection interval should be larger than or equal to 0'
         gc.disable()
         gc.collect()
 
@@ -2387,13 +2209,10 @@ def train(
         world = torch.distributed.get_world_size()
         rank = torch.distributed.get_rank()
         mmcnt = args.straggler_minmax_count
-        stimer.configure(
-            world,
-            rank,
-            mmcnt=mmcnt,
-            enabled=not args.disable_straggler_on_startup,
-            port=args.straggler_ctrlr_port,
-        )
+        stimer.configure(world, rank,
+                mmcnt = mmcnt,
+                enabled = not args.disable_straggler_on_startup,
+                port = args.straggler_ctrlr_port)
     num_floating_point_operations_since_last_log_event = 0.0
 
     num_microbatches = get_num_microbatches()
@@ -2410,10 +2229,10 @@ def train(
     extra_eval_iterations = 0
 
     def get_e2e_base_metrics():
-        """Get base metrics values for one-logger to calculate E2E tracking metrics."""
-        num_floating_point_operations_since_current_train_start = (
+        """Get base metrics values for one-logger to calculate E2E tracking metrics.
+        """
+        num_floating_point_operations_since_current_train_start = \
             num_floating_point_operations_so_far - args.num_floating_point_operations_so_far
-        )
         return {
             'iteration': iteration,
             'train_duration': timers('interval-time').active_time(),
@@ -2427,29 +2246,22 @@ def train(
             'extra_eval_duration': extra_eval_duration,
             'extra_eval_iterations': extra_eval_iterations,
         }
-
     # Cache into one-logger for callback.
     if one_logger:
         with one_logger.get_context_manager():
             one_logger.store_set('get_e2e_base_metrics', get_e2e_base_metrics)
 
     prof = None
-    if (
-        args.profile
-        and torch.distributed.get_rank() in args.profile_ranks
-        and args.use_pytorch_profiler
-    ):
+    if args.profile and torch.distributed.get_rank() in args.profile_ranks and args.use_pytorch_profiler:
         prof = torch.profiler.profile(
-            schedule=torch.profiler.schedule(
-                wait=max(args.profile_step_start - 1, 0),
-                warmup=1 if args.profile_step_start > 0 else 0,
-                active=args.profile_step_end - args.profile_step_start,
-                repeat=1,
-            ),
-            on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),
-            record_shapes=True,
-            with_stack=True,
-        )
+        schedule=torch.profiler.schedule(
+            wait=max(args.profile_step_start-1, 0),
+            warmup=1 if args.profile_step_start > 0 else 0,
+            active=args.profile_step_end-args.profile_step_start,
+            repeat=1),
+        on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),
+        record_shapes=True,
+        with_stack=True)
         prof.start()
 
     start_iteration = iteration
@@ -2465,9 +2277,8 @@ def train(
         pre_hook_enabled = False
     # Also, check weight hash across DP replicas to be very pedantic.
     if args.check_weight_hash_across_dp_replicas_interval is not None:
-        assert check_param_hashes_across_dp_replicas(
-            model, cross_check=True
-        ), "Parameter hashes not matching across DP replicas"
+        assert check_param_hashes_across_dp_replicas(model, cross_check=True), \
+            "Parameter hashes not matching across DP replicas"
         torch.distributed.barrier()
         print_rank_0(f">>> Weight hashes match after {iteration} iterations...")
 
@@ -2479,7 +2290,7 @@ def train(
             elif iteration == args.profile_step_start:
                 torch.cuda.cudart().cudaProfilerStart()
                 torch.autograd.profiler.emit_nvtx(record_shapes=True).__enter__()
-        torch.cuda.nvtx.range_push(f"iteration num {iteration}") # NOTE(lizhiyu): add iteration num tag for profile
+
         ft_integration.on_checkpointing_start()
         maybe_finalize_async_save(blocking=False)
         ft_integration.on_checkpointing_end(is_async_finalization=True)
@@ -2491,20 +2302,14 @@ def train(
         update_num_microbatches(args.consumed_train_samples, consistency_check=False, verbose=True)
         if get_num_microbatches() != num_microbatches and iteration != 0 \
             and args.save_when_num_microbatches_change:
-            assert get_num_microbatches() > num_microbatches, (
-                f"Number of microbatches should be increasing due to batch size rampup; "
-                f"instead going from {num_microbatches} to {get_num_microbatches()}"
-            )
+            assert get_num_microbatches() > num_microbatches, \
+                (f"Number of microbatches should be increasing due to batch size rampup; "
+                 f"instead going from {num_microbatches} to {get_num_microbatches()}")
             if args.save is not None:
-                save_checkpoint_and_time(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                )
+                save_checkpoint_and_time(iteration, model, optimizer,
+                                         opt_param_scheduler,
+                                         num_floating_point_operations_so_far,
+                                         checkpointing_context, train_data_iterator=train_data_iterator)
         num_microbatches = get_num_microbatches()
         update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
 
@@ -2513,9 +2318,9 @@ def train(
             # Dummy train_step to fast forward train_data_iterator.
             dummy_train_step(train_data_iterator)
             iteration += 1
-            batch_size = (
-                mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()
-            )
+            batch_size = mpu.get_data_parallel_world_size() * \
+                         args.micro_batch_size * \
+                         get_num_microbatches()
             args.consumed_train_samples += batch_size
             args.skipped_train_samples += batch_size
             continue
@@ -2555,28 +2360,19 @@ def train(
         ########## FlagScale end ##########
 
         ft_integration.on_training_step_start()
-        (
-            loss_dict,
-            skipped_iter,
-            should_checkpoint,
-            should_exit,
-            exit_code,
-            grad_norm,
-            num_zeros_in_grad,
-        ) = train_step(
-            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config
-        )
+        loss_dict, skipped_iter, should_checkpoint, should_exit, exit_code, grad_norm, num_zeros_in_grad = \
+            train_step(forward_step_func,
+                       train_data_iterator,
+                       model,
+                       optimizer,
+                       opt_param_scheduler,
+                       config)
         ft_integration.on_training_step_end()
         if should_checkpoint:
-            save_checkpoint_and_time(
-                iteration,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                num_floating_point_operations_so_far,
-                checkpointing_context,
-                train_data_iterator=train_data_iterator,
-            )
+            save_checkpoint_and_time(iteration, model, optimizer,
+                                     opt_param_scheduler,
+                                     num_floating_point_operations_so_far,
+                                     checkpointing_context, train_data_iterator=train_data_iterator)
         if should_exit:
             break
 
@@ -2599,13 +2395,12 @@ def train(
                     pre_hook_enabled = True
 
         iteration += 1
-        batch_size = (
-            mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()
-        )
+        batch_size = mpu.get_data_parallel_world_size() * \
+                     args.micro_batch_size * \
+                     get_num_microbatches()
         args.consumed_train_samples += batch_size
-        num_skipped_samples_in_batch = (
-            get_current_global_batch_size() - get_current_running_global_batch_size()
-        )
+        num_skipped_samples_in_batch = (get_current_global_batch_size() -
+                                        get_current_running_global_batch_size())
         if args.decrease_batch_size_if_needed:
             assert num_skipped_samples_in_batch >= 0
         else:
@@ -2631,22 +2426,16 @@ def train(
                 decoupled_learning_rate = param_group['lr']
             else:
                 learning_rate = param_group['lr']
-        report_memory_flag = training_log(
-            loss_dict,
-            total_loss_dict,
-            learning_rate,
-            decoupled_learning_rate,
-            iteration,
-            loss_scale,
-            report_memory_flag,
-            skipped_iter,
-            grad_norm,
-            params_norm,
-            num_zeros_in_grad,
-        )
+        report_memory_flag = training_log(loss_dict, total_loss_dict,
+                                          learning_rate,
+                                          decoupled_learning_rate,
+                                          iteration, loss_scale,
+                                          report_memory_flag, skipped_iter,
+                                          grad_norm, params_norm, num_zeros_in_grad)
 
         # Evaluation.
-        if args.eval_interval and iteration % args.eval_interval == 0 and args.do_valid:
+        if args.eval_interval and iteration % args.eval_interval == 0 and \
+            args.do_valid:
             timers('interval-time').stop()
             if should_disable_forward_pre_hook(args):
                 disable_forward_pre_hook(model)
@@ -2656,18 +2445,11 @@ def train(
                 gc.collect()
             prefix = f'iteration {iteration}'
             timers('eval-time', log_level=0).start(barrier=True)
-            evaluate_and_print_results(
-                prefix,
-                forward_step_func,
-                valid_data_iterator,
-                model,
-                iteration,
-                process_non_loss_data_func,
-                config,
-                verbose=False,
-                write_to_tensorboard=True,
-                non_loss_data_func=non_loss_data_func,
-            )
+            evaluate_and_print_results(prefix, forward_step_func,
+                                       valid_data_iterator, model,
+                                       iteration, process_non_loss_data_func,
+                                       config, verbose=False, write_to_tensorboard=True,
+                                       non_loss_data_func=non_loss_data_func)
             eval_duration += timers('eval-time').elapsed()
             eval_iterations += args.eval_iters
             timers('eval-time').stop()
@@ -2691,19 +2473,18 @@ def train(
                 extra_valid_data_iterator = []
                 for i in range(len(model)):
                     mpu.set_virtual_pipeline_model_parallel_rank(i)
-                    extra_iterators = build_extra_valid_data_iterators(extra_valid_dataset_provider)
+                    extra_iterators = build_extra_valid_data_iterators(
+                        extra_valid_dataset_provider)
                     extra_valid_data_iterator.append(extra_iterators)
             else:
-                extra_valid_data_iterator = (
-                    build_extra_valid_data_iterators(extra_valid_dataset_provider)
-                )
+                extra_valid_data_iterator = build_extra_valid_data_iterators(
+                    extra_valid_dataset_provider)
             timers('interval-time').stop()
             # do_extra_valid flag is used to indicate that we are doing extra validation
             # and is set in the build_extra_valid_data_iterators function
             if getattr(args, "do_extra_valid", False):
-                if should_disable_forward_pre_hook(args):
+                if args.use_distributed_optimizer and args.overlap_param_gather:
                     disable_forward_pre_hook(model)
-                    pre_hook_enabled = False
                 if args.manual_gc and args.manual_gc_eval:
                     # Collect all objects.
                     gc.collect()
@@ -2711,19 +2492,11 @@ def train(
                 for extra_valid_index, extra_valid_data_itr in enumerate(extra_valid_data_iterator):
                     timers('extra-eval-time', log_level=0).start(barrier=True)
                     extra_eval_iters = args.extra_eval_iters_list[extra_valid_index]
-                    extra_evaluate_and_print_results(
-                        extra_valid_index,
-                        prefix,
-                        forward_step_func,
-                        extra_valid_data_itr,
-                        model,
-                        iteration,
-                        process_non_loss_data_func,
-                        config,
-                        verbose=False,
-                        write_to_tensorboard=True,
-                        non_loss_data_func=non_loss_data_func
-                    )
+                    extra_evaluate_and_print_results(extra_valid_index, prefix, forward_step_func,
+                                                     extra_valid_data_itr, model,
+                                                     iteration, process_non_loss_data_func,
+                                                     config, verbose=False, write_to_tensorboard=True,
+                                                     non_loss_data_func=non_loss_data_func)
                     extra_eval_duration += timers('extra-eval-time').elapsed()
                     extra_eval_iterations += extra_eval_iters
                     timers('extra-eval-time').stop()
@@ -2732,33 +2505,25 @@ def train(
                 if args.manual_gc and args.manual_gc_eval:
                     # Collect only the objects created and used in evaluation.
                     gc.collect(generation=0)
-                if should_disable_forward_pre_hook(args):
+                if args.use_distributed_optimizer and args.overlap_param_gather:
                     enable_forward_pre_hook(model)
                     pre_hook_enabled = True
                 timers('interval-time', log_level=0).start(barrier=True)
+
+                if args.enable_ft_package and ft_integration.get_rank_monitor_client() is not None:
+                    ft_integration.get_rank_monitor_client(
+                        ft_integration.StateMachineActions.EVAL_HEARTBEAT).send_heartbeat()
         # =======================================================================================
 
         # Miscellaneous post-training-step functions (e.g., FT heartbeats, GC).
         # Some of these only happen at specific iterations.
-        post_training_step_callbacks(
-            model,
-            optimizer,
-            opt_param_scheduler,
-            iteration,
-            prof,
-            num_floating_point_operations_since_last_log_event,
-        )
+        post_training_step_callbacks(model, optimizer, opt_param_scheduler, iteration, prof,
+                                     num_floating_point_operations_since_last_log_event)
 
         # Checkpoint and decide whether to exit.
-        should_exit = checkpoint_and_decide_exit(
-            model,
-            optimizer,
-            opt_param_scheduler,
-            iteration,
-            num_floating_point_operations_so_far,
-            checkpointing_context,
-            train_data_iterator,
-        )
+        should_exit = checkpoint_and_decide_exit(model, optimizer, opt_param_scheduler, iteration,
+                                                 num_floating_point_operations_so_far,
+                                                 checkpointing_context, train_data_iterator)
         if should_exit:
             break
 
@@ -2793,16 +2558,14 @@ def train(
     return iteration, num_floating_point_operations_so_far
 
 
-def evaluate(
-    forward_step_func,
-    data_iterator,
-    model,
-    process_non_loss_data_func,
-    config,
-    verbose=False,
-    non_loss_data_func=None,
-    extra_valid_index=None,
-):
+def evaluate(forward_step_func,
+             data_iterator,
+             model,
+             process_non_loss_data_func,
+             config,
+             verbose=False,
+             non_loss_data_func=None,
+             extra_valid_index=None):
     """Evaluation."""
     args = get_args()
     timers = get_timers()
@@ -2811,7 +2574,6 @@ def evaluate(
 
     if args.vision_pretraining and args.vision_pretraining_type == "dino":
         from megatron.legacy.model.vision.knn_monitor import compute_feature_bank
-
         compute_feature_bank(model)
 
     # Turn on evaluation mode which disables dropout.
@@ -2827,7 +2589,8 @@ def evaluate(
 
     # make validation batch size independent from training batch size
     eval_batch_size = args.global_batch_size
-    eval_num_microbatches = eval_batch_size // (args.micro_batch_size * args.data_parallel_size)
+    eval_num_microbatches = eval_batch_size // \
+        (args.micro_batch_size * args.data_parallel_size)
 
     if extra_valid_index is not None:
         assert getattr(args, "extra_eval_iters_list") is not None, \
@@ -2857,8 +2620,7 @@ def evaluate(
                 seq_length=args.seq_length,
                 micro_batch_size=args.micro_batch_size,
                 decoder_seq_length=args.decoder_seq_length,
-                forward_only=True,
-            )
+                forward_only=True)
             ft_integration.on_eval_step_end()
             config.timers = get_timers()
 
@@ -2868,34 +2630,27 @@ def evaluate(
 
             if mpu.is_pipeline_last_stage(ignore_virtual=True):
                 # Reduce across processes.
-                for key in loss_dicts[0].keys():
-                    if key not in total_loss_dict:
-                        total_loss_dict[key] = torch.tensor(
-                            [0.0, 0.0], dtype=torch.float
-                        ).cuda()
-                    val = [x[key].view(-1) for x in loss_dicts]
-                    if val[0].numel() == 2:
-                        val = torch.vstack(val).sum(dim=0)
-                        torch.distributed.all_reduce(
-                            val,
-                            group=mpu.get_data_parallel_group(with_context_parallel=True)
-                        )
-                        total_loss_dict[key] += val
-                    elif val[0].numel() == 1:
-                        val = torch.cat(val).sum()
-                        total_loss_dict[key][0] += val
-                        total_loss_dict[key][1] += len(loss_dicts)
-                    else:
-                        raise ValueError(f"Invalid value shape: {val[0].shape} for key {key}")
+                for loss_dict in loss_dicts:
+                    for key in loss_dict:
+                        if key not in total_loss_dict:
+                            total_loss_dict[key] = torch.tensor([0.0, 0.0], dtype=torch.float).cuda()
+                        val = loss_dict[key]
+                        if isinstance(val, tuple) or isinstance(val, list):
+                            total_loss_dict[key][0] += val[0]
+                            total_loss_dict[key][1] += val[1]
+                        else:
+                            total_loss_dict[key][0] += val
+                            total_loss_dict[key][1] += 1
 
             args.consumed_valid_samples += eval_batch_size
 
             if args.exit_duration_in_mins:
                 train_time = (time.time() - _TRAIN_START_TIME) / 60.0
                 done_cuda = torch.tensor(
-                    [train_time > args.exit_duration_in_mins], dtype=torch.int, device='cuda'
-                )
-                torch.distributed.all_reduce(done_cuda, op=torch.distributed.ReduceOp.MAX)
+                    [train_time > args.exit_duration_in_mins],
+                    dtype=torch.int, device='cuda')
+                torch.distributed.all_reduce(
+                    done_cuda, op=torch.distributed.ReduceOp.MAX)
                 done = done_cuda.item()
                 if done:
                     rerun_state_machine.set_mode(rerun_mode)
@@ -2915,8 +2670,7 @@ def evaluate(
                 micro_batch_size=args.micro_batch_size,
                 decoder_seq_length=args.decoder_seq_length,
                 forward_only=True,
-                collect_non_loss_data=True,
-            )
+                collect_non_loss_data=True)
 
     # Move model back to the train mode.
     for model_module in model:
@@ -2935,19 +2689,10 @@ def evaluate(
 
     return total_loss_dict, collected_non_loss_data, False
 
-
-def evaluate_and_print_results(
-    prefix,
-    forward_step_func,
-    data_iterator,
-    model,
-    iteration,
-    process_non_loss_data_func,
-    config,
-    verbose=False,
-    write_to_tensorboard=True,
-    non_loss_data_func=None,
-):
+def evaluate_and_print_results(prefix, forward_step_func,
+                               data_iterator, model,
+                               iteration, process_non_loss_data_func, config,
+                               verbose=False, write_to_tensorboard=True, non_loss_data_func=None):
     """Helper function to evaluate and dump results on screen."""
     args = get_args()
     if write_to_tensorboard:
@@ -2958,14 +2703,8 @@ def evaluate_and_print_results(
     wandb_writer = get_wandb_writer()
 
     total_loss_dict, collected_non_loss_data, timelimit = evaluate(
-        forward_step_func,
-        data_iterator,
-        model,
-        process_non_loss_data_func,
-        config,
-        verbose,
-        non_loss_data_func,
-    )
+        forward_step_func, data_iterator, model,
+        process_non_loss_data_func, config, verbose, non_loss_data_func)
     # Timelimit hit during evaluation
     if timelimit:
         return
@@ -2975,21 +2714,21 @@ def evaluate_and_print_results(
         ppl = math.exp(min(20, total_loss_dict[key].item()))
         string += '{} PPL: {:.6E} | '.format(key, ppl)
         if writer:
-            writer.add_scalar('{} validation'.format(key), total_loss_dict[key].item(), iteration)
-            writer.add_scalar(
-                '{} validation vs samples'.format(key),
-                total_loss_dict[key].item(),
-                args.consumed_train_samples,
-            )
+            writer.add_scalar('{} validation'.format(key),
+                              total_loss_dict[key].item(),
+                              iteration)
+            writer.add_scalar('{} validation vs samples'.format(key),
+                              total_loss_dict[key].item(),
+                              args.consumed_train_samples)
             if args.log_validation_ppl_to_tensorboard:
-                writer.add_scalar('{} validation ppl'.format(key), ppl, iteration)
-                writer.add_scalar(
-                    '{} validation ppl vs samples'.format(key), ppl, args.consumed_train_samples
-                )
+                writer.add_scalar('{} validation ppl'.format(key), ppl,
+                                  iteration)
+                writer.add_scalar('{} validation ppl vs samples'.format(key),
+                                  ppl, args.consumed_train_samples)
             if wandb_writer and is_last_rank():
-                wandb_writer.log(
-                    {'{} validation'.format(key): total_loss_dict[key].item()}, iteration
-                )
+                wandb_writer.log({
+                    '{} validation'.format(key): total_loss_dict[key].item()},
+                    iteration)
                 wandb_writer.log({
                     '{} validation vs samples'.format(key): args.consumed_train_samples},
                     iteration)
@@ -3019,10 +2758,15 @@ def get_train_valid_test_num_samples():
         train_samples = args.train_samples
     else:
         train_samples = args.train_iters * args.global_batch_size
-    eval_iters = (args.train_iters // args.eval_interval + 1) * args.eval_iters
+    eval_iters = (args.train_iters // args.eval_interval + 1) * \
+                 args.eval_iters
     test_iters = args.eval_iters
 
-    return (train_samples, eval_iters * args.global_batch_size, test_iters * args.global_batch_size)
+    return (
+        train_samples,
+        eval_iters * args.global_batch_size,
+        test_iters * args.global_batch_size,
+    )
 
 
 def build_train_valid_test_datasets(build_train_valid_test_datasets_provider):
@@ -3035,7 +2779,8 @@ def build_train_valid_test_datasets(build_train_valid_test_datasets_provider):
     return build_train_valid_test_datasets_provider(train_valid_test_num_samples)
 
 
-def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider):
+def build_train_valid_test_data_loaders(
+        build_train_valid_test_datasets_provider):
     """Build pretraining data loaders."""
 
     args = get_args()
@@ -3046,15 +2791,13 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
 
     # Backward compatibility, assume fixed batch size.
     if args.iteration > 0 and args.consumed_train_samples == 0:
-        assert (
-            args.train_samples is None
-        ), 'Only backward compatiblity support for iteration-based training'
+        assert args.train_samples is None, \
+            'Only backward compatiblity support for iteration-based training'
         args.consumed_train_samples = args.iteration * args.global_batch_size
     if args.iteration > 0 and args.consumed_valid_samples == 0:
         if args.train_samples is None:
-            args.consumed_valid_samples = (
-                (args.iteration // args.eval_interval) * args.eval_iters * args.global_batch_size
-            )
+            args.consumed_valid_samples = (args.iteration // args.eval_interval) * \
+                args.eval_iters * args.global_batch_size
 
     # Rely on distributed-aware core datasets, temporary
     is_distributed = getattr(build_train_valid_test_datasets_provider, "is_distributed", False)
@@ -3064,14 +2807,15 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
 
         # Build datasets.
         train_ds, valid_ds, test_ds = build_train_valid_test_datasets(
-            build_train_valid_test_datasets_provider
-        )
+            build_train_valid_test_datasets_provider)
         # Build dataloders.
-        train_dataloader = build_pretraining_data_loader(train_ds, args.consumed_train_samples)
+        train_dataloader = build_pretraining_data_loader(
+            train_ds, args.consumed_train_samples)
         if args.skip_train:
             valid_dataloader = build_pretraining_data_loader(valid_ds, 0)
         else:
-            valid_dataloader = build_pretraining_data_loader(valid_ds, args.consumed_valid_samples)
+            valid_dataloader = build_pretraining_data_loader(
+                valid_ds, args.consumed_valid_samples)
         test_dataloader = build_pretraining_data_loader(test_ds, 0)
 
         # Flags to know if we need to do training/validation/testing.
@@ -3079,8 +2823,8 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
         do_valid = valid_dataloader is not None and args.eval_iters > 0
         do_test = test_dataloader is not None and args.eval_iters > 0
         flags = torch.tensor(
-            [int(do_train), int(do_valid), int(do_test)], dtype=torch.long, device=get_device_type_for_comm()
-        )
+            [int(do_train), int(do_valid), int(do_test)],
+            dtype=torch.long, device=get_device_type_for_comm())
     else:
         flags = torch.tensor([0, 0, 0], dtype=torch.long, device=get_device_type_for_comm())
 
@@ -3093,15 +2837,16 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
     return train_dataloader, valid_dataloader, test_dataloader
 
 
-def build_train_valid_test_data_iterators(build_train_valid_test_datasets_provider):
+def build_train_valid_test_data_iterators(
+        build_train_valid_test_datasets_provider):
     """Build pretraining data iterators."""
 
     args = get_args()
 
     # Build loaders.
-    train_dataloader, valid_dataloader, test_dataloader = build_train_valid_test_data_loaders(
-        build_train_valid_test_datasets_provider
-    )
+    train_dataloader, valid_dataloader, test_dataloader = \
+        build_train_valid_test_data_loaders(
+            build_train_valid_test_datasets_provider)
 
     # Build iterators.
     dl_type = args.dataloader_type
diff --git a/flagscale/train/train_gpt.py b/flagscale/train/train_gpt.py
index c7b69cab..59848184 100644
--- a/flagscale/train/train_gpt.py
+++ b/flagscale/train/train_gpt.py
@@ -7,7 +7,7 @@ from typing import List, Optional, Tuple, Union
 
 import torch
 
-from megatron.core import parallel_state
+from megatron.core import mpu
 from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder
 from megatron.core.datasets.gpt_dataset import GPTDataset, GPTDatasetConfig, MockGPTDataset
 from megatron.core.enums import ModelType
@@ -34,7 +34,6 @@ from megatron.training.utils import (
 from megatron.training.yaml_arguments import core_transformer_config_from_yaml
 
 import megatron.legacy.model  # isort: skip
-
 # NOTE: Loading `megatron.legacy.model` earlier fails due to circular import
 
 try:
@@ -51,13 +50,11 @@ from flagscale.train.extra_valid import extra_valid_datasets_provider
 from flagscale.train.train import pretrain
 from flagscale.train.global_vars import get_parallel_context
 
+from dcu_megatron import megatron_adaptor
 
 stimer = StragglerDetector()
 
-
-def model_provider(
-    pre_process=True, post_process=True, vp_stage: Optional[int] = None
-) -> Union[GPTModel, megatron.legacy.model.GPTModel]:
+def model_provider(pre_process=True, post_process=True) -> Union[GPTModel, megatron.legacy.model.GPTModel]:
     """Builds the model.
 
     If you set the use_legacy_models to True, it will return the legacy GPT model and if not the mcore GPT model.
@@ -78,24 +75,19 @@ def model_provider(
     use_te = args.transformer_impl == "transformer_engine"
 
     if args.record_memory_history:
-        torch.cuda.memory._record_memory_history(
-            True,
+        torch.cuda.memory._record_memory_history(True,
             # keep 100,000 alloc/free events from before the snapshot
             trace_alloc_max_entries=100000,
+
             # record stack information for the trace events
-            trace_alloc_record_context=True,
-        )
+            trace_alloc_record_context=True)
 
         def oom_observer(device, alloc, device_alloc, device_free):
             # snapshot right after an OOM happened
             print('saving allocated state during OOM')
             snapshot = torch.cuda.memory._snapshot()
             from pickle import dump
-
-            dump(
-                snapshot,
-                open(f"oom_rank-{torch.distributed.get_rank()}_{args.memory_snapshot_path}", 'wb'),
-            )
+            dump(snapshot, open(f"oom_rank-{torch.distributed.get_rank()}_{args.memory_snapshot_path}", 'wb'))
 
         torch._C._cuda_attach_out_of_memory_observer(oom_observer)
 
@@ -120,41 +112,29 @@ def model_provider(
             pre_process=pre_process,
             post_process=post_process,
         )
-    else:  # using core models
+    else: # using core models
         if args.spec is not None:
             transformer_layer_spec = import_module(args.spec)
         else:
             if args.num_experts:
                 # Define the decoder block spec
-                transformer_layer_spec = get_gpt_decoder_block_spec(
-                    config, use_transformer_engine=use_te, normalization=args.normalization
-                )
+                transformer_layer_spec = get_gpt_decoder_block_spec(config, use_transformer_engine=use_te, normalization=args.normalization)
             elif args.heterogeneous_layers_config_path is not None:
                 transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)
             else:
                 # Define the decoder layer spec
                 if use_te:
                     transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(
-                        args.num_experts,
-                        args.moe_grouped_gemm,
-                        args.qk_layernorm,
-                        args.multi_latent_attention,
-                        args.moe_use_legacy_grouped_gemm,
-                    )
+                        args.num_experts, args.moe_grouped_gemm,
+                        args.qk_layernorm, args.multi_latent_attention, args.moe_use_legacy_grouped_gemm)
                 else:
                     transformer_layer_spec = get_gpt_layer_local_spec(
-                        args.num_experts,
-                        args.moe_grouped_gemm,
-                        args.qk_layernorm,
-                        args.multi_latent_attention,
-                        args.moe_use_legacy_grouped_gemm,
-                        normalization=args.normalization,
-                    )
+                        args.num_experts, args.moe_grouped_gemm,
+                        args.qk_layernorm, args.multi_latent_attention, args.moe_use_legacy_grouped_gemm,
+                        normalization=args.normalization)
         mtp_block_spec = None
         if args.mtp_num_layers is not None:
-            mtp_block_spec = get_gpt_mtp_block_spec(
-                config, transformer_layer_spec, use_transformer_engine=use_te
-            )
+            mtp_block_spec = get_gpt_mtp_block_spec(config, transformer_layer_spec, use_transformer_engine=use_te)
 
         model = GPTModel(
             config=config,
@@ -171,7 +151,6 @@ def model_provider(
             rotary_base=args.rotary_base,
             rope_scaling=args.use_rope_scaling,
             mtp_block_spec=mtp_block_spec,
-            vp_stage=vp_stage,
         )
 
     return model
@@ -181,9 +160,7 @@ def get_batch(data_iterator):
     """Generate a batch."""
 
     # TODO: this is pretty hacky, find a better way
-    if (not parallel_state.is_pipeline_first_stage(ignore_virtual=True)) and (
-        not parallel_state.is_pipeline_last_stage(ignore_virtual=True)
-    ):
+    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):
         return None, None, None, None, None
 
     # get batches based on the TP rank you are on
@@ -199,9 +176,7 @@ def get_batch(data_iterator):
 SPIKY_LOSS_FACTOR = 10
 
 
-def loss_func(
-    loss_mask: torch.Tensor, output_tensor: torch.Tensor, model: Optional[GPTModel] = None
-):
+def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor, model: Optional[GPTModel] = None):
     """Loss function.
 
     Args:
@@ -220,45 +195,57 @@ def loss_func(
     if has_nvidia_modelopt and modelopt_args_enabled(args):  # [ModelOpt]
         return loss_func_modelopt(loss_mask, output_tensor, model=model)
 
-    losses = output_tensor.view(-1).float()
+    losses = output_tensor.float()
     loss_mask = loss_mask.view(-1).float()
-    loss = torch.sum(losses * loss_mask)
+    total_tokens = loss_mask.sum()
+    loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), total_tokens.view(1)])
+
+    if args.context_parallel_size > 1:
+        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())
 
     # Check individual rank losses are not NaN prior to DP all-reduce.
     rerun_state_machine = get_rerun_state_machine()
     if args.check_for_nan_in_loss_and_grad:
         rerun_state_machine.validate_result(
-            result=loss,
+            result=loss[0],
             rejection_func=torch.isnan,
             message="found NaN in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
+            tolerance=0.0,        # forward pass calculations are determinisic
             fatal=True,
         )
         rerun_state_machine.validate_result(
-            result=loss,
+            result=loss[0],
             rejection_func=torch.isinf,
             message="found Inf in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
+            tolerance=0.0,        # forward pass calculations are determinisic
             fatal=True,
         )
     # Check for spiky loss
     if args.check_for_spiky_loss:
         rerun_state_machine.validate_result(
-            result=loss,
+            result=loss[0],
             rejection_func=partial(
                 rerun_state_machine.is_unexpectedly_large,
                 threshold=SPIKY_LOSS_FACTOR,
                 context="loss",
             ),
             message="Spiky loss",
-            tolerance=0.0,  # forward pass calculations are determinisic
+            tolerance=0.0,        # forward pass calculations are determinisic
             fatal=False,
         )
-
-    num_tokens = loss_mask.sum().clone().detach().to(torch.int)
-    reporting_loss = torch.cat([loss.clone().detach().view(1), num_tokens.view(1)])
-
-    return (loss, num_tokens, {'lm loss': reporting_loss})
+    # Reduce loss for logging.
+    reporting_loss = loss.clone().detach()
+    torch.distributed.all_reduce(reporting_loss, group=mpu.get_data_parallel_group())
+
+    # loss[0] is a view of loss, so it has ._base not None, which triggers assert error
+    # in core/pipeline_parallel/schedule.py::deallocate_output_tensor, calling .clone()
+    # on loss[0] fixes this
+    local_num_tokens = loss[1].clone().detach().to(torch.int)
+    return (
+        loss[0].clone(),
+        local_num_tokens,
+        {'lm loss': (reporting_loss[0], reporting_loss[1])},
+    )
 
 
 def forward_step(data_iterator, model: GPTModel):
@@ -275,16 +262,17 @@ def forward_step(data_iterator, model: GPTModel):
     timers('batch-generator', log_level=2).start()
     global stimer
     with stimer(bdata=True):
-        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)
+        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(
+            data_iterator)
     timers('batch-generator').stop()
 
     with stimer:
         if args.use_legacy_models:
-            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)
+            output_tensor = model(tokens, position_ids, attention_mask,
+                                labels=labels)
         else:
-            output_tensor = model(
-                tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask
-            )
+            output_tensor = model(tokens, position_ids, attention_mask,
+                                labels=labels, loss_mask=loss_mask)
 
     # [ModelOpt]: model is needed to access ModelOpt distillation losses
     return output_tensor, partial(loss_func, loss_mask, model=model)
@@ -292,9 +280,8 @@ def forward_step(data_iterator, model: GPTModel):
 
 def is_dataset_built_on_rank():
     return (
-        parallel_state.is_pipeline_first_stage(ignore_virtual=True)
-        or parallel_state.is_pipeline_last_stage(ignore_virtual=True)
-    ) and parallel_state.get_tensor_model_parallel_rank() == 0
+        mpu.is_pipeline_first_stage() or mpu.is_pipeline_last_stage()
+    ) and mpu.get_tensor_model_parallel_rank() == 0
 
 
 def core_gpt_dataset_config_from_args(args):
@@ -379,7 +366,10 @@ def train_valid_test_datasets_provider(train_val_test_num_samples):
     print_rank_0("> building train, validation, and test datasets for GPT ...")
 
     train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(
-        dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config
+        dataset_type,
+        train_val_test_num_samples,
+        is_dataset_built_on_rank,
+        config
     ).build()
 
     print_rank_0("> finished creating GPT datasets ...")
diff --git a/flagscale/train/train_qwen2_5_vl.py b/flagscale/train/train_qwen2_5_vl.py
index 9befeec7..73958bd3 100644
--- a/flagscale/train/train_qwen2_5_vl.py
+++ b/flagscale/train/train_qwen2_5_vl.py
@@ -16,10 +16,9 @@
 
 import os
 import sys
-import logging
 from functools import partial
 from copy import deepcopy
-from typing import List, Optional, Tuple, Union
+from typing import Union, Optional, Tuple
 
 import torch
 import torch._dynamo
@@ -29,54 +28,21 @@ from argparse import Namespace
 # # For pytorch 2.6
 # torch.serialization.add_safe_globals([Namespace])
 
+from megatron.core import mpu
+
 from megatron.core import parallel_state
-from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder
-from megatron.core.datasets.gpt_dataset import GPTDataset, GPTDatasetConfig, MockGPTDataset
-from megatron.training.checkpointing import get_checkpoint_name # for dataloder
+from megatron.training.checkpointing import get_checkpoint_name
 from megatron.core.enums import ModelType
-from megatron.core.models.gpt import GPTModel
-from megatron.core.models.gpt.gpt_layer_specs import (
-    get_gpt_decoder_block_spec,
-    get_gpt_layer_local_spec,
-    get_gpt_layer_with_transformer_engine_spec,
-    get_gpt_mtp_block_spec,
-)
-from megatron.core.models.gpt.heterogeneous.heterogeneous_layer_specs import (
-    get_gpt_heterogeneous_layer_spec,
-)
-from megatron.core.rerun_state_machine import get_rerun_state_machine
-from megatron.core.transformer.spec_utils import import_module
-from megatron.core.utils import StragglerDetector
-from megatron.training import get_args, get_timers, get_tokenizer, print_rank_0
+from megatron.training import get_args, get_timers, pretrain, print_rank_0
 from megatron.training.arguments import core_transformer_config_from_args
 from megatron.training.utils import (
-    get_batch_on_this_cp_rank,
-    get_batch_on_this_tp_rank,
-    get_blend_and_blend_per_split,
+    average_losses_across_data_parallel_group,
 )
-from megatron.training.yaml_arguments import core_transformer_config_from_yaml
-
-import megatron.legacy.model  # isort: skip
-
-# NOTE: Loading `megatron.legacy.model` earlier fails due to circular import
-
-try:
-    from megatron.post_training.arguments import add_modelopt_args, modelopt_args_enabled
-    from megatron.post_training.loss_func import loss_func as loss_func_modelopt
-    from megatron.post_training.model_provider import model_provider as model_provider_modelopt
-
-    has_nvidia_modelopt = True
-except ImportError:
-    has_nvidia_modelopt = False
+from megatron.core.num_microbatches_calculator import get_num_microbatches
 
-from flagscale.train.datasets.sft_dataset import SFTDatasetConfig, SFTDataset
-from flagscale.train.extra_valid import extra_valid_datasets_provider
-from flagscale.train.train import pretrain
-stimer = StragglerDetector()
 
-#### especially for qwen2.5-vl ####
-from megatron.core.num_microbatches_calculator import get_num_microbatches
 torch._dynamo.config.suppress_errors = True
+from megatron.core import mpu
 from megatron.core.parallel_state import get_tensor_model_parallel_rank, get_pipeline_model_parallel_world_size, get_pipeline_model_parallel_rank
 from megatron.energon import (
     LimitDataset,
@@ -104,10 +70,7 @@ from flagscale.train.models.qwen2_5_vl.transformer_config import (
     get_vision_projection_config
 )
 from tools.datasets.qwenvl.data.dataset_helpers import TaskEncoder, print_error_handler
-#### especially for qwen2.5-vl ####
-IGNORE_IDX=-100
-FIRST_MAX_PADDING_FLAG = True
-LAST_LARGE_IMG=False
+
 def model_provider(
     pre_process=True, post_process=True, add_encoder=True, add_decoder=True
 ) -> Union[Qwen2_5VLModel]:
@@ -135,8 +98,6 @@ def model_provider(
     transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.qk_layernorm)
     vision_model_spec = get_qwen2vl_vision_model_spec()
     vision_projector_spec = get_mlp_module_spec(add_norm=False).submodules
-    if args.enable_variable_seq_lengths:
-        config.variable_seq_lengths = True
 
     model = Qwen2_5VLModel(
         language_transformer_config=config,
@@ -175,7 +136,7 @@ def model_provider(
 
     return model
 
-# copy from https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1404
+# Slightly modified from Qwen2_5VLForConditionalGeneration.get_rope_index
 def get_rope_index(
     input_ids: Optional[torch.LongTensor] = None,
     image_grid_thw: Optional[torch.LongTensor] = None,
@@ -401,16 +362,9 @@ def get_batch(data_iterator):
     torch.cuda.nvtx.range_push("get_data")
     if data_iterator is not None and get_tensor_model_parallel_rank() == 0:
         data = next(data_iterator)
-        # pad_token_id = get_tokenizer().pad_token_id
-        pad_token_id = IGNORE_IDX
-        # while (data["target"] == pad_token_id).all() or (data["target"].shape[-1] < 986 or data["target"].shape[-1] > 1000): # for debug
-        while (data["target"] == pad_token_id).all():
-            logging.getLogger(__name__).warning("The current data is invalid because the target is all pad_token_id! Get next data to avoid fail, but it's better to check the data!")
-            data = next(data_iterator)
     else:
         data = None
 
-
     data_text =  broadcast_data(["text"], data, torch.int64)["text"]
 
     target =  broadcast_data(["target"], data, torch.int64)["target"]
@@ -422,17 +376,6 @@ def get_batch(data_iterator):
 
     # shape: n_image_samples
     image_thw_grids = broadcast_data(["image_thw_grids"], data, torch.long)["image_thw_grids"]
-
-    # global LAST_LARGE_IMG
-    # if LAST_LARGE_IMG:
-    #     torch.cuda.empty_cache()
-    #     LAST_LARGE_IMG=False
-    # if image_thw_grids.prod(axis=-1).sum() // 4 > 3000:
-    #     torch.cuda.empty_cache()
-    #     LAST_LARGE_IMG = True
-    args = get_args()
-    if data_text.shape[-1] == args.max_padding_length and get_pipeline_model_parallel_rank() == 0:
-        torch.cuda.empty_cache()
     # shape: n_video_samples
     video_thw_grids = broadcast_data(["video_thw_grids"], data, torch.long)["video_thw_grids"]
     # shape: n_video_samples
@@ -455,7 +398,7 @@ def get_batch(data_iterator):
     # NOTE: no sequence packing in LLM inputs
     torch.cuda.nvtx.range_push("get_ltor_masks_and_position_ids")
     attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
-        tokens, image_thw_grids, video_thw_grids, labels, IGNORE_IDX, second_per_grid_ts
+        tokens, image_thw_grids, video_thw_grids, labels, tokenizer.pad_token_id, second_per_grid_ts
     )
     torch.cuda.nvtx.range_pop()
 
@@ -473,71 +416,34 @@ def get_batch(data_iterator):
         video_input_mask
     )
 
-# define spiky loss as a loss that's 10x the max loss observed
-SPIKY_LOSS_FACTOR = 10
-
-
-def loss_func(
-    loss_mask: torch.Tensor, output_tensor: torch.Tensor, model: Optional[Qwen2_5VLModel] = None
-):
+def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):
     """Loss function.
 
     Args:
         loss_mask (torch.Tensor): Used to mask out some portions of the loss
         output_tensor (torch.Tensor): The tensor with the losses
-        model (Qwen2_5VLModel, optional): The model (can be wrapped)
-
-    Returns:
-        the loss scalar for this micro-batch
-        the number of non-padded tokens in this microbatch
-        a dict containing reporting metrics on the loss and number of tokens across
-            the data parallel ranks
     """
     args = get_args()
 
-    if has_nvidia_modelopt and modelopt_args_enabled(args):  # [ModelOpt]
-        return loss_func_modelopt(loss_mask, output_tensor, model=model)
-
-    losses = output_tensor.view(-1).float()
+    losses = output_tensor.float()
     loss_mask = loss_mask.view(-1).float()
-    loss = torch.sum(losses * loss_mask)
+
+    loss = torch.stack([torch.sum(losses.view(-1) * loss_mask), loss_mask.sum()])
+    if args.context_parallel_size > 1:
+        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())
 
     # Check individual rank losses are not NaN prior to DP all-reduce.
-    rerun_state_machine = get_rerun_state_machine()
     if args.check_for_nan_in_loss_and_grad:
-        rerun_state_machine.validate_result(
-            result=loss,
-            rejection_func=torch.isnan,
-            message="found NaN in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
-            fatal=True,
+        global_rank = torch.distributed.get_rank()
+        assert not loss.isnan().any(), (
+            f"Rank {global_rank}: found NaN in local forward loss calculation. "
+            f"Device: {torch.cuda.current_device()}, node: {os.uname()[1]}"
         )
-        rerun_state_machine.validate_result(
-            result=loss,
-            rejection_func=torch.isinf,
-            message="found Inf in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
-            fatal=True,
-        )
-    # Check for spiky loss
-    if args.check_for_spiky_loss:
-        rerun_state_machine.validate_result(
-            result=loss,
-            rejection_func=partial(
-                rerun_state_machine.is_unexpectedly_large,
-                threshold=SPIKY_LOSS_FACTOR,
-                context="loss",
-            ),
-            message="Spiky loss",
-            tolerance=0.0,  # forward pass calculations are determinisic
-            fatal=False,
-        )
-
-    num_tokens = loss_mask.sum().clone().detach().to(torch.int)
-    reporting_loss = torch.cat([loss.clone().detach().view(1), num_tokens.view(1)])
 
-    return (loss, num_tokens, {'lm loss': reporting_loss})
+    averaged_loss = average_losses_across_data_parallel_group(loss)
+    averaged_loss = averaged_loss[0] / averaged_loss[1]
 
+    return loss[0] * args.context_parallel_size, {"lm loss": averaged_loss}
 
 def forward_step(data_iterator, model: Qwen2_5VLModel):
     """Forward training step.
@@ -546,43 +452,40 @@ def forward_step(data_iterator, model: Qwen2_5VLModel):
         data_iterator : Input data iterator
         model (GPTModel): The GPT Model
     """
-    args = get_args()
     timers = get_timers()
-
     # Get the batch.
-    timers('batch-generator', log_level=2).start()
-    global stimer
-    with stimer(bdata=True):
-        (
-            tokens,
-            labels,
-            loss_mask,
-            attention_mask,
-            position_ids,
-            imgs,
-            videos,
-            image_thw_grids,
-            video_thw_grids,
-            image_input_mask,
-            video_input_mask
-        ) = get_batch(data_iterator)
-    timers('batch-generator').stop()
+    timers("batch-generator", log_level=2).start()
+    (
+        tokens,
+        labels,
+        loss_mask,
+        attention_mask,
+        position_ids,
+        imgs,
+        videos,
+        image_thw_grids,
+        video_thw_grids,
+        image_input_mask,
+        video_input_mask
+    ) = get_batch(data_iterator)
+    timers("batch-generator").stop()
+
     vision_data = torch.cat([imgs, videos], dim=0)
     vision_grid = torch.cat([image_thw_grids, video_thw_grids], dim=0)
-    with stimer:
-        output_tensor = model(
-            input_ids = tokens,
-            position_ids = position_ids,
-            vision_data = vision_data,
-            vision_grid_thw =  vision_grid,
-            video_start_index = image_input_mask.sum().cpu().item(),
-            image_input_mask = image_input_mask,
-            video_input_mask = video_input_mask,
-            attention_mask = attention_mask,
-            labels = labels
-        )
 
-    return output_tensor, partial(loss_func, loss_mask, model=model)
+    output_tensor = model(
+        input_ids = tokens,
+        position_ids = position_ids,
+        vision_data = vision_data,
+        vision_grid_thw =  vision_grid,
+        video_start_index = image_input_mask.sum().cpu().item(),
+        image_input_mask = image_input_mask,
+        video_input_mask = video_input_mask,
+        attention_mask = attention_mask,
+        labels = labels
+    )
+
+    return output_tensor, partial(loss_func, loss_mask)
 
 def run_online_eval(model):
     """Run an evaluation benchmark during training."""
@@ -607,36 +510,34 @@ def datasets_provider(worker_config=None):
         batch_size=args.micro_batch_size,
         task_encoder=TaskEncoder(),
         worker_config=worker_config,
-        virtual_epoch_length=0,
-        max_samples_per_sequence=args.max_samples_per_sequence, # sequential shuffle in a tar
-        shuffle_buffer_size=args.shuffle_buffer_size, # shuffle in a sequential
+        virtual_epoch_length=1000,
+        max_samples_per_sequence=100,
+        shuffle_buffer_size=100,
         handler=print_error_handler,
-        repeat=True,
         image_decode="pil",
     )
-    val_datasets_without_source_datasets = None
-    if args.eval_iters > 0:
-        val_datasets = get_val_datasets(
-            dname,
-            batch_size=args.micro_batch_size,
-            # This is the total number over all workers
-            # limit=args.eval_iters * get_num_microbatches(),
-            task_encoder=TaskEncoder(),
+
+    val_datasets = get_val_datasets(
+        dname,
+        batch_size=args.micro_batch_size,
+        # This is the total number over all workers
+        # limit=args.eval_iters * get_num_microbatches(),
+        task_encoder=TaskEncoder(),
+        worker_config=worker_config,
+        handler=print_error_handler,
+        image_decode="pil",
+    )
+    val_datasets_without_source_datasets = [
+        # Limit the dataset to eval_iters * num_microbatches
+        LimitDataset(
+            # Repeat the inner dataset in case it's too short
+            RepeatDataset(val_ds, worker_config=worker_config),
+            length=args.eval_iters * get_num_microbatches(),
             worker_config=worker_config,
-            handler=print_error_handler,
-            image_decode="pil",
+            reset_after_epoch=True,
         )
-        val_datasets_without_source_datasets = [
-            # Limit the dataset to eval_iters * num_microbatches
-            LimitDataset(
-                # Repeat the inner dataset in case it's too short
-                RepeatDataset(val_ds, worker_config=worker_config),
-                length=args.eval_iters * get_num_microbatches(),
-                worker_config=worker_config,
-                reset_after_epoch=True,
-            )
-            for val_ds, _src_ds in val_datasets
-        ]
+        for val_ds, _src_ds in val_datasets
+    ]
 
     return train_dataset, val_datasets_without_source_datasets, None
 
@@ -663,9 +564,8 @@ def is_dataloader_rank(encoder_pipeline_model_parallel_size):
     # Run dataloader only on the first tensor parallel rank (will be broadcasted to others).
     is_first_rank = get_tensor_model_parallel_rank() == 0
 
-    # NOTE(lizhiyu): when pp_size > 2
-    # pp_size = get_pipeline_model_parallel_world_size()
-    # is_first_rank = is_first_rank and is_first_or_last_stage(pp_size, encoder_pipeline_model_parallel_size)
+    pp_size = get_pipeline_model_parallel_world_size()
+    is_first_rank = is_first_rank and is_first_or_last_stage(pp_size, encoder_pipeline_model_parallel_size)
 
     return is_first_rank
 
@@ -711,13 +611,10 @@ def train_valid_test_dataloaders_provider(train_val_test_num_samples):
                 except Exception as e:
                     print_rank_0("loading dataloader checkpoint failed. Skipping. " + str(e))
 
-    if valid_ds1 is not None:
-        valid_dataloader = [
-            EnergonDataloader(get_loader(valid_ds, worker_config=worker_config))
-            for valid_ds in valid_ds1
-        ]
-    else:
-        valid_dataloader = EnergonDataloader(None)
+    valid_dataloader = [
+        EnergonDataloader(get_loader(valid_ds, worker_config=worker_config))
+        for valid_ds in valid_ds1
+    ]
     test_dataloader = None # NOTE: no test
 
     return EnergonDataloader(train_dataloader), valid_dataloader, EnergonDataloader(test_dataloader)
@@ -758,17 +655,6 @@ def add_multimodal_extra_args(parser):
     group.add_argument("--temporal-patch-size", type=int, default=2)
     group.add_argument("--patch-size", type=int, default=14)
     group.add_argument("--max-padding-length", type=int, default=2048)
-    group.add_argument("--enable-variable-seq-lengths", action="store_true", default=False, help="Enable variable sequence lengths")
-    group.add_argument("--vision-root", type=str, default = None, help="The vision dirctory root path.")
-    group.add_argument("--max-samples-per-sequence", type=int, default=2**31-1, help="max sequencial seqence samples in a slice")
-    group.add_argument("--shuffle-buffer-size", type=int, default=0, help="the buffer size to shuffle the samples in a seqence")
-    # learning rate
-    group.add_argument("--vision-ration", type=float, default=0.1, help="the learning rate ration of vision(inlude merger) compared with llm")
-    group.add_argument("--image-max-pixels", type=int, default=768*768, help="the maximum pixels of a single image")
-    group.add_argument("--image-min-pixels", type=int, default=32*32, help="the minimum pixels of a single image")
-    group.add_argument("--vision-recompute-layer-steps", type=int, default=0, help="the recmoute layers for vision using uniform method. 0 is disable.")
-
-
 
     # just for checkpoint conversion
     group.add_argument(
diff --git a/tools/datasets/qwenvl/build_llava_frame_dataset.py b/tools/datasets/qwenvl/build_llava_frame_dataset.py
index a907b234..587866b6 100644
--- a/tools/datasets/qwenvl/build_llava_frame_dataset.py
+++ b/tools/datasets/qwenvl/build_llava_frame_dataset.py
@@ -92,7 +92,7 @@ def process(dataset_root, output_file, interval=1.0, num_workers: int = 32, vide
                 data = json.load(f)
         except:
             with open(file, "r") as f:
-                data = [json.loads(l) for l in f.readlines()]
+                data = [json.loads(f) for l in f.readlines()]
 
         print(f"processing {file}")
         for d in tqdm(data):
diff --git a/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py b/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py
new file mode 100644
index 00000000..b74a1a0c
--- /dev/null
+++ b/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py
@@ -0,0 +1,144 @@
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/convert_custom_dataset_to_wds_chatml.py
+import json
+import os
+import pickle
+
+from argparse import ArgumentParser
+
+import cv2
+import webdataset as wds
+import yaml
+
+from tqdm import tqdm
+from webdataset.writer import add_handlers, default_handlers, imageencoder
+
+from megatron.energon.epathlib import EPath
+from megatron.energon.flavors import BaseWebdatasetFactory
+
+
+def convert(dataset_dir, json_name, sort_function=sorted, max_count=10000):
+    """
+    Here we provide an example to convert llava-pretrain dataset to ChatMLSample
+    """
+    # Paths to the dataset files
+    json_file = os.path.join(dataset_dir, json_name)
+    output = os.path.join(dataset_dir, "wds")
+
+    if not os.path.exists(output):
+        os.mkdir(output)
+
+    # Load data
+    with open(json_file, "r") as f:
+        data = json.load(f)
+
+    # custom webdataset ShardWriter Encoder
+    add_handlers(
+        default_handlers, "jpgs", lambda data: pickle.dumps([imageencoder(d, "jpg") for d in data])
+    )
+    add_handlers(
+        default_handlers,
+        "videos",
+        lambda data: pickle.dumps([[imageencoder(d, "jpg") for d in video] for video in data]),
+    )
+
+    has_idx = None
+    with wds.ShardWriter(
+        os.path.join(output, "pretrain-%d.tar"), maxcount=max_count
+    ) as shard_writer:
+        for idx, entry in enumerate(tqdm(data)):
+            # NOTE: read a dataset in sharegpt format
+            image_datas = []
+            for image in entry.pop("images", []):
+                image_datas.append(
+                    cv2.imread(os.path.join(dataset_dir, image), cv2.IMREAD_UNCHANGED)
+                )
+
+            video_datas = []
+            second_per_grid_ts = []
+            for video in entry.pop("videos", []):
+                video_noext, _ = os.path.splitext(video)
+                frame_folder = os.path.join(dataset_dir, video_noext)
+                # NOTE: we implicitly require a `${frame_folder}.json`` file containing fps rates of each video
+                # otherwise fps will be regarded as `1` by default.
+                if os.path.exists(frame_folder + ".json"):
+                    with open(frame_folder + ".json", "r") as f:
+                        fps = float(json.load(f)["fps"])
+                else:
+                    fps = 2.0
+
+                frames = []
+                for frame in sort_function(os.listdir(frame_folder)):
+                    frames.append(
+                        cv2.imread(os.path.join(frame_folder, frame), cv2.IMREAD_UNCHANGED)
+                    )
+
+                if len(frames) % 2 == 1:
+                    frames = frames[:-1]
+                video_datas.append(frames)
+                second_per_grid_ts.append(1 / fps)
+
+            if has_idx is None:
+                has_idx = "id" in entry
+            assert has_idx == ("id" in entry), "All entries should either all contain idx or not."
+
+            sample = {
+                "__key__": entry.pop("id", str(idx)),
+                "jpgs": image_datas,
+                "videos": video_datas,
+                "json": json.dumps(
+                    {
+                        "conversations": entry["conversations"],
+                        "second_per_grid_ts": second_per_grid_ts,
+                    }
+                ).encode("utf-8"),
+            }
+            shard_writer.write(sample)
+
+    print(f"Dataset successfully converted to wds")
+    return output
+
+
+def generate_configs(path: EPath, split, shuffle_tars=True, num_workers=32):
+    path = path.absolute()
+    all_tars = list(path.glob("**/*.tar")) + list(path.glob("**/*.tgz"))
+    all_tars = [str(p.relative_to(path)) for p in sorted(all_tars)]
+    split_parts_ratio = [("train", split[0]), ("val", split[1]), ("test", split[2])]
+    split_parts_patterns = None
+
+    # NOTE: generate .info.yaml and split.yaml
+    _ = BaseWebdatasetFactory.prepare_dataset(
+        path,
+        all_tars,
+        split_parts_ratio=split_parts_ratio,
+        split_parts_patterns=split_parts_patterns,
+        tar_index_only=False,
+        shuffle_seed=42 if shuffle_tars else None,
+        workers=num_workers,
+    )
+
+    # NOTE: dump dataset.yaml
+    metadata = {
+        "__class__": "ChatMLWebdataset",
+        "__module__": "tools.datasets.qwenvl.data..energon.chatml",
+        "field_map": {"imgs": "jpgs", "videos": "videos", "conversation": "json"},
+    }
+    with open(os.path.join(path.url, ".nv-meta", "dataset.yaml"), "w") as f:
+        yaml.safe_dump(metadata, f)
+
+
+if __name__ == "__main__":
+    argparser = ArgumentParser()
+    argparser.add_argument("--dataset-root", required=True, type=str)
+    argparser.add_argument("--json", default="dataset.json", type=str)
+    argparser.add_argument("--max-samples-per-tar", default=10000, type=float)
+    argparser.add_argument("--train-split", default=9, type=float)
+    argparser.add_argument("--val-split", default=1, type=float)
+    argparser.add_argument("--test-split", default=0, type=float)
+    args = argparser.parse_args()
+
+    output_dir = convert(args.dataset_root, args.json, max_count=args.max_samples_per_tar)
+    print(f"Generating Configurations")
+    # NOTE: split_ratio: train/val/test
+    split = [args.train_split, args.val_split, args.test_split]
+    generate_configs(EPath(output_dir), split)
+    print(f"Configurations Generated")
diff --git a/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml_str.py b/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml_str.py
deleted file mode 100644
index e7e1517b..00000000
--- a/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml_str.py
+++ /dev/null
@@ -1,216 +0,0 @@
-# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/convert_custom_dataset_to_wds_chatml.py
-# We must store the path of vision data, not the real data.
-
-import json
-import math
-import os
-import pickle
-
-from argparse import ArgumentParser
-from typing import List, Union
-
-import cv2
-import webdataset as wds
-import yaml
-
-from tqdm import tqdm
-from webdataset.writer import add_handlers, default_handlers, imageencoder
-
-from megatron.energon.epathlib import EPath
-from megatron.energon.flavors import BaseWebdatasetFactory
-
-
-def convert(
-    dataset_dir,
-    output_dir,
-    json_name,
-    sort_function=sorted,
-    max_count=10000,
-    image_key="images",
-    video_key="videos",
-    vision_dir=None,
-    dp_size=1,
-    drop_last=False,
-):
-    """
-    Here we provide an example to convert llava-pretrain dataset to ChatMLSample
-    """
-    if vision_dir is None:
-        vision_dir = dataset_dir
-    # Paths to the dataset files
-    json_file = os.path.join(dataset_dir, json_name)
-    output = os.path.join(output_dir, f"wds-{dp_size}")
-    os.makedirs(output, exist_ok=True)
-
-    # support both json and jsonl
-    try:
-        with open(json_file, "r") as f:
-            data = json.load(f)
-    except:
-        with open(json_file, "r") as f:
-            data = [json.loads(l) for l in f.readlines()]
-    data_len = len(data)
-    print(f"Loaded {data_len} entries")
-
-    print(f"The fisrt entry in the dataset is {data[0]}")
-    if image_key not in data[0]:
-        print(f"Warning: {image_key} not found in the first entry")
-    if video_key not in data[0]:
-        print(f"Warning: {video_key} not found in the first entry")
-
-    # custom webdataset ShardWriter Encoder
-    # "jpgs": the key when saving the image, see line 93
-    # "videos": the key when saving the video, see line 92
-    add_handlers(default_handlers, 'jpgs', lambda data: pickle.dumps(data))
-    add_handlers(default_handlers, 'videos', lambda data: pickle.dumps(data))
-
-    def write_sample(entry, vision_dir, has_idx=None, idx=0):
-        # NOTE: read a dataset in sharegpt format
-        image_datas: List[str] = []
-        # NOTE: we support both list and str for image path.
-        image_paths = entry.get(image_key, [])
-        if isinstance(image_paths, str):
-            image_paths = [image_paths]
-        image_datas = image_paths
-
-        video_datas: List[List[str]] = []
-        second_per_grid_ts = []
-
-        for video in entry.pop(video_key, []):
-            video_noext, _ = os.path.splitext(video)
-            frame_folder = os.path.join(vision_dir, video_noext)
-            # NOTE: we implicitly require a `${frame_folder}.json`` file containing fps rates of each video
-            # otherwise fps will be regarded as `1` by default.
-            if os.path.exists(frame_folder + ".json"):
-                with open(frame_folder + ".json", "r") as f:
-                    fps = float(json.load(f)["fps"])
-            else:
-                fps = 2.0
-
-            frames: List[str] = []
-            for frame in sort_function(os.listdir(frame_folder)):
-                # get relative path（remove "vision_dir"）
-                relative_path = os.path.relpath(os.path.join(frame_folder, frame), start=vision_dir)
-                frames.appen(relative_path)
-
-            if len(frames) % 2 == 1:
-                frames = frames[:-1]
-            video_datas.append(frames)
-            second_per_grid_ts.append(1 / fps)
-
-        if has_idx is None:
-            has_idx = "id" in entry
-        assert has_idx == ("id" in entry), "All entries should either all contain idx or not."
-
-        sample = {
-            "__key__": entry.pop("id", str(idx)),
-            "jpgs": image_datas,
-            "videos": video_datas,
-            "json": json.dumps(
-                {"conversations": entry["conversations"], "second_per_grid_ts": second_per_grid_ts}
-            ).encode("utf-8"),
-        }
-        shard_writer.write(sample)
-
-    has_idx = None
-    if drop_last:
-        num_per_rank = data_len // dp_size
-        left_data_count = data_len % dp_size
-        with wds.ShardWriter(
-            os.path.join(output, "pretrain-%d.tar"), maxcount=max_count, maxsize=9e9
-        ) as shard_writer:
-            for rank in tqdm(range(dp_size)):
-                for id in tqdm(range(num_per_rank)):
-                    data_id = id * dp_size + rank
-                    entry = data[data_id]
-                    write_sample(entry, vision_dir, has_idx=has_idx, idx=data_id)
-            if left_data_count > 0:
-                for idx, entry in enumerate(data[data_len - left_data_count :]):
-                    write_sample(
-                        entry, vision_dir, has_idx=has_idx, idx=data_len - left_data_count + idx
-                    )
-    else:
-        num_per_rank = math.ceil(data_len / dp_size)
-        with wds.ShardWriter(
-            os.path.join(output, "pretrain-%d.tar"), maxcount=max_count, maxsize=9e9
-        ) as shard_writer:
-            for rank in tqdm(range(dp_size)):
-                for id in tqdm(range(num_per_rank)):
-                    data_id = id * dp_size + rank
-                    if data_id >= data_len:
-                        break
-                    entry = data[data_id]
-                    write_sample(entry, vision_dir, has_idx=has_idx, idx=data_id)
-
-    print(f"Dataset successfully converted to wds")
-    return output
-
-
-def generate_configs(path: EPath, split, shuffle_tars=True, num_workers=1):
-    # path = path.absolute()
-    all_tars = list(path.glob("**/*.tar")) + list(path.glob("**/*.tgz"))
-    all_tars = [str(p.relative_to(path)) for p in sorted(all_tars)]
-    split_parts_ratio = [("train", split[0]), ("val", split[1]), ("test", split[2])]
-    split_parts_patterns = None
-
-    # NOTE: generate .info.yaml and split.yaml
-    _ = BaseWebdatasetFactory.prepare_dataset(
-        path,
-        all_tars,
-        split_parts_ratio=split_parts_ratio,
-        split_parts_patterns=split_parts_patterns,
-        tar_index_only=False,
-        shuffle_seed=42 if shuffle_tars else None,
-        workers=num_workers,
-    )
-
-    # NOTE: dump dataset.yaml
-    metadata = {
-        "__class__": "ChatMLWebdataset",
-        "__module__": "tools.datasets.qwenvl.data.energon.chatml",
-        "field_map": {"imgs": "jpgs", "videos": "videos", "conversation": "json"},
-    }
-    with open(os.path.join(path.url, ".nv-meta", "dataset.yaml"), "w") as f:
-        yaml.safe_dump(metadata, f)
-
-
-if __name__ == "__main__":
-    argparser = ArgumentParser()
-    argparser.add_argument("--dataset-root", required=True, type=str)
-    argparser.add_argument("--output-root", required=True, type=str)
-    argparser.add_argument("--vision-root", default=None, type=str)
-    argparser.add_argument("--json", default="dataset.json", type=str)
-    argparser.add_argument(
-        "--images-key", default="images", type=str, help="The key for images in json"
-    )
-    argparser.add_argument(
-        "--videos-key", default="videos", type=str, help="The key for videos in json"
-    )
-    argparser.add_argument("--max-samples-per-tar", default=10000, type=float)
-    argparser.add_argument("--train-split", default=1, type=float)
-    argparser.add_argument("--val-split", default=0, type=float)
-    argparser.add_argument("--test-split", default=0, type=float)
-    argparser.add_argument("--shuffle-tars", action="store_true")
-    argparser.add_argument("--num-workers", default=1, type=int)
-    argparser.add_argument("--dp-size", default=1, type=int)
-    argparser.add_argument("--drop-last", action="store_true")
-    args = argparser.parse_args()
-    print(f"=======input args=======\n{args}\n=======input args=======\n")
-    output_dir = convert(
-        args.dataset_root,
-        args.output_root,
-        args.json,
-        max_count=args.max_samples_per_tar,
-        image_key=args.images_key,
-        video_key=args.videos_key,
-        vision_dir=args.vision_root,
-        dp_size=args.dp_size,
-        drop_last=args.drop_last,
-    )
-    print(f"Generating Configurations")
-    # NOTE: split_ratio: train/val/test
-    split = [args.train_split, args.val_split, args.test_split]
-    generate_configs(
-        EPath(output_dir), split, shuffle_tars=args.shuffle_tars, num_workers=args.num_workers
-    )
-    print(f"Configurations Generated")
diff --git a/tools/datasets/qwenvl/data/dataset_helpers.py b/tools/datasets/qwenvl/data/dataset_helpers.py
index 93930ac9..846fddee 100644
--- a/tools/datasets/qwenvl/data/dataset_helpers.py
+++ b/tools/datasets/qwenvl/data/dataset_helpers.py
@@ -14,9 +14,6 @@
 # limitations under the License.
 import dataclasses
 import json
-import logging
-import math
-import os
 import re
 import sys
 import traceback
@@ -26,10 +23,8 @@ from dataclasses import dataclass
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
-import PIL
 import torch
 
-from PIL import Image
 from torchvision import transforms as T
 
 from megatron.energon import Batch, DefaultTaskEncoder, VQASample
@@ -38,11 +33,6 @@ from megatron.training.global_vars import get_tokenizer
 from tools.datasets.qwenvl.data.energon.chatml import ChatMLSample
 from tools.datasets.qwenvl.data.image_processing import get_visual_transform
 
-dataset_logger = logging.getLogger(__name__)
-FIRST_MAX_PADDING_FLAG = True
-IGNORE_IDX = -100
-MAX_IMG_THRESHHOLD = 5000
-
 
 # Type for intermediate batch, after batch()
 @dataclass
@@ -98,9 +88,8 @@ def convert_to_qwen2vl_content(
     mm_idx = defaultdict(int)
     for matched in re.finditer(pattern, user_input):
         start, end = matched.span()
-        text = user_input[cur:start]
-        if text:
-            contents.append({"type": "text", "text": text})
+        if start > cur:
+            contents.append({"type": "text", "text": user_input[cur:start].strip()})
 
         contents.append(
             {
@@ -113,7 +102,7 @@ def convert_to_qwen2vl_content(
         mm_idx[matched.string[start:end][1:-1]] += 1
 
     if cur < len(user_input):
-        contents.append({"type": "text", "text": user_input[cur : len(user_input)]})
+        contents.append({"type": "text", "text": user_input[cur : len(user_input)].strip()})
 
     return contents
 
@@ -129,9 +118,6 @@ class TaskEncoder(
         super().__init__()
 
         self.args = get_args()
-        self.tp_size = self.args.tensor_model_parallel_size
-        self.cp_size = self.args.context_parallel_size
-        self.sequence_parallel = self.args.sequence_parallel
 
         self.tokenizer = get_tokenizer()
 
@@ -141,9 +127,6 @@ class TaskEncoder(
 
         self.seq_len = self.args.max_padding_length
 
-        self.vision_root = self.args.vision_root
-        assert self.vision_root is not None, "Please give the vision root."
-
     def encode_sample(self, sample: Union[VQASample, ChatMLSample]):
         if isinstance(sample, VQASample):
             is_llava_training = (
@@ -201,100 +184,17 @@ class TaskEncoder(
             thw_grids.append((grid_t, grid_h, grid_w))
         return flattened, np.array(thw_grids)
 
-    # copy from
-    def _preprocess_image(
-        self, image: PIL.Image, image_max_pixels: int = 768 * 768, image_min_pixels: int = 32 * 32
-    ) -> PIL.Image:
-        r"""
-        Pre-processes a single image.
-        """
-        if (image.width * image.height) > image_max_pixels:
-            resize_factor = math.sqrt(image_max_pixels / (image.width * image.height))
-            width, height = int(image.width * resize_factor), int(image.height * resize_factor)
-            image = image.resize((width, height))
-
-        if (image.width * image.height) < image_min_pixels:
-            resize_factor = math.sqrt(image_min_pixels / (image.width * image.height))
-            width, height = int(image.width * resize_factor), int(image.height * resize_factor)
-            image = image.resize((width, height))
-
-        if image.mode != "RGB":
-            image = image.convert("RGB")
-
-        if min(image.width, image.height) < 28:
-            width, height = max(image.width, 28), max(image.height, 28)
-            image = image.resize((width, height), resample=Image.Resampling.NEAREST)
-
-        if image.width / image.height > 200:
-            width, height = image.height * 180, image.height
-            image = image.resize((width, height), resample=Image.Resampling.NEAREST)
-
-        if image.height / image.width > 200:
-            width, height = image.width, image.width * 180
-            image = image.resize((width, height), resample=Image.Resampling.NEAREST)
-
-        return image
-
     def encode_chatml(self, sample: ChatMLSample):
-        # # TODO: modify get_visual_transform to add more augmentations
-        # imgs = [get_visual_transform(os.path.join(self.vision_root, img))[0] for img in sample.imgs]
-        # videos = [
-        #     [get_visual_transform(os.path.join(self.vision_root, frame))[0] for frame in video]
-        #     for video in sample.videos
-        # ]
-        # # NOTE: make n_frames even foreach video
-        # for i, video in enumerate(videos):
-        #     videos[i] = video[: len(video) // 2 * 2]
-
-        # # NOTE: flatten all images
-        # flattened_imgs, image_thw_grids = self._flatten_visual_inputs(imgs, is_image=True)
-        # flattened_videos, video_thw_grids = self._flatten_visual_inputs(videos, is_image=False)
-
-        #######################################################################################
-        # NOTE(lizhiyu): use the transformers processor
-        if sample.imgs is not None and len(sample.imgs) > 0:
-            imgs = []
-            for img in sample.imgs:
-                img_path = os.path.join(self.vision_root, img)
-                try:
-                    image = PIL.Image.open(img_path)
-                    image = self._preprocess_image(
-                        image=image,
-                        image_max_pixels=self.args.image_max_pixels,
-                        image_min_pixels=self.args.image_min_pixels,
-                    )
-                    imgs.append(image)
-                except Exception as e:
-                    raise ValueError(
-                        f"Failed to open image: {img_path}. Error: {e} of smaple[{sample.__key__}]"
-                    )
-                    # raise InternalWarning(
-                    #     f"Failed to open image: {img_path}. Error: {e} of smaple[{sample.__key__}]"
-                    # )
-            imgs_info = self.tokenizer.processor.image_processor(imgs, return_tensors="np")
-            flattened_imgs = imgs_info["pixel_values"]
-            image_thw_grids = imgs_info["image_grid_thw"]
-        else:
-            flattened_imgs = []
-            image_thw_grids = []
-
-        if sample.videos is not None and len(sample.videos) > 0:
-            videos = [
-                [PIL.Image.open(os.path.join(self.vision_root, frame)) for frame in video]
-                for video in sample.videos
-            ]
-            # NOTE: make n_frames even foreach video
-            for i, video in enumerate(videos):
-                videos[i] = video[: len(video) // 2 * 2]
-            videos_info = self.tokenizer.processor.image_processor(
-                images=None, videos=videos, return_tensors="pt"
-            )
-            flattened_videos = videos_info["pixel_values_videos"]
-            video_thw_grids = videos_info["video_grid_thw"]
-        else:
-            flattened_videos = []
-            video_thw_grids = []
-        #######################################################################################
+        # TODO: modify get_visual_transform to add more augmentations
+        imgs = [get_visual_transform(img)[0] for img in sample.imgs]
+        videos = [[get_visual_transform(frame)[0] for frame in video] for video in sample.videos]
+        # NOTE: make n_frames even foreach video
+        for i, video in enumerate(videos):
+            videos[i] = video[: len(video) // 2 * 2]
+
+        # NOTE: flatten all images
+        flattened_imgs, image_thw_grids = self._flatten_visual_inputs(imgs, is_image=True)
+        flattened_videos, video_thw_grids = self._flatten_visual_inputs(videos, is_image=False)
 
         # NOTE: generate qwen2vl conversations
         conversation = (
@@ -312,8 +212,6 @@ class TaskEncoder(
         content_key = "value" if "from" in conversation[0] else "content"
 
         # NOTE: assume the conversation format is: [System]? (User Assistant)+
-        # convert text message to standand format
-        #  add system as first item, refercence: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/blob/main/chat_template.json
         converted_conversation = []
         if len(conversation) % 2 == 0:
             # Default Prompt
@@ -321,19 +219,11 @@ class TaskEncoder(
                 {"role": "system", "content": "You are a helpful assistant."}
             )
         else:
-            dataset_logger.warning(
-                f"The sample [{sample.__key__}] has odd number of conversation turns, and we will use the first turn as system prompt. BUT this may be wrong. Pelase check the sample."
-            )
             converted_conversation.append(
                 {"role": "system", "content": conversation[0][content_key]}
             )
-            ## NOTE(lizhiyu): Force set system Prompt: "You are a helpful assistant."
-            # converted_conversation.append(
-            #     {"role": "system", "content": "You are a helpful assistant."}
-            # )
             conversation = conversation[1:]
 
-        # add QA conversion as the left items
         EXPECTED_ROLE = ["human", "gpt"]
         for turn_idx, turn in enumerate(conversation):
             role = turn[role_key]
@@ -361,10 +251,9 @@ class TaskEncoder(
         system_prompt_prefix = len(
             self.tokenizer.apply_chat_template([conversation[0]], tokenize=True)
         )
-        assistant_generation_prefix = 3  # <im_start>assistant\n
-        # pad_token_id = self.tokenizer.pad_token_id
-        # NOTE(lizhiyu): Align to llama-f
-        pad_token_id = IGNORE_IDX
+        assistant_generation_prefix = 3
+        pad_token_id = self.tokenizer.pad_token_id
+
         target[:system_prompt_prefix] = pad_token_id
         offset = system_prompt_prefix
         for turn_idx, turn in enumerate(conversation[1:]):
@@ -381,13 +270,11 @@ class TaskEncoder(
             elif turn["role"] == "assistant":
                 target[offset : offset + assistant_generation_prefix] = pad_token_id
             offset += n_tokens
-        # current "target" don't pad vision token.
 
         # NOTE: expand image_pad & video_pad
-        merge_length = self.merge_size**2  # 2**2 = 4
+        merge_length = self.merge_size**2
         image_token_id, video_token_id = self.tokenizer.encode(["<|image_pad|>", "<|video_pad|>"])
 
-        # get the indices of the origin <|image_pad|> and <|video_pad|>
         image_token_indices = np.where(input_ids == image_token_id)[0]
         assert len(image_token_indices) == len(
             image_thw_grids
@@ -400,8 +287,6 @@ class TaskEncoder(
             video_thw_grids, dtype=np.int64
         )
 
-        # video_thw_grids shape: [n, 3]
-        # origin_seq_len + (all_image_token - 1) + (all_vision_token - 1)  ----> -1 because the pad token in origin text
         target_length = (
             input_ids.shape[0]
             - image_thw_grids.shape[0]
@@ -410,17 +295,13 @@ class TaskEncoder(
             + video_thw_grids.prod(axis=-1).sum() // merge_length
         )
         if target_length > self.seq_len:
-            # raise InternalWarning(f"Long sequence with length {target_length} found, dropped...")
-            dataset_logger.warning(
-                f"Samle id [{sample.__key__}] has long sequence with length {target_length}, cutoff to max [self.seq_len+64={self.seq_len}] in batch function..."
-            )
+            raise InternalWarning(f"Long sequence with length {target_length} found, dropped...")
         final_input_ids = np.zeros(target_length, dtype=input_ids.dtype)
         final_input_masks = final_input_ids.copy()
 
         image_idx, video_idx = 0, 0
         indices = np.sort(np.concatenate([image_token_indices, video_token_indices]))
 
-        # cur_x: origin text token idx,  cur_y: final text token idx
         cur_x, cur_y = 0, 0
         for idx in indices:
             token_id = input_ids[idx]
@@ -448,15 +329,11 @@ class TaskEncoder(
         target = np.roll(final_input_masks, shift=-1)
         target[-1] = pad_token_id
 
-        # NOTE(lizhiyu): we also check it in the train scripts.
         if (target == pad_token_id).all():
-            raise InternalWarning(
-                f"Sample id [{sample.__key__}] with all masked label, the data is invalid! Dropped!"
-            )
+            raise InternalWarning("Sample with all masked label, dropped.")
 
         image_input_mask = final_input_ids == self.tokenizer.image_token_id
         video_input_mask = final_input_ids == self.tokenizer.video_token_id
-
         # collect data
         return ImageTaskSample(
             __key__=sample.__key__,
@@ -530,14 +407,14 @@ class TaskEncoder(
         if len(input_ids) > self.seq_len:
             raise InternalWarning(f"Long sequence with length {len(input_ids)} found, dropped...")
 
-        target = np.array(input_ids[1:] + [IGNORE_IDX])
+        target = np.array(input_ids[1:] + [self.tokenizer.pad_token_id])
         if len(user_input_ids) >= len(input_ids):
             raise InternalWarning(f"Sample not supported, dropped...")
         # ensure user inputs is a prefix of full text
         if not (np.array(user_input_ids) == np.array(input_ids[: len(user_input_ids)])).all():
             raise InternalWarning(f"Sample not supported, dropped...")
         # mask input
-        target[: len(user_input_ids) - 1] = IGNORE_IDX
+        target[: len(user_input_ids) - 1] = self.tokenizer.pad_token_id
 
         img_token_id = self.tokenizer.image_token_id
         image_input_mask = np.array(input_ids) == img_token_id
@@ -559,12 +436,7 @@ class TaskEncoder(
 
     def batch(self, samples: List[ImageTaskSample]) -> VQATaskBatch:
         # Stack images to [num_tiles, c, h, w]. If there are no images (text-only), then use a dummy image.
-        # imgs = [img for s in samples for img in s.imgs]
-
-        ####################################################
-        # NOTE(lizhiyu): use the transformers processor
-        imgs = [s.imgs for s in samples if isinstance(s.imgs, np.ndarray) and s.imgs.size > 0]
-        ####################################################
+        imgs = [img for s in samples for img in s.imgs]
         if len(imgs) > 0:
             imgs = torch.cat([torch.from_numpy(img) for img in imgs])
         else:
@@ -581,14 +453,7 @@ class TaskEncoder(
             image_thw_grids = torch.empty([0, 3], dtype=torch.long)
 
         # Stack videos to [num_tiles, c, h, w]. If there are no videos (text-only), then use a dummy video.
-        # videos = [video for s in samples for video in s.videos]
-
-        ####################################################
-        # NOTE(lizhiyu): use the transformers processor
-        videos = [
-            s.videos for s in samples if isinstance(s.videos, np.ndarray) and s.videos.size > 0
-        ]
-        ####################################################
+        videos = [video for s in samples for video in s.videos]
         if len(videos) > 0:
             videos = torch.cat([torch.from_numpy(video) for video in videos])
         else:
@@ -612,30 +477,16 @@ class TaskEncoder(
         else:
             video_thw_grids = torch.empty([0, 3], dtype=torch.long)
 
-        global FIRST_MAX_PADDING_FLAG, MAX_IMG_THRESHHOLD
-        # NOTE(lizhiyu): Clear the cache only when the current image length is longer than the past maxisum length.
-        if image_thw_grids.prod(axis=-1).sum() // 4 > MAX_IMG_THRESHHOLD:
-            MAX_IMG_THRESHHOLD = image_thw_grids.prod(axis=-1).sum() // 4
-            FIRST_MAX_PADDING_FLAG = True
+        # If the user hasn't defined a target sequence length, then use the max along the sample lengths.
+        max_seq_len = self.seq_len
+        if not max_seq_len:
+            max_seq_len = max(len(s.text) for s in samples)
 
-        if not self.args.enable_variable_seq_lengths:
-            max_seq_len = self.seq_len
-        else:
-            # NOTE: this is a hack to get the max padding length for the first batch to avoid OOM because of cached memory in torch
-            if FIRST_MAX_PADDING_FLAG:
-                max_seq_len = self.seq_len
-                FIRST_MAX_PADDING_FLAG = False
-            else:
-                max_seq_len = max(len(s.text) for s in samples)
-                max_seq_len = min(max_seq_len, self.seq_len)
-        # NOTE: we need to make sure the max_seq_len is divisible by tp_size * cp_size
-        if self.cp_size > 1 or self.sequence_parallel:
-            max_seq_len = math.ceil(max_seq_len / (self.tp_size * self.cp_size)) * (
-                self.tp_size * self.cp_size
-            )
         text_mat = np.full((len(samples), max_seq_len), self.tokenizer.pad_token_id, dtype=np.int64)
         # +1 to accommodate shift to left by one later.
-        target_mat = np.full((len(samples), max_seq_len), IGNORE_IDX, dtype=np.int64)
+        target_mat = np.full(
+            (len(samples), max_seq_len), self.tokenizer.pad_token_id, dtype=np.int64
+        )
 
         image_input_masks = np.zeros_like(text_mat, dtype=bool)
         video_input_masks = np.zeros_like(text_mat, dtype=bool)
diff --git a/tools/datasets/qwenvl/data/energon/chatml.py b/tools/datasets/qwenvl/data/energon/chatml.py
index 7c71e91e..643968e4 100644
--- a/tools/datasets/qwenvl/data/energon/chatml.py
+++ b/tools/datasets/qwenvl/data/energon/chatml.py
@@ -9,7 +9,7 @@ from typing import List, Union
 
 import torch
 
-from webdataset.autodecode import Decoder
+from webdataset.autodecode import Decoder, imagehandler
 
 from megatron.energon.epathlib.epath import EPath
 from megatron.energon.flavors.base_dataset import Sample
@@ -20,12 +20,12 @@ from megatron.energon.flavors.webdataset import DefaultDecoderWebdatasetFactory
 class ChatMLSample(Sample):
     """multi-turn complex samples with images and videos"""
 
-    imgs: List[str]
-    videos: List[List[str]]
+    imgs: List[torch.Tensor]
+    videos: List[List[torch.Tensor]]
     conversation: str  # JSON string of GPT-format conversations
 
 
-class NestedImagesPathHandler:
+class NestedImagesHandler:
     def __init__(self, imagespec):
         """Create an image handler.
 
@@ -33,6 +33,7 @@ class NestedImagesPathHandler:
         """
         self.extensions = ["jpgs", "videos"]
         self.extensions_mapping = {"jpgs": "jpg", "videos": "jpg"}
+        self.image_handler = imagehandler(imagespec)
 
     def __call__(self, key, data):
         """Perform nested image decoding.
@@ -44,34 +45,24 @@ class NestedImagesPathHandler:
         if extension.lower() not in self.extensions:
             return None
         data = pickle.loads(data)
+        key = self.extensions_mapping[extension]
+        if extension.lower() == "jpgs":
+            data = [self.image_handler(key, d) for d in data]
+        else:
+            data = [[self.image_handler(key, d) for d in video] for video in data]
         return data
 
 
-# During training, data is automatically decoded to from default webdataset to 'ChatMLSample' when loaded using energon-dataloader,
-# and this is not done during preparation!!!
-# After decoding, the data is passed into the TaskEncoder for further processing.
 class ChatMLWebdataset(DefaultDecoderWebdatasetFactory[ChatMLSample]):
     __sample_type__ = ChatMLSample
 
-    def __init__(
-        self,
-        path: EPath,
-        *,
-        auto_decode: bool = True,
-        image_decode="torchrgb",
-        ignore_decoder_errors: bool = False,
-        av_decode="AVDecoder",
-        video_decode_audio: bool = False,
-        **kwargs,
-    ):
-        super().__init__(
-            path,
-            auto_decode=auto_decode,
-            image_decode=image_decode,
-            ignore_decoder_errors=ignore_decoder_errors,
-            av_decode=av_decode,
-            video_decode_audio=video_decode_audio,
-            **kwargs,
-        )
+    def __init__(self, path: EPath, *, auto_decode: bool = True, **kwargs):
+        super().__init__(path, auto_decode=auto_decode, **kwargs)
         if auto_decode:
-            self._decoder = Decoder([NestedImagesPathHandler(self.image_decode)])
+            self._decoder = Decoder(
+                [
+                    imagehandler(self.image_decode),
+                    NestedImagesHandler(self.image_decode),
+                    self._video_decoder,
+                ]
+            )
diff --git a/tools/datasets/qwenvl/data/image_processing.py b/tools/datasets/qwenvl/data/image_processing.py
index 822b3fc3..2b446613 100644
--- a/tools/datasets/qwenvl/data/image_processing.py
+++ b/tools/datasets/qwenvl/data/image_processing.py
@@ -11,24 +11,17 @@ from PIL import Image, ImageDraw
 from torchvision import transforms as T
 from torchvision.transforms import Compose, RandAugment, RandomResizedCrop, Resize, ToPILImage
 
-# config :https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/blob/main/preprocessor_config.json
 # Imagenet's mean and std.
-pixel_mean = [0.48145466, 0.4578275, 0.40821073]
-pixel_std = [0.26862954, 0.26130258, 0.27577711]
+pixel_mean = [123.675, 116.28, 103.53]
+pixel_std = [58.395, 57.12, 57.375]
 
 # Reshape for broadcasting.
 pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)
 pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)
 
 
-# https://github.com/QwenLM/Qwen2.5-VL/blob/477fd9d4317266508705366ce36cac5b68d70936/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L89C1-L95C40
-def convert_to_rgb(pil_image: Image.Image) -> Image.Image:
-    if pil_image.mode == 'RGBA':
-        white_background = Image.new("RGB", pil_image.size, (255, 255, 255))
-        white_background.paste(pil_image, mask=pil_image.split()[3])  # Use alpha channel as mask
-        return white_background
-    else:
-        return pil_image.convert("RGB")
+def convert_to_rgb(image):
+    return image.convert("RGB")
 
 
 def _transform_train_aug():
@@ -64,20 +57,24 @@ def _transform_test():
 
 def standardize_image(img):
     """Standardize image pixel values."""
-    return (T.ToTensor()(img) - pixel_mean) / pixel_std
+    return (torch.Tensor(np.array(img)).permute(2, 0, 1) - pixel_mean) / pixel_std
 
 
 def get_visual_transform(
-    img,  # Path
+    img,
     factor: int = 28,
-    min_pixels: int = 4 * 28 * 28,
-    max_pixels: int = 16384 * 28 * 28,
+    min_pixels: int = 56 * 56,
+    max_pixels: int = 14 * 14 * 4 * 1280,
     augment=False,
 ):
-    # TODO(lizhiyu): Need to limit the aspect ratio of the image.
-    # (reference https://github.com/QwenLM/Qwen2.5-VL/blob/477fd9d4317266508705366ce36cac5b68d70936/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L72)
-    img = Image.open(img)
-    img = convert_to_rgb(img)
+    img = np.array(img)
+
+    if augment:
+        visual_transform = _transform_train_aug()
+    else:
+        visual_transform = _transform_test()
+
+    img = visual_transform(img)
     w, h = img.size
     h_bar, w_bar = smart_resize(h, w, factor, min_pixels, max_pixels)
     img = img.resize((w_bar, h_bar))
@@ -93,8 +90,8 @@ def smart_resize(
     height: int,
     width: int,
     factor: int = 28,
-    min_pixels: int = 4 * 28 * 28,
-    max_pixels: int = 16384 * 28 * 28,
+    min_pixels: int = 56 * 56,
+    max_pixels: int = 14 * 14 * 4 * 1280,
 ):
     """Rescales the image so that the following conditions are met:
 
diff --git a/tools/datasets/qwenvl/dataset_preparation.md b/tools/datasets/qwenvl/dataset_preparation.md
index 1bd24a84..14d517f0 100644
--- a/tools/datasets/qwenvl/dataset_preparation.md
+++ b/tools/datasets/qwenvl/dataset_preparation.md
@@ -1,7 +1,7 @@
 # 📎 Reference
 Mainly based on [Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch/tree/main/toolkits/multimodal_data_preprocessing/),with necessary modifications for integration into the current training framework.
 
-# Dataset Download & Preprocessing
+# Dataset Download
 
 ```bash
 cd /mnt
@@ -13,25 +13,30 @@ cd LLaVA-Pretrain
 unzip images.zip
 
 #convert to webdataset format:
-cd ./tools/datasets/qwenvl/
-
-
-export PYTHONPATH=$PYTHONPATH:../../../../third_party/Megatron-LM/
-
-python convert_custom_dataset_to_wds_chatml_str.py \
-    --dataset-root=/mnt/LLaVA-Pretrain \
-    --output-root=/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/ \
-    --json=blip_laion_cc_sbu_558k.json \
-    --train-split 1 \
-    --val-split 0 \
-    --images-key=image \
-    --videos-key=video \
-    --vision-root=/mnt/LLaVA-Pretrain \
-    --max-samples-per-tar 100000000 \
-    --dp-size 1 \
-    --num-workers 20
+cd /workspace/tools/datasets/qwenvl/
+python convert_llava_pretrain_to_wds.py /mnt/llava-datasets/LLaVA-Pretrain/
+
+#convert to megatron-energon format:
+cd /mnt/llava-datasets/LLaVA-Pretrain/wds
+energon prepare ./
+
+#select the following values for the presented options:
+> Please enter a desired train/val/test split like "0.5, 0.2, 0.3" or "8,1,1": 9,1,0
+> Do you want to create a dataset.yaml interactively? [Y/n]: Y
+> Please enter a number to choose a class: 10 (VQAWebdataset)
+> Do you want to set a simple field_map[Y] (or write your own sample_loader [n])? [Y/n]: Y
+> Please enter a webdataset field name for 'image' (<class 'torch.Tensor'>): jpg
+> Please enter a webdataset field name for 'context' (<class 'str'>): json[0][value]
+> Please enter a webdataset field name for 'answers' (typing.Optional[typing.List[str]], default: None): json[1][value]
+> Please enter a webdataset field name for 'answer_weights' (typing.Optional[torch.Tensor], default: None):
+```
+
+You can also directly get the preprocessed data:
+```bash
+cd /mnt/llava-datasets/LLaVA-Pretrain/
+wget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/vlm-datasets/wds.tgz
+tar -zxf wds.tgz
 ```
-The preprocessed datas will stored at the output-root path `/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1`.
 
 ## Prepare Multimodal Datasets Based on ShareGPT Format
 
