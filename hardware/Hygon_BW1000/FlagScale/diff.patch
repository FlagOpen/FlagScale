diff --git a/examples/deepseek_r1/conf/hostfile.txt b/examples/deepseek_r1/conf/hostfile.txt
index 0d8b1e05..9b9adeca 100644
--- a/examples/deepseek_r1/conf/hostfile.txt
+++ b/examples/deepseek_r1/conf/hostfile.txt
@@ -3,3 +3,5 @@
 x.x.x.x slots=8 type=gpu
 # worker nodes
 x.x.x.x slots=8 type=gpu
+x.x.x.x slots=8 type=gpu
+x.x.x.x slots=8 type=gpu
diff --git a/examples/deepseek_r1/conf/serve.yaml b/examples/deepseek_r1/conf/serve.yaml
index f63820f4..8c70724f 100644
--- a/examples/deepseek_r1/conf/serve.yaml
+++ b/examples/deepseek_r1/conf/serve.yaml
@@ -9,13 +9,14 @@ experiment:
   deploy:
     use_fs_serve: false
   runner:
-    hostfile: examples/deepseek_r1/conf/hostfile.txt
-    docker: flagrelease_nv
+    hostfile: /share/project/PUBLIC/suma_test/ghw_test/FlagScale/examples/deepseek_r1/conf/hostfile.txt
+    docker: flagscale_vllm
     ssh_port: 22
   envs:
-    CUDA_DEVICE_MAX_CONNECTIONS: 1
+    CUDA_DEVICE_MAX_CONNECTIONS: 8
   cmds:
-    before_start: source /root/miniconda3/bin/activate flagscale-inference && export GLOO_SOCKET_IFNAME=bond0 # replace "bond0" with your own network card
+    before_start: source /etc/env.sh
+
 action: run
 hydra:
   run:
diff --git a/examples/deepseek_r1/conf/serve/671b.yaml b/examples/deepseek_r1/conf/serve/671b.yaml
index 719a6726..de915317 100644
--- a/examples/deepseek_r1/conf/serve/671b.yaml
+++ b/examples/deepseek_r1/conf/serve/671b.yaml
@@ -1,12 +1,15 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /models/deepseek_r1 # path of weight of deepseek r1
-    tensor_parallel_size: 8
-    pipeline_parallel_size: 4
-    gpu_memory_utilization: 0.9
-    max_model_len: 32768
-    max_num_seqs: 256
-    enforce_eager: true
-    trust_remote_code: true
-    enable_chunked_prefill: true
+    model: /deepseek_r1_BF16              # 模型路径，对应命令中的路径
+    distributed-executor-backend: ray
+    tensor_parallel_size: 32              # -tp 32
+    gpu_memory_utilization: 0.93           # --gpu-memory-utilization 0.93
+    dtype: bfloat16                       # --dtype bfloat16
+    max_model_len: 74000                  # --max-model-len 74000
+    max-seq-len-to-capture: 74000
+    max_num_seqs: 128                    # --max-num-seqs 128
+    enforce_eager: true                   # --enforce-eager
+    trust_remote_code: true               # --trust-remote-code
+    speculative_config: '{"num_speculative_tokens": 3}'          # --speculative_config '{"num_speculative_tokens": 3}'
+    block_size: 64                       # --block-size 64
