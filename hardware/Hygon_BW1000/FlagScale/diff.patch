diff --git a/examples/deepseek_v3/conf/train.yaml b/examples/deepseek_v3/conf/train.yaml
index e030db9e..2101e491 100644
--- a/examples/deepseek_v3/conf/train.yaml
+++ b/examples/deepseek_v3/conf/train.yaml
@@ -4,30 +4,53 @@ defaults:
   - train: 16b_a3b
 
 experiment:
-  exp_name: DeepSeek-16b-a3b
+  exp_name: train_deepseek_v3_16b_a3b
   seed: 42
-  save_steps: 10000
-  load: null
-  exp_dir: xxx
-  ckpt_format: torch
+  save_steps: 1000
+  exp_dir: ./${experiment.exp_name}
   task:
     type: train
     backend: megatron
     entrypoint: flagscale/train/train_gpt.py
   runner:
-    per_node_task: false
-    no_shared_fs: false
+    backend: torchrun
+    nnodes: 4
+    nproc_per_node: 8
+    hostfile: /share/project/hostfile
+    master_port: xxxxx
+    ssh_port: xxxxxx
+    master_addr: xx.x.xx.xxx
     rdzv_backend: static
-    hostfile: null
-  cmds:
-    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
   envs:
-    LOGLEVEL: "INFO"
-    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
+    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
     CUDA_DEVICE_MAX_CONNECTIONS: 1
+    NVTE_APPLY_QK_LAYER_SCALING: 0
+    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
+    NCCL_NET_GDR_LEVEL: 7
+    NCCL_NET_GDR_READ: 1
+    NCCL_MAX_NCHANNELS: 32
+    NCCL_MIN_NCHANNELS: 32
+    RCCL_SDMA_COPY_ENABLE: 0
+    NCCL_ALGO: Ring
+    HSA_FORCE_FINE_GRAIN_PCIE: 1
+    OMP_NUM_THREADS: 1
+    NCCL_SOCKET_IFNAME: enp225s0f0np0
+    GLOO_SOCKET_IFNAME: enp225s0f0np0
+    NCCL_IB_HCA: mlx5_0:1,mlx5_6:1
+    ALLREDUCE_STREAM_WITH_COMPUTE: 1
+    SENDRECV_STREAM_WITH_COMPUTE: 1
+    cache_size_limit: 64
+    GPU_MAX_HW_QUEUES: 20
+  cmds:
+    before_start: |-
+      source /opt/dtk/env.sh
+      export LD_LIBRARY_PATH=/root/xxx01/lib:$LD_LIBRARY_PATH
+      export LD_LIBRARY_PATH=/root/xxx02/lib:$LD_LIBRARY_PATH
+    #before_start: "ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train"
 
 action: run
 
 hydra:
   run:
     dir: ${experiment.exp_dir}/hydra
+
diff --git a/examples/deepseek_v3/conf/train/16b_a3b.yaml b/examples/deepseek_v3/conf/train/16b_a3b.yaml
index 532d9621..b4e9d5a7 100644
--- a/examples/deepseek_v3/conf/train/16b_a3b.yaml
+++ b/examples/deepseek_v3/conf/train/16b_a3b.yaml
@@ -1,15 +1,20 @@
 system:
-  no_shared_fs: ${experiment.runner.no_shared_fs}
-  num_workers: 2
+  num_workers: 16
+  deterministic_mode: true
   tensor_model_parallel_size: 1
   pipeline_model_parallel_size: 2
   decoder_first_pipeline_num_layers: 13
-  expert_model_parallel_size: 4
+  expert_model_parallel_size: 2
   context_parallel_size: 1
+  disable_bias_linear: true
+  reset_position_ids: True
+  reset_attention_mask: True
+  qk_layernorm: true
   sequence_parallel: true
   use_distributed_optimizer: true
   overlap_grad_reduce: true
   overlap_param_gather: true
+  finetune: true
   precision:
     bf16: true
     attention_softmax_in_fp32: true
@@ -26,9 +31,9 @@ system:
     log_num_zeros_in_grad: true
     log_memory_to_tensorboard: true
   checkpoint:
-    save_interval: ${experiment.save_steps}
-    load: ${experiment.load}
-    ckpt_format: ${experiment.ckpt_format}
+    save_interval: 1500
+    load: /share/projset_public/perf_logs/XLC_2025_openseek/flagscale_ckpt_tp1_pp2_ep2
+    ckpt_format: torch
 
 model:
   transformer_impl: transformer_engine
@@ -43,15 +48,14 @@ model:
   rotary_base: 1000000
   swiglu: true
   normalization: RMSNorm
-  qk_layernorm: true
   init_method_std: 0.02
   attention_dropout: 0.0
   hidden_dropout: 0.0
+  clip_grad: 1.0
   position_embedding_type: rope
   untie_embeddings_and_output_weights: true
   no_position_embedding: true
   no_rope_fusion: true
-  disable_bias_linear: true
 
   # mla args ==================
   multi_latent_attention: true
@@ -72,46 +76,41 @@ model:
   moe_router_bias_update_rate: 0.001
   moe_aux_loss_coeff: 0.02
   moe_layer_freq: "[0]+[1]*26"
-  # node limited routing
   moe_router_num_groups: 1
   moe_router_group_topk: 1
   moe_router_topk: 6
   moe_router_topk_scaling_factor: 2.446
   moe_token_dispatcher_type: "alltoall"
 
-  # mtp args ====================
-  mtp_num_layers: 1
-  mtp_loss_scaling_factor: 0.3
-
   # training
   seed: ${experiment.seed}
-  finetune: false
+  # finetune: false
   micro_batch_size: 1
-  global_batch_size: 128 #2048
+  global_batch_size: 256
   eval_iters: 0
-  train_iters: 102400
+  train_samples: 768000 
 
   optimizer:
-    clip_grad: 1.0
     weight_decay: 0.1
     adam_beta1: 0.9
     adam_beta2: 0.95
     lr_scheduler:
       lr: 3.0e-3
       min_lr: 3.0e-4
-      lr_warmup_fraction: 0.01
+      lr_warmup_samples: 76800
       lr_decay_style: WSD
       lr_wsd_decay_style: cosine
-      lr_wsd_decay_iters: 10
+      lr_wsd_decay_samples: 768
+
 
 data:
-  reset_position_ids: True
-  reset_attention_mask: True
-  data_path: /path
+  data_path: /share/project/DNOT_MOVE/XLC_2025_openseek/dataset/cosmopedia-v2-full_text_document
   split: 1
   no_mmap_bin_files: true
   tokenizer:
     tokenizer_type: QwenTokenizerFS
-    tokenizer_path: examples/aquila/qwentokenizer
+    tokenizer_path: /share/projset_public/perf_logs/XLC_2025_openseek/hf_ckpt
     vocab_size: 151851
     make_vocab_size_divisible_by: 64
+
+
diff --git a/examples/llama3/conf/train.yaml b/examples/llama3/conf/train.yaml
index 8b3acdfd..47ef91d3 100644
--- a/examples/llama3/conf/train.yaml
+++ b/examples/llama3/conf/train.yaml
@@ -1,24 +1,50 @@
 defaults:
-  - train: 70b
   - _self_
+  - train: 70b_finetune
 
 experiment:
-  exp_name: llama3
-  exp_dir: ./outputs_llama3_70b
+  exp_name: train_llama3_70b
+  seed: 42
+  save_steps: 1000
+  exp_dir: ./${experiment.exp_name}
   task:
     type: train
     backend: megatron
-    entrypoint: ./flagscale/train/train_gpt.py
+    entrypoint: flagscale/train/train_gpt.py
   runner:
     backend: torchrun
     nnodes: 4
     nproc_per_node: 8
-    hostfile: ${hostfile??}
+    hostfile: /share/project/hostfile
+    master_port: xxxx
+    ssh_port: xxxx
+    master_addr: xx.x.xx.xx
+    rdzv_backend: static
   envs:
     CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
     CUDA_DEVICE_MAX_CONNECTIONS: 1
     NVTE_APPLY_QK_LAYER_SCALING: 0
     NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
+    NCCL_NET_GDR_LEVEL: 7
+    NCCL_NET_GDR_READ: 1
+    NCCL_MAX_NCHANNELS: 32
+    NCCL_MIN_NCHANNELS: 32
+    RCCL_SDMA_COPY_ENABLE: 0
+    NCCL_ALGO: Ring
+    HSA_FORCE_FINE_GRAIN_PCIE: 1
+    OMP_NUM_THREADS: 1
+    NCCL_SOCKET_IFNAME: enp225s0f0np0
+    NCCL_IB_HCA: mlx5_0:1,mlx5_6:1
+    ALLREDUCE_STREAM_WITH_COMPUTE: 1
+    SENDRECV_STREAM_WITH_COMPUTE: 1
+    cache_size_limit: 64
+    GPU_MAX_HW_QUEUES: 20
+  cmds:
+    before_start: |-
+      source /opt/dtk/env.sh
+      export LD_LIBRARY_PATH=/xxx/xx01/lib:$LD_LIBRARY_PATH
+      export LD_LIBRARY_PATH=/xxx/xx02/lib:$LD_LIBRARY_PATH
+
 action: run
 
 hydra:
diff --git a/examples/llama3/conf/train/70b_finetune.yaml b/examples/llama3/conf/train/70b_finetune.yaml
index aca7f56b..42da9d88 100644
--- a/examples/llama3/conf/train/70b_finetune.yaml
+++ b/examples/llama3/conf/train/70b_finetune.yaml
@@ -18,9 +18,9 @@ system:
     wandb_project: "train-llama3-70B"
     wandb_exp_name: "train-llama3-70B"
   checkpoint:
-    load: ${ckpt_path:??}
+    load: /share/projset_public/perf_logs/XLC_2025_llama3/flagscale_ckpt_tp8pp4
     ckpt_format: torch
-    save_interval: 100
+    save_interval: 250
     finetune: True
 
 model:
@@ -30,7 +30,7 @@ model:
   group_query_attention: True
   num_query_groups: 8
   ffn_hidden_size: 28672
-  seq_length: 8192
+  seq_length: 4096
   max_position_embeddings: 8192
   norm_epsilon: 1e-5
   norm_init_weight: 0.02
@@ -48,10 +48,11 @@ model:
   hidden_dropout: 0.0
   clip_grad: 1.0
 
-  train_samples: 6160066
+  train_samples: 64000
   micro_batch_size: 1
-  global_batch_size: 1024
+  global_batch_size: 128
   seed: 42
+  log_throughput: True
 
   optimizer:
     start_weight_decay: 0
@@ -62,13 +63,13 @@ model:
     lr_scheduler:
       lr: 5e-6
       min_lr: 0
-      lr_warmup_samples: 2048000
+      lr_warmup_samples: 6400
       lr_decay_style: cosine
 
 data:
-  data_path: ${data_path:??}
+  data_path: /share/project/DNOT_MOVE/dataset/dedup-md5-pile-pile-cc_text_document
   split: 1
   tokenizer:
     tokenizer_type: Llama3TokenizerFS
-    tokenizer_path: ${tokenizer_path:??}
+    tokenizer_path:  /share/project/DNOT_MOVE/tokenizer
     vocab_size: 128256
diff --git a/examples/qwen2_5_vl/conf/train.yaml b/examples/qwen2_5_vl/conf/train.yaml
index 7159e1b1..bab349c8 100644
--- a/examples/qwen2_5_vl/conf/train.yaml
+++ b/examples/qwen2_5_vl/conf/train.yaml
@@ -11,18 +11,38 @@ experiment:
     entrypoint: ./flagscale/train/train_qwen2_5_vl.py
   runner:
     backend: torchrun
-    nnodes: 1
+    nnodes: 4
     nproc_per_node: 8
+    hostfile: /share/project/hostfile
+    master_port: xxxxx
+    ssh_port: xxxx
+    master_addr: xx.x.xx.xxx
     rdzv_backend: static
-  cmds:  
-    before_start: ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale-train
+  cmds:
+    before_start: |-
+      source /opt/dtk/env.sh
+      export LD_LIBRARY_PATH=/xxx/xx01/lib:$LD_LIBRARY_PATH
+      export LD_LIBRARY_PATH=/xxx/xx02/lib:$LD_LIBRARY_PATH
   envs:
-    # NCCL_DEBUG: INFO
-    # NCCL_DEBUG_SUBSYSTEM: ALL
     CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
     CUDA_DEVICE_MAX_CONNECTIONS: 1
     NVTE_APPLY_QK_LAYER_SCALING: 0
     NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
+    NCCL_NET_GDR_LEVEL: 7
+    NCCL_NET_GDR_READ: 1
+    NCCL_MAX_NCHANNELS: 32
+    NCCL_MIN_NCHANNELS: 32
+    RCCL_SDMA_COPY_ENABLE: 0
+    NCCL_ALGO: Ring
+    HSA_FORCE_FINE_GRAIN_PCIE: 1
+    OMP_NUM_THREADS: 1
+    NCCL_SOCKET_IFNAME: enp225s0f0np0
+    GLOO_SOCKET_IFNAME: enp225s0f0np0
+    NCCL_IB_HCA: mlx5_0:1,mlx5_6:1
+    ALLREDUCE_STREAM_WITH_COMPUTE: 1
+    SENDRECV_STREAM_WITH_COMPUTE: 1
+    cache_size_limit: 64
+    GPU_MAX_HW_QUEUES: 20
 
 action: run
 
diff --git a/examples/qwen2_5_vl/conf/train/7b.yaml b/examples/qwen2_5_vl/conf/train/7b.yaml
index 62b2e072..5d0a7710 100644
--- a/examples/qwen2_5_vl/conf/train/7b.yaml
+++ b/examples/qwen2_5_vl/conf/train/7b.yaml
@@ -1,22 +1,20 @@
 system:
-  num_workers: 1
-  calculate_per_token_loss: true
   tensor_model_parallel_size: 2
   pipeline_model_parallel_size: 1
   context_parallel_size: 1
-  # decoder_first_pipeline_num_layers: 12
+  # decoder_first_pipeline_num_layers: 10
   disable_bias_linear: True
   use_flash_attn: True
   use_distributed_optimizer: True
   sequence_parallel: True
   tp_comm_overlap: False
-  overlap_grad_reduce: False # if has text-only must be false
-  overlap_param_gather: False # if has text-only must be false
+  overlap_grad_reduce: True
+  overlap_param_gather: True
   use_mcore_models: True
   transformer_impl: transformer_engine
-  recompute_method: "uniform"
-  recompute_granularity: "full"
-  recompute_num_layers: 1
+  # recompute_method: "uniform"
+  # recompute_granularity: "full"
+  # recompute_num_layers: 1
   use_te: True
   precision:
     bf16: True
@@ -30,15 +28,14 @@ system:
     log_params_norm: True
     log_num_zeros_in_grad: True
   checkpoint:
-    save_interval: 1000
-    pretrained_checkpoint: xxxx
+    save_interval: 2500
+    pretrained_checkpoint: /share/projset_public/perf_logs/XLC_2025_qwen2.5_vl/flagscale_ckpt_tp2pp1
     dataloader_save: ${experiment.exp_dir}/checkpoints/dataloader
     use_dist_ckpt: False
     ckpt_format: torch
     async_save: False
 
 model:
-  attention_backend: flash # don't use "auto(nvte_flash_attn)"
   disable_bias_linear: True
   add_qkv_bias: True
   num_layers: 28
@@ -46,22 +43,20 @@ model:
   ffn_hidden_size: 18944
   num_attention_heads: 28
   num_query_groups: 4
-  seq_length: 2048
-  max_padding_length: 2048 # (cutoff_len)max 32768, change according the dataset
-  # especial for qwen2.5-vl
-  enable_variable_seq_lengths: True
+  seq_length: 4096 # origin LLM 32768
+  max_padding_length: 4096 # real seq_length
   max_position_embeddings: 128000 # only useful for additional position embedding
   swiglu: True
   normalization: RMSNorm
   norm_epsilon: 1e-6
-  init_method_std: 0.02
+  init_method_std: 0.014
   attention_dropout: 0.0
   hidden_dropout: 0.0
   clip_grad: 1.0
-  train_iters: 62
-  eval_iters: 0 # no valid
-  micro_batch_size: 1
-  global_batch_size: 16
+  train_iters: 5000
+  eval_iters: 0
+  micro_batch_size: 2
+  global_batch_size: 256
   allow_missing_vision_projection_checkpoint: False
   apply_layernorm_1p: False
   group_query_attention: True
@@ -84,24 +79,22 @@ model:
   seed: 42
 
   optimizer:
-    weight_decay: 0.1
+    weight_decay: 0.0
     adam_beta1: 0.9
-    adam_beta2: 0.999
+    adam_beta2: 0.95
     lr_scheduler:
       lr: 1.0e-5
       min_lr: 1.0e-6
-      # lr_warmup_fraction: .03
-      lr_warmup_iters: 10
+      lr_warmup_fraction: .03
       lr_decay_style: cosine
 
 data:
-  data_path: xxxx
-  vision_root: xxxx
+  data_path: /share/project/DNOT_MOVE/XLC_2025_qwen2.5_vl/dataset
   dataloader_type: external
   split: 100,0,0
   tokenizer:
     tokenizer_type: Qwen2VLTokenizer
-    tokenizer_path: xxxx
+    tokenizer_path: /share/project/DNOT_MOVE/XLC_2025_qwen2.5_vl/flagscale_ckpt_tp2pp1
     vocab_size: 152064 # 7b
     extra_vocab_size: 421
     make_vocab_size_divisible_by: 64
diff --git a/flagscale/train/models/qwen2_5_vl/QuickStart.md b/flagscale/train/models/qwen2_5_vl/QuickStart.md
deleted file mode 100644
index 8c2fa82d..00000000
--- a/flagscale/train/models/qwen2_5_vl/QuickStart.md
+++ /dev/null
@@ -1,111 +0,0 @@
-
-# 1. Install the FlagScale
-
-## 1.1. Downlowd the source code 
-
-```bash
-git clone https://github.com/FlagOpen/FlagScale.git
-cd FlagScale
-```
-
-## 1.2. Apply the submodule patch code
-
-```bash
-python ./tools/patch/unpatch.py --backend=Megatron-LM
-python ./tools/patch/unpatch.py --backend=Megatron-Energon
-cd ./third_party/Megatron-Energon/
-pip install -e .
-cp -r src/megatron/energon/ ../Megatron-LM/megatron/
-```
-
-You can also refered the readme in `https://github.com/FlagOpen/FlagScale.git`
-# 2. Prepare checkpoint
-
-Reference [convert.md](../../../../tools/checkpoint/qwen2_5_vl/convert.md)
-```bash
-mkdir -p /mnt/qwen2.5-vl-ckpts
-cd /mnt/qwen2.5-vl-ckpts
-git clone https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
-cd Qwen2.5-VL-7B-Instruct
-git lfs pull
-
-cd ./tools/checkpoint/qwen2_5_vl/
-bash hf2mcore_qwen2.5_vl_convertor.sh 7B \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-tp2 \
-2 1 false bf16  \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct
-```
-
-# 3. Preprocess dataset
-
-Reference [dataset_preparation.md](../../../../tools/datasets/qwenvl/dataset_preparation.md)
-
-```bash
-cd /mnt # custom your path
-
-mkdir llava-datasets
-cd llava-datasets
-git clone https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain
-cd LLaVA-Pretrain
-unzip images.zip
-
-#convert to webdataset format
-cd ./tools/datasets/qwenvl/
-export PYTHONPATH=$PYTHONPATH:../../../../third_party/Megatron-LM/
-
-python convert_custom_dataset_to_wds_chatml_str.py \
-    --dataset-root=/mnt/LLaVA-Pretrain \
-    --output-root=/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/ \
-    --json=blip_laion_cc_sbu_558k.json \
-    --train-split 1 \
-    --val-split 0 \
-    --images-key=image \
-    --videos-key=video \
-    --vision-root=/mnt/LLaVA-Pretrain \
-    --dp-size 1 \
-    --num-workers 20
-```
-The preprocessed dataset will be stored at the output-root path `/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1`.
-The configuration of `data-path` is `/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1` and the configuration of `vision-path` is `/mnt/LLaVA-Pretrain` in the step 4.
-
-# 4. Add your configuration
-
-Add the data path and checkpoint path in ./examples/qwen2_5_vl/conf/train/7b.yaml as shown below:
-
-```bash
-# dataset
-data_path: /mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1
-vision_root: /mnt/LLaVA-Pretrain
-
-# ckpt
-pretrained_checkpoint: /mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-tp2
-tokenizer_path: /mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-tp2
-```
-
-Start training.
-```bash
-python run.py --config-path ./examples/qwen2_5_vl/conf  --config-name train action=run
-```
-
-Stop training.
-```bash
-python run.py --config-path ./examples/qwen2_5_vl/conf  --config-name train action=stop
-```
-
-# 5. Convert the checkpoint to HuggingFace
-
-Reference [convert.md](../../../../tools/checkpoint/qwen2_5_vl/convert.md)
-
-``` bash
-cd ./tools/checkpoint/qwen2_5_vl/
-bash hf2mcore_qwen2.5_vl_convertor.sh 7B \
-./train_qwen2_5_vl_7b/checkpoints \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-fs2hf-tp2 \
-2 1 true bf16  \
-/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct
-```
-The converved checkpoint is stored in `/mnt/qwen2.5-vl-ckpts/Qwen2.5-VL-7B-Instruct-fs2hf-tp2`
-
-# PS
-The path `./` represents the path of `FlagScale` that you download.
\ No newline at end of file
diff --git a/flagscale/train/models/qwen2_5_vl/language_module.py b/flagscale/train/models/qwen2_5_vl/language_module.py
index 81768fdd..a5e266d7 100644
--- a/flagscale/train/models/qwen2_5_vl/language_module.py
+++ b/flagscale/train/models/qwen2_5_vl/language_module.py
@@ -1,175 +1,175 @@
-# Copyright (c) 2025, BAAI. All rights reserved.
-#
-# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_vl/gpt_model.py. Below is the original copyright:
-#   Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved.
-
-from typing import Literal, Optional
-from torch import Tensor
-
-from megatron.core.transformer.spec_utils import ModuleSpec
-from megatron.core.models.gpt.gpt_model import GPTModel
-from megatron.core import tensor_parallel
-from megatron.core.transformer.transformer_config import TransformerConfig
-from megatron.core.models.common.embeddings.language_model_embedding import LanguageModelEmbedding
-
-
-class QwenVLLanguageModelEmbedding(LanguageModelEmbedding):
-    """Language model embeddings. Used for Qwen2.5-VL, inserting the image and video hidden states.
-
-    Args:
-        config (TransformerConfig): config object with all necessary configs for TransformerBlock
-        vocab_size (int): vocabulary size
-        max_sequence_length (int): maximum size of sequence. This
-                             is used for positional embedding
-        add_position_embedding (bool): Add a position embedding.
-        embedding_dropout_prob (float): dropout probability for embeddings
-        num_tokentypes (int): Set to 0 without binary head, and 2 with a binary head. Defaults to 0.
-        scatter_to_sequence_parallel (bool): Set to False to disable scatter of embedding
-            across sequence parallel region. Defaults to True.
-    """
-
-    def __init__(
-        self,
-        config: TransformerConfig,
-        vocab_size: int,
-        max_sequence_length: int,
-        position_embedding_type: Literal['learned_absolute', 'rope', 'none'] = 'learned_absolute',
-        num_tokentypes: int = 0,
-        scatter_to_sequence_parallel: bool = False, # chage default to False
-    ):
-        assert scatter_to_sequence_parallel == False, "QwenVLLanguageModelEmbedding does not support scatter_to_sequence_parallel"
-        super().__init__(config, vocab_size, max_sequence_length, position_embedding_type, num_tokentypes, scatter_to_sequence_parallel)
-
-
-    def forward(
-        self,
-        input_ids: Tensor,
-        position_ids: Tensor,
-        tokentype_ids: int = None,
-        image_input_mask: Tensor = None,
-        video_input_mask: Tensor = None,
-        image_embeds: Tensor = None,
-        video_embeds: Tensor = None
-    ) -> Tensor:
-        """Forward pass of the embedding module.
-
-        Args:
-            input_ids (Tensor): The input tokens
-            position_ids (Tensor): The position id's used to calculate position embeddings
-            tokentype_ids (int): The token type ids. Used when args.bert_binary_head is set to True. Defaults to None
-
-        Returns:
-            Tensor: The output embeddings
-        """
-        word_embeddings = self.word_embeddings(input_ids)
-        if self.add_position_embedding:
-            position_embeddings = self.position_embeddings(position_ids)
-            embeddings = word_embeddings + position_embeddings
-        else:
-            embeddings = word_embeddings
-
-        if not self.reduce_scatter_embeddings:
-            # Data format change to avoid explicit tranposes : [b s h] --> [s b h].
-            embeddings = embeddings.transpose(0, 1).contiguous()
-
-        if tokentype_ids is not None:
-            assert self.tokentype_embeddings is not None
-            # [b s h] -> [s b h] (So that it can be added with embeddings)
-            tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)
-            embeddings = embeddings + tokentype_embedding
-        else:
-            assert self.tokentype_embeddings is None
-
-        # If the input flag for fp32 residual connection is set, convert for float.
-        if self.config.fp32_residual_connection:
-            embeddings = embeddings.float()
-
-        # Dropout.
-        if self.config.sequence_parallel:
-            if not self.reduce_scatter_embeddings:
-                embeddings = embeddings.clone()
-                if image_embeds is not None:
-                    embeddings[image_input_mask] = image_embeds.to(embeddings.device, embeddings.dtype)
-                if video_embeds is not None:
-                    embeddings[video_input_mask] = video_embeds.to(embeddings.device, embeddings.dtype)
-                embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)
-            # `scatter_to_sequence_parallel_region` returns a view, which prevents
-            # the original tensor from being garbage collected. Clone to facilitate GC.
-            # Has a small runtime cost (~0.5%).
-            if self.config.clone_scatter_output_in_embedding:
-                embeddings = embeddings.clone()
-            with tensor_parallel.get_cuda_rng_tracker().fork():
-                embeddings = self.embedding_dropout(embeddings)
-        else:
-            embeddings = embeddings.clone()
-            if image_embeds is not None:
-                embeddings[image_input_mask] = image_embeds.to(embeddings.device, embeddings.dtype)
-            if video_embeds is not None:
-                embeddings[video_input_mask] = video_embeds.to(embeddings.device, embeddings.dtype)
-            embeddings = self.embedding_dropout(embeddings)
-
-        return embeddings
-
-
-class QwenVLLanguageModel(GPTModel):
-    """GPT Transformer language model, replace language embedding using QwenVLLanguageModelEmbedding.
-
-    Args:
-        config (TransformerConfig): Transformer config
-        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers
-        vocab_size (int): Vocabulary size
-        max_sequence_length (int): maximum size of sequence. This is used for positional embedding
-        pre_process (bool, optional): Include embedding layer (used with pipeline parallelism). Defaults to True.
-        post_process (bool, optional): Include an output layer (used with pipeline parallelism). Defaults to True.
-        fp16_lm_cross_entropy (bool, optional): Defaults to False.
-        parallel_output (bool, optional): Do not gather the outputs, keep them split across tensor parallel ranks. Defaults to True.
-        share_embeddings_and_output_weights (bool, optional): When True, input embeddings and output logit weights are shared. Defaults to False.
-        position_embedding_type (Literal[learned_absolute,rope], optional):  Position embedding type.. Defaults to 'learned_absolute'.
-        rotary_percent (float, optional): Percent of rotary dimension to use for rotary position embeddings. Ignored unless position_embedding_type is 'rope'. Defaults to 1.0.
-        rotary_base (int, optional): Base period for rotary position embeddings. Ignored unless position_embedding_type is 'rope'. Defaults to 10000.
-        seq_len_interpolation_factor (Optional[float], optional): scale of linearly interpolating RoPE for longer sequences. The value must be a float larger than 1.0. Defaults to None.
-    """
-
-    def __init__(
-        self,
-        config: TransformerConfig,
-        transformer_layer_spec: ModuleSpec,
-        vocab_size: int,
-        max_sequence_length: int,
-        pre_process: bool = True,
-        post_process: bool = True,
-        fp16_lm_cross_entropy: bool = False,
-        parallel_output: bool = True,
-        share_embeddings_and_output_weights: bool = False,
-        position_embedding_type: Literal[
-            'learned_absolute', 'rope', 'mrope', 'none'
-        ] = 'learned_absolute',
-        rotary_percent: float = 1.0,
-        rotary_base: int = 10000,
-        rope_scaling: bool = False,
-        rope_scaling_factor: float = 8.0,
-        scatter_embedding_sequence_parallel: bool = True,
-        seq_len_interpolation_factor: Optional[float] = None,
-        mtp_block_spec: Optional[ModuleSpec] = None,
-    ) -> None:
-        super().__init__(config=config, transformer_layer_spec=transformer_layer_spec,
-                         vocab_size=vocab_size, max_sequence_length=max_sequence_length,
-                         pre_process=pre_process, post_process=post_process,
-                         fp16_lm_cross_entropy=fp16_lm_cross_entropy,
-                         parallel_output=parallel_output,
-                         share_embeddings_and_output_weights=share_embeddings_and_output_weights,
-                         position_embedding_type=position_embedding_type,
-                         rotary_percent=rotary_percent,
-                         rotary_base=rotary_base,
-                         rope_scaling=rope_scaling,
-                         rope_scaling_factor=rope_scaling_factor,
-                         scatter_embedding_sequence_parallel=scatter_embedding_sequence_parallel,
-                         seq_len_interpolation_factor=seq_len_interpolation_factor,
-                         mtp_block_spec=mtp_block_spec)
-        if self.pre_process:
-            self.embedding = QwenVLLanguageModelEmbedding(
-                config=self.config,
-                vocab_size=self.vocab_size,
-                max_sequence_length=self.max_sequence_length,
-                position_embedding_type=position_embedding_type,
-            )
+# Copyright (c) 2025, BAAI. All rights reserved.
+#
+# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_vl/gpt_model.py. Below is the original copyright:
+#   Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved.
+
+from typing import Literal, Optional
+from torch import Tensor
+
+from megatron.core.transformer.spec_utils import ModuleSpec
+from megatron.core.models.gpt.gpt_model import GPTModel
+from megatron.core import tensor_parallel
+from megatron.core.transformer.transformer_config import TransformerConfig
+from megatron.core.models.common.embeddings.language_model_embedding import LanguageModelEmbedding
+
+
+class QwenVLLanguageModelEmbedding(LanguageModelEmbedding):
+    """Language model embeddings. Used for Qwen2.5-VL, inserting the image and video hidden states.
+
+    Args:
+        config (TransformerConfig): config object with all necessary configs for TransformerBlock
+        vocab_size (int): vocabulary size
+        max_sequence_length (int): maximum size of sequence. This
+                             is used for positional embedding
+        add_position_embedding (bool): Add a position embedding.
+        embedding_dropout_prob (float): dropout probability for embeddings
+        num_tokentypes (int): Set to 0 without binary head, and 2 with a binary head. Defaults to 0.
+        scatter_to_sequence_parallel (bool): Set to False to disable scatter of embedding
+            across sequence parallel region. Defaults to True.
+    """
+
+    def __init__(
+        self,
+        config: TransformerConfig,
+        vocab_size: int,
+        max_sequence_length: int,
+        position_embedding_type: Literal['learned_absolute', 'rope', 'none'] = 'learned_absolute',
+        num_tokentypes: int = 0,
+        scatter_to_sequence_parallel: bool = False, # chage default to False
+    ):
+        assert scatter_to_sequence_parallel == False, "QwenVLLanguageModelEmbedding does not support scatter_to_sequence_parallel"
+        super().__init__(config, vocab_size, max_sequence_length, position_embedding_type, num_tokentypes, scatter_to_sequence_parallel)
+
+
+    def forward(
+        self,
+        input_ids: Tensor,
+        position_ids: Tensor,
+        tokentype_ids: int = None,
+        image_input_mask: Tensor = None,
+        video_input_mask: Tensor = None,
+        image_embeds: Tensor = None,
+        video_embeds: Tensor = None
+    ) -> Tensor:
+        """Forward pass of the embedding module.
+
+        Args:
+            input_ids (Tensor): The input tokens
+            position_ids (Tensor): The position id's used to calculate position embeddings
+            tokentype_ids (int): The token type ids. Used when args.bert_binary_head is set to True. Defaults to None
+
+        Returns:
+            Tensor: The output embeddings
+        """
+        word_embeddings = self.word_embeddings(input_ids)
+        if self.add_position_embedding:
+            position_embeddings = self.position_embeddings(position_ids)
+            embeddings = word_embeddings + position_embeddings
+        else:
+            embeddings = word_embeddings
+
+        if not self.reduce_scatter_embeddings:
+            # Data format change to avoid explicit tranposes : [b s h] --> [s b h].
+            embeddings = embeddings.transpose(0, 1).contiguous()
+
+        if tokentype_ids is not None:
+            assert self.tokentype_embeddings is not None
+            # [b s h] -> [s b h] (So that it can be added with embeddings)
+            tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)
+            embeddings = embeddings + tokentype_embedding
+        else:
+            assert self.tokentype_embeddings is None
+
+        # If the input flag for fp32 residual connection is set, convert for float.
+        if self.config.fp32_residual_connection:
+            embeddings = embeddings.float()
+
+        # Dropout.
+        if self.config.sequence_parallel:
+            if not self.reduce_scatter_embeddings:
+                embeddings = embeddings.clone()
+                if image_embeds is not None:
+                    embeddings[image_input_mask] = image_embeds.to(embeddings.device, embeddings.dtype)
+                if video_embeds is not None:
+                    embeddings[video_input_mask] = video_embeds.to(embeddings.device, embeddings.dtype)
+                embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)
+            # `scatter_to_sequence_parallel_region` returns a view, which prevents
+            # the original tensor from being garbage collected. Clone to facilitate GC.
+            # Has a small runtime cost (~0.5%).
+            if self.config.clone_scatter_output_in_embedding:
+                embeddings = embeddings.clone()
+            with tensor_parallel.get_cuda_rng_tracker().fork():
+                embeddings = self.embedding_dropout(embeddings)
+        else:
+            embeddings = embeddings.clone()
+            if image_embeds is not None:
+                embeddings[image_input_mask] = image_embeds.to(embeddings.device, embeddings.dtype)
+            if video_embeds is not None:
+                embeddings[video_input_mask] = video_embeds.to(embeddings.device, embeddings.dtype)
+            embeddings = self.embedding_dropout(embeddings)
+
+        return embeddings
+
+
+class QwenVLLanguageModel(GPTModel):
+    """GPT Transformer language model, replace language embedding using QwenVLLanguageModelEmbedding.
+
+    Args:
+        config (TransformerConfig): Transformer config
+        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers
+        vocab_size (int): Vocabulary size
+        max_sequence_length (int): maximum size of sequence. This is used for positional embedding
+        pre_process (bool, optional): Include embedding layer (used with pipeline parallelism). Defaults to True.
+        post_process (bool, optional): Include an output layer (used with pipeline parallelism). Defaults to True.
+        fp16_lm_cross_entropy (bool, optional): Defaults to False.
+        parallel_output (bool, optional): Do not gather the outputs, keep them split across tensor parallel ranks. Defaults to True.
+        share_embeddings_and_output_weights (bool, optional): When True, input embeddings and output logit weights are shared. Defaults to False.
+        position_embedding_type (Literal[learned_absolute,rope], optional):  Position embedding type.. Defaults to 'learned_absolute'.
+        rotary_percent (float, optional): Percent of rotary dimension to use for rotary position embeddings. Ignored unless position_embedding_type is 'rope'. Defaults to 1.0.
+        rotary_base (int, optional): Base period for rotary position embeddings. Ignored unless position_embedding_type is 'rope'. Defaults to 10000.
+        seq_len_interpolation_factor (Optional[float], optional): scale of linearly interpolating RoPE for longer sequences. The value must be a float larger than 1.0. Defaults to None.
+    """
+
+    def __init__(
+        self,
+        config: TransformerConfig,
+        transformer_layer_spec: ModuleSpec,
+        vocab_size: int,
+        max_sequence_length: int,
+        pre_process: bool = True,
+        post_process: bool = True,
+        fp16_lm_cross_entropy: bool = False,
+        parallel_output: bool = True,
+        share_embeddings_and_output_weights: bool = False,
+        position_embedding_type: Literal[
+            'learned_absolute', 'rope', 'mrope', 'none'
+        ] = 'learned_absolute',
+        rotary_percent: float = 1.0,
+        rotary_base: int = 10000,
+        rope_scaling: bool = False,
+        rope_scaling_factor: float = 8.0,
+        scatter_embedding_sequence_parallel: bool = True,
+        seq_len_interpolation_factor: Optional[float] = None,
+        mtp_block_spec: Optional[ModuleSpec] = None,
+    ) -> None:
+        super().__init__(config=config, transformer_layer_spec=transformer_layer_spec,
+                         vocab_size=vocab_size, max_sequence_length=max_sequence_length,
+                         pre_process=pre_process, post_process=post_process,
+                         fp16_lm_cross_entropy=fp16_lm_cross_entropy,
+                         parallel_output=parallel_output,
+                         share_embeddings_and_output_weights=share_embeddings_and_output_weights,
+                         position_embedding_type=position_embedding_type,
+                         rotary_percent=rotary_percent,
+                         rotary_base=rotary_base,
+                         rope_scaling=rope_scaling,
+                         rope_scaling_factor=rope_scaling_factor,
+                         scatter_embedding_sequence_parallel=scatter_embedding_sequence_parallel,
+                         seq_len_interpolation_factor=seq_len_interpolation_factor,
+                         mtp_block_spec=mtp_block_spec)
+        if self.pre_process:
+            self.embedding = QwenVLLanguageModelEmbedding(
+                config=self.config,
+                vocab_size=self.vocab_size,
+                max_sequence_length=self.max_sequence_length,
+                position_embedding_type=position_embedding_type,
+            )
diff --git a/flagscale/train/models/qwen2_5_vl/layer_specs.py b/flagscale/train/models/qwen2_5_vl/layer_specs.py
index b74cb4ff..31d6eee5 100644
--- a/flagscale/train/models/qwen2_5_vl/layer_specs.py
+++ b/flagscale/train/models/qwen2_5_vl/layer_specs.py
@@ -1,123 +1,123 @@
-# Mainly Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_vl/layer_specs.py. Below is the original copyright:
-# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from megatron.core.fusions.fused_bias_dropout import get_bias_dropout_add
-
-from megatron.core.extensions.transformer_engine import (
-    TEDotProductAttention,
-    TELayerNormColumnParallelLinear,
-    TENorm,
-    TERowParallelLinear,
-    TEColumnParallelLinear
-)
-
-from megatron.core.transformer.enums import AttnMaskType
-from megatron.core.transformer.identity_op import IdentityOp
-
-from megatron.core.transformer.spec_utils import ModuleSpec
-from megatron.core.transformer.transformer_layer import TransformerLayer, TransformerLayerSubmodules
-
-from megatron.core.transformer.mlp import MLP, MLPSubmodules
-from megatron.core.transformer.attention import (SelfAttentionSubmodules, SelfAttention)
-
-from .vision_attention import SelfAttentionVision
-
-# Use this spec to use lower level Transformer Engine modules (required for fp8 training)
-def get_gpt_layer_with_transformer_engine_spec(
-    qk_layernorm: bool = False
-) -> ModuleSpec:
-    mlp = get_mlp_module_spec(
-        use_te=True, num_experts=None, moe_grouped_gemm=False
-    )
-    return ModuleSpec(
-        module=TransformerLayer,
-        submodules=TransformerLayerSubmodules(
-            self_attention=ModuleSpec(
-                module=SelfAttention,
-                params={"attn_mask_type": AttnMaskType.causal},
-                submodules=SelfAttentionSubmodules(
-                    linear_qkv=TELayerNormColumnParallelLinear,
-                    core_attention=TEDotProductAttention,
-                    linear_proj=TERowParallelLinear,
-                    q_layernorm=TENorm if qk_layernorm else IdentityOp,
-                    k_layernorm=TENorm if qk_layernorm else IdentityOp,
-                ),
-            ),
-            self_attn_bda=get_bias_dropout_add,
-            pre_mlp_layernorm=IdentityOp,
-            mlp=mlp,
-            mlp_bda=get_bias_dropout_add,
-        ),
-    )
-
-def get_qwen2vl_vision_model_spec(
-    is_vit=False
-) -> ModuleSpec:
-    attn_mask_type = AttnMaskType.no_mask # THD --> causal_pad
-
-    mlp = ModuleSpec(
-        module=MLP,
-        submodules=MLPSubmodules(
-            linear_fc1=TELayerNormColumnParallelLinear,
-            linear_fc2=TERowParallelLinear,
-        ),
-    )
-    return ModuleSpec(
-        module=TransformerLayer,
-        submodules=TransformerLayerSubmodules(
-            self_attention=ModuleSpec(
-                module=SelfAttentionVision,
-                params={"attn_mask_type": attn_mask_type},
-                submodules=SelfAttentionSubmodules(
-                    linear_qkv=TELayerNormColumnParallelLinear,
-                    core_attention=TEDotProductAttention,
-                    linear_proj=TERowParallelLinear,
-                    q_layernorm=IdentityOp,
-                    k_layernorm=IdentityOp,
-                ),
-            ),
-            self_attn_bda=get_bias_dropout_add,
-            pre_mlp_layernorm=IdentityOp,
-            mlp=mlp,
-            mlp_bda=get_bias_dropout_add,
-        ),
-    )
-
-
-# Helper function to get module spec for MLP/MoE
-def get_mlp_module_spec(
-    use_te: bool = True, num_experts: int = None, moe_grouped_gemm: bool = False, add_norm: bool = True
-) -> ModuleSpec:
-    if num_experts is None:
-        # Dense MLP w/ or w/o TE modules.
-        if add_norm:
-            return ModuleSpec(
-                module=MLP,
-                submodules=MLPSubmodules(
-                    linear_fc1=TELayerNormColumnParallelLinear if use_te else ColumnParallelLinear,
-                    linear_fc2=TERowParallelLinear if use_te else RowParallelLinear,
-                ),
-            )
-        else:
-            return ModuleSpec(
-                module=MLP,
-                submodules=MLPSubmodules(
-                    linear_fc1=TEColumnParallelLinear if use_te else ColumnParallelLinear,
-                    linear_fc2=TERowParallelLinear if use_te else RowParallelLinear,
-                ),
-            )
-    else:
-        # Mixture of experts with modules in megatron core.
-        raise NotImplementedError()
+# Mainly Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_vl/layer_specs.py. Below is the original copyright:
+# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from megatron.core.fusions.fused_bias_dropout import get_bias_dropout_add
+
+from megatron.core.extensions.transformer_engine import (
+    TEDotProductAttention,
+    TELayerNormColumnParallelLinear,
+    TENorm,
+    TERowParallelLinear,
+    TEColumnParallelLinear
+)
+
+from megatron.core.transformer.enums import AttnMaskType
+from megatron.core.transformer.identity_op import IdentityOp
+
+from megatron.core.transformer.spec_utils import ModuleSpec
+from megatron.core.transformer.transformer_layer import TransformerLayer, TransformerLayerSubmodules
+
+from megatron.core.transformer.mlp import MLP, MLPSubmodules
+from megatron.core.transformer.attention import (SelfAttentionSubmodules, SelfAttention)
+
+from .vision_attention import SelfAttentionVision
+
+# Use this spec to use lower level Transformer Engine modules (required for fp8 training)
+def get_gpt_layer_with_transformer_engine_spec(
+    qk_layernorm: bool = False
+) -> ModuleSpec:
+    mlp = get_mlp_module_spec(
+        use_te=True, num_experts=None, moe_grouped_gemm=False
+    )
+    return ModuleSpec(
+        module=TransformerLayer,
+        submodules=TransformerLayerSubmodules(
+            self_attention=ModuleSpec(
+                module=SelfAttention,
+                params={"attn_mask_type": AttnMaskType.causal},
+                submodules=SelfAttentionSubmodules(
+                    linear_qkv=TELayerNormColumnParallelLinear,
+                    core_attention=TEDotProductAttention,
+                    linear_proj=TERowParallelLinear,
+                    q_layernorm=TENorm if qk_layernorm else IdentityOp,
+                    k_layernorm=TENorm if qk_layernorm else IdentityOp,
+                ),
+            ),
+            self_attn_bda=get_bias_dropout_add,
+            pre_mlp_layernorm=IdentityOp,
+            mlp=mlp,
+            mlp_bda=get_bias_dropout_add,
+        ),
+    )
+
+def get_qwen2vl_vision_model_spec(
+    is_vit=False
+) -> ModuleSpec:
+    attn_mask_type = AttnMaskType.no_mask # THD --> causal_pad
+
+    mlp = ModuleSpec(
+        module=MLP,
+        submodules=MLPSubmodules(
+            linear_fc1=TELayerNormColumnParallelLinear,
+            linear_fc2=TERowParallelLinear,
+        ),
+    )
+    return ModuleSpec(
+        module=TransformerLayer,
+        submodules=TransformerLayerSubmodules(
+            self_attention=ModuleSpec(
+                module=SelfAttentionVision,
+                params={"attn_mask_type": attn_mask_type},
+                submodules=SelfAttentionSubmodules(
+                    linear_qkv=TELayerNormColumnParallelLinear,
+                    core_attention=TEDotProductAttention,
+                    linear_proj=TERowParallelLinear,
+                    q_layernorm=IdentityOp,
+                    k_layernorm=IdentityOp,
+                ),
+            ),
+            self_attn_bda=get_bias_dropout_add,
+            pre_mlp_layernorm=IdentityOp,
+            mlp=mlp,
+            mlp_bda=get_bias_dropout_add,
+        ),
+    )
+
+
+# Helper function to get module spec for MLP/MoE
+def get_mlp_module_spec(
+    use_te: bool = True, num_experts: int = None, moe_grouped_gemm: bool = False, add_norm: bool = True
+) -> ModuleSpec:
+    if num_experts is None:
+        # Dense MLP w/ or w/o TE modules.
+        if add_norm:
+            return ModuleSpec(
+                module=MLP,
+                submodules=MLPSubmodules(
+                    linear_fc1=TELayerNormColumnParallelLinear if use_te else ColumnParallelLinear,
+                    linear_fc2=TERowParallelLinear if use_te else RowParallelLinear,
+                ),
+            )
+        else:
+            return ModuleSpec(
+                module=MLP,
+                submodules=MLPSubmodules(
+                    linear_fc1=TEColumnParallelLinear if use_te else ColumnParallelLinear,
+                    linear_fc2=TERowParallelLinear if use_te else RowParallelLinear,
+                ),
+            )
+    else:
+        # Mixture of experts with modules in megatron core.
+        raise NotImplementedError()
diff --git a/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py b/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py
index 5f0ce50e..89abcb1e 100644
--- a/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py
+++ b/flagscale/train/models/qwen2_5_vl/qwen2_5_vl_model.py
@@ -1,281 +1,276 @@
-# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/model.py. Below is the original copyright:
-#  Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
-
-import logging
-from collections import namedtuple
-from typing import List
-
-import torch
-
-from megatron.core import InferenceParams
-from megatron.core.transformer import MegatronModule
-from megatron.core.transformer.spec_utils import ModuleSpec
-from megatron.core.transformer.transformer_config import TransformerConfig
-from megatron.core.packed_seq_params import PackedSeqParams
-
-
-from flagscale.train.models.qwen2_5_vl.vit_model import Qwen2_5VisionModel
-from flagscale.train.models.qwen2_5_vl.language_module import QwenVLLanguageModel
-
-# Note: This is under development and may be missing features.
-class Qwen2_5VLModel(MegatronModule):
-    """Qwen2.5VL multi-modal model.
-
-    Args:
-        language_transformer_config (TransformerConfig): Transformer config for the language model.
-        language_transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers of the language model.
-        language_vocab_size (int): Language model vocabulary size.
-        language_max_sequence_length (int): Language model maximum sequence length. This is used for positional embedding.
-        vision_transformer_config (TransformerConfig): Transformer config for the vision model.
-        vision_transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers of the vision model.
-        drop_vision_class_token (bool): Drop vision class token(s) before input to the language model.
-        vision_projection_config (TransformerConfig): Config for the projection from vision model outputs to language model inputs.
-        vision_projection_layer_spec (ModuleSpec): Specifies the module to use for the vision projection.
-        vision_projection_type (str): Type of the vision projection to use. Default is a 2-layer MLP.
-        allow_missing_vision_projection_checkpoint (bool): Allow vision projection weights to be missing when loading a checkpoint. Default False.
-        parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks. This is typically True for training and False for inference.
-        language_position_embedding_type (str): Position embedding type to use in the language model. Default learned absolute.
-        language_rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings in the language model. Defaults to 1.0.
-        pre_process (bool): Include the embedding layer in the gpt decoder (used with pipeline parallelism). Defaults to True.
-        post_process (bool): Include an output layer and a layernorm in the gpt decoder (used with pipeline parallelism). Defaults to True.
-        add_encoder (bool): Construct the encoder module (used with pipeline parallelism). Defaults to True. When we use pipelining, the encoder
-            will live on only a subset of the pipeline stages (specifically, only the first stage).
-        add_decoder (bool): Construct the decoder module (used with pipeline parallelism). Defaults to True. When we use pipelining, the decoder
-            will live on only a subset of the pipeline stages (specifically, every stage after the first one).
-        language_rotary_base (int): RoPE base.
-        fp16_lm_cross_entropy (bool, optional): Defaults to False.
-        language_share_embeddings_and_output_weights (bool, optional): When True, input embeddings and output logit weights are shared for language model. Defaults to False.
-    """
-
-    def __init__(
-        self,
-        language_transformer_config: TransformerConfig,
-        language_transformer_layer_spec: ModuleSpec,
-        language_vocab_size: int,
-        language_max_sequence_length: int,
-        vision_transformer_config: TransformerConfig,
-        vision_transformer_layer_spec: ModuleSpec,
-        drop_vision_class_token: bool,
-        vision_projection_config: TransformerConfig,
-        vision_projection_layer_spec: ModuleSpec,
-        vision_projection_type: str = "mlp",
-
-        allow_missing_vision_projection_checkpoint: bool = False,
-        parallel_output: bool = True,
-        language_position_embedding_type: str = 'rope',
-        language_rotary_percent: float = 1.0,
-        pre_process: bool = True,
-        post_process: bool = True,
-        add_encoder: bool = True,
-        add_decoder: bool = True,
-        language_rotary_base: int = 10000,
-        fp16_lm_cross_entropy: bool = False,
-        language_share_embeddings_and_output_weights: bool=False
-    ) -> None:
-        super().__init__(config=language_transformer_config)
-
-        logging.getLogger(__name__).warning(
-            "Qwen2VL model is under development and may be missing features."
-        )
-
-        self.pre_process = pre_process
-        self.post_process = post_process
-        self.add_encoder = add_encoder
-        self.add_decoder = add_decoder
-
-        self.encoder_hidden_state = None
-        self.vision_model = None
-        self.vision_projection = None
-        self.language_model = None
-
-        self.square_merge_size = vision_projection_config.ffn_hidden_size // vision_transformer_config.hidden_size
-
-        # This attribute is needed to check if an all-reduce is required
-        # on the word embeddings inside `finalize_model_grads._allreduce_word_embedding_grads`.
-        self.share_embeddings_and_output_weights = False
-        if self.pre_process:
-            self.vision_model = Qwen2_5VisionModel(
-                vision_transformer_config,
-                vision_transformer_layer_spec,
-                vision_projection_config,
-                vision_projection_layer_spec,
-                projection_type=vision_projection_type,
-                pre_process=True,
-                post_process=True
-            )
-
-        self.language_model = QwenVLLanguageModel(
-            config=language_transformer_config,
-            transformer_layer_spec=language_transformer_layer_spec,
-            vocab_size=language_vocab_size,
-            max_sequence_length=language_max_sequence_length,
-            parallel_output=parallel_output,
-            position_embedding_type=language_position_embedding_type,
-            rotary_percent=language_rotary_percent,
-            pre_process=self.pre_process,
-            post_process=self.post_process,
-            rotary_base=language_rotary_base,
-            fp16_lm_cross_entropy=fp16_lm_cross_entropy,
-            share_embeddings_and_output_weights=language_share_embeddings_and_output_weights
-        )
-        self.share_embeddings_and_output_weights = (
-            self.language_model.share_embeddings_and_output_weights
-        )
-
-    def shared_embedding_or_output_weight(self):
-        """This is a convenience method to surface the language model's word embeddings, which is
-        necessary for `finalize_model_grads._allreduce_word_embedding_grads`."""
-        if self.add_decoder:
-            return self.language_model.shared_embedding_or_output_weight()
-        return None
-
-    def set_input_tensor(self, input_tensor) -> None:
-        # This is usually handled in schedules.py but some inference code still
-        # gives us non-lists or None
-        if not isinstance(input_tensor, list):
-            input_tensor = [input_tensor]
-        assert len(input_tensor) == 1, 'input_tensor should only be length 1 for Qwen2VL'
-
-        if self.pre_process:
-            self.encoder_hidden_state = input_tensor[0]
-        else:
-            self.language_model.set_input_tensor(input_tensor[0])
-
-    def freeze(
-        self, freeze_language_model: bool, freeze_vision_model: bool, freeze_vision_projection: bool
-    ):
-        """Freeze model modules.
-
-        Make specific modules non-trainable by setting requires_grad to False for the module's parameters.
-
-        Args:
-            freeze_language_model (bool): Freeze the language model module.
-            freeze_vision_model (bool): Freeze the vision model module.
-            freeze_vision_projection (bool): Freeze the vision projection module.
-        """
-        modules = []
-        if freeze_language_model and self.language_model is not None:
-            modules.append(self.language_model)
-        if freeze_vision_model and self.vision_model is not None:
-            modules.append(self.vision_model)
-        if freeze_vision_projection and self.vision_projection is not None:
-            modules.append(self.vision_projection)
-
-        for module in modules:
-            for param in module.parameters():
-                param.requires_grad = False
-
-    def forward(
-        self,
-        input_ids: torch.Tensor,
-        position_ids: torch.Tensor,
-        vision_data: torch.Tensor = None,
-        vision_grid_thw: torch.Tensor = None,
-        video_start_index: int = -1,
-        image_input_mask: torch.Tensor = None,
-        video_input_mask: torch.Tensor = None,
-
-        attention_mask: torch.Tensor = None,
-        labels: torch.Tensor = None,
-        inference_params: InferenceParams = None,
-        packed_seq_params: PackedSeqParams = None,
-        extra_block_kwargs: dict = None,
-    ) -> torch.Tensor:
-        """Forward function of the Qwen2VL model.
-
-        Args:
-            image_data (torch.Tensor): input image of shape [total_thw_size, n_features].
-            input_ids (torch.Tensor): input text ids [batch, text_seq_len].
-            position_ids (torch.Tensor): input text position ids [batch, text_seq_len].
-            attention_mask (torch.Tensor): attention mask for the language model [batch, 1, combined_seq_len, combined_seq_len].
-            labels (torch.Tensor): Optional target text labels [batch, combined_seq_len].
-            inference_params (InferenceParams): Inference-time parameters including KV cache.
-
-            video_start_index:
-                0 -- all video
-                len(video_seq) -- all image
-                others -- mixture
-            *_input_mask: should not be None in the first PP stage
-        Returns:
-            output (torch.Tensor): Loss of shape [b, s] if labels are provided, otherwise logits of shape [b, s, vocab_size].
-        """
-
-        use_inference_kv_cache = (
-            inference_params is not None
-            and "image_tokens_count" in inference_params.key_value_memory_dict
-        )
-        if use_inference_kv_cache:
-            raise NotImplementedError()
-
-        if self.pre_process:
-            vision_embeds = None
-            if vision_grid_thw.shape[0] > 0:
-                # NOTE(lizhiyu): Reference https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1612
-                if self.config.bf16:
-                    vision_data = vision_data.to(torch.bfloat16)
-                elif self.config.fp16:
-                    vision_data = vision_data.to(torch.float16)
-                vision_embeds = self.vision_model(
-                    vision_data=vision_data, # If None, vision model should use intermediate outputs (EPP > 1)
-                    grid_thw=vision_grid_thw # should provided in each EPP stage
-                )
-
-            # If running inference, the language model KV cache will be updated for image token positions.
-            # Here we store the image tokens sequence length, which can be used as an offset to the KV cache later.
-            if inference_params is not None:
-                raise NotImplementedError()
-                # inference_params.key_value_memory_dict["image_tokens_count"] = (
-                #     vision_embeddings.shape[0]
-                # )
-
-            # If running inference, we can skip image token computation if they were computed already earlier for this sample.
-            if use_inference_kv_cache:
-                language_embeddings: torch.Tensor = self.language_model.embedding(
-                input_ids=input_ids,
-                position_ids=None # NOTE: disable
-                )  # [text_seq_len, b, h_language]
-                # NOTE: why not cat here? is it the combined embeddings useless?
-                combined_embeddings = language_embeddings
-            elif vision_embeds is not None:
-                if video_start_index == 0:
-                    image_embeds = None
-                    video_embeds = vision_embeds
-                elif video_start_index == vision_embeds.shape[0]:
-                    image_embeds = vision_embeds
-                    video_embeds = None
-                elif 0 < video_start_index < vision_embeds.shape[0]:
-                    image_embeds = vision_embeds[:video_start_index]
-                    video_embeds = vision_embeds[video_start_index:]
-                else:
-                    raise ValueError(f"Expect video token start index in range [0, {vision_embeds.shape[0]}], but got {video_start_index}")
-
-                if image_embeds is not None:
-                    image_input_mask = image_input_mask.T # shape [seqlen, mbs]
-                if video_embeds is not None:
-                    video_input_mask = video_input_mask.T
-                combined_embeddings = self.language_model.embedding(
-                    input_ids=input_ids,
-                    position_ids=None, # NOTE: disable
-                    image_input_mask=image_input_mask,
-                    video_input_mask=video_input_mask,
-                    image_embeds=image_embeds,
-                    video_embeds=video_embeds
-                )  # [text_seq_len, b, h_language]
-            else:
-                combined_embeddings = self.language_model.embedding(
-                    input_ids=input_ids,
-                    position_ids=None # NOTE: disable
-                )  # [text_seq_len, b, h_language]
-        else:
-            combined_embeddings = None
-        output = self.language_model(
-            input_ids=None,
-            position_ids=position_ids,              # None in encoder
-            attention_mask=attention_mask,          # None in encoder
-            decoder_input=combined_embeddings,      # only not None in the first decoder PP stage
-            labels=labels,                          # only not None in the last decoder PP stage
-            inference_params=inference_params,      # currently always None
-            packed_seq_params=packed_seq_params,    # currently always None
-            **(extra_block_kwargs or {}),
-        )
-        return output
+# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/model.py. Below is the original copyright:
+#  Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
+
+import logging
+from collections import namedtuple
+from typing import List
+
+import torch
+
+from megatron.core import InferenceParams
+from megatron.core.transformer import MegatronModule
+from megatron.core.transformer.spec_utils import ModuleSpec
+from megatron.core.transformer.transformer_config import TransformerConfig
+from megatron.core.packed_seq_params import PackedSeqParams
+
+
+from flagscale.train.models.qwen2_5_vl.vit_model import Qwen2_5VisionModel
+from flagscale.train.models.qwen2_5_vl.language_module import QwenVLLanguageModel
+
+# Note: This is under development and may be missing features.
+class Qwen2_5VLModel(MegatronModule):
+    """Qwen2.5VL multi-modal model.
+
+    Args:
+        language_transformer_config (TransformerConfig): Transformer config for the language model.
+        language_transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers of the language model.
+        language_vocab_size (int): Language model vocabulary size.
+        language_max_sequence_length (int): Language model maximum sequence length. This is used for positional embedding.
+        vision_transformer_config (TransformerConfig): Transformer config for the vision model.
+        vision_transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers of the vision model.
+        drop_vision_class_token (bool): Drop vision class token(s) before input to the language model.
+        vision_projection_config (TransformerConfig): Config for the projection from vision model outputs to language model inputs.
+        vision_projection_layer_spec (ModuleSpec): Specifies the module to use for the vision projection.
+        vision_projection_type (str): Type of the vision projection to use. Default is a 2-layer MLP.
+        allow_missing_vision_projection_checkpoint (bool): Allow vision projection weights to be missing when loading a checkpoint. Default False.
+        parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks. This is typically True for training and False for inference.
+        language_position_embedding_type (str): Position embedding type to use in the language model. Default learned absolute.
+        language_rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings in the language model. Defaults to 1.0.
+        pre_process (bool): Include the embedding layer in the gpt decoder (used with pipeline parallelism). Defaults to True.
+        post_process (bool): Include an output layer and a layernorm in the gpt decoder (used with pipeline parallelism). Defaults to True.
+        add_encoder (bool): Construct the encoder module (used with pipeline parallelism). Defaults to True. When we use pipelining, the encoder
+            will live on only a subset of the pipeline stages (specifically, only the first stage).
+        add_decoder (bool): Construct the decoder module (used with pipeline parallelism). Defaults to True. When we use pipelining, the decoder
+            will live on only a subset of the pipeline stages (specifically, every stage after the first one).
+        language_rotary_base (int): RoPE base.
+        fp16_lm_cross_entropy (bool, optional): Defaults to False.
+        language_share_embeddings_and_output_weights (bool, optional): When True, input embeddings and output logit weights are shared for language model. Defaults to False.
+    """
+
+    def __init__(
+        self,
+        language_transformer_config: TransformerConfig,
+        language_transformer_layer_spec: ModuleSpec,
+        language_vocab_size: int,
+        language_max_sequence_length: int,
+        vision_transformer_config: TransformerConfig,
+        vision_transformer_layer_spec: ModuleSpec,
+        drop_vision_class_token: bool,
+        vision_projection_config: TransformerConfig,
+        vision_projection_layer_spec: ModuleSpec,
+        vision_projection_type: str = "mlp",
+
+        allow_missing_vision_projection_checkpoint: bool = False,
+        parallel_output: bool = True,
+        language_position_embedding_type: str = 'rope',
+        language_rotary_percent: float = 1.0,
+        pre_process: bool = True,
+        post_process: bool = True,
+        add_encoder: bool = True,
+        add_decoder: bool = True,
+        language_rotary_base: int = 10000,
+        fp16_lm_cross_entropy: bool = False,
+        language_share_embeddings_and_output_weights: bool=False
+    ) -> None:
+        super().__init__(config=language_transformer_config)
+
+        logging.getLogger(__name__).warning(
+            "Qwen2VL model is under development and may be missing features."
+        )
+
+        self.pre_process = pre_process
+        self.post_process = post_process
+        self.add_encoder = add_encoder
+        self.add_decoder = add_decoder
+
+        self.encoder_hidden_state = None
+        self.vision_model = None
+        self.vision_projection = None
+        self.language_model = None
+
+        self.square_merge_size = vision_projection_config.ffn_hidden_size // vision_transformer_config.hidden_size
+
+        # This attribute is needed to check if an all-reduce is required
+        # on the word embeddings inside `finalize_model_grads._allreduce_word_embedding_grads`.
+        self.share_embeddings_and_output_weights = False
+        if self.pre_process:
+            self.vision_model = Qwen2_5VisionModel(
+                vision_transformer_config,
+                vision_transformer_layer_spec,
+                vision_projection_config,
+                vision_projection_layer_spec,
+                projection_type=vision_projection_type,
+                pre_process=True,
+                post_process=True
+            )
+
+        self.language_model = QwenVLLanguageModel(
+            config=language_transformer_config,
+            transformer_layer_spec=language_transformer_layer_spec,
+            vocab_size=language_vocab_size,
+            max_sequence_length=language_max_sequence_length,
+            parallel_output=parallel_output,
+            position_embedding_type=language_position_embedding_type,
+            rotary_percent=language_rotary_percent,
+            pre_process=self.pre_process,
+            post_process=self.post_process,
+            rotary_base=language_rotary_base,
+            fp16_lm_cross_entropy=fp16_lm_cross_entropy,
+            share_embeddings_and_output_weights=language_share_embeddings_and_output_weights
+        )
+        self.share_embeddings_and_output_weights = (
+            self.language_model.share_embeddings_and_output_weights
+        )
+
+    def shared_embedding_or_output_weight(self):
+        """This is a convenience method to surface the language model's word embeddings, which is
+        necessary for `finalize_model_grads._allreduce_word_embedding_grads`."""
+        if self.add_decoder:
+            return self.language_model.shared_embedding_or_output_weight()
+        return None
+
+    def set_input_tensor(self, input_tensor) -> None:
+        # This is usually handled in schedules.py but some inference code still
+        # gives us non-lists or None
+        if not isinstance(input_tensor, list):
+            input_tensor = [input_tensor]
+        assert len(input_tensor) == 1, 'input_tensor should only be length 1 for Qwen2VL'
+
+        if self.pre_process:
+            self.encoder_hidden_state = input_tensor[0]
+        else:
+            self.language_model.set_input_tensor(input_tensor[0])
+
+    def freeze(
+        self, freeze_language_model: bool, freeze_vision_model: bool, freeze_vision_projection: bool
+    ):
+        """Freeze model modules.
+
+        Make specific modules non-trainable by setting requires_grad to False for the module's parameters.
+
+        Args:
+            freeze_language_model (bool): Freeze the language model module.
+            freeze_vision_model (bool): Freeze the vision model module.
+            freeze_vision_projection (bool): Freeze the vision projection module.
+        """
+        modules = []
+        if freeze_language_model and self.language_model is not None:
+            modules.append(self.language_model)
+        if freeze_vision_model and self.vision_model is not None:
+            modules.append(self.vision_model)
+        if freeze_vision_projection and self.vision_projection is not None:
+            modules.append(self.vision_projection)
+
+        for module in modules:
+            for param in module.parameters():
+                param.requires_grad = False
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        position_ids: torch.Tensor,
+        vision_data: torch.Tensor = None,
+        vision_grid_thw: torch.Tensor = None,
+        video_start_index: int = -1,
+        image_input_mask: torch.Tensor = None,
+        video_input_mask: torch.Tensor = None,
+
+        attention_mask: torch.Tensor = None,
+        labels: torch.Tensor = None,
+        inference_params: InferenceParams = None,
+        packed_seq_params: PackedSeqParams = None,
+        extra_block_kwargs: dict = None,
+    ) -> torch.Tensor:
+        """Forward function of the Qwen2VL model.
+
+        Args:
+            image_data (torch.Tensor): input image of shape [total_thw_size, n_features].
+            input_ids (torch.Tensor): input text ids [batch, text_seq_len].
+            position_ids (torch.Tensor): input text position ids [batch, text_seq_len].
+            attention_mask (torch.Tensor): attention mask for the language model [batch, 1, combined_seq_len, combined_seq_len].
+            labels (torch.Tensor): Optional target text labels [batch, combined_seq_len].
+            inference_params (InferenceParams): Inference-time parameters including KV cache.
+
+            video_start_index:
+                0 -- all video
+                len(video_seq) -- all image
+                others -- mixture
+            *_input_mask: should not be None in the first PP stage
+        Returns:
+            output (torch.Tensor): Loss of shape [b, s] if labels are provided, otherwise logits of shape [b, s, vocab_size].
+        """
+        use_inference_kv_cache = (
+            inference_params is not None
+            and "image_tokens_count" in inference_params.key_value_memory_dict
+        )
+        if use_inference_kv_cache:
+            raise NotImplementedError()
+
+        if self.pre_process:
+            vision_embeds = None
+            if vision_grid_thw.shape[0] > 0:
+                vision_embeds = self.vision_model(
+                    vision_data=vision_data, # If None, vision model should use intermediate outputs (EPP > 1)
+                    grid_thw=vision_grid_thw # should provided in each EPP stage
+                )
+
+            # If running inference, the language model KV cache will be updated for image token positions.
+            # Here we store the image tokens sequence length, which can be used as an offset to the KV cache later.
+            if inference_params is not None:
+                raise NotImplementedError()
+                # inference_params.key_value_memory_dict["image_tokens_count"] = (
+                #     vision_embeddings.shape[0]
+                # )
+
+            # If running inference, we can skip image token computation if they were computed already earlier for this sample.
+            if use_inference_kv_cache:
+                language_embeddings: torch.Tensor = self.language_model.embedding(
+                input_ids=input_ids,
+                position_ids=None # NOTE: disable
+                )  # [text_seq_len, b, h_language]
+                # NOTE: why not cat here? is it the combined embeddings useless?
+                combined_embeddings = language_embeddings
+            elif vision_embeds is not None:
+                if video_start_index == 0:
+                    image_embeds = None
+                    video_embeds = vision_embeds
+                elif video_start_index == vision_embeds.shape[0]:
+                    image_embeds = vision_embeds
+                    video_embeds = None
+                elif 0 < video_start_index < vision_embeds.shape[0]:
+                    image_embeds = vision_embeds[:video_start_index]
+                    video_embeds = vision_embeds[video_start_index:]
+                else:
+                    raise ValueError(f"Expect video token start index in range [0, {vision_embeds.shape[0]}], but got {video_start_index}")
+
+                if image_embeds is not None:
+                    image_input_mask = image_input_mask.T # shape [seqlen, mbs]
+                if video_embeds is not None:
+                    video_input_mask = video_input_mask.T
+                combined_embeddings = self.language_model.embedding(
+                    input_ids=input_ids,
+                    position_ids=None, # NOTE: disable
+                    image_input_mask=image_input_mask,
+                    video_input_mask=video_input_mask,
+                    image_embeds=image_embeds,
+                    video_embeds=video_embeds
+                )  # [text_seq_len, b, h_language]
+            else:
+                combined_embeddings = self.language_model.embedding(
+                    input_ids=input_ids,
+                    position_ids=None # NOTE: disable
+                )  # [text_seq_len, b, h_language]
+        else:
+            combined_embeddings = None
+
+        output = self.language_model(
+            input_ids=None,
+            position_ids=position_ids,              # None in encoder
+            attention_mask=attention_mask,          # None in encoder
+            decoder_input=combined_embeddings,      # only not None in the first decoder PP stage
+            labels=labels,                          # only not None in the last decoder PP stage
+            inference_params=inference_params,      # currently always None
+            packed_seq_params=packed_seq_params,    # currently always None
+            **(extra_block_kwargs or {}),
+        )
+        return output
diff --git a/flagscale/train/models/qwen2_5_vl/tensor_parallel.py b/flagscale/train/models/qwen2_5_vl/tensor_parallel.py
index b29dda13..04868fe2 100644
--- a/flagscale/train/models/qwen2_5_vl/tensor_parallel.py
+++ b/flagscale/train/models/qwen2_5_vl/tensor_parallel.py
@@ -1,108 +1,108 @@
-# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/tensor_parallel.py
-
-# NOTE: we slightly modify this file to support zero-size tensor, such as [0, 128] when we don't have video data
-import torch
-
-from megatron.core.parallel_state import (
-    get_tensor_model_parallel_group,
-    get_tensor_model_parallel_rank,
-    get_tensor_model_parallel_src_rank,
-)
-
-_MAX_DATA_DIM = 5
-
-
-def _check_data_types(keys, data, target_dtype):
-    """Check that all the keys have the same target data type."""
-    for key in keys:
-        assert (
-            data[key].dtype == target_dtype
-        ), '{} has data type {} which ' 'is different than {}'.format(
-            key, data[key].dtype, target_dtype
-        )
-
-
-def _build_key_size_numel_dictionaries(keys, data):
-    """Build the size on rank 0 and broadcast."""
-    max_dim = _MAX_DATA_DIM
-    # support to zero-size tensor, such as [0, 128]
-    sizes = [-1 for _ in range(max_dim) for _ in keys]
-
-    # Pack the sizes on rank zero.
-    if get_tensor_model_parallel_rank() == 0:
-        offset = 0
-        for key in keys:
-            assert data[key].dim() < max_dim, 'you should increase MAX_DATA_DIM'
-            size = data[key].size()
-            for i, s in enumerate(size):
-                sizes[i + offset] = s
-            offset += max_dim
-
-    # Move to GPU and broadcast.
-    sizes_cuda = torch.tensor(sizes, dtype=torch.long, device='cuda')
-    torch.distributed.broadcast(
-        sizes_cuda, get_tensor_model_parallel_src_rank(), group=get_tensor_model_parallel_group()
-    )
-
-    # Move back to cpu and unpack.
-    sizes_cpu = sizes_cuda.cpu()
-    key_size = {}
-    key_numel = {}
-    total_numel = 0
-    offset = 0
-    for key in keys:
-        i = 0
-        size = []
-        numel = 1
-        # support to zero-size tensor, such as [0, 128]
-        while sizes_cpu[offset + i] >= 0:
-            this_size = sizes_cpu[offset + i]
-            size.append(this_size)
-            numel *= this_size
-            i += 1
-        key_size[key] = size
-        key_numel[key] = numel
-        total_numel += numel
-        offset += max_dim
-
-    return key_size, key_numel, total_numel
-
-
-def broadcast_data(keys, data, datatype):
-    """Broadcast data from rank zero of each model parallel group to the
-    members of the same model parallel group.
-
-    Args:
-        keys: list of keys in the data disctionary to be broadcasted
-        data: data dictionary of string keys and cpu tensor values.
-        datatype: torch data type of all tensors in data associated
-                  with keys.
-    """
-    # Build (key, size) and (key, number of elements) dictionaries along
-    # with the total number of elements on all ranks.
-    key_size, key_numel, total_numel = _build_key_size_numel_dictionaries(keys, data)
-
-    # Pack on rank zero.
-    if get_tensor_model_parallel_rank() == 0:
-        # Check that all keys have the same data type.
-        _check_data_types(keys, data, datatype)
-        # Flatten the data associated with the keys
-        flatten_data = torch.cat([data[key].contiguous().view(-1) for key in keys], dim=0).cuda()
-    else:
-        flatten_data = torch.empty(total_numel, device=torch.cuda.current_device(), dtype=datatype)
-
-    # Broadcast
-    torch.distributed.broadcast(
-        flatten_data, get_tensor_model_parallel_src_rank(), group=get_tensor_model_parallel_group()
-    )
-
-    # Unpack
-    output = {}
-    offset = 0
-    for key in keys:
-        size = key_size[key]
-        numel = key_numel[key]
-        output[key] = flatten_data.narrow(0, offset, numel).view(size)
-        offset += numel
-
-    return output
+# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/tensor_parallel.py
+
+# NOTE: we slightly modify this file to support zero-size tensor, such as [0, 128] when we don't have video data
+import torch
+
+from megatron.core.parallel_state import (
+    get_tensor_model_parallel_group,
+    get_tensor_model_parallel_rank,
+    get_tensor_model_parallel_src_rank,
+)
+
+_MAX_DATA_DIM = 5
+
+
+def _check_data_types(keys, data, target_dtype):
+    """Check that all the keys have the same target data type."""
+    for key in keys:
+        assert (
+            data[key].dtype == target_dtype
+        ), '{} has data type {} which ' 'is different than {}'.format(
+            key, data[key].dtype, target_dtype
+        )
+
+
+def _build_key_size_numel_dictionaries(keys, data):
+    """Build the size on rank 0 and broadcast."""
+    max_dim = _MAX_DATA_DIM
+    # support to zero-size tensor, such as [0, 128]
+    sizes = [-1 for _ in range(max_dim) for _ in keys]
+
+    # Pack the sizes on rank zero.
+    if get_tensor_model_parallel_rank() == 0:
+        offset = 0
+        for key in keys:
+            assert data[key].dim() < max_dim, 'you should increase MAX_DATA_DIM'
+            size = data[key].size()
+            for i, s in enumerate(size):
+                sizes[i + offset] = s
+            offset += max_dim
+
+    # Move to GPU and broadcast.
+    sizes_cuda = torch.tensor(sizes, dtype=torch.long, device='cuda')
+    torch.distributed.broadcast(
+        sizes_cuda, get_tensor_model_parallel_src_rank(), group=get_tensor_model_parallel_group()
+    )
+
+    # Move back to cpu and unpack.
+    sizes_cpu = sizes_cuda.cpu()
+    key_size = {}
+    key_numel = {}
+    total_numel = 0
+    offset = 0
+    for key in keys:
+        i = 0
+        size = []
+        numel = 1
+        # support to zero-size tensor, such as [0, 128]
+        while sizes_cpu[offset + i] >= 0:
+            this_size = sizes_cpu[offset + i]
+            size.append(this_size)
+            numel *= this_size
+            i += 1
+        key_size[key] = size
+        key_numel[key] = numel
+        total_numel += numel
+        offset += max_dim
+
+    return key_size, key_numel, total_numel
+
+
+def broadcast_data(keys, data, datatype):
+    """Broadcast data from rank zero of each model parallel group to the
+    members of the same model parallel group.
+
+    Args:
+        keys: list of keys in the data disctionary to be broadcasted
+        data: data dictionary of string keys and cpu tensor values.
+        datatype: torch data type of all tensors in data associated
+                  with keys.
+    """
+    # Build (key, size) and (key, number of elements) dictionaries along
+    # with the total number of elements on all ranks.
+    key_size, key_numel, total_numel = _build_key_size_numel_dictionaries(keys, data)
+
+    # Pack on rank zero.
+    if get_tensor_model_parallel_rank() == 0:
+        # Check that all keys have the same data type.
+        _check_data_types(keys, data, datatype)
+        # Flatten the data associated with the keys
+        flatten_data = torch.cat([data[key].contiguous().view(-1) for key in keys], dim=0).cuda()
+    else:
+        flatten_data = torch.empty(total_numel, device=torch.cuda.current_device(), dtype=datatype)
+
+    # Broadcast
+    torch.distributed.broadcast(
+        flatten_data, get_tensor_model_parallel_src_rank(), group=get_tensor_model_parallel_group()
+    )
+
+    # Unpack
+    output = {}
+    offset = 0
+    for key in keys:
+        size = key_size[key]
+        numel = key_numel[key]
+        output[key] = flatten_data.narrow(0, offset, numel).view(size)
+        offset += numel
+
+    return output
diff --git a/flagscale/train/models/qwen2_5_vl/transformer_config.py b/flagscale/train/models/qwen2_5_vl/transformer_config.py
index c138dbef..7fa32dca 100644
--- a/flagscale/train/models/qwen2_5_vl/transformer_config.py
+++ b/flagscale/train/models/qwen2_5_vl/transformer_config.py
@@ -1,92 +1,88 @@
-# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/transformer_config.py. Below is the original copyright:
-# Copyright (c) 2024 Alibaba PAI and Nvidia Megatron-LM Team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import torch
-from megatron.core import parallel_state
-
-
-def get_vision_model_config(args, config):
-    # Given a Transformer Config from decoder, build vision encoder config
-    # diff: out_hidden_size & intermediate_size
-
-    # mlp: hidden_size -> intermediate_size -> embed_dim, silu
-    # NOTE: here we provide a workaround to solve the wrong layer amount when VPP of decoder is on
-    if config.num_layers in[28, 36]:
-        config.ffn_hidden_size = 3420 # 7B 72B
-    else:
-        config.ffn_hidden_size = 3456 # 32B
-
-    if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
-        config.num_layers = 32 * parallel_state.get_virtual_pipeline_model_parallel_world_size() # depth
-    else:
-        config.num_layers = 32 # depth
-    config.num_attention_heads = 16 # num_heads
-    config.add_bias_linear = True # all nn.Linear has bias (MLP, attn)
-    config.add_qkv_bias = True # qkv_proj in attn has bias
-    config.hidden_size = 1280 # hidden_size
-    config.hidden_dropout = 0.0
-    config.attention_dropout = 0.0
-
-    # config.gated_linear_unit = False # no gated
-    # config.activation_func = quick_gelu # hidden_act
-    config.kv_channels = config.hidden_size // config.num_attention_heads
-    config.num_query_groups = config.num_attention_heads # no GQA
-    config.layernorm_zero_centered_gamma = False # False
-    config.apply_query_key_layer_scaling = False # factor=math.sqrt(head_dim)
-    config.bias_activation_fusion = False # no swiglu, set false
-    config.bias_dropout_fusion = False # no dropout, set false
-    config.attention_softmax_in_fp32 = True # use True
-    # config.normalization = 'LayerNorm' # use RMSNorm
-    config.seq_length = args.seq_length
-
-    config.tp_comm_overlap = False
-    config.sequence_parallel = False
-    config.temporal_patch_size = 2
-    config.patch_size = 14
-    config.in_channels = 3
-    config.spatial_merge_size = 2
-
-    config.fullatt_block_indexes = [7, 15, 23, 31]
-    config._qwen2_5_vl_window_size = 112
-    # NOTE(lizhyu): Add following configs from huggingface config
-    config.tokens_per_second = 2
-    # for pipeline parallelism
-    config.pipeline_model_parallel_size = 1
-    config.first_pipeline_num_layers = None
-    config.num_layers_in_first_pipeline_stage = None
-    config.num_layers_in_last_pipeline_stage = None
-    if args.vision_recompute_layer_steps != 0:
-        config.recompute_method="uniform"
-        config.recompute_granularity="full"
-        config.recompute_num_layers=args.vision_recompute_layer_steps # 16 for 32B
-    return config
-
-
-def get_vision_projection_config(config, embed_dim, spatial_merge_size):
-    # merger:
-    # context_dim = hidden_size * merge_size**2
-    # out_hidden_size = hidden_size
-    # context_dim -> context_dim -> out_hidden_size
-    # MLP:
-    # input_size -> ffn_hidden_size -> hidden_size
-    # spec: LN -> Linear(bias=True) -> GELU -> Linear(bias=True)
-    config.gated_linear_unit = False
-    config.bias_activation_fusion = False
-    config.add_bias_linear = True
-    config.ffn_hidden_size = embed_dim * (spatial_merge_size ** 2)
-    config.activation_func = torch.nn.functional.gelu
-    config.tp_comm_overlap = False
-    config.sequence_parallel = False
-    return config
+# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/transformer_config.py. Below is the original copyright:
+# Copyright (c) 2024 Alibaba PAI and Nvidia Megatron-LM Team.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+from megatron.core import parallel_state
+
+
+def get_vision_model_config(args, config):
+    # Given a Transformer Config from decoder, build vision encoder config
+    # diff: out_hidden_size & intermediate_size
+
+    # mlp: hidden_size -> intermediate_size -> embed_dim, silu
+    # NOTE: here we provide a workaround to solve the wrong layer amount when VPP of decoder is on
+    if config.num_layers in[28, 36]:
+        config.ffn_hidden_size = 3420 # 7B num_layers: 28
+    else:
+        config.ffn_hidden_size = 3456
+
+    if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
+        config.num_layers = 32 * parallel_state.get_virtual_pipeline_model_parallel_world_size() # depth
+    else:
+        config.num_layers = 32 # depth
+    config.num_attention_heads = 16 # num_heads
+    config.add_bias_linear = True # all nn.Linear has bias (MLP, attn)
+    config.add_qkv_bias = True # qkv_proj in attn has bias
+    config.hidden_size = 1280 # hidden_size
+    config.hidden_dropout = 0.0
+    config.attention_dropout = 0.0
+
+    # config.gated_linear_unit = False # no gated
+    # config.activation_func = quick_gelu # hidden_act
+    config.kv_channels = config.hidden_size // config.num_attention_heads
+    config.num_query_groups = config.num_attention_heads # no GQA
+    config.layernorm_zero_centered_gamma = False # False
+    config.apply_query_key_layer_scaling = False # factor=math.sqrt(head_dim)
+    config.bias_activation_fusion = False # no swiglu, set false
+    config.bias_dropout_fusion = False # no dropout, set false
+    config.attention_softmax_in_fp32 = True # use True
+    # config.normalization = 'LayerNorm' # use RMSNorm
+    config.seq_length = args.seq_length
+
+    config.tp_comm_overlap = False
+    config.sequence_parallel = False
+    config.temporal_patch_size = 2
+    config.patch_size = 14
+    config.in_channels = 3
+    config.spatial_merge_size = 2
+
+    config.fullatt_block_indexes = [7, 15, 23, 31]
+    config._qwen2_5_vl_window_size = 112
+    # NOTE(lizhyu): Add following configs from huggingface config
+    config.tokens_per_second = 2
+    # for pipeline parallelism
+    config.pipeline_model_parallel_size = 1
+    config.first_pipeline_num_layers = None
+    config.num_layers_in_first_pipeline_stage = None
+    config.num_layers_in_last_pipeline_stage = None
+    return config
+
+
+def get_vision_projection_config(config, embed_dim, spatial_merge_size):
+    # merger:
+    # context_dim = hidden_size * merge_size**2
+    # out_hidden_size = hidden_size
+    # context_dim -> context_dim -> out_hidden_size
+    # MLP:
+    # input_size -> ffn_hidden_size -> hidden_size
+    # spec: LN -> Linear(bias=True) -> GELU -> Linear(bias=True)
+    config.gated_linear_unit = False
+    config.bias_activation_fusion = False
+    config.add_bias_linear = True
+    config.ffn_hidden_size = embed_dim * (spatial_merge_size ** 2)
+    config.activation_func = torch.nn.functional.gelu
+    config.tp_comm_overlap = False
+    config.sequence_parallel = False
+    return config
diff --git a/flagscale/train/models/qwen2_5_vl/vision_attention.py b/flagscale/train/models/qwen2_5_vl/vision_attention.py
index f92a97b0..22978308 100644
--- a/flagscale/train/models/qwen2_5_vl/vision_attention.py
+++ b/flagscale/train/models/qwen2_5_vl/vision_attention.py
@@ -1,791 +1,791 @@
-# Copyright (c) 2025, BAAI. All rights reserved.
-#
-# Mainly adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_vl/attention_vision.py. Below is the original copyright:
-# Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved.
-
-from __future__ import annotations
-from abc import  abstractmethod
-from typing import Optional, Tuple, Union
-import logging
-logger = logging.getLogger(__name__)
-
-import torch
-from torch import Tensor
-
-from megatron.core.inference.contexts import BaseInferenceContext
-from megatron.core.packed_seq_params import PackedSeqParams
-from megatron.core.parallel_state import (
-    get_data_parallel_group,
-    get_data_parallel_rank,
-    get_data_parallel_world_size,
-    get_tensor_model_parallel_group,
-    get_tensor_model_parallel_rank,
-    get_tensor_model_parallel_world_size,
-)
-from megatron.core.process_groups_config import ModelCommProcessGroups
-from megatron.core.transformer.spec_utils import build_module
-from megatron.core.utils import deprecate_inference_params, is_fa_min_version
-
-from megatron.core.transformer.enums import AttnMaskType
-from megatron.core.transformer.transformer_config import TransformerConfig
-
-from megatron.core.transformer.attention import Attention, SelfAttentionSubmodules, CrossAttentionSubmodules
-try:
-    from einops import rearrange
-except ImportError:
-    rearrange = None
-
-try:
-    from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
-except:
-    flash_attn_varlen_func = None
-    flash_attn_with_kvcache = None
-
-
-try:
-    import transformer_engine  # pylint: disable=unused-import
-
-    HAVE_TE = True
-    from megatron.core.extensions.transformer_engine import SplitAlongDim
-except ImportError:
-    HAVE_TE = False
-    SplitAlongDim = None
-
-try:
-    from megatron.core.extensions.transformer_engine import (
-        fused_apply_rotary_pos_emb,
-    )
-
-    HAVE_APPLY_ROPE_FUSION = True
-except ImportError:
-    try:
-        from apex.transformer.functional import (
-            fused_apply_rotary_pos_emb,
-        )
-
-        HAVE_APPLY_ROPE_FUSION = True
-    except ImportError:
-        HAVE_APPLY_ROPE_FUSION = False
-
-
-try:
-    from flash_attn.layers.rotary import apply_rotary_emb as apply_rotary_emb_flash
-except ImportError:
-    apply_rotary_emb_flash = None
-
-from megatron.core.models.common.embeddings.rope_utils import _rotate_half
-
-
-def _apply_rotary_pos_emb_bshd_vision(
-    t: Tensor,
-    freqs: Tensor,
-    rotary_interleaved: bool = False,
-    multi_latent_attention: bool = False,
-    mscale: float = 1.0,
-) -> Tensor:
-    """Apply rotary positional embedding to input tensor T.
-
-    check https://kexue.fm/archives/8265 for detailed formulas
-
-    Args:
-        t (Tensor): Input tensor T is of shape [seq_length, ... , dim]
-        freqs (Tensor): Rotary Positional embedding tensor freq is of shape [seq_length, ..., dim]
-
-    Returns:
-        Tensor: The input tensor after applying RoPE
-    """
-    rot_dim = freqs.shape[-1]
-    input_dtype = t.dtype
-    t = t.float()
-    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t
-    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]
-
-    if multi_latent_attention:
-        x1 = t[..., 0::2]
-        x2 = t[..., 1::2]
-        t = torch.cat((x1, x2), dim=-1)
-
-    # first part is cosine component
-    # second part is sine component, need to change signs with _rotate_half method
-    cos_ = torch.cos(freqs).float()
-    sin_ = torch.sin(freqs).float()
-    t = (t * cos_) + (_rotate_half(t, rotary_interleaved) * sin_)
-    return torch.cat((t, t_pass), dim=-1).to(input_dtype)
-
-def apply_rotary_pos_emb_vision(
-    t: Tensor,
-    freqs: Tensor,
-    config: TransformerConfig,
-    cu_seqlens: Optional[Tensor] = None,
-    mscale: float = 1.0,
-    cp_group: torch.distributed.ProcessGroup = None,
-):
-    """
-    Reroute to the appropriate apply_rotary_pos_emb function depending on
-    fused/unfused kernels
-
-    NOTE: the RoPE of vision model should not be applied like thd-format because all
-    freqs of each token have been computed.
-    """
-
-    if config.apply_rope_fusion:
-        return fused_apply_rotary_pos_emb(t.unsqueeze(1), freqs).squeeze(1)
-
-    return _apply_rotary_pos_emb_bshd_vision(
-        t.unsqueeze(1),
-        freqs,
-        rotary_interleaved=config.rotary_interleaved,
-        multi_latent_attention=config.multi_latent_attention,
-        mscale=mscale,
-    ).squeeze(1)
-
-
-def apply_rotary_pos_emb_with_cos_sin_vision(
-    t: Tensor, cos: Tensor, sin: Tensor, rotary_interleaved: bool = False
-) -> Tensor:
-    """
-    This function applies rotary positional embedding to the target tensor t
-    using precomputed cos and sin of size (seq_len, d_rot / 2)
-    """
-    cos = cos.to(t.dtype)
-    sin = sin.to(t.dtype)
-
-    if apply_rotary_emb_flash is None:
-        # Combine cos and sin into freqs
-        freqs = torch.stack([cos, sin], dim=-1).flatten(start_dim=-2)
-
-        # Expand freqs to match t's shape
-        while freqs.dim() < t.dim():
-            freqs = freqs.unsqueeze(1)
-        freqs = freqs.expand(t.shape[:-1] + (-1,))
-
-        y = _apply_rotary_pos_emb_bshd_vision(
-            t,
-            freqs,
-            rotary_interleaved=rotary_interleaved,
-            multi_latent_attention=False,
-            mscale=1.0,
-        )
-    else:
-        # Use Flash Attention's optimized kernel for rotary embedding
-        t = t.permute(1, 0, 2, 3)
-        y = apply_rotary_emb_flash(t, cos, sin, rotary_interleaved)
-        y = y.permute(1, 0, 2, 3)
-
-    return y
-
-class VisionAttention(Attention):
-    """Attention layer abstract class.
-
-    This layer only contains common modules required for the "self attn" and
-    "cross attn" specializations.
-    """
-
-    def __init__(
-        self,
-        config: TransformerConfig,
-        submodules: Union[SelfAttentionSubmodules, CrossAttentionSubmodules],
-        layer_number: int,
-        attn_mask_type: AttnMaskType,
-        attention_type: str,
-        cp_comm_type: str = None,
-        model_comm_pgs: ModelCommProcessGroups = None,
-    ):
-        super().__init__(config=config, submodules=submodules, layer_number=layer_number, attn_mask_type=attn_mask_type, attention_type=attention_type, cp_comm_type=cp_comm_type, model_comm_pgs=model_comm_pgs)
-
-
-    def _adjust_key_value_for_inference(
-        self,
-        inference_context: BaseInferenceContext,
-        query: Tensor,
-        key: Tensor,
-        value: Tensor,
-        rotary_pos_emb: Tensor,
-        rotary_pos_cos: Optional[Tensor] = None,
-        rotary_pos_sin: Optional[Tensor] = None,
-        sequence_len_offset: Optional[int] = None,
-        *,
-        inference_params: Optional[BaseInferenceContext] = None,
-    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
-        """
-        Saves the generated key and value tensors to the end of the buffers in inference_context.
-        Returns the full size keys and values from the provided inference_context, as well as
-        adjusted rotary_pos_emb.
-
-        Args:
-            query (Tensor): Query tensor.
-            key (Tensor): Key tensor.
-            value (Tensor): Value tensor.
-            rotary_pos_emb (Optional[Union[Tensor, Tuple[Tensor, Tensor]]]): Rotary
-                embedding tensor(s).
-            rotary_pos_cos (Optional[Tensor]): Rotary embedding cosine.
-            rotary_pos_sin (Optional[Tensor]): Rotary embedding sine.
-            sequence_len_offset (Optional[int]): Sequence length offset used for
-                inference CUDA graphs.
-
-        Return:
-            Tuple of: query, key, value, rotary_pos_emb, attn_mask_type, block_table.
-        """
-
-        inference_context = deprecate_inference_params(inference_context, inference_params)
-
-        attn_mask_type = self.attn_mask_type
-        if inference_context is None:
-            return query, key, value, rotary_pos_emb, attn_mask_type, None
-
-        # =================================================
-        # Pre-allocate memory for key-values for inference.
-        # =================================================
-        if inference_context.is_static_batching():
-            if self.layer_number not in inference_context.key_value_memory_dict:
-                inf_max_seq_length = inference_context.max_sequence_length
-                inf_max_batch_size = inference_context.max_batch_size
-                inference_key_memory = self._allocate_memory(
-                    inf_max_seq_length, inf_max_batch_size, self.key_hidden_size, key.dtype
-                )
-                inference_value_memory = self._allocate_memory(
-                    inf_max_seq_length, inf_max_batch_size, self.val_hidden_size, value.dtype
-                )
-                inference_context.key_value_memory_dict[self.layer_number] = (
-                    inference_key_memory,
-                    inference_value_memory,
-                )
-            else:
-                # Get the pre-allocated buffers for this layer
-                inference_key_memory, inference_value_memory = (
-                    inference_context.key_value_memory_dict[self.layer_number]
-                )
-
-        if not inference_context.is_static_batching() or inference_context.sequence_len_offset > 0:
-            # This should mean that we are past the prompt forward_step
-            # and so we need to turn off masking
-            attn_mask_type = AttnMaskType.no_mask
-
-        if inference_context.is_static_batching():
-            batch_start = inference_context.batch_size_offset
-            batch_end = batch_start + key.size(1)
-            assert batch_end <= inference_key_memory.size(1)
-            sequence_start = inference_context.sequence_len_offset
-            sequence_end = sequence_start + key.size(0)
-            assert sequence_end <= inference_key_memory.size(0), (
-                "Current sequence length is longer than expected maximum sequence length! "
-                "Increase inference_max_seq_length."
-            )
-
-        if self.config.flash_decode:
-            rotary_pos_cos_q = None
-            rotary_pos_sin_q = None
-            rotary_pos_cos_k = None
-            rotary_pos_sin_k = None
-
-            assert inference_context.is_static_batching()
-            if (
-                inference_context.is_decode_only() and rotary_pos_cos is not None
-            ):  # Decode phase, not prefill
-                rotary_pos_cos_q = rotary_pos_cos[sequence_end - 1 : sequence_end]
-                rotary_pos_sin_q = rotary_pos_sin[sequence_end - 1 : sequence_end]
-                rotary_pos_cos_k = rotary_pos_cos[sequence_end - 1 : sequence_end]
-                rotary_pos_sin_k = rotary_pos_sin[sequence_end - 1 : sequence_end]
-            elif rotary_pos_cos is not None:  # Prefill
-                rotary_pos_cos_q = rotary_pos_cos[:sequence_end]
-                rotary_pos_sin_q = rotary_pos_sin[:sequence_end]
-                rotary_pos_cos_k = rotary_pos_cos[:sequence_end]
-                rotary_pos_sin_k = rotary_pos_sin[:sequence_end]
-
-            # Flash Decoding assumes that the keys stored in the KV Cache already have RoPE applied.
-            # Apply RoPE before we store the keys to make it compatible with flash decoding kernel
-            if rotary_pos_sin_q is not None and rotary_pos_sin_k is not None:
-                key = apply_rotary_pos_emb_with_cos_sin_vision(key, rotary_pos_cos_k, rotary_pos_sin_k)
-                query = apply_rotary_pos_emb_with_cos_sin_vision(query, rotary_pos_cos_q, rotary_pos_sin_q)
-        else:
-            rotary_pos_cos_q = None
-            rotary_pos_sin_q = None
-
-        # Adjust rotary embeddings.
-        if rotary_pos_emb is not None:
-            q_pos_emb, k_pos_emb = rotary_pos_emb
-            if inference_context.is_static_batching():
-                q_pos_emb = q_pos_emb[sequence_start:sequence_end, :, :, :]
-                k_pos_emb = k_pos_emb[:sequence_end, :, :, :]
-            else:
-                pass
-            rotary_pos_emb = (q_pos_emb, k_pos_emb)
-
-        block_table = None
-        if inference_context.is_static_batching():
-            # Copy key and values.
-            inference_key_memory[sequence_start:sequence_end, batch_start:batch_end, ...] = key
-            inference_value_memory[sequence_start:sequence_end, batch_start:batch_end, ...] = value
-            key = inference_key_memory[:sequence_end, batch_start:batch_end, ...]
-            value = inference_value_memory[:sequence_end, batch_start:batch_end, ...]
-        else:
-            # Apply rotary embeddings before appending KV cache.
-            if rotary_pos_emb is not None:
-                q_pos_emb, k_pos_emb = rotary_pos_emb
-                key = inference_context.apply_rotary_emb_key(
-                    key, k_pos_emb, self.config, self.model_comm_pgs.cp
-                )
-                rotary_pos_emb = (q_pos_emb, None)  # key rotary emb has been applied
-
-            # Append key/value data tensors to cache.
-            inference_context.append_key_value_cache(self.layer_number, key, value)
-
-            # Read key/value *pointer* tensors from cache.
-            key, value, block_table = inference_context.key_value_cache(self.layer_number)
-
-        return query, key, value, rotary_pos_emb, attn_mask_type, block_table
-
-    @abstractmethod
-    def get_query_key_value_tensors(self, hidden_states, key_value_states):
-        """
-        This method needs to be implemented based on whether the derived class
-        is "self-attn" or "cross-attn".
-        """
-
-    def flash_decoding(
-        self,
-        sequence_len_offset: Tensor,
-        query_layer: Tensor,
-        key_layer: Tensor,
-        value_layer: Tensor,
-        inference_key_memory: Tensor,
-        inference_value_memory: Tensor,
-        rotary_cos: Tensor,
-        rotary_sin: Tensor,
-    ) -> (Tensor, Tensor):
-        """
-        The flash decoding kernel will do the following in a single execution:
-        1. Compute RoPE embedding with precomputed cos & sin tensors
-        2. Update the KV Cache
-        3. Performs the flash attention operation
-        """
-        assert flash_attn_with_kvcache is not None, (
-            "Flash Decoding requires the flash_attn_with_kvcache kernel, "
-            "available in the flash-attn package."
-        )
-        cache_seqlens = sequence_len_offset - 1
-        q = query_layer.permute(1, 0, 2, 3)
-        k = key_layer.permute(1, 0, 2, 3)
-        v = value_layer.permute(1, 0, 2, 3)
-        k_cache = inference_key_memory.permute(1, 0, 2, 3)
-        v_cache = inference_value_memory.permute(1, 0, 2, 3)
-
-        if rotary_cos is not None:
-            rotary_cos = rotary_cos.to(query_layer.dtype)
-        if rotary_sin is not None:
-            rotary_sin = rotary_sin.to(query_layer.dtype)
-
-        out = flash_attn_with_kvcache(
-            q=q,
-            k_cache=k_cache,
-            v_cache=v_cache,
-            k=k,
-            v=v,
-            rotary_cos=rotary_cos,
-            rotary_sin=rotary_sin,
-            cache_seqlens=cache_seqlens,
-            rotary_interleaved=False,
-        )
-        return out
-
-    def forward(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Tensor,
-        key_value_states: Optional[Tensor] = None,
-        inference_context: Optional[BaseInferenceContext] = None,
-        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,
-        rotary_pos_cos: Optional[Tensor] = None,
-        rotary_pos_sin: Optional[Tensor] = None,
-        attention_bias: Optional[Tensor] = None,
-        packed_seq_params: Optional[PackedSeqParams] = None,
-        sequence_len_offset: Optional[int] = None,
-        *,
-        inference_params: Optional[BaseInferenceContext] = None,
-    ) -> Tuple[Tensor, Tensor]:
-        """
-        Perform a forward pass through the attention module.
-
-        Args:
-            hidden_states (Tensor): Hidden states.
-            attention_mask (Tensor): Attention mask.
-            key_value_states (Optional[Tensor]): Key/value states (for cross attention).
-            inference_context (Optional[BaseInferenceContext]): Inference context that manages
-                KV cache.
-            rotary_pos_emb (Optional[Union[Tensor, Tuple[Tensor, Tensor]]]): Rotary
-                embedding tensor(s).
-            rotary_pos_cos (Optional[Tensor]): Rotary embedding cosine.
-            rotary_pos_sin (Optional[Tensor]): Rotary embedding sine.
-            attention_bias (Optional[Tensor]): Attention bias.
-            packed_seq_params (Optional[PackedSeqparams]): Parameters used for THD format.
-            sequence_len_offset (Optional[int]): Sequence length offset used for
-                inference CUDA graphs.
-
-        Return:
-            (Tuple[Tensor, Tensor]) Attention output and bias.
-
-        """
-
-        inference_context = deprecate_inference_params(inference_context, inference_params)
-
-        if inference_context and inference_context.is_dynamic_batching():
-            assert is_fa_min_version(
-                "2.7.3"
-            ), "flash attn verion v2.7.3 and above is required for dynamic batching."
-
-        # hidden_states: [sq, b, h]
-        if self.config.flash_decode and not self.training and inference_context is not None:
-            rotary_pos_emb = None
-        else:
-            assert rotary_pos_cos is None and rotary_pos_sin is None
-
-        # For self attention we just duplicate the rotary_pos_emb if it isn't already
-        if rotary_pos_emb is not None and not isinstance(rotary_pos_emb, tuple):
-            rotary_pos_emb = (rotary_pos_emb,) * 2
-
-        # =====================
-        # Query, Key, and Value
-        # =====================
-        # Get the query, key and value tensors based on the type of attention -
-        # self or cross attn.
-        query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)
-
-        # ===================================================
-        # Adjust key, value, and rotary_pos_emb for inference
-        # ===================================================
-
-        # This branch only runs in the decode phase of flash decoding and returns after the linear
-        # projection. This conditional is not used in the prefill phase or non-flash-decoding cases.
-        if (
-            self.config.flash_decode
-            and inference_context is not None
-            and inference_context.is_decode_only()
-            and not self.training
-            and rotary_pos_cos is not None
-        ):
-            assert self.layer_number in inference_context.key_value_memory_dict
-            assert inference_context.sequence_len_offset is not None
-            inference_key_memory, inference_value_memory = inference_context.key_value_memory_dict[
-                self.layer_number
-            ]
-            output = self.flash_decode(
-                sequence_len_offset=sequence_len_offset,
-                query_layer=query,
-                key_layer=key,
-                value_layer=value,
-                inference_key_memory=inference_key_memory,
-                inference_value_memory=inference_value_memory,
-                rotary_cos=rotary_pos_cos,
-                rotary_sin=rotary_pos_sin,
-            )
-            out = output.transpose(0, 1).contiguous()
-            context_layer = out.view(out.size(0), out.size(1), -1)
-            output, bias = self.linear_proj(context_layer)
-            return output, bias
-
-        query, key, value, rotary_pos_emb, attn_mask_type, block_table = (
-            self._adjust_key_value_for_inference(
-                inference_context,
-                query,
-                key,
-                value,
-                rotary_pos_emb,
-                rotary_pos_cos,
-                rotary_pos_sin,
-                sequence_len_offset,
-            )
-        )
-
-        if packed_seq_params is not None:
-            query = query.squeeze(1)
-            key = key.squeeze(1)
-            value = value.squeeze(1)
-
-        # ================================================
-        # relative positional embedding (rotary embedding)
-        # ================================================
-        if rotary_pos_emb is not None and not self.config.flash_decode:
-            q_pos_emb, k_pos_emb = rotary_pos_emb
-
-            if packed_seq_params is not None:
-                if packed_seq_params.cu_seqlens_q_padded is not None:
-                    cu_seqlens_q = packed_seq_params.cu_seqlens_q_padded
-                else:
-                    cu_seqlens_q = packed_seq_params.cu_seqlens_q
-                if packed_seq_params.cu_seqlens_kv_padded is not None:
-                    cu_seqlens_kv = packed_seq_params.cu_seqlens_kv_padded
-                else:
-                    cu_seqlens_kv = packed_seq_params.cu_seqlens_kv
-            else:
-                cu_seqlens_q = cu_seqlens_kv = None
-
-            if q_pos_emb is not None:
-                # TODO VIJAY: simplify
-                if inference_context is None or inference_context.is_static_batching():
-                    query = apply_rotary_pos_emb_vision(
-                        query,
-                        q_pos_emb,
-                        config=self.config,
-                        cu_seqlens=cu_seqlens_q,
-                        cp_group=self.model_comm_pgs.cp,
-                    )
-                else:
-                    query = inference_context.apply_rotary_emb_query(
-                        query, q_pos_emb, self.config, cu_seqlens_q, self.model_comm_pgs.cp
-                    )
-            if k_pos_emb is not None:
-                key = apply_rotary_pos_emb_vision(
-                    key,
-                    k_pos_emb,
-                    config=self.config,
-                    cu_seqlens=cu_seqlens_kv,
-                    cp_group=self.model_comm_pgs.cp,
-                )
-
-            # TODO, can apply positional embedding to value_layer so it has
-            # absolute positional embedding.
-            # otherwise, only relative positional embedding takes effect
-            # value_layer = apply_rotary_pos_emb_vision(value_layer, k_pos_emb)
-
-        # ==================================
-        # core attention computation
-        # ==================================
-
-        if self.checkpoint_core_attention and self.training:
-            core_attn_out = self._checkpointed_attention_forward(
-                query,
-                key,
-                value,
-                attention_mask,
-                attn_mask_type=attn_mask_type,
-                attention_bias=attention_bias,
-                packed_seq_params=packed_seq_params,
-            )
-        else:
-            if inference_context is None or inference_context.is_static_batching():
-                # Static batching attention kernel.
-                core_attn_out = self.core_attention(
-                    query,
-                    key,
-                    value,
-                    attention_mask,
-                    attn_mask_type=attn_mask_type,
-                    attention_bias=attention_bias,
-                    packed_seq_params=packed_seq_params,
-                )
-
-            else:
-                # Dynamic batching attention kernel.
-                q, k, v = (query, key, value)
-                cu_query_lengths, max_seqlen_q = inference_context.cu_query_lengths()
-                cu_kv_lengths, kv_lengths, max_seqlen_k = inference_context.cu_kv_lengths()
-
-                core_attn_out = self.flash_decode_and_prefill(
-                    q,
-                    k,
-                    v,
-                    max_seqlen_q,
-                    max_seqlen_k,
-                    cu_query_lengths,
-                    cu_kv_lengths,
-                    kv_lengths,
-                    block_table,
-                )
-                core_attn_out = rearrange(core_attn_out, 's b h d -> s b (h d)')
-
-        if packed_seq_params is not None and packed_seq_params.qkv_format == 'thd':
-            # reshape to same output shape as unpacked case
-            # (t, np, hn) -> (t, b=1, h=np*hn)
-            # t is the pack size = sum (sq_i)
-            # note that batch is a dummy dimension in the packed case
-            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)
-
-        # =================
-        # Output. [sq, b, h]
-        # =================
-
-        output, bias = self.linear_proj(core_attn_out)
-
-        return output, bias
-
-class SelfAttentionVision(VisionAttention):
-    """Self-attention layer class
-
-    Self-attention layer takes input with size [s, b, h]
-    and returns output of the same size.
-    """
-
-    def __init__(
-        self,
-        config: TransformerConfig,
-        submodules: SelfAttentionSubmodules,
-        layer_number: int,
-        attn_mask_type=AttnMaskType.padding,
-        cp_comm_type: str = None,
-        model_comm_pgs: ModelCommProcessGroups = None,
-    ):
-        super().__init__(
-            config=config,
-            submodules=submodules,
-            layer_number=layer_number,
-            attn_mask_type=attn_mask_type,
-            attention_type="self",
-            cp_comm_type=cp_comm_type,
-            model_comm_pgs=model_comm_pgs,
-        )
-
-        self.linear_qkv = build_module(
-            submodules.linear_qkv,
-            self.config.hidden_size,
-            self.query_projection_size + 2 * self.kv_projection_size,
-            config=self.config,
-            init_method=self.config.init_method,
-            gather_output=False,
-            bias=self.config.add_bias_linear or self.config.add_qkv_bias,
-            skip_bias_add=False,
-            is_expert=False,
-            tp_comm_buffer_name='qkv',
-        )
-
-        if submodules.q_layernorm is not None:
-            self.q_layernorm = build_module(
-                submodules.q_layernorm,
-                hidden_size=self.hidden_size_per_attention_head,
-                config=self.config,
-                eps=self.config.layernorm_epsilon,
-            )
-        else:
-            self.q_layernorm = None
-
-        if submodules.k_layernorm is not None:
-            self.k_layernorm = build_module(
-                submodules.k_layernorm,
-                hidden_size=self.hidden_size_per_attention_head,
-                config=self.config,
-                eps=self.config.layernorm_epsilon,
-            )
-        else:
-            self.k_layernorm = None
-
-    def run_realtime_tests(self):
-        """Performs a consistency check.
-
-        This function makes sure that tensors across devices are the same during an experiment.
-        This is often not guaranteed to be so because of silent hardware failures (eg, memory
-        corruption loading a checkpoint, network traffic corruption encountered during
-        data transmission).
-
-        (TODO) In the future, more tensors should be checked across the training run and
-        checked every X iterations. This is left for future work. Equality of tensors is probably
-        not required; transmitting hashes is sufficient."""
-
-        if not self.config.qk_layernorm:
-            return
-
-        # check that all tensor parallel and data parallel ranks have the same
-        # Q & K layernorm parameters.
-        rank = get_data_parallel_rank()
-        inputs = torch.stack(
-            [
-                self.q_layernorm.weight.data,
-                self.q_layernorm.bias.data,
-                self.k_layernorm.weight.data,
-                self.k_layernorm.bias.data,
-            ]
-        )
-        dp_list = [torch.empty_like(inputs) for _ in range(get_data_parallel_world_size())]
-        dp_list[rank] = inputs
-        torch.distributed.all_gather(dp_list, inputs, group=get_data_parallel_group())
-
-        def _compare(srcs, tgts, names, parallelism):
-            assert len(srcs) == len(tgts) == len(names)
-            for src, tgt, name in zip(srcs, tgts, names):
-                assert torch.all(src == tgt), (
-                    f"Discrepancy between {name} in {parallelism} ranks {i} and {rank}. "
-                    f"Diff: {torch.norm(src - tgt)}"
-                )
-
-        for i, dp in enumerate(dp_list):
-            q_w, q_b, k_w, k_b = torch.unbind(dp)
-            _compare(
-                [q_w, q_b, k_w, k_b],
-                [
-                    self.q_layernorm.weight.data,
-                    self.q_layernorm.bias.data,
-                    self.k_layernorm.weight.data,
-                    self.k_layernorm.bias.data,
-                ],
-                ["q_w", "q_b", "k_w", "k_b"],
-                "DP",
-            )
-
-        rank = get_tensor_model_parallel_rank()
-        tp_list = [torch.empty_like(inputs) for _ in range(get_tensor_model_parallel_world_size())]
-        tp_list[rank] = inputs
-        torch.distributed.all_gather(tp_list, inputs, group=get_tensor_model_parallel_group())
-
-        for i, tp in enumerate(tp_list):
-            q_w, q_b, k_w, k_b = torch.unbind(tp)
-            _compare(
-                [q_w, q_b, k_w, k_b],
-                [
-                    self.q_layernorm.weight.data,
-                    self.q_layernorm.bias.data,
-                    self.k_layernorm.weight.data,
-                    self.k_layernorm.bias.data,
-                ],
-                ["q_w", "q_b", "k_w", "k_b"],
-                "TP",
-            )
-
-    def get_query_key_value_tensors(self, hidden_states, key_value_states=None):
-        """
-        Derives `query`, `key` and `value` tensors from `hidden_states`.
-        """
-        # Attention heads [sq, b, h] --> [sq, b, ng * (np/ng + 2) * hn)]
-        mixed_qkv, _ = self.linear_qkv(hidden_states)
-
-        # [sq, b, hp] --> [sq, b, ng, (np/ng + 2) * hn]
-        new_tensor_shape = mixed_qkv.size()[:-1] + (
-            self.num_query_groups_per_partition,
-            (
-                (self.num_attention_heads_per_partition // self.num_query_groups_per_partition + 2)
-                * self.hidden_size_per_attention_head
-            ),
-        )
-        mixed_qkv = mixed_qkv.view(*new_tensor_shape)
-
-        split_arg_list = [
-            (
-                self.num_attention_heads_per_partition
-                // self.num_query_groups_per_partition
-                * self.hidden_size_per_attention_head
-            ),
-            self.hidden_size_per_attention_head,
-            self.hidden_size_per_attention_head,
-        ]
-
-        if SplitAlongDim is not None:
-
-            # [sq, b, ng, (np/ng + 2) * hn]
-            # --> [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]
-            (query, key, value) = SplitAlongDim(mixed_qkv, 3, split_arg_list)
-        else:
-
-            # [sq, b, ng, (np/ng + 2) * hn]
-            # --> [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]
-            (query, key, value) = torch.split(mixed_qkv, split_arg_list, dim=3)
-
-        # [sq, b, ng, np/ng * hn] -> [sq, b, np, hn]
-        query = query.reshape(query.size(0), query.size(1), -1, self.hidden_size_per_attention_head)
-
-        if self.q_layernorm is not None:
-            query = self.q_layernorm(query)
-
-        if self.k_layernorm is not None:
-            key = self.k_layernorm(key)
-
-        if self.config.test_mode:
-            self.run_realtime_tests()
-
-        return query, key, value
+# Copyright (c) 2025, BAAI. All rights reserved.
+#
+# Mainly adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_vl/attention_vision.py. Below is the original copyright:
+# Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved.
+
+from __future__ import annotations
+from abc import  abstractmethod
+from typing import Optional, Tuple, Union
+import logging
+logger = logging.getLogger(__name__)
+
+import torch
+from torch import Tensor
+
+from megatron.core.inference.contexts import BaseInferenceContext
+from megatron.core.packed_seq_params import PackedSeqParams
+from megatron.core.parallel_state import (
+    get_data_parallel_group,
+    get_data_parallel_rank,
+    get_data_parallel_world_size,
+    get_tensor_model_parallel_group,
+    get_tensor_model_parallel_rank,
+    get_tensor_model_parallel_world_size,
+)
+from megatron.core.process_groups_config import ModelCommProcessGroups
+from megatron.core.transformer.spec_utils import build_module
+from megatron.core.utils import deprecate_inference_params, is_fa_min_version
+
+from megatron.core.transformer.enums import AttnMaskType
+from megatron.core.transformer.transformer_config import TransformerConfig
+
+from megatron.core.transformer.attention import Attention, SelfAttentionSubmodules, CrossAttentionSubmodules
+try:
+    from einops import rearrange
+except ImportError:
+    rearrange = None
+
+try:
+    from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
+except:
+    flash_attn_varlen_func = None
+    flash_attn_with_kvcache = None
+
+
+try:
+    import transformer_engine  # pylint: disable=unused-import
+
+    HAVE_TE = True
+    from megatron.core.extensions.transformer_engine import SplitAlongDim
+except ImportError:
+    HAVE_TE = False
+    SplitAlongDim = None
+
+try:
+    from megatron.core.extensions.transformer_engine import (
+        fused_apply_rotary_pos_emb,
+    )
+
+    HAVE_APPLY_ROPE_FUSION = True
+except ImportError:
+    try:
+        from apex.transformer.functional import (
+            fused_apply_rotary_pos_emb,
+        )
+
+        HAVE_APPLY_ROPE_FUSION = True
+    except ImportError:
+        HAVE_APPLY_ROPE_FUSION = False
+
+
+try:
+    from flash_attn.layers.rotary import apply_rotary_emb as apply_rotary_emb_flash
+except ImportError:
+    apply_rotary_emb_flash = None
+
+from megatron.core.models.common.embeddings.rope_utils import _rotate_half
+
+
+def _apply_rotary_pos_emb_bshd_vision(
+    t: Tensor,
+    freqs: Tensor,
+    rotary_interleaved: bool = False,
+    multi_latent_attention: bool = False,
+    mscale: float = 1.0,
+) -> Tensor:
+    """Apply rotary positional embedding to input tensor T.
+
+    check https://kexue.fm/archives/8265 for detailed formulas
+
+    Args:
+        t (Tensor): Input tensor T is of shape [seq_length, ... , dim]
+        freqs (Tensor): Rotary Positional embedding tensor freq is of shape [seq_length, ..., dim]
+
+    Returns:
+        Tensor: The input tensor after applying RoPE
+    """
+    rot_dim = freqs.shape[-1]
+    input_dtype = t.dtype
+    t = t.to(freqs.dtype)
+    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t
+    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]
+
+    if multi_latent_attention:
+        x1 = t[..., 0::2]
+        x2 = t[..., 1::2]
+        t = torch.cat((x1, x2), dim=-1)
+
+    # first part is cosine component
+    # second part is sine component, need to change signs with _rotate_half method
+    cos_ = (torch.cos(freqs) * mscale)
+    sin_ = (torch.sin(freqs) * mscale)
+    t = (t * cos_) + (_rotate_half(t, rotary_interleaved) * sin_)
+    return torch.cat((t, t_pass), dim=-1).to(input_dtype)
+
+def apply_rotary_pos_emb_vision(
+    t: Tensor,
+    freqs: Tensor,
+    config: TransformerConfig,
+    cu_seqlens: Optional[Tensor] = None,
+    mscale: float = 1.0,
+    cp_group: torch.distributed.ProcessGroup = None,
+):
+    """
+    Reroute to the appropriate apply_rotary_pos_emb function depending on
+    fused/unfused kernels
+
+    NOTE: the RoPE of vision model should not be applied like thd-format because all
+    freqs of each token have been computed.
+    """
+
+    if config.apply_rope_fusion:
+        return fused_apply_rotary_pos_emb(t.unsqueeze(1), freqs).squeeze(1)
+
+    return _apply_rotary_pos_emb_bshd_vision(
+        t.unsqueeze(1),
+        freqs,
+        rotary_interleaved=config.rotary_interleaved,
+        multi_latent_attention=config.multi_latent_attention,
+        mscale=mscale,
+    ).squeeze(1)
+
+
+def apply_rotary_pos_emb_with_cos_sin_vision(
+    t: Tensor, cos: Tensor, sin: Tensor, rotary_interleaved: bool = False
+) -> Tensor:
+    """
+    This function applies rotary positional embedding to the target tensor t
+    using precomputed cos and sin of size (seq_len, d_rot / 2)
+    """
+    cos = cos.to(t.dtype)
+    sin = sin.to(t.dtype)
+
+    if apply_rotary_emb_flash is None:
+        # Combine cos and sin into freqs
+        freqs = torch.stack([cos, sin], dim=-1).flatten(start_dim=-2)
+
+        # Expand freqs to match t's shape
+        while freqs.dim() < t.dim():
+            freqs = freqs.unsqueeze(1)
+        freqs = freqs.expand(t.shape[:-1] + (-1,))
+
+        y = _apply_rotary_pos_emb_bshd_vision(
+            t,
+            freqs,
+            rotary_interleaved=rotary_interleaved,
+            multi_latent_attention=False,
+            mscale=1.0,
+        )
+    else:
+        # Use Flash Attention's optimized kernel for rotary embedding
+        t = t.permute(1, 0, 2, 3)
+        y = apply_rotary_emb_flash(t, cos, sin, rotary_interleaved)
+        y = y.permute(1, 0, 2, 3)
+
+    return y
+
+class VisionAttention(Attention):
+    """Attention layer abstract class.
+
+    This layer only contains common modules required for the "self attn" and
+    "cross attn" specializations.
+    """
+
+    def __init__(
+        self,
+        config: TransformerConfig,
+        submodules: Union[SelfAttentionSubmodules, CrossAttentionSubmodules],
+        layer_number: int,
+        attn_mask_type: AttnMaskType,
+        attention_type: str,
+        cp_comm_type: str = None,
+        model_comm_pgs: ModelCommProcessGroups = None,
+    ):
+        super().__init__(config=config, submodules=submodules, layer_number=layer_number, attn_mask_type=attn_mask_type, attention_type=attention_type, cp_comm_type=cp_comm_type, model_comm_pgs=model_comm_pgs)
+
+
+    def _adjust_key_value_for_inference(
+        self,
+        inference_context: BaseInferenceContext,
+        query: Tensor,
+        key: Tensor,
+        value: Tensor,
+        rotary_pos_emb: Tensor,
+        rotary_pos_cos: Optional[Tensor] = None,
+        rotary_pos_sin: Optional[Tensor] = None,
+        sequence_len_offset: Optional[int] = None,
+        *,
+        inference_params: Optional[BaseInferenceContext] = None,
+    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
+        """
+        Saves the generated key and value tensors to the end of the buffers in inference_context.
+        Returns the full size keys and values from the provided inference_context, as well as
+        adjusted rotary_pos_emb.
+
+        Args:
+            query (Tensor): Query tensor.
+            key (Tensor): Key tensor.
+            value (Tensor): Value tensor.
+            rotary_pos_emb (Optional[Union[Tensor, Tuple[Tensor, Tensor]]]): Rotary
+                embedding tensor(s).
+            rotary_pos_cos (Optional[Tensor]): Rotary embedding cosine.
+            rotary_pos_sin (Optional[Tensor]): Rotary embedding sine.
+            sequence_len_offset (Optional[int]): Sequence length offset used for
+                inference CUDA graphs.
+
+        Return:
+            Tuple of: query, key, value, rotary_pos_emb, attn_mask_type, block_table.
+        """
+
+        inference_context = deprecate_inference_params(inference_context, inference_params)
+
+        attn_mask_type = self.attn_mask_type
+        if inference_context is None:
+            return query, key, value, rotary_pos_emb, attn_mask_type, None
+
+        # =================================================
+        # Pre-allocate memory for key-values for inference.
+        # =================================================
+        if inference_context.is_static_batching():
+            if self.layer_number not in inference_context.key_value_memory_dict:
+                inf_max_seq_length = inference_context.max_sequence_length
+                inf_max_batch_size = inference_context.max_batch_size
+                inference_key_memory = self._allocate_memory(
+                    inf_max_seq_length, inf_max_batch_size, self.key_hidden_size, key.dtype
+                )
+                inference_value_memory = self._allocate_memory(
+                    inf_max_seq_length, inf_max_batch_size, self.val_hidden_size, value.dtype
+                )
+                inference_context.key_value_memory_dict[self.layer_number] = (
+                    inference_key_memory,
+                    inference_value_memory,
+                )
+            else:
+                # Get the pre-allocated buffers for this layer
+                inference_key_memory, inference_value_memory = (
+                    inference_context.key_value_memory_dict[self.layer_number]
+                )
+
+        if not inference_context.is_static_batching() or inference_context.sequence_len_offset > 0:
+            # This should mean that we are past the prompt forward_step
+            # and so we need to turn off masking
+            attn_mask_type = AttnMaskType.no_mask
+
+        if inference_context.is_static_batching():
+            batch_start = inference_context.batch_size_offset
+            batch_end = batch_start + key.size(1)
+            assert batch_end <= inference_key_memory.size(1)
+            sequence_start = inference_context.sequence_len_offset
+            sequence_end = sequence_start + key.size(0)
+            assert sequence_end <= inference_key_memory.size(0), (
+                "Current sequence length is longer than expected maximum sequence length! "
+                "Increase inference_max_seq_length."
+            )
+
+        if self.config.flash_decode:
+            rotary_pos_cos_q = None
+            rotary_pos_sin_q = None
+            rotary_pos_cos_k = None
+            rotary_pos_sin_k = None
+
+            assert inference_context.is_static_batching()
+            if (
+                inference_context.is_decode_only() and rotary_pos_cos is not None
+            ):  # Decode phase, not prefill
+                rotary_pos_cos_q = rotary_pos_cos[sequence_end - 1 : sequence_end]
+                rotary_pos_sin_q = rotary_pos_sin[sequence_end - 1 : sequence_end]
+                rotary_pos_cos_k = rotary_pos_cos[sequence_end - 1 : sequence_end]
+                rotary_pos_sin_k = rotary_pos_sin[sequence_end - 1 : sequence_end]
+            elif rotary_pos_cos is not None:  # Prefill
+                rotary_pos_cos_q = rotary_pos_cos[:sequence_end]
+                rotary_pos_sin_q = rotary_pos_sin[:sequence_end]
+                rotary_pos_cos_k = rotary_pos_cos[:sequence_end]
+                rotary_pos_sin_k = rotary_pos_sin[:sequence_end]
+
+            # Flash Decoding assumes that the keys stored in the KV Cache already have RoPE applied.
+            # Apply RoPE before we store the keys to make it compatible with flash decoding kernel
+            if rotary_pos_sin_q is not None and rotary_pos_sin_k is not None:
+                key = apply_rotary_pos_emb_with_cos_sin_vision(key, rotary_pos_cos_k, rotary_pos_sin_k)
+                query = apply_rotary_pos_emb_with_cos_sin_vision(query, rotary_pos_cos_q, rotary_pos_sin_q)
+        else:
+            rotary_pos_cos_q = None
+            rotary_pos_sin_q = None
+
+        # Adjust rotary embeddings.
+        if rotary_pos_emb is not None:
+            q_pos_emb, k_pos_emb = rotary_pos_emb
+            if inference_context.is_static_batching():
+                q_pos_emb = q_pos_emb[sequence_start:sequence_end, :, :, :]
+                k_pos_emb = k_pos_emb[:sequence_end, :, :, :]
+            else:
+                pass
+            rotary_pos_emb = (q_pos_emb, k_pos_emb)
+
+        block_table = None
+        if inference_context.is_static_batching():
+            # Copy key and values.
+            inference_key_memory[sequence_start:sequence_end, batch_start:batch_end, ...] = key
+            inference_value_memory[sequence_start:sequence_end, batch_start:batch_end, ...] = value
+            key = inference_key_memory[:sequence_end, batch_start:batch_end, ...]
+            value = inference_value_memory[:sequence_end, batch_start:batch_end, ...]
+        else:
+            # Apply rotary embeddings before appending KV cache.
+            if rotary_pos_emb is not None:
+                q_pos_emb, k_pos_emb = rotary_pos_emb
+                key = inference_context.apply_rotary_emb_key(
+                    key, k_pos_emb, self.config, self.model_comm_pgs.cp
+                )
+                rotary_pos_emb = (q_pos_emb, None)  # key rotary emb has been applied
+
+            # Append key/value data tensors to cache.
+            inference_context.append_key_value_cache(self.layer_number, key, value)
+
+            # Read key/value *pointer* tensors from cache.
+            key, value, block_table = inference_context.key_value_cache(self.layer_number)
+
+        return query, key, value, rotary_pos_emb, attn_mask_type, block_table
+
+    @abstractmethod
+    def get_query_key_value_tensors(self, hidden_states, key_value_states):
+        """
+        This method needs to be implemented based on whether the derived class
+        is "self-attn" or "cross-attn".
+        """
+
+    def flash_decoding(
+        self,
+        sequence_len_offset: Tensor,
+        query_layer: Tensor,
+        key_layer: Tensor,
+        value_layer: Tensor,
+        inference_key_memory: Tensor,
+        inference_value_memory: Tensor,
+        rotary_cos: Tensor,
+        rotary_sin: Tensor,
+    ) -> (Tensor, Tensor):
+        """
+        The flash decoding kernel will do the following in a single execution:
+        1. Compute RoPE embedding with precomputed cos & sin tensors
+        2. Update the KV Cache
+        3. Performs the flash attention operation
+        """
+        assert flash_attn_with_kvcache is not None, (
+            "Flash Decoding requires the flash_attn_with_kvcache kernel, "
+            "available in the flash-attn package."
+        )
+        cache_seqlens = sequence_len_offset - 1
+        q = query_layer.permute(1, 0, 2, 3)
+        k = key_layer.permute(1, 0, 2, 3)
+        v = value_layer.permute(1, 0, 2, 3)
+        k_cache = inference_key_memory.permute(1, 0, 2, 3)
+        v_cache = inference_value_memory.permute(1, 0, 2, 3)
+
+        if rotary_cos is not None:
+            rotary_cos = rotary_cos.to(query_layer.dtype)
+        if rotary_sin is not None:
+            rotary_sin = rotary_sin.to(query_layer.dtype)
+
+        out = flash_attn_with_kvcache(
+            q=q,
+            k_cache=k_cache,
+            v_cache=v_cache,
+            k=k,
+            v=v,
+            rotary_cos=rotary_cos,
+            rotary_sin=rotary_sin,
+            cache_seqlens=cache_seqlens,
+            rotary_interleaved=False,
+        )
+        return out
+
+    def forward(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Tensor,
+        key_value_states: Optional[Tensor] = None,
+        inference_context: Optional[BaseInferenceContext] = None,
+        rotary_pos_emb: Optional[Union[Tensor, Tuple[Tensor, Tensor]]] = None,
+        rotary_pos_cos: Optional[Tensor] = None,
+        rotary_pos_sin: Optional[Tensor] = None,
+        attention_bias: Optional[Tensor] = None,
+        packed_seq_params: Optional[PackedSeqParams] = None,
+        sequence_len_offset: Optional[int] = None,
+        *,
+        inference_params: Optional[BaseInferenceContext] = None,
+    ) -> Tuple[Tensor, Tensor]:
+        """
+        Perform a forward pass through the attention module.
+
+        Args:
+            hidden_states (Tensor): Hidden states.
+            attention_mask (Tensor): Attention mask.
+            key_value_states (Optional[Tensor]): Key/value states (for cross attention).
+            inference_context (Optional[BaseInferenceContext]): Inference context that manages
+                KV cache.
+            rotary_pos_emb (Optional[Union[Tensor, Tuple[Tensor, Tensor]]]): Rotary
+                embedding tensor(s).
+            rotary_pos_cos (Optional[Tensor]): Rotary embedding cosine.
+            rotary_pos_sin (Optional[Tensor]): Rotary embedding sine.
+            attention_bias (Optional[Tensor]): Attention bias.
+            packed_seq_params (Optional[PackedSeqparams]): Parameters used for THD format.
+            sequence_len_offset (Optional[int]): Sequence length offset used for
+                inference CUDA graphs.
+
+        Return:
+            (Tuple[Tensor, Tensor]) Attention output and bias.
+
+        """
+
+        inference_context = deprecate_inference_params(inference_context, inference_params)
+
+        if inference_context and inference_context.is_dynamic_batching():
+            assert is_fa_min_version(
+                "2.7.3"
+            ), "flash attn verion v2.7.3 and above is required for dynamic batching."
+
+        # hidden_states: [sq, b, h]
+        if self.config.flash_decode and not self.training and inference_context is not None:
+            rotary_pos_emb = None
+        else:
+            assert rotary_pos_cos is None and rotary_pos_sin is None
+
+        # For self attention we just duplicate the rotary_pos_emb if it isn't already
+        if rotary_pos_emb is not None and not isinstance(rotary_pos_emb, tuple):
+            rotary_pos_emb = (rotary_pos_emb,) * 2
+
+        # =====================
+        # Query, Key, and Value
+        # =====================
+        # Get the query, key and value tensors based on the type of attention -
+        # self or cross attn.
+        query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)
+
+        # ===================================================
+        # Adjust key, value, and rotary_pos_emb for inference
+        # ===================================================
+
+        # This branch only runs in the decode phase of flash decoding and returns after the linear
+        # projection. This conditional is not used in the prefill phase or non-flash-decoding cases.
+        if (
+            self.config.flash_decode
+            and inference_context is not None
+            and inference_context.is_decode_only()
+            and not self.training
+            and rotary_pos_cos is not None
+        ):
+            assert self.layer_number in inference_context.key_value_memory_dict
+            assert inference_context.sequence_len_offset is not None
+            inference_key_memory, inference_value_memory = inference_context.key_value_memory_dict[
+                self.layer_number
+            ]
+            output = self.flash_decode(
+                sequence_len_offset=sequence_len_offset,
+                query_layer=query,
+                key_layer=key,
+                value_layer=value,
+                inference_key_memory=inference_key_memory,
+                inference_value_memory=inference_value_memory,
+                rotary_cos=rotary_pos_cos,
+                rotary_sin=rotary_pos_sin,
+            )
+            out = output.transpose(0, 1).contiguous()
+            context_layer = out.view(out.size(0), out.size(1), -1)
+            output, bias = self.linear_proj(context_layer)
+            return output, bias
+
+        query, key, value, rotary_pos_emb, attn_mask_type, block_table = (
+            self._adjust_key_value_for_inference(
+                inference_context,
+                query,
+                key,
+                value,
+                rotary_pos_emb,
+                rotary_pos_cos,
+                rotary_pos_sin,
+                sequence_len_offset,
+            )
+        )
+
+        if packed_seq_params is not None:
+            query = query.squeeze(1)
+            key = key.squeeze(1)
+            value = value.squeeze(1)
+
+        # ================================================
+        # relative positional embedding (rotary embedding)
+        # ================================================
+        if rotary_pos_emb is not None and not self.config.flash_decode:
+            q_pos_emb, k_pos_emb = rotary_pos_emb
+
+            if packed_seq_params is not None:
+                if packed_seq_params.cu_seqlens_q_padded is not None:
+                    cu_seqlens_q = packed_seq_params.cu_seqlens_q_padded
+                else:
+                    cu_seqlens_q = packed_seq_params.cu_seqlens_q
+                if packed_seq_params.cu_seqlens_kv_padded is not None:
+                    cu_seqlens_kv = packed_seq_params.cu_seqlens_kv_padded
+                else:
+                    cu_seqlens_kv = packed_seq_params.cu_seqlens_kv
+            else:
+                cu_seqlens_q = cu_seqlens_kv = None
+
+            if q_pos_emb is not None:
+                # TODO VIJAY: simplify
+                if inference_context is None or inference_context.is_static_batching():
+                    query = apply_rotary_pos_emb_vision(
+                        query,
+                        q_pos_emb,
+                        config=self.config,
+                        cu_seqlens=cu_seqlens_q,
+                        cp_group=self.model_comm_pgs.cp,
+                    )
+                else:
+                    query = inference_context.apply_rotary_emb_query(
+                        query, q_pos_emb, self.config, cu_seqlens_q, self.model_comm_pgs.cp
+                    )
+            if k_pos_emb is not None:
+                key = apply_rotary_pos_emb_vision(
+                    key,
+                    k_pos_emb,
+                    config=self.config,
+                    cu_seqlens=cu_seqlens_kv,
+                    cp_group=self.model_comm_pgs.cp,
+                )
+
+            # TODO, can apply positional embedding to value_layer so it has
+            # absolute positional embedding.
+            # otherwise, only relative positional embedding takes effect
+            # value_layer = apply_rotary_pos_emb_vision(value_layer, k_pos_emb)
+
+        # ==================================
+        # core attention computation
+        # ==================================
+
+        if self.checkpoint_core_attention and self.training:
+            core_attn_out = self._checkpointed_attention_forward(
+                query,
+                key,
+                value,
+                attention_mask,
+                attn_mask_type=attn_mask_type,
+                attention_bias=attention_bias,
+                packed_seq_params=packed_seq_params,
+            )
+        else:
+            if inference_context is None or inference_context.is_static_batching():
+                # Static batching attention kernel.
+                core_attn_out = self.core_attention(
+                    query,
+                    key,
+                    value,
+                    attention_mask,
+                    attn_mask_type=attn_mask_type,
+                    attention_bias=attention_bias,
+                    packed_seq_params=packed_seq_params,
+                )
+
+            else:
+                # Dynamic batching attention kernel.
+                q, k, v = (query, key, value)
+                cu_query_lengths, max_seqlen_q = inference_context.cu_query_lengths()
+                cu_kv_lengths, kv_lengths, max_seqlen_k = inference_context.cu_kv_lengths()
+
+                core_attn_out = self.flash_decode_and_prefill(
+                    q,
+                    k,
+                    v,
+                    max_seqlen_q,
+                    max_seqlen_k,
+                    cu_query_lengths,
+                    cu_kv_lengths,
+                    kv_lengths,
+                    block_table,
+                )
+                core_attn_out = rearrange(core_attn_out, 's b h d -> s b (h d)')
+
+        if packed_seq_params is not None and packed_seq_params.qkv_format == 'thd':
+            # reshape to same output shape as unpacked case
+            # (t, np, hn) -> (t, b=1, h=np*hn)
+            # t is the pack size = sum (sq_i)
+            # note that batch is a dummy dimension in the packed case
+            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)
+
+        # =================
+        # Output. [sq, b, h]
+        # =================
+
+        output, bias = self.linear_proj(core_attn_out)
+
+        return output, bias
+
+class SelfAttentionVision(VisionAttention):
+    """Self-attention layer class
+
+    Self-attention layer takes input with size [s, b, h]
+    and returns output of the same size.
+    """
+
+    def __init__(
+        self,
+        config: TransformerConfig,
+        submodules: SelfAttentionSubmodules,
+        layer_number: int,
+        attn_mask_type=AttnMaskType.padding,
+        cp_comm_type: str = None,
+        model_comm_pgs: ModelCommProcessGroups = None,
+    ):
+        super().__init__(
+            config=config,
+            submodules=submodules,
+            layer_number=layer_number,
+            attn_mask_type=attn_mask_type,
+            attention_type="self",
+            cp_comm_type=cp_comm_type,
+            model_comm_pgs=model_comm_pgs,
+        )
+
+        self.linear_qkv = build_module(
+            submodules.linear_qkv,
+            self.config.hidden_size,
+            self.query_projection_size + 2 * self.kv_projection_size,
+            config=self.config,
+            init_method=self.config.init_method,
+            gather_output=False,
+            bias=self.config.add_bias_linear or self.config.add_qkv_bias,
+            skip_bias_add=False,
+            is_expert=False,
+            tp_comm_buffer_name='qkv',
+        )
+
+        if submodules.q_layernorm is not None:
+            self.q_layernorm = build_module(
+                submodules.q_layernorm,
+                hidden_size=self.hidden_size_per_attention_head,
+                config=self.config,
+                eps=self.config.layernorm_epsilon,
+            )
+        else:
+            self.q_layernorm = None
+
+        if submodules.k_layernorm is not None:
+            self.k_layernorm = build_module(
+                submodules.k_layernorm,
+                hidden_size=self.hidden_size_per_attention_head,
+                config=self.config,
+                eps=self.config.layernorm_epsilon,
+            )
+        else:
+            self.k_layernorm = None
+
+    def run_realtime_tests(self):
+        """Performs a consistency check.
+
+        This function makes sure that tensors across devices are the same during an experiment.
+        This is often not guaranteed to be so because of silent hardware failures (eg, memory
+        corruption loading a checkpoint, network traffic corruption encountered during
+        data transmission).
+
+        (TODO) In the future, more tensors should be checked across the training run and
+        checked every X iterations. This is left for future work. Equality of tensors is probably
+        not required; transmitting hashes is sufficient."""
+
+        if not self.config.qk_layernorm:
+            return
+
+        # check that all tensor parallel and data parallel ranks have the same
+        # Q & K layernorm parameters.
+        rank = get_data_parallel_rank()
+        inputs = torch.stack(
+            [
+                self.q_layernorm.weight.data,
+                self.q_layernorm.bias.data,
+                self.k_layernorm.weight.data,
+                self.k_layernorm.bias.data,
+            ]
+        )
+        dp_list = [torch.empty_like(inputs) for _ in range(get_data_parallel_world_size())]
+        dp_list[rank] = inputs
+        torch.distributed.all_gather(dp_list, inputs, group=get_data_parallel_group())
+
+        def _compare(srcs, tgts, names, parallelism):
+            assert len(srcs) == len(tgts) == len(names)
+            for src, tgt, name in zip(srcs, tgts, names):
+                assert torch.all(src == tgt), (
+                    f"Discrepancy between {name} in {parallelism} ranks {i} and {rank}. "
+                    f"Diff: {torch.norm(src - tgt)}"
+                )
+
+        for i, dp in enumerate(dp_list):
+            q_w, q_b, k_w, k_b = torch.unbind(dp)
+            _compare(
+                [q_w, q_b, k_w, k_b],
+                [
+                    self.q_layernorm.weight.data,
+                    self.q_layernorm.bias.data,
+                    self.k_layernorm.weight.data,
+                    self.k_layernorm.bias.data,
+                ],
+                ["q_w", "q_b", "k_w", "k_b"],
+                "DP",
+            )
+
+        rank = get_tensor_model_parallel_rank()
+        tp_list = [torch.empty_like(inputs) for _ in range(get_tensor_model_parallel_world_size())]
+        tp_list[rank] = inputs
+        torch.distributed.all_gather(tp_list, inputs, group=get_tensor_model_parallel_group())
+
+        for i, tp in enumerate(tp_list):
+            q_w, q_b, k_w, k_b = torch.unbind(tp)
+            _compare(
+                [q_w, q_b, k_w, k_b],
+                [
+                    self.q_layernorm.weight.data,
+                    self.q_layernorm.bias.data,
+                    self.k_layernorm.weight.data,
+                    self.k_layernorm.bias.data,
+                ],
+                ["q_w", "q_b", "k_w", "k_b"],
+                "TP",
+            )
+
+    def get_query_key_value_tensors(self, hidden_states, key_value_states=None):
+        """
+        Derives `query`, `key` and `value` tensors from `hidden_states`.
+        """
+        # Attention heads [sq, b, h] --> [sq, b, ng * (np/ng + 2) * hn)]
+        mixed_qkv, _ = self.linear_qkv(hidden_states)
+
+        # [sq, b, hp] --> [sq, b, ng, (np/ng + 2) * hn]
+        new_tensor_shape = mixed_qkv.size()[:-1] + (
+            self.num_query_groups_per_partition,
+            (
+                (self.num_attention_heads_per_partition // self.num_query_groups_per_partition + 2)
+                * self.hidden_size_per_attention_head
+            ),
+        )
+        mixed_qkv = mixed_qkv.view(*new_tensor_shape)
+
+        split_arg_list = [
+            (
+                self.num_attention_heads_per_partition
+                // self.num_query_groups_per_partition
+                * self.hidden_size_per_attention_head
+            ),
+            self.hidden_size_per_attention_head,
+            self.hidden_size_per_attention_head,
+        ]
+
+        if SplitAlongDim is not None:
+
+            # [sq, b, ng, (np/ng + 2) * hn]
+            # --> [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]
+            (query, key, value) = SplitAlongDim(mixed_qkv, 3, split_arg_list)
+        else:
+
+            # [sq, b, ng, (np/ng + 2) * hn]
+            # --> [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]
+            (query, key, value) = torch.split(mixed_qkv, split_arg_list, dim=3)
+
+        # [sq, b, ng, np/ng * hn] -> [sq, b, np, hn]
+        query = query.reshape(query.size(0), query.size(1), -1, self.hidden_size_per_attention_head)
+
+        if self.q_layernorm is not None:
+            query = self.q_layernorm(query)
+
+        if self.k_layernorm is not None:
+            key = self.k_layernorm(key)
+
+        if self.config.test_mode:
+            self.run_realtime_tests()
+
+        return query, key, value
diff --git a/flagscale/train/models/qwen2_5_vl/vision_transformer_block.py b/flagscale/train/models/qwen2_5_vl/vision_transformer_block.py
index e573e708..4407f3d4 100644
--- a/flagscale/train/models/qwen2_5_vl/vision_transformer_block.py
+++ b/flagscale/train/models/qwen2_5_vl/vision_transformer_block.py
@@ -1,313 +1,313 @@
-# Copyright (c) 2025, BAAI. All rights reserved.
-#
-# Mainly Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/transformer_block.py. Below is the original copyright:
-#  Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
-
-from contextlib import nullcontext
-from typing import List, Optional, Union
-
-import torch
-from torch import Tensor
-
-from megatron.core import parallel_state, tensor_parallel
-from megatron.core.enums import Fp8Recipe
-from megatron.core.fp8_utils import get_fp8_context
-from megatron.core.fusions.fused_layer_norm import FusedLayerNorm
-from megatron.core.inference.contexts import BaseInferenceContext
-from megatron.core.packed_seq_params import PackedSeqParams
-from megatron.core.process_groups_config import ModelCommProcessGroups
-from megatron.core.transformer.spec_utils import ModuleSpec
-from megatron.core.transformer.transformer_config import TransformerConfig
-from megatron.core.transformer.transformer_block import TransformerBlock, TransformerBlockSubmodules
-from megatron.core.utils import WrappedTensor, deprecate_inference_params, make_viewless_tensor
-
-try:
-    from megatron.core.extensions.transformer_engine import (
-        TENorm,
-        te_checkpoint,
-    )
-
-    HAVE_TE = True
-    LayerNormImpl = TENorm
-except ImportError:
-    HAVE_TE = False
-    get_cpu_offload_context = None
-
-    try:
-        import apex  # pylint: disable=unused-import
-
-        LayerNormImpl = FusedLayerNorm
-
-    except ImportError:
-        from megatron.core.transformer.torch_norm import WrappedTorchNorm
-
-        LayerNormImpl = WrappedTorchNorm
-
-
-class VisionTransformerBlock(TransformerBlock):
-    """Transformer class for Qwen2.5-VL vit model. Support to use window attention."""
-
-    def __init__(
-        self,
-        config: TransformerConfig,
-        spec: Union[TransformerBlockSubmodules, ModuleSpec],
-        post_layer_norm: bool = True,
-        pre_process: bool = True,
-        post_process: bool = True,
-        model_comm_pgs: ModelCommProcessGroups = None,
-    ):
-        super().__init__(config=config, spec=spec, post_layer_norm=post_layer_norm,
-                         pre_process=pre_process, post_process=post_process,
-                         model_comm_pgs=model_comm_pgs)
-
-    def _checkpointed_forward(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Tensor,
-        context: Tensor,
-        context_mask: Tensor,
-        rotary_pos_emb: Tensor,
-        attention_bias: Tensor,
-        packed_seq_params: PackedSeqParams,
-        packed_seq_params_full: PackedSeqParams,
-        fullatt_block_indexes: List[int],
-    ):
-        """Forward method with activation checkpointing."""
-
-        def custom(start: int, end: int):
-            def custom_forward(
-                hidden_states, attention_mask, context, context_mask, rotary_pos_emb
-            ):
-                for index in range(start, end):
-                    if index in fullatt_block_indexes:
-                        packed_seq_params_now = packed_seq_params_full
-                    else:
-                        packed_seq_params_now = packed_seq_params
-                    layer = self._get_layer(index)
-                    hidden_states, context = layer(
-                        hidden_states=hidden_states,
-                        attention_mask=attention_mask,
-                        context=context,
-                        context_mask=context_mask,
-                        rotary_pos_emb=rotary_pos_emb,
-                        attention_bias=attention_bias,
-                        inference_context=None,
-                        packed_seq_params=packed_seq_params_now,
-                    )
-                return hidden_states, context
-
-            return custom_forward
-
-        def checkpoint_handler(forward_func):
-            """Determines whether to use the `te_checkpoint` or `tensor_parallel.checkpoint`"""
-            if self.config.fp8:
-                return te_checkpoint(
-                    forward_func,
-                    self.config.distribute_saved_activations,
-                    tensor_parallel.random.get_cuda_rng_tracker,
-                    parallel_state.get_tensor_model_parallel_group(),
-                    hidden_states,
-                    attention_mask,
-                    context,
-                    context_mask,
-                    rotary_pos_emb,
-                )
-            else:
-                return tensor_parallel.checkpoint(
-                    forward_func,
-                    self.config.distribute_saved_activations,
-                    hidden_states,
-                    attention_mask,
-                    context,
-                    context_mask,
-                    rotary_pos_emb,
-                )
-
-        if self.config.recompute_method == 'uniform':
-            # Uniformly divide the total number of Transformer layers and checkpoint
-            # the input activation of each divided chunk.
-            # A method to further reduce memory usage reducing checkpoints.
-            layer_idx = 0
-            while layer_idx < self.num_layers_per_pipeline_rank:
-                hidden_states, context = checkpoint_handler(
-                    custom(layer_idx, layer_idx + self.config.recompute_num_layers)
-                )
-
-                layer_idx += self.config.recompute_num_layers
-
-        elif self.config.recompute_method == 'block':
-            # Checkpoint the input activation of only a set number of individual
-            # Transformer layers and skip the rest.
-            # A method fully use the device memory removing redundant re-computation.
-            recompute_skip_num_layers = 0
-            for layer_idx in range(self.num_layers_per_pipeline_rank):
-                # Skip recomputation when input grad computation is not needed.
-                # Need to have at least one input tensor with gradient computation
-                # for re-enterant autograd engine.
-                if self.config.fp8 and not hidden_states.requires_grad:
-                    recompute_skip_num_layers += 1
-                if (
-                    layer_idx >= recompute_skip_num_layers
-                    and layer_idx < self.config.recompute_num_layers + recompute_skip_num_layers
-                ):
-                    hidden_states, context = checkpoint_handler(custom(layer_idx, layer_idx + 1))
-                else:
-                    hidden_states, context = custom(layer_idx, layer_idx + 1)(
-                        hidden_states, attention_mask, context, context_mask, rotary_pos_emb
-                    )
-        else:
-            raise ValueError("Invalid activation recompute method.")
-
-        return hidden_states
-
-
-    def forward(
-        self,
-        hidden_states: Union[Tensor, WrappedTensor],
-        attention_mask: Optional[Tensor],
-        context: Optional[Tensor] = None,
-        context_mask: Optional[Tensor] = None,
-        rotary_pos_emb: Optional[Tensor] = None,
-        rotary_pos_cos: Optional[Tensor] = None,
-        rotary_pos_sin: Optional[Tensor] = None,
-        attention_bias: Optional[Tensor] = None,
-        inference_context: Optional[BaseInferenceContext] = None,
-        packed_seq_params: Optional[PackedSeqParams] = None,
-        sequence_len_offset: Optional[Tensor] = None,
-        *,
-        inference_params: Optional[BaseInferenceContext] = None,
-        packed_seq_params_full: Optional[PackedSeqParams] = None,
-        fullatt_block_indexes = None,
-    ):
-        """
-        Perform the forward pass through the transformer block.
-
-        This method handles the core computation of the transformer, including
-        self-attention, optional cross-attention, and feed-forward operations.
-
-        Args:
-            hidden_states (Union[Tensor, WrappedTensor]): Input tensor of shape [s, b, h]
-                where s is the sequence length, b is the batch size, and h is the hidden size.
-                Can be passed as a WrappedTensor during inference to avoid an obsolete
-                reference in the calling function.
-            attention_mask (Tensor): Boolean tensor of shape [1, 1, s, s] for masking
-                self-attention.
-            context (Tensor, optional): Context tensor for cross-attention.
-            context_mask (Tensor, optional): Mask for cross-attention context
-            rotary_pos_emb (Tensor, optional): Rotary positional embeddings.
-            attention_bias (Tensor): Bias tensor for Q * K.T of shape in shape broadcastable
-                to [b, num_head, sq, skv], e.g. [1, 1, sq, skv].
-                Used as an alternative to apply attention mask for TE cuDNN attention.
-            inference_context (BaseInferenceContext, optional): Parameters for inference-time
-                optimizations.
-            packed_seq_params (PackedSeqParams, optional): Parameters for packed sequence
-                processing.
-
-        Returns:
-            Union[Tensor, Tuple[Tensor, Tensor]]: The output hidden states tensor of shape
-            [s, b, h], and optionally the updated context tensor if cross-attention is used.
-        """
-
-        inference_context = deprecate_inference_params(inference_context, inference_params)
-
-        # Delete the obsolete reference to the initial input tensor if necessary
-        if isinstance(hidden_states, WrappedTensor):
-            hidden_states = hidden_states.unwrap()
-
-        if not self.pre_process:
-            # See set_input_tensor()
-            hidden_states = self.input_tensor
-
-        # Update the inference parameters with the current batch size in case it is variable
-        if inference_context and not self.training:
-            inference_context.current_batch_size = hidden_states.size(1)
-
-        # Viewless tensor.
-        # - We only need to create a viewless tensor in the case of micro batch
-        #   size (mbs) == 1, since in this case, 'hidden_states.transpose()'
-        #   above creates a view tensor, and '.contiguous()' is a pass-through.
-        #   For mbs >= 2, '.contiguous()' creates a new tensor, eliminating
-        #   the need to make it viewless.
-        #
-        #   However, we don't explicitly check mbs == 1 here because
-        #   make_viewless_tensor() has negligible overhead when its input
-        #   is already viewless.
-        #
-        # - For the 'else' case above, calling make_viewless_tensor() here is
-        #   likely redundant, since p2p_communication.py (likely originator)
-        #   already creates viewless tensors. That said, make_viewless_tensor()
-        #   is called here to be future-proof and corner-case-proof.
-        hidden_states = make_viewless_tensor(inp=hidden_states, requires_grad=True, keep_graph=True)
-
-        if self.config.sequence_parallel:
-            rng_context = tensor_parallel.get_cuda_rng_tracker().fork()
-        else:
-            rng_context = nullcontext()
-
-        # If fp8_recipe is delayed, wrap the entire pass with get_fp8_context(),
-        # otherwise do nothing extra at the outer level
-        # if we are using other fp8 recipes, then the context manager enter&exit are free
-        # we can wrap fp8_context within the for loop over layers, so that we can fine-grained
-        # control which layer will be fp8 or bf16
-        use_outer_fp8_context = self.config.fp8 and self.config.fp8_recipe == Fp8Recipe.delayed
-        use_inner_fp8_context = self.config.fp8 and self.config.fp8_recipe != Fp8Recipe.delayed
-        outer_fp8_context = get_fp8_context(self.config) if use_outer_fp8_context else nullcontext()
-
-        with rng_context, outer_fp8_context:
-            # Forward pass.
-            if self.config.recompute_granularity == 'full' and self.training:
-                hidden_states = self._checkpointed_forward(
-                    hidden_states=hidden_states,
-                    attention_mask=attention_mask,
-                    context=context,
-                    context_mask=context_mask,
-                    rotary_pos_emb=rotary_pos_emb,
-                    attention_bias=attention_bias,
-                    packed_seq_params=packed_seq_params,
-                    packed_seq_params_full=packed_seq_params_full,
-                    fullatt_block_indexes=fullatt_block_indexes
-                )
-            else:
-                for l_no, layer in enumerate(self.layers):
-                    if l_no in fullatt_block_indexes:
-                        packed_seq_params_now = packed_seq_params_full
-                    else:
-                        packed_seq_params_now = packed_seq_params
-                    inner_fp8_context = (
-                        get_fp8_context(self.config, layer.layer_number - 1)
-                        if use_inner_fp8_context
-                        else nullcontext()
-                    )
-                    with self.offload_context, inner_fp8_context:
-                        hidden_states, context = layer(
-                            hidden_states=hidden_states,
-                            attention_mask=attention_mask,
-                            context=context,
-                            context_mask=context_mask,
-                            rotary_pos_emb=rotary_pos_emb,
-                            rotary_pos_cos=rotary_pos_cos,
-                            rotary_pos_sin=rotary_pos_sin,
-                            attention_bias=attention_bias,
-                            inference_context=inference_context,
-                            packed_seq_params=packed_seq_params_now,
-                            sequence_len_offset=sequence_len_offset,
-                        )
-
-                    if (
-                        torch.is_grad_enabled()
-                        and self.config.cpu_offloading
-                        and self.group_prefetch_offload_commit_async is not None
-                    ):
-                        hidden_states = self.group_prefetch_offload_commit_async(hidden_states)
-
-        # Final layer norm.
-        if self.final_layernorm is not None:
-            hidden_states = self.final_layernorm(hidden_states)
-            # TENorm produces a "viewed" tensor. This will result in schedule.py's
-            # deallocate_output_tensor() throwing an error, so a viewless tensor is
-            # created to prevent this.
-            hidden_states = make_viewless_tensor(
-                inp=hidden_states, requires_grad=True, keep_graph=True
-            )
-
-        return hidden_states
+# Copyright (c) 2025, BAAI. All rights reserved.
+#
+# Mainly Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/transformer_block.py. Below is the original copyright:
+#  Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
+
+from contextlib import nullcontext
+from typing import List, Optional, Union
+
+import torch
+from torch import Tensor
+
+from megatron.core import parallel_state, tensor_parallel
+from megatron.core.enums import Fp8Recipe
+from megatron.core.fp8_utils import get_fp8_context
+from megatron.core.fusions.fused_layer_norm import FusedLayerNorm
+from megatron.core.inference.contexts import BaseInferenceContext
+from megatron.core.packed_seq_params import PackedSeqParams
+from megatron.core.process_groups_config import ModelCommProcessGroups
+from megatron.core.transformer.spec_utils import ModuleSpec
+from megatron.core.transformer.transformer_config import TransformerConfig
+from megatron.core.transformer.transformer_block import TransformerBlock, TransformerBlockSubmodules
+from megatron.core.utils import WrappedTensor, deprecate_inference_params, make_viewless_tensor
+
+try:
+    from megatron.core.extensions.transformer_engine import (
+        TENorm,
+        te_checkpoint,
+    )
+
+    HAVE_TE = True
+    LayerNormImpl = TENorm
+except ImportError:
+    HAVE_TE = False
+    get_cpu_offload_context = None
+
+    try:
+        import apex  # pylint: disable=unused-import
+
+        LayerNormImpl = FusedLayerNorm
+
+    except ImportError:
+        from megatron.core.transformer.torch_norm import WrappedTorchNorm
+
+        LayerNormImpl = WrappedTorchNorm
+
+
+class VisionTransformerBlock(TransformerBlock):
+    """Transformer class for Qwen2.5-VL vit model. Support to use window attention."""
+
+    def __init__(
+        self,
+        config: TransformerConfig,
+        spec: Union[TransformerBlockSubmodules, ModuleSpec],
+        post_layer_norm: bool = True,
+        pre_process: bool = True,
+        post_process: bool = True,
+        model_comm_pgs: ModelCommProcessGroups = None,
+    ):
+        super().__init__(config=config, spec=spec, post_layer_norm=post_layer_norm,
+                         pre_process=pre_process, post_process=post_process,
+                         model_comm_pgs=model_comm_pgs)
+
+    def _checkpointed_forward(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Tensor,
+        context: Tensor,
+        context_mask: Tensor,
+        rotary_pos_emb: Tensor,
+        attention_bias: Tensor,
+        packed_seq_params: PackedSeqParams,
+        packed_seq_params_full: PackedSeqParams,
+        fullatt_block_indexes: List[int],
+    ):
+        """Forward method with activation checkpointing."""
+
+        def custom(start: int, end: int):
+            def custom_forward(
+                hidden_states, attention_mask, context, context_mask, rotary_pos_emb
+            ):
+                for index in range(start, end):
+                    if index in fullatt_block_indexes:
+                        packed_seq_params_now = packed_seq_params_full
+                    else:
+                        packed_seq_params_now = packed_seq_params
+                    layer = self._get_layer(index)
+                    hidden_states, context = layer(
+                        hidden_states=hidden_states,
+                        attention_mask=attention_mask,
+                        context=context,
+                        context_mask=context_mask,
+                        rotary_pos_emb=rotary_pos_emb,
+                        attention_bias=attention_bias,
+                        inference_context=None,
+                        packed_seq_params=packed_seq_params_now,
+                    )
+                return hidden_states, context
+
+            return custom_forward
+
+        def checkpoint_handler(forward_func):
+            """Determines whether to use the `te_checkpoint` or `tensor_parallel.checkpoint`"""
+            if self.config.fp8:
+                return te_checkpoint(
+                    forward_func,
+                    self.config.distribute_saved_activations,
+                    tensor_parallel.random.get_cuda_rng_tracker,
+                    parallel_state.get_tensor_model_parallel_group(),
+                    hidden_states,
+                    attention_mask,
+                    context,
+                    context_mask,
+                    rotary_pos_emb,
+                )
+            else:
+                return tensor_parallel.checkpoint(
+                    forward_func,
+                    self.config.distribute_saved_activations,
+                    hidden_states,
+                    attention_mask,
+                    context,
+                    context_mask,
+                    rotary_pos_emb,
+                )
+
+        if self.config.recompute_method == 'uniform':
+            # Uniformly divide the total number of Transformer layers and checkpoint
+            # the input activation of each divided chunk.
+            # A method to further reduce memory usage reducing checkpoints.
+            layer_idx = 0
+            while layer_idx < self.num_layers_per_pipeline_rank:
+                hidden_states, context = checkpoint_handler(
+                    custom(layer_idx, layer_idx + self.config.recompute_num_layers)
+                )
+
+                layer_idx += self.config.recompute_num_layers
+
+        elif self.config.recompute_method == 'block':
+            # Checkpoint the input activation of only a set number of individual
+            # Transformer layers and skip the rest.
+            # A method fully use the device memory removing redundant re-computation.
+            recompute_skip_num_layers = 0
+            for layer_idx in range(self.num_layers_per_pipeline_rank):
+                # Skip recomputation when input grad computation is not needed.
+                # Need to have at least one input tensor with gradient computation
+                # for re-enterant autograd engine.
+                if self.config.fp8 and not hidden_states.requires_grad:
+                    recompute_skip_num_layers += 1
+                if (
+                    layer_idx >= recompute_skip_num_layers
+                    and layer_idx < self.config.recompute_num_layers + recompute_skip_num_layers
+                ):
+                    hidden_states, context = checkpoint_handler(custom(layer_idx, layer_idx + 1))
+                else:
+                    hidden_states, context = custom(layer_idx, layer_idx + 1)(
+                        hidden_states, attention_mask, context, context_mask, rotary_pos_emb
+                    )
+        else:
+            raise ValueError("Invalid activation recompute method.")
+
+        return hidden_states
+
+
+    def forward(
+        self,
+        hidden_states: Union[Tensor, WrappedTensor],
+        attention_mask: Optional[Tensor],
+        context: Optional[Tensor] = None,
+        context_mask: Optional[Tensor] = None,
+        rotary_pos_emb: Optional[Tensor] = None,
+        rotary_pos_cos: Optional[Tensor] = None,
+        rotary_pos_sin: Optional[Tensor] = None,
+        attention_bias: Optional[Tensor] = None,
+        inference_context: Optional[BaseInferenceContext] = None,
+        packed_seq_params: Optional[PackedSeqParams] = None,
+        sequence_len_offset: Optional[Tensor] = None,
+        *,
+        inference_params: Optional[BaseInferenceContext] = None,
+        packed_seq_params_full: Optional[PackedSeqParams] = None,
+        fullatt_block_indexes = None,
+    ):
+        """
+        Perform the forward pass through the transformer block.
+
+        This method handles the core computation of the transformer, including
+        self-attention, optional cross-attention, and feed-forward operations.
+
+        Args:
+            hidden_states (Union[Tensor, WrappedTensor]): Input tensor of shape [s, b, h]
+                where s is the sequence length, b is the batch size, and h is the hidden size.
+                Can be passed as a WrappedTensor during inference to avoid an obsolete
+                reference in the calling function.
+            attention_mask (Tensor): Boolean tensor of shape [1, 1, s, s] for masking
+                self-attention.
+            context (Tensor, optional): Context tensor for cross-attention.
+            context_mask (Tensor, optional): Mask for cross-attention context
+            rotary_pos_emb (Tensor, optional): Rotary positional embeddings.
+            attention_bias (Tensor): Bias tensor for Q * K.T of shape in shape broadcastable
+                to [b, num_head, sq, skv], e.g. [1, 1, sq, skv].
+                Used as an alternative to apply attention mask for TE cuDNN attention.
+            inference_context (BaseInferenceContext, optional): Parameters for inference-time
+                optimizations.
+            packed_seq_params (PackedSeqParams, optional): Parameters for packed sequence
+                processing.
+
+        Returns:
+            Union[Tensor, Tuple[Tensor, Tensor]]: The output hidden states tensor of shape
+            [s, b, h], and optionally the updated context tensor if cross-attention is used.
+        """
+
+        inference_context = deprecate_inference_params(inference_context, inference_params)
+
+        # Delete the obsolete reference to the initial input tensor if necessary
+        if isinstance(hidden_states, WrappedTensor):
+            hidden_states = hidden_states.unwrap()
+
+        if not self.pre_process:
+            # See set_input_tensor()
+            hidden_states = self.input_tensor
+
+        # Update the inference parameters with the current batch size in case it is variable
+        if inference_context and not self.training:
+            inference_context.current_batch_size = hidden_states.size(1)
+
+        # Viewless tensor.
+        # - We only need to create a viewless tensor in the case of micro batch
+        #   size (mbs) == 1, since in this case, 'hidden_states.transpose()'
+        #   above creates a view tensor, and '.contiguous()' is a pass-through.
+        #   For mbs >= 2, '.contiguous()' creates a new tensor, eliminating
+        #   the need to make it viewless.
+        #
+        #   However, we don't explicitly check mbs == 1 here because
+        #   make_viewless_tensor() has negligible overhead when its input
+        #   is already viewless.
+        #
+        # - For the 'else' case above, calling make_viewless_tensor() here is
+        #   likely redundant, since p2p_communication.py (likely originator)
+        #   already creates viewless tensors. That said, make_viewless_tensor()
+        #   is called here to be future-proof and corner-case-proof.
+        hidden_states = make_viewless_tensor(inp=hidden_states, requires_grad=True, keep_graph=True)
+
+        if self.config.sequence_parallel:
+            rng_context = tensor_parallel.get_cuda_rng_tracker().fork()
+        else:
+            rng_context = nullcontext()
+
+        # If fp8_recipe is delayed, wrap the entire pass with get_fp8_context(),
+        # otherwise do nothing extra at the outer level
+        # if we are using other fp8 recipes, then the context manager enter&exit are free
+        # we can wrap fp8_context within the for loop over layers, so that we can fine-grained
+        # control which layer will be fp8 or bf16
+        use_outer_fp8_context = self.config.fp8 and self.config.fp8_recipe == Fp8Recipe.delayed
+        use_inner_fp8_context = self.config.fp8 and self.config.fp8_recipe != Fp8Recipe.delayed
+        outer_fp8_context = get_fp8_context(self.config) if use_outer_fp8_context else nullcontext()
+
+        with rng_context, outer_fp8_context:
+            # Forward pass.
+            if self.config.recompute_granularity == 'full' and self.training:
+                hidden_states = self._checkpointed_forward(
+                    hidden_states=hidden_states,
+                    attention_mask=attention_mask,
+                    context=context,
+                    context_mask=context_mask,
+                    rotary_pos_emb=rotary_pos_emb,
+                    attention_bias=attention_bias,
+                    packed_seq_params=packed_seq_params,
+                    packed_seq_params_full=packed_seq_params_full,
+                    fullatt_block_indexes=fullatt_block_indexes
+                )
+            else:
+                for l_no, layer in enumerate(self.layers):
+                    if l_no in fullatt_block_indexes:
+                        packed_seq_params_now = packed_seq_params_full
+                    else:
+                        packed_seq_params_now = packed_seq_params
+                    inner_fp8_context = (
+                        get_fp8_context(self.config, layer.layer_number - 1)
+                        if use_inner_fp8_context
+                        else nullcontext()
+                    )
+                    with self.offload_context, inner_fp8_context:
+                        hidden_states, context = layer(
+                            hidden_states=hidden_states,
+                            attention_mask=attention_mask,
+                            context=context,
+                            context_mask=context_mask,
+                            rotary_pos_emb=rotary_pos_emb,
+                            rotary_pos_cos=rotary_pos_cos,
+                            rotary_pos_sin=rotary_pos_sin,
+                            attention_bias=attention_bias,
+                            inference_context=inference_context,
+                            packed_seq_params=packed_seq_params_now,
+                            sequence_len_offset=sequence_len_offset,
+                        )
+
+                    if (
+                        torch.is_grad_enabled()
+                        and self.config.cpu_offloading
+                        and self.group_prefetch_offload_commit_async is not None
+                    ):
+                        hidden_states = self.group_prefetch_offload_commit_async(hidden_states)
+
+        # Final layer norm.
+        if self.final_layernorm is not None:
+            hidden_states = self.final_layernorm(hidden_states)
+            # TENorm produces a "viewed" tensor. This will result in schedule.py's
+            # deallocate_output_tensor() throwing an error, so a viewless tensor is
+            # created to prevent this.
+            hidden_states = make_viewless_tensor(
+                inp=hidden_states, requires_grad=True, keep_graph=True
+            )
+
+        return hidden_states
diff --git a/flagscale/train/models/qwen2_5_vl/vit_model.py b/flagscale/train/models/qwen2_5_vl/vit_model.py
index 2a21fe14..dbf3335e 100644
--- a/flagscale/train/models/qwen2_5_vl/vit_model.py
+++ b/flagscale/train/models/qwen2_5_vl/vit_model.py
@@ -1,324 +1,323 @@
-# Mainly adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/visionmodel.py.
-
-from typing import Optional
-
-import torch
-from torch import nn
-from torch.nn import functional as F
-
-from megatron.core.models.common.vision_module.vision_module import VisionModule
-from megatron.core.transformer.enums import ModelType
-from megatron.core.transformer.spec_utils import ModuleSpec
-from megatron.core.transformer.transformer_config import TransformerConfig
-from megatron.core.packed_seq_params import PackedSeqParams
-from megatron.core import InferenceParams
-from megatron.core.models.vision.multimodal_projector import MultimodalProjector
-
-from flagscale.train.models.qwen2_5_vl.vision_transformer_block import VisionTransformerBlock
-
-
-# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
-class PatchEmbed(nn.Module):
-    def __init__(
-        self,
-        patch_size: int = 14,
-        temporal_patch_size: int = 2,
-        in_channels: int = 3,
-        embed_dim: int = 1152,
-    ) -> None:
-        """
-        Patch Embedding layer for Qwen2.5 Vision Model. Change the original image/video to patches.
-        Merge the patches in the dimension of temporal.
-        Args:
-            patch_size (int): The size of the spatial patch. Defaults to 14.
-            temporal_patch_size (int): The size of the temporal patch. Defaults to 2.
-            in_channels (int): The number of input channels. Defaults to 3.
-            embed_dim (int): The dimension of the embedded representation. Defaults to 1152.
-        """
-        super().__init__()
-        self.patch_size = patch_size
-        self.temporal_patch_size = temporal_patch_size
-        self.in_channels = in_channels
-        self.embed_dim = embed_dim
-
-        kernel_size = [temporal_patch_size, patch_size, patch_size]
-        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=False)
-
-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-        """
-        hidden_states: [tiles, in_chanels] --> (num_patches, embed_dim)
-        """
-        target_dtype = self.proj.weight.dtype
-        hidden_states = hidden_states.view(
-            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
-        )
-        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
-        return hidden_states
-
-# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
-class VisionRotaryEmbedding(nn.Module):
-    """
-
-    """
-    def __init__(self, dim: int, theta: float = 10000.0) -> None:
-        super().__init__()
-        # NOTE(lizhiyu): print inv_freq to check it.
-        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.bfloat16) / dim))
-        self.register_buffer("inv_freq", inv_freq, persistent=False)
-
-    def forward(self, seqlen: int) -> torch.Tensor:
-        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
-        # freqs [seq_len, dim // 2]
-        freqs = torch.outer(seq, self.inv_freq)
-        return freqs
-
-# reference from https://github.com/huggingface/transformers/blob/0ad3710d4767d4ac7ee95f33f8554373e59efade/src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py#L243
-class Qwen2_5VisionModel(VisionModule):
-    """Qwen2.5 ViT vision model.
-
-    Args:
-        transformer_config (TransformerConfig): Transformer config.
-        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers.
-        ln_pre_impl (ModuleSpec or type): Specifies the layer norm type to use for ln_pre.
-        add_class_token (bool, optional): Include a class token. Defaults to True.
-        class_token_len (int): Class token length. Defaults to 1 but 8 may be faster.
-        patch_dim (int): Image patch size.
-        img_h (int): Input image height.
-        img_w (int): Input image width.
-    """
-
-    def __init__(
-        self,
-        transformer_config: TransformerConfig,
-        transformer_layer_spec: ModuleSpec,
-        projection_config: TransformerConfig,
-        projection_layer_spec: ModuleSpec,
-        projection_type: str = "mlp",
-
-        pre_process: bool = True,
-        post_process: bool = False
-    ) -> None:
-        super().__init__(config=transformer_config)
-
-        self.spatial_merge_size = transformer_config.spatial_merge_size
-
-        embed_dim = transformer_config.hidden_size
-        num_heads = transformer_config.num_attention_heads
-        temporal_patch_size = transformer_config.temporal_patch_size
-        patch_size = transformer_config.patch_size
-        in_channels = transformer_config.in_channels
-
-        self.patch_size = transformer_config.patch_size
-        self.fullatt_block_indexes = transformer_config.fullatt_block_indexes
-        self.window_size = transformer_config._qwen2_5_vl_window_size
-        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size
-
-        self.max_sequence_length = transformer_config.seq_length
-        self.patch_embed = PatchEmbed(
-            patch_size=patch_size,
-            temporal_patch_size=temporal_patch_size,
-            in_channels=in_channels,
-            embed_dim=embed_dim,
-        )
-
-        head_dim = embed_dim // num_heads
-        self.rotary_pos_emb = VisionRotaryEmbedding(head_dim // 2)
-
-        self.model_type = ModelType.encoder_or_decoder
-        self.pre_process = pre_process
-        self.post_process = post_process
-
-        # Transformer layers.
-        # TODO: Follow-up changes will make pre and post_process configurable. They are needed for supporting pipeline parallelism.
-        # NOTE: a final layer norm and/or linear layer present in some implementations are omitted here.
-        self.decoder = VisionTransformerBlock(
-            config=transformer_config,
-            spec=transformer_layer_spec,
-            pre_process=self.pre_process,
-            post_process=self.post_process,
-            post_layer_norm=True
-        )
-
-        self.merge_hidden_size = projection_config.ffn_hidden_size
-        self.square_merge_size = self.merge_hidden_size // embed_dim
-
-        if self.post_process:
-            self.projection = MultimodalProjector(
-                projection_config,
-                projection_layer_spec,
-                projection_type,
-                projection_config.ffn_hidden_size
-            )
-        else:
-            self.projection = None
-
-        self.input_tensor = None
-
-    def set_input_tensor(self, input_tensor: torch.Tensor) -> None:
-        """Sets input tensor to the model.
-
-        Args:
-            input_tensor (Tensor): Sets the input tensor for the model.
-        """
-        if self.pre_process: # always True
-            self.input_tensor = input_tensor
-        else:
-            raise NotImplementedError()
-
-    def rot_pos_emb(self, grid_thw):
-        pos_ids = []
-        for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            )
-            hpos_ids = hpos_ids.permute(0, 2, 1, 3)
-            hpos_ids = hpos_ids.flatten()
-
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            )
-            wpos_ids = wpos_ids.permute(0, 2, 1, 3)
-            wpos_ids = wpos_ids.flatten()
-            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
-        pos_ids = torch.cat(pos_ids, dim=0).to(grid_thw.device)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(grid_thw.device)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
-
-    def get_window_index(self, grid_thw):
-        '''
-        grid_thw: (tiles, 3) ->
-        '''
-        window_index: list = []
-        cu_window_seqlens: list = [0]
-        window_index_id = 0
-        # 112 // 2 // 14 = 4
-        vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size
-
-        for grid_t, grid_h, grid_w in grid_thw:
-            llm_grid_h, llm_grid_w = (
-                grid_h // self.spatial_merge_size, # 224 // 2 = 112
-                grid_w // self.spatial_merge_size,
-            )
-            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(grid_t, llm_grid_h, llm_grid_w)
-            pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size # vit_merger_window_size = 4
-            pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size
-            num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size # 
-            num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size
-            index_padded = F.pad(index, (0, pad_w, 0, pad_h), "constant", -100)
-            index_padded = index_padded.reshape(
-                grid_t,
-                num_windows_h,
-                vit_merger_window_size,
-                num_windows_w,
-                vit_merger_window_size,
-            )
-            index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(
-                grid_t,
-                num_windows_h * num_windows_w,
-                vit_merger_window_size,
-                vit_merger_window_size,
-            )
-            seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)
-            index_padded = index_padded.reshape(-1)
-            index_new = index_padded[index_padded != -100]
-            # grid_t * llm_grid_h * llm_grid_w: num_windows
-            window_index.append(index_new + window_index_id)
-            # seqlens: [grid_t, num_windows]
-            cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1]
-            cu_window_seqlens.extend(cu_seqlens_tmp.tolist())
-            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()
-        window_index = torch.cat(window_index, dim=0)
-        # window_index: [tiles, num_windows]
-        # cu_window_seqlens: the step of cu_seqlens is window_size, not sampel seq_length
-        return window_index, cu_window_seqlens
-
-    def forward(
-        self,
-        vision_data: Optional[torch.Tensor],
-        grid_thw: torch.Tensor,
-        inference_params: Optional[InferenceParams] = None,
-        extra_block_kwargs: dict = None,
-    ) -> torch.Tensor:
-        """Forward function of the Qwen2 Vision Model. This function passes the input tensors
-        through the embedding layer and then the transformer.
-
-        Args:
-            x (torch.Tensor): input image/video data of shape [n_tokens, n_dims]
-            grid_thw (torch.Tensor): the size tensor indicates grid size of each image/frame
-            packed_seq_params (PackedSeqParams): parameters to build attention mask in the backend
-
-        Returns:
-            x (torch.Tensor): output after final transformer block of shape [b, s, h].
-        """
-        assert grid_thw is not None
-        assert self.input_tensor is None
-        assert inference_params is None
-
-        # Rotary positional embeddings (embedding is None for PP intermediate devices)
-        #vision_data (t, 3) --> (t, embed_dim)
-        vision_data = self.patch_embed(vision_data)
-        # window_index: [tiles, num_windows]   cu_window_seqlens: [tiles * num_windows]
-        window_index, cu_window_seqlens = self.get_window_index(grid_thw)
-        cu_window_seqlens = torch.tensor(
-            cu_window_seqlens,
-            device=vision_data.device,
-            dtype=torch.int32,
-        )
-        cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)
-
-        seq_len, _ = vision_data.size()
-        vision_data = vision_data.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        vision_data = vision_data[window_index, :, :]
-        vision_data = vision_data.reshape(seq_len, 1, -1)
-
-        rotary_pos_emb = self.rot_pos_emb(grid_thw)
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, 1, 1, -1).repeat(1, 1, 1, 2)
-
-        hidden_states = self.decoder(
-            hidden_states = vision_data,
-            attention_mask = None,
-            inference_params = inference_params,
-            rotary_pos_emb=rotary_pos_emb,
-            packed_seq_params=self.build_packed_seq_params(None, cu_window_seqlens),
-            packed_seq_params_full=self.build_packed_seq_params(grid_thw),
-            fullatt_block_indexes=self.fullatt_block_indexes,
-            **(extra_block_kwargs or {}),
-        )
-
-        hidden_states = self.projection(hidden_states.view(-1, self.merge_hidden_size))
-        reverse_indices = torch.argsort(window_index)
-        return hidden_states[reverse_indices, :]
-
-    def build_packed_seq_params(
-        self,
-        grid_thw: Optional[torch.Tensor],
-        cu_seqlens: Optional[torch.Tensor] = None,
-    ) -> PackedSeqParams:
-        # NOTE: each frame is a sequence (rather than each grid)
-        if grid_thw is not None:
-            seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0])
-            cu_seqlens = seqlens.cumsum(dim=0)
-            cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0).int()
-        else: # the step of cu_seqlens is window_size, not sampel seq_length
-            seqlens = cu_seqlens[1:] - cu_seqlens[:-1]
-
-        max_seqlen_q = seqlens.max()
-        return PackedSeqParams(
-            cu_seqlens_q=cu_seqlens,
-            cu_seqlens_kv=cu_seqlens,
-            qkv_format='thd',
-            max_seqlen_q=max_seqlen_q,
-            max_seqlen_kv=max_seqlen_q
-        )
+# Mainly adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/model/qwen2_5_vl/visionmodel.py.
+
+from typing import Optional
+
+import torch
+from torch import nn
+from torch.nn import functional as F
+
+from megatron.core.models.common.vision_module.vision_module import VisionModule
+from megatron.core.transformer.enums import ModelType
+from megatron.core.transformer.spec_utils import ModuleSpec
+from megatron.core.transformer.transformer_config import TransformerConfig
+from megatron.core.packed_seq_params import PackedSeqParams
+from megatron.core import InferenceParams
+from megatron.core.models.vision.multimodal_projector import MultimodalProjector
+
+from flagscale.train.models.qwen2_5_vl.vision_transformer_block import VisionTransformerBlock
+
+
+# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
+class PatchEmbed(nn.Module):
+    def __init__(
+        self,
+        patch_size: int = 14,
+        temporal_patch_size: int = 2,
+        in_channels: int = 3,
+        embed_dim: int = 1152,
+    ) -> None:
+        """
+        Patch Embedding layer for Qwen2.5 Vision Model. Change the original image/video to patches.
+        Merge the patches in the dimension of temporal.
+        Args:
+            patch_size (int): The size of the spatial patch. Defaults to 14.
+            temporal_patch_size (int): The size of the temporal patch. Defaults to 2.
+            in_channels (int): The number of input channels. Defaults to 3.
+            embed_dim (int): The dimension of the embedded representation. Defaults to 1152.
+        """
+        super().__init__()
+        self.patch_size = patch_size
+        self.temporal_patch_size = temporal_patch_size
+        self.in_channels = in_channels
+        self.embed_dim = embed_dim
+
+        kernel_size = [temporal_patch_size, patch_size, patch_size]
+        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=False)
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        """
+        hidden_states: [tiles, in_chanels] --> (num_patches, embed_dim)
+        """
+        target_dtype = self.proj.weight.dtype
+        hidden_states = hidden_states.view(
+            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
+        )
+        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
+        return hidden_states
+
+# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
+class VisionRotaryEmbedding(nn.Module):
+    """
+
+    """
+    def __init__(self, dim: int, theta: float = 10000.0) -> None:
+        super().__init__()
+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
+        self.register_buffer("inv_freq", inv_freq, persistent=False)
+
+    def forward(self, seqlen: int) -> torch.Tensor:
+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
+        # freqs [seq_len, dim // 2]
+        freqs = torch.outer(seq, self.inv_freq)
+        return freqs.float()
+
+# reference from https://github.com/huggingface/transformers/blob/0ad3710d4767d4ac7ee95f33f8554373e59efade/src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py#L243
+class Qwen2_5VisionModel(VisionModule):
+    """Qwen2.5 ViT vision model.
+
+    Args:
+        transformer_config (TransformerConfig): Transformer config.
+        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers.
+        ln_pre_impl (ModuleSpec or type): Specifies the layer norm type to use for ln_pre.
+        add_class_token (bool, optional): Include a class token. Defaults to True.
+        class_token_len (int): Class token length. Defaults to 1 but 8 may be faster.
+        patch_dim (int): Image patch size.
+        img_h (int): Input image height.
+        img_w (int): Input image width.
+    """
+
+    def __init__(
+        self,
+        transformer_config: TransformerConfig,
+        transformer_layer_spec: ModuleSpec,
+        projection_config: TransformerConfig,
+        projection_layer_spec: ModuleSpec,
+        projection_type: str = "mlp",
+
+        pre_process: bool = True,
+        post_process: bool = False
+    ) -> None:
+        super().__init__(config=transformer_config)
+
+        self.spatial_merge_size = transformer_config.spatial_merge_size
+
+        embed_dim = transformer_config.hidden_size
+        num_heads = transformer_config.num_attention_heads
+        temporal_patch_size = transformer_config.temporal_patch_size
+        patch_size = transformer_config.patch_size
+        in_channels = transformer_config.in_channels
+
+        self.patch_size = transformer_config.patch_size
+        self.fullatt_block_indexes = transformer_config.fullatt_block_indexes
+        self.window_size = transformer_config._qwen2_5_vl_window_size
+        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size
+
+        self.max_sequence_length = transformer_config.seq_length
+        self.patch_embed = PatchEmbed(
+            patch_size=patch_size,
+            temporal_patch_size=temporal_patch_size,
+            in_channels=in_channels,
+            embed_dim=embed_dim,
+        )
+
+        head_dim = embed_dim // num_heads
+        self.rotary_pos_emb = VisionRotaryEmbedding(head_dim // 2)
+
+        self.model_type = ModelType.encoder_or_decoder
+        self.pre_process = pre_process
+        self.post_process = post_process
+
+        # Transformer layers.
+        # TODO: Follow-up changes will make pre and post_process configurable. They are needed for supporting pipeline parallelism.
+        # NOTE: a final layer norm and/or linear layer present in some implementations are omitted here.
+        self.decoder = VisionTransformerBlock(
+            config=transformer_config,
+            spec=transformer_layer_spec,
+            pre_process=self.pre_process,
+            post_process=self.post_process,
+            post_layer_norm=True
+        )
+
+        self.merge_hidden_size = projection_config.ffn_hidden_size
+        self.square_merge_size = self.merge_hidden_size // embed_dim
+
+        if self.post_process:
+            self.projection = MultimodalProjector(
+                projection_config,
+                projection_layer_spec,
+                projection_type,
+                projection_config.ffn_hidden_size
+            )
+        else:
+            self.projection = None
+
+        self.input_tensor = None
+
+    def set_input_tensor(self, input_tensor: torch.Tensor) -> None:
+        """Sets input tensor to the model.
+
+        Args:
+            input_tensor (Tensor): Sets the input tensor for the model.
+        """
+        if self.pre_process: # always True
+            self.input_tensor = input_tensor
+        else:
+            raise NotImplementedError()
+
+    def rot_pos_emb(self, grid_thw):
+        pos_ids = []
+        for t, h, w in grid_thw:
+            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
+            hpos_ids = hpos_ids.reshape(
+                h // self.spatial_merge_size,
+                self.spatial_merge_size,
+                w // self.spatial_merge_size,
+                self.spatial_merge_size,
+            )
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3)
+            hpos_ids = hpos_ids.flatten()
+
+            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            wpos_ids = wpos_ids.reshape(
+                h // self.spatial_merge_size,
+                self.spatial_merge_size,
+                w // self.spatial_merge_size,
+                self.spatial_merge_size,
+            )
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3)
+            wpos_ids = wpos_ids.flatten()
+            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
+        pos_ids = torch.cat(pos_ids, dim=0).to(grid_thw.device)
+        max_grid_size = grid_thw[:, 1:].max()
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(grid_thw.device)
+        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
+        return rotary_pos_emb
+
+    def get_window_index(self, grid_thw):
+        '''
+        grid_thw: (tiles, 3) ->
+        '''
+        window_index: list = []
+        cu_window_seqlens: list = [0]
+        window_index_id = 0
+        # 112 // 2 // 14 = 4
+        vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size
+
+        for grid_t, grid_h, grid_w in grid_thw:
+            llm_grid_h, llm_grid_w = (
+                grid_h // self.spatial_merge_size, # 224 // 2 = 112
+                grid_w // self.spatial_merge_size,
+            )
+            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(grid_t, llm_grid_h, llm_grid_w)
+            pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size # vit_merger_window_size = 4
+            pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size
+            num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size # 
+            num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size
+            index_padded = F.pad(index, (0, pad_w, 0, pad_h), "constant", -100)
+            index_padded = index_padded.reshape(
+                grid_t,
+                num_windows_h,
+                vit_merger_window_size,
+                num_windows_w,
+                vit_merger_window_size,
+            )
+            index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(
+                grid_t,
+                num_windows_h * num_windows_w,
+                vit_merger_window_size,
+                vit_merger_window_size,
+            )
+            seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)
+            index_padded = index_padded.reshape(-1)
+            index_new = index_padded[index_padded != -100]
+            # grid_t * llm_grid_h * llm_grid_w: num_windows
+            window_index.append(index_new + window_index_id)
+            # seqlens: [grid_t, num_windows]
+            cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1]
+            cu_window_seqlens.extend(cu_seqlens_tmp.tolist())
+            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()
+        window_index = torch.cat(window_index, dim=0)
+        # window_index: [tiles, num_windows]
+        # cu_window_seqlens: the step of cu_seqlens is window_size, not sampel seq_length
+        return window_index, cu_window_seqlens
+
+    def forward(
+        self,
+        vision_data: Optional[torch.Tensor],
+        grid_thw: torch.Tensor,
+        inference_params: Optional[InferenceParams] = None,
+        extra_block_kwargs: dict = None,
+    ) -> torch.Tensor:
+        """Forward function of the Qwen2 Vision Model. This function passes the input tensors
+        through the embedding layer and then the transformer.
+
+        Args:
+            x (torch.Tensor): input image/video data of shape [n_tokens, n_dims]
+            grid_thw (torch.Tensor): the size tensor indicates grid size of each image/frame
+            packed_seq_params (PackedSeqParams): parameters to build attention mask in the backend
+
+        Returns:
+            x (torch.Tensor): output after final transformer block of shape [b, s, h].
+        """
+        assert grid_thw is not None
+        assert self.input_tensor is None
+        assert inference_params is None
+
+        # Rotary positional embeddings (embedding is None for PP intermediate devices)
+        #vision_data (t, 3) --> (t, embed_dim)
+        vision_data = self.patch_embed(vision_data)
+        # window_index: [tiles, num_windows]   cu_window_seqlens: [tiles * num_windows]
+        window_index, cu_window_seqlens = self.get_window_index(grid_thw)
+        cu_window_seqlens = torch.tensor(
+            cu_window_seqlens,
+            device=vision_data.device,
+            dtype=torch.int32,
+        )
+        cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)
+
+        seq_len, _ = vision_data.size()
+        vision_data = vision_data.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
+        vision_data = vision_data[window_index, :, :]
+        vision_data = vision_data.reshape(seq_len, 1, -1)
+
+        rotary_pos_emb = self.rot_pos_emb(grid_thw)
+        rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
+        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
+        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, 1, 1, -1).repeat(1, 1, 1, 2)
+
+        hidden_states = self.decoder(
+            hidden_states = vision_data,
+            attention_mask = None,
+            inference_params = inference_params,
+            rotary_pos_emb=rotary_pos_emb,
+            packed_seq_params=self.build_packed_seq_params(None, cu_window_seqlens),
+            packed_seq_params_full=self.build_packed_seq_params(grid_thw),
+            fullatt_block_indexes=self.fullatt_block_indexes,
+            **(extra_block_kwargs or {}),
+        )
+
+        hidden_states = self.projection(hidden_states.view(-1, self.merge_hidden_size))
+        reverse_indices = torch.argsort(window_index)
+        return hidden_states[reverse_indices, :]
+
+    def build_packed_seq_params(
+        self,
+        grid_thw: Optional[torch.Tensor],
+        cu_seqlens: Optional[torch.Tensor] = None,
+    ) -> PackedSeqParams:
+        # NOTE: each frame is a sequence (rather than each grid)
+        if grid_thw is not None:
+            seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0])
+            cu_seqlens = seqlens.cumsum(dim=0)
+            cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0).int()
+        else: # the step of cu_seqlens is window_size, not sampel seq_length
+            seqlens = cu_seqlens[1:] - cu_seqlens[:-1]
+
+        max_seqlen_q = seqlens.max()
+        return PackedSeqParams(
+            cu_seqlens_q=cu_seqlens,
+            cu_seqlens_kv=cu_seqlens,
+            qkv_format='thd',
+            max_seqlen_q=max_seqlen_q,
+            max_seqlen_kv=max_seqlen_q
+        )
diff --git a/flagscale/train/train.py b/flagscale/train/train.py
index 7039c1a2..5da202d0 100644
--- a/flagscale/train/train.py
+++ b/flagscale/train/train.py
@@ -14,20 +14,16 @@ from typing import List
 
 import torch.distributed
 from megatron.training.log_handler import CustomHandler
-
 # Make default logging level INFO, but filter out all log messages not from MCore.
 logging.basicConfig(handlers=[CustomHandler()], level=logging.INFO)
 from megatron.training.theoretical_memory_usage import report_theoretical_memory
 import time
-
 # The earliest we can measure the start time.
 _TRAIN_START_TIME = time.time()
 import torch
 
 try:
-    from megatron.post_training.algos.distillation import (
-        get_tensor_shapes_adjust_fn_for_distillation,
-    )
+    from megatron.post_training.algos.distillation import get_tensor_shapes_adjust_fn_for_distillation
 
     has_nvidia_modelopt = True
 except ImportError:
@@ -45,10 +41,9 @@ from megatron.training.checkpointing import load_checkpoint
 from megatron.training.checkpointing import save_checkpoint
 from megatron.training.checkpointing import checkpoint_exists
 from megatron.core.transformer.module import Float16Module
-from megatron.core.distributed import DistributedDataParallelConfig, TorchFullyShardedDataParallelConfig
+from megatron.core.distributed import DistributedDataParallelConfig
 from megatron.core.distributed import DistributedDataParallel as DDP
 from megatron.core.distributed.custom_fsdp import FullyShardedDataParallel as custom_FSDP
-
 try:
     from megatron.core.distributed import TorchFullyShardedDataParallel as torch_FSDP
 
@@ -68,7 +63,10 @@ from megatron.core.rerun_state_machine import (
 from megatron.training.initialize import initialize_megatron
 from megatron.training.initialize import write_args_to_tensorboard
 from megatron.training.initialize import set_jit_fusion_options
-from megatron.training.utils import get_batch_on_this_cp_rank, get_batch_on_this_tp_rank
+from megatron.training.utils import (
+    get_batch_on_this_cp_rank,
+    get_batch_on_this_tp_rank,
+)
 from megatron.legacy.data.data_samplers import build_pretraining_data_loader
 from megatron.core.optimizer_param_scheduler import OptimizerParamScheduler
 from megatron.core.transformer.moe import upcycling_utils
@@ -91,8 +89,7 @@ from megatron.core.num_microbatches_calculator import (
     get_current_global_batch_size,
     get_current_running_global_batch_size,
     get_num_microbatches,
-    update_num_microbatches,
-)
+    update_num_microbatches)
 
 from megatron.training.async_utils import maybe_finalize_async_save
 from megatron.training.utils import (
@@ -164,22 +161,15 @@ def num_floating_point_operations(args, batch_size):
     def mlp_layer_flops(batch_size, seq_len, hidden_size, expansion=4.0, swiglu=False):
         """Calculate FLOPs for an MLP layer."""
         scale_factor = 3.0 / 2.0 if swiglu else 1.0
-        return 4 * expansion * scale_factor * batch_size * seq_len * hidden_size**2
+        return 4 * expansion * scale_factor * batch_size * seq_len * hidden_size ** 2
 
-    def attn_layer_flops(
-        batch_size, seq_len, hidden_size, num_heads, gqa=True, gqa_groups=8, kv_channels=None
-    ):
+    def attn_layer_flops(batch_size, seq_len, hidden_size, num_heads, gqa=True,
+                         gqa_groups=8, kv_channels=None):
         """Calculate FLOPs for an attention layer."""
         p = (kv_channels * num_heads / hidden_size) if kv_channels else 1
         g = gqa_groups if gqa else num_heads
-        return (
-            4
-            * batch_size
-            * seq_len
-            * hidden_size
-            * p
-            * (hidden_size + (hidden_size * (g / num_heads)) + (seq_len / 2))
-        )
+        return 4 * batch_size * seq_len * hidden_size * p * (
+                hidden_size + (hidden_size * (g / num_heads)) + (seq_len / 2 ))
 
     def mamba_layer_flops(batch_size, seq_len, hidden_size, state_dim=16,
                           head_dim=64, num_groups=1, num_heads=128):
@@ -192,15 +182,10 @@ def num_floating_point_operations(args, batch_size):
         else:
             nheads = d_in // head_dim
         return (
-            (
-                2
-                * batch_size
-                * seq_len
-                * hidden_size
-                * (2 * d_in + 2 * num_groups * state_dim + nheads)
-            )  # in_proj
-            + (7 * batch_size * seq_len * d_in * state_dim)  # scan
-            + (2 * batch_size * seq_len * d_in * hidden_size)  # out_proj
+                (2 * batch_size * seq_len * hidden_size * (
+                        2 * d_in + 2 * num_groups * state_dim + nheads)) +  # in_proj
+                (7 * batch_size * seq_len * d_in * state_dim) +  # scan
+                (2 * batch_size * seq_len * d_in * hidden_size)  # out_proj
         )
 
     def hybrid_flops(batch_size, seq_len, hidden_size,
@@ -269,11 +254,7 @@ def num_floating_point_operations(args, batch_size):
             mtp_num_layers = 0
             num_layers = args.num_layers
 
-        moe_ffn_hidden_size = (
-            args.moe_ffn_hidden_size
-            if args.moe_ffn_hidden_size is not None
-            else args.ffn_hidden_size
-        )
+        moe_ffn_hidden_size = args.moe_ffn_hidden_size if args.moe_ffn_hidden_size is not None else args.ffn_hidden_size
         shared_expert_ffn_hidden_size = (
             0
             if args.moe_shared_expert_intermediate_size is None
@@ -309,38 +290,26 @@ def num_floating_point_operations(args, batch_size):
             '''
             ## MLA
             if args.q_lora_rank is None:
-                q_term = (
-                    args.hidden_size
-                    * args.num_attention_heads
-                    * (args.qk_head_dim + args.qk_pos_emb_head_dim)
-                )
+                q_term = args.hidden_size * args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim)
             else:
-                q_term = args.q_lora_rank * (
-                    args.hidden_size
-                    + args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim)
-                    + 1
-                )
+                q_term = args.q_lora_rank * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim) + 1)
             self_attn_term = (
-                3
-                * 2  # fwd(1) + bwd(2) *FMA
+                3*2 # fwd(1) + bwd(2) *FMA
                 * num_layers
                 * (
                     ## q lora + rope + q norm
                     q_term
+
                     ## kv lora + rope + kv norm
                     + args.kv_lora_rank
-                    * (
-                        args.hidden_size
-                        + args.num_attention_heads * (args.qk_head_dim + args.v_head_dim)
-                        + 1
-                    )
+                    * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.v_head_dim) + 1)
                     + args.hidden_size * args.qk_pos_emb_head_dim
+
                     ## o proj
                     + (args.num_attention_heads * args.v_head_dim) * args.hidden_size
+
                     ## core attn
-                    + args.seq_length
-                    * (args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim))
-                    / 2
+                    + args.seq_length * (args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim)) / 2
                     + args.seq_length * args.num_attention_heads * args.v_head_dim / 2
                 )
             )
@@ -358,48 +327,53 @@ def num_floating_point_operations(args, batch_size):
                         + (args.num_query_groups / args.num_attention_heads)
                         # # Only half of the attention matrix is non-zero and needs to be multiplied with V.
                         + (args.seq_length / args.hidden_size / 2)
-                    )
-                    * query_projection_to_hidden_size_ratio
+                    ) * query_projection_to_hidden_size_ratio
                 )
             )
 
-        total_floating_point_operations = (
-            batch_size
-            * args.seq_length
+        total_floating_point_operations = batch_size * args.seq_length * (
+            # MLP
+            expansion_factor
+            * num_layers
+            * args.hidden_size
             * (
-                # MLP
-                expansion_factor
-                * num_layers
-                * args.hidden_size
-                * (
-                    # dense layer (deepseek v2, v3 style)
-                    (args.ffn_hidden_size * gated_linear_multiplier)
-                    * (num_dense_layers / num_layers)
-                    # routed experts
-                    + (moe_ffn_hidden_size * num_experts_routed_to * gated_linear_multiplier)
-                    * (num_moe_layers / num_layers)
-                    # Shared Experts.
-                    + (shared_expert_ffn_hidden_size * gated_linear_multiplier)
-                    * (num_moe_layers / num_layers)
-                )
-                # Self Attention
-                + self_attn_term
-                # MTP norms and proj
-                + 3
-                * 2
-                * mtp_num_layers
-                * (
-                    # MTP eh norm + final nrom
-                    3 * args.hidden_size
-                    # MTH eh proj
-                    + 2 * args.hidden_size * args.hidden_size
-                )
-                # Logit.
-                + 3 * 2 * args.hidden_size * args.padded_vocab_size * (mtp_num_layers + 1)
+                # dense layer (deepseek v2, v3 style)
+                (
+                    args.ffn_hidden_size
+                    * gated_linear_multiplier
+                ) * (num_dense_layers/num_layers)
+                # routed experts
+                + (
+                    moe_ffn_hidden_size
+                    * num_experts_routed_to
+                    * gated_linear_multiplier
+                ) * (num_moe_layers/num_layers)
+                # Shared Experts.
+                + (
+                    shared_expert_ffn_hidden_size
+                    * gated_linear_multiplier
+                ) * (num_moe_layers/num_layers)
             )
+            # Self Attention
+            + self_attn_term
+            # MTP norms and proj
+            + 3*2
+            * mtp_num_layers
+            * (
+                # MTP eh norm + final nrom
+                3 * args.hidden_size
+                # MTH eh proj
+                + 2 * args.hidden_size * args.hidden_size
+            )
+            # Logit.
+            + 3*2
+            * args.hidden_size
+            * args.padded_vocab_size
+            * (mtp_num_layers + 1)
         )
         return total_floating_point_operations
 
+
     # Main entrypoint for FLOPs calculation.
     if args.is_hybrid_model:
         # Calculate the number of each type of layer.
@@ -423,7 +397,7 @@ def num_floating_point_operations(args, batch_size):
             kv_channels=args.kv_channels,
             mlp_expansion=args.ffn_hidden_size / args.hidden_size,
             swiglu=args.swiglu,
-            vocab_size=args.padded_vocab_size,
+            vocab_size=args.padded_vocab_size
         )
     else:
         # Compute standard Transformer model FLOPs.
@@ -656,7 +630,8 @@ def get_start_time_from_progress_log():
             line_tokens = line.split('\t')
             world_size_in_line = _get_field(line_tokens[2], int)
             if line_tokens[3] == "Saved checkpoint":
-                latest_num_floating_point_operations = _get_field(line_tokens[7], float)
+                latest_num_floating_point_operations = \
+                    _get_field(line_tokens[7], float)
             if world_size_in_line != args.world_size:
                 # Re-start search if we see a different world size.
                 start_time = None
@@ -665,16 +640,16 @@ def get_start_time_from_progress_log():
             if line_tokens[3] == "Starting job":
                 if start_time is None:
                     start_time = line_tokens[0]
-                    start_num_floating_point_operations = latest_num_floating_point_operations
-    assert (
-        start_time is not None and start_num_floating_point_operations is not None
-    ), "Should have seen at least one 'Starting job' entry with same world_size"
-    return datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S'), start_num_floating_point_operations
+                    start_num_floating_point_operations = \
+                        latest_num_floating_point_operations
+    assert start_time is not None and start_num_floating_point_operations is not None, \
+        "Should have seen at least one 'Starting job' entry with same world_size"
+    return datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S'), \
+        start_num_floating_point_operations
 
 
 def preprocess_common_state_dict(common_state_dict):
     import copy
-
     # Convert args key of type namespace to dictionary
     preprocessed_common_state_dict = copy.deepcopy(common_state_dict)
     preprocessed_common_state_dict['args'] = vars(preprocessed_common_state_dict['args'])
@@ -914,7 +889,7 @@ def pretrain(
         extra_args_provider=extra_args_provider,
         args_defaults=args_defaults,
         get_embedding_ranks=get_embedding_ranks,
-        get_position_embedding_ranks=get_position_embedding_ranks,
+        get_position_embedding_ranks=get_position_embedding_ranks
     )
 
     args = get_args()
@@ -937,19 +912,24 @@ def pretrain(
     # image ... launches.
     global _TRAIN_START_TIME
     if "cpu:gloo" == torch.distributed.get_backend():
-        start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device='cpu')
+        start_time_tensor = torch.tensor([_TRAIN_START_TIME],
+                                         dtype=torch.double,
+                                         device='cpu')
     else:
-        start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device='cuda')
-    torch.distributed.all_reduce(start_time_tensor, op=torch.distributed.ReduceOp.MIN)
+        start_time_tensor = torch.tensor([_TRAIN_START_TIME],
+                                         dtype=torch.double,
+                                         device='cuda')
+
+    torch.distributed.all_reduce(start_time_tensor,
+                                 op=torch.distributed.ReduceOp.MIN)
     _TRAIN_START_TIME = start_time_tensor.item()
 
     app_metrics = {}
     app_metrics['app_start_time'] = round(_TRAIN_START_TIME * 1000.0)
     app_metrics['app_model_init_start_time'] = round(_TRAIN_START_TIME * 1000.0)
 
-    print_rank_0(
-        'time to initialize megatron (seconds): {:.3f}'.format(time.time() - _TRAIN_START_TIME)
-    )
+    print_rank_0('time to initialize megatron (seconds): {:.3f}'.format(
+        time.time() - _TRAIN_START_TIME))
     print_datetime('after megatron is initialized')
     app_metrics['app_model_init_finish_time'] = one_logger_utils.get_timestamp_in_ms()
 
@@ -959,33 +939,28 @@ def pretrain(
     # Context used for persisting some state between checkpoint saves.
     if args.non_persistent_ckpt_type == 'local':
         try:
-            from nvidia_resiliency_ext.checkpointing.local.ckpt_managers.local_manager import (
-                LocalCheckpointManager,
-            )
-            from nvidia_resiliency_ext.checkpointing.local.replication.group_utils import (
-                parse_group_sequence,
-                GroupWrapper,
-            )
-            from nvidia_resiliency_ext.checkpointing.local.replication.strategies import (
-                CliqueReplicationStrategy,
-            )
+            from nvidia_resiliency_ext.checkpointing.local.ckpt_managers.local_manager import \
+                LocalCheckpointManager
+            from nvidia_resiliency_ext.checkpointing.local.replication.group_utils import \
+                parse_group_sequence, GroupWrapper
+            from nvidia_resiliency_ext.checkpointing.local.replication.strategies import \
+                CliqueReplicationStrategy
         except ModuleNotFoundError:
-            raise RuntimeError(
-                "The 'nvidia_resiliency_ext' module is required for local "
-                "checkpointing but was not found. Please ensure it is installed."
-            )
+            raise RuntimeError("The 'nvidia_resiliency_ext' module is required for local "
+                               "checkpointing but was not found. Please ensure it is installed.")
 
         if args.replication:
             repl_strategy = CliqueReplicationStrategy.from_replication_params(
-                args.replication_jump, args.replication_factor
+                args.replication_jump,
+                args.replication_factor
             )
         else:
             repl_strategy = None
 
         checkpointing_context = {
-            'local_checkpoint_manager': LocalCheckpointManager(
-                args.non_persistent_local_ckpt_dir, repl_strategy=repl_strategy
-            )
+            'local_checkpoint_manager': LocalCheckpointManager(args.non_persistent_local_ckpt_dir,
+                                                               repl_strategy=repl_strategy
+                                                               )
         }
     else:
         checkpointing_context = {}
@@ -999,50 +974,46 @@ def pretrain(
     timers('model-and-optimizer-setup', log_level=0).start(barrier=True)
     app_metrics['app_build_optimizer_start_time'] = one_logger_utils.get_timestamp_in_ms()
     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
-        model_provider, model_type, checkpointing_context=checkpointing_context
-    )
+        model_provider, model_type, checkpointing_context=checkpointing_context)
 
     timers('model-and-optimizer-setup').stop()
-    print_datetime('after model, optimizer, and learning rate ' 'scheduler are built')
+    print_datetime('after model, optimizer, and learning rate '
+                   'scheduler are built')
     app_metrics['app_build_optimizer_finish_time'] = one_logger_utils.get_timestamp_in_ms()
     config = get_model_config(model[0])
 
     # Data stuff.
     app_metrics['app_build_dataiters_start_time'] = one_logger_utils.get_timestamp_in_ms()
-    timers('train/valid/test-data-iterators-setup', log_level=0).start(barrier=True)
+    timers('train/valid/test-data-iterators-setup', log_level=0).start(
+        barrier=True)
     if args.virtual_pipeline_model_parallel_size is not None:
         train_data_iterator = []
         valid_data_iterator = []
         test_data_iterator = []
         for i in range(len(model)):
             mpu.set_virtual_pipeline_model_parallel_rank(i)
-            iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
+            iterators = build_train_valid_test_data_iterators(
+                train_valid_test_dataset_provider)
             train_data_iterator.append(iterators[0])
             valid_data_iterator.append(iterators[1])
             test_data_iterator.append(iterators[2])
     else:
-        train_data_iterator, valid_data_iterator, test_data_iterator = (
-            build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
-        )
+        train_data_iterator, valid_data_iterator, test_data_iterator \
+            = build_train_valid_test_data_iterators(
+                train_valid_test_dataset_provider)
     timers('train/valid/test-data-iterators-setup').stop()
     print_datetime('after dataloaders are built')
     app_metrics['app_build_dataiters_finish_time'] = one_logger_utils.get_timestamp_in_ms()
 
     # Track if training is enabled. Can only be done once args.do_train is assigned after dataloader is built.
-    one_logger_utils.track_config_flags(
-        args.train_iters,
-        args.skip_train,
-        args.do_train,
-        args.do_valid,
-        args.do_test,
-        args.dataloader_type,
-        args.retro_project_dir,
-        args.retro_cyclic_train_iters,
-    )
+    one_logger_utils.track_config_flags(args.train_iters, args.skip_train, args.do_train,
+                                        args.do_valid, args.do_test, args.dataloader_type,
+                                        args.retro_project_dir, args.retro_cyclic_train_iters)
 
     # Print setup timing.
     print_rank_0('done with setup ...')
-    timers.log(['model-and-optimizer-setup', 'train/valid/test-data-iterators-setup'], barrier=True)
+    timers.log(['model-and-optimizer-setup',
+                'train/valid/test-data-iterators-setup'], barrier=True)
 
     one_logger = get_one_logger()
     one_logger and one_logger.log_metrics(app_metrics)
@@ -1059,36 +1030,23 @@ def pretrain(
         if args.do_train and args.train_iters > 0:
             iteration, num_floating_point_operations_so_far = train(
                 forward_step_func,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                train_data_iterator,
-                valid_data_iterator,
-                process_non_loss_data_func,
-                config,
-                checkpointing_context,
-                non_loss_data_func,
-                extra_valid_dataset_provider,
-            )
+                model, optimizer, opt_param_scheduler,
+                train_data_iterator, valid_data_iterator,
+                process_non_loss_data_func, config, checkpointing_context,
+                non_loss_data_func, extra_valid_dataset_provider)
 
         print_datetime('after training is done')
 
         if not args.auto_tune:
             if args.save and iteration != 0 and iteration % args.save_interval != 0:
-                save_checkpoint(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                    preprocess_common_state_dict_fn=preprocess_common_state_dict,
-                )
+                save_checkpoint(iteration, model, optimizer, opt_param_scheduler,
+                                num_floating_point_operations_so_far, checkpointing_context,
+                                train_data_iterator=train_data_iterator,
+                                preprocess_common_state_dict_fn=preprocess_common_state_dict)
 
-        one_logger and one_logger.log_metrics(
-            {'app_train_loop_finish_time': one_logger_utils.get_timestamp_in_ms()}
-        )
+        one_logger and one_logger.log_metrics({
+            'app_train_loop_finish_time': one_logger_utils.get_timestamp_in_ms()
+        })
 
     else:
         print_rank_0('skipping training (--skip-train is on) ...')
@@ -1097,33 +1055,19 @@ def pretrain(
 
     if args.do_valid:
         prefix = f'iteration {iteration} on validation set'
-        evaluate_and_print_results(
-            prefix,
-            forward_step_func,
-            valid_data_iterator,
-            model,
-            iteration,
-            process_non_loss_data_func,
-            config,
-            verbose=True,
-            write_to_tensorboard=not args.skip_train,
-            non_loss_data_func=non_loss_data_func,
-        )
+        evaluate_and_print_results(prefix, forward_step_func,
+                                   valid_data_iterator, model,
+                                   iteration, process_non_loss_data_func, config,
+                                   verbose=True, write_to_tensorboard=not args.skip_train,
+                                   non_loss_data_func=non_loss_data_func)
 
     if args.do_test:
         prefix = f'iteration {iteration} on test set'
-        evaluate_and_print_results(
-            prefix,
-            forward_step_func,
-            test_data_iterator,
-            model,
-            iteration,
-            process_non_loss_data_func,
-            config,
-            verbose=True,
-            write_to_tensorboard=not args.skip_train,
-            non_loss_data_func=non_loss_data_func,
-        )
+        evaluate_and_print_results(prefix, forward_step_func,
+                                   test_data_iterator, model,
+                                   iteration, process_non_loss_data_func, config,
+                                   verbose=True, write_to_tensorboard=not args.skip_train,
+                                   non_loss_data_func=non_loss_data_func)
 
     if extra_valid_dataset_provider is not None:
         # NOTE(zhaoyinglia): Must rebuild the dataloaders for extra validation here,
@@ -1133,28 +1077,20 @@ def pretrain(
             extra_valid_data_iterator = []
             for i in range(len(model)):
                 mpu.set_virtual_pipeline_model_parallel_rank(i)
-                extra_iterators = build_extra_valid_data_iterators(extra_valid_dataset_provider)
+                extra_iterators = build_extra_valid_data_iterators(
+                    extra_valid_dataset_provider)
                 extra_valid_data_iterator.append(extra_iterators)
         else:
-            extra_valid_data_iterator = (
-                build_extra_valid_data_iterators(extra_valid_dataset_provider)
-            )
+            extra_valid_data_iterator = build_extra_valid_data_iterators(
+                extra_valid_dataset_provider)
         if getattr(args, "do_extra_valid", False):
             prefix = f'iteration {iteration} on extra validation set'
             for extra_valid_index, extra_valid_data_itr in enumerate(extra_valid_data_iterator):
-                extra_evaluate_and_print_results(
-                    extra_valid_index,
-                    prefix,
-                    forward_step_func,
-                    extra_valid_data_itr,
-                    model,
-                    iteration,
-                    process_non_loss_data_func,
-                    config,
-                    verbose=True,
-                    write_to_tensorboard=not args.skip_train,
-                    non_loss_data_func=non_loss_data_func
-                )
+                extra_evaluate_and_print_results(extra_valid_index, prefix, forward_step_func,
+                                                 extra_valid_data_itr, model,
+                                                 iteration, process_non_loss_data_func, config,
+                                                 verbose=True, write_to_tensorboard=not args.skip_train,
+                                                 non_loss_data_func=non_loss_data_func)
 
     wandb_writer = get_wandb_writer()
     if wandb_writer:
@@ -1164,9 +1100,9 @@ def pretrain(
     maybe_finalize_async_save(blocking=True, terminate=True)
     ft_integration.on_checkpointing_end(is_async_finalization=True)
 
-    one_logger and one_logger.log_metrics(
-        {'app_finish_time': one_logger_utils.get_timestamp_in_ms()}
-    )
+    one_logger and one_logger.log_metrics({
+        'app_finish_time': one_logger_utils.get_timestamp_in_ms()
+    })
 
     ft_integration.shutdown()
     one_logger_utils.finish()
@@ -1187,10 +1123,7 @@ def update_train_iters(args):
         iterations = 0
         consumed_samples = 0
         # Rampup phase.
-        while (
-            consumed_samples <= int(args.rampup_batch_size[2])
-            and consumed_samples <= args.train_samples
-        ):
+        while consumed_samples <= int(args.rampup_batch_size[2]) and consumed_samples <= args.train_samples:
             update_num_microbatches(consumed_samples, consistency_check=False)
             consumed_samples += get_current_global_batch_size()
             iterations += 1
@@ -1199,7 +1132,8 @@ def update_train_iters(args):
         # Constant phase
         # Note that we throw away any partial last batch.
         if args.train_samples > consumed_samples:
-            iterations += (args.train_samples - consumed_samples) // args.global_batch_size
+            iterations += (args.train_samples - consumed_samples) // \
+                          args.global_batch_size
         args.train_iters = iterations
 
     print_rank_0(f'setting training iterations to {args.train_iters}')
@@ -1212,28 +1146,26 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
 
     # Build model.
     def build_model():
-        if (
-            mpu.get_pipeline_model_parallel_world_size() > 1
-            and args.virtual_pipeline_model_parallel_size is not None
-        ):
+        if mpu.get_pipeline_model_parallel_world_size() > 1 and \
+        args.virtual_pipeline_model_parallel_size is not None:
             if model_type == ModelType.encoder_and_decoder:
-                assert (
-                    args.encoder_pipeline_model_parallel_size == 0
-                ), "Interleaved schedule not supported for model with encoder on separate PP rank"
+                assert args.encoder_pipeline_model_parallel_size == 0, \
+                    "Interleaved schedule not supported for model with encoder on separate PP rank"
             model = []
             for i in range(args.virtual_pipeline_model_parallel_size):
                 mpu.set_virtual_pipeline_model_parallel_rank(i)
                 # Set pre_process and post_process only after virtual rank is set.
-                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False)
-                post_process = mpu.is_pipeline_last_stage(ignore_virtual=False)
+                pre_process = mpu.is_pipeline_first_stage()
+                post_process = mpu.is_pipeline_last_stage()
                 this_model = model_provider_func(
-                    pre_process=pre_process, post_process=post_process, vp_stage=i)
+                    pre_process=pre_process,
+                    post_process=post_process
+                )
                 this_model.model_type = model_type
-                this_model.vp_stage = i
                 model.append(this_model)
         else:
-            pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False)
-            post_process = mpu.is_pipeline_last_stage(ignore_virtual=False)
+            pre_process = mpu.is_pipeline_first_stage()
+            post_process = mpu.is_pipeline_last_stage()
             add_encoder = True
             add_decoder = True
             if model_type == ModelType.encoder_and_decoder:
@@ -1249,13 +1181,14 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
                     pre_process=pre_process,
                     post_process=post_process,
                     add_encoder=add_encoder,
-                    add_decoder=add_decoder,
-                )
+                    add_decoder=add_decoder)
             else:
-                model = model_provider_func(pre_process=pre_process, post_process=post_process)
+                model = model_provider_func(
+                    pre_process=pre_process,
+                    post_process=post_process
+                )
             model.model_type = model_type
         return model
-
     if args.init_model_with_meta_device:
         with torch.device('meta'):
             model = build_model()
@@ -1275,26 +1208,20 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
 
     # Print number of parameters.
     num_parameters = sum(
-        [sum([p.nelement() for p in model_module.parameters()]) for model_module in model]
+        [sum([p.nelement() for p in model_module.parameters()])
+         for model_module in model]
     )
     if mpu.get_data_parallel_rank() == 0:
-        print(
-            ' > number of parameters on (tensor, pipeline) '
-            'model parallel rank ({}, {}): {}'.format(
-                mpu.get_tensor_model_parallel_rank(),
-                mpu.get_pipeline_model_parallel_rank(),
-                num_parameters,
-            ),
-            flush=True,
-        )
+        print(' > number of parameters on (tensor, pipeline) '
+              'model parallel rank ({}, {}): {}'.format(
+            mpu.get_tensor_model_parallel_rank(),
+            mpu.get_pipeline_model_parallel_rank(),
+            num_parameters), flush=True)
 
     # GPU allocation.
     # For FSDP2, we don't allocate GPU memory here. We allocate GPU memory
     # in the fully_shard function of FSDP2 instead.
-    if (
-        not (args.use_torch_fsdp2 and args.use_cpu_initialization)
-        and not args.init_model_with_meta_device
-    ):
+    if not (args.use_torch_fsdp2 and args.use_cpu_initialization) and not args.init_model_with_meta_device:
         for model_module in model:
             model_module.cuda(torch.cuda.current_device())
 
@@ -1321,32 +1248,30 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
 
         config = get_model_config(model[0])
 
-        if getattr(args, "use_torch_fsdp2", False):
-            reshard_after_forward = getattr(args, "torch_fsdp2_reshard_after_forward", True)
-            ddp_config = TorchFullyShardedDataParallelConfig(reshard_after_forward=reshard_after_forward)
+        kwargs = {}
+        for f in dataclasses.fields(DistributedDataParallelConfig):
+            if hasattr(args, f.name):
+                kwargs[f.name] = getattr(args, f.name)
+        kwargs['grad_reduce_in_fp32'] = args.accumulate_allreduce_grads_in_fp32
+        kwargs['check_for_nan_in_grad'] = args.check_for_nan_in_loss_and_grad
+        kwargs['check_for_large_grads'] = args.check_for_large_grads
+        if args.ddp_num_buckets is not None:
+            assert args.ddp_bucket_size is None, \
+                "Cannot specify both --ddp-num-buckets and --ddp-bucket-size"
+            assert args.ddp_num_buckets > 0, \
+                "--ddp-num-buckets must be greater than 0"
+            kwargs['bucket_size'] = num_parameters // args.ddp_num_buckets
         else:
-            kwargs = {}
-            for f in dataclasses.fields(DistributedDataParallelConfig):
-                if hasattr(args, f.name):
-                    kwargs[f.name] = getattr(args, f.name)
-            kwargs['grad_reduce_in_fp32'] = args.accumulate_allreduce_grads_in_fp32
-            kwargs['check_for_nan_in_grad'] = args.check_for_nan_in_loss_and_grad
-            kwargs['check_for_large_grads'] = args.check_for_large_grads
-            if args.ddp_num_buckets is not None:
-                assert args.ddp_bucket_size is None, \
-                    "Cannot specify both --ddp-num-buckets and --ddp-bucket-size"
-                assert args.ddp_num_buckets > 0, \
-                    "--ddp-num-buckets must be greater than 0"
-                kwargs['bucket_size'] = num_parameters // args.ddp_num_buckets
-            else:
-                kwargs['bucket_size'] = args.ddp_bucket_size
-            kwargs['pad_buckets_for_high_nccl_busbw'] = args.ddp_pad_buckets_for_high_nccl_busbw
-            kwargs['average_in_collective'] = args.ddp_average_in_collective
-            if args.use_custom_fsdp and args.use_precision_aware_optimizer:
-                kwargs["preserve_fp32_weights"] = False
-            ddp_config = DistributedDataParallelConfig(**kwargs)
-
+            kwargs['bucket_size'] = args.ddp_bucket_size
+        kwargs['pad_buckets_for_high_nccl_busbw'] = args.ddp_pad_buckets_for_high_nccl_busbw
+        kwargs['average_in_collective'] = args.ddp_average_in_collective
+        if args.use_custom_fsdp and args.use_precision_aware_optimizer:
+            kwargs["preserve_fp32_weights"] = False
+        ddp_config = DistributedDataParallelConfig(**kwargs)
+
+        if not getattr(args, "use_torch_fsdp2", False):
             # In the custom FSDP and DDP use path, we need to initialize the bucket size.
+
             # If bucket_size is not provided as an input, use sane default.
             # If using very large dp_sizes, make buckets larger to ensure that chunks used in NCCL
             # ring-reduce implementations are large enough to remain bandwidth-bound rather than
@@ -1359,18 +1284,13 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
             if not ddp_config.overlap_grad_reduce:
                 ddp_config.bucket_size = None
 
-        model = [
-            DP(
-                config=config,
-                ddp_config=ddp_config,
-                module=model_chunk,
-                # Turn off bucketing for model_chunk 2 onwards, since communication for these
-                # model chunks is overlapped with compute anyway.
-                disable_bucketing=(model_chunk_idx > 0)
-                or args.overlap_param_gather_with_optimizer_step,
-            )
-            for (model_chunk_idx, model_chunk) in enumerate(model)
-        ]
+        model = [DP(config=config,
+                     ddp_config=ddp_config,
+                     module=model_chunk,
+                     # Turn off bucketing for model_chunk 2 onwards, since communication for these
+                     # model chunks is overlapped with compute anyway.
+                     disable_bucketing=(model_chunk_idx > 0) or args.overlap_param_gather_with_optimizer_step)
+                 for (model_chunk_idx, model_chunk) in enumerate(model)]
 
         # Broadcast params from data parallel src rank to other data parallel ranks.
         if args.data_parallel_random_init:
@@ -1413,7 +1333,8 @@ def get_optimizer_param_scheduler(optimizer):
         else:
             lr_warmup_steps = args.lr_warmup_samples
     else:
-        raise Exception('either train-iters or train-samples should be provided.')
+        raise Exception(
+            'either train-iters or train-samples should be provided.')
 
     stablelm2_scheduler_config = None
     if args.lr_decay_style == 'stablelm2-scheduler':
@@ -1443,20 +1364,17 @@ def get_optimizer_param_scheduler(optimizer):
         override_opt_param_scheduler=args.override_opt_param_scheduler,
         wsd_decay_steps=wsd_decay_steps,
         lr_wsd_decay_style=args.lr_wsd_decay_style,
-        stablelm2_scheduler_config=stablelm2_scheduler_config,
-    )
+        stablelm2_scheduler_config=stablelm2_scheduler_config)
 
     return opt_param_scheduler
 
 
-def setup_model_and_optimizer(
-    model_provider_func,
-    model_type,
-    no_wd_decay_cond=None,
-    scale_lr_cond=None,
-    lr_mult=1.0,
-    checkpointing_context=None,
-):
+def setup_model_and_optimizer(model_provider_func,
+                              model_type,
+                              no_wd_decay_cond=None,
+                              scale_lr_cond=None,
+                              lr_mult=1.0,
+                              checkpointing_context=None):
     """Setup model and optimizer."""
     args = get_args()
     timers = get_timers()
@@ -1477,24 +1395,19 @@ def setup_model_and_optimizer(
                 kwargs[f.name] = getattr(args, f.name)
         config = OptimizerConfig(**kwargs)
     config.timers = timers
-    optimizer = get_megatron_optimizer(
-        config,
-        model,
-        no_wd_decay_cond,
-        scale_lr_cond,
-        lr_mult,
-        use_gloo_process_groups=args.enable_gloo_process_groups,
-    )
+    optimizer = get_megatron_optimizer(config, model, no_wd_decay_cond,
+                                       scale_lr_cond, lr_mult,
+                                       use_gloo_process_groups=args.enable_gloo_process_groups)
     opt_param_scheduler = get_optimizer_param_scheduler(optimizer)
 
     if args.moe_use_upcycling:
         torch.distributed.barrier()
-        assert not checkpoint_exists(args.save), (
-            "The upcycling destination directory already exists. "
+        assert not checkpoint_exists(
+            args.save
+        ), ("The upcycling destination directory already exists. "
             "Please check if --moe-use-upcycling is mistakenly enabled. "
             "Upcycling should only be set for the first run when converting the dense model. "
-            "All subsequent runs should remove this flag. "
-        )
+            "All subsequent runs should remove this flag. ")
         num_experts = args.num_experts
         args.num_experts = None
         expert_model_parallel_size = args.expert_model_parallel_size
@@ -1506,57 +1419,38 @@ def setup_model_and_optimizer(
             load_checkpoint,
             unwrapped_model,
             dense_model_for_upcycling,
-            load_kwargs={
-                'model': dense_model_for_upcycling,
-                'optimizer': None,
-                'opt_param_scheduler': None,
-            },
+            load_kwargs = {'model': dense_model_for_upcycling, 'optimizer': None, 'opt_param_scheduler': None}
         )
         args.iteration = 1
-        save_checkpoint(
-            args.iteration, model, None, None, args.num_floating_point_operations_so_far
-        )
+        save_checkpoint(args.iteration, model, None, None, args.num_floating_point_operations_so_far)
         torch.distributed.barrier()
         del dense_model_for_upcycling
         if (args.fp16 or args.bf16) and optimizer is not None:
             optimizer.reload_model_params()
         print_rank_0(f'Upcycled checkpoint saved to {args.save}')
 
-    if (
-        args.load is not None or args.pretrained_checkpoint is not None
-    ) and not args.moe_use_upcycling:
-        one_logger and one_logger.log_metrics(
-            {'load_checkpoint_start_time': one_logger_utils.get_timestamp_in_ms()}
-        )
+    if (args.load is not None or args.pretrained_checkpoint is not None) and not args.moe_use_upcycling:
+        one_logger and one_logger.log_metrics({
+            'load_checkpoint_start_time': one_logger_utils.get_timestamp_in_ms()
+        })
         timers('load-checkpoint', log_level=0).start(barrier=True)
 
         args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(
-            model,
-            optimizer,
-            opt_param_scheduler,
-            checkpointing_context=checkpointing_context,
-            skip_load_to_model_and_opt=HAVE_FSDP2
-            and getattr(args, "use_torch_fsdp2", False)
-            and args.ckpt_format == "torch_dist",
-        )
+                model, optimizer, opt_param_scheduler, checkpointing_context=checkpointing_context,
+                skip_load_to_model_and_opt=HAVE_FSDP2 and getattr(args, "use_torch_fsdp2", False) and args.ckpt_format == "torch_dist")
         timers('load-checkpoint').stop(barrier=True)
         timers.log(['load-checkpoint'])
-        one_logger and one_logger.log_metrics(
-            {
-                'load_checkpoint_finish_time': one_logger_utils.get_timestamp_in_ms(),
-                'load_checkpoint_time': timers('load-checkpoint').active_time(),
-            }
-        )
+        one_logger and one_logger.log_metrics({
+            'load_checkpoint_finish_time': one_logger_utils.get_timestamp_in_ms(),
+            'load_checkpoint_time': timers('load-checkpoint').active_time()
+        })
     else:
         args.iteration = 0
         args.num_floating_point_operations_so_far = 0
 
     # get model without FP16 and/or DDP wrappers
-    if (
-        args.iteration == 0
-        and len(unwrapped_model) == 1
-        and hasattr(unwrapped_model[0], 'init_state_dict_from_bert')
-    ):
+    if args.iteration == 0 and len(unwrapped_model) == 1 \
+        and hasattr(unwrapped_model[0], 'init_state_dict_from_bert'):
         print_rank_0("Initializing ICT from pretrained BERT model")
         unwrapped_model[0].init_state_dict_from_bert()
         if args.fp16:
@@ -1569,14 +1463,9 @@ def setup_model_and_optimizer(
         args.save = os.path.join(args.ckpt_convert_save, args.ckpt_convert_format)
         update_use_dist_ckpt(args)
 
-        save_checkpoint(
-            args.iteration,
-            model,
-            optimizer,
-            opt_param_scheduler,
-            args.num_floating_point_operations_so_far,
-            preprocess_common_state_dict_fn=preprocess_common_state_dict,
-        )
+        save_checkpoint(args.iteration, model, optimizer, opt_param_scheduler,
+                        args.num_floating_point_operations_so_far,
+                        preprocess_common_state_dict_fn=preprocess_common_state_dict)
 
         print_rank_0("> converted checkpoint: %s -> %s." % (load_ckpt_format, args.ckpt_format))
         torch.distributed.barrier()
@@ -1594,7 +1483,8 @@ def dummy_train_step(data_iterator):
         batch = get_batch_on_this_cp_rank(batch)
 
 
-def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config):
+def train_step(forward_step_func, data_iterator,
+               model, optimizer, opt_param_scheduler, config):
     """Single training step."""
     args = get_args()
     timers = get_timers()
@@ -1638,8 +1528,7 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
             micro_batch_size=args.micro_batch_size,
             decoder_seq_length=args.decoder_seq_length,
             forward_only=False,
-            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,
-        )
+            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn)
     should_checkpoint, should_exit, exit_code = rerun_state_machine.should_checkpoint_and_exit()
     if should_exit:
         return {}, True, should_checkpoint, should_exit, exit_code, None, None
@@ -1687,7 +1576,9 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
 
     # Update learning rate.
     if update_successful:
-        increment = get_num_microbatches() * args.micro_batch_size * args.data_parallel_size
+        increment = get_num_microbatches() * \
+                    args.micro_batch_size * \
+                    args.data_parallel_size
         opt_param_scheduler.step(increment=increment)
         skipped_iter = 0
     else:
@@ -1706,47 +1597,28 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
         # Average loss across microbatches.
         loss_reduced = {}
         for key in losses_reduced[0].keys():
-            val = [x[key].view(-1) for x in losses_reduced]
-            if val[0].numel() == 2:
+            numerator = 0
+            denominator = 0
+            for x in losses_reduced:
+                val = x[key]
                 # there is one dict per microbatch. in new reporting, we average
                 # over the total number of tokens across the global batch.
-                val = torch.vstack(val).sum(dim=0)
-                torch.distributed.all_reduce(
-                    val,
-                    group=mpu.get_data_parallel_group(with_context_parallel=True)
-                )
-                loss_reduced[key] = val[0] / val[1]
-            elif val[0].numel() == 1:
-                # legacy behavior, we average over the number of microbatches
-                val = torch.cat(val).mean()
-                loss_reduced[key] = val
-            else:
-                raise ValueError(f"Invalid value shape: {val[0].shape} for key {key}")
-        return (
-            loss_reduced,
-            skipped_iter,
-            should_checkpoint,
-            should_exit,
-            exit_code,
-            grad_norm,
-            num_zeros_in_grad,
-        )
+                if isinstance(val, tuple) or isinstance(val, list):
+                    numerator += val[0]
+                    denominator += val[1]
+                else:
+                    # legacy behavior. we average over the number of microbatches,
+                    # and so the denominator is 1.
+                    numerator += val
+                    denominator += 1
+            loss_reduced[key] = numerator / denominator
+        return loss_reduced, skipped_iter, should_checkpoint, should_exit, exit_code, grad_norm, num_zeros_in_grad
     return {}, skipped_iter, should_checkpoint, should_exit, exit_code, grad_norm, num_zeros_in_grad
 
 
-def training_log(
-    loss_dict,
-    total_loss_dict,
-    learning_rate,
-    decoupled_learning_rate,
-    iteration,
-    loss_scale,
-    report_memory_flag,
-    skipped_iter,
-    grad_norm,
-    params_norm,
-    num_zeros_in_grad,
-):
+def training_log(loss_dict, total_loss_dict, learning_rate, decoupled_learning_rate, iteration,
+                 loss_scale, report_memory_flag, skipped_iter,
+                 grad_norm, params_norm, num_zeros_in_grad):
     """Log training information such as losses, timing, ...."""
     args = get_args()
     timers = get_timers()
@@ -1760,25 +1632,28 @@ def training_log(
     nan_iters_key = 'nan iterations'
     # Advanced iterations.
     if not skipped_iter:
-        total_loss_dict[advanced_iters_key] = total_loss_dict.get(advanced_iters_key, 0) + 1
+        total_loss_dict[advanced_iters_key] = total_loss_dict.get(
+            advanced_iters_key, 0) + 1
     else:
         if advanced_iters_key not in total_loss_dict:
             total_loss_dict[advanced_iters_key] = 0
     # Skipped iterations.
-    total_loss_dict[skipped_iters_key] = total_loss_dict.get(skipped_iters_key, 0) + skipped_iter
+    total_loss_dict[skipped_iters_key] = total_loss_dict.get(
+        skipped_iters_key, 0) + skipped_iter
     # Update losses and set nan iterations
     got_nan = False
     for key in loss_dict:
         if not skipped_iter:
-            total_loss_dict[key] = (
-                total_loss_dict.get(key, torch.tensor([0.0], dtype=torch.float, device='cuda'))
-                + loss_dict[key]
-            )
+            total_loss_dict[key] = total_loss_dict.get(
+                key, torch.tensor([0.0], dtype=torch.float, device='cuda')) + loss_dict[key]
         else:
             value = loss_dict[key].float().sum().item()
-            is_nan = value == float('inf') or value == -float('inf') or value != value
+            is_nan = value == float('inf') or \
+                     value == -float('inf') or \
+                     value != value
             got_nan = got_nan or is_nan
-    total_loss_dict[nan_iters_key] = total_loss_dict.get(nan_iters_key, 0) + int(got_nan)
+    total_loss_dict[nan_iters_key] = total_loss_dict.get(
+        nan_iters_key, 0) + int(got_nan)
 
     # Logging.
     timers_to_log = [
@@ -1805,30 +1680,35 @@ def training_log(
         'optimizer-count-zeros',
         'optimizer-inner-step',
         'optimizer-copy-main-to-model-params',
-        'optimizer',
-    ]
+        'optimizer']
 
     # Calculate batch size.
-    batch_size = args.micro_batch_size * args.data_parallel_size * get_num_microbatches()
+    batch_size = args.micro_batch_size * args.data_parallel_size * \
+        get_num_microbatches()
 
     # Track app tag & app tag ID
     one_logger_utils.track_app_tag(batch_size, args.world_size, args.seq_length)
 
-    total_iterations = total_loss_dict[advanced_iters_key] + total_loss_dict[skipped_iters_key]
+    total_iterations = total_loss_dict[advanced_iters_key] + \
+                       total_loss_dict[skipped_iters_key]
 
     # learning rate will be None on ranks without trainable params, so we must gather across mp ranks
     learning_rate = reduce_max_stat_across_model_parallel_group(learning_rate)
     # Tensorboard values.
     # Timer requires all the ranks to call.
-    if args.log_timers_to_tensorboard and (iteration % args.tensorboard_log_interval == 0):
-        timers.write(timers_to_log, writer, iteration, normalizer=total_iterations)
+    if args.log_timers_to_tensorboard and \
+       (iteration % args.tensorboard_log_interval == 0):
+        timers.write(timers_to_log, writer, iteration,
+                     normalizer=total_iterations)
     if is_last_rank() and (iteration % args.tensorboard_log_interval == 0):
         if wandb_writer:
-            wandb_writer.log({'samples vs steps': args.consumed_train_samples}, iteration)
+            wandb_writer.log({'samples vs steps': args.consumed_train_samples},
+                             iteration)
             wandb_writer.log({'consumed-tokens': args.consumed_train_samples * args.seq_length / 1000. / 1000 / 1000}, iteration)
         if writer:
             writer.add_scalar('learning-rate', learning_rate, iteration)
-            writer.add_scalar('learning-rate vs samples', learning_rate, args.consumed_train_samples)
+            writer.add_scalar('learning-rate vs samples', learning_rate,
+                                args.consumed_train_samples)
         if wandb_writer:
             wandb_writer.log({'learning-rate': learning_rate}, iteration)
         if args.decoupled_lr is not None:
@@ -1841,69 +1721,87 @@ def training_log(
                 wandb_writer.log({'skipped-train-samples': args.skipped_train_samples}, iteration)
         if writer:
             writer.add_scalar('batch-size', batch_size, iteration)
-            writer.add_scalar('batch-size vs samples', batch_size, args.consumed_train_samples)
+            writer.add_scalar('batch-size vs samples', batch_size,
+                          args.consumed_train_samples)
         if wandb_writer:
             wandb_writer.log({'batch-size': batch_size}, iteration)
         for key in loss_dict:
             if writer:
-                writer.add_scalar(key, loss_dict[key], iteration)
-                writer.add_scalar(key + ' vs samples', loss_dict[key], args.consumed_train_samples)
+                writer.add_scalar(key , loss_dict[key], iteration)
+                writer.add_scalar(key + ' vs samples', loss_dict[key],
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({key: loss_dict[key]}, iteration)
         if args.log_loss_scale_to_tensorboard:
             if writer:
                 writer.add_scalar('loss-scale', loss_scale, iteration)
-                writer.add_scalar('loss-scale vs samples', loss_scale, args.consumed_train_samples)
+                writer.add_scalar('loss-scale vs samples', loss_scale,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'loss-scale': loss_scale}, iteration)
         if args.log_world_size_to_tensorboard:
             if writer:
                 writer.add_scalar('world-size', args.world_size, iteration)
-                writer.add_scalar('world-size vs samples', args.world_size, args.consumed_train_samples)
+                writer.add_scalar('world-size vs samples', args.world_size,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'world-size': args.world_size}, iteration)
         if grad_norm is not None:
             if writer:
                 writer.add_scalar('grad-norm', grad_norm, iteration)
-                writer.add_scalar('grad-norm vs samples', grad_norm, args.consumed_train_samples)
+                writer.add_scalar('grad-norm vs samples', grad_norm,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'grad-norm': grad_norm}, iteration)
         if num_zeros_in_grad is not None:
             if writer:
                 writer.add_scalar('num-zeros', num_zeros_in_grad, iteration)
-                writer.add_scalar(
-                    'num-zeros vs samples', num_zeros_in_grad, args.consumed_train_samples
-                )
+                writer.add_scalar('num-zeros vs samples', num_zeros_in_grad,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'num-zeros': num_zeros_in_grad}, iteration)
         if params_norm is not None:
             if writer:
                 writer.add_scalar('params-norm', params_norm, iteration)
-                writer.add_scalar('params-norm vs samples', params_norm, args.consumed_train_samples)
+                writer.add_scalar('params-norm vs samples', params_norm,
+                                  args.consumed_train_samples)
             if wandb_writer:
                 wandb_writer.log({'params-norm': params_norm}, iteration)
         if args.log_memory_to_tensorboard:
             mem_stats = torch.cuda.memory_stats()
             if writer:
                 writer.add_scalar(
-                    "mem-reserved-bytes", mem_stats["reserved_bytes.all.current"], iteration
+                    "mem-reserved-bytes",
+                    mem_stats["reserved_bytes.all.current"],
+                    iteration,
+                )
+                writer.add_scalar(
+                    "mem-allocated-bytes",
+                    mem_stats["allocated_bytes.all.current"],
+                    iteration,
                 )
                 writer.add_scalar(
-                    "mem-allocated-bytes", mem_stats["allocated_bytes.all.current"], iteration
+                    "mem-max-allocated-bytes",
+                    mem_stats["allocated_bytes.all.peak"],
+                    iteration,
                 )
                 writer.add_scalar(
-                    "mem-max-allocated-bytes", mem_stats["allocated_bytes.all.peak"], iteration
+                    "mem-allocated-count",
+                    mem_stats["allocation.all.current"],
+                    iteration,
                 )
-                writer.add_scalar("mem-allocated-count", mem_stats["allocation.all.current"], iteration)
             if wandb_writer:
                 wandb_writer.log(
-                    {"mem-reserved-bytes": mem_stats["reserved_bytes.all.current"]}, iteration
+                    {"mem-reserved-bytes": mem_stats["reserved_bytes.all.current"]},
+                    iteration,
                 )
                 wandb_writer.log(
-                    {"mem-allocated-bytes": mem_stats["allocated_bytes.all.current"]}, iteration
+                    {"mem-allocated-bytes": mem_stats["allocated_bytes.all.current"]},
+                    iteration,
                 )
                 wandb_writer.log(
-                    {"mem-allocated-count": mem_stats["allocation.all.current"]}, iteration
+                    {"mem-allocated-count": mem_stats["allocation.all.current"]},
+                    iteration,
                 )
 
     if args.num_experts is not None:
@@ -1923,18 +1821,17 @@ def training_log(
             force_initialize=True,
             track_names=track_names,
             num_layers=args.num_layers,
-            moe_layer_freq=args.moe_layer_freq,
+            moe_layer_freq=args.moe_layer_freq
         )
     if args.mtp_num_layers is not None:
         mtp_loss_scale = 1 / get_num_microbatches()
         MTPLossLoggingHelper.track_mtp_metrics(
             mtp_loss_scale, iteration, writer, wandb_writer, total_loss_dict
-        )
+            )
     if iteration % args.log_interval == 0:
         if args.record_memory_history and is_last_rank():
             snapshot = torch.cuda.memory._snapshot()
             from pickle import dump
-
             with open(args.memory_snapshot_path, 'wb') as f:
                 dump(snapshot, f)
 
@@ -1942,24 +1839,27 @@ def training_log(
         elapsed_time_per_iteration = elapsed_time / total_iterations
 
         throughput = num_floating_point_operations(args, batch_size) / (
-            elapsed_time_per_iteration * 10**12 * args.world_size
-        )
+            elapsed_time_per_iteration * 10**12 * args.world_size)
 
         one_logger_utils.track_e2e_metrics(args.log_throughput, throughput)
 
         if args.log_timers_to_tensorboard:
             if writer:
-                writer.add_scalar('iteration-time', elapsed_time_per_iteration, iteration)
+                writer.add_scalar('iteration-time',
+                                  elapsed_time_per_iteration, iteration)
             if wandb_writer:
-                wandb_writer.log({'iteration-time': elapsed_time_per_iteration}, iteration)
+                wandb_writer.log({'iteration-time': elapsed_time_per_iteration},
+                                 iteration)
         log_string = f" [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]"
-        log_string += ' iteration {:8d}/{:8d} |'.format(iteration, args.train_iters)
-        log_string += ' consumed samples: {:12d} |'.format(args.consumed_train_samples)
+        log_string += ' iteration {:8d}/{:8d} |'.format(
+            iteration, args.train_iters)
+        log_string += ' consumed samples: {:12d} |'.format(
+            args.consumed_train_samples)
         if args.skipped_train_samples > 0:
-            log_string += ' skipped samples: {:12d} |'.format(args.skipped_train_samples)
+            log_string += ' skipped samples: {:12d} |'.format(
+                args.skipped_train_samples)
         log_string += ' elapsed time per iteration (ms): {:.1f} |'.format(
-            elapsed_time_per_iteration * 1000.0
-        )
+            elapsed_time_per_iteration * 1000.0)
         if args.log_throughput:
             log_string += f' throughput per GPU (TFLOP/s/GPU): {throughput:.1f} |'
             if args.log_timers_to_tensorboard:
@@ -1969,20 +1869,18 @@ def training_log(
                     wandb_writer.log({'throughput': throughput}, iteration)
         # Decoupled_learning_rate should be not None only on first and last pipeline stage.
         log_string += f' learning rate: {learning_rate:.6E} |'
-        if args.decoupled_lr is not None and (
-            mpu.is_pipeline_first_stage(ignore_virtual=True)
-            or mpu.is_pipeline_last_stage(ignore_virtual=True)
-        ):
+        if args.decoupled_lr is not None and (mpu.is_pipeline_first_stage(ignore_virtual=True) or
+                                              mpu.is_pipeline_last_stage(ignore_virtual=True)):
             assert decoupled_learning_rate is not None
             log_string += f' decoupled learning rate: {decoupled_learning_rate:.6E} |'
         else:
             assert decoupled_learning_rate is None
         log_string += f' global batch size: {batch_size:5d} |'
         for key in total_loss_dict:
-            if key not in [advanced_iters_key, skipped_iters_key, nan_iters_key]:
-                avg = total_loss_dict[key].item() / float(
-                    max(1, total_loss_dict[advanced_iters_key])
-                )
+            if key not in [advanced_iters_key, skipped_iters_key,
+                           nan_iters_key]:
+                avg = total_loss_dict[key].item() / \
+                      float(max(1, total_loss_dict[advanced_iters_key]))
                 if avg > 0.0:
                     log_string += ' {}: {:.6E} |'.format(key, avg)
                 total_loss_dict[key] = torch.tensor([0.0], dtype=torch.float, device='cuda')
@@ -1994,9 +1892,9 @@ def training_log(
         if params_norm is not None:
             log_string += f' params norm: {params_norm:.3f} |'
         log_string += ' number of skipped iterations: {:3d} |'.format(
-            total_loss_dict[skipped_iters_key]
-        )
-        log_string += ' number of nan iterations: {:3d} |'.format(total_loss_dict[nan_iters_key])
+            total_loss_dict[skipped_iters_key])
+        log_string += ' number of nan iterations: {:3d} |'.format(
+            total_loss_dict[nan_iters_key])
         total_loss_dict[advanced_iters_key] = 0
         total_loss_dict[skipped_iters_key] = 0
         total_loss_dict[nan_iters_key] = 0
@@ -2019,7 +1917,8 @@ def training_log(
     return report_memory_flag
 
 
-def compute_throughputs_and_append_to_progress_log(iteration, num_floating_point_operations_so_far):
+def compute_throughputs_and_append_to_progress_log(iteration,
+                                                   num_floating_point_operations_so_far):
     args = get_args()
     if args.save is None:
         return
@@ -2028,28 +1927,28 @@ def compute_throughputs_and_append_to_progress_log(iteration, num_floating_point
     # args.num_floating_point_operations_so_far keeps track of floating-point operations
     # completed at the start of job.
     global _TRAIN_START_TIME
-    job_throughput = (
-        num_floating_point_operations_so_far - args.num_floating_point_operations_so_far
-    ) / ((time.time() - _TRAIN_START_TIME) * 10**12 * args.world_size)
+    job_throughput = \
+        (num_floating_point_operations_so_far -
+         args.num_floating_point_operations_so_far) / (
+            (time.time() - _TRAIN_START_TIME) * 10**12 * args.world_size)
 
     # Compute cumulative throughput since jobs of this world size were launched.
     # `get_start_time_from_progress_log` returns start time and number of floating-point
     # operations of first job of this world size.
     start_time, start_num_floating_point_operations = get_start_time_from_progress_log()
     elapsed_time = (datetime.now() - start_time).total_seconds()
-    cumulative_throughput = (
-        num_floating_point_operations_so_far - start_num_floating_point_operations
-    ) / (elapsed_time * 10**12 * args.world_size)
+    cumulative_throughput = \
+        (num_floating_point_operations_so_far -
+         start_num_floating_point_operations) / (
+            elapsed_time * 10**12 * args.world_size)
 
     tokens_so_far = args.consumed_train_samples * args.seq_length
     saved_ckpt_prefix = 'Saving async checkpoint' if args.async_save else 'Saved checkpoint'
-    append_to_progress_log(
-        f"{saved_ckpt_prefix}\tIteration: {iteration}\t"
-        f"Job throughput: {job_throughput:.1f} TFLOP/s/GPU\t"
-        f"Cumulative throughput: {cumulative_throughput:.1f} TFLOP/s/GPU\t"
-        f"Floating-point operations: {num_floating_point_operations_so_far:.2e}\t"
-        f"Tokens (in billions): {tokens_so_far / 10**9:.2f}"
-    )
+    append_to_progress_log(f"{saved_ckpt_prefix}\tIteration: {iteration}\t"
+                           f"Job throughput: {job_throughput:.1f} TFLOP/s/GPU\t"
+                           f"Cumulative throughput: {cumulative_throughput:.1f} TFLOP/s/GPU\t"
+                           f"Floating-point operations: {num_floating_point_operations_so_far:.2e}\t"
+                           f"Tokens (in billions): {tokens_so_far / 10**9:.2f}")
 
 
 def enable_forward_pre_hook(model_chunks):
@@ -2064,16 +1963,9 @@ def disable_forward_pre_hook(model_chunks, param_sync=True):
         model_chunk.disable_forward_pre_hook(param_sync=param_sync)
 
 
-def save_checkpoint_and_time(
-    iteration,
-    model,
-    optimizer,
-    opt_param_scheduler,
-    num_floating_point_operations_so_far,
-    checkpointing_context,
-    non_persistent_ckpt=False,
-    train_data_iterator=None,
-):
+def save_checkpoint_and_time(iteration, model, optimizer, opt_param_scheduler,
+                             num_floating_point_operations_so_far, checkpointing_context,
+                             non_persistent_ckpt=False, train_data_iterator=None):
     args = get_args()
     timers = get_timers()
 
@@ -2087,17 +1979,10 @@ def save_checkpoint_and_time(
     one_logger_utils.track_e2e_metrics()
     if should_disable_forward_pre_hook(args):
         disable_forward_pre_hook(model)
-    save_checkpoint(
-        iteration,
-        model,
-        optimizer,
-        opt_param_scheduler,
-        num_floating_point_operations_so_far,
-        checkpointing_context,
-        non_persistent_ckpt=non_persistent_ckpt,
-        train_data_iterator=train_data_iterator,
-        preprocess_common_state_dict_fn=preprocess_common_state_dict,
-    )
+    save_checkpoint(iteration, model, optimizer, opt_param_scheduler,
+                    num_floating_point_operations_so_far, checkpointing_context,
+                    non_persistent_ckpt=non_persistent_ckpt, train_data_iterator=train_data_iterator,
+                    preprocess_common_state_dict_fn=preprocess_common_state_dict)
     if should_disable_forward_pre_hook(args):
         enable_forward_pre_hook(model)
     timers(timer_key).stop(barrier=True)
@@ -2109,22 +1994,15 @@ def save_checkpoint_and_time(
     one_logger_utils.on_save_checkpoint_end(save_checkpoint_duration, iteration, args.async_save)
 
     if args.log_progress and not non_persistent_ckpt:
-        compute_throughputs_and_append_to_progress_log(
-            iteration, num_floating_point_operations_so_far
-        )
+        compute_throughputs_and_append_to_progress_log(iteration,
+                                                       num_floating_point_operations_so_far)
 
     # Recover timing
     timers('interval-time', log_level=0).start(barrier=True)
 
 
-def post_training_step_callbacks(
-    model,
-    optimizer,
-    opt_param_scheduler,
-    iteration,
-    prof,
-    num_floating_point_operations_since_last_log_event,
-):
+def post_training_step_callbacks(model, optimizer, opt_param_scheduler, iteration, prof,
+                                 num_floating_point_operations_since_last_log_event):
     """Run all post-training-step functions (e.g., FT heartbeats, GC)."""
     args = get_args()
 
@@ -2138,31 +2016,27 @@ def post_training_step_callbacks(
         num_floating_point_operations_since_last_log_event = 0.0
 
     # Check weight hash across DP replicas.
-    if (
-        args.check_weight_hash_across_dp_replicas_interval is not None
-        and iteration % args.check_weight_hash_across_dp_replicas_interval == 0
-    ):
+    if args.check_weight_hash_across_dp_replicas_interval is not None and \
+            iteration % args.check_weight_hash_across_dp_replicas_interval == 0:
         if should_disable_forward_pre_hook(args):
             disable_forward_pre_hook(model)
-        assert check_param_hashes_across_dp_replicas(
-            model, cross_check=True
-        ), "Parameter hashes not matching across DP replicas"
+        assert check_param_hashes_across_dp_replicas(model, cross_check=True), \
+            "Parameter hashes not matching across DP replicas"
         torch.distributed.barrier()
         print_rank_0(f">>> Weight hashes match after {iteration} iterations...")
         if should_disable_forward_pre_hook(args):
             enable_forward_pre_hook(model)
 
     # Autoresume.
-    if args.adlr_autoresume and (iteration % args.adlr_autoresume_interval == 0):
-        check_adlr_autoresume_termination(iteration, model, optimizer, opt_param_scheduler)
+    if args.adlr_autoresume and \
+        (iteration % args.adlr_autoresume_interval == 0):
+        check_adlr_autoresume_termination(iteration, model, optimizer,
+                                          opt_param_scheduler)
 
     # Profiling.
-    torch.cuda.nvtx.range_pop() # for iteratrion
-    if (
-        args.profile
-        and iteration == args.profile_step_end
-        and torch.distributed.get_rank() in args.profile_ranks
-    ):
+    if args.profile and \
+        iteration == args.profile_step_end and \
+        torch.distributed.get_rank() in args.profile_ranks:
         if args.use_pytorch_profiler:
             assert prof is not None
             prof.stop()
@@ -2175,15 +2049,9 @@ def post_training_step_callbacks(
             gc.collect()
 
 
-def checkpoint_and_decide_exit(
-    model,
-    optimizer,
-    opt_param_scheduler,
-    iteration,
-    num_floating_point_operations_so_far,
-    checkpointing_context,
-    train_data_iterator,
-):
+def checkpoint_and_decide_exit(model, optimizer, opt_param_scheduler, iteration,
+                               num_floating_point_operations_so_far, checkpointing_context,
+                               train_data_iterator):
     """Save checkpoint and decide whether to exit based on arguments (e.g., if
     --exit-duration-in-mins is set). Actual exit happens in main training loop
     based on the return value of this function."""
@@ -2196,68 +2064,47 @@ def checkpoint_and_decide_exit(
         signal_handler = get_signal_handler()
         if any(signal_handler.signals_received()):
             if args.save:
-                save_checkpoint_and_time(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                )
+                save_checkpoint_and_time(iteration, model, optimizer,
+                                         opt_param_scheduler,
+                                         num_floating_point_operations_so_far,
+                                         checkpointing_context, train_data_iterator=train_data_iterator)
             print_datetime('exiting program after receiving SIGTERM.')
 
             return True
 
     # Regular save (persistent and non-persistent).
-    if args.save and args.save_interval and iteration % args.save_interval == 0:
-        save_checkpoint_and_time(
-            iteration,
-            model,
-            optimizer,
-            opt_param_scheduler,
-            num_floating_point_operations_so_far,
-            checkpointing_context,
-            train_data_iterator=train_data_iterator,
-        )
+    if args.save and args.save_interval and \
+        iteration % args.save_interval == 0:
+        save_checkpoint_and_time(iteration, model, optimizer,
+                                 opt_param_scheduler,
+                                 num_floating_point_operations_so_far,
+                                 checkpointing_context, train_data_iterator=train_data_iterator)
         saved_checkpoint = True
 
-    elif (
-        args.save
-        and args.non_persistent_save_interval
-        and iteration % args.non_persistent_save_interval == 0
-    ):
-        save_checkpoint_and_time(
-            iteration,
-            model,
-            optimizer,
-            opt_param_scheduler,
-            num_floating_point_operations_so_far,
-            checkpointing_context,
-            non_persistent_ckpt=True,
-            train_data_iterator=train_data_iterator,
-        )
+    elif args.save and args.non_persistent_save_interval and \
+        iteration % args.non_persistent_save_interval == 0:
+        save_checkpoint_and_time(iteration, model, optimizer,
+                                 opt_param_scheduler,
+                                 num_floating_point_operations_so_far,
+                                 checkpointing_context,
+                                 non_persistent_ckpt=True, train_data_iterator=train_data_iterator)
         saved_checkpoint = True
 
     # Exit based on duration.
     if args.exit_duration_in_mins:
         train_time = (time.time() - _TRAIN_START_TIME) / 60.0
         done_cuda = torch.tensor(
-            [train_time > args.exit_duration_in_mins], dtype=torch.int, device='cuda'
-        )
-        torch.distributed.all_reduce(done_cuda, op=torch.distributed.ReduceOp.MAX)
+            [train_time > args.exit_duration_in_mins],
+            dtype=torch.int, device='cuda')
+        torch.distributed.all_reduce(
+            done_cuda, op=torch.distributed.ReduceOp.MAX)
         done = done_cuda.item()
         if done:
             if args.save and not saved_checkpoint:
-                save_checkpoint_and_time(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                )
+                save_checkpoint_and_time(iteration, model, optimizer,
+                                         opt_param_scheduler,
+                                         num_floating_point_operations_so_far,
+                                         checkpointing_context, train_data_iterator=train_data_iterator)
             print_datetime(f'exiting program after {train_time} minutes')
 
             return True
@@ -2265,15 +2112,10 @@ def checkpoint_and_decide_exit(
     # Exit based on iterations.
     if args.exit_interval and iteration % args.exit_interval == 0:
         if args.save and not saved_checkpoint:
-            save_checkpoint_and_time(
-                iteration,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                num_floating_point_operations_so_far,
-                checkpointing_context,
-                train_data_iterator=train_data_iterator,
-            )
+            save_checkpoint_and_time(iteration, model, optimizer,
+                                     opt_param_scheduler,
+                                     num_floating_point_operations_so_far,
+                                     checkpointing_context, train_data_iterator=train_data_iterator)
         torch.distributed.barrier()
         print_datetime(f'exiting program at iteration {iteration}')
 
@@ -2282,19 +2124,10 @@ def checkpoint_and_decide_exit(
     return False
 
 
-def train(
-    forward_step_func,
-    model,
-    optimizer,
-    opt_param_scheduler,
-    train_data_iterator,
-    valid_data_iterator,
-    process_non_loss_data_func,
-    config,
-    checkpointing_context,
-    non_loss_data_func,
-    extra_valid_dataset_provider=None,
-):
+def train(forward_step_func, model, optimizer, opt_param_scheduler,
+          train_data_iterator, valid_data_iterator,
+          process_non_loss_data_func, config, checkpointing_context, non_loss_data_func,
+          extra_valid_dataset_provider=None):
     """Training function: run train_step desired number of times, run validation, checkpoint."""
     args = get_args()
     timers = get_timers()
@@ -2304,10 +2137,7 @@ def train(
         try:
             from workload_inspector.utils.webserver import run_server
             import threading
-
-            threading.Thread(
-                target=run_server, daemon=True, args=(torch.distributed.get_rank(),)
-            ).start()
+            threading.Thread(target=run_server, daemon=True, args=(torch.distributed.get_rank(), )).start()
         except ModuleNotFoundError:
             print_rank_0("workload inspector module not found.")
 
@@ -2330,17 +2160,11 @@ def train(
         rerun_state_machine.current_iteration = iteration
 
     # Track E2E metrics at the start of training.
-    one_logger_utils.on_train_start(
-        iteration=iteration,
-        consumed_train_samples=args.consumed_train_samples,
-        train_samples=args.train_samples,
-        seq_length=args.seq_length,
-        train_iters=args.train_iters,
-        save=args.save,
-        async_save=args.async_save,
-        log_throughput=args.log_throughput,
-        num_floating_point_operations_so_far=args.num_floating_point_operations_so_far,
-    )
+    one_logger_utils.on_train_start(iteration=iteration, consumed_train_samples=args.consumed_train_samples,
+                                    train_samples=args.train_samples, seq_length=args.seq_length,
+                                    train_iters=args.train_iters, save=args.save, async_save=args.async_save,
+                                    log_throughput=args.log_throughput,
+                                    num_floating_point_operations_so_far=args.num_floating_point_operations_so_far)
 
     num_floating_point_operations_so_far = args.num_floating_point_operations_so_far
 
@@ -2348,10 +2172,9 @@ def train(
     config.grad_scale_func = optimizer.scale_loss
     config.timers = timers
     if isinstance(model[0], (custom_FSDP, DDP)) and args.overlap_grad_reduce:
-        assert config.no_sync_func is None, (
-            'When overlap_grad_reduce is True, config.no_sync_func must be None; '
-            'a custom no_sync_func is not supported when overlapping grad-reduce'
-        )
+        assert config.no_sync_func is None, \
+            ('When overlap_grad_reduce is True, config.no_sync_func must be None; '
+             'a custom no_sync_func is not supported when overlapping grad-reduce')
         config.no_sync_func = [model_chunk.no_sync for model_chunk in model]
         if len(model) == 1:
             config.no_sync_func = config.no_sync_func[0]
@@ -2375,9 +2198,8 @@ def train(
     if args.manual_gc:
         # Disable the default garbage collector and perform the collection manually.
         # This is to align the timing of garbage collection across ranks.
-        assert (
-            args.manual_gc_interval >= 0
-        ), 'Manual garbage collection interval should be larger than or equal to 0'
+        assert args.manual_gc_interval >= 0, \
+            'Manual garbage collection interval should be larger than or equal to 0'
         gc.disable()
         gc.collect()
 
@@ -2387,13 +2209,10 @@ def train(
         world = torch.distributed.get_world_size()
         rank = torch.distributed.get_rank()
         mmcnt = args.straggler_minmax_count
-        stimer.configure(
-            world,
-            rank,
-            mmcnt=mmcnt,
-            enabled=not args.disable_straggler_on_startup,
-            port=args.straggler_ctrlr_port,
-        )
+        stimer.configure(world, rank,
+                mmcnt = mmcnt,
+                enabled = not args.disable_straggler_on_startup,
+                port = args.straggler_ctrlr_port)
     num_floating_point_operations_since_last_log_event = 0.0
 
     num_microbatches = get_num_microbatches()
@@ -2410,10 +2229,10 @@ def train(
     extra_eval_iterations = 0
 
     def get_e2e_base_metrics():
-        """Get base metrics values for one-logger to calculate E2E tracking metrics."""
-        num_floating_point_operations_since_current_train_start = (
+        """Get base metrics values for one-logger to calculate E2E tracking metrics.
+        """
+        num_floating_point_operations_since_current_train_start = \
             num_floating_point_operations_so_far - args.num_floating_point_operations_so_far
-        )
         return {
             'iteration': iteration,
             'train_duration': timers('interval-time').active_time(),
@@ -2427,29 +2246,22 @@ def train(
             'extra_eval_duration': extra_eval_duration,
             'extra_eval_iterations': extra_eval_iterations,
         }
-
     # Cache into one-logger for callback.
     if one_logger:
         with one_logger.get_context_manager():
             one_logger.store_set('get_e2e_base_metrics', get_e2e_base_metrics)
 
     prof = None
-    if (
-        args.profile
-        and torch.distributed.get_rank() in args.profile_ranks
-        and args.use_pytorch_profiler
-    ):
+    if args.profile and torch.distributed.get_rank() in args.profile_ranks and args.use_pytorch_profiler:
         prof = torch.profiler.profile(
-            schedule=torch.profiler.schedule(
-                wait=max(args.profile_step_start - 1, 0),
-                warmup=1 if args.profile_step_start > 0 else 0,
-                active=args.profile_step_end - args.profile_step_start,
-                repeat=1,
-            ),
-            on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),
-            record_shapes=True,
-            with_stack=True,
-        )
+        schedule=torch.profiler.schedule(
+            wait=max(args.profile_step_start-1, 0),
+            warmup=1 if args.profile_step_start > 0 else 0,
+            active=args.profile_step_end-args.profile_step_start,
+            repeat=1),
+        on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),
+        record_shapes=True,
+        with_stack=True)
         prof.start()
 
     start_iteration = iteration
@@ -2465,9 +2277,8 @@ def train(
         pre_hook_enabled = False
     # Also, check weight hash across DP replicas to be very pedantic.
     if args.check_weight_hash_across_dp_replicas_interval is not None:
-        assert check_param_hashes_across_dp_replicas(
-            model, cross_check=True
-        ), "Parameter hashes not matching across DP replicas"
+        assert check_param_hashes_across_dp_replicas(model, cross_check=True), \
+            "Parameter hashes not matching across DP replicas"
         torch.distributed.barrier()
         print_rank_0(f">>> Weight hashes match after {iteration} iterations...")
 
@@ -2479,7 +2290,7 @@ def train(
             elif iteration == args.profile_step_start:
                 torch.cuda.cudart().cudaProfilerStart()
                 torch.autograd.profiler.emit_nvtx(record_shapes=True).__enter__()
-        torch.cuda.nvtx.range_push(f"iteration num {iteration}") # NOTE(lizhiyu): add iteration num tag for profile
+
         ft_integration.on_checkpointing_start()
         maybe_finalize_async_save(blocking=False)
         ft_integration.on_checkpointing_end(is_async_finalization=True)
@@ -2491,20 +2302,14 @@ def train(
         update_num_microbatches(args.consumed_train_samples, consistency_check=False, verbose=True)
         if get_num_microbatches() != num_microbatches and iteration != 0 \
             and args.save_when_num_microbatches_change:
-            assert get_num_microbatches() > num_microbatches, (
-                f"Number of microbatches should be increasing due to batch size rampup; "
-                f"instead going from {num_microbatches} to {get_num_microbatches()}"
-            )
+            assert get_num_microbatches() > num_microbatches, \
+                (f"Number of microbatches should be increasing due to batch size rampup; "
+                 f"instead going from {num_microbatches} to {get_num_microbatches()}")
             if args.save is not None:
-                save_checkpoint_and_time(
-                    iteration,
-                    model,
-                    optimizer,
-                    opt_param_scheduler,
-                    num_floating_point_operations_so_far,
-                    checkpointing_context,
-                    train_data_iterator=train_data_iterator,
-                )
+                save_checkpoint_and_time(iteration, model, optimizer,
+                                         opt_param_scheduler,
+                                         num_floating_point_operations_so_far,
+                                         checkpointing_context, train_data_iterator=train_data_iterator)
         num_microbatches = get_num_microbatches()
         update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
 
@@ -2513,9 +2318,9 @@ def train(
             # Dummy train_step to fast forward train_data_iterator.
             dummy_train_step(train_data_iterator)
             iteration += 1
-            batch_size = (
-                mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()
-            )
+            batch_size = mpu.get_data_parallel_world_size() * \
+                         args.micro_batch_size * \
+                         get_num_microbatches()
             args.consumed_train_samples += batch_size
             args.skipped_train_samples += batch_size
             continue
@@ -2555,28 +2360,19 @@ def train(
         ########## FlagScale end ##########
 
         ft_integration.on_training_step_start()
-        (
-            loss_dict,
-            skipped_iter,
-            should_checkpoint,
-            should_exit,
-            exit_code,
-            grad_norm,
-            num_zeros_in_grad,
-        ) = train_step(
-            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config
-        )
+        loss_dict, skipped_iter, should_checkpoint, should_exit, exit_code, grad_norm, num_zeros_in_grad = \
+            train_step(forward_step_func,
+                       train_data_iterator,
+                       model,
+                       optimizer,
+                       opt_param_scheduler,
+                       config)
         ft_integration.on_training_step_end()
         if should_checkpoint:
-            save_checkpoint_and_time(
-                iteration,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                num_floating_point_operations_so_far,
-                checkpointing_context,
-                train_data_iterator=train_data_iterator,
-            )
+            save_checkpoint_and_time(iteration, model, optimizer,
+                                     opt_param_scheduler,
+                                     num_floating_point_operations_so_far,
+                                     checkpointing_context, train_data_iterator=train_data_iterator)
         if should_exit:
             break
 
@@ -2599,13 +2395,12 @@ def train(
                     pre_hook_enabled = True
 
         iteration += 1
-        batch_size = (
-            mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()
-        )
+        batch_size = mpu.get_data_parallel_world_size() * \
+                     args.micro_batch_size * \
+                     get_num_microbatches()
         args.consumed_train_samples += batch_size
-        num_skipped_samples_in_batch = (
-            get_current_global_batch_size() - get_current_running_global_batch_size()
-        )
+        num_skipped_samples_in_batch = (get_current_global_batch_size() -
+                                        get_current_running_global_batch_size())
         if args.decrease_batch_size_if_needed:
             assert num_skipped_samples_in_batch >= 0
         else:
@@ -2631,22 +2426,16 @@ def train(
                 decoupled_learning_rate = param_group['lr']
             else:
                 learning_rate = param_group['lr']
-        report_memory_flag = training_log(
-            loss_dict,
-            total_loss_dict,
-            learning_rate,
-            decoupled_learning_rate,
-            iteration,
-            loss_scale,
-            report_memory_flag,
-            skipped_iter,
-            grad_norm,
-            params_norm,
-            num_zeros_in_grad,
-        )
+        report_memory_flag = training_log(loss_dict, total_loss_dict,
+                                          learning_rate,
+                                          decoupled_learning_rate,
+                                          iteration, loss_scale,
+                                          report_memory_flag, skipped_iter,
+                                          grad_norm, params_norm, num_zeros_in_grad)
 
         # Evaluation.
-        if args.eval_interval and iteration % args.eval_interval == 0 and args.do_valid:
+        if args.eval_interval and iteration % args.eval_interval == 0 and \
+            args.do_valid:
             timers('interval-time').stop()
             if should_disable_forward_pre_hook(args):
                 disable_forward_pre_hook(model)
@@ -2656,18 +2445,11 @@ def train(
                 gc.collect()
             prefix = f'iteration {iteration}'
             timers('eval-time', log_level=0).start(barrier=True)
-            evaluate_and_print_results(
-                prefix,
-                forward_step_func,
-                valid_data_iterator,
-                model,
-                iteration,
-                process_non_loss_data_func,
-                config,
-                verbose=False,
-                write_to_tensorboard=True,
-                non_loss_data_func=non_loss_data_func,
-            )
+            evaluate_and_print_results(prefix, forward_step_func,
+                                       valid_data_iterator, model,
+                                       iteration, process_non_loss_data_func,
+                                       config, verbose=False, write_to_tensorboard=True,
+                                       non_loss_data_func=non_loss_data_func)
             eval_duration += timers('eval-time').elapsed()
             eval_iterations += args.eval_iters
             timers('eval-time').stop()
@@ -2691,19 +2473,18 @@ def train(
                 extra_valid_data_iterator = []
                 for i in range(len(model)):
                     mpu.set_virtual_pipeline_model_parallel_rank(i)
-                    extra_iterators = build_extra_valid_data_iterators(extra_valid_dataset_provider)
+                    extra_iterators = build_extra_valid_data_iterators(
+                        extra_valid_dataset_provider)
                     extra_valid_data_iterator.append(extra_iterators)
             else:
-                extra_valid_data_iterator = (
-                    build_extra_valid_data_iterators(extra_valid_dataset_provider)
-                )
+                extra_valid_data_iterator = build_extra_valid_data_iterators(
+                    extra_valid_dataset_provider)
             timers('interval-time').stop()
             # do_extra_valid flag is used to indicate that we are doing extra validation
             # and is set in the build_extra_valid_data_iterators function
             if getattr(args, "do_extra_valid", False):
-                if should_disable_forward_pre_hook(args):
+                if args.use_distributed_optimizer and args.overlap_param_gather:
                     disable_forward_pre_hook(model)
-                    pre_hook_enabled = False
                 if args.manual_gc and args.manual_gc_eval:
                     # Collect all objects.
                     gc.collect()
@@ -2711,19 +2492,11 @@ def train(
                 for extra_valid_index, extra_valid_data_itr in enumerate(extra_valid_data_iterator):
                     timers('extra-eval-time', log_level=0).start(barrier=True)
                     extra_eval_iters = args.extra_eval_iters_list[extra_valid_index]
-                    extra_evaluate_and_print_results(
-                        extra_valid_index,
-                        prefix,
-                        forward_step_func,
-                        extra_valid_data_itr,
-                        model,
-                        iteration,
-                        process_non_loss_data_func,
-                        config,
-                        verbose=False,
-                        write_to_tensorboard=True,
-                        non_loss_data_func=non_loss_data_func
-                    )
+                    extra_evaluate_and_print_results(extra_valid_index, prefix, forward_step_func,
+                                                     extra_valid_data_itr, model,
+                                                     iteration, process_non_loss_data_func,
+                                                     config, verbose=False, write_to_tensorboard=True,
+                                                     non_loss_data_func=non_loss_data_func)
                     extra_eval_duration += timers('extra-eval-time').elapsed()
                     extra_eval_iterations += extra_eval_iters
                     timers('extra-eval-time').stop()
@@ -2732,33 +2505,25 @@ def train(
                 if args.manual_gc and args.manual_gc_eval:
                     # Collect only the objects created and used in evaluation.
                     gc.collect(generation=0)
-                if should_disable_forward_pre_hook(args):
+                if args.use_distributed_optimizer and args.overlap_param_gather:
                     enable_forward_pre_hook(model)
                     pre_hook_enabled = True
                 timers('interval-time', log_level=0).start(barrier=True)
+
+                if args.enable_ft_package and ft_integration.get_rank_monitor_client() is not None:
+                    ft_integration.get_rank_monitor_client(
+                        ft_integration.StateMachineActions.EVAL_HEARTBEAT).send_heartbeat()
         # =======================================================================================
 
         # Miscellaneous post-training-step functions (e.g., FT heartbeats, GC).
         # Some of these only happen at specific iterations.
-        post_training_step_callbacks(
-            model,
-            optimizer,
-            opt_param_scheduler,
-            iteration,
-            prof,
-            num_floating_point_operations_since_last_log_event,
-        )
+        post_training_step_callbacks(model, optimizer, opt_param_scheduler, iteration, prof,
+                                     num_floating_point_operations_since_last_log_event)
 
         # Checkpoint and decide whether to exit.
-        should_exit = checkpoint_and_decide_exit(
-            model,
-            optimizer,
-            opt_param_scheduler,
-            iteration,
-            num_floating_point_operations_so_far,
-            checkpointing_context,
-            train_data_iterator,
-        )
+        should_exit = checkpoint_and_decide_exit(model, optimizer, opt_param_scheduler, iteration,
+                                                 num_floating_point_operations_so_far,
+                                                 checkpointing_context, train_data_iterator)
         if should_exit:
             break
 
@@ -2793,16 +2558,14 @@ def train(
     return iteration, num_floating_point_operations_so_far
 
 
-def evaluate(
-    forward_step_func,
-    data_iterator,
-    model,
-    process_non_loss_data_func,
-    config,
-    verbose=False,
-    non_loss_data_func=None,
-    extra_valid_index=None,
-):
+def evaluate(forward_step_func,
+             data_iterator,
+             model,
+             process_non_loss_data_func,
+             config,
+             verbose=False,
+             non_loss_data_func=None,
+             extra_valid_index=None):
     """Evaluation."""
     args = get_args()
     timers = get_timers()
@@ -2811,7 +2574,6 @@ def evaluate(
 
     if args.vision_pretraining and args.vision_pretraining_type == "dino":
         from megatron.legacy.model.vision.knn_monitor import compute_feature_bank
-
         compute_feature_bank(model)
 
     # Turn on evaluation mode which disables dropout.
@@ -2827,7 +2589,8 @@ def evaluate(
 
     # make validation batch size independent from training batch size
     eval_batch_size = args.global_batch_size
-    eval_num_microbatches = eval_batch_size // (args.micro_batch_size * args.data_parallel_size)
+    eval_num_microbatches = eval_batch_size // \
+        (args.micro_batch_size * args.data_parallel_size)
 
     if extra_valid_index is not None:
         assert getattr(args, "extra_eval_iters_list") is not None, \
@@ -2857,8 +2620,7 @@ def evaluate(
                 seq_length=args.seq_length,
                 micro_batch_size=args.micro_batch_size,
                 decoder_seq_length=args.decoder_seq_length,
-                forward_only=True,
-            )
+                forward_only=True)
             ft_integration.on_eval_step_end()
             config.timers = get_timers()
 
@@ -2868,34 +2630,27 @@ def evaluate(
 
             if mpu.is_pipeline_last_stage(ignore_virtual=True):
                 # Reduce across processes.
-                for key in loss_dicts[0].keys():
-                    if key not in total_loss_dict:
-                        total_loss_dict[key] = torch.tensor(
-                            [0.0, 0.0], dtype=torch.float
-                        ).cuda()
-                    val = [x[key].view(-1) for x in loss_dicts]
-                    if val[0].numel() == 2:
-                        val = torch.vstack(val).sum(dim=0)
-                        torch.distributed.all_reduce(
-                            val,
-                            group=mpu.get_data_parallel_group(with_context_parallel=True)
-                        )
-                        total_loss_dict[key] += val
-                    elif val[0].numel() == 1:
-                        val = torch.cat(val).sum()
-                        total_loss_dict[key][0] += val
-                        total_loss_dict[key][1] += len(loss_dicts)
-                    else:
-                        raise ValueError(f"Invalid value shape: {val[0].shape} for key {key}")
+                for loss_dict in loss_dicts:
+                    for key in loss_dict:
+                        if key not in total_loss_dict:
+                            total_loss_dict[key] = torch.tensor([0.0, 0.0], dtype=torch.float).cuda()
+                        val = loss_dict[key]
+                        if isinstance(val, tuple) or isinstance(val, list):
+                            total_loss_dict[key][0] += val[0]
+                            total_loss_dict[key][1] += val[1]
+                        else:
+                            total_loss_dict[key][0] += val
+                            total_loss_dict[key][1] += 1
 
             args.consumed_valid_samples += eval_batch_size
 
             if args.exit_duration_in_mins:
                 train_time = (time.time() - _TRAIN_START_TIME) / 60.0
                 done_cuda = torch.tensor(
-                    [train_time > args.exit_duration_in_mins], dtype=torch.int, device='cuda'
-                )
-                torch.distributed.all_reduce(done_cuda, op=torch.distributed.ReduceOp.MAX)
+                    [train_time > args.exit_duration_in_mins],
+                    dtype=torch.int, device='cuda')
+                torch.distributed.all_reduce(
+                    done_cuda, op=torch.distributed.ReduceOp.MAX)
                 done = done_cuda.item()
                 if done:
                     rerun_state_machine.set_mode(rerun_mode)
@@ -2915,8 +2670,7 @@ def evaluate(
                 micro_batch_size=args.micro_batch_size,
                 decoder_seq_length=args.decoder_seq_length,
                 forward_only=True,
-                collect_non_loss_data=True,
-            )
+                collect_non_loss_data=True)
 
     # Move model back to the train mode.
     for model_module in model:
@@ -2935,19 +2689,10 @@ def evaluate(
 
     return total_loss_dict, collected_non_loss_data, False
 
-
-def evaluate_and_print_results(
-    prefix,
-    forward_step_func,
-    data_iterator,
-    model,
-    iteration,
-    process_non_loss_data_func,
-    config,
-    verbose=False,
-    write_to_tensorboard=True,
-    non_loss_data_func=None,
-):
+def evaluate_and_print_results(prefix, forward_step_func,
+                               data_iterator, model,
+                               iteration, process_non_loss_data_func, config,
+                               verbose=False, write_to_tensorboard=True, non_loss_data_func=None):
     """Helper function to evaluate and dump results on screen."""
     args = get_args()
     if write_to_tensorboard:
@@ -2958,14 +2703,8 @@ def evaluate_and_print_results(
     wandb_writer = get_wandb_writer()
 
     total_loss_dict, collected_non_loss_data, timelimit = evaluate(
-        forward_step_func,
-        data_iterator,
-        model,
-        process_non_loss_data_func,
-        config,
-        verbose,
-        non_loss_data_func,
-    )
+        forward_step_func, data_iterator, model,
+        process_non_loss_data_func, config, verbose, non_loss_data_func)
     # Timelimit hit during evaluation
     if timelimit:
         return
@@ -2975,21 +2714,21 @@ def evaluate_and_print_results(
         ppl = math.exp(min(20, total_loss_dict[key].item()))
         string += '{} PPL: {:.6E} | '.format(key, ppl)
         if writer:
-            writer.add_scalar('{} validation'.format(key), total_loss_dict[key].item(), iteration)
-            writer.add_scalar(
-                '{} validation vs samples'.format(key),
-                total_loss_dict[key].item(),
-                args.consumed_train_samples,
-            )
+            writer.add_scalar('{} validation'.format(key),
+                              total_loss_dict[key].item(),
+                              iteration)
+            writer.add_scalar('{} validation vs samples'.format(key),
+                              total_loss_dict[key].item(),
+                              args.consumed_train_samples)
             if args.log_validation_ppl_to_tensorboard:
-                writer.add_scalar('{} validation ppl'.format(key), ppl, iteration)
-                writer.add_scalar(
-                    '{} validation ppl vs samples'.format(key), ppl, args.consumed_train_samples
-                )
+                writer.add_scalar('{} validation ppl'.format(key), ppl,
+                                  iteration)
+                writer.add_scalar('{} validation ppl vs samples'.format(key),
+                                  ppl, args.consumed_train_samples)
             if wandb_writer and is_last_rank():
-                wandb_writer.log(
-                    {'{} validation'.format(key): total_loss_dict[key].item()}, iteration
-                )
+                wandb_writer.log({
+                    '{} validation'.format(key): total_loss_dict[key].item()},
+                    iteration)
                 wandb_writer.log({
                     '{} validation vs samples'.format(key): args.consumed_train_samples},
                     iteration)
@@ -3019,10 +2758,15 @@ def get_train_valid_test_num_samples():
         train_samples = args.train_samples
     else:
         train_samples = args.train_iters * args.global_batch_size
-    eval_iters = (args.train_iters // args.eval_interval + 1) * args.eval_iters
+    eval_iters = (args.train_iters // args.eval_interval + 1) * \
+                 args.eval_iters
     test_iters = args.eval_iters
 
-    return (train_samples, eval_iters * args.global_batch_size, test_iters * args.global_batch_size)
+    return (
+        train_samples,
+        eval_iters * args.global_batch_size,
+        test_iters * args.global_batch_size,
+    )
 
 
 def build_train_valid_test_datasets(build_train_valid_test_datasets_provider):
@@ -3035,7 +2779,8 @@ def build_train_valid_test_datasets(build_train_valid_test_datasets_provider):
     return build_train_valid_test_datasets_provider(train_valid_test_num_samples)
 
 
-def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider):
+def build_train_valid_test_data_loaders(
+        build_train_valid_test_datasets_provider):
     """Build pretraining data loaders."""
 
     args = get_args()
@@ -3046,15 +2791,13 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
 
     # Backward compatibility, assume fixed batch size.
     if args.iteration > 0 and args.consumed_train_samples == 0:
-        assert (
-            args.train_samples is None
-        ), 'Only backward compatiblity support for iteration-based training'
+        assert args.train_samples is None, \
+            'Only backward compatiblity support for iteration-based training'
         args.consumed_train_samples = args.iteration * args.global_batch_size
     if args.iteration > 0 and args.consumed_valid_samples == 0:
         if args.train_samples is None:
-            args.consumed_valid_samples = (
-                (args.iteration // args.eval_interval) * args.eval_iters * args.global_batch_size
-            )
+            args.consumed_valid_samples = (args.iteration // args.eval_interval) * \
+                args.eval_iters * args.global_batch_size
 
     # Rely on distributed-aware core datasets, temporary
     is_distributed = getattr(build_train_valid_test_datasets_provider, "is_distributed", False)
@@ -3064,14 +2807,15 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
 
         # Build datasets.
         train_ds, valid_ds, test_ds = build_train_valid_test_datasets(
-            build_train_valid_test_datasets_provider
-        )
+            build_train_valid_test_datasets_provider)
         # Build dataloders.
-        train_dataloader = build_pretraining_data_loader(train_ds, args.consumed_train_samples)
+        train_dataloader = build_pretraining_data_loader(
+            train_ds, args.consumed_train_samples)
         if args.skip_train:
             valid_dataloader = build_pretraining_data_loader(valid_ds, 0)
         else:
-            valid_dataloader = build_pretraining_data_loader(valid_ds, args.consumed_valid_samples)
+            valid_dataloader = build_pretraining_data_loader(
+                valid_ds, args.consumed_valid_samples)
         test_dataloader = build_pretraining_data_loader(test_ds, 0)
 
         # Flags to know if we need to do training/validation/testing.
@@ -3079,8 +2823,8 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
         do_valid = valid_dataloader is not None and args.eval_iters > 0
         do_test = test_dataloader is not None and args.eval_iters > 0
         flags = torch.tensor(
-            [int(do_train), int(do_valid), int(do_test)], dtype=torch.long, device=get_device_type_for_comm()
-        )
+            [int(do_train), int(do_valid), int(do_test)],
+            dtype=torch.long, device=get_device_type_for_comm())
     else:
         flags = torch.tensor([0, 0, 0], dtype=torch.long, device=get_device_type_for_comm())
 
@@ -3093,15 +2837,16 @@ def build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider
     return train_dataloader, valid_dataloader, test_dataloader
 
 
-def build_train_valid_test_data_iterators(build_train_valid_test_datasets_provider):
+def build_train_valid_test_data_iterators(
+        build_train_valid_test_datasets_provider):
     """Build pretraining data iterators."""
 
     args = get_args()
 
     # Build loaders.
-    train_dataloader, valid_dataloader, test_dataloader = build_train_valid_test_data_loaders(
-        build_train_valid_test_datasets_provider
-    )
+    train_dataloader, valid_dataloader, test_dataloader = \
+        build_train_valid_test_data_loaders(
+            build_train_valid_test_datasets_provider)
 
     # Build iterators.
     dl_type = args.dataloader_type
diff --git a/flagscale/train/train_gpt.py b/flagscale/train/train_gpt.py
index c7b69cab..59848184 100644
--- a/flagscale/train/train_gpt.py
+++ b/flagscale/train/train_gpt.py
@@ -7,7 +7,7 @@ from typing import List, Optional, Tuple, Union
 
 import torch
 
-from megatron.core import parallel_state
+from megatron.core import mpu
 from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder
 from megatron.core.datasets.gpt_dataset import GPTDataset, GPTDatasetConfig, MockGPTDataset
 from megatron.core.enums import ModelType
@@ -34,7 +34,6 @@ from megatron.training.utils import (
 from megatron.training.yaml_arguments import core_transformer_config_from_yaml
 
 import megatron.legacy.model  # isort: skip
-
 # NOTE: Loading `megatron.legacy.model` earlier fails due to circular import
 
 try:
@@ -51,13 +50,11 @@ from flagscale.train.extra_valid import extra_valid_datasets_provider
 from flagscale.train.train import pretrain
 from flagscale.train.global_vars import get_parallel_context
 
+from dcu_megatron import megatron_adaptor
 
 stimer = StragglerDetector()
 
-
-def model_provider(
-    pre_process=True, post_process=True, vp_stage: Optional[int] = None
-) -> Union[GPTModel, megatron.legacy.model.GPTModel]:
+def model_provider(pre_process=True, post_process=True) -> Union[GPTModel, megatron.legacy.model.GPTModel]:
     """Builds the model.
 
     If you set the use_legacy_models to True, it will return the legacy GPT model and if not the mcore GPT model.
@@ -78,24 +75,19 @@ def model_provider(
     use_te = args.transformer_impl == "transformer_engine"
 
     if args.record_memory_history:
-        torch.cuda.memory._record_memory_history(
-            True,
+        torch.cuda.memory._record_memory_history(True,
             # keep 100,000 alloc/free events from before the snapshot
             trace_alloc_max_entries=100000,
+
             # record stack information for the trace events
-            trace_alloc_record_context=True,
-        )
+            trace_alloc_record_context=True)
 
         def oom_observer(device, alloc, device_alloc, device_free):
             # snapshot right after an OOM happened
             print('saving allocated state during OOM')
             snapshot = torch.cuda.memory._snapshot()
             from pickle import dump
-
-            dump(
-                snapshot,
-                open(f"oom_rank-{torch.distributed.get_rank()}_{args.memory_snapshot_path}", 'wb'),
-            )
+            dump(snapshot, open(f"oom_rank-{torch.distributed.get_rank()}_{args.memory_snapshot_path}", 'wb'))
 
         torch._C._cuda_attach_out_of_memory_observer(oom_observer)
 
@@ -120,41 +112,29 @@ def model_provider(
             pre_process=pre_process,
             post_process=post_process,
         )
-    else:  # using core models
+    else: # using core models
         if args.spec is not None:
             transformer_layer_spec = import_module(args.spec)
         else:
             if args.num_experts:
                 # Define the decoder block spec
-                transformer_layer_spec = get_gpt_decoder_block_spec(
-                    config, use_transformer_engine=use_te, normalization=args.normalization
-                )
+                transformer_layer_spec = get_gpt_decoder_block_spec(config, use_transformer_engine=use_te, normalization=args.normalization)
             elif args.heterogeneous_layers_config_path is not None:
                 transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)
             else:
                 # Define the decoder layer spec
                 if use_te:
                     transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(
-                        args.num_experts,
-                        args.moe_grouped_gemm,
-                        args.qk_layernorm,
-                        args.multi_latent_attention,
-                        args.moe_use_legacy_grouped_gemm,
-                    )
+                        args.num_experts, args.moe_grouped_gemm,
+                        args.qk_layernorm, args.multi_latent_attention, args.moe_use_legacy_grouped_gemm)
                 else:
                     transformer_layer_spec = get_gpt_layer_local_spec(
-                        args.num_experts,
-                        args.moe_grouped_gemm,
-                        args.qk_layernorm,
-                        args.multi_latent_attention,
-                        args.moe_use_legacy_grouped_gemm,
-                        normalization=args.normalization,
-                    )
+                        args.num_experts, args.moe_grouped_gemm,
+                        args.qk_layernorm, args.multi_latent_attention, args.moe_use_legacy_grouped_gemm,
+                        normalization=args.normalization)
         mtp_block_spec = None
         if args.mtp_num_layers is not None:
-            mtp_block_spec = get_gpt_mtp_block_spec(
-                config, transformer_layer_spec, use_transformer_engine=use_te
-            )
+            mtp_block_spec = get_gpt_mtp_block_spec(config, transformer_layer_spec, use_transformer_engine=use_te)
 
         model = GPTModel(
             config=config,
@@ -171,7 +151,6 @@ def model_provider(
             rotary_base=args.rotary_base,
             rope_scaling=args.use_rope_scaling,
             mtp_block_spec=mtp_block_spec,
-            vp_stage=vp_stage,
         )
 
     return model
@@ -181,9 +160,7 @@ def get_batch(data_iterator):
     """Generate a batch."""
 
     # TODO: this is pretty hacky, find a better way
-    if (not parallel_state.is_pipeline_first_stage(ignore_virtual=True)) and (
-        not parallel_state.is_pipeline_last_stage(ignore_virtual=True)
-    ):
+    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):
         return None, None, None, None, None
 
     # get batches based on the TP rank you are on
@@ -199,9 +176,7 @@ def get_batch(data_iterator):
 SPIKY_LOSS_FACTOR = 10
 
 
-def loss_func(
-    loss_mask: torch.Tensor, output_tensor: torch.Tensor, model: Optional[GPTModel] = None
-):
+def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor, model: Optional[GPTModel] = None):
     """Loss function.
 
     Args:
@@ -220,45 +195,57 @@ def loss_func(
     if has_nvidia_modelopt and modelopt_args_enabled(args):  # [ModelOpt]
         return loss_func_modelopt(loss_mask, output_tensor, model=model)
 
-    losses = output_tensor.view(-1).float()
+    losses = output_tensor.float()
     loss_mask = loss_mask.view(-1).float()
-    loss = torch.sum(losses * loss_mask)
+    total_tokens = loss_mask.sum()
+    loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), total_tokens.view(1)])
+
+    if args.context_parallel_size > 1:
+        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())
 
     # Check individual rank losses are not NaN prior to DP all-reduce.
     rerun_state_machine = get_rerun_state_machine()
     if args.check_for_nan_in_loss_and_grad:
         rerun_state_machine.validate_result(
-            result=loss,
+            result=loss[0],
             rejection_func=torch.isnan,
             message="found NaN in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
+            tolerance=0.0,        # forward pass calculations are determinisic
             fatal=True,
         )
         rerun_state_machine.validate_result(
-            result=loss,
+            result=loss[0],
             rejection_func=torch.isinf,
             message="found Inf in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
+            tolerance=0.0,        # forward pass calculations are determinisic
             fatal=True,
         )
     # Check for spiky loss
     if args.check_for_spiky_loss:
         rerun_state_machine.validate_result(
-            result=loss,
+            result=loss[0],
             rejection_func=partial(
                 rerun_state_machine.is_unexpectedly_large,
                 threshold=SPIKY_LOSS_FACTOR,
                 context="loss",
             ),
             message="Spiky loss",
-            tolerance=0.0,  # forward pass calculations are determinisic
+            tolerance=0.0,        # forward pass calculations are determinisic
             fatal=False,
         )
-
-    num_tokens = loss_mask.sum().clone().detach().to(torch.int)
-    reporting_loss = torch.cat([loss.clone().detach().view(1), num_tokens.view(1)])
-
-    return (loss, num_tokens, {'lm loss': reporting_loss})
+    # Reduce loss for logging.
+    reporting_loss = loss.clone().detach()
+    torch.distributed.all_reduce(reporting_loss, group=mpu.get_data_parallel_group())
+
+    # loss[0] is a view of loss, so it has ._base not None, which triggers assert error
+    # in core/pipeline_parallel/schedule.py::deallocate_output_tensor, calling .clone()
+    # on loss[0] fixes this
+    local_num_tokens = loss[1].clone().detach().to(torch.int)
+    return (
+        loss[0].clone(),
+        local_num_tokens,
+        {'lm loss': (reporting_loss[0], reporting_loss[1])},
+    )
 
 
 def forward_step(data_iterator, model: GPTModel):
@@ -275,16 +262,17 @@ def forward_step(data_iterator, model: GPTModel):
     timers('batch-generator', log_level=2).start()
     global stimer
     with stimer(bdata=True):
-        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)
+        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(
+            data_iterator)
     timers('batch-generator').stop()
 
     with stimer:
         if args.use_legacy_models:
-            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)
+            output_tensor = model(tokens, position_ids, attention_mask,
+                                labels=labels)
         else:
-            output_tensor = model(
-                tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask
-            )
+            output_tensor = model(tokens, position_ids, attention_mask,
+                                labels=labels, loss_mask=loss_mask)
 
     # [ModelOpt]: model is needed to access ModelOpt distillation losses
     return output_tensor, partial(loss_func, loss_mask, model=model)
@@ -292,9 +280,8 @@ def forward_step(data_iterator, model: GPTModel):
 
 def is_dataset_built_on_rank():
     return (
-        parallel_state.is_pipeline_first_stage(ignore_virtual=True)
-        or parallel_state.is_pipeline_last_stage(ignore_virtual=True)
-    ) and parallel_state.get_tensor_model_parallel_rank() == 0
+        mpu.is_pipeline_first_stage() or mpu.is_pipeline_last_stage()
+    ) and mpu.get_tensor_model_parallel_rank() == 0
 
 
 def core_gpt_dataset_config_from_args(args):
@@ -379,7 +366,10 @@ def train_valid_test_datasets_provider(train_val_test_num_samples):
     print_rank_0("> building train, validation, and test datasets for GPT ...")
 
     train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(
-        dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config
+        dataset_type,
+        train_val_test_num_samples,
+        is_dataset_built_on_rank,
+        config
     ).build()
 
     print_rank_0("> finished creating GPT datasets ...")
diff --git a/flagscale/train/train_qwen2_5_vl.py b/flagscale/train/train_qwen2_5_vl.py
index 9befeec7..01c72a64 100644
--- a/flagscale/train/train_qwen2_5_vl.py
+++ b/flagscale/train/train_qwen2_5_vl.py
@@ -1,806 +1,692 @@
-# Mainly Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/examples/qwen2_5_vl/pretrain_qwen.py.Below is the original copyright:
-# Copyright (c) 2024 Alibaba PAI and Nvidia Megatron-LM Team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-
-import os
-import sys
-import logging
-from functools import partial
-from copy import deepcopy
-from typing import List, Optional, Tuple, Union
-
-import torch
-import torch._dynamo
-
-from argparse import Namespace
-
-# # For pytorch 2.6
-# torch.serialization.add_safe_globals([Namespace])
-
-from megatron.core import parallel_state
-from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder
-from megatron.core.datasets.gpt_dataset import GPTDataset, GPTDatasetConfig, MockGPTDataset
-from megatron.training.checkpointing import get_checkpoint_name # for dataloder
-from megatron.core.enums import ModelType
-from megatron.core.models.gpt import GPTModel
-from megatron.core.models.gpt.gpt_layer_specs import (
-    get_gpt_decoder_block_spec,
-    get_gpt_layer_local_spec,
-    get_gpt_layer_with_transformer_engine_spec,
-    get_gpt_mtp_block_spec,
-)
-from megatron.core.models.gpt.heterogeneous.heterogeneous_layer_specs import (
-    get_gpt_heterogeneous_layer_spec,
-)
-from megatron.core.rerun_state_machine import get_rerun_state_machine
-from megatron.core.transformer.spec_utils import import_module
-from megatron.core.utils import StragglerDetector
-from megatron.training import get_args, get_timers, get_tokenizer, print_rank_0
-from megatron.training.arguments import core_transformer_config_from_args
-from megatron.training.utils import (
-    get_batch_on_this_cp_rank,
-    get_batch_on_this_tp_rank,
-    get_blend_and_blend_per_split,
-)
-from megatron.training.yaml_arguments import core_transformer_config_from_yaml
-
-import megatron.legacy.model  # isort: skip
-
-# NOTE: Loading `megatron.legacy.model` earlier fails due to circular import
-
-try:
-    from megatron.post_training.arguments import add_modelopt_args, modelopt_args_enabled
-    from megatron.post_training.loss_func import loss_func as loss_func_modelopt
-    from megatron.post_training.model_provider import model_provider as model_provider_modelopt
-
-    has_nvidia_modelopt = True
-except ImportError:
-    has_nvidia_modelopt = False
-
-from flagscale.train.datasets.sft_dataset import SFTDatasetConfig, SFTDataset
-from flagscale.train.extra_valid import extra_valid_datasets_provider
-from flagscale.train.train import pretrain
-stimer = StragglerDetector()
-
-#### especially for qwen2.5-vl ####
-from megatron.core.num_microbatches_calculator import get_num_microbatches
-torch._dynamo.config.suppress_errors = True
-from megatron.core.parallel_state import get_tensor_model_parallel_rank, get_pipeline_model_parallel_world_size, get_pipeline_model_parallel_rank
-from megatron.energon import (
-    LimitDataset,
-    RepeatDataset,
-    WorkerConfig,
-    get_loader,
-    get_savable_loader,
-    get_train_dataset,
-    get_val_datasets,
-)
-
-from megatron.training.tokenizer.tokenizer import build_tokenizer
-from megatron.training.global_vars import get_tokenizer
-
-from flagscale.train.models.qwen2_5_vl.layer_specs import (
-    get_gpt_layer_with_transformer_engine_spec,
-    get_qwen2vl_vision_model_spec,
-    get_mlp_module_spec
-
-)
-from flagscale.train.models.qwen2_5_vl.qwen2_5_vl_model import Qwen2_5VLModel
-from flagscale.train.models.qwen2_5_vl.tensor_parallel import broadcast_data
-from flagscale.train.models.qwen2_5_vl.transformer_config import (
-    get_vision_model_config,
-    get_vision_projection_config
-)
-from tools.datasets.qwenvl.data.dataset_helpers import TaskEncoder, print_error_handler
-#### especially for qwen2.5-vl ####
-IGNORE_IDX=-100
-FIRST_MAX_PADDING_FLAG = True
-LAST_LARGE_IMG=False
-def model_provider(
-    pre_process=True, post_process=True, add_encoder=True, add_decoder=True
-) -> Union[Qwen2_5VLModel]:
-    args = get_args()
-    build_tokenizer(args)
-    print_rank_0("start building qwen2-vl model ...")
-
-    # Config of vit, llm and projector
-    config = core_transformer_config_from_args(args)
-    use_te = args.transformer_impl == "transformer_engine"
-    if not use_te:
-        raise NotImplementedError("The Qwen2-VL model is only implemented with TransformerEngine!")
-
-    if args.rotary_seq_len_interpolation_factor is not None or args.rotary_seq_len_interpolation_factor != 1:
-        print_rank_0('Multimodal RoPE currently not support RoPE interpolation, set to None...')
-        args.rotary_seq_len_interpolation_factor = None
-
-    vision_config = get_vision_model_config(args, deepcopy(config))
-    vision_config.pipeline_model_parallel_size = 1
-    vision_config.first_pipeline_num_layers = None
-    vision_projector_config = get_vision_projection_config(deepcopy(config), vision_config.hidden_size, args.spatial_merge_size)
-
-    print_rank_0("building Qwen2-5-VL model in TE...")
-    # Layer Specs of vit, llm and projector
-    transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.qk_layernorm)
-    vision_model_spec = get_qwen2vl_vision_model_spec()
-    vision_projector_spec = get_mlp_module_spec(add_norm=False).submodules
-    if args.enable_variable_seq_lengths:
-        config.variable_seq_lengths = True
-
-    model = Qwen2_5VLModel(
-        language_transformer_config=config,
-        language_transformer_layer_spec=transformer_layer_spec,
-        language_vocab_size=args.padded_vocab_size,
-        language_max_sequence_length=args.max_position_embeddings,
-
-        vision_transformer_config=vision_config,
-        vision_transformer_layer_spec=vision_model_spec,
-        drop_vision_class_token=False, # NOTE: no class token to drop?
-
-        vision_projection_config=vision_projector_config,
-        vision_projection_layer_spec=vision_projector_spec,
-        vision_projection_type='mlp',
-        allow_missing_vision_projection_checkpoint= args.allow_missing_vision_projection_checkpoint,
-
-        language_position_embedding_type=args.position_embedding_type,
-        language_rotary_percent=args.rotary_percent,
-        language_rotary_base=args.rotary_base,
-
-        pre_process=pre_process,
-        post_process=post_process,
-        add_decoder=add_decoder,
-        add_encoder=add_encoder,
-
-        fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,
-        parallel_output=True,
-        language_share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,
-    )
-
-    model.freeze(
-        freeze_language_model=args.freeze_LM,
-        freeze_vision_model=args.freeze_ViT,
-        freeze_vision_projection=False
-    )
-
-    return model
-
-# copy from https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1404
-def get_rope_index(
-    input_ids: Optional[torch.LongTensor] = None,
-    image_grid_thw: Optional[torch.LongTensor] = None,
-    video_grid_thw: Optional[torch.LongTensor] = None,
-    second_per_grid_ts: Optional[torch.Tensor] = None,
-    attention_mask: Optional[torch.Tensor] = None,
-) -> Tuple[torch.Tensor, torch.Tensor]:
-    """
-    Calculate the 3D rope index based on image and video's temporal, height and width in LLM.
-
-    Explanation:
-        Each embedding sequence contains vision embedding and text embedding or just contains text embedding.
-
-        For pure text embedding sequence, the rotary position embedding has no difference with modern LLMs.
-        Examples:
-            input_ids: [T T T T T], here T is for text.
-            temporal position_ids: [0, 1, 2, 3, 4]
-            height position_ids: [0, 1, 2, 3, 4]
-            width position_ids: [0, 1, 2, 3, 4]
-
-        For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
-        and 1D rotary position embedding for text part.
-        Examples:
-            Temporal (Time): 3 patches, representing different segments of the video in time.
-            Height: 2 patches, dividing each frame vertically.
-            Width: 2 patches, dividing each frame horizontally.
-            We also have some important parameters:
-            fps (Frames Per Second): The video's frame rate, set to 1. This means one frame is processed each second.
-            tokens_per_second: This is a crucial parameter. It dictates how many "time-steps" or "temporal tokens" are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
-            temporal_patch_size: The number of frames that compose one temporal patch. Here, it's 2 frames.
-            interval: The step size for the temporal position IDs, calculated as tokens_per_second * temporal_patch_size / fps. In this case, 25 * 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
-            input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
-            vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
-            vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
-            vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
-            text temporal position_ids: [101, 102, 103, 104, 105]
-            text height position_ids: [101, 102, 103, 104, 105]
-            text width position_ids: [101, 102, 103, 104, 105]
-            Here we calculate the text start position_ids as the max vision position_ids plus 1.
-
-    Args:
-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
-            it.
-        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
-            The temporal, height and width of feature shape of each image in LLM.
-        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
-            The temporal, height and width of feature shape of each video in LLM.
-        second_per_grid_ts (`torch.Tensor` of shape `(num_videos)`, *optional*):
-            The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.
-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
-
-            - 1 for tokens that are **not masked**,
-            - 0 for tokens that are **masked**.
-
-    Returns:
-        position_ids (`torch.LongTensor` of shape `(3, batch_size, sequence_length)`)
-        mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)
-    """
-    args = get_args()
-    tokenizer = get_tokenizer()
-    spatial_merge_size = args.spatial_merge_size
-    image_token_id = tokenizer.image_token_id
-    video_token_id = tokenizer.video_token_id
-    vision_start_token_id = tokenizer.vision_start_token_id
-    tokens_per_second = 2
-    if second_per_grid_ts is not None:
-        second_per_grid_ts = second_per_grid_ts.cpu()
-
-    mrope_position_deltas = []
-    if image_grid_thw is not None or video_grid_thw is not None:
-        total_input_ids = input_ids
-        if attention_mask is None:
-            attention_mask = torch.ones_like(total_input_ids)
-        position_ids = torch.ones(
-            3,
-            input_ids.shape[0],
-            input_ids.shape[1],
-            dtype=input_ids.dtype,
-            device=input_ids.device,
-        )
-        image_index, video_index = 0, 0
-        attention_mask = attention_mask.to(total_input_ids.device)
-        for i, input_ids in enumerate(total_input_ids):
-            input_ids = input_ids[attention_mask[i] == 1]
-            image_nums, video_nums = 0, 0
-            vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)
-            vision_tokens = input_ids[vision_start_indices + 1]
-            image_nums = (vision_tokens == image_token_id).sum()
-            video_nums = (vision_tokens == video_token_id).sum()
-            input_tokens = input_ids.tolist()
-            llm_pos_ids_list: list = []
-            st = 0
-            remain_images, remain_videos = image_nums, video_nums
-            for _ in range(image_nums + video_nums):
-                if image_token_id in input_tokens and remain_images > 0:
-                    ed_image = input_tokens.index(image_token_id, st)
-                else:
-                    ed_image = len(input_tokens) + 1
-                if video_token_id in input_tokens and remain_videos > 0:
-                    ed_video = input_tokens.index(video_token_id, st)
-                else:
-                    ed_video = len(input_tokens) + 1
-                if ed_image < ed_video:
-                    t, h, w = (
-                        image_grid_thw[image_index][0],
-                        image_grid_thw[image_index][1],
-                        image_grid_thw[image_index][2],
-                    )
-                    second_per_grid_t = 0
-                    image_index += 1
-                    remain_images -= 1
-                    ed = ed_image
-
-                else:
-                    t, h, w = (
-                        video_grid_thw[video_index][0],
-                        video_grid_thw[video_index][1],
-                        video_grid_thw[video_index][2],
-                    )
-                    if second_per_grid_ts is not None:
-                        second_per_grid_t = second_per_grid_ts[video_index]
-                    else:
-                        second_per_grid_t = 1.0
-                    video_index += 1
-                    remain_videos -= 1
-                    ed = ed_video
-                llm_grid_t, llm_grid_h, llm_grid_w = (
-                    t.item(),
-                    h.item() // spatial_merge_size,
-                    w.item() // spatial_merge_size,
-                )
-                text_len = ed - st
-
-                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
-                llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)
-
-                range_tensor = torch.arange(llm_grid_t).view(-1, 1)
-                expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)
-
-                time_tensor = expanded_range * second_per_grid_t * tokens_per_second
-
-                time_tensor_long = time_tensor.long()
-                t_index = time_tensor_long.flatten()
-
-                h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
-                w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
-                llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)
-                st = ed + llm_grid_t * llm_grid_h * llm_grid_w
-
-            if st < len(input_tokens):
-                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
-                text_len = len(input_tokens) - st
-                llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)
-
-            llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
-            position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
-            mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
-        mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
-        return position_ids, mrope_position_deltas
-    else:
-        if attention_mask is not None:
-            position_ids = attention_mask.long().cumsum(-1) - 1
-            position_ids.masked_fill_(attention_mask == 0, 1)
-            position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)
-            max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
-            mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
-        else:
-            position_ids = (
-                torch.arange(input_ids.shape[1], device=input_ids.device)
-                .view(1, 1, -1)
-                .expand(3, input_ids.shape[0], -1)
-            )
-            mrope_position_deltas = torch.zeros(
-                [input_ids.shape[0], 1],
-                device=input_ids.device,
-                dtype=input_ids.dtype,
-            )
-
-        return position_ids, mrope_position_deltas
-
-def get_ltor_masks_and_position_ids(
-        input_ids,
-        image_thw_grids,
-        video_thw_grids,
-        target,
-        pad_token,
-        second_per_grid_ts,
-        ignore_index=None
-    ):
-    """Build masks and position id for left to right model."""
-    # Position ids. [3 X bs X seqlen]
-    position_ids, _ = get_rope_index(
-        input_ids=input_ids,
-        image_grid_thw=image_thw_grids,
-        video_grid_thw=video_thw_grids,
-        second_per_grid_ts=second_per_grid_ts,
-        attention_mask=input_ids != pad_token
-    )
-
-    # Loss mask.
-    loss_mask = torch.ones(target.size(), dtype=torch.float, device=input_ids.device)
-    loss_mask[target == pad_token] = 0.0  # mask paddings
-    if ignore_index is not None:
-        loss_mask[target == ignore_index] = 0.0  # mask prompts
-
-    # Attention mask.
-    attention_mask = None
-
-    return attention_mask, loss_mask, position_ids
-
-def get_batch(data_iterator):
-    """Generate a batch"""
-    imgs = None
-    tokens = None
-    labels = None
-    loss_mask = None
-    attention_mask = None
-    position_ids = None
-
-    # Broadcast data.
-    torch.cuda.nvtx.range_push("get_data")
-    if data_iterator is not None and get_tensor_model_parallel_rank() == 0:
-        data = next(data_iterator)
-        # pad_token_id = get_tokenizer().pad_token_id
-        pad_token_id = IGNORE_IDX
-        # while (data["target"] == pad_token_id).all() or (data["target"].shape[-1] < 986 or data["target"].shape[-1] > 1000): # for debug
-        while (data["target"] == pad_token_id).all():
-            logging.getLogger(__name__).warning("The current data is invalid because the target is all pad_token_id! Get next data to avoid fail, but it's better to check the data!")
-            data = next(data_iterator)
-    else:
-        data = None
-
-
-    data_text =  broadcast_data(["text"], data, torch.int64)["text"]
-
-    target =  broadcast_data(["target"], data, torch.int64)["target"]
-    # shape: num_tiles x c x h x w
-    imgs = broadcast_data(["imgs"], data, torch.float32)["imgs"]
-
-    # shape: num_tiles x c x h x w
-    videos = broadcast_data(["videos"], data, torch.float32)["videos"]
-
-    # shape: n_image_samples
-    image_thw_grids = broadcast_data(["image_thw_grids"], data, torch.long)["image_thw_grids"]
-
-    # global LAST_LARGE_IMG
-    # if LAST_LARGE_IMG:
-    #     torch.cuda.empty_cache()
-    #     LAST_LARGE_IMG=False
-    # if image_thw_grids.prod(axis=-1).sum() // 4 > 3000:
-    #     torch.cuda.empty_cache()
-    #     LAST_LARGE_IMG = True
-    args = get_args()
-    if data_text.shape[-1] == args.max_padding_length and get_pipeline_model_parallel_rank() == 0:
-        torch.cuda.empty_cache()
-    # shape: n_video_samples
-    video_thw_grids = broadcast_data(["video_thw_grids"], data, torch.long)["video_thw_grids"]
-    # shape: n_video_samples
-    second_per_grid_ts = broadcast_data(['second_per_grid_ts'], data, torch.float32)['second_per_grid_ts']
-
-
-    image_input_mask = broadcast_data(["image_input_mask"], data, torch.bool)["image_input_mask"]
-    video_input_mask = broadcast_data(["video_input_mask"], data, torch.bool)["video_input_mask"]
-    torch.cuda.nvtx.range_pop()
-
-    torch.cuda.nvtx.range_push("index tokens")
-    tokenizer = get_tokenizer()
-
-    tokens = data_text.long().contiguous()
-    labels = target.contiguous()
-
-    assert tokens.shape == labels.shape, f"tokens: {tokens.shape} != labels: {labels.shape}"
-    torch.cuda.nvtx.range_pop()
-
-    # NOTE: no sequence packing in LLM inputs
-    torch.cuda.nvtx.range_push("get_ltor_masks_and_position_ids")
-    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
-        tokens, image_thw_grids, video_thw_grids, labels, IGNORE_IDX, second_per_grid_ts
-    )
-    torch.cuda.nvtx.range_pop()
-
-    return (
-        tokens,
-        labels,
-        loss_mask,
-        attention_mask,
-        position_ids,
-        imgs,
-        videos,
-        image_thw_grids,
-        video_thw_grids,
-        image_input_mask,
-        video_input_mask
-    )
-
-# define spiky loss as a loss that's 10x the max loss observed
-SPIKY_LOSS_FACTOR = 10
-
-
-def loss_func(
-    loss_mask: torch.Tensor, output_tensor: torch.Tensor, model: Optional[Qwen2_5VLModel] = None
-):
-    """Loss function.
-
-    Args:
-        loss_mask (torch.Tensor): Used to mask out some portions of the loss
-        output_tensor (torch.Tensor): The tensor with the losses
-        model (Qwen2_5VLModel, optional): The model (can be wrapped)
-
-    Returns:
-        the loss scalar for this micro-batch
-        the number of non-padded tokens in this microbatch
-        a dict containing reporting metrics on the loss and number of tokens across
-            the data parallel ranks
-    """
-    args = get_args()
-
-    if has_nvidia_modelopt and modelopt_args_enabled(args):  # [ModelOpt]
-        return loss_func_modelopt(loss_mask, output_tensor, model=model)
-
-    losses = output_tensor.view(-1).float()
-    loss_mask = loss_mask.view(-1).float()
-    loss = torch.sum(losses * loss_mask)
-
-    # Check individual rank losses are not NaN prior to DP all-reduce.
-    rerun_state_machine = get_rerun_state_machine()
-    if args.check_for_nan_in_loss_and_grad:
-        rerun_state_machine.validate_result(
-            result=loss,
-            rejection_func=torch.isnan,
-            message="found NaN in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
-            fatal=True,
-        )
-        rerun_state_machine.validate_result(
-            result=loss,
-            rejection_func=torch.isinf,
-            message="found Inf in local forward loss calculation",
-            tolerance=0.0,  # forward pass calculations are determinisic
-            fatal=True,
-        )
-    # Check for spiky loss
-    if args.check_for_spiky_loss:
-        rerun_state_machine.validate_result(
-            result=loss,
-            rejection_func=partial(
-                rerun_state_machine.is_unexpectedly_large,
-                threshold=SPIKY_LOSS_FACTOR,
-                context="loss",
-            ),
-            message="Spiky loss",
-            tolerance=0.0,  # forward pass calculations are determinisic
-            fatal=False,
-        )
-
-    num_tokens = loss_mask.sum().clone().detach().to(torch.int)
-    reporting_loss = torch.cat([loss.clone().detach().view(1), num_tokens.view(1)])
-
-    return (loss, num_tokens, {'lm loss': reporting_loss})
-
-
-def forward_step(data_iterator, model: Qwen2_5VLModel):
-    """Forward training step.
-
-    Args:
-        data_iterator : Input data iterator
-        model (GPTModel): The GPT Model
-    """
-    args = get_args()
-    timers = get_timers()
-
-    # Get the batch.
-    timers('batch-generator', log_level=2).start()
-    global stimer
-    with stimer(bdata=True):
-        (
-            tokens,
-            labels,
-            loss_mask,
-            attention_mask,
-            position_ids,
-            imgs,
-            videos,
-            image_thw_grids,
-            video_thw_grids,
-            image_input_mask,
-            video_input_mask
-        ) = get_batch(data_iterator)
-    timers('batch-generator').stop()
-    vision_data = torch.cat([imgs, videos], dim=0)
-    vision_grid = torch.cat([image_thw_grids, video_thw_grids], dim=0)
-    with stimer:
-        output_tensor = model(
-            input_ids = tokens,
-            position_ids = position_ids,
-            vision_data = vision_data,
-            vision_grid_thw =  vision_grid,
-            video_start_index = image_input_mask.sum().cpu().item(),
-            image_input_mask = image_input_mask,
-            video_input_mask = video_input_mask,
-            attention_mask = attention_mask,
-            labels = labels
-        )
-
-    return output_tensor, partial(loss_func, loss_mask, model=model)
-
-def run_online_eval(model):
-    """Run an evaluation benchmark during training."""
-    # Do nothing.
-    return []
-
-def write_online_eval_to_tensorboard(data, iteration, writer):
-    """Write online evaluation data to Tensorboard."""
-    if not writer:
-        return
-
-    for item in data:
-        for k, v in item.items():
-            writer.add_scalar(k, v, iteration)
-
-def datasets_provider(worker_config=None):
-    """Create multimodal train, validation and test datasets."""
-    args = get_args()
-    dname = args.data_path[0] if type(args.data_path) is list else args.data_path
-    train_dataset = get_train_dataset(
-        dname,
-        batch_size=args.micro_batch_size,
-        task_encoder=TaskEncoder(),
-        worker_config=worker_config,
-        virtual_epoch_length=0,
-        max_samples_per_sequence=args.max_samples_per_sequence, # sequential shuffle in a tar
-        shuffle_buffer_size=args.shuffle_buffer_size, # shuffle in a sequential
-        handler=print_error_handler,
-        repeat=True,
-        image_decode="pil",
-    )
-    val_datasets_without_source_datasets = None
-    if args.eval_iters > 0:
-        val_datasets = get_val_datasets(
-            dname,
-            batch_size=args.micro_batch_size,
-            # This is the total number over all workers
-            # limit=args.eval_iters * get_num_microbatches(),
-            task_encoder=TaskEncoder(),
-            worker_config=worker_config,
-            handler=print_error_handler,
-            image_decode="pil",
-        )
-        val_datasets_without_source_datasets = [
-            # Limit the dataset to eval_iters * num_microbatches
-            LimitDataset(
-                # Repeat the inner dataset in case it's too short
-                RepeatDataset(val_ds, worker_config=worker_config),
-                length=args.eval_iters * get_num_microbatches(),
-                worker_config=worker_config,
-                reset_after_epoch=True,
-            )
-            for val_ds, _src_ds in val_datasets
-        ]
-
-    return train_dataset, val_datasets_without_source_datasets, None
-
-def is_first_or_last_stage(pp_size, encoder_pipeline_model_parallel_size):
-    """Check if the current pipeline parallel stage is the first or last stage."""
-    if pp_size == 1:    # No pipeline parallelism.
-        return True
-
-    is_valid_rank = False
-    pp_rank = get_pipeline_model_parallel_rank()
-    if encoder_pipeline_model_parallel_size == 0:
-        # No separate pipeline stage for the vision model. Run the dataloader on the first and last pipeline stage.
-        is_valid_rank = pp_rank in (0, pp_size-1)
-    elif encoder_pipeline_model_parallel_size == 1:
-        # Separate pipeline stage for the vision model. Run the dataloader on the first vision and LM stage and last LM stage.
-        is_valid_rank = pp_rank in (0, 1, pp_size-1)
-    else:
-        raise NotImplementedError("encoder-pipeline-model-parallel-size > 1 is not supported yet")
-
-    return is_valid_rank
-
-def is_dataloader_rank(encoder_pipeline_model_parallel_size):
-    """Check if we should have the dataloader on this tensor and pipeline parallel rank."""
-    # Run dataloader only on the first tensor parallel rank (will be broadcasted to others).
-    is_first_rank = get_tensor_model_parallel_rank() == 0
-
-    # NOTE(lizhiyu): when pp_size > 2
-    # pp_size = get_pipeline_model_parallel_world_size()
-    # is_first_rank = is_first_rank and is_first_or_last_stage(pp_size, encoder_pipeline_model_parallel_size)
-
-    return is_first_rank
-
-def train_valid_test_dataloaders_provider(train_val_test_num_samples):
-    """Build multimodal train, validation and test dataloaders."""
-    args = get_args()
-    # Dataloader is only on specific ranks.
-    if not is_dataloader_rank(args.encoder_pipeline_model_parallel_size):
-        return None, None, None
-
-    worker_debug_path = None
-    worker_log_level = 0
-
-    rank = parallel_state.get_data_parallel_rank()
-    world_size = parallel_state.get_data_parallel_world_size()
-    data_parallel_group = parallel_state.get_data_parallel_group()
-
-    worker_config = WorkerConfig(
-        rank=rank,
-        world_size=world_size,
-        num_workers=args.num_workers,
-        data_parallel_group=data_parallel_group,
-        worker_debug_path=worker_debug_path,
-        worker_log_level=worker_log_level,
-    )
-    train_ds, valid_ds1, test_ds = datasets_provider(worker_config)
-
-    train_dataloader = get_savable_loader(train_ds, worker_config=worker_config)
-    if args.load is not None:
-        if getattr(args, "dataloader_save", None):
-            dp_rank = parallel_state.get_data_parallel_rank()
-            data_save_name = get_checkpoint_name(
-                args.dataloader_save,
-                args.iteration,
-                pipeline_rank=0,    # Only the first pipeline parallel rank stores the dataloader checkpoint.
-                basename=f"train_dataloader_dprank{dp_rank:03d}.pt",
-            )
-            if os.path.exists(data_save_name):
-                try:
-                    dataset_state_dict = torch.load(data_save_name, map_location="cpu", weights_only=False)
-                    train_dataloader.restore_state_rank(dataset_state_dict["dataloader_state_dict"])
-                    print_rank_0(f"restored dataset state from {data_save_name}")
-                except Exception as e:
-                    print_rank_0("loading dataloader checkpoint failed. Skipping. " + str(e))
-
-    if valid_ds1 is not None:
-        valid_dataloader = [
-            EnergonDataloader(get_loader(valid_ds, worker_config=worker_config))
-            for valid_ds in valid_ds1
-        ]
-    else:
-        valid_dataloader = EnergonDataloader(None)
-    test_dataloader = None # NOTE: no test
-
-    return EnergonDataloader(train_dataloader), valid_dataloader, EnergonDataloader(test_dataloader)
-
-class EnergonDataloader:
-    """A wrapper to use Megatron Energon dataloader with the Megatron-LM training loop."""
-    def __init__(self, dataloader):
-        self._dataloader = dataloader
-        self._iter = iter(cyclic_iter(dataloader))
-
-    def __next__(self):
-        return self._iter.__next__()
-
-    def __iter__(self):
-        return self._iter.__iter__()
-
-    def save_state(self):
-        return self._dataloader.save_state_rank()
-
-
-def cyclic_iter(iter):
-    while True:
-        for x in iter:
-            yield x
-
-
-def add_multimodal_extra_args(parser):
-    """Extra arguments."""
-    group = parser.add_argument_group(title="multimodal arguments")
-    group.add_argument("--disable-vision-class-token", action="store_true", default=False, help="Disable vision class token")
-    group.add_argument(
-        "--dataloader-save", type=str, default=None, help="Energon dataloader state save path"
-    )
-
-    # qwen2-vl specific arguments
-    group.add_argument("--extra-vocab-size", type=int, default=421)
-    group.add_argument("--spatial-merge-size", type=int, default=2)
-    group.add_argument("--temporal-patch-size", type=int, default=2)
-    group.add_argument("--patch-size", type=int, default=14)
-    group.add_argument("--max-padding-length", type=int, default=2048)
-    group.add_argument("--enable-variable-seq-lengths", action="store_true", default=False, help="Enable variable sequence lengths")
-    group.add_argument("--vision-root", type=str, default = None, help="The vision dirctory root path.")
-    group.add_argument("--max-samples-per-sequence", type=int, default=2**31-1, help="max sequencial seqence samples in a slice")
-    group.add_argument("--shuffle-buffer-size", type=int, default=0, help="the buffer size to shuffle the samples in a seqence")
-    # learning rate
-    group.add_argument("--vision-ration", type=float, default=0.1, help="the learning rate ration of vision(inlude merger) compared with llm")
-    group.add_argument("--image-max-pixels", type=int, default=768*768, help="the maximum pixels of a single image")
-    group.add_argument("--image-min-pixels", type=int, default=32*32, help="the minimum pixels of a single image")
-    group.add_argument("--vision-recompute-layer-steps", type=int, default=0, help="the recmoute layers for vision using uniform method. 0 is disable.")
-
-
-
-    # just for checkpoint conversion
-    group.add_argument(
-        "--convert-checkpoint-from-megatron-to-transformers",
-        action="store_true",
-        help=(
-            "If True, convert a Megatron checkpoint to a Transformers checkpoint. "
-            "If False, convert a Transformers checkpoint to a Megatron checkpoint."
-        ),
-    )
-    group.add_argument("--freeze-LM", action="store_true", default=False, help="Freeze the language model")
-    group.add_argument("--freeze-ViT", action="store_true", default=False, help="Freeze the vision model")
-    group.add_argument(
-        "--allow-missing-vision-projection-checkpoint",
-        action="store_true",
-        default=False,
-        help="Allow missing vision projection checkpoint",
-    )
-    group.add_argument("--use-te", action="store_true", default=False, help="Use transformer engine")
-    return parser
-
-
-if __name__ == "__main__":
-    train_valid_test_dataloaders_provider.is_distributed = True
-
-    pretrain(
-        train_valid_test_dataloaders_provider,
-        model_provider,
-        ModelType.encoder_or_decoder,
-        forward_step,
-        args_defaults={'tokenizer_type': 'Qwen2VLTokenizer'},
-        extra_args_provider=add_multimodal_extra_args,
-        process_non_loss_data_func=write_online_eval_to_tensorboard,
-        non_loss_data_func=run_online_eval,
-    )
+# Mainly Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/examples/qwen2_5_vl/pretrain_qwen.py.Below is the original copyright:
+# Copyright (c) 2024 Alibaba PAI and Nvidia Megatron-LM Team.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+import os
+import sys
+from functools import partial
+from copy import deepcopy
+from typing import Union, Optional, Tuple
+
+import torch
+import torch._dynamo
+
+from argparse import Namespace
+
+# # For pytorch 2.6
+# torch.serialization.add_safe_globals([Namespace])
+
+from megatron.core import mpu
+
+from megatron.core import parallel_state
+from megatron.training.checkpointing import get_checkpoint_name
+from megatron.core.enums import ModelType
+from megatron.training import get_args, get_timers, pretrain, print_rank_0
+from megatron.training.arguments import core_transformer_config_from_args
+from megatron.training.utils import (
+    average_losses_across_data_parallel_group,
+)
+from megatron.core.num_microbatches_calculator import get_num_microbatches
+
+
+torch._dynamo.config.suppress_errors = True
+from megatron.core import mpu
+from megatron.core.parallel_state import get_tensor_model_parallel_rank, get_pipeline_model_parallel_world_size, get_pipeline_model_parallel_rank
+from megatron.energon import (
+    LimitDataset,
+    RepeatDataset,
+    WorkerConfig,
+    get_loader,
+    get_savable_loader,
+    get_train_dataset,
+    get_val_datasets,
+)
+
+from megatron.training.tokenizer.tokenizer import build_tokenizer
+from megatron.training.global_vars import get_tokenizer
+
+from flagscale.train.models.qwen2_5_vl.layer_specs import (
+    get_gpt_layer_with_transformer_engine_spec,
+    get_qwen2vl_vision_model_spec,
+    get_mlp_module_spec
+
+)
+from flagscale.train.models.qwen2_5_vl.qwen2_5_vl_model import Qwen2_5VLModel
+from flagscale.train.models.qwen2_5_vl.tensor_parallel import broadcast_data
+from flagscale.train.models.qwen2_5_vl.transformer_config import (
+    get_vision_model_config,
+    get_vision_projection_config
+)
+from tools.datasets.qwenvl.data.dataset_helpers import TaskEncoder, print_error_handler
+
+def model_provider(
+    pre_process=True, post_process=True, add_encoder=True, add_decoder=True
+) -> Union[Qwen2_5VLModel]:
+    args = get_args()
+    build_tokenizer(args)
+    print_rank_0("start building qwen2-vl model ...")
+
+    # Config of vit, llm and projector
+    config = core_transformer_config_from_args(args)
+    use_te = args.transformer_impl == "transformer_engine"
+    if not use_te:
+        raise NotImplementedError("The Qwen2-VL model is only implemented with TransformerEngine!")
+
+    if args.rotary_seq_len_interpolation_factor is not None or args.rotary_seq_len_interpolation_factor != 1:
+        print_rank_0('Multimodal RoPE currently not support RoPE interpolation, set to None...')
+        args.rotary_seq_len_interpolation_factor = None
+
+    vision_config = get_vision_model_config(args, deepcopy(config))
+    vision_config.pipeline_model_parallel_size = 1
+    vision_config.first_pipeline_num_layers = None
+    vision_projector_config = get_vision_projection_config(deepcopy(config), vision_config.hidden_size, args.spatial_merge_size)
+
+    print_rank_0("building Qwen2-5-VL model in TE...")
+    # Layer Specs of vit, llm and projector
+    transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.qk_layernorm)
+    vision_model_spec = get_qwen2vl_vision_model_spec()
+    vision_projector_spec = get_mlp_module_spec(add_norm=False).submodules
+
+    model = Qwen2_5VLModel(
+        language_transformer_config=config,
+        language_transformer_layer_spec=transformer_layer_spec,
+        language_vocab_size=args.padded_vocab_size,
+        language_max_sequence_length=args.max_position_embeddings,
+
+        vision_transformer_config=vision_config,
+        vision_transformer_layer_spec=vision_model_spec,
+        drop_vision_class_token=False, # NOTE: no class token to drop?
+
+        vision_projection_config=vision_projector_config,
+        vision_projection_layer_spec=vision_projector_spec,
+        vision_projection_type='mlp',
+        allow_missing_vision_projection_checkpoint= args.allow_missing_vision_projection_checkpoint,
+
+        language_position_embedding_type=args.position_embedding_type,
+        language_rotary_percent=args.rotary_percent,
+        language_rotary_base=args.rotary_base,
+
+        pre_process=pre_process,
+        post_process=post_process,
+        add_decoder=add_decoder,
+        add_encoder=add_encoder,
+
+        fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,
+        parallel_output=True,
+        language_share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,
+    )
+
+    model.freeze(
+        freeze_language_model=args.freeze_LM,
+        freeze_vision_model=args.freeze_ViT,
+        freeze_vision_projection=False
+    )
+
+    return model
+
+# Slightly modified from Qwen2_5VLForConditionalGeneration.get_rope_index
+def get_rope_index(
+    input_ids: Optional[torch.LongTensor] = None,
+    image_grid_thw: Optional[torch.LongTensor] = None,
+    video_grid_thw: Optional[torch.LongTensor] = None,
+    second_per_grid_ts: Optional[torch.Tensor] = None,
+    attention_mask: Optional[torch.Tensor] = None,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Calculate the 3D rope index based on image and video's temporal, height and width in LLM.
+
+    Explanation:
+        Each embedding sequence contains vision embedding and text embedding or just contains text embedding.
+
+        For pure text embedding sequence, the rotary position embedding has no difference with modern LLMs.
+        Examples:
+            input_ids: [T T T T T], here T is for text.
+            temporal position_ids: [0, 1, 2, 3, 4]
+            height position_ids: [0, 1, 2, 3, 4]
+            width position_ids: [0, 1, 2, 3, 4]
+
+        For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
+        and 1D rotary position embedding for text part.
+        Examples:
+            Temporal (Time): 3 patches, representing different segments of the video in time.
+            Height: 2 patches, dividing each frame vertically.
+            Width: 2 patches, dividing each frame horizontally.
+            We also have some important parameters:
+            fps (Frames Per Second): The video's frame rate, set to 1. This means one frame is processed each second.
+            tokens_per_second: This is a crucial parameter. It dictates how many "time-steps" or "temporal tokens" are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
+            temporal_patch_size: The number of frames that compose one temporal patch. Here, it's 2 frames.
+            interval: The step size for the temporal position IDs, calculated as tokens_per_second * temporal_patch_size / fps. In this case, 25 * 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
+            input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
+            vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
+            vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
+            vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
+            text temporal position_ids: [101, 102, 103, 104, 105]
+            text height position_ids: [101, 102, 103, 104, 105]
+            text width position_ids: [101, 102, 103, 104, 105]
+            Here we calculate the text start position_ids as the max vision position_ids plus 1.
+
+    Args:
+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
+            it.
+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
+            The temporal, height and width of feature shape of each image in LLM.
+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
+            The temporal, height and width of feature shape of each video in LLM.
+        second_per_grid_ts (`torch.Tensor` of shape `(num_videos)`, *optional*):
+            The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.
+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+    Returns:
+        position_ids (`torch.LongTensor` of shape `(3, batch_size, sequence_length)`)
+        mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)
+    """
+    args = get_args()
+    tokenizer = get_tokenizer()
+    spatial_merge_size = args.spatial_merge_size
+    image_token_id = tokenizer.image_token_id
+    video_token_id = tokenizer.video_token_id
+    vision_start_token_id = tokenizer.vision_start_token_id
+    tokens_per_second = 2
+    if second_per_grid_ts is not None:
+        second_per_grid_ts = second_per_grid_ts.cpu()
+
+    mrope_position_deltas = []
+    if image_grid_thw is not None or video_grid_thw is not None:
+        total_input_ids = input_ids
+        if attention_mask is None:
+            attention_mask = torch.ones_like(total_input_ids)
+        position_ids = torch.ones(
+            3,
+            input_ids.shape[0],
+            input_ids.shape[1],
+            dtype=input_ids.dtype,
+            device=input_ids.device,
+        )
+        image_index, video_index = 0, 0
+        attention_mask = attention_mask.to(total_input_ids.device)
+        for i, input_ids in enumerate(total_input_ids):
+            input_ids = input_ids[attention_mask[i] == 1]
+            image_nums, video_nums = 0, 0
+            vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)
+            vision_tokens = input_ids[vision_start_indices + 1]
+            image_nums = (vision_tokens == image_token_id).sum()
+            video_nums = (vision_tokens == video_token_id).sum()
+            input_tokens = input_ids.tolist()
+            llm_pos_ids_list: list = []
+            st = 0
+            remain_images, remain_videos = image_nums, video_nums
+            for _ in range(image_nums + video_nums):
+                if image_token_id in input_tokens and remain_images > 0:
+                    ed_image = input_tokens.index(image_token_id, st)
+                else:
+                    ed_image = len(input_tokens) + 1
+                if video_token_id in input_tokens and remain_videos > 0:
+                    ed_video = input_tokens.index(video_token_id, st)
+                else:
+                    ed_video = len(input_tokens) + 1
+                if ed_image < ed_video:
+                    t, h, w = (
+                        image_grid_thw[image_index][0],
+                        image_grid_thw[image_index][1],
+                        image_grid_thw[image_index][2],
+                    )
+                    second_per_grid_t = 0
+                    image_index += 1
+                    remain_images -= 1
+                    ed = ed_image
+
+                else:
+                    t, h, w = (
+                        video_grid_thw[video_index][0],
+                        video_grid_thw[video_index][1],
+                        video_grid_thw[video_index][2],
+                    )
+                    if second_per_grid_ts is not None:
+                        second_per_grid_t = second_per_grid_ts[video_index]
+                    else:
+                        second_per_grid_t = 1.0
+                    video_index += 1
+                    remain_videos -= 1
+                    ed = ed_video
+                llm_grid_t, llm_grid_h, llm_grid_w = (
+                    t.item(),
+                    h.item() // spatial_merge_size,
+                    w.item() // spatial_merge_size,
+                )
+                text_len = ed - st
+
+                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+                llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)
+
+                range_tensor = torch.arange(llm_grid_t).view(-1, 1)
+                expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)
+
+                time_tensor = expanded_range * second_per_grid_t * tokens_per_second
+
+                time_tensor_long = time_tensor.long()
+                t_index = time_tensor_long.flatten()
+
+                h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
+                w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
+                llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)
+                st = ed + llm_grid_t * llm_grid_h * llm_grid_w
+
+            if st < len(input_tokens):
+                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+                text_len = len(input_tokens) - st
+                llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)
+
+            llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
+            position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
+            mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
+        mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
+        return position_ids, mrope_position_deltas
+    else:
+        if attention_mask is not None:
+            position_ids = attention_mask.long().cumsum(-1) - 1
+            position_ids.masked_fill_(attention_mask == 0, 1)
+            position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)
+            max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
+            mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
+        else:
+            position_ids = (
+                torch.arange(input_ids.shape[1], device=input_ids.device)
+                .view(1, 1, -1)
+                .expand(3, input_ids.shape[0], -1)
+            )
+            mrope_position_deltas = torch.zeros(
+                [input_ids.shape[0], 1],
+                device=input_ids.device,
+                dtype=input_ids.dtype,
+            )
+
+        return position_ids, mrope_position_deltas
+
+def get_ltor_masks_and_position_ids(
+        input_ids,
+        image_thw_grids,
+        video_thw_grids,
+        target,
+        pad_token,
+        second_per_grid_ts,
+        ignore_index=None
+    ):
+    """Build masks and position id for left to right model."""
+    # Position ids. [3 X bs X seqlen]
+    position_ids, _ = get_rope_index(
+        input_ids=input_ids,
+        image_grid_thw=image_thw_grids,
+        video_grid_thw=video_thw_grids,
+        second_per_grid_ts=second_per_grid_ts,
+        attention_mask=input_ids != pad_token
+    )
+
+    # Loss mask.
+    loss_mask = torch.ones(target.size(), dtype=torch.float, device=input_ids.device)
+    loss_mask[target == pad_token] = 0.0  # mask paddings
+    if ignore_index is not None:
+        loss_mask[target == ignore_index] = 0.0  # mask prompts
+
+    # Attention mask.
+    attention_mask = None
+
+    return attention_mask, loss_mask, position_ids
+
+def get_batch(data_iterator):
+    """Generate a batch"""
+    imgs = None
+    tokens = None
+    labels = None
+    loss_mask = None
+    attention_mask = None
+    position_ids = None
+
+    # Broadcast data.
+    torch.cuda.nvtx.range_push("get_data")
+    if data_iterator is not None and get_tensor_model_parallel_rank() == 0:
+        data = next(data_iterator)
+    else:
+        data = None
+
+    data_text =  broadcast_data(["text"], data, torch.int64)["text"]
+
+    target =  broadcast_data(["target"], data, torch.int64)["target"]
+    # shape: num_tiles x c x h x w
+    imgs = broadcast_data(["imgs"], data, torch.float32)["imgs"]
+
+    # shape: num_tiles x c x h x w
+    videos = broadcast_data(["videos"], data, torch.float32)["videos"]
+
+    # shape: n_image_samples
+    image_thw_grids = broadcast_data(["image_thw_grids"], data, torch.long)["image_thw_grids"]
+    # shape: n_video_samples
+    video_thw_grids = broadcast_data(["video_thw_grids"], data, torch.long)["video_thw_grids"]
+    # shape: n_video_samples
+    second_per_grid_ts = broadcast_data(['second_per_grid_ts'], data, torch.float32)['second_per_grid_ts']
+
+
+    image_input_mask = broadcast_data(["image_input_mask"], data, torch.bool)["image_input_mask"]
+    video_input_mask = broadcast_data(["video_input_mask"], data, torch.bool)["video_input_mask"]
+    torch.cuda.nvtx.range_pop()
+
+    torch.cuda.nvtx.range_push("index tokens")
+    tokenizer = get_tokenizer()
+
+    tokens = data_text.long().contiguous()
+    labels = target.contiguous()
+
+    assert tokens.shape == labels.shape, f"tokens: {tokens.shape} != labels: {labels.shape}"
+    torch.cuda.nvtx.range_pop()
+
+    # NOTE: no sequence packing in LLM inputs
+    torch.cuda.nvtx.range_push("get_ltor_masks_and_position_ids")
+    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
+        tokens, image_thw_grids, video_thw_grids, labels, tokenizer.pad_token_id, second_per_grid_ts
+    )
+    torch.cuda.nvtx.range_pop()
+
+    return (
+        tokens,
+        labels,
+        loss_mask,
+        attention_mask,
+        position_ids,
+        imgs,
+        videos,
+        image_thw_grids,
+        video_thw_grids,
+        image_input_mask,
+        video_input_mask
+    )
+
+def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):
+    """Loss function.
+
+    Args:
+        loss_mask (torch.Tensor): Used to mask out some portions of the loss
+        output_tensor (torch.Tensor): The tensor with the losses
+    """
+    args = get_args()
+
+    losses = output_tensor.float()
+    loss_mask = loss_mask.view(-1).float()
+
+    loss = torch.stack([torch.sum(losses.view(-1) * loss_mask), loss_mask.sum()])
+    if args.context_parallel_size > 1:
+        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())
+
+    # Check individual rank losses are not NaN prior to DP all-reduce.
+    if args.check_for_nan_in_loss_and_grad:
+        global_rank = torch.distributed.get_rank()
+        assert not loss.isnan().any(), (
+            f"Rank {global_rank}: found NaN in local forward loss calculation. "
+            f"Device: {torch.cuda.current_device()}, node: {os.uname()[1]}"
+        )
+
+    averaged_loss = average_losses_across_data_parallel_group(loss)
+    averaged_loss = averaged_loss[0] / averaged_loss[1]
+
+    return loss[0] * args.context_parallel_size, {"lm loss": averaged_loss}
+
+def forward_step(data_iterator, model: Qwen2_5VLModel):
+    """Forward training step.
+
+    Args:
+        data_iterator : Input data iterator
+        model (GPTModel): The GPT Model
+    """
+    timers = get_timers()
+    # Get the batch.
+    timers("batch-generator", log_level=2).start()
+    (
+        tokens,
+        labels,
+        loss_mask,
+        attention_mask,
+        position_ids,
+        imgs,
+        videos,
+        image_thw_grids,
+        video_thw_grids,
+        image_input_mask,
+        video_input_mask
+    ) = get_batch(data_iterator)
+    timers("batch-generator").stop()
+
+    vision_data = torch.cat([imgs, videos], dim=0)
+    vision_grid = torch.cat([image_thw_grids, video_thw_grids], dim=0)
+
+    output_tensor = model(
+        input_ids = tokens,
+        position_ids = position_ids,
+        vision_data = vision_data,
+        vision_grid_thw =  vision_grid,
+        video_start_index = image_input_mask.sum().cpu().item(),
+        image_input_mask = image_input_mask,
+        video_input_mask = video_input_mask,
+        attention_mask = attention_mask,
+        labels = labels
+    )
+
+    return output_tensor, partial(loss_func, loss_mask)
+
+def run_online_eval(model):
+    """Run an evaluation benchmark during training."""
+    # Do nothing.
+    return []
+
+def write_online_eval_to_tensorboard(data, iteration, writer):
+    """Write online evaluation data to Tensorboard."""
+    if not writer:
+        return
+
+    for item in data:
+        for k, v in item.items():
+            writer.add_scalar(k, v, iteration)
+
+def datasets_provider(worker_config=None):
+    """Create multimodal train, validation and test datasets."""
+    args = get_args()
+    dname = args.data_path[0] if type(args.data_path) is list else args.data_path
+    train_dataset = get_train_dataset(
+        dname,
+        batch_size=args.micro_batch_size,
+        task_encoder=TaskEncoder(),
+        worker_config=worker_config,
+        virtual_epoch_length=1000,
+        max_samples_per_sequence=100,
+        shuffle_buffer_size=100,
+        handler=print_error_handler,
+        image_decode="pil",
+    )
+
+    val_datasets = get_val_datasets(
+        dname,
+        batch_size=args.micro_batch_size,
+        # This is the total number over all workers
+        # limit=args.eval_iters * get_num_microbatches(),
+        task_encoder=TaskEncoder(),
+        worker_config=worker_config,
+        handler=print_error_handler,
+        image_decode="pil",
+    )
+    val_datasets_without_source_datasets = [
+        # Limit the dataset to eval_iters * num_microbatches
+        LimitDataset(
+            # Repeat the inner dataset in case it's too short
+            RepeatDataset(val_ds, worker_config=worker_config),
+            length=args.eval_iters * get_num_microbatches(),
+            worker_config=worker_config,
+            reset_after_epoch=True,
+        )
+        for val_ds, _src_ds in val_datasets
+    ]
+
+    return train_dataset, val_datasets_without_source_datasets, None
+
+def is_first_or_last_stage(pp_size, encoder_pipeline_model_parallel_size):
+    """Check if the current pipeline parallel stage is the first or last stage."""
+    if pp_size == 1:    # No pipeline parallelism.
+        return True
+
+    is_valid_rank = False
+    pp_rank = get_pipeline_model_parallel_rank()
+    if encoder_pipeline_model_parallel_size == 0:
+        # No separate pipeline stage for the vision model. Run the dataloader on the first and last pipeline stage.
+        is_valid_rank = pp_rank in (0, pp_size-1)
+    elif encoder_pipeline_model_parallel_size == 1:
+        # Separate pipeline stage for the vision model. Run the dataloader on the first vision and LM stage and last LM stage.
+        is_valid_rank = pp_rank in (0, 1, pp_size-1)
+    else:
+        raise NotImplementedError("encoder-pipeline-model-parallel-size > 1 is not supported yet")
+
+    return is_valid_rank
+
+def is_dataloader_rank(encoder_pipeline_model_parallel_size):
+    """Check if we should have the dataloader on this tensor and pipeline parallel rank."""
+    # Run dataloader only on the first tensor parallel rank (will be broadcasted to others).
+    is_first_rank = get_tensor_model_parallel_rank() == 0
+
+    pp_size = get_pipeline_model_parallel_world_size()
+    is_first_rank = is_first_rank and is_first_or_last_stage(pp_size, encoder_pipeline_model_parallel_size)
+
+    return is_first_rank
+
+def train_valid_test_dataloaders_provider(train_val_test_num_samples):
+    """Build multimodal train, validation and test dataloaders."""
+    args = get_args()
+    # Dataloader is only on specific ranks.
+    if not is_dataloader_rank(args.encoder_pipeline_model_parallel_size):
+        return None, None, None
+
+    worker_debug_path = None
+    worker_log_level = 0
+
+    rank = parallel_state.get_data_parallel_rank()
+    world_size = parallel_state.get_data_parallel_world_size()
+    data_parallel_group = parallel_state.get_data_parallel_group()
+
+    worker_config = WorkerConfig(
+        rank=rank,
+        world_size=world_size,
+        num_workers=args.num_workers,
+        data_parallel_group=data_parallel_group,
+        worker_debug_path=worker_debug_path,
+        worker_log_level=worker_log_level,
+    )
+    train_ds, valid_ds1, test_ds = datasets_provider(worker_config)
+
+    train_dataloader = get_savable_loader(train_ds, worker_config=worker_config)
+    if args.load is not None:
+        if getattr(args, "dataloader_save", None):
+            dp_rank = parallel_state.get_data_parallel_rank()
+            data_save_name = get_checkpoint_name(
+                args.dataloader_save,
+                args.iteration,
+                pipeline_rank=0,    # Only the first pipeline parallel rank stores the dataloader checkpoint.
+                basename=f"train_dataloader_dprank{dp_rank:03d}.pt",
+            )
+            if os.path.exists(data_save_name):
+                try:
+                    dataset_state_dict = torch.load(data_save_name, map_location="cpu", weights_only=False)
+                    train_dataloader.restore_state_rank(dataset_state_dict["dataloader_state_dict"])
+                    print_rank_0(f"restored dataset state from {data_save_name}")
+                except Exception as e:
+                    print_rank_0("loading dataloader checkpoint failed. Skipping. " + str(e))
+
+    valid_dataloader = [
+        EnergonDataloader(get_loader(valid_ds, worker_config=worker_config))
+        for valid_ds in valid_ds1
+    ]
+    test_dataloader = None # NOTE: no test
+
+    return EnergonDataloader(train_dataloader), valid_dataloader, EnergonDataloader(test_dataloader)
+
+class EnergonDataloader:
+    """A wrapper to use Megatron Energon dataloader with the Megatron-LM training loop."""
+    def __init__(self, dataloader):
+        self._dataloader = dataloader
+        self._iter = iter(cyclic_iter(dataloader))
+
+    def __next__(self):
+        return self._iter.__next__()
+
+    def __iter__(self):
+        return self._iter.__iter__()
+
+    def save_state(self):
+        return self._dataloader.save_state_rank()
+
+
+def cyclic_iter(iter):
+    while True:
+        for x in iter:
+            yield x
+
+
+def add_multimodal_extra_args(parser):
+    """Extra arguments."""
+    group = parser.add_argument_group(title="multimodal arguments")
+    group.add_argument("--disable-vision-class-token", action="store_true", default=False, help="Disable vision class token")
+    group.add_argument(
+        "--dataloader-save", type=str, default=None, help="Energon dataloader state save path"
+    )
+
+    # qwen2-vl specific arguments
+    group.add_argument("--extra-vocab-size", type=int, default=421)
+    group.add_argument("--spatial-merge-size", type=int, default=2)
+    group.add_argument("--temporal-patch-size", type=int, default=2)
+    group.add_argument("--patch-size", type=int, default=14)
+    group.add_argument("--max-padding-length", type=int, default=2048)
+
+    # just for checkpoint conversion
+    group.add_argument(
+        "--convert-checkpoint-from-megatron-to-transformers",
+        action="store_true",
+        help=(
+            "If True, convert a Megatron checkpoint to a Transformers checkpoint. "
+            "If False, convert a Transformers checkpoint to a Megatron checkpoint."
+        ),
+    )
+    group.add_argument("--freeze-LM", action="store_true", default=False, help="Freeze the language model")
+    group.add_argument("--freeze-ViT", action="store_true", default=False, help="Freeze the vision model")
+    group.add_argument(
+        "--allow-missing-vision-projection-checkpoint",
+        action="store_true",
+        default=False,
+        help="Allow missing vision projection checkpoint",
+    )
+    group.add_argument("--use-te", action="store_true", default=False, help="Use transformer engine")
+    return parser
+
+
+if __name__ == "__main__":
+    train_valid_test_dataloaders_provider.is_distributed = True
+
+    pretrain(
+        train_valid_test_dataloaders_provider,
+        model_provider,
+        ModelType.encoder_or_decoder,
+        forward_step,
+        args_defaults={'tokenizer_type': 'Qwen2VLTokenizer'},
+        extra_args_provider=add_multimodal_extra_args,
+        process_non_loss_data_func=write_online_eval_to_tensorboard,
+        non_loss_data_func=run_online_eval,
+    )
diff --git a/tools/datasets/qwenvl/build_llava_frame_dataset.py b/tools/datasets/qwenvl/build_llava_frame_dataset.py
index a907b234..f6a37f49 100644
--- a/tools/datasets/qwenvl/build_llava_frame_dataset.py
+++ b/tools/datasets/qwenvl/build_llava_frame_dataset.py
@@ -1,172 +1,172 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/build_llava_frame_dataset.py
-import glob
-import json
-import multiprocessing.pool as mpp
-import os
-import tarfile
-
-from argparse import ArgumentParser
-from multiprocessing import Pool
-from pathlib import Path
-
-import cv2
-
-from tqdm import tqdm
-
-
-def istarmap(self, func, iterable, chunksize=1):
-    """starmap-version of imap"""
-    self._check_running()
-    if chunksize < 1:
-        raise ValueError("Chunksize must be 1+, not {0:n}".format(chunksize))
-
-    task_batches = mpp.Pool._get_tasks(func, iterable, chunksize)
-    result = mpp.IMapIterator(self)
-    self._taskqueue.put(
-        (
-            self._guarded_task_generation(result._job, mpp.starmapstar, task_batches),
-            result._set_length,
-        )
-    )
-    return (item for chunk in result for item in chunk)
-
-
-mpp.Pool.istarmap = istarmap
-
-
-def find_json_files(dataset_root):
-    root_path = Path(dataset_root).resolve()
-    json_files = list(root_path.rglob("*.json"))
-    jsonl_files = list(root_path.rglob("*.jsonl"))
-
-    all_files = json_files + jsonl_files
-    relative_paths = [p.relative_to(root_path) for p in all_files]
-    return [str(p) for p in relative_paths]
-
-
-def extract_video_frames(dataset_root: str, video_paths: list, time_interval: float = 1.0):
-    for rel_path in video_paths:
-        input_path = os.path.join(dataset_root, rel_path)
-        output_subdir, _ = os.path.splitext(input_path)
-        os.makedirs(output_subdir, exist_ok=True)
-
-        cap = cv2.VideoCapture(input_path)
-        if not cap.isOpened():
-            print(f"Video not opened: {input_path}")
-            continue
-
-        fps = cap.get(cv2.CAP_PROP_FPS)
-        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
-
-        interval_frames = max(1, int(fps * time_interval))
-        current_frame = 0
-        while True:
-            ret, frame = cap.read()
-            if not ret:
-                break
-
-            if current_frame % interval_frames == 0:
-                filename = f"frame_{current_frame:06}.jpg"
-                save_path = os.path.join(output_subdir, filename)
-                cv2.imwrite(save_path, frame)
-
-            current_frame += 1
-
-        with open(output_subdir + ".json", "w") as f:
-            json.dump({"fps": str(fps / interval_frames)}, f)
-
-
-def process(dataset_root, output_file, interval=1.0, num_workers: int = 32, video_token="<image>"):
-    json_or_jsonl = glob.glob(os.path.join(dataset_root, "*.json")) + glob.glob(
-        os.path.join(dataset_root, "*.jsonl")
-    )
-
-    full_data = []
-
-    args_list = []
-    for file in find_json_files(dataset_root):
-        rel_to_dir, _ = os.path.split(file)
-        file = os.path.join(dataset_root, file)
-        try:
-            with open(file, "r") as f:
-                data = json.load(f)
-        except:
-            with open(file, "r") as f:
-                data = [json.loads(l) for l in f.readlines()]
-
-        print(f"processing {file}")
-        for d in tqdm(data):
-            if isinstance(d, list):
-                assert len(d) == 1
-                d = d[0]
-            if "image" in d:
-                d["images"] = [os.path.join(rel_to_dir, d.pop("image"))]
-            if "video" in d:
-                d["videos"] = [os.path.join(rel_to_dir, d.pop("video"))]
-                for v in d["videos"]:
-                    args_list.append((dataset_root, [v], interval))
-
-            for c in d["conversations"]:
-                c["value"] = c["value"].replace(video_token, "<video>")
-            full_data.append(d)
-
-    pool = Pool(32)
-    it = pool.istarmap(extract_video_frames, args_list)
-    for _ in tqdm(it, total=len(args_list)):
-        pass
-
-    with open(os.path.join(dataset_root, output_file), "w") as f:
-        json.dump(full_data, f)
-
-
-def extract_video(dataset_root):
-    # extract all .tar.gz to the split folder
-    splits = os.listdir(dataset_root)
-    for split in splits:
-        p = os.path.join(dataset_root, split)
-        if not os.path.isdir(p):
-            continue
-        files = [f for f in os.listdir(p) if f.endswith(".tar.gz")]
-        for f in files:
-            with tarfile.open(os.path.join(p, f), "r:gz") as tar:
-                tar.extractall(path=p)
-
-
-if __name__ == "__main__":
-    argparser = ArgumentParser()
-
-    argparser.add_argument(
-        "--dataset-root", type=str, required=True, help="The root of LLaVA-Video-178K dataset"
-    )
-    argparser.add_argument(
-        "--time-interval",
-        type=float,
-        default=1.0,
-        help="The time interval to extract frame from videos",
-    )
-    argparser.add_argument(
-        "--output-json",
-        type=str,
-        default="dataset.json",
-        help="Filename of the merged json dataset",
-    )
-    argparser.add_argument("--skip-extraction", action="store_true")
-    argparser.add_argument(
-        "--video-token",
-        type=str,
-        default="<image>",
-        help="The default video token in LLaVA-Video-178K is <image> instead of <video>",
-    )
-
-    args = argparser.parse_args()
-
-    if not args.skip_extraction:
-        print("video extraction starting")
-        extract_video(args.dataset_root)
-        print("video extraction finished")
-    process(
-        args.dataset_root,
-        args.output_json,
-        interval=args.time_interval,
-        video_token=args.video_token,
-    )
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/build_llava_frame_dataset.py
+import glob
+import json
+import multiprocessing.pool as mpp
+import os
+import tarfile
+
+from argparse import ArgumentParser
+from multiprocessing import Pool
+from pathlib import Path
+
+import cv2
+
+from tqdm import tqdm
+
+
+def istarmap(self, func, iterable, chunksize=1):
+    """starmap-version of imap"""
+    self._check_running()
+    if chunksize < 1:
+        raise ValueError("Chunksize must be 1+, not {0:n}".format(chunksize))
+
+    task_batches = mpp.Pool._get_tasks(func, iterable, chunksize)
+    result = mpp.IMapIterator(self)
+    self._taskqueue.put(
+        (
+            self._guarded_task_generation(result._job, mpp.starmapstar, task_batches),
+            result._set_length,
+        )
+    )
+    return (item for chunk in result for item in chunk)
+
+
+mpp.Pool.istarmap = istarmap
+
+
+def find_json_files(dataset_root):
+    root_path = Path(dataset_root).resolve()
+    json_files = list(root_path.rglob("*.json"))
+    jsonl_files = list(root_path.rglob("*.jsonl"))
+
+    all_files = json_files + jsonl_files
+    relative_paths = [p.relative_to(root_path) for p in all_files]
+    return [str(p) for p in relative_paths]
+
+
+def extract_video_frames(dataset_root: str, video_paths: list, time_interval: float = 1.0):
+    for rel_path in video_paths:
+        input_path = os.path.join(dataset_root, rel_path)
+        output_subdir, _ = os.path.splitext(input_path)
+        os.makedirs(output_subdir, exist_ok=True)
+
+        cap = cv2.VideoCapture(input_path)
+        if not cap.isOpened():
+            print(f"Video not opened: {input_path}")
+            continue
+
+        fps = cap.get(cv2.CAP_PROP_FPS)
+        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+
+        interval_frames = max(1, int(fps * time_interval))
+        current_frame = 0
+        while True:
+            ret, frame = cap.read()
+            if not ret:
+                break
+
+            if current_frame % interval_frames == 0:
+                filename = f"frame_{current_frame:06}.jpg"
+                save_path = os.path.join(output_subdir, filename)
+                cv2.imwrite(save_path, frame)
+
+            current_frame += 1
+
+        with open(output_subdir + ".json", "w") as f:
+            json.dump({"fps": str(fps / interval_frames)}, f)
+
+
+def process(dataset_root, output_file, interval=1.0, num_workers: int = 32, video_token="<image>"):
+    json_or_jsonl = glob.glob(os.path.join(dataset_root, "*.json")) + glob.glob(
+        os.path.join(dataset_root, "*.jsonl")
+    )
+
+    full_data = []
+
+    args_list = []
+    for file in find_json_files(dataset_root):
+        rel_to_dir, _ = os.path.split(file)
+        file = os.path.join(dataset_root, file)
+        try:
+            with open(file, "r") as f:
+                data = json.load(f)
+        except:
+            with open(file, "r") as f:
+                data = [json.loads(f) for l in f.readlines()]
+
+        print(f"processing {file}")
+        for d in tqdm(data):
+            if isinstance(d, list):
+                assert len(d) == 1
+                d = d[0]
+            if "image" in d:
+                d["images"] = [os.path.join(rel_to_dir, d.pop("image"))]
+            if "video" in d:
+                d["videos"] = [os.path.join(rel_to_dir, d.pop("video"))]
+                for v in d["videos"]:
+                    args_list.append((dataset_root, [v], interval))
+
+            for c in d["conversations"]:
+                c["value"] = c["value"].replace(video_token, "<video>")
+            full_data.append(d)
+
+    pool = Pool(32)
+    it = pool.istarmap(extract_video_frames, args_list)
+    for _ in tqdm(it, total=len(args_list)):
+        pass
+
+    with open(os.path.join(dataset_root, output_file), "w") as f:
+        json.dump(full_data, f)
+
+
+def extract_video(dataset_root):
+    # extract all .tar.gz to the split folder
+    splits = os.listdir(dataset_root)
+    for split in splits:
+        p = os.path.join(dataset_root, split)
+        if not os.path.isdir(p):
+            continue
+        files = [f for f in os.listdir(p) if f.endswith(".tar.gz")]
+        for f in files:
+            with tarfile.open(os.path.join(p, f), "r:gz") as tar:
+                tar.extractall(path=p)
+
+
+if __name__ == "__main__":
+    argparser = ArgumentParser()
+
+    argparser.add_argument(
+        "--dataset-root", type=str, required=True, help="The root of LLaVA-Video-178K dataset"
+    )
+    argparser.add_argument(
+        "--time-interval",
+        type=float,
+        default=1.0,
+        help="The time interval to extract frame from videos",
+    )
+    argparser.add_argument(
+        "--output-json",
+        type=str,
+        default="dataset.json",
+        help="Filename of the merged json dataset",
+    )
+    argparser.add_argument("--skip-extraction", action="store_true")
+    argparser.add_argument(
+        "--video-token",
+        type=str,
+        default="<image>",
+        help="The default video token in LLaVA-Video-178K is <image> instead of <video>",
+    )
+
+    args = argparser.parse_args()
+
+    if not args.skip_extraction:
+        print("video extraction starting")
+        extract_video(args.dataset_root)
+        print("video extraction finished")
+    process(
+        args.dataset_root,
+        args.output_json,
+        interval=args.time_interval,
+        video_token=args.video_token,
+    )
diff --git a/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py b/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py
new file mode 100644
index 00000000..d7910687
--- /dev/null
+++ b/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py
@@ -0,0 +1,144 @@
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/convert_custom_dataset_to_wds_chatml.py
+import json
+import os
+import pickle
+
+from argparse import ArgumentParser
+
+import cv2
+import webdataset as wds
+import yaml
+
+from tqdm import tqdm
+from webdataset.writer import add_handlers, default_handlers, imageencoder
+
+from megatron.energon.epathlib import EPath
+from megatron.energon.flavors import BaseWebdatasetFactory
+
+
+def convert(dataset_dir, json_name, sort_function=sorted, max_count=10000):
+    """
+    Here we provide an example to convert llava-pretrain dataset to ChatMLSample
+    """
+    # Paths to the dataset files
+    json_file = os.path.join(dataset_dir, json_name)
+    output = os.path.join(dataset_dir, "wds")
+
+    if not os.path.exists(output):
+        os.mkdir(output)
+
+    # Load data
+    with open(json_file, "r") as f:
+        data = json.load(f)
+
+    # custom webdataset ShardWriter Encoder
+    add_handlers(
+        default_handlers, "jpgs", lambda data: pickle.dumps([imageencoder(d, "jpg") for d in data])
+    )
+    add_handlers(
+        default_handlers,
+        "videos",
+        lambda data: pickle.dumps([[imageencoder(d, "jpg") for d in video] for video in data]),
+    )
+
+    has_idx = None
+    with wds.ShardWriter(
+        os.path.join(output, "pretrain-%d.tar"), maxcount=max_count
+    ) as shard_writer:
+        for idx, entry in enumerate(tqdm(data)):
+            # NOTE: read a dataset in sharegpt format
+            image_datas = []
+            for image in entry.pop("images", []):
+                image_datas.append(
+                    cv2.imread(os.path.join(dataset_dir, image), cv2.IMREAD_UNCHANGED)
+                )
+
+            video_datas = []
+            second_per_grid_ts = []
+            for video in entry.pop("videos", []):
+                video_noext, _ = os.path.splitext(video)
+                frame_folder = os.path.join(dataset_dir, video_noext)
+                # NOTE: we implicitly require a `${frame_folder}.json`` file containing fps rates of each video
+                # otherwise fps will be regarded as `1` by default.
+                if os.path.exists(frame_folder + ".json"):
+                    with open(frame_folder + ".json", "r") as f:
+                        fps = float(json.load(f)["fps"])
+                else:
+                    fps = 2.0
+
+                frames = []
+                for frame in sort_function(os.listdir(frame_folder)):
+                    frames.append(
+                        cv2.imread(os.path.join(frame_folder, frame), cv2.IMREAD_UNCHANGED)
+                    )
+
+                if len(frames) % 2 == 1:
+                    frames = frames[:-1]
+                video_datas.append(frames)
+                second_per_grid_ts.append(1 / fps)
+
+            if has_idx is None:
+                has_idx = "id" in entry
+            assert has_idx == ("id" in entry), "All entries should either all contain idx or not."
+
+            sample = {
+                "__key__": entry.pop("id", str(idx)),
+                "jpgs": image_datas,
+                "videos": video_datas,
+                "json": json.dumps(
+                    {
+                        "conversations": entry["conversations"],
+                        "second_per_grid_ts": second_per_grid_ts,
+                    }
+                ).encode("utf-8"),
+            }
+            shard_writer.write(sample)
+
+    print(f"Dataset successfully converted to wds")
+    return output
+
+
+def generate_configs(path: EPath, split, shuffle_tars=True, num_workers=32):
+    path = path.absolute()
+    all_tars = list(path.glob("**/*.tar")) + list(path.glob("**/*.tgz"))
+    all_tars = [str(p.relative_to(path)) for p in sorted(all_tars)]
+    split_parts_ratio = [("train", split[0]), ("val", split[1]), ("test", split[2])]
+    split_parts_patterns = None
+
+    # NOTE: generate .info.yaml and split.yaml
+    _ = BaseWebdatasetFactory.prepare_dataset(
+        path,
+        all_tars,
+        split_parts_ratio=split_parts_ratio,
+        split_parts_patterns=split_parts_patterns,
+        tar_index_only=False,
+        shuffle_seed=42 if shuffle_tars else None,
+        workers=num_workers,
+    )
+
+    # NOTE: dump dataset.yaml
+    metadata = {
+        "__class__": "ChatMLWebdataset",
+        "__module__": "tools.datasets.qwenvl.data..energon.chatml",
+        "field_map": {"imgs": "jpgs", "videos": "videos", "conversation": "json"},
+    }
+    with open(os.path.join(path.url, ".nv-meta", "dataset.yaml"), "w") as f:
+        yaml.safe_dump(metadata, f)
+
+
+if __name__ == "__main__":
+    argparser = ArgumentParser()
+    argparser.add_argument("--dataset-root", required=True, type=str)
+    argparser.add_argument("--json", default="dataset.json", type=str)
+    argparser.add_argument("--max-samples-per-tar", default=10000, type=float)
+    argparser.add_argument("--train-split", default=9, type=float)
+    argparser.add_argument("--val-split", default=1, type=float)
+    argparser.add_argument("--test-split", default=0, type=float)
+    args = argparser.parse_args()
+
+    output_dir = convert(args.dataset_root, args.json, max_count=args.max_samples_per_tar)
+    print(f"Generating Configurations")
+    # NOTE: split_ratio: train/val/test
+    split = [args.train_split, args.val_split, args.test_split]
+    generate_configs(EPath(output_dir), split)
+    print(f"Configurations Generated")
diff --git a/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml_str.py b/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml_str.py
deleted file mode 100644
index e7e1517b..00000000
--- a/tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml_str.py
+++ /dev/null
@@ -1,216 +0,0 @@
-# Adopted from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/convert_custom_dataset_to_wds_chatml.py
-# We must store the path of vision data, not the real data.
-
-import json
-import math
-import os
-import pickle
-
-from argparse import ArgumentParser
-from typing import List, Union
-
-import cv2
-import webdataset as wds
-import yaml
-
-from tqdm import tqdm
-from webdataset.writer import add_handlers, default_handlers, imageencoder
-
-from megatron.energon.epathlib import EPath
-from megatron.energon.flavors import BaseWebdatasetFactory
-
-
-def convert(
-    dataset_dir,
-    output_dir,
-    json_name,
-    sort_function=sorted,
-    max_count=10000,
-    image_key="images",
-    video_key="videos",
-    vision_dir=None,
-    dp_size=1,
-    drop_last=False,
-):
-    """
-    Here we provide an example to convert llava-pretrain dataset to ChatMLSample
-    """
-    if vision_dir is None:
-        vision_dir = dataset_dir
-    # Paths to the dataset files
-    json_file = os.path.join(dataset_dir, json_name)
-    output = os.path.join(output_dir, f"wds-{dp_size}")
-    os.makedirs(output, exist_ok=True)
-
-    # support both json and jsonl
-    try:
-        with open(json_file, "r") as f:
-            data = json.load(f)
-    except:
-        with open(json_file, "r") as f:
-            data = [json.loads(l) for l in f.readlines()]
-    data_len = len(data)
-    print(f"Loaded {data_len} entries")
-
-    print(f"The fisrt entry in the dataset is {data[0]}")
-    if image_key not in data[0]:
-        print(f"Warning: {image_key} not found in the first entry")
-    if video_key not in data[0]:
-        print(f"Warning: {video_key} not found in the first entry")
-
-    # custom webdataset ShardWriter Encoder
-    # "jpgs": the key when saving the image, see line 93
-    # "videos": the key when saving the video, see line 92
-    add_handlers(default_handlers, 'jpgs', lambda data: pickle.dumps(data))
-    add_handlers(default_handlers, 'videos', lambda data: pickle.dumps(data))
-
-    def write_sample(entry, vision_dir, has_idx=None, idx=0):
-        # NOTE: read a dataset in sharegpt format
-        image_datas: List[str] = []
-        # NOTE: we support both list and str for image path.
-        image_paths = entry.get(image_key, [])
-        if isinstance(image_paths, str):
-            image_paths = [image_paths]
-        image_datas = image_paths
-
-        video_datas: List[List[str]] = []
-        second_per_grid_ts = []
-
-        for video in entry.pop(video_key, []):
-            video_noext, _ = os.path.splitext(video)
-            frame_folder = os.path.join(vision_dir, video_noext)
-            # NOTE: we implicitly require a `${frame_folder}.json`` file containing fps rates of each video
-            # otherwise fps will be regarded as `1` by default.
-            if os.path.exists(frame_folder + ".json"):
-                with open(frame_folder + ".json", "r") as f:
-                    fps = float(json.load(f)["fps"])
-            else:
-                fps = 2.0
-
-            frames: List[str] = []
-            for frame in sort_function(os.listdir(frame_folder)):
-                # get relative pathremove "vision_dir"
-                relative_path = os.path.relpath(os.path.join(frame_folder, frame), start=vision_dir)
-                frames.appen(relative_path)
-
-            if len(frames) % 2 == 1:
-                frames = frames[:-1]
-            video_datas.append(frames)
-            second_per_grid_ts.append(1 / fps)
-
-        if has_idx is None:
-            has_idx = "id" in entry
-        assert has_idx == ("id" in entry), "All entries should either all contain idx or not."
-
-        sample = {
-            "__key__": entry.pop("id", str(idx)),
-            "jpgs": image_datas,
-            "videos": video_datas,
-            "json": json.dumps(
-                {"conversations": entry["conversations"], "second_per_grid_ts": second_per_grid_ts}
-            ).encode("utf-8"),
-        }
-        shard_writer.write(sample)
-
-    has_idx = None
-    if drop_last:
-        num_per_rank = data_len // dp_size
-        left_data_count = data_len % dp_size
-        with wds.ShardWriter(
-            os.path.join(output, "pretrain-%d.tar"), maxcount=max_count, maxsize=9e9
-        ) as shard_writer:
-            for rank in tqdm(range(dp_size)):
-                for id in tqdm(range(num_per_rank)):
-                    data_id = id * dp_size + rank
-                    entry = data[data_id]
-                    write_sample(entry, vision_dir, has_idx=has_idx, idx=data_id)
-            if left_data_count > 0:
-                for idx, entry in enumerate(data[data_len - left_data_count :]):
-                    write_sample(
-                        entry, vision_dir, has_idx=has_idx, idx=data_len - left_data_count + idx
-                    )
-    else:
-        num_per_rank = math.ceil(data_len / dp_size)
-        with wds.ShardWriter(
-            os.path.join(output, "pretrain-%d.tar"), maxcount=max_count, maxsize=9e9
-        ) as shard_writer:
-            for rank in tqdm(range(dp_size)):
-                for id in tqdm(range(num_per_rank)):
-                    data_id = id * dp_size + rank
-                    if data_id >= data_len:
-                        break
-                    entry = data[data_id]
-                    write_sample(entry, vision_dir, has_idx=has_idx, idx=data_id)
-
-    print(f"Dataset successfully converted to wds")
-    return output
-
-
-def generate_configs(path: EPath, split, shuffle_tars=True, num_workers=1):
-    # path = path.absolute()
-    all_tars = list(path.glob("**/*.tar")) + list(path.glob("**/*.tgz"))
-    all_tars = [str(p.relative_to(path)) for p in sorted(all_tars)]
-    split_parts_ratio = [("train", split[0]), ("val", split[1]), ("test", split[2])]
-    split_parts_patterns = None
-
-    # NOTE: generate .info.yaml and split.yaml
-    _ = BaseWebdatasetFactory.prepare_dataset(
-        path,
-        all_tars,
-        split_parts_ratio=split_parts_ratio,
-        split_parts_patterns=split_parts_patterns,
-        tar_index_only=False,
-        shuffle_seed=42 if shuffle_tars else None,
-        workers=num_workers,
-    )
-
-    # NOTE: dump dataset.yaml
-    metadata = {
-        "__class__": "ChatMLWebdataset",
-        "__module__": "tools.datasets.qwenvl.data.energon.chatml",
-        "field_map": {"imgs": "jpgs", "videos": "videos", "conversation": "json"},
-    }
-    with open(os.path.join(path.url, ".nv-meta", "dataset.yaml"), "w") as f:
-        yaml.safe_dump(metadata, f)
-
-
-if __name__ == "__main__":
-    argparser = ArgumentParser()
-    argparser.add_argument("--dataset-root", required=True, type=str)
-    argparser.add_argument("--output-root", required=True, type=str)
-    argparser.add_argument("--vision-root", default=None, type=str)
-    argparser.add_argument("--json", default="dataset.json", type=str)
-    argparser.add_argument(
-        "--images-key", default="images", type=str, help="The key for images in json"
-    )
-    argparser.add_argument(
-        "--videos-key", default="videos", type=str, help="The key for videos in json"
-    )
-    argparser.add_argument("--max-samples-per-tar", default=10000, type=float)
-    argparser.add_argument("--train-split", default=1, type=float)
-    argparser.add_argument("--val-split", default=0, type=float)
-    argparser.add_argument("--test-split", default=0, type=float)
-    argparser.add_argument("--shuffle-tars", action="store_true")
-    argparser.add_argument("--num-workers", default=1, type=int)
-    argparser.add_argument("--dp-size", default=1, type=int)
-    argparser.add_argument("--drop-last", action="store_true")
-    args = argparser.parse_args()
-    print(f"=======input args=======\n{args}\n=======input args=======\n")
-    output_dir = convert(
-        args.dataset_root,
-        args.output_root,
-        args.json,
-        max_count=args.max_samples_per_tar,
-        image_key=args.images_key,
-        video_key=args.videos_key,
-        vision_dir=args.vision_root,
-        dp_size=args.dp_size,
-        drop_last=args.drop_last,
-    )
-    print(f"Generating Configurations")
-    # NOTE: split_ratio: train/val/test
-    split = [args.train_split, args.val_split, args.test_split]
-    generate_configs(
-        EPath(output_dir), split, shuffle_tars=args.shuffle_tars, num_workers=args.num_workers
-    )
-    print(f"Configurations Generated")
diff --git a/tools/datasets/qwenvl/convert_llava_pretrain_to_wds.py b/tools/datasets/qwenvl/convert_llava_pretrain_to_wds.py
index 45f51ccb..04771688 100644
--- a/tools/datasets/qwenvl/convert_llava_pretrain_to_wds.py
+++ b/tools/datasets/qwenvl/convert_llava_pretrain_to_wds.py
@@ -1,39 +1,39 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/convert_llava_pretrain_to_wds.py
-import json
-import os
-import sys
-
-import webdataset as wds
-
-from tqdm import tqdm
-
-
-def convert(llava_pretrain_dir):
-
-    # Paths to the dataset files
-    json_file = os.path.join(llava_pretrain_dir, "blip_laion_cc_sbu_558k.json")
-    output = os.path.join(llava_pretrain_dir, "wds")
-
-    if not os.path.exists(output):
-        os.mkdir(output)
-
-    # Load data
-    with open(json_file, "r") as f:
-        data = json.load(f)
-
-    with wds.ShardWriter(os.path.join(output, "pretrain-%d.tar"), maxcount=10000) as shard_writer:
-        for entry in tqdm(data):
-            with open(os.path.join(llava_pretrain_dir, entry["image"]), "rb") as img_file:
-                image_data = img_file.read()
-            sample = {
-                "__key__": entry["id"],
-                "jpg": image_data,
-                "json": json.dumps(entry["conversations"]).encode("utf-8"),
-            }
-            shard_writer.write(sample)
-
-    print(f"Dataset successfully converted to wds")
-
-
-if __name__ == "__main__":
-    convert(sys.argv[1])
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/convert_llava_pretrain_to_wds.py
+import json
+import os
+import sys
+
+import webdataset as wds
+
+from tqdm import tqdm
+
+
+def convert(llava_pretrain_dir):
+
+    # Paths to the dataset files
+    json_file = os.path.join(llava_pretrain_dir, "blip_laion_cc_sbu_558k.json")
+    output = os.path.join(llava_pretrain_dir, "wds")
+
+    if not os.path.exists(output):
+        os.mkdir(output)
+
+    # Load data
+    with open(json_file, "r") as f:
+        data = json.load(f)
+
+    with wds.ShardWriter(os.path.join(output, "pretrain-%d.tar"), maxcount=10000) as shard_writer:
+        for entry in tqdm(data):
+            with open(os.path.join(llava_pretrain_dir, entry["image"]), "rb") as img_file:
+                image_data = img_file.read()
+            sample = {
+                "__key__": entry["id"],
+                "jpg": image_data,
+                "json": json.dumps(entry["conversations"]).encode("utf-8"),
+            }
+            shard_writer.write(sample)
+
+    print(f"Dataset successfully converted to wds")
+
+
+if __name__ == "__main__":
+    convert(sys.argv[1])
diff --git a/tools/datasets/qwenvl/data/dataset_helpers.py b/tools/datasets/qwenvl/data/dataset_helpers.py
index 93930ac9..0df0ac00 100644
--- a/tools/datasets/qwenvl/data/dataset_helpers.py
+++ b/tools/datasets/qwenvl/data/dataset_helpers.py
@@ -1,684 +1,535 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/dataset_helpers.py. Below is the original copyright:
-# Copyright (c) 2024 Alibaba PAI and Nvidia Megatron-LM Team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-import dataclasses
-import json
-import logging
-import math
-import os
-import re
-import sys
-import traceback
-
-from collections import defaultdict
-from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import numpy as np
-import PIL
-import torch
-
-from PIL import Image
-from torchvision import transforms as T
-
-from megatron.energon import Batch, DefaultTaskEncoder, VQASample
-from megatron.training import get_args
-from megatron.training.global_vars import get_tokenizer
-from tools.datasets.qwenvl.data.energon.chatml import ChatMLSample
-from tools.datasets.qwenvl.data.image_processing import get_visual_transform
-
-dataset_logger = logging.getLogger(__name__)
-FIRST_MAX_PADDING_FLAG = True
-IGNORE_IDX = -100
-MAX_IMG_THRESHHOLD = 5000
-
-
-# Type for intermediate batch, after batch()
-@dataclass
-class ImageTaskSample:
-    __key__: str
-    __subflavors__: Dict
-
-    imgs: List[np.ndarray]  # (c, h, w)
-    videos: List[np.ndarray]  # (c, h, w)
-
-    image_thw_grids: np.ndarray
-    video_thw_grids: np.ndarray
-    image_input_mask: np.ndarray
-    video_input_mask: np.ndarray
-    second_per_grid_ts: np.ndarray  # (n_videos, )
-
-    text: np.ndarray
-    target: np.ndarray
-
-
-# Typing for the resulting batch data after encode_batch()
-@dataclass
-class VQATaskBatch(Batch):
-    __keys__: List[str]
-    __subflavors__: List[Dict]
-    # (num_tiles, c, h, w)
-    imgs: torch.Tensor
-    videos: torch.Tensor
-    image_thw_grids: torch.Tensor
-    video_thw_grids: torch.Tensor
-    image_input_mask: torch.Tensor
-    video_input_mask: torch.Tensor
-    second_per_grid_ts: torch.Tensor  # (n_videos, ), read from metadata?
-
-    # (n, seq_len)
-    text: torch.Tensor
-    # (n, seq_len)
-    target: torch.Tensor
-
-
-class InternalWarning(Warning): ...
-
-
-def convert_to_qwen2vl_content(
-    user_input: str, image_pattern: str = "<image>", video_pattern: str = "<video>"
-):
-    """
-    Split user input into format Qwen2VL tokenizer accepts.
-    """
-    pattern = r"({image}|{video})".format(image=image_pattern, video=video_pattern)
-    contents = []
-    cur = 0
-    mm_idx = defaultdict(int)
-    for matched in re.finditer(pattern, user_input):
-        start, end = matched.span()
-        text = user_input[cur:start]
-        if text:
-            contents.append({"type": "text", "text": text})
-
-        contents.append(
-            {
-                "type": matched.string[start:end][1:-1],
-                matched.string[start:end][1:-1]: str(mm_idx[matched.string[start:end][1:-1]]),
-            }
-        )
-
-        cur = end
-        mm_idx[matched.string[start:end][1:-1]] += 1
-
-    if cur < len(user_input):
-        contents.append({"type": "text", "text": user_input[cur : len(user_input)]})
-
-    return contents
-
-
-class TaskEncoder(
-    DefaultTaskEncoder[Union[VQASample, ChatMLSample], ImageTaskSample, VQATaskBatch, dict]
-):
-    """A simple task encoder for captioning."""
-
-    def __init__(self):
-        # Specify the batch_type for default batching (batching is performed here "manually" by
-        # overwriting the `batch` method)
-        super().__init__()
-
-        self.args = get_args()
-        self.tp_size = self.args.tensor_model_parallel_size
-        self.cp_size = self.args.context_parallel_size
-        self.sequence_parallel = self.args.sequence_parallel
-
-        self.tokenizer = get_tokenizer()
-
-        self.temporal_patch_size = self.args.temporal_patch_size
-        self.merge_size = self.args.spatial_merge_size
-        self.patch_size = self.args.patch_size
-
-        self.seq_len = self.args.max_padding_length
-
-        self.vision_root = self.args.vision_root
-        assert self.vision_root is not None, "Please give the vision root."
-
-    def encode_sample(self, sample: Union[VQASample, ChatMLSample]):
-        if isinstance(sample, VQASample):
-            is_llava_training = (
-                sample.__subflavors__["is_llava_training"]
-                if "is_llava_training" in sample.__subflavors__
-                else False
-            )
-            if is_llava_training:
-                raise NotImplementedError("Sample format not supported")
-            else:
-                yield self.encode_vqa(sample)
-        elif isinstance(sample, ChatMLSample):
-            yield self.encode_chatml(sample)
-        else:
-            raise NotImplementedError("Sample format not supported")
-
-    def _flatten_visual_inputs(self, visuals, is_image: bool = True):
-        """
-        visuals: list of visual inputs, each input is a tensor of shape (c, h, w)
-        """
-        flattened = []
-        thw_grids = []
-        for visual in visuals:
-            if is_image:
-                resized_height, resized_width = visual.shape[-2:]
-                # temporal_patch_size = 2 If the image is a single frame, copy it to the temporal patch size
-                patches = np.tile(np.array(visual), (self.temporal_patch_size, 1, 1, 1))
-            else:
-                # videos
-                assert len(visual) % self.temporal_patch_size == 0
-                patches = np.array(visual)
-                resized_height, resized_width = patches.shape[-2:]
-
-            channel = patches.shape[1]
-            grid_t = patches.shape[0] // self.temporal_patch_size
-            grid_h, grid_w = (resized_height // self.patch_size, resized_width // self.patch_size)
-            patches = patches.reshape(
-                grid_t,
-                self.temporal_patch_size,
-                channel,
-                grid_h // self.merge_size,
-                self.merge_size,
-                self.patch_size,
-                grid_w // self.merge_size,
-                self.merge_size,
-                self.patch_size,
-            )
-            # grid_t, grid_h, grid_w = patches.shape[0], patches.shape[3], patches.shape[6], patches
-            patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)
-            flatten_patches = patches.reshape(
-                grid_t * grid_h * grid_w,
-                channel * self.temporal_patch_size * self.patch_size * self.patch_size,
-            )
-            flattened.append(flatten_patches)
-            thw_grids.append((grid_t, grid_h, grid_w))
-        return flattened, np.array(thw_grids)
-
-    # copy from
-    def _preprocess_image(
-        self, image: PIL.Image, image_max_pixels: int = 768 * 768, image_min_pixels: int = 32 * 32
-    ) -> PIL.Image:
-        r"""
-        Pre-processes a single image.
-        """
-        if (image.width * image.height) > image_max_pixels:
-            resize_factor = math.sqrt(image_max_pixels / (image.width * image.height))
-            width, height = int(image.width * resize_factor), int(image.height * resize_factor)
-            image = image.resize((width, height))
-
-        if (image.width * image.height) < image_min_pixels:
-            resize_factor = math.sqrt(image_min_pixels / (image.width * image.height))
-            width, height = int(image.width * resize_factor), int(image.height * resize_factor)
-            image = image.resize((width, height))
-
-        if image.mode != "RGB":
-            image = image.convert("RGB")
-
-        if min(image.width, image.height) < 28:
-            width, height = max(image.width, 28), max(image.height, 28)
-            image = image.resize((width, height), resample=Image.Resampling.NEAREST)
-
-        if image.width / image.height > 200:
-            width, height = image.height * 180, image.height
-            image = image.resize((width, height), resample=Image.Resampling.NEAREST)
-
-        if image.height / image.width > 200:
-            width, height = image.width, image.width * 180
-            image = image.resize((width, height), resample=Image.Resampling.NEAREST)
-
-        return image
-
-    def encode_chatml(self, sample: ChatMLSample):
-        # # TODO: modify get_visual_transform to add more augmentations
-        # imgs = [get_visual_transform(os.path.join(self.vision_root, img))[0] for img in sample.imgs]
-        # videos = [
-        #     [get_visual_transform(os.path.join(self.vision_root, frame))[0] for frame in video]
-        #     for video in sample.videos
-        # ]
-        # # NOTE: make n_frames even foreach video
-        # for i, video in enumerate(videos):
-        #     videos[i] = video[: len(video) // 2 * 2]
-
-        # # NOTE: flatten all images
-        # flattened_imgs, image_thw_grids = self._flatten_visual_inputs(imgs, is_image=True)
-        # flattened_videos, video_thw_grids = self._flatten_visual_inputs(videos, is_image=False)
-
-        #######################################################################################
-        # NOTE(lizhiyu): use the transformers processor
-        if sample.imgs is not None and len(sample.imgs) > 0:
-            imgs = []
-            for img in sample.imgs:
-                img_path = os.path.join(self.vision_root, img)
-                try:
-                    image = PIL.Image.open(img_path)
-                    image = self._preprocess_image(
-                        image=image,
-                        image_max_pixels=self.args.image_max_pixels,
-                        image_min_pixels=self.args.image_min_pixels,
-                    )
-                    imgs.append(image)
-                except Exception as e:
-                    raise ValueError(
-                        f"Failed to open image: {img_path}. Error: {e} of smaple[{sample.__key__}]"
-                    )
-                    # raise InternalWarning(
-                    #     f"Failed to open image: {img_path}. Error: {e} of smaple[{sample.__key__}]"
-                    # )
-            imgs_info = self.tokenizer.processor.image_processor(imgs, return_tensors="np")
-            flattened_imgs = imgs_info["pixel_values"]
-            image_thw_grids = imgs_info["image_grid_thw"]
-        else:
-            flattened_imgs = []
-            image_thw_grids = []
-
-        if sample.videos is not None and len(sample.videos) > 0:
-            videos = [
-                [PIL.Image.open(os.path.join(self.vision_root, frame)) for frame in video]
-                for video in sample.videos
-            ]
-            # NOTE: make n_frames even foreach video
-            for i, video in enumerate(videos):
-                videos[i] = video[: len(video) // 2 * 2]
-            videos_info = self.tokenizer.processor.image_processor(
-                images=None, videos=videos, return_tensors="pt"
-            )
-            flattened_videos = videos_info["pixel_values_videos"]
-            video_thw_grids = videos_info["video_grid_thw"]
-        else:
-            flattened_videos = []
-            video_thw_grids = []
-        #######################################################################################
-
-        # NOTE: generate qwen2vl conversations
-        conversation = (
-            json.loads(sample.conversation)
-            if isinstance(sample.conversation, (str, bytes))
-            else sample.conversation
-        )
-        second_per_grid_ts = [1 / 2.0] * len(video_thw_grids)
-        if "conversations" in conversation:
-            second_per_grid_ts = conversation.get("second_per_grid_ts", second_per_grid_ts)
-            second_per_grid_ts = [float(i) for i in second_per_grid_ts]
-            conversation = conversation["conversations"]
-
-        role_key = "from" if "from" in conversation[0] else "role"
-        content_key = "value" if "from" in conversation[0] else "content"
-
-        # NOTE: assume the conversation format is: [System]? (User Assistant)+
-        # convert text message to standand format
-        #  add system as first item, refercence: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/blob/main/chat_template.json
-        converted_conversation = []
-        if len(conversation) % 2 == 0:
-            # Default Prompt
-            converted_conversation.append(
-                {"role": "system", "content": "You are a helpful assistant."}
-            )
-        else:
-            dataset_logger.warning(
-                f"The sample [{sample.__key__}] has odd number of conversation turns, and we will use the first turn as system prompt. BUT this may be wrong. Pelase check the sample."
-            )
-            converted_conversation.append(
-                {"role": "system", "content": conversation[0][content_key]}
-            )
-            ## NOTE(lizhiyu): Force set system Prompt: "You are a helpful assistant."
-            # converted_conversation.append(
-            #     {"role": "system", "content": "You are a helpful assistant."}
-            # )
-            conversation = conversation[1:]
-
-        # add QA conversion as the left items
-        EXPECTED_ROLE = ["human", "gpt"]
-        for turn_idx, turn in enumerate(conversation):
-            role = turn[role_key]
-            if role != EXPECTED_ROLE[turn_idx % len(EXPECTED_ROLE)]:
-                raise InternalWarning(
-                    f"Expect conversation organized in order: [sys] human gpt human gpt..., but got role '{role}' in turn {turn_idx}"
-                )
-            content = turn[content_key]
-
-            if role == "human":
-                role = "user"
-                content = convert_to_qwen2vl_content(content)
-            elif role == "gpt":
-                role = "assistant"
-
-            converted_conversation.append({"role": role, "content": content})
-        conversation = converted_conversation
-
-        # NOTE: we need to mask all system/user input tokens and assistant generation prefix tokens
-        input_ids = self.tokenizer.apply_chat_template(
-            conversation, tokenize=True, return_tensors="np"
-        )[0]
-        target = input_ids.copy()
-
-        system_prompt_prefix = len(
-            self.tokenizer.apply_chat_template([conversation[0]], tokenize=True)
-        )
-        assistant_generation_prefix = 3  # <im_start>assistant\n
-        # pad_token_id = self.tokenizer.pad_token_id
-        # NOTE(lizhiyu): Align to llama-f
-        pad_token_id = IGNORE_IDX
-        target[:system_prompt_prefix] = pad_token_id
-        offset = system_prompt_prefix
-        for turn_idx, turn in enumerate(conversation[1:]):
-            turn_tokens = self.tokenizer.apply_chat_template(
-                [turn], tokenize=True, return_tensors="np"
-            )[0]
-            turn_content = turn_tokens[system_prompt_prefix:]
-            n_tokens = len(turn_content)
-            if (target[offset : offset + n_tokens] != turn_content).any():
-                raise InternalWarning("Encode Error")
-
-            if turn["role"] == "user":
-                target[offset : offset + n_tokens] = pad_token_id
-            elif turn["role"] == "assistant":
-                target[offset : offset + assistant_generation_prefix] = pad_token_id
-            offset += n_tokens
-        # current "target" don't pad vision token.
-
-        # NOTE: expand image_pad & video_pad
-        merge_length = self.merge_size**2  # 2**2 = 4
-        image_token_id, video_token_id = self.tokenizer.encode(["<|image_pad|>", "<|video_pad|>"])
-
-        # get the indices of the origin <|image_pad|> and <|video_pad|>
-        image_token_indices = np.where(input_ids == image_token_id)[0]
-        assert len(image_token_indices) == len(
-            image_thw_grids
-        ), f"With {len(image_thw_grids)} images in the sample, but {len(image_token_indices)} image placeholders!"
-        video_token_indices = np.where(input_ids == video_token_id)[0]
-        assert len(video_token_indices) == len(
-            video_thw_grids
-        ), f"With {len(video_thw_grids)} images in the sample, but {len(video_token_indices)} video placeholders!"
-        image_thw_grids, video_thw_grids = np.array(image_thw_grids, dtype=np.int64), np.array(
-            video_thw_grids, dtype=np.int64
-        )
-
-        # video_thw_grids shape: [n, 3]
-        # origin_seq_len + (all_image_token - 1) + (all_vision_token - 1)  ----> -1 because the pad token in origin text
-        target_length = (
-            input_ids.shape[0]
-            - image_thw_grids.shape[0]
-            + image_thw_grids.prod(axis=-1).sum() // merge_length
-            - video_thw_grids.shape[0]
-            + video_thw_grids.prod(axis=-1).sum() // merge_length
-        )
-        if target_length > self.seq_len:
-            # raise InternalWarning(f"Long sequence with length {target_length} found, dropped...")
-            dataset_logger.warning(
-                f"Samle id [{sample.__key__}] has long sequence with length {target_length}, cutoff to max [self.seq_len+64={self.seq_len}] in batch function..."
-            )
-        final_input_ids = np.zeros(target_length, dtype=input_ids.dtype)
-        final_input_masks = final_input_ids.copy()
-
-        image_idx, video_idx = 0, 0
-        indices = np.sort(np.concatenate([image_token_indices, video_token_indices]))
-
-        # cur_x: origin text token idx,  cur_y: final text token idx
-        cur_x, cur_y = 0, 0
-        for idx in indices:
-            token_id = input_ids[idx]
-            if token_id == image_token_id:
-                size = image_thw_grids[image_idx].prod() // merge_length
-                image_idx += 1
-            elif token_id == video_token_id:
-                size = video_thw_grids[video_idx].prod() // merge_length
-                video_idx += 1
-            # NOTE:
-            # input_ids[cur_x:idx] -> final_input_ids[cur_y:cur_y + idx - cur_x]
-            # input_ids[idx] -> final_input_ids[cur_y + idx - cur_x: cur_y + idx - cur_x + size]
-            final_input_ids[cur_y : cur_y + idx - cur_x] = input_ids[cur_x:idx]
-            final_input_masks[cur_y : cur_y + idx - cur_x] = target[cur_x:idx]
-            cur_y += idx - cur_x
-            final_input_ids[cur_y : cur_y + size] = token_id
-            final_input_masks[cur_y : cur_y + size] = pad_token_id
-            cur_y += size
-            cur_x = idx + 1
-
-        if cur_x < len(input_ids):
-            final_input_ids[cur_y:] = input_ids[cur_x:]
-            final_input_masks[cur_y:] = target[cur_x:]
-
-        target = np.roll(final_input_masks, shift=-1)
-        target[-1] = pad_token_id
-
-        # NOTE(lizhiyu): we also check it in the train scripts.
-        if (target == pad_token_id).all():
-            raise InternalWarning(
-                f"Sample id [{sample.__key__}] with all masked label, the data is invalid! Dropped!"
-            )
-
-        image_input_mask = final_input_ids == self.tokenizer.image_token_id
-        video_input_mask = final_input_ids == self.tokenizer.video_token_id
-
-        # collect data
-        return ImageTaskSample(
-            __key__=sample.__key__,
-            __subflavors__=sample.__subflavors__,
-            imgs=flattened_imgs,
-            videos=flattened_videos,
-            image_thw_grids=image_thw_grids,
-            video_thw_grids=video_thw_grids,
-            second_per_grid_ts=np.array(second_per_grid_ts, dtype=np.float32),
-            image_input_mask=image_input_mask,
-            video_input_mask=video_input_mask,
-            text=final_input_ids,
-            target=target,
-        )
-
-    def encode_vqa(self, sample: VQASample):
-        augment = (
-            sample.__subflavors__["augmentation"]
-            if "augmentation" in sample.__subflavors__
-            else False
-        )
-        has_video = (
-            sample.__subflavors__["has_video"] if "has_video" in sample.__subflavors__ else False
-        )
-
-        if has_video:
-            raise NotImplementedError("You should use sharegpt dataset to train with videos.")
-        else:
-            # TODO: add args
-            imgs = get_visual_transform(sample.image)
-            flatten_patches, thw_grids = self._flatten_visual_inputs(imgs, is_image=True)
-
-        assert "<image>" in sample.context  # ?
-        # NOTE: we expect a context is a string with <image> conetnt
-        if isinstance(sample.answers, list):
-            answer_list = sample.answers
-            weight_list = np.array(sample.answer_weights).astype(np.float32)
-            weight_list = weight_list / np.sum(weight_list)
-            answer_idx = np.random.choice(weight_list.shape[0], 1, p=weight_list)[0]
-            answer = answer_list[answer_idx]
-        else:
-            answer = sample.answers
-
-        conversation = [
-            {"role": "user", "content": convert_to_qwen2vl_content(sample.context)},
-            {"role": "assistant", "content": answer},
-        ]
-
-        user_inputs = self.tokenizer.apply_chat_template(conversation[:-1], tokenize=False)
-        text = self.tokenizer.apply_chat_template(conversation, tokenize=False)
-
-        # text, target = self.tokenizer.tokenize_conversation(conversation, False, False)
-        # replace <image> token by <image> * (thw)
-        merge_length = self.merge_size**2
-        image_token = "<|image_pad|>"
-        assert len(thw_grids) == 1, "Only one image per sample is supported!"
-        index = 0
-        while image_token in text:
-            grid_t, grid_h, grid_w = thw_grids[index]
-            l = grid_t * grid_h * grid_w
-            text = text.replace(image_token, "<|placeholder|>" * (l // merge_length), 1)
-            user_inputs = user_inputs.replace(
-                image_token, "<|placeholder|>" * (l // merge_length), 1
-            )
-            index += 1
-        text = text.replace("<|placeholder|>", image_token)
-        user_inputs = user_inputs.replace("<|placeholder|>", image_token)
-
-        input_ids = self.tokenizer.tokenize(text)
-        user_input_ids = self.tokenizer.tokenize(user_inputs)
-        if len(input_ids) > self.seq_len:
-            raise InternalWarning(f"Long sequence with length {len(input_ids)} found, dropped...")
-
-        target = np.array(input_ids[1:] + [IGNORE_IDX])
-        if len(user_input_ids) >= len(input_ids):
-            raise InternalWarning(f"Sample not supported, dropped...")
-        # ensure user inputs is a prefix of full text
-        if not (np.array(user_input_ids) == np.array(input_ids[: len(user_input_ids)])).all():
-            raise InternalWarning(f"Sample not supported, dropped...")
-        # mask input
-        target[: len(user_input_ids) - 1] = IGNORE_IDX
-
-        img_token_id = self.tokenizer.image_token_id
-        image_input_mask = np.array(input_ids) == img_token_id
-
-        # collect data
-        return ImageTaskSample(
-            __key__=sample.__key__,
-            __subflavors__=sample.__subflavors__,
-            imgs=flatten_patches,
-            videos=list(),
-            image_thw_grids=thw_grids,
-            video_thw_grids=torch.empty([0, 3], dtype=torch.long),
-            image_input_mask=image_input_mask,
-            video_input_mask=None,
-            second_per_grid_ts=np.zeros(0, dtype=np.float32),
-            text=input_ids,
-            target=target,
-        )
-
-    def batch(self, samples: List[ImageTaskSample]) -> VQATaskBatch:
-        # Stack images to [num_tiles, c, h, w]. If there are no images (text-only), then use a dummy image.
-        # imgs = [img for s in samples for img in s.imgs]
-
-        ####################################################
-        # NOTE(lizhiyu): use the transformers processor
-        imgs = [s.imgs for s in samples if isinstance(s.imgs, np.ndarray) and s.imgs.size > 0]
-        ####################################################
-        if len(imgs) > 0:
-            imgs = torch.cat([torch.from_numpy(img) for img in imgs])
-        else:
-            imgs = torch.empty(
-                [0, 3 * self.temporal_patch_size * self.patch_size * self.patch_size],
-                dtype=torch.float32,
-            )
-
-        image_thw_grids = [thw_grids for s in samples for thw_grids in s.image_thw_grids]
-        if len(image_thw_grids) > 0:
-            image_thw_grids = torch.from_numpy(np.array(image_thw_grids)).long()
-            assert image_thw_grids.prod(dim=-1).sum() == imgs.shape[0]
-        else:
-            image_thw_grids = torch.empty([0, 3], dtype=torch.long)
-
-        # Stack videos to [num_tiles, c, h, w]. If there are no videos (text-only), then use a dummy video.
-        # videos = [video for s in samples for video in s.videos]
-
-        ####################################################
-        # NOTE(lizhiyu): use the transformers processor
-        videos = [
-            s.videos for s in samples if isinstance(s.videos, np.ndarray) and s.videos.size > 0
-        ]
-        ####################################################
-        if len(videos) > 0:
-            videos = torch.cat([torch.from_numpy(video) for video in videos])
-        else:
-            videos = torch.empty(
-                [0, 3 * self.temporal_patch_size * self.patch_size * self.patch_size],
-                dtype=torch.float32,
-            )
-
-        second_per_grid_ts = [
-            second_per_grid for s in samples for second_per_grid in s.second_per_grid_ts
-        ]
-        if len(second_per_grid_ts) > 0:
-            second_per_grid_ts = torch.from_numpy(np.array(second_per_grid_ts)).float()
-        else:
-            second_per_grid_ts = torch.empty([0], dtype=torch.float32)
-
-        video_thw_grids = [thw_grids for s in samples for thw_grids in s.video_thw_grids]
-        if len(video_thw_grids) > 0:
-            video_thw_grids = torch.from_numpy(np.array(video_thw_grids)).long()
-            assert video_thw_grids.prod(dim=-1).sum() == videos.shape[0]
-        else:
-            video_thw_grids = torch.empty([0, 3], dtype=torch.long)
-
-        global FIRST_MAX_PADDING_FLAG, MAX_IMG_THRESHHOLD
-        # NOTE(lizhiyu): Clear the cache only when the current image length is longer than the past maxisum length.
-        if image_thw_grids.prod(axis=-1).sum() // 4 > MAX_IMG_THRESHHOLD:
-            MAX_IMG_THRESHHOLD = image_thw_grids.prod(axis=-1).sum() // 4
-            FIRST_MAX_PADDING_FLAG = True
-
-        if not self.args.enable_variable_seq_lengths:
-            max_seq_len = self.seq_len
-        else:
-            # NOTE: this is a hack to get the max padding length for the first batch to avoid OOM because of cached memory in torch
-            if FIRST_MAX_PADDING_FLAG:
-                max_seq_len = self.seq_len
-                FIRST_MAX_PADDING_FLAG = False
-            else:
-                max_seq_len = max(len(s.text) for s in samples)
-                max_seq_len = min(max_seq_len, self.seq_len)
-        # NOTE: we need to make sure the max_seq_len is divisible by tp_size * cp_size
-        if self.cp_size > 1 or self.sequence_parallel:
-            max_seq_len = math.ceil(max_seq_len / (self.tp_size * self.cp_size)) * (
-                self.tp_size * self.cp_size
-            )
-        text_mat = np.full((len(samples), max_seq_len), self.tokenizer.pad_token_id, dtype=np.int64)
-        # +1 to accommodate shift to left by one later.
-        target_mat = np.full((len(samples), max_seq_len), IGNORE_IDX, dtype=np.int64)
-
-        image_input_masks = np.zeros_like(text_mat, dtype=bool)
-        video_input_masks = np.zeros_like(text_mat, dtype=bool)
-        for i, s in enumerate(samples):
-            # If the sample/target length exceeds the target sequence length, then truncate.
-            text_len = min(max_seq_len, len(s.text))
-            target_len = min(max_seq_len, len(s.target))
-
-            text_mat[i, :text_len] = np.array(s.text)[:text_len]
-            # NOTE: we should assert user input sequence will not be truncated
-            if s.image_input_mask is not None:
-                image_input_masks[i, :text_len] = np.array(s.image_input_mask)[:text_len]
-            if s.video_input_mask is not None:
-                video_input_masks[i, :text_len] = np.array(s.video_input_mask)[:text_len]
-            target_mat[i, :target_len] = np.array(s.target)[:target_len]
-
-        batch = VQATaskBatch(
-            __keys__=[s.__key__ for s in samples],
-            __subflavors__=[s.__subflavors__ for s in samples],
-            imgs=imgs,
-            videos=videos,
-            image_thw_grids=image_thw_grids,
-            video_thw_grids=video_thw_grids,
-            second_per_grid_ts=second_per_grid_ts,
-            image_input_mask=torch.from_numpy(image_input_masks),
-            video_input_mask=torch.from_numpy(video_input_masks),
-            text=torch.from_numpy(text_mat),
-            target=torch.from_numpy(target_mat),
-        )
-
-        return batch
-
-    def encode_batch(self, batch: VQATaskBatch) -> dict:
-        raw = dataclasses.asdict(batch)
-        del raw["__subflavors__"]
-        return raw
-
-
-def print_error_handler(exc: Exception, key: Optional[str], debug=False):
-    if not debug and isinstance(exc, InternalWarning):
-        return
-    print(
-        f"The following exception occurred in the dataloader for sample {key} and is skipped",
-        file=sys.stderr,
-    )
-    traceback.print_exc()
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/dataset_helpers.py. Below is the original copyright:
+# Copyright (c) 2024 Alibaba PAI and Nvidia Megatron-LM Team.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import dataclasses
+import json
+import re
+import sys
+import traceback
+
+from collections import defaultdict
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import numpy as np
+import torch
+
+from torchvision import transforms as T
+
+from megatron.energon import Batch, DefaultTaskEncoder, VQASample
+from megatron.training import get_args
+from megatron.training.global_vars import get_tokenizer
+from tools.datasets.qwenvl.data.energon.chatml import ChatMLSample
+from tools.datasets.qwenvl.data.image_processing import get_visual_transform
+
+
+# Type for intermediate batch, after batch()
+@dataclass
+class ImageTaskSample:
+    __key__: str
+    __subflavors__: Dict
+
+    imgs: List[np.ndarray]  # (c, h, w)
+    videos: List[np.ndarray]  # (c, h, w)
+
+    image_thw_grids: np.ndarray
+    video_thw_grids: np.ndarray
+    image_input_mask: np.ndarray
+    video_input_mask: np.ndarray
+    second_per_grid_ts: np.ndarray  # (n_videos, )
+
+    text: np.ndarray
+    target: np.ndarray
+
+
+# Typing for the resulting batch data after encode_batch()
+@dataclass
+class VQATaskBatch(Batch):
+    __keys__: List[str]
+    __subflavors__: List[Dict]
+    # (num_tiles, c, h, w)
+    imgs: torch.Tensor
+    videos: torch.Tensor
+    image_thw_grids: torch.Tensor
+    video_thw_grids: torch.Tensor
+    image_input_mask: torch.Tensor
+    video_input_mask: torch.Tensor
+    second_per_grid_ts: torch.Tensor  # (n_videos, ), read from metadata?
+
+    # (n, seq_len)
+    text: torch.Tensor
+    # (n, seq_len)
+    target: torch.Tensor
+
+
+class InternalWarning(Warning): ...
+
+
+def convert_to_qwen2vl_content(
+    user_input: str, image_pattern: str = "<image>", video_pattern: str = "<video>"
+):
+    """
+    Split user input into format Qwen2VL tokenizer accepts.
+    """
+    pattern = r"({image}|{video})".format(image=image_pattern, video=video_pattern)
+    contents = []
+    cur = 0
+    mm_idx = defaultdict(int)
+    for matched in re.finditer(pattern, user_input):
+        start, end = matched.span()
+        if start > cur:
+            contents.append({"type": "text", "text": user_input[cur:start].strip()})
+
+        contents.append(
+            {
+                "type": matched.string[start:end][1:-1],
+                matched.string[start:end][1:-1]: str(mm_idx[matched.string[start:end][1:-1]]),
+            }
+        )
+
+        cur = end
+        mm_idx[matched.string[start:end][1:-1]] += 1
+
+    if cur < len(user_input):
+        contents.append({"type": "text", "text": user_input[cur : len(user_input)].strip()})
+
+    return contents
+
+
+class TaskEncoder(
+    DefaultTaskEncoder[Union[VQASample, ChatMLSample], ImageTaskSample, VQATaskBatch, dict]
+):
+    """A simple task encoder for captioning."""
+
+    def __init__(self):
+        # Specify the batch_type for default batching (batching is performed here "manually" by
+        # overwriting the `batch` method)
+        super().__init__()
+
+        self.args = get_args()
+
+        self.tokenizer = get_tokenizer()
+
+        self.temporal_patch_size = self.args.temporal_patch_size
+        self.merge_size = self.args.spatial_merge_size
+        self.patch_size = self.args.patch_size
+
+        self.seq_len = self.args.max_padding_length
+
+    def encode_sample(self, sample: Union[VQASample, ChatMLSample]):
+        if isinstance(sample, VQASample):
+            is_llava_training = (
+                sample.__subflavors__["is_llava_training"]
+                if "is_llava_training" in sample.__subflavors__
+                else False
+            )
+            if is_llava_training:
+                raise NotImplementedError("Sample format not supported")
+            else:
+                yield self.encode_vqa(sample)
+        elif isinstance(sample, ChatMLSample):
+            yield self.encode_chatml(sample)
+        else:
+            raise NotImplementedError("Sample format not supported")
+
+    def _flatten_visual_inputs(self, visuals, is_image: bool = True):
+        """
+        visuals: list of visual inputs, each input is a tensor of shape (c, h, w)
+        """
+        flattened = []
+        thw_grids = []
+        for visual in visuals:
+            if is_image:
+                resized_height, resized_width = visual.shape[-2:]
+                # temporal_patch_size = 2 If the image is a single frame, copy it to the temporal patch size
+                patches = np.tile(np.array(visual), (self.temporal_patch_size, 1, 1, 1))
+            else:
+                # videos
+                assert len(visual) % self.temporal_patch_size == 0
+                patches = np.array(visual)
+                resized_height, resized_width = patches.shape[-2:]
+
+            channel = patches.shape[1]
+            grid_t = patches.shape[0] // self.temporal_patch_size
+            grid_h, grid_w = (resized_height // self.patch_size, resized_width // self.patch_size)
+            patches = patches.reshape(
+                grid_t,
+                self.temporal_patch_size,
+                channel,
+                grid_h // self.merge_size,
+                self.merge_size,
+                self.patch_size,
+                grid_w // self.merge_size,
+                self.merge_size,
+                self.patch_size,
+            )
+            # grid_t, grid_h, grid_w = patches.shape[0], patches.shape[3], patches.shape[6], patches
+            patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)
+            flatten_patches = patches.reshape(
+                grid_t * grid_h * grid_w,
+                channel * self.temporal_patch_size * self.patch_size * self.patch_size,
+            )
+            flattened.append(flatten_patches)
+            thw_grids.append((grid_t, grid_h, grid_w))
+        return flattened, np.array(thw_grids)
+
+    def encode_chatml(self, sample: ChatMLSample):
+        # TODO: modify get_visual_transform to add more augmentations
+        imgs = [get_visual_transform(img)[0] for img in sample.imgs]
+        videos = [[get_visual_transform(frame)[0] for frame in video] for video in sample.videos]
+        # NOTE: make n_frames even foreach video
+        for i, video in enumerate(videos):
+            videos[i] = video[: len(video) // 2 * 2]
+
+        # NOTE: flatten all images
+        flattened_imgs, image_thw_grids = self._flatten_visual_inputs(imgs, is_image=True)
+        flattened_videos, video_thw_grids = self._flatten_visual_inputs(videos, is_image=False)
+
+        # NOTE: generate qwen2vl conversations
+        conversation = (
+            json.loads(sample.conversation)
+            if isinstance(sample.conversation, (str, bytes))
+            else sample.conversation
+        )
+        second_per_grid_ts = [1 / 2.0] * len(video_thw_grids)
+        if "conversations" in conversation:
+            second_per_grid_ts = conversation.get("second_per_grid_ts", second_per_grid_ts)
+            second_per_grid_ts = [float(i) for i in second_per_grid_ts]
+            conversation = conversation["conversations"]
+
+        role_key = "from" if "from" in conversation[0] else "role"
+        content_key = "value" if "from" in conversation[0] else "content"
+
+        # NOTE: assume the conversation format is: [System]? (User Assistant)+
+        converted_conversation = []
+        if len(conversation) % 2 == 0:
+            # Default Prompt
+            converted_conversation.append(
+                {"role": "system", "content": "You are a helpful assistant."}
+            )
+        else:
+            converted_conversation.append(
+                {"role": "system", "content": conversation[0][content_key]}
+            )
+            conversation = conversation[1:]
+
+        EXPECTED_ROLE = ["human", "gpt"]
+        for turn_idx, turn in enumerate(conversation):
+            role = turn[role_key]
+            if role != EXPECTED_ROLE[turn_idx % len(EXPECTED_ROLE)]:
+                raise InternalWarning(
+                    f"Expect conversation organized in order: [sys] human gpt human gpt..., but got role '{role}' in turn {turn_idx}"
+                )
+            content = turn[content_key]
+
+            if role == "human":
+                role = "user"
+                content = convert_to_qwen2vl_content(content)
+            elif role == "gpt":
+                role = "assistant"
+
+            converted_conversation.append({"role": role, "content": content})
+        conversation = converted_conversation
+
+        # NOTE: we need to mask all system/user input tokens and assistant generation prefix tokens
+        input_ids = self.tokenizer.apply_chat_template(
+            conversation, tokenize=True, return_tensors="np"
+        )[0]
+        target = input_ids.copy()
+
+        system_prompt_prefix = len(
+            self.tokenizer.apply_chat_template([conversation[0]], tokenize=True)
+        )
+        assistant_generation_prefix = 3
+        pad_token_id = self.tokenizer.pad_token_id
+
+        target[:system_prompt_prefix] = pad_token_id
+        offset = system_prompt_prefix
+        for turn_idx, turn in enumerate(conversation[1:]):
+            turn_tokens = self.tokenizer.apply_chat_template(
+                [turn], tokenize=True, return_tensors="np"
+            )[0]
+            turn_content = turn_tokens[system_prompt_prefix:]
+            n_tokens = len(turn_content)
+            if (target[offset : offset + n_tokens] != turn_content).any():
+                raise InternalWarning("Encode Error")
+
+            if turn["role"] == "user":
+                target[offset : offset + n_tokens] = pad_token_id
+            elif turn["role"] == "assistant":
+                target[offset : offset + assistant_generation_prefix] = pad_token_id
+            offset += n_tokens
+
+        # NOTE: expand image_pad & video_pad
+        merge_length = self.merge_size**2
+        image_token_id, video_token_id = self.tokenizer.encode(["<|image_pad|>", "<|video_pad|>"])
+
+        image_token_indices = np.where(input_ids == image_token_id)[0]
+        assert len(image_token_indices) == len(
+            image_thw_grids
+        ), f"With {len(image_thw_grids)} images in the sample, but {len(image_token_indices)} image placeholders!"
+        video_token_indices = np.where(input_ids == video_token_id)[0]
+        assert len(video_token_indices) == len(
+            video_thw_grids
+        ), f"With {len(video_thw_grids)} images in the sample, but {len(video_token_indices)} video placeholders!"
+        image_thw_grids, video_thw_grids = np.array(image_thw_grids, dtype=np.int64), np.array(
+            video_thw_grids, dtype=np.int64
+        )
+
+        target_length = (
+            input_ids.shape[0]
+            - image_thw_grids.shape[0]
+            + image_thw_grids.prod(axis=-1).sum() // merge_length
+            - video_thw_grids.shape[0]
+            + video_thw_grids.prod(axis=-1).sum() // merge_length
+        )
+        if target_length > self.seq_len:
+            raise InternalWarning(f"Long sequence with length {target_length} found, dropped...")
+        final_input_ids = np.zeros(target_length, dtype=input_ids.dtype)
+        final_input_masks = final_input_ids.copy()
+
+        image_idx, video_idx = 0, 0
+        indices = np.sort(np.concatenate([image_token_indices, video_token_indices]))
+
+        cur_x, cur_y = 0, 0
+        for idx in indices:
+            token_id = input_ids[idx]
+            if token_id == image_token_id:
+                size = image_thw_grids[image_idx].prod() // merge_length
+                image_idx += 1
+            elif token_id == video_token_id:
+                size = video_thw_grids[video_idx].prod() // merge_length
+                video_idx += 1
+            # NOTE:
+            # input_ids[cur_x:idx] -> final_input_ids[cur_y:cur_y + idx - cur_x]
+            # input_ids[idx] -> final_input_ids[cur_y + idx - cur_x: cur_y + idx - cur_x + size]
+            final_input_ids[cur_y : cur_y + idx - cur_x] = input_ids[cur_x:idx]
+            final_input_masks[cur_y : cur_y + idx - cur_x] = target[cur_x:idx]
+            cur_y += idx - cur_x
+            final_input_ids[cur_y : cur_y + size] = token_id
+            final_input_masks[cur_y : cur_y + size] = pad_token_id
+            cur_y += size
+            cur_x = idx + 1
+
+        if cur_x < len(input_ids):
+            final_input_ids[cur_y:] = input_ids[cur_x:]
+            final_input_masks[cur_y:] = target[cur_x:]
+
+        target = np.roll(final_input_masks, shift=-1)
+        target[-1] = pad_token_id
+
+        if (target == pad_token_id).all():
+            raise InternalWarning("Sample with all masked label, dropped.")
+
+        image_input_mask = final_input_ids == self.tokenizer.image_token_id
+        video_input_mask = final_input_ids == self.tokenizer.video_token_id
+        # collect data
+        return ImageTaskSample(
+            __key__=sample.__key__,
+            __subflavors__=sample.__subflavors__,
+            imgs=flattened_imgs,
+            videos=flattened_videos,
+            image_thw_grids=image_thw_grids,
+            video_thw_grids=video_thw_grids,
+            second_per_grid_ts=np.array(second_per_grid_ts, dtype=np.float32),
+            image_input_mask=image_input_mask,
+            video_input_mask=video_input_mask,
+            text=final_input_ids,
+            target=target,
+        )
+
+    def encode_vqa(self, sample: VQASample):
+        augment = (
+            sample.__subflavors__["augmentation"]
+            if "augmentation" in sample.__subflavors__
+            else False
+        )
+        has_video = (
+            sample.__subflavors__["has_video"] if "has_video" in sample.__subflavors__ else False
+        )
+
+        if has_video:
+            raise NotImplementedError("You should use sharegpt dataset to train with videos.")
+        else:
+            # TODO: add args
+            imgs = get_visual_transform(sample.image)
+            flatten_patches, thw_grids = self._flatten_visual_inputs(imgs, is_image=True)
+
+        assert "<image>" in sample.context  # ?
+        # NOTE: we expect a context is a string with <image> conetnt
+        if isinstance(sample.answers, list):
+            answer_list = sample.answers
+            weight_list = np.array(sample.answer_weights).astype(np.float32)
+            weight_list = weight_list / np.sum(weight_list)
+            answer_idx = np.random.choice(weight_list.shape[0], 1, p=weight_list)[0]
+            answer = answer_list[answer_idx]
+        else:
+            answer = sample.answers
+
+        conversation = [
+            {"role": "user", "content": convert_to_qwen2vl_content(sample.context)},
+            {"role": "assistant", "content": answer},
+        ]
+
+        user_inputs = self.tokenizer.apply_chat_template(conversation[:-1], tokenize=False)
+        text = self.tokenizer.apply_chat_template(conversation, tokenize=False)
+
+        # text, target = self.tokenizer.tokenize_conversation(conversation, False, False)
+        # replace <image> token by <image> * (thw)
+        merge_length = self.merge_size**2
+        image_token = "<|image_pad|>"
+        assert len(thw_grids) == 1, "Only one image per sample is supported!"
+        index = 0
+        while image_token in text:
+            grid_t, grid_h, grid_w = thw_grids[index]
+            l = grid_t * grid_h * grid_w
+            text = text.replace(image_token, "<|placeholder|>" * (l // merge_length), 1)
+            user_inputs = user_inputs.replace(
+                image_token, "<|placeholder|>" * (l // merge_length), 1
+            )
+            index += 1
+        text = text.replace("<|placeholder|>", image_token)
+        user_inputs = user_inputs.replace("<|placeholder|>", image_token)
+
+        input_ids = self.tokenizer.tokenize(text)
+        user_input_ids = self.tokenizer.tokenize(user_inputs)
+        if len(input_ids) > self.seq_len:
+            raise InternalWarning(f"Long sequence with length {len(input_ids)} found, dropped...")
+
+        target = np.array(input_ids[1:] + [self.tokenizer.pad_token_id])
+        if len(user_input_ids) >= len(input_ids):
+            raise InternalWarning(f"Sample not supported, dropped...")
+        # ensure user inputs is a prefix of full text
+        if not (np.array(user_input_ids) == np.array(input_ids[: len(user_input_ids)])).all():
+            raise InternalWarning(f"Sample not supported, dropped...")
+        # mask input
+        target[: len(user_input_ids) - 1] = self.tokenizer.pad_token_id
+
+        img_token_id = self.tokenizer.image_token_id
+        image_input_mask = np.array(input_ids) == img_token_id
+
+        # collect data
+        return ImageTaskSample(
+            __key__=sample.__key__,
+            __subflavors__=sample.__subflavors__,
+            imgs=flatten_patches,
+            videos=list(),
+            image_thw_grids=thw_grids,
+            video_thw_grids=torch.empty([0, 3], dtype=torch.long),
+            image_input_mask=image_input_mask,
+            video_input_mask=None,
+            second_per_grid_ts=np.zeros(0, dtype=np.float32),
+            text=input_ids,
+            target=target,
+        )
+
+    def batch(self, samples: List[ImageTaskSample]) -> VQATaskBatch:
+        # Stack images to [num_tiles, c, h, w]. If there are no images (text-only), then use a dummy image.
+        imgs = [img for s in samples for img in s.imgs]
+        if len(imgs) > 0:
+            imgs = torch.cat([torch.from_numpy(img) for img in imgs])
+        else:
+            imgs = torch.empty(
+                [0, 3 * self.temporal_patch_size * self.patch_size * self.patch_size],
+                dtype=torch.float32,
+            )
+
+        image_thw_grids = [thw_grids for s in samples for thw_grids in s.image_thw_grids]
+        if len(image_thw_grids) > 0:
+            image_thw_grids = torch.from_numpy(np.array(image_thw_grids)).long()
+            assert image_thw_grids.prod(dim=-1).sum() == imgs.shape[0]
+        else:
+            image_thw_grids = torch.empty([0, 3], dtype=torch.long)
+
+        # Stack videos to [num_tiles, c, h, w]. If there are no videos (text-only), then use a dummy video.
+        videos = [video for s in samples for video in s.videos]
+        if len(videos) > 0:
+            videos = torch.cat([torch.from_numpy(video) for video in videos])
+        else:
+            videos = torch.empty(
+                [0, 3 * self.temporal_patch_size * self.patch_size * self.patch_size],
+                dtype=torch.float32,
+            )
+
+        second_per_grid_ts = [
+            second_per_grid for s in samples for second_per_grid in s.second_per_grid_ts
+        ]
+        if len(second_per_grid_ts) > 0:
+            second_per_grid_ts = torch.from_numpy(np.array(second_per_grid_ts)).float()
+        else:
+            second_per_grid_ts = torch.empty([0], dtype=torch.float32)
+
+        video_thw_grids = [thw_grids for s in samples for thw_grids in s.video_thw_grids]
+        if len(video_thw_grids) > 0:
+            video_thw_grids = torch.from_numpy(np.array(video_thw_grids)).long()
+            assert video_thw_grids.prod(dim=-1).sum() == videos.shape[0]
+        else:
+            video_thw_grids = torch.empty([0, 3], dtype=torch.long)
+
+        # If the user hasn't defined a target sequence length, then use the max along the sample lengths.
+        max_seq_len = self.seq_len
+        if not max_seq_len:
+            max_seq_len = max(len(s.text) for s in samples)
+
+        text_mat = np.full((len(samples), max_seq_len), self.tokenizer.pad_token_id, dtype=np.int64)
+        # +1 to accommodate shift to left by one later.
+        target_mat = np.full(
+            (len(samples), max_seq_len), self.tokenizer.pad_token_id, dtype=np.int64
+        )
+
+        image_input_masks = np.zeros_like(text_mat, dtype=bool)
+        video_input_masks = np.zeros_like(text_mat, dtype=bool)
+        for i, s in enumerate(samples):
+            # If the sample/target length exceeds the target sequence length, then truncate.
+            text_len = min(max_seq_len, len(s.text))
+            target_len = min(max_seq_len, len(s.target))
+
+            text_mat[i, :text_len] = np.array(s.text)[:text_len]
+            # NOTE: we should assert user input sequence will not be truncated
+            if s.image_input_mask is not None:
+                image_input_masks[i, :text_len] = np.array(s.image_input_mask)[:text_len]
+            if s.video_input_mask is not None:
+                video_input_masks[i, :text_len] = np.array(s.video_input_mask)[:text_len]
+            target_mat[i, :target_len] = np.array(s.target)[:target_len]
+
+        batch = VQATaskBatch(
+            __keys__=[s.__key__ for s in samples],
+            __subflavors__=[s.__subflavors__ for s in samples],
+            imgs=imgs,
+            videos=videos,
+            image_thw_grids=image_thw_grids,
+            video_thw_grids=video_thw_grids,
+            second_per_grid_ts=second_per_grid_ts,
+            image_input_mask=torch.from_numpy(image_input_masks),
+            video_input_mask=torch.from_numpy(video_input_masks),
+            text=torch.from_numpy(text_mat),
+            target=torch.from_numpy(target_mat),
+        )
+
+        return batch
+
+    def encode_batch(self, batch: VQATaskBatch) -> dict:
+        raw = dataclasses.asdict(batch)
+        del raw["__subflavors__"]
+        return raw
+
+
+def print_error_handler(exc: Exception, key: Optional[str], debug=False):
+    if not debug and isinstance(exc, InternalWarning):
+        return
+    print(
+        f"The following exception occurred in the dataloader for sample {key} and is skipped",
+        file=sys.stderr,
+    )
+    traceback.print_exc()
diff --git a/tools/datasets/qwenvl/data/energon/chatml.py b/tools/datasets/qwenvl/data/energon/chatml.py
index 7c71e91e..3f4a2cf1 100644
--- a/tools/datasets/qwenvl/data/energon/chatml.py
+++ b/tools/datasets/qwenvl/data/energon/chatml.py
@@ -1,77 +1,68 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/energon/chatml.py.
-
-import pickle
-import re
-import warnings
-
-from dataclasses import dataclass
-from typing import List, Union
-
-import torch
-
-from webdataset.autodecode import Decoder
-
-from megatron.energon.epathlib.epath import EPath
-from megatron.energon.flavors.base_dataset import Sample
-from megatron.energon.flavors.webdataset import DefaultDecoderWebdatasetFactory
-
-
-@dataclass
-class ChatMLSample(Sample):
-    """multi-turn complex samples with images and videos"""
-
-    imgs: List[str]
-    videos: List[List[str]]
-    conversation: str  # JSON string of GPT-format conversations
-
-
-class NestedImagesPathHandler:
-    def __init__(self, imagespec):
-        """Create an image handler.
-
-        :param imagespec: short string indicating the type of decoding
-        """
-        self.extensions = ["jpgs", "videos"]
-        self.extensions_mapping = {"jpgs": "jpg", "videos": "jpg"}
-
-    def __call__(self, key, data):
-        """Perform nested image decoding.
-
-        :param key: file name extension
-        :param data: binary data
-        """
-        extension = re.sub(r".*[.]", "", key)
-        if extension.lower() not in self.extensions:
-            return None
-        data = pickle.loads(data)
-        return data
-
-
-# During training, data is automatically decoded to from default webdataset to 'ChatMLSample' when loaded using energon-dataloader,
-# and this is not done during preparation!!!
-# After decoding, the data is passed into the TaskEncoder for further processing.
-class ChatMLWebdataset(DefaultDecoderWebdatasetFactory[ChatMLSample]):
-    __sample_type__ = ChatMLSample
-
-    def __init__(
-        self,
-        path: EPath,
-        *,
-        auto_decode: bool = True,
-        image_decode="torchrgb",
-        ignore_decoder_errors: bool = False,
-        av_decode="AVDecoder",
-        video_decode_audio: bool = False,
-        **kwargs,
-    ):
-        super().__init__(
-            path,
-            auto_decode=auto_decode,
-            image_decode=image_decode,
-            ignore_decoder_errors=ignore_decoder_errors,
-            av_decode=av_decode,
-            video_decode_audio=video_decode_audio,
-            **kwargs,
-        )
-        if auto_decode:
-            self._decoder = Decoder([NestedImagesPathHandler(self.image_decode)])
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/energon/chatml.py.
+
+import pickle
+import re
+import warnings
+
+from dataclasses import dataclass
+from typing import List, Union
+
+import torch
+
+from webdataset.autodecode import Decoder, imagehandler
+
+from megatron.energon.epathlib.epath import EPath
+from megatron.energon.flavors.base_dataset import Sample
+from megatron.energon.flavors.webdataset import DefaultDecoderWebdatasetFactory
+
+
+@dataclass
+class ChatMLSample(Sample):
+    """multi-turn complex samples with images and videos"""
+
+    imgs: List[torch.Tensor]
+    videos: List[List[torch.Tensor]]
+    conversation: str  # JSON string of GPT-format conversations
+
+
+class NestedImagesHandler:
+    def __init__(self, imagespec):
+        """Create an image handler.
+
+        :param imagespec: short string indicating the type of decoding
+        """
+        self.extensions = ["jpgs", "videos"]
+        self.extensions_mapping = {"jpgs": "jpg", "videos": "jpg"}
+        self.image_handler = imagehandler(imagespec)
+
+    def __call__(self, key, data):
+        """Perform nested image decoding.
+
+        :param key: file name extension
+        :param data: binary data
+        """
+        extension = re.sub(r".*[.]", "", key)
+        if extension.lower() not in self.extensions:
+            return None
+        data = pickle.loads(data)
+        key = self.extensions_mapping[extension]
+        if extension.lower() == "jpgs":
+            data = [self.image_handler(key, d) for d in data]
+        else:
+            data = [[self.image_handler(key, d) for d in video] for video in data]
+        return data
+
+
+class ChatMLWebdataset(DefaultDecoderWebdatasetFactory[ChatMLSample]):
+    __sample_type__ = ChatMLSample
+
+    def __init__(self, path: EPath, *, auto_decode: bool = True, **kwargs):
+        super().__init__(path, auto_decode=auto_decode, **kwargs)
+        if auto_decode:
+            self._decoder = Decoder(
+                [
+                    imagehandler(self.image_decode),
+                    NestedImagesHandler(self.image_decode),
+                    self._video_decoder,
+                ]
+            )
diff --git a/tools/datasets/qwenvl/data/image_processing.py b/tools/datasets/qwenvl/data/image_processing.py
index 822b3fc3..0a5cdd52 100644
--- a/tools/datasets/qwenvl/data/image_processing.py
+++ b/tools/datasets/qwenvl/data/image_processing.py
@@ -1,124 +1,121 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/image_processing.py. Below is the original copyright:
-# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved. Except portions as noted which are Copyright (c) 2023 OpenGVLab and licensed under the MIT license found in LICENSE.
-
-import math
-import random
-
-import numpy as np
-import torch
-
-from PIL import Image, ImageDraw
-from torchvision import transforms as T
-from torchvision.transforms import Compose, RandAugment, RandomResizedCrop, Resize, ToPILImage
-
-# config :https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/blob/main/preprocessor_config.json
-# Imagenet's mean and std.
-pixel_mean = [0.48145466, 0.4578275, 0.40821073]
-pixel_std = [0.26862954, 0.26130258, 0.27577711]
-
-# Reshape for broadcasting.
-pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)
-pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)
-
-
-# https://github.com/QwenLM/Qwen2.5-VL/blob/477fd9d4317266508705366ce36cac5b68d70936/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L89C1-L95C40
-def convert_to_rgb(pil_image: Image.Image) -> Image.Image:
-    if pil_image.mode == 'RGBA':
-        white_background = Image.new("RGB", pil_image.size, (255, 255, 255))
-        white_background.paste(pil_image, mask=pil_image.split()[3])  # Use alpha channel as mask
-        return white_background
-    else:
-        return pil_image.convert("RGB")
-
-
-def _transform_train_aug():
-    return Compose(
-        [
-            ToPILImage(),
-            Resize(scale=random.random() / 2 + 0.5),
-            convert_to_rgb,
-            RandAugment(
-                2,
-                5,
-                isPIL=True,
-                augs=[
-                    "Identity",
-                    "AutoContrast",
-                    "Brightness",
-                    "Sharpness",
-                    "Equalize",
-                    "ShearX",
-                    "ShearY",
-                    "TranslateX",
-                    "TranslateY",
-                    "Rotate",
-                ],
-            ),
-        ]
-    )
-
-
-def _transform_test():
-    return Compose([ToPILImage(), convert_to_rgb])
-
-
-def standardize_image(img):
-    """Standardize image pixel values."""
-    return (T.ToTensor()(img) - pixel_mean) / pixel_std
-
-
-def get_visual_transform(
-    img,  # Path
-    factor: int = 28,
-    min_pixels: int = 4 * 28 * 28,
-    max_pixels: int = 16384 * 28 * 28,
-    augment=False,
-):
-    # TODO(lizhiyu): Need to limit the aspect ratio of the image.
-    # (reference https://github.com/QwenLM/Qwen2.5-VL/blob/477fd9d4317266508705366ce36cac5b68d70936/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L72)
-    img = Image.open(img)
-    img = convert_to_rgb(img)
-    w, h = img.size
-    h_bar, w_bar = smart_resize(h, w, factor, min_pixels, max_pixels)
-    img = img.resize((w_bar, h_bar))
-
-    # Standardize pixel values.
-    img = standardize_image(img)
-    imgs = [img]
-    return imgs
-
-
-# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py
-def smart_resize(
-    height: int,
-    width: int,
-    factor: int = 28,
-    min_pixels: int = 4 * 28 * 28,
-    max_pixels: int = 16384 * 28 * 28,
-):
-    """Rescales the image so that the following conditions are met:
-
-    1. Both dimensions (height and width) are divisible by 'factor'.
-
-    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].
-
-    3. The aspect ratio of the image is maintained as closely as possible.
-
-    """
-    if height < factor or width < factor:
-        raise ValueError(f"height:{height} or width:{width} must be larger than factor:{factor}")
-    elif max(height, width) / min(height, width) > 200:
-        raise ValueError(
-            f"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}"
-        )
-    h_bar = round(height / factor) * factor
-    w_bar = round(width / factor) * factor
-    if h_bar * w_bar > max_pixels:
-        beta = math.sqrt((height * width) / max_pixels)
-        h_bar = math.floor(height / beta / factor) * factor
-        w_bar = math.floor(width / beta / factor) * factor
-    elif h_bar * w_bar < min_pixels:
-        beta = math.sqrt(min_pixels / (height * width))
-        h_bar = math.ceil(height * beta / factor) * factor
-        w_bar = math.ceil(width * beta / factor) * factor
-    return h_bar, w_bar
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/image_processing.py. Below is the original copyright:
+# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved. Except portions as noted which are Copyright (c) 2023 OpenGVLab and licensed under the MIT license found in LICENSE.
+
+import math
+import random
+
+import numpy as np
+import torch
+
+from PIL import Image, ImageDraw
+from torchvision import transforms as T
+from torchvision.transforms import Compose, RandAugment, RandomResizedCrop, Resize, ToPILImage
+
+# Imagenet's mean and std.
+pixel_mean = [123.675, 116.28, 103.53]
+pixel_std = [58.395, 57.12, 57.375]
+
+# Reshape for broadcasting.
+pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)
+pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)
+
+
+def convert_to_rgb(image):
+    return image.convert("RGB")
+
+
+def _transform_train_aug():
+    return Compose(
+        [
+            ToPILImage(),
+            Resize(scale=random.random() / 2 + 0.5),
+            convert_to_rgb,
+            RandAugment(
+                2,
+                5,
+                isPIL=True,
+                augs=[
+                    "Identity",
+                    "AutoContrast",
+                    "Brightness",
+                    "Sharpness",
+                    "Equalize",
+                    "ShearX",
+                    "ShearY",
+                    "TranslateX",
+                    "TranslateY",
+                    "Rotate",
+                ],
+            ),
+        ]
+    )
+
+
+def _transform_test():
+    return Compose([ToPILImage(), convert_to_rgb])
+
+
+def standardize_image(img):
+    """Standardize image pixel values."""
+    return (torch.Tensor(np.array(img)).permute(2, 0, 1) - pixel_mean) / pixel_std
+
+
+def get_visual_transform(
+    img,
+    factor: int = 28,
+    min_pixels: int = 56 * 56,
+    max_pixels: int = 14 * 14 * 4 * 1280,
+    augment=False,
+):
+    img = np.array(img)
+
+    if augment:
+        visual_transform = _transform_train_aug()
+    else:
+        visual_transform = _transform_test()
+
+    img = visual_transform(img)
+    w, h = img.size
+    h_bar, w_bar = smart_resize(h, w, factor, min_pixels, max_pixels)
+    img = img.resize((w_bar, h_bar))
+
+    # Standardize pixel values.
+    img = standardize_image(img)
+    imgs = [img]
+    return imgs
+
+
+# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py
+def smart_resize(
+    height: int,
+    width: int,
+    factor: int = 28,
+    min_pixels: int = 56 * 56,
+    max_pixels: int = 14 * 14 * 4 * 1280,
+):
+    """Rescales the image so that the following conditions are met:
+
+    1. Both dimensions (height and width) are divisible by 'factor'.
+
+    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].
+
+    3. The aspect ratio of the image is maintained as closely as possible.
+
+    """
+    if height < factor or width < factor:
+        raise ValueError(f"height:{height} or width:{width} must be larger than factor:{factor}")
+    elif max(height, width) / min(height, width) > 200:
+        raise ValueError(
+            f"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}"
+        )
+    h_bar = round(height / factor) * factor
+    w_bar = round(width / factor) * factor
+    if h_bar * w_bar > max_pixels:
+        beta = math.sqrt((height * width) / max_pixels)
+        h_bar = math.floor(height / beta / factor) * factor
+        w_bar = math.floor(width / beta / factor) * factor
+    elif h_bar * w_bar < min_pixels:
+        beta = math.sqrt(min_pixels / (height * width))
+        h_bar = math.ceil(height * beta / factor) * factor
+        w_bar = math.ceil(width * beta / factor) * factor
+    return h_bar, w_bar
diff --git a/tools/datasets/qwenvl/data/utils.py b/tools/datasets/qwenvl/data/utils.py
index e665867a..0f5d3d81 100644
--- a/tools/datasets/qwenvl/data/utils.py
+++ b/tools/datasets/qwenvl/data/utils.py
@@ -1,501 +1,501 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/utils.py. Below is the original copyright:
-# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import torch
-
-from megatron.core import mpu
-
-try:
-    from megatron import get_args
-except:
-    from megatron.training import get_args
-
-from megatron.training.global_vars import get_tokenizer
-
-
-def get_ltor_masks_and_position_ids(
-    data,
-    eod_token,
-    reset_position_ids,
-    reset_attention_mask,
-    eod_mask_loss,
-    create_attention_mask: bool = True,
-):
-    """Build masks and position id for left to right model."""
-
-    # Extract batch size and sequence length.
-    micro_batch_size, seq_length = data.size()
-
-    # Attention mask (lower triangular).
-    if reset_attention_mask:
-        att_mask_batch = micro_batch_size
-    else:
-        att_mask_batch = 1
-    if create_attention_mask:
-        attention_mask = torch.tril(
-            torch.ones((att_mask_batch, seq_length, seq_length), device=data.device)
-        ).view(att_mask_batch, 1, seq_length, seq_length)
-    else:
-        attention_mask = None
-
-    # Loss mask.
-    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)
-    if eod_mask_loss:
-        loss_mask[data == eod_token] = 0.0
-
-    # Position ids.
-    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)
-    position_ids = position_ids.unsqueeze(0).expand_as(data)
-    # We need to clone as the ids will be modifed based on batch index.
-    if reset_position_ids:
-        position_ids = position_ids.clone()
-
-    if reset_position_ids or reset_attention_mask:
-        # Loop through the batches:
-        for b in range(micro_batch_size):
-
-            # Find indecies where EOD token is.
-            eod_index = position_ids[b, data[b] == eod_token]
-            # Detach indecies from positions if going to modify positions.
-            if reset_position_ids:
-                eod_index = eod_index.clone()
-
-            # Loop through EOD indecies:
-            prev_index = 0
-            for j in range(eod_index.size()[0]):
-                i = eod_index[j]
-                # Mask attention loss.
-                if reset_attention_mask and attention_mask is not None:
-                    attention_mask[b, 0, (i + 1) :, : (i + 1)] = 0
-                # Reset positions.
-                if reset_position_ids:
-                    position_ids[b, (i + 1) :] -= i + 1 - prev_index
-                    prev_index = i + 1
-
-    if attention_mask is not None:
-        # Convert attention mask to binary:
-        attention_mask = attention_mask < 0.5
-
-    return attention_mask, loss_mask, position_ids
-
-
-def get_ltor_position_ids_packed_seq(data):
-    """
-    Given a input_seqs from custom mmap dataset, generate a
-    position_ids by searching negative tokens.
-    """
-
-    # Extract batch size and sequence length.
-    micro_batch_size, seq_length = data.size()
-
-    # Position ids.
-    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)
-    position_ids = position_ids.unsqueeze(0).expand_as(data)
-    # We need to clone as the ids will be modifed based on batch index.
-    position_ids = position_ids.clone()
-
-    # Loop through the batches:
-    for b in range(micro_batch_size):
-        # Find indecies where EOD token is.
-        eod_index = position_ids[b, data[b] < 0]
-        # Detach indecies from positions if going to modify positions.
-        eod_index = eod_index.clone()
-        # Loop through EOD indecies:
-        prev_index = 0
-        for j in range(eod_index.size()[0]):
-            i = eod_index[j]
-            position_ids[b, (i + 1) :] -= i + 1 - prev_index
-            prev_index = i + 1
-
-    return position_ids
-
-
-def get_batch_on_this_tp_rank_original(data_iterator, per_seq_average=False):
-    args = get_args()
-    tokenizer = get_tokenizer()
-
-    def _broadcast(item):
-        if item is None:
-            return
-        torch.distributed.broadcast(
-            item,
-            mpu.get_tensor_model_parallel_src_rank(),
-            group=mpu.get_tensor_model_parallel_group(),
-        )
-
-    if mpu.get_tensor_model_parallel_rank() == 0:
-
-        if isinstance(data_iterator, dict):
-            data = data_iterator
-        else:
-            data = next(data_iterator)
-
-        tokens_ = data["input_ids"].long()
-        labels_ = data["labels"].long()
-        tokens = tokens_[:, :-1].contiguous()
-        labels = labels_[:, 1:].contiguous()
-        # core/tensor_parallel/cross_entropy.py, target_mask = (target < vocab_start_index) | (target >= vocab_end_index)
-        # labels[labels == tokenizer.eos_token_id] = -100
-        # NOTE: if eos == pad, we map <eos> to  - 1 - eos_id, map these tokens back
-        tokens[tokens < 0] = -1 - tokens[tokens < 0]
-        eos_indices = (labels < 0).nonzero()
-        labels[labels == tokenizer.pad_token_id] = -100
-        labels[eos_indices[:, 0], eos_indices[:, 1]] = (
-            -1 - labels[eos_indices[:, 0], eos_indices[:, 1]]
-        )
-
-        attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
-            labels, -100, args.reset_position_ids, args.reset_attention_mask, args.eod_mask_loss
-        )
-
-        num_seqs = None
-        if per_seq_average:
-            # NOTE: raw dataset does not support sequence packing
-            num_seqs = torch.ones(
-                position_ids.shape[0], device=torch.cuda.current_device(), dtype=torch.int64
-            )
-            loss_mask = loss_mask / loss_mask.sum(dim=-1, keepdims=True)  # [mbs]
-
-        batch = {
-            "tokens": tokens.cuda(non_blocking=True),
-            "labels": labels.cuda(non_blocking=True),
-            "loss_mask": loss_mask.cuda(non_blocking=True),
-            "attention_mask": attention_mask.cuda(non_blocking=True),
-            "position_ids": position_ids.cuda(non_blocking=True),
-            "num_seqs": (num_seqs.cuda(non_blocking=True) if num_seqs is not None else None),
-        }
-
-        if args.pipeline_model_parallel_size == 1:
-            _broadcast(batch["tokens"])
-            _broadcast(batch["labels"])
-            _broadcast(batch["loss_mask"])
-            _broadcast(batch["attention_mask"])
-            _broadcast(batch["position_ids"])
-            _broadcast(batch["num_seqs"])
-
-        elif mpu.is_pipeline_first_stage():
-            _broadcast(batch["tokens"])
-            _broadcast(batch["attention_mask"])
-            _broadcast(batch["position_ids"])
-
-        elif mpu.is_pipeline_last_stage():
-            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
-            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
-            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
-            if getattr(args, "mtp_num_layers", None) is not None:
-                _broadcast(batch["tokens"])
-                _broadcast(batch["position_ids"])
-            _broadcast(batch["labels"])
-            _broadcast(batch["loss_mask"])
-            _broadcast(batch["attention_mask"])
-            _broadcast(batch["num_seqs"])
-
-    else:
-
-        tokens = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-        labels = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-        loss_mask = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.float32,
-            device=torch.cuda.current_device(),
-        )
-        mbs = args.micro_batch_size if args.reset_attention_mask else 1
-        attention_mask = torch.empty(
-            (mbs, 1, args.seq_length, args.seq_length),
-            dtype=torch.bool,
-            device=torch.cuda.current_device(),
-        )
-        position_ids = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-
-        num_seqs = None
-        if per_seq_average:
-            num_seqs = torch.empty(
-                (args.micro_batch_size,), dtype=torch.int64, device=torch.cuda.current_device()
-            )
-
-        if args.pipeline_model_parallel_size == 1:
-            _broadcast(tokens)
-            _broadcast(labels)
-            _broadcast(loss_mask)
-            _broadcast(attention_mask)
-            _broadcast(position_ids)
-            _broadcast(num_seqs)
-
-        elif mpu.is_pipeline_first_stage():
-            labels = None
-            loss_mask = None
-            num_seqs = None
-
-            _broadcast(tokens)
-            _broadcast(attention_mask)
-            _broadcast(position_ids)
-
-        elif mpu.is_pipeline_last_stage():
-            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
-            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
-            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
-            if getattr(args, "mtp_num_layers", None) is not None:
-                _broadcast(tokens)
-                _broadcast(position_ids)
-            else:
-                tokens = None
-                position_ids = None
-
-            _broadcast(labels)
-            _broadcast(loss_mask)
-            _broadcast(attention_mask)
-            _broadcast(num_seqs)
-
-        batch = {
-            "tokens": tokens,
-            "labels": labels,
-            "loss_mask": loss_mask,
-            "attention_mask": attention_mask,
-            "position_ids": position_ids,
-            "num_seqs": num_seqs,
-        }
-
-    return batch
-
-
-def get_position_id_on_this_tp_rank_idxmap_sft_packing(data_iterator):
-    args = get_args()
-    tokenizer = get_tokenizer()
-
-    def _broadcast(item):
-        if item is None:
-            return
-        torch.distributed.broadcast(
-            item,
-            mpu.get_tensor_model_parallel_src_rank(),
-            group=mpu.get_tensor_model_parallel_group(),
-        )
-
-    if mpu.get_tensor_model_parallel_rank() == 0:
-        if isinstance(data_iterator, dict):
-            data = data_iterator
-        else:
-            data = next(data_iterator)
-
-        actual_seqlen = args.seq_length
-        data["tokens"] = data["tokens"].long()
-        tokens = data["tokens"][..., :actual_seqlen]
-        position_ids = get_ltor_position_ids_packed_seq(tokens).cuda(non_blocking=True)
-    else:
-        # dtype: long
-        position_ids = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-    _broadcast(position_ids)
-    return position_ids
-
-
-def get_batch_on_this_tp_rank_idxmap_sft(data_iterator, per_seq_average=False):
-    args = get_args()
-    tokenizer = get_tokenizer()
-
-    def _broadcast(item):
-        if item is None:
-            return
-        torch.distributed.broadcast(
-            item,
-            mpu.get_tensor_model_parallel_src_rank(),
-            group=mpu.get_tensor_model_parallel_group(),
-        )
-
-    if mpu.get_tensor_model_parallel_rank() == 0:
-
-        if isinstance(data_iterator, dict):
-            data = data_iterator
-        else:
-            data = next(data_iterator)
-
-        # sanity check
-        assert data["tokens"].shape[-1] == 2 * args.seq_length
-        actual_seqlen = args.seq_length
-        data["tokens"] = data["tokens"].long()
-        tokens = data["tokens"][..., :actual_seqlen]
-        labels = data["tokens"][..., actual_seqlen:]
-        loss_mask = (labels != -100).float()
-
-        if args.reset_position_ids:
-            attention_mask = None
-            position_ids = get_ltor_position_ids_packed_seq(tokens)
-            has_pad = tokens[:, -1] >= 0
-            tokens[tokens < 0] = -tokens[tokens < 0] - 1
-        else:
-            tokens[tokens < 0] = -tokens[tokens < 0] - 1
-            attention_mask, _, position_ids = get_ltor_masks_and_position_ids(
-                tokens,
-                tokenizer.eod,
-                args.reset_position_ids,
-                args.reset_attention_mask,
-                False,
-                args.create_attention_mask_in_dataloader,
-            )
-
-        num_seqs = None
-        if per_seq_average:
-            num_seqs = torch.ones(
-                position_ids.shape[0], device=torch.cuda.current_device(), dtype=torch.int64
-            )
-            if args.reset_position_ids:
-                for b in range(position_ids.shape[0]):
-                    p = position_ids[b]
-                    start_indices = (p == 0).nonzero(as_tuple=True)[0]
-                    seqlens = start_indices[1:] - start_indices[:-1]
-                    seqlens = seqlens.cpu().numpy().tolist() + [
-                        p.shape[0] - start_indices[-1].item()
-                    ]
-                    subseqs = torch.split(loss_mask[b], seqlens)
-                    num_seqs[b] = len(seqlens) - int(has_pad[b])
-                    for subseq_idx, (start_idx, seqlen, subseq) in enumerate(
-                        zip(start_indices, seqlens, subseqs)
-                    ):
-                        if subseq_idx == num_seqs[b]:
-                            # NOTE: do not process pad sequence
-                            continue
-                        assert subseq.sum() > 0
-                        loss_mask[b, start_idx : start_idx + seqlen] /= subseq.sum()
-            else:
-                loss_mask = loss_mask / loss_mask.sum(dim=-1, keepdims=True)  # [mbs]
-
-        # dtype: long, long, float, bool, long
-        batch = {
-            "tokens": tokens.cuda(non_blocking=True),
-            "labels": labels.cuda(non_blocking=True),
-            "loss_mask": loss_mask.cuda(non_blocking=True),
-            "attention_mask": (
-                attention_mask.cuda(non_blocking=True) if attention_mask is not None else None
-            ),
-            "position_ids": position_ids.cuda(non_blocking=True),
-            "num_seqs": (num_seqs.cuda(non_blocking=True) if num_seqs is not None else None),
-        }
-
-        if args.pipeline_model_parallel_size == 1:
-            _broadcast(batch["tokens"])
-            _broadcast(batch["labels"])
-            _broadcast(batch["loss_mask"])
-            _broadcast(batch["attention_mask"])
-            _broadcast(batch["num_seqs"])
-
-        elif mpu.is_pipeline_first_stage():
-            _broadcast(batch["tokens"])
-            _broadcast(batch["attention_mask"])
-
-        elif mpu.is_pipeline_last_stage():
-            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
-            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
-            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
-            if getattr(args, "mtp_num_layers", None) is not None:
-                _broadcast(batch["tokens"])
-            _broadcast(batch["labels"])
-            _broadcast(batch["loss_mask"])
-            _broadcast(batch["attention_mask"])
-            _broadcast(batch["num_seqs"])
-
-        _broadcast(batch["position_ids"])
-
-    else:
-        # dtype: long, long, float, bool, long
-        tokens = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-        labels = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-        loss_mask = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.float32,
-            device=torch.cuda.current_device(),
-        )
-
-        attention_mask = None
-        if args.create_attention_mask_in_dataloader:
-            mbs = args.micro_batch_size if args.reset_attention_mask else 1
-            attention_mask = torch.empty(
-                (mbs, 1, args.seq_length, args.seq_length),
-                dtype=torch.bool,
-                device=torch.cuda.current_device(),
-            )
-        position_ids = torch.empty(
-            (args.micro_batch_size, args.seq_length),
-            dtype=torch.int64,
-            device=torch.cuda.current_device(),
-        )
-
-        num_seqs = None
-        if per_seq_average:
-            num_seqs = torch.empty(
-                (args.micro_batch_size,), dtype=torch.int64, device=torch.cuda.current_device()
-            )
-
-        if args.pipeline_model_parallel_size == 1:
-            _broadcast(tokens)
-            _broadcast(labels)
-            _broadcast(loss_mask)
-            _broadcast(attention_mask)
-            _broadcast(num_seqs)
-
-        elif mpu.is_pipeline_first_stage():
-            labels = None
-            loss_mask = None
-            num_seqs = None
-
-            _broadcast(tokens)
-            _broadcast(attention_mask)
-
-        elif mpu.is_pipeline_last_stage():
-            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
-            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
-            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
-            if getattr(args, "mtp_num_layers", None) is not None:
-                _broadcast(tokens)
-            else:
-                tokens = None
-            _broadcast(labels)
-            _broadcast(loss_mask)
-            _broadcast(attention_mask)
-            _broadcast(num_seqs)
-
-        _broadcast(position_ids)
-        batch = {
-            "tokens": tokens,
-            "labels": labels,
-            "loss_mask": loss_mask,
-            "attention_mask": attention_mask,
-            "position_ids": position_ids,
-            "num_seqs": num_seqs,
-        }
-
-    return batch
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/megatron_patch/data/utils.py. Below is the original copyright:
+# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+
+from megatron.core import mpu
+
+try:
+    from megatron import get_args
+except:
+    from megatron.training import get_args
+
+from megatron.training.global_vars import get_tokenizer
+
+
+def get_ltor_masks_and_position_ids(
+    data,
+    eod_token,
+    reset_position_ids,
+    reset_attention_mask,
+    eod_mask_loss,
+    create_attention_mask: bool = True,
+):
+    """Build masks and position id for left to right model."""
+
+    # Extract batch size and sequence length.
+    micro_batch_size, seq_length = data.size()
+
+    # Attention mask (lower triangular).
+    if reset_attention_mask:
+        att_mask_batch = micro_batch_size
+    else:
+        att_mask_batch = 1
+    if create_attention_mask:
+        attention_mask = torch.tril(
+            torch.ones((att_mask_batch, seq_length, seq_length), device=data.device)
+        ).view(att_mask_batch, 1, seq_length, seq_length)
+    else:
+        attention_mask = None
+
+    # Loss mask.
+    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)
+    if eod_mask_loss:
+        loss_mask[data == eod_token] = 0.0
+
+    # Position ids.
+    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)
+    position_ids = position_ids.unsqueeze(0).expand_as(data)
+    # We need to clone as the ids will be modifed based on batch index.
+    if reset_position_ids:
+        position_ids = position_ids.clone()
+
+    if reset_position_ids or reset_attention_mask:
+        # Loop through the batches:
+        for b in range(micro_batch_size):
+
+            # Find indecies where EOD token is.
+            eod_index = position_ids[b, data[b] == eod_token]
+            # Detach indecies from positions if going to modify positions.
+            if reset_position_ids:
+                eod_index = eod_index.clone()
+
+            # Loop through EOD indecies:
+            prev_index = 0
+            for j in range(eod_index.size()[0]):
+                i = eod_index[j]
+                # Mask attention loss.
+                if reset_attention_mask and attention_mask is not None:
+                    attention_mask[b, 0, (i + 1) :, : (i + 1)] = 0
+                # Reset positions.
+                if reset_position_ids:
+                    position_ids[b, (i + 1) :] -= i + 1 - prev_index
+                    prev_index = i + 1
+
+    if attention_mask is not None:
+        # Convert attention mask to binary:
+        attention_mask = attention_mask < 0.5
+
+    return attention_mask, loss_mask, position_ids
+
+
+def get_ltor_position_ids_packed_seq(data):
+    """
+    Given a input_seqs from custom mmap dataset, generate a
+    position_ids by searching negative tokens.
+    """
+
+    # Extract batch size and sequence length.
+    micro_batch_size, seq_length = data.size()
+
+    # Position ids.
+    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)
+    position_ids = position_ids.unsqueeze(0).expand_as(data)
+    # We need to clone as the ids will be modifed based on batch index.
+    position_ids = position_ids.clone()
+
+    # Loop through the batches:
+    for b in range(micro_batch_size):
+        # Find indecies where EOD token is.
+        eod_index = position_ids[b, data[b] < 0]
+        # Detach indecies from positions if going to modify positions.
+        eod_index = eod_index.clone()
+        # Loop through EOD indecies:
+        prev_index = 0
+        for j in range(eod_index.size()[0]):
+            i = eod_index[j]
+            position_ids[b, (i + 1) :] -= i + 1 - prev_index
+            prev_index = i + 1
+
+    return position_ids
+
+
+def get_batch_on_this_tp_rank_original(data_iterator, per_seq_average=False):
+    args = get_args()
+    tokenizer = get_tokenizer()
+
+    def _broadcast(item):
+        if item is None:
+            return
+        torch.distributed.broadcast(
+            item,
+            mpu.get_tensor_model_parallel_src_rank(),
+            group=mpu.get_tensor_model_parallel_group(),
+        )
+
+    if mpu.get_tensor_model_parallel_rank() == 0:
+
+        if isinstance(data_iterator, dict):
+            data = data_iterator
+        else:
+            data = next(data_iterator)
+
+        tokens_ = data["input_ids"].long()
+        labels_ = data["labels"].long()
+        tokens = tokens_[:, :-1].contiguous()
+        labels = labels_[:, 1:].contiguous()
+        # core/tensor_parallel/cross_entropy.py, target_mask = (target < vocab_start_index) | (target >= vocab_end_index)
+        # labels[labels == tokenizer.eos_token_id] = -100
+        # NOTE: if eos == pad, we map <eos> to  - 1 - eos_id, map these tokens back
+        tokens[tokens < 0] = -1 - tokens[tokens < 0]
+        eos_indices = (labels < 0).nonzero()
+        labels[labels == tokenizer.pad_token_id] = -100
+        labels[eos_indices[:, 0], eos_indices[:, 1]] = (
+            -1 - labels[eos_indices[:, 0], eos_indices[:, 1]]
+        )
+
+        attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
+            labels, -100, args.reset_position_ids, args.reset_attention_mask, args.eod_mask_loss
+        )
+
+        num_seqs = None
+        if per_seq_average:
+            # NOTE: raw dataset does not support sequence packing
+            num_seqs = torch.ones(
+                position_ids.shape[0], device=torch.cuda.current_device(), dtype=torch.int64
+            )
+            loss_mask = loss_mask / loss_mask.sum(dim=-1, keepdims=True)  # [mbs]
+
+        batch = {
+            "tokens": tokens.cuda(non_blocking=True),
+            "labels": labels.cuda(non_blocking=True),
+            "loss_mask": loss_mask.cuda(non_blocking=True),
+            "attention_mask": attention_mask.cuda(non_blocking=True),
+            "position_ids": position_ids.cuda(non_blocking=True),
+            "num_seqs": (num_seqs.cuda(non_blocking=True) if num_seqs is not None else None),
+        }
+
+        if args.pipeline_model_parallel_size == 1:
+            _broadcast(batch["tokens"])
+            _broadcast(batch["labels"])
+            _broadcast(batch["loss_mask"])
+            _broadcast(batch["attention_mask"])
+            _broadcast(batch["position_ids"])
+            _broadcast(batch["num_seqs"])
+
+        elif mpu.is_pipeline_first_stage():
+            _broadcast(batch["tokens"])
+            _broadcast(batch["attention_mask"])
+            _broadcast(batch["position_ids"])
+
+        elif mpu.is_pipeline_last_stage():
+            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
+            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
+            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
+            if getattr(args, "mtp_num_layers", None) is not None:
+                _broadcast(batch["tokens"])
+                _broadcast(batch["position_ids"])
+            _broadcast(batch["labels"])
+            _broadcast(batch["loss_mask"])
+            _broadcast(batch["attention_mask"])
+            _broadcast(batch["num_seqs"])
+
+    else:
+
+        tokens = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+        labels = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+        loss_mask = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.float32,
+            device=torch.cuda.current_device(),
+        )
+        mbs = args.micro_batch_size if args.reset_attention_mask else 1
+        attention_mask = torch.empty(
+            (mbs, 1, args.seq_length, args.seq_length),
+            dtype=torch.bool,
+            device=torch.cuda.current_device(),
+        )
+        position_ids = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+
+        num_seqs = None
+        if per_seq_average:
+            num_seqs = torch.empty(
+                (args.micro_batch_size,), dtype=torch.int64, device=torch.cuda.current_device()
+            )
+
+        if args.pipeline_model_parallel_size == 1:
+            _broadcast(tokens)
+            _broadcast(labels)
+            _broadcast(loss_mask)
+            _broadcast(attention_mask)
+            _broadcast(position_ids)
+            _broadcast(num_seqs)
+
+        elif mpu.is_pipeline_first_stage():
+            labels = None
+            loss_mask = None
+            num_seqs = None
+
+            _broadcast(tokens)
+            _broadcast(attention_mask)
+            _broadcast(position_ids)
+
+        elif mpu.is_pipeline_last_stage():
+            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
+            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
+            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
+            if getattr(args, "mtp_num_layers", None) is not None:
+                _broadcast(tokens)
+                _broadcast(position_ids)
+            else:
+                tokens = None
+                position_ids = None
+
+            _broadcast(labels)
+            _broadcast(loss_mask)
+            _broadcast(attention_mask)
+            _broadcast(num_seqs)
+
+        batch = {
+            "tokens": tokens,
+            "labels": labels,
+            "loss_mask": loss_mask,
+            "attention_mask": attention_mask,
+            "position_ids": position_ids,
+            "num_seqs": num_seqs,
+        }
+
+    return batch
+
+
+def get_position_id_on_this_tp_rank_idxmap_sft_packing(data_iterator):
+    args = get_args()
+    tokenizer = get_tokenizer()
+
+    def _broadcast(item):
+        if item is None:
+            return
+        torch.distributed.broadcast(
+            item,
+            mpu.get_tensor_model_parallel_src_rank(),
+            group=mpu.get_tensor_model_parallel_group(),
+        )
+
+    if mpu.get_tensor_model_parallel_rank() == 0:
+        if isinstance(data_iterator, dict):
+            data = data_iterator
+        else:
+            data = next(data_iterator)
+
+        actual_seqlen = args.seq_length
+        data["tokens"] = data["tokens"].long()
+        tokens = data["tokens"][..., :actual_seqlen]
+        position_ids = get_ltor_position_ids_packed_seq(tokens).cuda(non_blocking=True)
+    else:
+        # dtype: long
+        position_ids = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+    _broadcast(position_ids)
+    return position_ids
+
+
+def get_batch_on_this_tp_rank_idxmap_sft(data_iterator, per_seq_average=False):
+    args = get_args()
+    tokenizer = get_tokenizer()
+
+    def _broadcast(item):
+        if item is None:
+            return
+        torch.distributed.broadcast(
+            item,
+            mpu.get_tensor_model_parallel_src_rank(),
+            group=mpu.get_tensor_model_parallel_group(),
+        )
+
+    if mpu.get_tensor_model_parallel_rank() == 0:
+
+        if isinstance(data_iterator, dict):
+            data = data_iterator
+        else:
+            data = next(data_iterator)
+
+        # sanity check
+        assert data["tokens"].shape[-1] == 2 * args.seq_length
+        actual_seqlen = args.seq_length
+        data["tokens"] = data["tokens"].long()
+        tokens = data["tokens"][..., :actual_seqlen]
+        labels = data["tokens"][..., actual_seqlen:]
+        loss_mask = (labels != -100).float()
+
+        if args.reset_position_ids:
+            attention_mask = None
+            position_ids = get_ltor_position_ids_packed_seq(tokens)
+            has_pad = tokens[:, -1] >= 0
+            tokens[tokens < 0] = -tokens[tokens < 0] - 1
+        else:
+            tokens[tokens < 0] = -tokens[tokens < 0] - 1
+            attention_mask, _, position_ids = get_ltor_masks_and_position_ids(
+                tokens,
+                tokenizer.eod,
+                args.reset_position_ids,
+                args.reset_attention_mask,
+                False,
+                args.create_attention_mask_in_dataloader,
+            )
+
+        num_seqs = None
+        if per_seq_average:
+            num_seqs = torch.ones(
+                position_ids.shape[0], device=torch.cuda.current_device(), dtype=torch.int64
+            )
+            if args.reset_position_ids:
+                for b in range(position_ids.shape[0]):
+                    p = position_ids[b]
+                    start_indices = (p == 0).nonzero(as_tuple=True)[0]
+                    seqlens = start_indices[1:] - start_indices[:-1]
+                    seqlens = seqlens.cpu().numpy().tolist() + [
+                        p.shape[0] - start_indices[-1].item()
+                    ]
+                    subseqs = torch.split(loss_mask[b], seqlens)
+                    num_seqs[b] = len(seqlens) - int(has_pad[b])
+                    for subseq_idx, (start_idx, seqlen, subseq) in enumerate(
+                        zip(start_indices, seqlens, subseqs)
+                    ):
+                        if subseq_idx == num_seqs[b]:
+                            # NOTE: do not process pad sequence
+                            continue
+                        assert subseq.sum() > 0
+                        loss_mask[b, start_idx : start_idx + seqlen] /= subseq.sum()
+            else:
+                loss_mask = loss_mask / loss_mask.sum(dim=-1, keepdims=True)  # [mbs]
+
+        # dtype: long, long, float, bool, long
+        batch = {
+            "tokens": tokens.cuda(non_blocking=True),
+            "labels": labels.cuda(non_blocking=True),
+            "loss_mask": loss_mask.cuda(non_blocking=True),
+            "attention_mask": (
+                attention_mask.cuda(non_blocking=True) if attention_mask is not None else None
+            ),
+            "position_ids": position_ids.cuda(non_blocking=True),
+            "num_seqs": (num_seqs.cuda(non_blocking=True) if num_seqs is not None else None),
+        }
+
+        if args.pipeline_model_parallel_size == 1:
+            _broadcast(batch["tokens"])
+            _broadcast(batch["labels"])
+            _broadcast(batch["loss_mask"])
+            _broadcast(batch["attention_mask"])
+            _broadcast(batch["num_seqs"])
+
+        elif mpu.is_pipeline_first_stage():
+            _broadcast(batch["tokens"])
+            _broadcast(batch["attention_mask"])
+
+        elif mpu.is_pipeline_last_stage():
+            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
+            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
+            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
+            if getattr(args, "mtp_num_layers", None) is not None:
+                _broadcast(batch["tokens"])
+            _broadcast(batch["labels"])
+            _broadcast(batch["loss_mask"])
+            _broadcast(batch["attention_mask"])
+            _broadcast(batch["num_seqs"])
+
+        _broadcast(batch["position_ids"])
+
+    else:
+        # dtype: long, long, float, bool, long
+        tokens = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+        labels = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+        loss_mask = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.float32,
+            device=torch.cuda.current_device(),
+        )
+
+        attention_mask = None
+        if args.create_attention_mask_in_dataloader:
+            mbs = args.micro_batch_size if args.reset_attention_mask else 1
+            attention_mask = torch.empty(
+                (mbs, 1, args.seq_length, args.seq_length),
+                dtype=torch.bool,
+                device=torch.cuda.current_device(),
+            )
+        position_ids = torch.empty(
+            (args.micro_batch_size, args.seq_length),
+            dtype=torch.int64,
+            device=torch.cuda.current_device(),
+        )
+
+        num_seqs = None
+        if per_seq_average:
+            num_seqs = torch.empty(
+                (args.micro_batch_size,), dtype=torch.int64, device=torch.cuda.current_device()
+            )
+
+        if args.pipeline_model_parallel_size == 1:
+            _broadcast(tokens)
+            _broadcast(labels)
+            _broadcast(loss_mask)
+            _broadcast(attention_mask)
+            _broadcast(num_seqs)
+
+        elif mpu.is_pipeline_first_stage():
+            labels = None
+            loss_mask = None
+            num_seqs = None
+
+            _broadcast(tokens)
+            _broadcast(attention_mask)
+
+        elif mpu.is_pipeline_last_stage():
+            # Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.
+            # Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need
+            # to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.
+            if getattr(args, "mtp_num_layers", None) is not None:
+                _broadcast(tokens)
+            else:
+                tokens = None
+            _broadcast(labels)
+            _broadcast(loss_mask)
+            _broadcast(attention_mask)
+            _broadcast(num_seqs)
+
+        _broadcast(position_ids)
+        batch = {
+            "tokens": tokens,
+            "labels": labels,
+            "loss_mask": loss_mask,
+            "attention_mask": attention_mask,
+            "position_ids": position_ids,
+            "num_seqs": num_seqs,
+        }
+
+    return batch
diff --git a/tools/datasets/qwenvl/dataset_preparation.md b/tools/datasets/qwenvl/dataset_preparation.md
index 1bd24a84..6da409d6 100644
--- a/tools/datasets/qwenvl/dataset_preparation.md
+++ b/tools/datasets/qwenvl/dataset_preparation.md
@@ -1,142 +1,147 @@
-#  Reference
-Mainly based on [Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch/tree/main/toolkits/multimodal_data_preprocessing/),with necessary modifications for integration into the current training framework.
-
-# Dataset Download & Preprocessing
-
-```bash
-cd /mnt
-
-mkdir llava-datasets
-cd llava-datasets
-git clone https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain
-cd LLaVA-Pretrain
-unzip images.zip
-
-#convert to webdataset format:
-cd ./tools/datasets/qwenvl/
-
-
-export PYTHONPATH=$PYTHONPATH:../../../../third_party/Megatron-LM/
-
-python convert_custom_dataset_to_wds_chatml_str.py \
-    --dataset-root=/mnt/LLaVA-Pretrain \
-    --output-root=/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/ \
-    --json=blip_laion_cc_sbu_558k.json \
-    --train-split 1 \
-    --val-split 0 \
-    --images-key=image \
-    --videos-key=video \
-    --vision-root=/mnt/LLaVA-Pretrain \
-    --max-samples-per-tar 100000000 \
-    --dp-size 1 \
-    --num-workers 20
-```
-The preprocessed datas will stored at the output-root path `/mnt/LLaVA-Pretrain/blip_laion_cc_sbu_558k/wds-1`.
-
-## Prepare Multimodal Datasets Based on ShareGPT Format
-
-The current Qwen2-VL/Qwen2.5-VL supports training with complex multimodal samples in a specific ShareGPT-like format. Follow the instructions below to convert your datasets into the supported format.
-
-## ShareGPT Data Format
-You may need to manually convert your dataset into the ShareGPT format, structured as follows:
-```json
-[
-  {
-    "conversations": [
-        {
-            "from": "human",
-            "value": "<image>human instruction<image>"
-        },
-        {
-            "from": "gpt",
-            "value": "model response"
-        },
-        {
-            "from": "human",
-            "value": "<video><video>human instruction"
-        },
-        {
-            "from": "gpt",
-            "value": "model response"
-        }
-    ],
-    "images": [
-        "path/to/image1.jpg",
-        "path/to/image2.jpg",
-    ],
-    "videos": [
-        "path/to/video1.mp4",
-        "path/to/video2.mp4"
-    ]
-  },
-  {
-    // another sample ...
-  }
-]
-```
-Here,the images and videos lists should contain the raw file paths corresponding to `<image>` and `<video> `tokens in the conversation in order.
-
-### Video Frame Extraction
-Before training, you must convert video files into frame images using tools such as DataJuicer.
-
-For example, given path/to/video1.mp4 located at dataset_root/path/to/video1.mp4, the exported frames should be stored under dataset_root/path/to/video1/. Frame filenames should be in sequential order to ensure temporal alignment.
-
-Recommended filename format:
-```
-00001.jpg # frame 1
-00002.jpg # frame 2
-...
-```
-
-To enable temporal alignment and support dynamic resolution sampling in Qwen2.5-VL, you must save the exported frame rate for each video in a JSON file. For instance, for the video frames saved in `dataset_root/path/to/video1/`, create a `dataset_root/path/to/video1.json` with the following structure:
-```
-{
-    "fps": "2.0" // Exported frame rate
-}
-```
-
-### Video Frame Extraction(Demo)
-We provide a lightweight script to convert LLaVA-format video datasets (e.g., LLaVA-Video-178K) into the ShareGPT format for small-scale testing. For large-scale datasets, we recommend using a dedicated tool for frame extraction.
-```
-cd /mnt/llava-datasets
-wget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/qwen-datasets/LLaVA-Video-178K-demo.tar
-tar -xvf LLaVA-Video-178K-demo.tar
-
-cd /workspace/Pai-Megatron-Patch/toolkits/multimodal_data_preprocessing
-python build_llava_frame_dataset.py \
-    --dataset-root /mnt/llava-datasets/LLaVA-Video-178K \
-    --time-interval 0.5 # Save one frame every 0.5 seconds (FPS  2.0)
-
-```
-
-You can then run `convert_custom_dataset_to_wds_chatml.py` to convert `/mnt/llava-datasets/LLaVA-Video-178K` into the training format.
-
-### Converting LLaVA-style Image Datasets
-
-For LLaVA-style image datasets in .jsonl format, simply run the following script to convert them for use with `convert_custom_dataset_to_wds_chatml.py`:
-
-```
-# replace `image` key with `images`
-python replace_llava_image_key.py \
-    --input-file your_raw_dataset.json_or_jsonl \
-    --output-file dataset.json
-
-```
-
-Convert to ChatML Format
-Assuming your **ShareGPT-formatted** dataset looks like this:
-```
-dataset_root/
--   dataset.json
--   ...
-```
-
-Run the following to convert the dataset into WebDataset format for training. Output will be stored in `dataset_root/wds`.
-```
-python tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py \
---dataset-root dataset_root \
---json dataset.json \
---train-split 9 \
---val-split 1 \
---test-split 0
-```
+#  Reference
+Mainly based on [Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch/tree/main/toolkits/multimodal_data_preprocessing/),with necessary modifications for integration into the current training framework.
+
+# Dataset Download
+
+```bash
+cd /mnt
+
+mkdir llava-datasets
+cd llava-datasets
+git clone https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain
+cd LLaVA-Pretrain
+unzip images.zip
+
+#convert to webdataset format:
+cd /workspace/tools/datasets/qwenvl/
+python convert_llava_pretrain_to_wds.py /mnt/llava-datasets/LLaVA-Pretrain/
+
+#convert to megatron-energon format:
+cd /mnt/llava-datasets/LLaVA-Pretrain/wds
+energon prepare ./
+
+#select the following values for the presented options:
+> Please enter a desired train/val/test split like "0.5, 0.2, 0.3" or "8,1,1": 9,1,0
+> Do you want to create a dataset.yaml interactively? [Y/n]: Y
+> Please enter a number to choose a class: 10 (VQAWebdataset)
+> Do you want to set a simple field_map[Y] (or write your own sample_loader [n])? [Y/n]: Y
+> Please enter a webdataset field name for 'image' (<class 'torch.Tensor'>): jpg
+> Please enter a webdataset field name for 'context' (<class 'str'>): json[0][value]
+> Please enter a webdataset field name for 'answers' (typing.Optional[typing.List[str]], default: None): json[1][value]
+> Please enter a webdataset field name for 'answer_weights' (typing.Optional[torch.Tensor], default: None):
+```
+
+You can also directly get the preprocessed data:
+```bash
+cd /mnt/llava-datasets/LLaVA-Pretrain/
+wget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/vlm-datasets/wds.tgz
+tar -zxf wds.tgz
+```
+
+## Prepare Multimodal Datasets Based on ShareGPT Format
+
+The current Qwen2-VL/Qwen2.5-VL supports training with complex multimodal samples in a specific ShareGPT-like format. Follow the instructions below to convert your datasets into the supported format.
+
+## ShareGPT Data Format
+You may need to manually convert your dataset into the ShareGPT format, structured as follows:
+```json
+[
+  {
+    "conversations": [
+        {
+            "from": "human",
+            "value": "<image>human instruction<image>"
+        },
+        {
+            "from": "gpt",
+            "value": "model response"
+        },
+        {
+            "from": "human",
+            "value": "<video><video>human instruction"
+        },
+        {
+            "from": "gpt",
+            "value": "model response"
+        }
+    ],
+    "images": [
+        "path/to/image1.jpg",
+        "path/to/image2.jpg",
+    ],
+    "videos": [
+        "path/to/video1.mp4",
+        "path/to/video2.mp4"
+    ]
+  },
+  {
+    // another sample ...
+  }
+]
+```
+Here,the images and videos lists should contain the raw file paths corresponding to `<image>` and `<video> `tokens in the conversation in order.
+
+### Video Frame Extraction
+Before training, you must convert video files into frame images using tools such as DataJuicer.
+
+For example, given path/to/video1.mp4 located at dataset_root/path/to/video1.mp4, the exported frames should be stored under dataset_root/path/to/video1/. Frame filenames should be in sequential order to ensure temporal alignment.
+
+Recommended filename format:
+```
+00001.jpg # frame 1
+00002.jpg # frame 2
+...
+```
+
+To enable temporal alignment and support dynamic resolution sampling in Qwen2.5-VL, you must save the exported frame rate for each video in a JSON file. For instance, for the video frames saved in `dataset_root/path/to/video1/`, create a `dataset_root/path/to/video1.json` with the following structure:
+```
+{
+    "fps": "2.0" // Exported frame rate
+}
+```
+
+### Video Frame Extraction(Demo)
+We provide a lightweight script to convert LLaVA-format video datasets (e.g., LLaVA-Video-178K) into the ShareGPT format for small-scale testing. For large-scale datasets, we recommend using a dedicated tool for frame extraction.
+```
+cd /mnt/llava-datasets
+wget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/qwen-datasets/LLaVA-Video-178K-demo.tar
+tar -xvf LLaVA-Video-178K-demo.tar
+
+cd /workspace/Pai-Megatron-Patch/toolkits/multimodal_data_preprocessing
+python build_llava_frame_dataset.py \
+    --dataset-root /mnt/llava-datasets/LLaVA-Video-178K \
+    --time-interval 0.5 # Save one frame every 0.5 seconds (FPS  2.0)
+
+```
+
+You can then run `convert_custom_dataset_to_wds_chatml.py` to convert `/mnt/llava-datasets/LLaVA-Video-178K` into the training format.
+
+### Converting LLaVA-style Image Datasets
+
+For LLaVA-style image datasets in .jsonl format, simply run the following script to convert them for use with `convert_custom_dataset_to_wds_chatml.py`:
+
+```
+# replace `image` key with `images`
+python replace_llava_image_key.py \
+    --input-file your_raw_dataset.json_or_jsonl \
+    --output-file dataset.json
+
+```
+
+Convert to ChatML Format
+Assuming your **ShareGPT-formatted** dataset looks like this:
+```
+dataset_root/
+-   dataset.json
+-   ...
+```
+
+Run the following to convert the dataset into WebDataset format for training. Output will be stored in `dataset_root/wds`.
+```
+python tools/datasets/qwenvl/convert_custom_dataset_to_wds_chatml.py \
+--dataset-root dataset_root \
+--json dataset.json \
+--train-split 9 \
+--val-split 1 \
+--test-split 0
+```
diff --git a/tools/datasets/qwenvl/replace_llava_image_key.py b/tools/datasets/qwenvl/replace_llava_image_key.py
index b0431247..cd17e30e 100644
--- a/tools/datasets/qwenvl/replace_llava_image_key.py
+++ b/tools/datasets/qwenvl/replace_llava_image_key.py
@@ -1,36 +1,36 @@
-# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/replace_llava_image_key.py
-import json
-import os
-
-from argparse import ArgumentParser
-
-
-def process(in_file, out_file):
-    d = os.path.dirname(out_file)
-    os.makedirs(d, exist_ok=True)
-
-    try:
-        with open(in_file, "r") as f:
-            data = json.load(f)
-    except:
-        with open(in_file, "r") as f:
-            data = [json.loads(f) for l in f.readlines()]
-    for i, sample in enumerate(data):
-        if isinstance(sample, list):
-            assert len(sample) == 1
-            data[i] = sample[0]
-            if "image" in data[i]:
-                data[i]["images"] = [data[i].pop("image")]
-
-    with open(out_file, "w") as f:
-        json.dump(data, f)
-
-
-if __name__ == "__main__":
-    argparser = ArgumentParser()
-
-    argparser.add_argument("--input-file", type=str, required=True)
-    argparser.add_argument("--output-file", type=str, default="dataset.json")
-
-    args = argparser.parse_args()
-    process(args.input_file, args.output_file)
+# Copied from https://github.com/alibaba/Pai-Megatron-Patch/blob/8949a6647cbf6b39837ad3dd911fa4aa0726895b/toolkits/multimodal_data_preprocessing/replace_llava_image_key.py
+import json
+import os
+
+from argparse import ArgumentParser
+
+
+def process(in_file, out_file):
+    d = os.path.dirname(out_file)
+    os.makedirs(d, exist_ok=True)
+
+    try:
+        with open(in_file, "r") as f:
+            data = json.load(f)
+    except:
+        with open(in_file, "r") as f:
+            data = [json.loads(f) for l in f.readlines()]
+    for i, sample in enumerate(data):
+        if isinstance(sample, list):
+            assert len(sample) == 1
+            data[i] = sample[0]
+            if "image" in data[i]:
+                data[i]["images"] = [data[i].pop("image")]
+
+    with open(out_file, "w") as f:
+        json.dump(data, f)
+
+
+if __name__ == "__main__":
+    argparser = ArgumentParser()
+
+    argparser.add_argument("--input-file", type=str, required=True)
+    argparser.add_argument("--output-file", type=str, default="dataset.json")
+
+    args = argparser.parse_args()
+    process(args.input_file, args.output_file)
