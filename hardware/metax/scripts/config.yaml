defaults:
  - train: mixtral_8x16b 
  - _self_

experiment:
  exp_name: unified-runner 
  exp_dir: ./outputs
  entrypoint: ./flagscale/train/train_mixtral.py 
  task: train
  hostfile: /share/project/ayl/FlagScale/hostfile
  backend: megatron
  shell_cmds: null 
  ssh_port: null
  envs:
    CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NCCL_SOCKET_IFNAME: "eth0"
    NCCL_IB_DISABLE: 0
    NCCL_IB_CUDA_SUPPORT: 1
    NCCL_IB_GID_INDEX: 0
    NCCL_IB_TIMEOUT: 23
    NCCL_IB_RETRY_CNT: 7
    OMP_NUM_THREADS: 4
    GLOO_SOCKET_IFNAME: eth0
    NCCL_IB_HCA: "mlx5_2,mlx5_5"

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra 
