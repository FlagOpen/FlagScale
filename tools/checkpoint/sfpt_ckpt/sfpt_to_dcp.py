import argparse
from argparse import Namespace
import os
from pathlib import Path

import torch
from megatron.core.dist_checkpointing import ShardedTensor, save
from megatron.core.dist_checkpointing.serialization import get_default_save_common_strategy


def convert_sfpt_ckpt_to_dist_ckpt(input_dir, output_dir):
    # Distributed checkpoint loading requires the distributed environment to be initialized
    rank = int(os.getenv('RANK', '0'))
    world_size = int(os.getenv("WORLD_SIZE", '1'))
    print(f'Rank: {rank}, World size: {world_size}')
    torch.distributed.init_process_group(
        backend="gloo", world_size=world_size, rank=rank
    )

    input_ckpt_dir = os.path.join(input_dir)
    if not os.path.isdir(input_ckpt_dir):
        raise ValueError(f"Checkpoint directory {input_ckpt_dir} does not exist")

    ckpt_output_dir = os.path.join(output_dir, "iter_0000000")
    if not os.path.exists(ckpt_output_dir):
        os.makedirs(ckpt_output_dir)

    for root, dirs, files in os.walk(input_ckpt_dir):
        for file in files:
            file_path = os.path.join(root, file)
            print(f"Processing file: {file_path}")
            state_dict = torch.load(file_path)
            assert len(state_dict) == 1
            key = list(state_dict.keys())[0]
            tensor = state_dict[key]
            sharded_state_dict = {}
            sharded_state_dict[key] = ShardedTensor.from_rank_offsets(
                key,
                tensor,
            )
            save(sharded_state_dict, ckpt_output_dir)
    
    # Fake the minimal args for the checkpoint loading processing
    state_dict = {}
    args = Namespace(
       tensor_model_parallel_size=1,
       pipeline_model_parallel_size=1,
    )
    state_dict['args'] = args
    common_strategy = get_default_save_common_strategy()
    common_strategy.save_common(state_dict, Path(ckpt_output_dir))

    # add the latest_checkpointed_iteration file
    with open(os.path.join(output_dir, "latest_checkpointed_iteration.txt"), "w") as f:
        f.write("0")


def parse_args():
    parser = argparse.ArgumentParser(description="Convert single-file-per-tensor checkpoint to distributed checkpoint.")
    parser.add_argument("--input_dir", type=str, required=True, help="Input directory containing the single-file-per-tensor checkpoint.")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory to save the distributed checkpoint.")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    convert_sfpt_ckpt_to_dist_ckpt(args.input_dir, args.output_dir)