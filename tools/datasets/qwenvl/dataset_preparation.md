# ğŸ“ Reference
Mainly based on official [Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch/tree/main/toolkits/multimodal_data_preprocessing/),with necessary modifications for integration into the current training framework.

# æ•°æ®é›†ä¸‹è½½

```bash
cd /mnt

mkdir llava-datasets
cd llava-datasets
git clone https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain
cd LLaVA-Pretrain
unzip images.zip

#convert to webdataset format:
cd /workspace/Pai-Megatron-Patch/toolkits/pretrain_data_preprocessing
python convert_llava_pretrain_to_wds.py /mnt/llava-datasets/LLaVA-Pretrain/

#convert to megatron-energon format:
cd /mnt/llava-datasets/LLaVA-Pretrain/wds
energon prepare ./

#select the following values for the presented options:
> Please enter a desired train/val/test split like "0.5, 0.2, 0.3" or "8,1,1": 9,1,0
> Do you want to create a dataset.yaml interactively? [Y/n]: Y
> Please enter a number to choose a class: 10 (VQAWebdataset)
> Do you want to set a simple field_map[Y] (or write your own sample_loader [n])? [Y/n]: Y
> Please enter a webdataset field name for 'image' (<class 'torch.Tensor'>): jpg
> Please enter a webdataset field name for 'context' (<class 'str'>): json[0][value]
> Please enter a webdataset field name for 'answers' (typing.Optional[typing.List[str]], default: None): json[1][value]
> Please enter a webdataset field name for 'answer_weights' (typing.Optional[torch.Tensor], default: None):
```

## å‡†å¤‡åŸºäºShareGPTæ ¼å¼çš„Energonå¤šæ¨¡æ€å¤æ‚æ•°æ®é›†

å½“å‰Qwen2-VL/Qwen2.5-VLæ”¯æŒç‰¹å®šæ ¼å¼çš„å¤æ‚å¤šæ¨¡æ€æ ·æœ¬çš„è®­ç»ƒï¼Œæ‚¨å¯æŒ‰ç…§ä¸‹è¿°æµç¨‹å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸ºæˆ‘ä»¬æ”¯æŒçš„æ ¼å¼ã€‚

### åŸå§‹æ•°æ®

åœ¨è½¬æ¢å‰ï¼Œä½ å¯èƒ½éœ€è¦è‡ªè¡Œå°†æ•°æ®é›†è½¬æ¢ä¸º**sharegptæ ¼å¼**ï¼Œsharegptæ ¼å¼çš„ç¤ºä¾‹å¦‚ä¸‹:
```json
[
  {
    "conversations": [
        {
            "from": "human",
            "value": "<image>human instruction<image>"
        },
        {
            "from": "gpt",
            "value": "model response"
        },
        {
            "from": "human",
            "value": "<video><video>human instruction"
        },
        {
            "from": "gpt",
            "value": "model response"
        }
    ],
    "images": [
        "path/to/image1.jpg",
        "path/to/image2.jpg",
    ],
    "videos": [
        "path/to/video1.mp4",
        "path/to/video2.mp4"
    ]
  },
  {
    // another sample ...
  }
]
```
å…¶ä¸­ï¼Œ`images`ä¸`videos`åˆ—è¡¨ä¿å­˜æ‰€æœ‰å›¾åƒ/è§†é¢‘çš„åŸå§‹è·¯å¾„ï¼Œä¸”ä¾æ¬¡ä¸å¯¹è¯ä¸­çš„`<image>`ä¸`<video>`æ ‡è®°å¯¹åº”ã€‚

### è§†é¢‘æŠ½å¸§
åœ¨è®­ç»ƒå‰ï¼Œæ‚¨éœ€è¦ä½¿ç”¨DataJuicerç­‰å·¥å…·å°†æ•°æ®é›†ä¸­çš„è§†é¢‘è½¬æ¢ä¸ºä¸€ç³»åˆ—å¸§å›¾åƒã€‚

ä»¥`path/to/video1.mp4`ä¸ºä¾‹ï¼Œå‡è®¾å…¶ä¿å­˜åœ¨`dataset_root/path/to/video1.mp4`, æœ€ç»ˆæ‚¨å¯¼å‡ºçš„å¸§åº”å½“ä¿å­˜åœ¨ `dataset_root/path/to/video1/` è¿™ä¸€æ–‡ä»¶å¤¹ã€‚æ­¤å¤–ï¼Œæ‚¨éœ€è¦ä¿è¯å¸§å›¾åƒçš„æ—¶é—´é¡ºåºä¸æ–‡ä»¶åå­—å…¸åºé¡ºåºä¸€è‡´ã€‚
æ¨èæ–‡ä»¶åç¤ºä¾‹å¦‚ä¸‹
```
00001.jpg # frame 1
00002.jpg # frame 2
...
```

é€šè¿‡å¼•å…¥åŠ¨æ€åˆ†è¾¨ç‡é‡‡æ ·ä»¥åŠç»å¯¹æ—¶é—´å¯¹é½æŠ€æœ¯ï¼ŒQwen2.5-VLèƒ½æ›´å¥½åœ°æ”¯æŒå¯¹äºä¸åŒFPSçš„è§†é¢‘çš„ç†è§£ã€‚ä¸ºäº†å¯ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œå¯¹äºæ¯ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œåœ¨æŠ½å¸§çš„åŒæ—¶ï¼Œæ‚¨åŒæ—¶éœ€è¦ä¿å­˜æ‰€æŠ½å¸§çš„å¸§ç‡åˆ°jsonæ–‡ä»¶ä¸­ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¿å­˜åˆ°`dataset_root/path/to/video1/`çš„å¸§ï¼Œæ‚¨éœ€è¦å°†å¸§ç‡æŒ‰ä¸‹åˆ—æ ¼å¼ä¿å­˜åˆ°`dataset_root/path/to/video1.json`ä¸­ã€‚
```
{
    "fps": "2.0 (è¯¥è§†é¢‘å¯¼å‡ºå¸§çš„å¸§ç‡)"
}
```

å¯¹äºLLaVA-Video-178Kç­‰llavaæ ¼å¼è§†é¢‘æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›äº†ç®€æ˜“è„šæœ¬å°†å…¶å¤„ç†æˆsharegptæ ¼å¼ä¾›å°è§„æ¨¡æµ‹è¯•ä½¿ç”¨ã€‚å¯¹äºå¤§è§„æ¨¡ä»»åŠ¡ï¼Œæˆ‘ä»¬ä»æ¨èä½¿ç”¨ä¸“é—¨çš„æ•°æ®å¤„ç†å·¥å…·å¯¹å…¶è¿›è¡ŒæŠ½å¸§ã€‚
è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä¸‹è½½å¹¶å¤„ç†LLaVA-Video-178Kçš„éƒ¨åˆ†æ•°æ®(NextQA)ã€‚
```
cd /mnt/llava-datasets
wget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/qwen-datasets/LLaVA-Video-178K-demo.tar
tar -xvf LLaVA-Video-178K-demo.tar

cd /workspace/Pai-Megatron-Patch/toolkits/multimodal_data_preprocessing
python build_llava_frame_dataset.py \
    --dataset-root /mnt/llava-datasets/LLaVA-Video-178K \
    --time-interval 0.5 # æ¯0.5ç§’ä¿å­˜ä¸€å¸§ï¼Œå¯¼å‡ºå¸§å¸§ç‡ä¸º2.0 (å®é™…å¯èƒ½æœ‰èˆå…¥ï¼Œä»¥ä¿å­˜çš„jsonæ–‡ä»¶ä¸ºå‡†)

```

ç„¶åæ‚¨å¯ä»¥å¯¹`/mnt/llava-datasets/LLaVA-Video-178K`è°ƒç”¨`convert_custom_dataset_to_wds_chatml.py`åˆ¶ä½œè®­ç»ƒæ•°æ®é›†ã€‚

### å…¶ä»–

å¯¹äºllavaæ ¼å¼çš„å›¾åƒæ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹è¿°è„šæœ¬å¤„ç†jsonlï¼Œå³å¯ä½¿ç”¨`convert_custom_dataset_to_wds_chatml.py`åˆ¶ä½œè®­ç»ƒæ•°æ®é›†ã€‚

```
# replace `image` key with `images`
python replace_llava_image_key.py \
    --input-file your_raw_dataset.json_or_jsonl \
    --output-file dataset.json

```

### è½¬æ¢ä¸ºchatml
å‡è®¾**sharegptæ ¼å¼**æ ¼å¼çš„æ•°æ®é›†ç›®å½•æ–‡ä»¶ç»“æ„å¦‚ä¸‹:
```
dataset_root/
-   dataset.json
-   ...
```

è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†ä¸Šè¿°å‡†å¤‡å¥½çš„jsonæ•°æ®é›†è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼, å¹¶å­˜å‚¨åˆ°`dataset_root/wds`æ–‡ä»¶å¤¹
```
python toolkits/pretrain_data_preprocessing/convert_custom_dataset_to_wds_chatml.py \
--dataset-root dataset_root \
--json dataset.json \
--train-split 9 \
--val-split 1 \
--test-split 0
```
